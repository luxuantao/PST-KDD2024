<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting conversions in display advertising based on URL embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-08-27">27 Aug 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yang</forename><surname>Qiu</surname></persName>
							<email>yanq.qiu@jellyfish.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Ã‰cole Polytechnique &amp; Jellyfish</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikolaos</forename><surname>Tziortziotis</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>AdKDD &apos;20, August 23</addrLine>
									<postCode>2020</postCode>
									<settlement>San Diego</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting conversions in display advertising based on URL embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-08-27">27 Aug 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
					<idno type="arXiv">arXiv:2008.12003v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online display advertising is growing rapidly in recent years thanks to the automation of the ad buying process. Real-time bidding (RTB) allows the automated trading of ad impressions between advertisers and publishers through real-time auctions. In order to increase the effectiveness of their campaigns, advertisers should deliver ads to the users who are highly likely to be converted (i.e., purchase, registration, website visit, etc.) in the near future. In this study, we introduce and examine different models for estimating the probability of a user converting, given their history of visited URLs. Inspired by natural language processing, we introduce three URL embedding models to compute semantically meaningful URL representations. To demonstrate the effectiveness of the different proposed representation and conversion prediction models, we have conducted experiments on real logged events collected from an advertising platform.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In online display advertising <ref type="bibr" target="#b22">[23]</ref>, advertisers promote their products by embedding ads on the publisher's web page. The majority of all these online display ads are served through Real-Time Bidding (RTB) <ref type="bibr" target="#b5">[6]</ref>. RTB allows the publishers to sell their ad placements via the Supply-Side Platform (SSP) and the advertisers to purchase these via the Demand-Side Platform (DSP). More specifically, each time a user visits a website that contains a banner placement, an auction is triggered. The publisher sends user's information to the SSP, which forwards this information to the Ad exchange (AdX), and finally the AdX sends a bid request to the DSPs. Then each DSP decides if it will submit or not a bid response for this impression, based on its information about user, advertisement, urls, etc. Once the DSPs send back to the AdX their bids, a public auction takes place with the impression to be sold to the highest bidder. Figure <ref type="figure" target="#fig_0">1</ref> briefly illustrates the procedure of online display advertising. DSPs are agent platforms that help advertisers optimise their advertising strategies. Roughly speaking, DSPs try to estimate the optimal bid price for an ad impression in order to maximise the audience of the campaigns of their advertisers, given some budget constraints. The bid price of an ad impression is highly related to the additive value that this impression could have on the advertising campaign (i.e., the number of ad impressions, clicks or conversions, etc.). In this context, advertisers have at their disposal a number of different pricing models. In the case where the objective of the advertisers is to maximise the exposure of their advertising message to a targeted audience, paying per impression, referred as cost-permille (CPM), is probably the best option for them. Nevertheless, in most of the cases, performance display advertising is more attractive to advertisers that are interested in accomplishing specific goals reducing their risks. In this case, advertisers are willing to pay for an ad impression if and only if that impression will drive the user to take a predefined action/conversion <ref type="bibr" target="#b13">[14]</ref>, such as a visit on the advertiser's website, a purchase of a product, etc. Two performance payment models have been introduced for this purpose, referred as cost-per-click (CPC) and cost-per-action (CPA).</p><p>In performance-driven display advertising, DSPs submit a bid for a given ad impression based on the CPC or CPA that the advertiser is willing to pay. To determine the optimal bid price for an ad impression, DSPs estimate the expected cost per impression, called eCPI, which is either equal to the click-through-rate (CTR) for this impression multiplied by the value of CPC, or the conversion rate (CVR) multiplied by the value of CPA <ref type="bibr" target="#b4">[5]</ref>. As a result, accurate CTR/CVR prediction plays an important role in the success of online advertising. For this purpose, DSPs build CTR/CVR prediction models able to estimate the probability a user converting after their exposure to an advertisement. The accuracy of these models is of high importance for the success of the campaign as if we overestimate click or conversation rates, we will probably submit quite higher bids than we should do, winning possible useless ad impressions. On the other hand, if these rates are underestimated, we will probably miss ad impressions likely to lead to a user action.</p><p>In this work we examine the user conversion problem, where given an advertiser, we want to predict if a user will be converted or not based on their history. In contrast to previous works that use a number of features related to the user profile, ad information and context information, we consider only the user's browsing history. More specifically, each user is represented as a sequence of URLs visited by the user in a single day. Therefore, the problem examined in this paper can be formally described as: given a user's sequence of URLs from a single day, predict the probability this user to take a predefined action on the next day. In our case, a user is considered as converted if they visit the advertiser's website. Due to the high cardinality and diversity of URLs, a compact semantically meaningful representation of URLs is of high importance. For this purpose, we build and examine three URL embedding models following the idea of word embeddings <ref type="bibr" target="#b16">[17]</ref>. The sequential dependency between user's browsing history has also been considered by using a Recurrent Neural Network (RNN) <ref type="bibr" target="#b7">[8]</ref>. In total, ten different prediction conversion models have been introduced. A number of large scale experiments have been executed on a data collected from a real-world advertising platform in order to reveal and compare the prediction abilities of the proposed prediction schemes. Finally, our empirical analysis validates our claims about the effectiveness of our representation models showing that they achieve to group together URLs of the same category. It means that URLs with the same or similar context are also close on the embedding space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>As the performance of a campaign is directly related on how precisely the CVR/CTR is estimated, it has been the objective of considerable research in the past few years. Typically, the problem of CVR/CTR estimation is formulated as a standard binary classification problem. Logistic regression has been extensively used to accurately identify conversion events <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="bibr" target="#b6">[7]</ref> introduced a Bayesian learning model, called Bayesian probit regression, which quantifies the uncertainty over a model's parameters and hence about the CTR of a given ad-impression. A precise user representation (a set of features describing user behaviour) constitutes also the foundation for building a linear model able to estimate CVR with high accuracy. Nevertheless, in most cases it requires a lot of feature engineering effort and the knowledge of the domain. Moreover, linear models are not capable to reveal the relationship among feature. To overcome this problem, a number of non-linear models such as factorisation machines <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b20">21]</ref> and gradient boosted regression trees <ref type="bibr" target="#b9">[10]</ref> have been also proposed to capture higher order information among features. A number of different deep learning methods have been also proposed recently for CTR prediction <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">25]</ref>.</p><p>Representation learning has been applied with success in several applications and has become a field in itself <ref type="bibr" target="#b0">[1]</ref> in the recent years. The URL representation architectures presented in this manuscript have been inspired by those used in natural language processing (NLP) tasks. Learning high-quality representations of phrases or documents is a long-standing problem in a wide range of NLP tasks. Word2Vec <ref type="bibr" target="#b16">[17]</ref> is one of the most well-known word embeddings algorithms. The main idea behind Word2Vec is that words that appear in similar contexts should be close in the learned embedding space. For this purpose, a (shallow) neural network language model is applied that consists of an input, a projection, and an output layer. Its simple architecture makes the training extremely efficient. <ref type="bibr" target="#b8">[9]</ref> proposed search2vec model that learns user search action representations based on contextual co-occurrence in user search sessions. To the best of our knowledge, URLNet <ref type="bibr" target="#b11">[12]</ref> is the only work that learns a URL representation but for the task of malicious URL detection. In contrast to our unsupervised representation scheme that considers the sequential order of URLs, URLNet is an end-to-end (supervised) deep learning framework where its character-level and word-level CNNs are jointly optimized to learn the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED CONVERSION PREDICTION ARCHITECTURE</head><p>The goal of this paper is to predict the probability a user to be converted one day after, given their browsing history on a single day. More specifically, we consider each user as an ordered sequence of URLs, sorted chronologically. The notion of conversion corresponds to an action of interest for the advertiser, such as visit on the landing page, purchase of a product, registration, etc. Therefore, we can treat the problem of predicting the user conversion as a binary classification problem <ref type="bibr" target="#b1">[2]</ref>, where given a sequence of URLs visited by a user</p><formula xml:id="formula_0">U n = {url n 1 , . . . , url n T n }, n = 1, 2, .</formula><p>. . , N , we want to predict if U n will be converted or not, y n âˆˆ {0, 1}. The length of the URL sequence, T n , may be different for each user.</p><p>As an analogy to text classification, we view a sequence of URLs as a document, or a sequence of sentences. In our case, a URL is itself a sequence of tokens, of length at most three (we ignore the rest tokens as they are quite noisy). Each URL 1 is split with a '/' (slash) character, where the first token corresponds to the domain name. For instance, https://en.wikipedia.org/wiki/Main_Page is mapped to [en.wikipedia.org, wiki, Main_Page].</p><p>In order to apply any supervised classification model, such as logistic regression, etc., a semantically meaningful representation of each URL is needed. Therefore, a key intermediate step in our model is the learning of a URL representation. More precisely, the proposed conversion prediction scheme is composed of two consecutive training phases. The first one corresponds to the learning of the URL representations, while the second one corresponds to the training of a classifier. It should be mentioned that the training processes of these two models are independent.</p><p>Due to the high cardinality of URLs, we learn the URL representations implicitly by learning and aggregating their tokens representations. In this study, we present and examine four different URL representation models, f r : url â†’ x, where x âˆˆ R d and d is the dimensionality of the embedding space. The first one is the simple one-hot encoding that treats tokens as atomic units. The main limitation of this representation is that the representation size grows linearly with the corpus and it doesn't consider the similarity between URLs. To overcome these issues, we propose three different URL embedding models (see Section 4).</p><p>After having trained a representation model, f r , and given a training set</p><formula xml:id="formula_1">D = {(U n , y n )} N n=1 , we produce a new dataset D â€² = {(X n , y n )} N n=1 where X n = {x n 1 , . . . , x n T n</formula><p>} is a sequence of lengthT n</p><p>1 The http(s):// and www parts of each URL are stripped.</p><p>with elements x n i = f r (url n i ). In a nutshell, D â€² contains sequences of URL embedding vectors along with their labels. Then, we apply mapping f m : X â†’ z that aggregates the URLs embeddings into an embedding vector z âˆˆ R m , where m can be different from d. It results in a single compact representation z for each sequence of URLs. Next, our goal is to discover a classification model f c : z â†’ Å· from a set F of possible models with the minimum empirical risk min</p><formula xml:id="formula_2">f c âˆˆ F E (X,y)âˆ¼D â€² [â„“(f c (f m (X )), y)],<label>(1)</label></formula><p>where â„“ âˆˆ R is a non-negative loss function. In fact, classifier f c is trained on dataset</p><formula xml:id="formula_3">D â€²â€² = {(z n , y n )} N n=1 .</formula><p>In this work, we use logistic regression where the conversion conditional probability of user (n) given their browsing history X n , is modeled as:</p><formula xml:id="formula_4">p(y n = 1|X n ) = Ïƒ (Î¸ âŠ¤ f m (X n ) + b),<label>(2)</label></formula><p>where Î¸ is a vector with the unknown model parameters, b is the bias term and Ïƒ (â€¢) is the logistic sigmoid function. From a geometrical point of view, Î¸ âŠ¤ f m (X n ) + b is a hyperplane that separates the two classes. To learn the unknown model parameters, the crossentropy loss is applied:</p><formula xml:id="formula_5">L = âˆ’ E (X,y)âˆ¼D â€² [y log f c (f m (X ))+(1âˆ’y) log(1âˆ’ f c (f m (X )))].<label>(3)</label></formula><p>Next, we are describing the three different mapping functions f m adopted in our work. The first one returns the average of the URLs embedding vectors presented on a sequence:</p><formula xml:id="formula_6">f (1) m (X ) = 1 T T i=1 x i .</formula><p>The second one considers the dependencies among the features of the embedding vector returned by the first mapping function. To be more precise, it returns the output of a dense layer with rectified linear units (ReLU), that takes as input the average of the URLs embedding vectors, f</p><formula xml:id="formula_7">m (X ) = Ð´(Î¸ (1)âŠ¤ f (1)<label>(2)</label></formula><p>m (X ) + b (1) ). The ReLU uses the activation function Ð´(z) = max{0, z}. The main limitation of applying one of the two aforementioned mappings is that they do not take into account the chronological order in which the URLs appeared on the sequence. To overcome this limitation we resort to the well-known Long Short Term Memory network (LSTM) <ref type="bibr" target="#b10">[11]</ref> that is a special kind of RNNs <ref type="bibr" target="#b19">[20]</ref> and is suitable to process variablelength sequences. To be more precise, the third mapping function f <ref type="bibr" target="#b2">(3)</ref> m is an LSTM network able to map an input sequence of URL embeddings X to a fixed-sized vector z, that can be considered as the representation of the whole sequence. In all cases, we are feeding the produced vector z to a final dense layer with sigmoid activation function. In the rest of the paper, we denote as LR, DLR and RNN the prediction conversion models which are using the "average", "dense" and "LSTM" mapping functions, respectively. A graphical illustration of the proposed conversion prediction model architecture is presented at Fig. <ref type="figure" target="#fig_8">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">URL REPRESENTATION SCHEMES</head><p>This section introduces the four URL representation models proposed in our work. These models can be divided in two categories: i) one-hot encoding, and ii) embedding representation. There is no need for learning in the case of one-hot encoding. On the other hand, the embedding representations of URL tokens are learned in advance and then used to form the final URL representation. A representation is also learned for the so-called "rare" and "none" tokens, respectively. A token is considered "rare" if it is present less than a predefined number (we set it equal to 20) of times in our data. We denote the token embedding vectors as e.</p><p>One-hot encoding. First, we introduce a variant of the standard one-hot encoding for representing URLs that is our baseline. As already mentioned, the token representations are used to get the URL representation. Therefore, in our case, the cardinality of the one-hot encoding is equal to the number of all possible tokens appearing in our data. Given a one-hot encoding {e t } n_tokens â‰¤3 t =1 , for each one of the tokens appearing in url, we take their average to encode it: f r (url) =</p><formula xml:id="formula_8">1 n_tokens n_tokens t =1 e t .</formula><p>Embedding learning. Despite its simplicity, the aforementioned one-hot encoding does not take into account the similarity between URLs, while on the same time the representation size grows linearly with the corpus. In fact, one-hot encoding is sparse (curse of dimensionality) and it is not able to capture the distance between individual urls. To tackle this problem, we propose three representation schemes inspired by the idea of Word2Vec <ref type="bibr" target="#b16">[17]</ref>.More specifically, our representation schemes use the skip-gram model that given a target word (URL in our case) tries to predict its context words. More formally, the skip-gram model tries to find word representations that can be used for predicting the words in its neighborhood. Therefore, given a sequence of words (URLs) url 1 , . . . , url T , our objective is the maximisation of the average log probability 1</p><formula xml:id="formula_9">T T t =1 âˆ’c â‰¤j â‰¤c, j 0 log p(url t +j |url t ),<label>(4)</label></formula><p>where c specifies the neighborhood of target URL. The conditional probability is defined by using the softmax function, as p(url</p><formula xml:id="formula_10">c |url t ) â‰œ exp(x âŠ¤ c x t ) c â€² âˆˆC exp(x âŠ¤ c â€² x t )</formula><p>, where x c and x t are the representations for context and target URLs respectively, and C is the set of unique words.</p><p>As the direct optimisation of Eq. 4 is computationally expensive, we are adopting the negative sampling approach <ref type="bibr" target="#b16">[17]</ref>. In negative sampling, we treat the word's representation learning as a binary classification task where we try to distinguish the target-context pairs of words presented on the training data from those that are not. Following the suggestions of <ref type="bibr" target="#b16">[17]</ref>, for each positive pair we are creating k negative (target-context) pairs.</p><p>In contrast to the original Word2Vec model, the proposed representation learning architectures try to learn the tokens representations, instead of the URL representations directly. A token representation is also learned for the case of a Pad token. For instance, https://en.wikipedia.org/ is mapped to [en.wikipedia.org, Pad, Pad]. Since URLs are padded, the number of tokens of each URL is equal to three. Then, by combining the token representations we form the final URL representation that will be used for the training of the conversion prediction classifier. Actually, the second phase of our model (conversion classifier) can be seen as a way to test the effectiveness of our representation models. The main difference between the three proposed embedding representation models is how the token representations are combined to form the final URL embedding vector:</p><p>â€¢ Domain_only representation uses only the representation of the first token to represent the URL, ignoring the representations of the other two tokens. â€¢ Token_avg representation takes the average of the token embedding vectors to represent the URL. â€¢ Token_concat representation concatenates the token embedding vectors to represent the URL. In this case, the dimension of the URL embedding vector is three times the dimension of the token embedding vectors. For instance, let {e t } 3 t =1 be the token embedding vectors of the three tokens presented on url = [token 1 , token 2 , token 3 ]. Then, the Domain_only representation x is equal to e 1 , the Token_avg representation is equal to 1 1 , e âŠ¤ 2 , e âŠ¤ 3 ] âŠ¤ . A graphical illustration of the proposed embedding learning architecture is provided at appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>This section presents the results of our empirical analysis. A realworld RTB dataset was used to train and analyse the performance of the ten proposed prediction models. We built our dataset by using the auction system logs from campaigns launched in France. It should be also mentioned that our dataset is anonymised, and only visited URLs are considered. In this way, each record of the dataset corresponds to a chronologically ordered sequence of visited URLs along with a binary label (specific to the advertiser) that indicates whether or not a conversion has happened on the advertiser's website on the next day. More precisely, the data composed of sequences of URLs along with their labels of three successive dates,  performance of the models on five advertisers, belonging to four different categories: banking, e-shop, newspaper, and telecommunications (see Table <ref type="table" target="#tab_1">1</ref>). Details about the statistics of the data are provided in the appendix [url_link] (https://bit.ly/3cSNFXU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>All our predictors assume the existence of a URL representation model in order to vectorise the sequence of URLs for each one of the dataset records. Our baseline, One_hot/LR, represents the URL sequences using a one-hot encoding vector of size 193, 409, where the first two entries correspond to the "unknown" and "rare" tokens, respectively. The other models rely on an already trained embedding matrix. Each token is embedding into a 100-dimensional vector. The first three rows correspond to the "unknown", "rare", and Pad tokens, respectively. The rest rows contain the embedding vector of all non-rare tokens observed in the dataset D d . The number of non-rare tokens is 22, 098 for the Domain_only, and 187, 916 for both the Token_Average and Token_Concatenation. To train representations, we have considered two different {pos:neg} ratios ({1:1} and {1:4}). Due to page limitations, only the results of the {1:4} ratio are presented in this manuscript. See supplementary material <ref type="bibr">[url_link]</ref> for a full comparison between these two ratios.</p><p>The number of units of the hidden dense layer (dimensionality of its output space) of DLR model is set to 30. Furthermore, the number of hidden units of LSTM is set to 10 on the RNN model. A dense layer with a sigmoid activation function is applied at the end of each one of the three mapping functions f m ("average", "dense", "LSTM") in order to form a binary classifier. Through our empirical analysis we have observed that the DLR and RNN prediction models are prone to overfitting. To enhance the generalization capabilities of these two models, we are using dropout that is set to 0.5. To be more precise, a dropout layer is added right after the f m layer of the DLR model, while both the dropout and the recurrent_dropout parameter of LSTM layer are set equal to 0.5 on the RNN model.</p><p>For the training of all representation and prediction models, the mini-batch stochastic optimization has been applied by using Adam optimizer with the default Tensorflow 2.0 settings (i.e., lr=0.001, beta 1 = 0.9, beta 2 = 0.999). More precisely, to train the representation models we are doing one full pass over the whole data that is divided to 200 parquet files. The total number of epochs is 200, equal to the number of parquet files. At each epoch we are producing the positive and negative pairs based on the data contained on a single parquet file and are feeding them to the representation model. On the other hand, the size of batches for training prediction models is 64, while the number of epochs and number of steps per epoch is set to 100 in both cases. To tackle the problem of our unbalanced dataset (see Table <ref type="table" target="#tab_1">1</ref>), the ratio of positive and negative records is {1:1} in the batches used for the training of the classifiers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>In this section, we formally present the results of our empirical analysis. Firstly, to get an intuition about the ability of the three introduced URL embedding schemes (Sec. 4) to group together URLs that belong to the same category (i.e., sports, news, etc.), we will visualise (Fig. <ref type="figure" target="#fig_16">12</ref>) the embedding vectors of 24 selected domains along with those of the thirty closest URLs of each one of them. To be more precise, we are using the embedding matrix learned by the Domain_only model. Cosine similarity has been used to measure the similarity between the embedding vectors of two URLs. To project the original high-dimensional embedding vectors on a 2dimensional space, we apply the Barnes-Hut t-SNE algorithm <ref type="bibr" target="#b21">[22]</ref>.</p><p>To be guaranteed that the URLs belong to the same category with that of their closest domain, Table <ref type="table" target="#tab_2">2</ref> provides the 10-nearest URLs for each one of the 24 domains (see appendix for the full list of the 30-nearest neighbors). For instance, all URLs that are on the neighborhood of expedia.fr are about travelling. Moreover, all the neighbors of sport.fr <ref type="bibr" target="#b15">(16)</ref> are URLs about sports. The visualization of Fig. <ref type="figure" target="#fig_16">12</ref> illustrates the ability of our model to produce semantically Next, we formally present the numerical results of the ten prediction models on five different advertisers. The first part of the name of each model specifies the type of URL representation used, while the second one indicates the classifier type. To evaluate and compare the effectiveness of the models we are using the area under ROC curve (AUC) metric. More specifically, we consider the average (%) AUC across five independent runs (see Table <ref type="table" target="#tab_4">4</ref>), where each run corresponds to a specific seed used for the models initialisation. Moreover, Figure <ref type="figure" target="#fig_4">4</ref> illustrates the average ROC curves of the prediction models for each advertiser. The right plot of Fig. <ref type="figure" target="#fig_4">4</ref> illustrates the AUC of the models on the 25 independent runs (5 advertisers Ã— 5 runs for each advertiser)).</p><p>Based on the results presented in Table <ref type="table" target="#tab_4">4</ref>, the Token_avg/RNN model is more effective in predicting user's conversions compared to the rest models. Precisely, the Token_avg/RNN model has the highest average AUC in all advertisers. All models achieve their highest performance on the newspaper advertisers. It is also worth noticing that the performances of Domain_only/LR, Token_avg/LR, and Token_concat/LR are highly competitive, compared to our baseline, One_hot/LR. More specifically, Token_concat/LR clearly outperforms One_hot/LR in 2 out of 5 advertisers (E-shop, Newspa-per_1), and has slightly better performance in one of them (Newspa-per_2). That remark validates our claims that the proposed representation models produce meaningful embeddings, by distinguishing URLs of the same category and placing them close to the embedding space. Taking a closer look at the standard deviations, we can see that the performance of One_hot/LR is quite stable performance in the three advertisers. This was expected as the One_hot representation is identical over all runs, and therefore the only variation of the performance of One_hot/LR comes only from the training of the LR classifier. The performance of LR model is almost the same for each representation model. The same also holds for DLR and RNN model where its performance is more or less the same in the cases of Domain_only and Token_concat, and slightly better in the case of Token_avg. On the other hand, DLR performs significantly better when it is combined with Token_avg and Token_concat, with Token_avg to be more preferable (around 1% gain). Let us now compare the impact of the type of classifier on the performance of the prediction model. The overall comparison presented at Fig. <ref type="figure" target="#fig_4">4</ref> demonstrates that both DLR and RNN performs significantly better compared to LR, with the RNN to be the best one. More precisely, the AUC of RNN is around âˆ¼7% and âˆ¼3% higher compared to those of LR and DLR, respectively. This means that the consideration of the chronological order in which the URLs appeared on the sequence is of high importance. On the other hand, choosing DLR over LR improves around âˆ¼4% the performance of the prediction models independent to the representation model.</p><p>To sum up, the main conclusions of our empirical analysis are: i) all three proposed URL embedding models are able to learn highquality vector representations that precisely capture the URL relationships, ii) the performance of the LR model is relatively invariant to the selection of the representation model, iii) among the three representation models, Token_avg is more adequate to capture the relationship between URLs, with the Token_concat second best, iv) the consideration of the chronological order of the visited URLs (RNN) and the learning of dependencies among the embedding features (DLR) are also of high importance as both improve significantly the performance of the prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSIONS AND FUTURE DIRECTIONS</head><p>In this paper, we considered the problem of user response prediction in display advertising. Ten conversion prediction models were proposed to predict user response based on their browsing history. To represent the sequence of visited URLs, four different compact URL representations were examined. The effectiveness of the proposed models has been experimentally demonstrated offline in a real-world RTB dataset for five different advertisers. The impact of the sequential dependency between users' visited URLs on the performance of the predictors has been also examined. The main conclusions of our empirical analysis were that all three proposed representation models produce a meaningful URL representation, and considering the chronological order of the visited URLs by using RNN significantly improves the model's performance. In the future, we intend to propose an online version of our framework and to extend our empirical analysis to a real-world online scenario.</p><p>[25] G. Zhou, X. Zhu, C. Song, Y. Fan, H. Zhu, X. Ma, Y. Yan, J. Jin, H. Li, and K. Gai.</p><p>2018. Deep Interest Network for Click-Through Rate Prediction. In KDD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>The purpose of this Supplementary Material is to provide more details about: i) the proposed representation and prediction conversion model architectures, ii) the statistics of data, and iii) present additional experimental results. First in Section A, we illustrate graphically and explain the architecture of the prediction model. Next, Section B presents some statistics for the three data used for the representation training and for the training and testing of the representation and prediction models. Finally, Section C reports some additional experiments where we examine the impact of the {pos:neg} ratio (used for representation training) on the learning of meaningful URL representations and on the performance of the final prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PREDICTION CONVERSION MODEL ARCHITECTURE</head><p>A graphical illustration of the proposed prediction conversion model architecture is presented in this section. The objective of the proposed model is to predict with high accuracy if a user will be converted or not given his browsing history. Actually, the proposed conversion prediction scheme is composed of two consecutive training phases. The first one corresponds to the learning of the URL representations, while the second one corresponds to the training of a classifier. It should be mentioned that the training processes of these two models are independent. A high-level overview of the pipelines of these two training phases is illustrated at Fig. <ref type="figure" target="#fig_5">5</ref>. Figure <ref type="figure" target="#fig_8">6</ref> shows the three basic 'blocks' of our prediction model. First, the URL representation layer, f r : url â†’ x, maps a single url into a vector on the embedding space. For this purpose, a URL representation model is trained in advance to learn meaningful url representations. Second, the mapping f m : X â†’ z aggregates the URLs embeddings and produces a single compact representation z for the sequence of the URLs. Three different mapping functions f m examined in our work: the first one (f m ) uses an LSTM network <ref type="bibr">[11?</ref> ]. Finally, standard logistic regression (dense layer with a sigmoid activation function) is applied to estimate the user conversion probability.</p><p>Figure <ref type="figure" target="#fig_10">7</ref> illustrates a variant of the negative sampling skip-gram model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> used for learning url semantic meaningful representations. In general, skip-gram produces embedding vectors such that the embedding vector of url t is close to the embedding vectors of the URLs of its neighborhood. Therefore, the urlÃ¢Ä‚Å¹s representation learning is treated as a binary classification task where we try to distinguish the pairs of URLs presented close on the sequences. The main difference between our architecture with the standard one is the Target URL embedding layer (and the Context URL embedding layer). This layer combines the token representations and forms the final URL representation. Roughly speaking, the proposed representation model learns an embedding matrix where each row corresponds to a token appearing in the URLs of our data. To form the URL representation, we are combining the embedding vectors of each token appearing in the URL. In this work, we examine three different ways for combining token representations to form the final URL embedding vector (for more details, check Sec. 4 of the paper). To be more precise, the distribution of the URL sequence lengths is given. As it can be easily observed, the length of the URL sequences for most of the records is less than 500. Due to this fact, for the training and testing of our prediction models we are using the 500 most recent visited URLs of each sequence. On the other hand, for the training of the representation the whole sequences are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET STATISTICS</head><p>In our setting, a URL is split with a Ã¢Ä‚Å¸/Ã¢Ä‚Å¹ (slash) character and it is supposed to consist of at most three tokens. Figure <ref type="figure">9</ref> shows the token frequency distribution of the first three URL tokens for each dataset. As it was expected, the tokens at the last two spots are more rare in the data. Also, the number of unique tokens in the first place (domain token) is significantly smaller than the number of unique tokens on the other two spots. In our analysis, we consider a token "rare" if it appears less than 20 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL RESULTS</head><p>Next, we report the results of our complete empirical analysis, where we examine the impact of the selection of the {pos:neg} ratio (representation training) on the performance of the prediction model. More specifically, we present the performance of nineteen prediction models on the five advertisers described in the manuscript. In order to distinguish the nineteen different models, their names consist of three parts separated by a slash ('/') character apart from the One_hot/LR model. The first part indicates the type of representation (Domain_only, Token_avg, Token_concat), the second defines the kind of prediction model (LR, DLR, RNN), and the last one shows the {pos:neg} ratio ({1:1}, {1:4}) used for the training of the representation model.</p><p>As already highlighted at the paper, the area under ROC curve (AUC) is the metric that used to evaluate the prediction capabilities of each model. Table <ref type="table" target="#tab_4">4</ref> presents the average AUC of each model across five independent runs (five randomly selected seeds). Moreover, Figures 10 and 11 illustrate the average ROC curves of the ten prediction models for each advertiser, where {1:1} and {1:4} {pos:neg} ratios used for training representation models, respectively. Next, we present the main conclusions of our "full" empirical analysis:</p><p>â€¢ All three proposed URL embedding models are able to learn high-quality vector representations that capture precisely the URL relationships. Even the Domain_only/LR, Token_avg/LR, and Token_concat/LR models are able to predict with high accuracy the probability a user to be converted, and their performance is quite close or better compared to that of our baseline One_hot/LR. â€¢ It can be easily verified by the reported results that the prediction model performance is not so sensitive on the {pos:neg} ratio used for training representation models. â€¢ {1:1} performs better on the Domain_only representation compared to {1:4} that performs better on Token_avg. On the other hand, the performance of both {pos:neg} ratios are almost equivalent for the Token_concat model. â€¢ Among the three representation models, Token_avg seems to be more adequate to capture the relationships between URLs, with the Token_concat to be the second best. Moreover, the performance of Domain_only representation is quite close to that of Token_concat. â€¢ The consideration of the chronological order of the visited URLs (RNN) and the learning of dependencies among the embedding features (DLR) are also of high importance as both improve significantly the performance of the conversion prediction model. To be more precise, it is clear (see bottom right plot of Fig. <ref type="figure" target="#fig_13">10</ref> and Fig. <ref type="figure" target="#fig_14">11</ref>) that the RNN model surpass the performance of the rest two classifiers, while DLR performs better compared to LR. â€¢ The best prediction conversion models are Domain_only/RNN and Token_avg/RNN for {1:1} and {1:4} {pos:neg} ratios, respectively (see bottom right plot of Figs. 10 and Fig. <ref type="figure" target="#fig_14">11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Representation Visualisation</head><p>In this section we visualise the embedding vectors of 24 selected domains along with those of the thirty closest URLs of each one of them, which have been learned by Domain/1:1 (Fig. <ref type="figure" target="#fig_16">12a</ref>) and Domain/1:4 (Fig. <ref type="figure" target="#fig_16">12b</ref>) representation models. Looking carefully at Fig. <ref type="figure" target="#fig_16">12</ref>, we observe that clusters close on the embedding space produced by Domain/1:1 are also close on the embedding space</p><formula xml:id="formula_11">Target URL urlt = [token (t ) 1 , token (t ) 2 , token (t ) 3 ]</formula><p>Token Embedding Layer (dic_size Ã— embedding_dim)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Token Embedding</head><p>Target token1 embedding e            </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A high-level overview of RTB procedure.</figDesc><graphic url="image-1.png" coords="1,335.33,176.06,203.25,103.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3 3t</head><label>3</label><figDesc>=1 e t , and the Token_concat representation is given as [e âŠ¤</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>D d , D d +1 , and D d +2 , where D d is used for learning representations, and D d +1 and D d +2 for training and testing the prediction models, respectively. Moreover, the maximum length of a URL sequence is set equal to 500, where only the most recently visited URLs are kept in each sequence, T n â‰¤ 500, âˆ€n âˆˆ [1, N ]. In total we examine the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: t-SNE visualization of the thirty closest neighbors of 24 different domains.</figDesc><graphic url="image-2.png" coords="5,53.80,83.68,240.24,117.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average ROC curves of the ten conversion prediction models on the five advertisers. Shaded regions represent the std over 5 independent runs. The bottom right plot presents the AUC for each one of the 25 runs (5 advertisers Ã— 5 independent runs for each advertiser) of each model. The â€¢, â–¼ and Ã— marks indicate the LR, DLR and RNN classification models, respectively.</figDesc><graphic url="image-3.png" coords="6,53.80,84.87,82.21,84.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: URL representation and conversion classifier learning pipeline. The binary labels are not needed for training the URL representation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( 1 )</head><label>1</label><figDesc>m ) just returns the average of the URLs embedding vector, the second one (f(2)m ) considers the dependencies among the features of the embedding vector returned by the first mapping function, and the third one (f<ref type="bibr" target="#b2">(3)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8</head><label>8</label><figDesc>Figure 8  presents an analysis of the URL sequence lengths for D d , D d +1 , and D d +2 . To be more precise, the distribution of the URL sequence lengths is given. As it can be easily observed, the length of the URL sequences for most of the records is less than 500. Due to this fact, for the training and testing of our prediction models we are using the 500 most recent visited URLs of each sequence. On the other hand, for the training of the representation the whole sequences are used.In our setting, a URL is split with a Ã¢Ä‚Å¸/Ã¢Ä‚Å¹ (slash) character and it is supposed to consist of at most three tokens. Figure9shows the token frequency distribution of the first three URL tokens for each dataset. As it was expected, the tokens at the last two spots are more rare in the data. Also, the number of unique tokens in the first place (domain token) is significantly smaller than the number of unique tokens on the other two spots. In our analysis, we consider a token "rare" if it appears less than 20 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>zFigure 6 :</head><label>6</label><figDesc>Figure6: Proposed conversion prediction model architecture. It consists of three main parts: i) URL embedding layer (f r ), ii) URL sequence embedding layer (f m ), and iii) Logistic regression classifier (f c ). Only the unknown classifier parameters, Î¸ and b, (see Eq. 2 of the paper) of the dense layer and these of LSTM and "dense" mappings are trainable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The Skip-gram model architecture used for learning token embeddings. Only the (unknown) parameters of the red blocks are trainable. The dimensionality of the embedding matrices is equal to the number of tokens Ã— the preferable size of the embedding space.</figDesc><graphic url="image-10.png" coords="9,53.94,438.59,174.02,174.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Dataset D d (b) Dataset D d +1 (c) Dataset D d +2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Analysis of the URL sequences lengths for the data, D d , D d +1 , and D d +2 .</figDesc><graphic url="image-11.png" coords="9,218.99,438.59,174.02,174.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Average ROC curves of the ten conversion prediction ({1:1} pos-neg ratio) models on the five advertisers. Shaded regions represent the standard deviations over 5 independent runs. The bottom right plot presents the AUC for each one of the 25 independent runs (5 advertisers Ã— 5 independent runs for each advertiser) of each model. The â€¢, â–¼ and Ã— marks indicate the LR, DLR and RNN classification models, respectively.</figDesc><graphic url="image-25.png" coords="14,54.08,259.02,166.46,171.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Average ROC curves of the ten conversion prediction ({1:4} pos-neg ratio) models on the five advertisers. Shaded regions represent the standard deviations over 5 independent runs. The bottom right plot presents the AUC for each one of the 25 independent runs (5 advertisers Ã— 5 independent runs for each advertiser) of each model. The â€¢, â–¼ and Ã— marks indicate the LR, DLR and RNN classification models, respectively.</figDesc><graphic url="image-32.png" coords="15,61.64,253.73,161.42,166.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>(a) Domain_only/ 1 : 1</head><label>11</label><figDesc>URL representation model (b) Domain_only/1:4 URL representation model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: t-SNE visualization of the thirty closest neighbors of 24 different domains. The colors of the points indicate the closest domain of each URL.</figDesc><graphic url="image-37.png" coords="16,53.80,347.25,504.39,246.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: t-SNE visualization of the embedding matrix trained by Domain_only/1:4 representation model after 0, 50, 100, 150, and 200 epochs, respectively.</figDesc><graphic url="image-38.png" coords="17,58.15,86.92,97.34,93.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of converted vs. non-converted records for each one of the 5 advertisers on the training and testing data.</figDesc><table><row><cell cols="3">Advertiser Category Training (5, 452, 577) Testing (7, 164, 109)</cell></row><row><cell>Banking</cell><cell>(3, 746 âˆ’ 5, 448, 831)</cell><cell>(8, 539 âˆ’ 7, 155, 570)</cell></row><row><cell>E-shop</cell><cell>(1, 463 âˆ’ 5, 451, 114)</cell><cell>(1, 821 âˆ’ 7, 162, 288)</cell></row><row><cell>Newspaper_1</cell><cell>(1, 406 âˆ’ 5, 451, 171)</cell><cell>(2, 923 âˆ’ 7, 161, 186)</cell></row><row><cell>Newspaper_2</cell><cell>(1, 261 âˆ’ 5, 451, 316)</cell><cell>(1, 291 âˆ’ 7, 162, 818)</cell></row><row><cell>Telecom</cell><cell>(1, 781 âˆ’ 5, 450, 796)</cell><cell>(2, 201 âˆ’ 7, 161, 908)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The 10-nearest neighbors of 24 different domains according to our trained Domain_only representation model.</figDesc><table><row><cell>be;</cell></row></table><note>Domain 10-nearest neighbors huffingtonpost.es cope.es; m.eldiario.es; okdiario.com; verne.elpais.com; blogs.elconfidencial.com; vozpopuli.com; elespanol.com; smoda.elpais.com; libertaddigital.com; cadenaser.com lesechos.fr latribune.fr; afrique.latribune.fr; business.lesechos.fr; bfmbusiness.bfmtv.com; financedemarche.fr; challenges.fr; investopedia.com; actufinance.fr; lopinion.fr; contrepoints.org orange.fr actu.orange.fr; lemoteur.orange.fr; messagerie.orange.fr; login.orange.fr; finance.orange.fr; sports.orange.fr; meteo.orange.fr; tendances.orange.fr; programme-tv.orange.fr; news.orange.fr leparisien.fr cnews.fr; atlasinfo.fr; lefigaro.fr; lejdd.fr; jforum.fr; marianne.net; video.lefigaro.fr; tendanceouest.com; bladi.net; observalgerie.com reddit.com imgur.com; old.reddit.com; askreddit.reddit.com; pcgamer.com; anime.reddit.com; france.reddit.com; gamefaqs.gamespot.com; totalwar.reddit.com; nintendoswitch.reddit.com; gaming.reddit.com expedia.fr momondo.fr; skyscanner.fr; kayak.fr; fr.lastminute.com; fr.hotels.com; flights-results.liligo.fr; ebookers.fr; esky.fr; opodo.com; secure.lastminute.com tractorfan.fr discountfarmer.com; forum.farm-connexion.com; angleterre.meteosun.com; songs-tube.net; materieltp.fr; assovttroc.clicforum.fr; opel-mokka.forumpro.fr; spa-du-dauphine.fr; vanvesactualite.blog4ever.com; calcul-frais-de-notaire.fr welt.de zeit.de; sueddeutsche.de; faz.net; tagesspiegel.de; sport1.de; kicker.de; saarbruecker-zeitung.de; tz.de; bild.de; sportbild.bild.de foreca.fr my-meteo.com; fr.meteovista.be; fr.tutiempo.net; meteopassion.com; de.sat24.com; nosvolieres.com; meteo-sud-aveyron.overblog.com; xn-mto-bmab.fr; palombe.com; calculerdistance.fr auto-moto.com caradisiac.com; largus.fr; news.autojournal.fr; test-auto.auto-moto.com; auto-mag.info; feline.cc; motorlegend.com; essais.autojournal.fr; automobile-magazine.fr; turbo.fr az-online.de abountifulkitchen.com; thesurvivalgardener.com; leinetal24.de; brittanyherself.com; symbols.com; ourpaleolife.com; msl24.de; milliondollarjourney.com; arthritis-health.com; thehollywoodunlocked.com tempsdecuisson.net cuisine-facile.com; temps-de-cuisson.info; yummix.fr; aux-fourneaux.fr; cuisinenligne.com; audreycuisine.fr; mamina.fr; uneplumedanslacuisine.com; cnz.to; ricardocuisine.com cnn.com us.cnn.com; stadiumtalk.com; thedailybeast.com; itpro.co.uk; uk.reuters.com; euronews.com; theargus.co.uk; theatlantic.com; thedailymash.co.uk; trendscatchers.co.uk portailcloture.ooreka.fr bricolage-facile.net; mur.ooreka.fr; pierreetsol.com; bricolage.jg-laurent.com; abri-de-jardin.ooreka.fr; aac-mo.com; fr.rec.bricolage.narkive.com; decoration.ooreka.fr; piscineinfoservice.com; bricoleurpro.com sport.fr infomercato.fr; parisfans.fr; topmercato.com; vipsg.fr; footradio.com; mercatofootanglais.com; le10sport.com; buzzsport.fr; footparisien.com; foot-sur7.fr anti-crise.fr cfid.fr; forum.anti-crise.fr; gesti-odr.com; echantillonsclub.com; plusdebonsplans.com; cataloguemate.fr; promoalert.com; argentdubeurre.com; madstef.com; forum.madstef.com auchan.fr but.fr; conforama.fr; vente-unique.com; rueducommerce.fr; fr.shopping.com; cdiscount.com; touslesprix.com; promobutler.fr; gps-carminat.com; megane2.superforum.fr; lesamisdudiag.com; car-actu.com; diagnostic-auto.com; r25-safrane.net; lesamisdelaprog.com; forum.autocadre.com; renault-clio-4.forumpro.fr excel-plus.fr tech-connect.info; thehackernews.com; lecompagnon.info; panoptinet.com; slice42.com; aliasdmc.fr; astuces.jeanviet.info; nalaweb.com; patatos.over-blog.com; jiho.com jeuxvideo.org alsumaria.tv; minecraft-zh.gamepedia.com; infovisual.info; everyonepiano.com; footstream.live; memedroid.com; darkandlight.gamepedia.com; mbti.forumactif.fr; gachagames.net; honga.net farmville2free.com goldenlifegroup.com; fv2freegifts.org; juegossocial.com; fv-zprod-tc-0.farmville.com; fb1.farm2.zynga.com; zy2.farm2.zynga.com; gameskip.com; fv-zprod.farmville.com; megazebra-facebook-trails.mega-zebra.com; farmvilledirt.com vogue.fr vanityfair.fr; vogue.com; vivreparis.fr; fr.metrotime.be; brain-magazine.fr; o.nouvelobs.com; parismatch.be; pariszigzag.fr; admagazine.fr; unilad.co.uk tripadvisor.fr fr.hotels.com; cityzeum.com; voyages.michelin.fr; lonelyplanet.fr; monnuage.fr; voyageforum.com; rome2rio.com; toocamp.com; virail.fr; partir.com</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Avg (%) and std of the area under ROC curves (5 independent runs) of the 10 prediction models on 5 advertisers. Â± 0.093 66.4 Â± 0.053 75.5 Â± 0.379 73.3 Â± 0.400 65.4 Â± 0.085 Domain_only/LR 64.6 Â± 0.300 66.6 Â± 0.217 75.7 Â± 0.507 73.2 Â± 0.345 63.2 Â± 0.168 Domain_only/DLR 69.0 Â± 0.214 69.7 Â± 0.234 76.8 Â± 0.342 75.7 Â± 0.658 66.6 Â± 0.303 Domain_only/RNN 71.4 Â± 0.144 72.6 Â± 0.422 80.3 Â± 0.168 79.4 Â± 0.281 71.2 Â± 0.250 Token_avg/LR 64.5 Â± 0.241 67.2 Â± 0.390 76.4 Â± 0.152 73.1 Â± 0.184 62.9 Â± 0.468 Token_avg/DLR 69.4 Â± 0.294 72.1 Â± 0.263 79.2 Â± 0.242 77.7 Â± 0.274 67.9 Â± 0.348 Token_avg/RNN 71.9 Â± 0.082 73.1 Â± 0.246 81.2 Â± 0.153 80.2 Â± 0.322 71.8 Â± 0.153 Token_concat/LR 64.8 Â± 0.241 67.2 Â± 0.060 76.7 Â± 0.179 73.4 Â± 0.273 63.6 Â± 0.425 Token_concat/DLR 69.1 Â± 0.222 70.8 Â± 0.400 78.2 Â± 0.285 76.7 Â± 0.255 66.9 Â± 0.310 Token_concat/RNN 71.5 Â± 0.224 72.5 Â± 0.460 80.5 Â± 0.192 79.1 Â± 0.130 70.5 Â± 0.278 meaningful embeddings. Actually, it becomes apparent that we are getting 24 clearly distinguished clusters. Moreover, we can see that the clusters of 'similar' domains are also close on the embedding space. For instance, the URLs embeddings of clusters</figDesc><table><row><cell>Method</cell><cell cols="2">Adv Banking</cell><cell>E-shop</cell><cell>Newspaper_1 Newspaper_2 Telecom</cell></row><row><cell cols="2">One_hot/LR</cell><cell>65.7</cell><cell></cell></row></table><note><ref type="bibr" target="#b9">(10)</ref> [auto-moto.com] and<ref type="bibr" target="#b18">(19)</ref> [renault-laguna.com] are close as they are related to the automobile category. The same also holds for the URLs embeddings of clusters (2) [lesechos.fr], (4) [leparisien.fr], and (23) [vogue.fr] that belong to the news category.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average (%) and standard deviation of the area under ROC curves (5 independent runs) of the 10 prediction models on 5 different campaigns.</figDesc><table><row><cell>Method</cell><cell>Advertiser</cell><cell>Banking</cell><cell>E-shop</cell><cell>Newspaper_1 Newspaper_2</cell><cell>Telecom</cell></row></table><note>of Domain/1:4. For instance, clusters<ref type="bibr" target="#b5">(6)</ref> [expedia.fr] and<ref type="bibr" target="#b23">(24)</ref> [tripadvisor.fr] are close in both cases. That is normal, as these two clusters contain URLs about travelling. The same also holds for the URLs embeddings of clusters (2) [lesechos.fr], (4) [leparisien.fr], and<ref type="bibr" target="#b22">(23)</ref> [vogue.fr] that belong to the news category. Moreover, clusters<ref type="bibr" target="#b9">(10)</ref> [auto-moto.com] and<ref type="bibr" target="#b18">(19)</ref> [renault-laguna.com] are also close as both are related to automobile. Another example is that of clusters 16 [anti-crise.fr] and 17 [auchan.fr] that are about promotions. To be sure that the URLs belong to the same category (and are not just random URLs) with that of their closest domain, Tables5 and 6provide the 30-nearest URLs for each one of the 24 domains for the Domain/1:1 and Domain/1:4, respectively. Finally, Fig.13illustrates the t-SNE visualization of the embedding matrix (22101 Ã— 100) trained by Domain_only/1:4 representation model after 0, 50, 100, 150, and 200 epochs, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The 30-nearest neighbors of 24 different domains according to our trained Domain_only/1:1 representation model. .es; elconfidencial.com; smoda.elpais.com; verne.elpais.com; elmon.cat; elcorreo.com; expansion.com; infolibre.es; vozpopuli.com; blogs.elconfidencial.com; eldiario.es; okdiario.com; cope.es; m.eldiario.es; elespanol.com; elperiodico.com; levante-emv.com; diariodesevilla.es; eleconomista.es; elcaso.elnacional.cat; periodistadigital.com; farodevigo.es; guiadelocio.com; libertaddigital.com; 20minutos.es; motor.elpais.com; mitele.es; diariodenavarra.es; lasexta.com; diariovasco.com lesechos.fr business.lesechos.fr; latribune.fr; afrique.latribune.fr; financedemarche.fr; bfmbusiness.bfmtv.com; mieuxvivre-votreargent.fr; photo.capital.fr; capital.fr; challenges.fr; journaldunet.com; lopinion.fr; mtf-b2b-asq.fr; marianne.net; hbrfrance.fr; contrepoints.org; journaldeleconomie.fr; boursier.com; zonebourse.com; investopedia.com; managergo.com; e-marketing.fr; actufinance.fr; argent.boursier.com; btf-b2b-asq.fr; linkedin.com; mtf-finance-asq.fr; nasdaq.com; btf-finance-asq.fr; lecoindesentrepreneurs.fr; 07-rosbtf-laplacemedia-3.fr orange.fr actu.orange.fr; login.orange.fr; finance.orange.fr; tendances.orange.fr; lemoteur.orange.fr; messagerie.orange.fr; programme-tv.orange.fr; sports.orange.fr; news.orange.fr; boutique.orange.fr; auto.orange.fr; zapping-tv.orange.fr; webmail.orange.fr; people.orange.fr; occasion.auto.orange.fr; mescontacts.orange.fr; videostreaming.orange.fr; pro.orange.fr; mail02.orange.fr; agenda.orange.fr; tv.orange.fr; cineday.orange.fr; mail01.orange.fr; belote-coinchee.net; 118712.fr; ww.orange.fr; cliquojeux.com; freecell.fr; mundijeux.fr leparisien.fr btf-actu-asq.fr; lefigaro.fr; marianne.net; scoopnest.com; legorafi.fr; cnews.fr; actu17.fr; bfmtv.com; tendanceouest.com; atlasinfo.fr; opex360.com; jeuneafrique.com; lejdd.fr; video.lefigaro.fr; lalibre.be; rtl.be; liberation.fr; fdesouche.com; causeur.fr; 24matins.fr; amp.lefigaro.fr; courrierinternational.com; bladi.net; tunisienumerique.com; lci.fr; courrierpicard.fr; 7sur7.be; air-defense.net; mixcloud.com; pss-archi.eu reddit.com pcgaming.reddit.com; smashbros.reddit.com; old.reddit.com; funny.reddit.com; gaming.reddit.com; europe.reddit.com; france.reddit.com; soccer.reddit.com; askreddit.reddit.com; anime.reddit.com; memes.reddit.com; globaloffensive.reddit.com; starcitizen.reddit.com; freefolk.reddit.com; nintendoswitch.reddit.com; popular.reddit.com; games.reddit.com; aww.reddit.com; leagueoflegends.reddit.com; gameofthrones.reddit.com; politics.reddit.com; redditad.com; dota2.reddit.com; all.reddit.com; totalwar.reddit.com; knowyourmeme.com; pcmasterrace.reddit.com; movies.reddit.com; tinder.reddit.com; nba.reddit.com expedia.fr skyscanner.fr; kayak.fr; momondo.fr; fr.hotels.com; fr.lastminute.com; vol.lastminute.com; liligo.fr; quandpartir.com; esky.fr; govoyage.fr; opodo.fr; lonelyplanet.fr; virail.fr; carnetdescapades.com; voyage.lastminute.com; lastminute.com; bravofly.fr; edreams.fr; easyvols.fr; tripadvisor.fr; locations.lastminute.com; ebookers.fr; voyages.bravofly.fr; opodo.com; reservation.lastminute.com; trainhotel.lastminute.com; flights-results.liligo.fr; voyageforum.com; voyagespirates.fr; govoyages.com .fr; enviedechasser.fr; mash70-75.com; unimog-mania.com; pecheapied.net; renault5.forumactif.com; srxteam.forums-actifs.net; palombe.com; spa-du-dauphine.fr; foreca.fr; meteopassion.com; fiatagri.superforum.fr; meteosurfcanarias.com; super-tenere.org; actu-automobile.com; opel-mokka.forumpro.fr; motoconseils.com; fr.agrister.com; esoxiste.com; v-strom.superforum.fr; view.robothumb.com; sudoku-evolution.com; refugebeauregard.forumactif.com; m.meteorama.fr welt.de zeit..com tempsdecuisson.net cuisinenligne.com; mamina.fr; scally.typepad.com; audreycuisine.fr; atelierdeschefs.fr; temps-de-cuisson.info; aux-fourneaux.fr; uneplumedanslacuisine.com; cuisine-facile.com; .fr; bricolage-facile.net; forum-maconnerie.com; decoration.ooreka.fr; porte.ooreka.fr; abri-de-jardin.ooreka.fr; fr.rec.bricolage.narkive.com; expert-peinture.fr; muramur.ca; amenagementdujardin.net; tondeuse.ooreka.fr; desinsectisation.ooreka.fr; aac-mo.com; fenetre.ooreka.fr; bricoleurpro.com; recuperation-eau-pluie.ooreka.fr; pergola.ooreka.fr; volet.ooreka.fr; papier-peint.ooreka.fr; arrosoirs-secateurs.com; carrelage.ooreka.fr; assainissement.ooreka.fr; pierreetsol.com; poele-cheminee.ooreka.fr; forumbrico.fr; poimobile.fr; parquet.ooreka.fr; forum-plomberie.com; deconome.com; serrure.ooreka.fr sport.fr infomercato.tolino.fr; promo-conso.net; fr.testclub.com auchan.fr conforama.fr; but.fr; rueducommerce.fr; vente-unique.com; cdiscount.com; touslesprix.com; webmarchand.com; anti-crise.fr; fr.shopping.com; cataloguemate.fr; leguide.com; offrespascher.com; promoalert.com; fr.xmassaver.net; promobutler.be; plusdebonsplans.com; tiendeo.fr; promopascher.com; destockplus.com; meilleurvendeur.com; idealo.fr; pubeco.fr; cdiscountpro.com; argentdubeurre.com; clubpromos.fr; mistergooddeal.com; prixreduits.net; clients.cdiscount.com; horaires.lefigaro.fr; fr.clasf.com parissorbonne.academia.edu cnrs.academia.edu; ehess.academia.edu; uclouvain.academia.edu; ephe.academia.edu; univ-paris1.academia.edu; univ-paris8.academia.edu; univ-lorraine.academia.edu; mindtools.com; univ-catholyon.academia.edu; ancient.eu; ffmedievale.forumgratuit.org; oxford.academia.edu; unil.academia.edu; iprofesional.com; theartstory.org; univamu.academia.edu; marineinsight.com; mapsofindia.com; trend-online.com; coniugazione.it; diggita.it; docsity.com; biografiasyvidas.com; infoplease.com; fr.actualitix.com; docplayer.es; thelocal.de; lectures49.over-blog.com; diariodocentrodomundo.com.br; cinemagia.ro renault-laguna.com megane3.fr; gps-carminat.com; megane2.superforum.fr; r25-safrane.net; diagnostic-auto.com; renault-zoe.forumpro.fr; techniconnexion.com; gamblewiz.com; forum-super5.-recorder.com; mugenarchive.com; honga.net; strawpoll.com; chompy.app; gamepressure.com; cleverbot.com; rp-manga.forum-canada.com; filedropper.com farmville2free.com fv2freegifts.org; goldenlifegroup.com; fb1.farm2.zynga.com; juegossocial.com; fv-zprod-tc-0.farmville.com; megazebra-facebook-trails.mega-zebra.com; zy2.farm2.zynga.com; gameskip.com; fv-zprod.farmville.com; actiplay-asn.com; iscool.iscoolapp.com; farmvilledirt.com; secure1.mesmo.tv; megazebra-facebook.mega-zebra.com; prod-webpool.miniclip.com; banner2.cookappsgames.com; puzzledhearts.com; zynga.com; buggle.cookappsgames.com; apps.facebook.com; deliresetamities.1fr1.net; bubblecoco.cookappsgames.com; apps.fb.miniclip.com; rummikub-apps.com; amomama.fr; gifwi.com; jigsawpuzzlequest.com:3000; pengle.cookappsgames.com; webgl.exoty.com; fr.opossumsauce.com vogue.fr vanityfair.fr; vogue.com; fr.metrotime.be; vivreparis.fr; o.nouvelobs.com; apartmenttherapy.com; vice.com; lefaso.net; timeout.fr; unjourdeplusaparis.com; whosdatedwho.com; pariszigzag.fr; wwd.com; gq.com; taddlr.com; vanityfair.com; elle.com; brain-magazine.fr; admagazine.fr; fashiongonerogue.com; people.com; glamourparis.com; leplus.nouvelobs.com; unilad.co.uk; hellomagazine.com; maliactu.net; noisey.vice.com; france-hotel-guide.com; thisisinsider.com; lanouvelletribune.info tripadvisor.fr monnuage.fr; cityzeum.com; fr.hotels.com; voyageforum.com; rome2rio.com; virail.fr; carnetdescapades.com; petitfute.com; kayak.fr; voyages.michelin.fr; lonelyplanet.fr; ex-</figDesc><table><row><cell>Domain</cell><cell cols="2">30-nearest neighbors (URLS)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">huffingtonpost.es sevilla.abctractorfan.fr forum.farm-connexion.com;</cell><cell>discountfarmer.com;</cell><cell>heure-ouverture.com;</cell><cell>technikboerse.com;</cell><cell>meteo-sud-aveyron.over-blog.com;</cell><cell>ovniclub.com;</cell><cell>materiel-</cell></row><row><cell></cell><cell cols="7">agricole.annuairefrancaiscom;</cell></row><row><cell></cell><cell>nigella.com; trumpexcel.com</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>portail-cloture.ooreka.fr</cell><cell cols="7">mur.oorekafr;</cell></row><row><cell></cell><cell cols="7">lesamisdudiag.com; forum.autocadre.com; minivanchrysler.com; renault-clio-4.forumpro.fr; bmw-one.com; club.caradisiac.com; lesamisdelaprog.com; forum-bmw.fr; alfaromeo-</cell></row><row><cell></cell><cell cols="7">online.com; marcopolo.superforum.fr; v2-honda.com; alfa147-france.net; question-auto.fr; forum308.com; qashqai-passion.info; automobile-conseil.fr; fr.motocrossmag.be; mag-</cell></row><row><cell></cell><cell cols="3">motardes.com; auto-evasion.com; btf-automoto-asq.fr; fr.bmwfans.info</cell><cell></cell><cell></cell><cell></cell></row><row><cell>excel-plus.fr</cell><cell cols="7">tech-connect.info; jetaide.com; officepourtous.com; lame.buanzo.org; it.ccm.net; lecompagnon.info; patatos.over-blog.com; faclic.com; abracadabrapdf.net; faqword.com;</cell></row><row><cell></cell><cell cols="7">windows.developpez.com; thehackernews.com; jiho.com; cartoucherecharge.fr; questionbureautique.over-blog.com; comment-supprimer.com; cgsecurity.org; silky-</cell></row><row><cell></cell><cell cols="7">road.developpez.com; technologie.toutcomment.com; java.developpez.com; br.ccm.net; phptester.net; lephpfacile.com; blogosquare.com; monpc-pro.fr; python.developpez.com;</cell></row><row><cell></cell><cell cols="3">astuces.jeanviet.info; poftut.com; tecadmin.net; blog-nouvelles-technologies.fr</cell><cell></cell><cell></cell><cell></cell></row><row><cell>jeuxvideo.org</cell><cell cols="7">pvpro.com; fallout.fandom.com; pokecommunity.com; xbox-mag.net; make-fortnite-skins.com; garrycity.fr; en.riotpixels.com; brainly.com; gtalogo.com; pngimg.com; 3daim-</cell></row><row><cell></cell><cell cols="7">trainer.com; creativeuncut.com; twitchoverlay.com; en.magicgameworld.com; frondtech.com; discord.me; 11anim.com; kiranico.com; dllme.com; jeugeek.com; myinstants.com;</cell></row><row><cell></cell><cell>online-voice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>de; tagesspiegel.de; sueddeutsche.de; faz.net; sport1.de; badische-zeitung.de; merkur.de; rp-online.de; kicker.de; handelsblatt.com; tz.de; tvmovie.de; abendblatt.de; sportbild.bild.de; nzz.ch; bild.de; seattletimes.com; express.de; morgenpost.de; bz-berlin.de; netzwelt.de; bunte.de; derwesten.de; t-online.de; general-anzeiger-bonn.de; mopo.de; transfermarkt.de; techbook.de; finanzen.net; aargauerzeitung.ch foreca.fr meteo81.fr; meteosurfcanarias.com; sudoku-evolution.com; meteopassion.com; mxcircuit.fr; ledicodutour.com; pecheursunisdelille.com; moncompte-espaceclient.com; impactfm.fr; discountfarmer.com; meteo-normandie.fr; monde-du-velo.com; fr.tutiempo.net; meteojura.fr; ovniclub.com; palombe.com; sur.ly; videos-chasse-peche.com; retroplane.net; pointeduraz.info; carriere.annuairefrancais.fr; meteo-sud-aveyron.over-blog.com; horaires-douverture.fr; banquesenfrance.fr; genealogic.review; fr.meteovista.be; baboun57.over-blog.com; meteonews.ch; pecheapied.net; fournaise.info auto-moto.com feline.cc; largus.fr; news.autojournal.fr; auto-mag.info; test-auto.auto-moto.com; automobile-magazine.fr; caradisiac.com; neowebcar.com; essais.autojournal.fr; promoneuve.fr; autojournal.fr; latribuneauto.com; motorlegend.com; turbo.fr; actu-moteurs.com; worldscoop.forumpro.fr; palais-de-la-voiture.com; voiture.autojournal.fr; autoplus.fr; moniteurautomobile.be; recherche.autoplus.fr; news.sportauto.fr; abcmoteur.fr; fiches-auto.fr; ww2.autoscout24.fr; constructeur.autojournal.fr; zeperfs.com; essais-autos.com; motoservices.com; sportauto.fr az-online.de alittlecraftinyourday.com; theorganisedhousewife.com.au; brittanyherself.com; directoalpaladar.com.mx; mikseri.net; homeplans.com; thesurvivalgardener.com; dmv.org; milliondollarjourney.com; symbols.com; xataka.com.co; kiwilimon.com; mu-43.com; houseofjoyfulnoise.com; leinetal24.de; turniptheoven.com; wetterkanal.kachelmannwetter.com; thinksaveretire.com; lovechicliving.co.uk; wereparents.com; cookrepublic.com; foodinjars.com; lettermenrow.com; raegunramblings.com; thedailytay.com; losreplicantes.com; intmath.com; arthritis-health.com; ourpaleolife.com; juneauempiregateaux-et-delices.com; chefnini.com; humcasentbon.over-blog.com; yummix.fr; fruitsdelamer.com; toutlemondeatabl.canalblog.com; lesjoyauxdesherazade.com; gateauxchocolat.fr; petitsplatsentreamis.com; pechedegourmand.canalblog.com; certiferme.com; yumelise.fr; quelquesgrammesdegourmandise.com; toques2cuisine.com; ricardocuisine.com; companionetmoi.com; recettessimples.fr; docteurbonnebouffe.com; cuisinealafrancaise.com; lighttome.fr; recetteramadan.com cnn.com us.cnn.com; grimsbytelegraph.co.uk; stadiumtalk.com; uk.reuters.com; euronews.com; expressandstar.com; thedailymash.co.uk; politico.eu; nbcnews.com; thedailybeast.com; trendscatchers.co.uk; lancashiretelegraph.co.uk; blogs.spectator.co.uk; itpro.co.uk; standard.co.uk; kentonline.co.uk; doityourself.com; deliaonline.com; puzzles.independent.co.uk; breakingnews.ie; oxfordmail.co.uk; slashdot.org; sportinglife.com; abcactionnews.com; huffpost.com; indy100.com; anagrammer.com; drivepedia.fr; topmercato.com; mercatofootanglais.com; parisfans.fr; footlegende.fr; pariskop.fr; le10sport.com; allpaname.fr; le-onze-parisien.fr; vipsg.fr; planetepsg.com; mercatoparis.fr; paristeam.fr; buzzsport.fr; canal-supporters.com; football.fr; culturepsg.com; footparisien.com; foot-sur7.fr; footradio.com; mercatolive.fr; olympique-etlyonnais.com; planetelille.com; footballclubdemarseille.fr; blaugranas.fr; 90min.com; sportune.fr; footmarseillais.com; livefoot.fr; foot01.com anti-crise.fr echantillonsclub.com; cfid.fr; gesti-odr.com; plusdebonsplans.com; promoalert.com; forum.anti-crise.fr; madstef.com; franceechantillonsgratuits.com; cataloguemate.fr; mesechantillonsgratuits.fr; argentdubeurre.com; auchan.fr; maximum-echantillons.com; forum.madstef.com; tous-testeurs.com; jeu-concours.biz; vos-promos.fr; tiendeo.fr; echantillonsgratuits.fr; echantinet.com; pubeco.fr; bons-plans-astuces.com; lp.testonsensemble.com; toutdonner.com; conforama.fr; clubpromos.fr; vente-unique.com; oferpedia.fr; partir.com; quandpartir.com; salutbyebye.com; mackoo.com; actualitix.com; voyages.ideoz.fr; voyage.linternaute.com; week-end-voyage-lisbonne.com; skyscanner.fr; toocamp.com; plages.tv; routard.com; momondo.fr; gotoportugal.eu; evous.fr; esky.fr; l-itineraire.paris; lepetitmoutard.fr</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The 30-nearest neighbors of 24 different domains according to our trained Domain_only/1:4 representation model. .eldiario.es; okdiario.com; verne.elpais.com; blogs.elconfidencial.com; vozpopuli.com; elespanol.com; smoda.elpais.com; libertaddigital.com; cadenaser.com; sevilla.abc.es; elmon.cat; elperiodico.com; levante-emv.com; kiosko.net; elcorreo.com; motor.elpais.com; cronicaglobal.elespanol.com; elplural.com; ara.cat; rac1.cat; eldiario.es; heraldo.es; elperiodico.cat; eleconomista.es; diariodesevilla.es; guiadelocio.com; lasexta.com; periodistadigital.com; mismarcadores.com lesechos.fr latribune.fr; afrique.latribune.fr; business.lesechos.fr; bfmbusiness.bfmtv.com; financedemarche.fr; challenges.fr; investopedia.com; actufinance.fr; lopinion.fr; contrepoints.org; rfi.fr; etudiant.lefigaro.fr; hbrfrance.fr; capital.fr; e-marketing.fr; marianne.net; journaldunet.com; 05-habillages-theplacetobid.fr; courrierinternational.com; nouvelobs.com; mtf-b2b-asq.fr; lexpansion.lexpress.fr; zonebourse.com; start.lesechos.fr; lentreprise.lexpress.fr; boursier.com; manager-go.com; investing.com; boursedirect.fr; journaldeleconomie.fr orange.fr actu.orange.fr; lemoteur.orange.fr; messagerie.orange.fr; login.orange.fr; finance.orange.fr; sports.orange.fr; meteo.orange.fr; tendances.orange.fr; programme-tv.orange.fr; news.orange.fr; boutique.orange.fr; pro.orange.fr; chaines-tv.orange.fr; ww.orange.fr; agenda.orange.fr; people.orange.fr; zapping-tv.orange.fr; mescontacts.orange.fr; mail01.orange.fr; occasion.auto.orange.fr; video-streaming.orange.fr; musique.orange.fr; tv.orange.fr; mail02.orange.fr; auto.orange.fr; webmail.orange.fr; 118712.fr; cineday.orange.fr; belote-coinchee.net; mahjonggratuit.fr leparisien.fr cnews.fr; atlasinfo.fr; lefigaro.fr; lejdd.fr; jforum.fr; marianne.net; video.lefigaro.fr; tendanceouest.com; bladi.net; observalgerie.com; causeur.fr; scoopnest.com; etudiant.lefigaro.fr; actu17.fr; lalibre.be; fdesouche.com; people.bfmtv.com; rmc.bfmtv.com; bfmtv.com; ici.radio-canada.ca; nouvelobs.com; amp.lefigaro.fr; breizh-info.com; fr.euronews.com; observers.france24.com; tunisienumerique.com; courrierinternational.com; lopinion.fr; sfrpresse.sfr.fr; 94.citoyens.com reddit.com imgur.com; old.reddit.com; askreddit.reddit.com; pcgamer.com; anime.reddit.com; france.reddit.com; gamefaqs.gamespot.com; totalwar.reddit.com; nintendoswitch.reddit.com; gaming.reddit.com; knowyourmeme.com; redditad.com; pcgaming.reddit.com; funny.reddit.com; europe.reddit.com; leagueoflegends.reddit.com; all.reddit.com; freefolk.reddit.com; pcmasterrace.reddit.com; soccer.reddit.com; globaloffensive.reddit.com; gamesradar.com; dankmemes.reddit.com; gfycat.com; gameofthrones.reddit.com; overwatch.reddit.com; popular.reddit.com; smashbros.reddit.com; competitiveoverwatch.reddit.com; aww.reddit.com expedia.fr momondo.fr; skyscanner.fr; kayak.fr; fr.lastminute.com; fr.hotels.com; flights-results.liligo.fr; ebookers.fr; esky.fr; opodo.com; secure.lastminute.com; bravofly.fr; opodo.fr; vol.lastminute.com; vols.idealo.fr; locations.lastminute.com; quandpartir.com; opodo.ch; reservation.lastminute.com; quellecompagnie.com; trainhotel.lastminute.com; lonelyplanet.fr; easyvols.fr; voyages.bravofly.fr; edreams.fr; sejour.lastminute.com; rome2rio.com; voyagespirates.fr; jetcost.com; voyage.lastminute.com; virail.fr tractorfan.fr discountfarmer.com; forum.farm-connexion.com; angleterre.meteosun.com; songs-tube.net; materieltp.fr; assovttroc.clicforum.fr; opel-mokka.forumpro.fr; spa-du-dauphine.fr; vanvesactualite.blog4ever.com; calcul-frais-de-notaire.fr; sectr.net; cuir-creation.forum-box.com; fc-fief-geste.footeo.com; classements.snt-voile.org; rjm-radio.fr; gps-tomtom.fr; v-strom.superforum.fr; migrateurs.forumgratuit.org; gazoline.forumactif.com; recuperation-metaux.annuairefrancais.fr; voitures-societe.ooreka.fr; fcplouay.footeo.com; equishopping.com; annonce123.com; 36kines.com; bastia.onvasortir.com; renault5.forumactif.com; globaldjmix.com; enviedechasser.fr; squidtv.net welt.de zeit..net; mur.ooreka.fr; pierreetsol.com; bricolage.jg-laurent.com; abri-de-jardin.ooreka.fr; aac-mo.com; fr.rec.bricolage.narkive.com; decoration.ooreka.fr; piscineinfoservice.com; bricoleurpro.com; amenagementdujardin.net; betonniere.ooreka.fr; assainissement.ooreka.fr; fenetre.ooreka.fr; expert-peinture.fr; terrasse.ooreka.fr; tondeuse.ooreka.fr; debroussailleuse.ooreka.fr; peinture.ooreka.fr; carrelage.ooreka.fr; parquet.ooreka.fr; amenagement-de-jardin.ooreka.fr; toiture.ooreka.fr; poimobile.fr; schemaelectrique.net; installation-electrique.ooreka.fr; forum-maconnerie.com; muramur.ca; wc.ooreka.fr; plaque-de-cuisson.ooreka.fr sport.fr infomercato..fr; parisunited.fr; football-addict.com; olympique-et-lyonnais.com; football.fr anti-crise.fr cfid.fr; forum.anti-crise.fr; gesti-odr.com; echantillonsclub.com; plusdebonsplans.com; cataloguemate.fr; promoalert.com; argentdubeurre.com; madstef.com; forum.madstef.com; vos-promos.fr; mesechantillonsgratuits.fr; franceechantillonsgratuits.com; tiendeo.fr; tous-testeurs.com; maximum-echantillons.com; ofertolino.fr; auchan.fr; pubeco.fr; echantinet.com; echantillonsgratuits.fr; promo-conso.net; bons-plans-astuces.com; commerces.com; lp.testonsensemble.com; grattweb.fr; promobutler.be; jeu-concours.biz; mafamillezen.com; hitwest.com auchan.fr but.fr; conforama.fr; vente-unique.com; rueducommerce.fr; fr.shopping.com; cdiscount.com; touslesprix.com; promobutler.be; webmarchand.com; mistergooddeal.com; offrespascher.com; argentdubeurre.com; cataloguemate.fr; leguide.com; promoalert.com; vos-promos.fr; fr.xmassaver.net; cdiscountpro.com; clients.cdiscount.com; promopascher.com; meilleurvendeur.com; clubpromos.fr; tiendeo.fr; plusdebonsplans.com; promo-conso.net; iziva.com; destockplus.com; pubeco.fr; meonho.info; fr.clasf.com parissorbonne.academia.edu flux-info.fr; elmostrador.cl; makaan.com; univ-montp3.academia.edu; e-lawresources.co.uk; babycenter.com; newocr.com; insight.co.kr; grandes-inventions.com; policescientifique.com; entrainementfootballpro.fr; muyinteresante.es; ancient.eu; iprofesional.com; slovnik.aktuality.sk; midis101.com; quemas.mamaslatinas.com; adventureinyou.com; wardrawings.be; esky.ro; puzzle-futoshiki.com; univ-paris8.academia.edu; letssingit.com; learn101.org; mindtools.com; medicoresponde.com.br; br.rfi.fr; lazycatkitchen.com; realmenrealstyle.com; eve-adam.over-blog.com renault-laguna.com megane3.fr; gps-carminat.com; megane2.superforum.fr; lesamisdudiag.com; car-actu.com; diagnostic-auto.com; r25-safrane.net; lesamisdelaprog.com; forum.autocadre.com; renault-clio-4.forumpro.fr; renault-zoe.forumpro.fr; v2-honda.com; ccfrauto.com; minivanchrysler.com; techniconnexion.com; forum308.com; lemecano.fr; obd-data.com; forum-super5.fr; club.caradisiac.com; toutsurlamoto.com; cliomanuel.org; forum-kia-sportage.com; tlemcen-electronic.com; focusrstteam.com; 306inside.com; cmonofr.net; 207.fr; automobile-conseil.fr; gamblewiz.com excel-plus.fr tech-connect.info; thehackernews.com; lecompagnon.info; panoptinet.com; slice42.com; aliasdmc.fr; astuces.jeanviet.info; nalaweb.com; patatos.over-blog.com; jiho.com; abavala.com; ohmymac.fr; br.ccm.net; easy-pc.org; wisibility.com; filedesc.com; semageek.com; windows.developpez.com; jetaide.com; galaxynote.fr; fr.stealthsettings.com; chezcyril.over-blog.com; tuto4you.fr; faclic.com; alvinalexander.com; thegeekstuff.com; hirensbootcd.org; faqword.com; openshot.org; zoomonapps.com jeuxvideo.org alsumaria.tv; minecraft-zh.gamepedia.com; infovisual.info; everyonepiano.com; footstream.live; memedroid.com; darkandlight.gamepedia.com; mbti.forumactif.fr; gachagames.net; honga.net; en.magicgameworld.com; garrycity.fr; satelis-passion.forumactif.com; blog-insideout.com; fallout.fandom.com; howtomechatronics.com; cshort.org; fr.trackitonline.ru; openthefile.net; lutain.over-blog.com; alphabetagamer.com; solveyourtech.com; online-voice-recorder.com; png2jpg.com; ffxforever.overblog.com; arkhamhorrorfr.forumactif.com; en.riotpixels.com; pexiweb.be; petri.com; rasage-traditionnel.com farmville2free.com goldenlifegroup.com; fv2freegifts.org; juegossocial.com; fv-zprod-tc-0.farmville.com; fb1.farm2.zynga.com; zy2.farm2.zynga.com; gameskip.com; fv-zprod.farmville.com;</figDesc><table><row><cell>Domain</cell><cell>30-nearest neighbors (URLS)</cell></row><row><cell>huffingtonpost.es</cell><cell>cope.es; muk;</cell></row><row><cell></cell><cell>thenational.scot</cell></row><row><cell>portail-</cell><cell>bricolage-facile</cell></row><row><cell>cloture.ooreka.fr</cell><cell></cell></row></table><note>de; sueddeutsche.de; faz.net; tagesspiegel.de; sport1.de; kicker.de; saarbruecker-zeitung.de; tz.de; bild.de; sportbild.bild.de; hartgeld.com; nzz.ch; abendblatt.de; express.de; ntv.de; bunte.de; handelsblatt.com; merkur.de; bz-berlin.de; aargauerzeitung.ch; badische-zeitung.de; promiflash.de; focus.de; t-online.de; transfermarkt.de; finanzen.net; sport.de; flashscore.de; rp-online.de; stylebook.de foreca.fr my-meteo.com; fr.meteovista.be; fr.tutiempo.net; meteopassion.com; de.sat24.com; nosvolieres.com; meteo-sud-aveyron.over-blog.com; xn-mto-bmab.fr; palombe.com; calculerdistance.fr; meteo81.fr; meteosurfcanarias.com; meteo-normandie.fr; meteobelgique.be; planete-ardechoise.com; parisbrestparis2007.actifforum.com; indicatifs.htpweb.fr; etatdespistes.com; easycounter.com; hauteurdeneige.com; testadsl.net; m.meteorama.fr; discountfarmer.com; prevision-meteo.ch; refugeanimalierdupaysdelanderneau.overblog.com; infosski.com; grottes-france.com; meteolanguedoc.com; impactfm.fr; pont-ile-de-re.com auto-moto.com caradisiac.com; largus.fr; news.autojournal.fr; test-auto.auto-moto.com; auto-mag.info; feline.cc; motorlegend.com; essais.autojournal.fr; automobile-magazine.fr; turbo.fr; autoplus.fr; promoneuve.fr; automobile-sportive.com; neowebcar.com; latribuneauto.com; moniteurautomobile.be; palais-de-la-voiture.com; fr.automobiledimension.com; motoservices.com; autotitre.com; fiches-auto.fr; autojournal.fr; blogzineauto.com; voiture.autojournal.fr; essais-autos.com; notice-utilisation-voiture.fr; sportauto.fr; abcmoteur.fr; recherche.autoplus.fr; news.sportauto.fr az-online.de abountifulkitchen.com; thesurvivalgardener.com; leinetal24.de; brittanyherself.com; symbols.com; ourpaleolife.com; msl24.de; milliondollarjourney.com; arthritis-health.com; thehollywoodunlocked.com; preschoolmom.com; lovechicliving.co.uk; paleoglutenfree.com; lettermenrow.com; theeasyhomestead.com; raegunramblings.com; evolvingscience.com; mu-43.com; juneauempire.com; mikseri.net; e1.ru; thedailytay.com; alittlecraftinyourday.com; comfortablydomestic.com; chicksonright.com; brepurposed.porch.com; kiwilimon.com; grandforksherald.com; catholicstand.com; greatandhra.com tempsdecuisson.net cuisine-facile.com; temps-de-cuisson.info; yummix.fr; aux-fourneaux.fr; cuisinenligne.com; audreycuisine.fr; mamina.fr; uneplumedanslacuisine.com; cnz.to; ricardocuisine.com; chefnini.com; cuisinealafrancaise.com; toutlemondeatabl.canalblog.com; marciatack.fr; atelierdeschefs.fr; lesepicesrient.fr; yumelise.fr; lacuisinededoria.overblog.com; cuisinebassetemperature.com; recettessimples.fr; perleensucre.com; la-cuisine-des-jours.over-blog.com; lesjoyauxdesherazade.com; fraichementpresse.ca; gustave.com; gateaux-chocolat.fr; toques2cuisine.com; recettesduchef.fr; petitsplatsentreamis.com; amandinecooking.com cnn.com us.cnn.com; stadiumtalk.com; thedailybeast.com; itpro.co.uk; uk.reuters.com; euronews.com; theargus.co.uk; theatlantic.com; thedailymash.co.uk; trendscatchers.co.uk; grimsbytelegraph.co.uk; lancashiretelegraph.co.uk; digg.com; spectator.co.uk; politico.eu; blogs.spectator.co.uk; newstatesman.com; huffingtonpost.co.uk; expressandstar.com; puzzles.bestforpuzzles.com; chroniclelive.co.uk; derbytelegraph.co.uk; irishexaminer.com; globalnews.ca; sportinglife.com; slashdot.org; rte.ie; farandwide.com; kentonline.co.fr; parisfans.fr; topmercato.com; vipsg.fr; footradio.com; mercatofootanglais.com; le10sport.com; buzzsport.fr; footparisien.com; foot-sur7.fr; planetepsg.com; pariskop.fr; paristeam.fr; footlegende.fr; le-onze-parisien.fr; mercatoparis.fr; 90min.com; canal-supporters.com; allpaname.fr; sportune.fr; livefoot.fr; culturepsg.com; jeunesfooteux.com; losclive.com; mercatolive.fr; footballclubdemarseillemegazebra-facebook-trails.mega-zebra.com; farmvilledirt.com; megazebra-facebook.mega-zebra.com; iscool.iscoolapp.com; secure1.mesmo.tv; belote-prod-multi.iscoolapp.com; jigsawpuzzlequest.com:3000; fr.puzzle-loop.com; connect.arkadiumhosted.com; pengle.cookappsgames.com; buggle.cookappsgames.com; banner2.cookappsgames.com; apps.fb.miniclip.com; goobox.fr; prod-web-pool.miniclip.com; actiplay-asn.com; apps.facebook.com; snf-web.popreach.com; bubblecoco.cookappsgames.com; rummikubapps.com; watersplash.cookappsgames.com; zynga.com vogue.fr vanityfair.fr; vogue.com; vivreparis.fr; fr.metrotime.be; brain-magazine.fr; o.nouvelobs.com; parismatch.be; pariszigzag.fr; admagazine.fr; unilad.co.uk; konbini.com; monblogdefille.com; affairesdegars.com; neonmag.fr; vice.com; nordpresse.be; leplus.nouvelobs.com; lataille.fr; people-bokay.com; commeuncamion.com; fr.euronews.com; demotivateur.fr; star24.tv; madmoizelle.com; vl-media.fr; whosdatedwho.com; sciencepost.fr; physiquedereve.fr; ztele.com; twog.fr tripadvisor.fr fr.hotels.com; cityzeum.com; voyages.michelin.fr; lonelyplanet.fr; monnuage.fr; voyageforum.com; rome2rio.com; toocamp.com; virail.fr; partir.com; carnetdescapades.com; quellecompagnie.com; mackoo.com; expedia.fr; momondo.fr; salutbyebye.com; orangesmile.com; skyscanner.fr; voyages.ideoz.fr; cars.liligo.fr; quandpartir.com; kelbillet.com; gotoportugal.eu; officiel-des-vacances.com; busradar.fr; week-end-voyage-lisbonne.com; les-escapades.fr; voyage.linternaute.com; bouger-voyager.com; voyagespirates.fr</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
				<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks based Click-Through Rate Prediction with Multiple Feature Sequences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and Scalable Response Prediction for Display Advertising</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-time Bidding Algorithms for Performance-based Display Ad Allocation</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Berkhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><forename type="middle">R</forename><surname>Devanur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The arrival of real-time bidding</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Web-scale Bayesian Click-through Rate Prediction for Sponsored Search Advertising in Microsoft&apos;s Bing Search Engine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Borchert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Supervised Sequence Labelling with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scalable Semantic Matching of Queries to Ads in Sponsored Search Advertising</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grbovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ordentlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<title level="m">Practical Lessons from Predicting Clicks on Ads at Facebook. In ADKDD</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Convolutional Click Prediction Model</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pay-per-action Model for Online Advertising</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mahdian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerem</forename><surname>Tomak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In ADKDD</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ad Click Prediction: a View from the Trenches</title>
		<author>
			<persName><forename type="first">H</forename><surname>Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Hrafnkelsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
		<title level="m">Efficient Estimation of Word Representations in Vector Space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Predicting Response in Mobile Advertising with Hierarchical Importanceaware Factorization Machine</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Oentaryo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Wei</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Finegold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predicting Clicks: Estimating the Click-through Rate for New Ads</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neurocomputing: Foundations of Research</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chapter Learning Representations by Backpropagating Errors</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="696" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Factorization machines with follow-the-regularized-leader for CTR prediction in display advertising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerating t-SNE using Tree-Based Algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Display Advertising with Real-Time Bidding (RTB) and Behavioural Targeting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">/</forename><surname>One_Hot</surname></persName>
		</author>
		<author>
			<persName><surname>Lr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
