<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low Dimensional Manifold Model for Image Processing *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-10-10">electronically October 10, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Zuoqiang</forename><surname>Shi</surname></persName>
							<email>zqshi@mail.tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Yau Mathematical Sciences Center</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>90095</postCode>
									<settlement>Los Angeles</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low Dimensional Manifold Model for Image Processing *</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-10-10">electronically October 10, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">4448ECFDCB6A3F652D1CFD1054E9620A</idno>
					<idno type="DOI">10.1137/16M1058686</idno>
					<note type="submission">Received by the editors January 27, 2016; accepted for publication (in revised form) May 19, 2017;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>AMS subject classifications. 49M37, 65K05</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel low dimensional manifold model (LDMM) and apply it to some image processing problems. LDMM is based on the fact that the patch manifolds of many natural images have low dimensional structure. Based on this fact, the dimension of the patch manifold is used as a regularization to recover the image. The key step in LDMM is to solve a Laplace-Beltrami equation over a point cloud which is solved by the point integral method. The point integral method enforces the sample point constraints correctly and gives better results than the standard graph Laplacian. Numerical simulations in image denoising, inpainting, and superresolution problems show that LDMM is a powerful method in image processing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction. Many image processing problems can be formalized as the recovery of an image f ∈ R m×n from a set of noisy linear measurements (1.1)</p><formula xml:id="formula_0">y = Φf + ε,</formula><p>where ε is the noise. The operator, Φ, typically accounts for some damage to the image, for instance, blurring, missing pixels, or downsampling, so that the measured data y only captures a small portion of the original image f . It is an ill-posed problem to recover the original image from partial information. In order to solve this ill-posed problem, one needs to have some prior knowledge of the image. Usually, this prior information gives different regularizations. With the help of regularizations, many image processing problems are formulated as optimization problems.</p><p>One of the most widely used regularizations is total variation (TV), introduced by Rudin, Osher, and Fatemi (ROF) <ref type="bibr" target="#b13">[14]</ref>. In the ROF model, the following optimization problem is solved:</p><formula xml:id="formula_1">min f f T V + µ 2 y -Φf 2 L 2 , (1.2)</formula><p>where f T V = |∇f (x)|dx. With the Bregman techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref>, TV regularized problems can be solved efficiently. It is well known that TV based model can restore the "cartoon" part of the image very well, while the performance on the texture part of the image is not so good.</p><p>The nonlocal methods are another class widely used in image processing. These were first proposed by Buades, Coll, and Morel <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> as a nonlocal filter for image denoising and were later formulated in a variational framework by Gilboa and Osher <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. In the nonlocal method, the local derivatives are replaced by their nonlocal counterpart: ∇ w u(x, y) = w(x, y)(u(x)u(y)) <ref type="bibr">(1.3)</ref> where w is a weight function defined as w(x, y) = exp -G(s)|u(x + s)u(y + s)| 2 ds , <ref type="bibr">(1.4)</ref> G is a Gaussian. Using the nonlocal derivatives, the nonlocal total variation (NLTV) model is given as follows:</p><formula xml:id="formula_2">min f ∇ w f L 1 + µ 2 y -Φf 2 L 2 , (1.5)</formula><p>where</p><formula xml:id="formula_3">∇ w (f ) L 1 = x y w(x, y)(f (x) -f (y)) 2 1/2 . (1.6)</formula><p>It has been shown that the nonlocal model recovers textures well.</p><p>In this paper, inspired by the nonlocal method and the manifold model of image <ref type="bibr" target="#b12">[13]</ref>, we propose a low dimensional manifold model (LDMM) for image processing. Consider an m × n size image f ∈ R m×n . For any pixel (i, j), where 1 ≤ i ≤ m, 1 ≤ j ≤ n, let this pixel constitute the left-top pixel in an s 1 × s 2 patch and denote this patch as p ij . Let P(f ) denote the collection of all such patches, i.e., <ref type="bibr">(1.7)</ref> P(f ) = {p i,j : (i, j) ∈ Θ ⊂ {1, 2, . . . , m} × {1, 2, . . . , n}} , where Θ is an index set such that the union of the patch set P(f ) covers the whole image. There are many ways to choose Θ. For example, we can choose Θ = {1, 2, . . . , m}×{1, 2, . . . , n} or Θ = {1, s 1 + 1, 2s 1 + 1, . . . , m} × {1, s 2 + 1, 2s 2 + 1, . . . , n}. This freedom may be used to accelerate the computation in LDMM.</p><p>In this paper, P(f ) is called the patch set of f . P(f ) can be seen as a point set in R d with d = s 1 s 2 . The basic assumption in this paper is that P(f ) samples a low dimensional smooth manifold M(f ) embedded in R d , which is called the patch manifold of f . It was revealed that for many classes of images, this assumption holds <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. Based on this assumption, one natural regularization involves the dimension of the patch manifold. We want to recover the original image such that the dimension of its patch manifold is as small as possible. This idea formally gives the following optimization problem: For a natural image, the patch manifold usually is not a single smooth manifold. It may be a set of several manifolds with different dimensions, corresponding to different patterns of the image. In this case, the dimension of the patch manifold, dim(M(f )), becomes a function and we use the integration of dim(M(f )) over M as the regularization,</p><formula xml:id="formula_4">min f dim(M(f )) + λ y -Φf 2 2 . (1.</formula><formula xml:id="formula_5">min f M dim(M(f ))(x)dx + λ y -Φf 2 2 , (1.9)</formula><p>where dim(M(f ))(x) is the dimension of the patch manifold of f at x. Here x is a point in M(f ) which is also a patch of f .</p><p>The problem remaining is how to compute dim(M(f ))(x) for given image f at given patch x. Fortunately, using some basic tools in differential geometry, we find that the dimension of a smooth manifold embedded in R d can be calculated by a simple formula,</p><formula xml:id="formula_6">dim(M)(x) = d j=1 |∇ M α i (x)| 2 ,</formula><p>where α i is the coordinate function, for any</p><formula xml:id="formula_7">x = (x 1 , . . . , x d ) ∈ M ⊂ R d , α i (x) = x i . (1.10)</formula><p>Using this formula, the optimization problem (1.8) can be reformulated as</p><formula xml:id="formula_8">min f ∈R m×n , M⊂R d d i=1 ∇ M α i 2 L 2 (M) + λ y -Φf 2 2 subject to P(f ) ⊂ M,<label>(1.11)</label></formula><p>where (1.12)</p><formula xml:id="formula_9">∇ M α i L 2 (M) = M ∇ M α i (x) 2 dx 1/2 .</formula><p>In this paper, the optimization problem (1.11) is solved by an alternating direction iteration. First, we fix the manifold M and update the image f . Then the image is fixed and we update the manifold. This process is repeated until convergence. In this two-step iteration, the second step is relatively easy. It is done by directly applying the patch operator on the image f . The first step is more difficult. To update the image, we need to solve Laplace-Beltrami equations over the manifold.</p><formula xml:id="formula_10">     -∆ M u(x) + µ y∈Ω δ(x -y)(u(y) -v(y)) = 0, x ∈ M, ∂u ∂n (x) = 0, x ∈ ∂M,<label>(1.13)</label></formula><p>where M is a manifold, ∂M is the boundary of M, n is the out normal of ∂M, δ is the Dirac-δ function in M, and v is a given function in M. The explicit form of the manifold M is not known. We only know a set of unstructured points which samples the manifold M in high dimensional Euclidean space. It is not easy to solve this Laplace-Beltrami equation over this unstructured high dimensional point set. A graph Laplacian is usually used to approximate the Laplace-Beltrami operator on a point cloud. However, from the point of view of numerical PDE, the graph Laplacian was found to be an inconsistent method due to the lack of boundary correction. This is also confirmed by our numerical simulations, e.g., Figure <ref type="figure" target="#fig_5">2(d)</ref>.</p><p>Instead, in this paper, we use the point integral method (PIM) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to solve the Laplace-Beltrami equation over the point cloud. In PIM, the discretized linear system of the Laplace-Beltrami equation (1.13) is given as follows:</p><formula xml:id="formula_11">|M| N N j=1 R t (x i , x j )(u i -u j ) + µt N j=1 Rt (x i , x j )(u j -v j ) = 0, (1.14)</formula><p>where {x 1 , . . . , x N } samples M, v j = v(x j ), and |M| is the volume of the manifold M. R t and Rt are kernel functions which are given in section 4.1.</p><p>Compared with the graph Laplacian which is widely used in the machine learning and nonlocal methods, a boundary term is added in PIM based on an integral approximation of the Laplace-Beltrami operator. With the help of this boundary term, the values in the retained pixels correctly spread to the missing pixels, which gives much better recovery (Figure <ref type="figure" target="#fig_5">2(c)</ref>).</p><p>The rest of the paper is organized as follows. The patch manifold is analyzed in section 2. Several examples are given to show that it usually has a low dimensional structure. In section 3, we introduce the LDMM. The numerical methods including PIM are discussed in section 4. This is the most important section for any potential user of our method. In section 5, we compare the performance of the LDMM method and classical nonlocal methods and carefully explain the apparently slight differences between the two methods. Numerical results are shown in section 6. Concluding remarks are made in section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Patch manifold.</head><p>In this section, we analyze the patch manifold and give several examples. We consider a discrete image f ∈ R m×n . For any (i, j) ∈ {1, 2, . . . , m} × {1, 2, . . . , n}, we define a patch p ij (f ) as a two-dimensional (2D) piece of size s 1 × s 2 of the original image f , and the pixel (i, j) is the top-left corner of the rectangle of size s 1 × s 2 . The patch set P(f ) is defined as the collection of all patches:</p><p>(2.1)</p><formula xml:id="formula_12">P(f ) = {p ij (f ) : (i, j) ∈ {1, 2, . . . , m} × {1, 2, . . . , n}} ⊂ R d , d = s 1 × s 2 .</formula><p>The patch set P(f ) has a trivial 2D parameterization which is given as (i, j) → p ij (f ). In this sense, the patch set is locally a 2D submanifold embedded in R d . However, this parameterization is globally not injective and typically leads to high curvature variations and self-intersections in real applications. For a given image f , the patch set P(f ) gives a point cloud in R d . Many studies reveal that this point cloud is usually close to a smooth manifold M(f ) embedded in R d . This underlying smooth manifold is called the patch manifold associated with f , denoted as M(f ). Figure <ref type="figure" target="#fig_0">1</ref> gives a diagram shows the relation between patch set, trival parameterization, and patch manifold. The patch set and patch manifold have been well studied in the literature. Lee, Pederson, and Mornford studied the patch set for discretized images with 3 × 3 patches <ref type="bibr" target="#b7">[8]</ref>. Their results were refined later by Carlsson et al. <ref type="bibr" target="#b2">[3]</ref>, who perform a simplicial approximation of the manifold. Peyré studied several models of local image manifolds for which an explicit parameterization is available <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. One of the most important features of the patch manifold is that it is close to a low dimensional manifold for many natural images. Now, let us see several simple examples.</p><p>If f is a C 2 function which corresponds to a smooth image. Using Taylor's expansion, p x (f ) can be well approximated by a linear function</p><formula xml:id="formula_13">p x (f )(y) ≈ f (x) + (y -x) • ∇f (x). (2.2)</formula><p>This fact implies that M(f ) is close to a 3D manifold.</p><p>If f is a piecewise constant function which corresponds to a cartoon image, then any patch, p x (f ), is well approximated by a straight edge patch. Each patch is parametrized by the location and the orientation of the edge. This suggests that for a piecewise constant function, M(f ) is also close to a 2D manifold.</p><p>If f is an oscillatory function corresponding to a texture image, we assume that f can be represented as</p><formula xml:id="formula_14">f (x) ≈ a(x) cos θ(x), (2.3)</formula><p>where a(x) and θ(x) are both smooth functions. For each pixel x, the patch p x (f ) can be well approximated by</p><formula xml:id="formula_15">p x (f ) ≈ a L (y) cos θ L (y), (2.4)</formula><p>where a L (x) and θ L (x) are linear approximations of a(x) and θ(x) at x, i.e.,</p><formula xml:id="formula_16">a L (y) = a(x) + (y -x) • ∇a(x), θ L (y) = θ(x) + (y -x) • ∇θ(x).</formula><p>This means that the patch manifold M(f ) is approximately a six-dimensional manifold. Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p><p>These simple examples show that for many images, smooth, cartoon, and texture, the patch manifold is approximately a low dimensional manifold. Then one natural idea is to retrieve the original image by looking for the patch manifold with the lowest dimension. This idea leads to the LDMM introduced in the next section.</p><p>3. Low dimensional manifold model. Based on the discussion in the previous section, we know that an important feature of the patch manifold is low dimensionality. One natural idea is to use the dimension of the patch manifold as the regularization to recover the original image. In the LDMM, we want to recover the image f such that the dimension of its patch manifold M(f ) is as small as possible. This idea formally gives an optimization problem:</p><formula xml:id="formula_17">min f ∈R m×n , M⊂R d dim(M) subject to y = Φf + ε, P(f ) ⊂ M, (3.1)</formula><p>where dim(M) is the dimension of the manifold M.</p><p>However, this optimization problem is not mathematically well defined, since we do not know how to compute dim(M) with given P(f ). Next, we will derive a simple formula for dim(M) using some basic knowledge of differential geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Calculation of dim(M).</head><p>Here we assume M is a smooth manifold embedded in R d . First, we introduce some notation. Since M is a smooth submanifold isometrically embedded in R d , it can be locally parametrized as follows:</p><formula xml:id="formula_18">x = ψ(γ) : U ⊂ R k → M ⊂ R d ,<label>(3.2)</label></formula><p>where</p><formula xml:id="formula_19">k = dim(M), γ = (γ 1 , . . . , γ k ) t ∈ R k and x = (x 1 , . . . , x d ) t ∈ M.</formula><p>Let ∂ i = ∂ ∂γ i be the tangent vector along the direction γ i . Since M is a submanifold in R d with induced metric, ∂ i = (∂ i ψ 1 , . . . , ∂ i ψ d ) and the metric tensor</p><formula xml:id="formula_20">(3.3) g i j =&lt; ∂ i , ∂ j &gt;= d l=1 ∂ i ψ l ∂ j ψ l .</formula><p>Let g i j denote the inverse of g i j , i.e.,</p><formula xml:id="formula_21">k l =1 g i l g l j = δ i j = 1, i = j , 0, i = j . (3.4)</formula><p>For any function u on M, let ∇ M u denote the gradient of u on M,</p><formula xml:id="formula_22">(3.5) ∇ M u = k i ,j =1 g i j ∂ j u ∂ i .</formula><p>We can also view the gradient ∇ M u as a vector in the ambient space R d and let ∇ j M u denote the u component of the gradient ∇ M u in the ambient coordinates, i.e., </p><formula xml:id="formula_23">∇ j M u = k i ,j =1 ∂ i ψ j g i j ∂ j u, j = 1, . . . ,</formula><formula xml:id="formula_24">(x) = x i ∀x = (x 1 , . . . , x d ) ∈ M. (3.7)</formula><p>Then, we have the following formula. Proposition 3.1. Let M be a smooth submanifold isometrically embedded in R d . For any</p><formula xml:id="formula_25">x ∈ M, dim(M) = d j=1 ∇ M α j (x) 2 .</formula><p>Proof. First, following the definition of ∇ M , we have</p><formula xml:id="formula_26">d j=1 ∇ M α j 2 = d i,j=1 ∇ i M α j ∇ i M α j = d i,j=1   k i ,j =1 ∂ i ψ i g i j ∂ j α j     k i ,j =1 ∂ i ψ i g i j ∂ j α j   = d j=1 k i ,j ,i ,j =1 d i=1 ∂ i ψ i ∂ i ψ i g i j g i j ∂ j α j ∂ j α j = d j=1 k j ,i ,j =1 k i =1 g i i g i j g i j ∂ j α j ∂ j α j = d j=1 k j ,i ,j =1 δ i j g i j ∂ j α j ∂ j α j = d j=1 k j ,j =1 g j j ∂ j α j ∂ j α j .</formula><p>The second equality comes from the definition of gradient on M, (3.6). The third and fourth equalities are due to (3.3) and (3.4), respectively. Notice that </p><formula xml:id="formula_27">∂ j α j = ∂ ∂γ j α j (ψ(γ)) = ∂ j ψ j . (3.8) It follows that d j=1 ∇ M α j 2 = d j=1 k j ,j =1 g j j ∂ j ψ j ∂ j ψ j = k j ,j =1 g j j   d j=1 ∂ j ψ j ∂ j ψ j  </formula><formula xml:id="formula_28">= k j ,j =1 g j j g j j = k j =1 δ j j = k = dim(M). (3.9)</formula><p>Using the above proposition, the optimization problem (3.1) can be rewritten as</p><formula xml:id="formula_29">min f ∈R m×n , M⊂R d d i=1 ∇ M α i 2 L 2 (M) + λ y -Φf 2 2 subject to P(f ) ⊂ M, (3.10) where (3.11) ∇ M α i L 2 (M) = M ∇ M α i (x) 2 dx 1/2</formula><p>. This is the optimization problem we need to solve.</p><p>Remark 3.2. As we mentioned in the introduction, in this paper, we minimize the L 1 norm of dim M. It is well known that L 1 minimization tends to give sparse solution, which means that our model favors the patch manifold with dimension zero in many places. This kind of patch manifold actually corresponds to the cartoon image. In the numerical examples, we also observed that our model preserves the edges very well, which fits the intuition of L 1 minimization.</p><p>On the other hand, we can also consider other norms of dim M, for instance, the L 1/2 norm,</p><formula xml:id="formula_30">M (dim M(f )(x)) 1/2 dx = M d i=1 ∇ M α i (x) 2 1/2 dx.</formula><p>In terms of the coordinate functions, α i , it is just the familiar TV regularization. In some sense, L 1/2 norm gives an NLTV model. Corresponding to a different structure of the patch manifold in a different problem, we could minimize a different norm of the dimension, which suggests that the LDMM is very flexible to different kinds of problems.</p><p>4. Numerical method. The optimization problem (3.10) is highly nonlinear and nonconvex. In this paper, we propose an iterative method to solve it approximately. In the iteration, first the manifold is fixed, and the image and the coordinate functions are computed. Then the manifold is updated using the new image and coordinate functions. More specifically, the algorithm is as follows:</p><p>• With a guess of the manifold M n and a guess of the image </p><formula xml:id="formula_31">f n satisfying P(f n ) ⊂ M n ,</formula><formula xml:id="formula_32">) = arg min f ∈R m×n , α 1 ,...,α d ∈H 1 (M n ) d i=1 ∇ M n α i 2 L 2 (M n ) + λ y -Φf 2 2 (4.1) subject to α i (p x (f n )) = p i x (f ),</formula><p>where p i x (f ) is the ith element of patch p x (f ). • Update M by setting</p><formula xml:id="formula_33">M n+1 = (α n+1 1 (x), . . . , α n+1 d (x)) : x ∈ M n . (4.2)</formula><p>• Repeat these two steps until convergence. In the above iteration, the manifold is easy to update. The key step is to solve equality (4.1). Now, (4.1) is a linear optimization problem with constraints. We use the Bregman iteration <ref type="bibr" target="#b10">[11]</ref> to enforce the constraints in (4.1), which gives the following algorithm:</p><p>• Update (f n+1,k+1 , α n+1,k+1 ) by solving</p><formula xml:id="formula_34">(f n+1,k+1 , α n+1,k+1 1 , . . . , α n+1,k+1 d ) = arg min α 1 ,...,α d ∈H 1 (M n ), f ∈R m×n d i=1 ∇α i 2 L 2 (M n ) + µ α(P(f n )) -P(f ) + d k 2 F + λ y -Φf 2 2 ,</formula><p>where</p><formula xml:id="formula_35">α(P(f n )) =      α 1 (P(f n )) α 2 (P(f n )) . . . α d (P(f n ))      ∈ R d×N , N = |P(f n )|,</formula><p>and </p><formula xml:id="formula_36">α i (P(f n )) = (α i (x)) x∈P(f n ) , i = 1 . . . ,</formula><formula xml:id="formula_37">d k+1 = d k + α n+1,k+1 (P(f n )) -P(f n+1,k+1 ).</formula><p>To further simplify the algorithm, we use the idea of the split Bregman iteration <ref type="bibr" target="#b6">[7]</ref> to update f and α i sequentially.</p><p>• Solve α n+1,k+1 i</p><formula xml:id="formula_38">, i = 1, . . . , d, with fixed f n+1,k , (α n+1,k+1 1 , . . . , α n+1,k+1 d ) (4.3) = arg min α 1 ,...,α d ∈H 1 (M n ) d i=1 ∇α i 2 L 2 (M n ) + µ α(P(f n )) -P(f n+1,k ) + d k 2 F .</formula><p>• Update f n+1,k+1 as follows: </p><formula xml:id="formula_39">f n+1,k+1 = arg min f ∈R m×n λ y -Φf 2 2 + µ α n+1,k+1 (P(f n )) -P(f ) + d k 2 F . (<label>4</label></formula><formula xml:id="formula_40">d k+1 = d k + α n+1,k+1 (P(f n )) -P(f n+1,k+1 ).</formula><p>Notice that in (4.3), α n+1,k+1 i , i = 1, . . . , d, can be solved separately,</p><formula xml:id="formula_41">α n+1,k+1 i = arg min α i ∈H 1 (M n ) ∇α i 2 L 2 (M n ) + µ α i (P(f n )) -P i (f n+1,k ) + d k i 2 ,<label>(4.5)</label></formula><p>where P i (f n ) is the ith row of matrix P(f n ).</p><p>Summarizing the above discussion, we get Algorithm 1 to solve the optimization problem (3.10) in LDMM. while not converge do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>With fixed manifold M n , for i = 1, . . . , d, solve</p><formula xml:id="formula_42">α n+1,k+1 i = arg min α i ∈H 1 (M n ) ∇ M n α i 2 L 2 (M n ) + µ α i (P(f n )) -P i (f n+1,k ) + d k i 2 .<label>(4.6) 4:</label></formula><p>Update f n+1,k+1 ,</p><formula xml:id="formula_43">f n+1,k+1 = arg min f ∈R m×n λ y -Φf 2 2 + µ α n+1,k+1 (P(f n )) -P(f ) + d k 2 F . (4.7) 5: Update d k+1 , d k+1 = d k + α n+1,k+1 (P(f n )) -P(f n+1,k+1</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Set</p><formula xml:id="formula_44">f n+1 = f n+1,k , α n+1 = α n+1,k 8: Update M M n+1 = (α n+1 1 (x), . . . , α n+1 d (x)) : x ∈ M n . (4.8)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9: end while</head><p>In Algorithm 1, the most difficult part is to solve following type of optimization problem:</p><formula xml:id="formula_45">min u∈H 1 (M) ∇ M u 2 L 2 (M) + µ y∈P |u(y) -v(y)| 2 , (4.9)</formula><p>where u can be any α i , M = M n , P = P(f n ), and v(y) is a given function on P . Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php By a standard variational approach, we know that the solution of (4.9) can be obtained by solving the following PDE:</p><formula xml:id="formula_46">     -∆ M u(x) + µ y∈P δ(x -y)(u(y) -v(y)) = 0, x ∈ M, ∂u ∂n (x) = 0, x ∈ ∂M,<label>(4.10)</label></formula><p>where ∂M is the boundary of M and n is the out normal of ∂M. If M has no boundary, ∂M = ∅. The problem remaining is to solve the PDE (4.10) numerically. Notice that we do not know the analytical form of the manifold M. Instead, we know P(f n ) is a sample of the manifold M. Then we need to solve (4.10) on this unstructured point set P(f n ). In this paper, we use the PIM <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to solve (4.10) on P(f n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Point integral method.</head><p>The PIM was recently proposed to solve elliptic equations over a point cloud. For the Laplace-Beltrami equation, the key observation in the PIM is the following integral approximation:</p><formula xml:id="formula_47">M ∆ M u(y) Rt (x, y)dy ≈ - 1 t M (u(x) -u(y))R t (x, y)dy + 2 ∂M ∂u(y) ∂n Rt (x, y)dτ y ,<label>(4.11)</label></formula><p>where t &gt; 0 is a parameter and (4.12) We usually set R(r) = e -r ; then Rt (x, y) = R t (x, y) = C t exp( |x-y| 2 4t ) are Gaussians. Next, we give a brief derivation of the integral approximation (4.11) in Euclidean space.</p><formula xml:id="formula_48">R t (x, y) = C t R |x -y| 2 4t . R : R + → R + is a positive C 2 function which is integrable over [0, +∞),</formula><p>Here we assume M is an open set on R d . For a general submanifold, the derivation follows from the same idea but is technically more involved. Interested readers are referred to <ref type="bibr" target="#b15">[16]</ref>. Thinking of Rt (x, y) as test functions, and integrating by parts, we have</p><formula xml:id="formula_49">M ∆u(y) Rt (x, y)dy (4.14) = - M ∇u • ∇ Rt (x, y)dy + ∂M ∂u ∂n Rt (x, y)dτ y = 1 2t M (y -x) • ∇u(y)R t (x, y)dy + ∂M ∂u ∂n Rt (x, y)dτ y .</formula><p>The Taylor expansion of the function u tells us that ). We only need to estimate the following term:</p><formula xml:id="formula_50">u(y) -u(x) =(y -x) • ∇u(y) - 1 2 (y -x) T H u (y)(y -x) + O( y -x 3 )</formula><formula xml:id="formula_51">1 4t M (y -x) T H u (y)(y -x)R t (x, y)dy (4.15) = 1 4t M (y i -x i )(y j -x j )∂ ij u(y)R t (x, y)dy = - 1 2 M (y i -x i )∂ ij u(y)∂ j Rt (x, y) dy = 1 2 M ∂ j (y i -x i )∂ ij u(y) Rt (x, y)dy + 1 2 M (y i -x i )∂ ijj u(y) Rt (y, x)dy - 1 2 ∂M (y i -x i )n j ∂ ij u(y) Rt (x, y)dτ y = 1 2 M ∆u(y) Rt (x, y)dy - 1 2 ∂M (y i -x i )n j ∂ ij u(y) Rt (x, y)dτ y + O(t 1/2 ).</formula><p>The second summand in the last line is O(t 1/2 ). Although its L ∞ (M) norm is of constant order, its L 2 (M) norm is of the order O(t 1/2 ) due to the fast decay of w t (x, y). Therefore, we get Theorem 4.1 to follow from (4.14) and (4.15). The detailed proof can be found in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Using the integral approximation (4.11), we get an integral equation to approximate the original Laplace-Beltrami equation (4.10),</p><formula xml:id="formula_52">M (u(x) -u(y))R t (x, y)dy + µt y∈P Rt (x, y)(u(y) -v(y)) = 0. (4.17)</formula><p>This integral equation has no derivatives and is easy to discretize over the point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discretization.</head><p>Next, we discretize the integral equation (4.17) over the point set P(f n ). To simplify the notation, denote the point cloud as X. Notice that the point cloud</p><formula xml:id="formula_53">X = P(f n ) in the nth iteration.</formula><p>Assume that the point set X = {x 1 , . . . , x N } samples the submanifold M and it is uniformly distributed. The integral equation can be discretized very easily as follows:</p><formula xml:id="formula_54">|M| N N j=1 R t (x i , x j )(u i -u j ) + µt N j=1 Rt (x i , x j )(u j -v j ) = 0, (4.18)</formula><p>where v j = v(x j ) and |M| is the volume of the manifold M. </p><formula xml:id="formula_55">(L + μ W )u = μ W v, (4.19)</formula><p>where v = (v 1 , . . . , v N ) and μ = µtN |M| . L is an N × N matrix which is given as</p><formula xml:id="formula_56">L = D -W , (4.20)</formula><p>where W = (w ij ), i, j = 1, . . . , N , is the weight matrix and D = diag(d i ) with d i = N j=1 w ij . W = ( wij ), i, j = 1, . . . , N , is also a weight matrix. From (4.18), the weight matrices are</p><formula xml:id="formula_57">w ij = R t (x i , x j ), wij = Rt (x i , x j ), x i , x j ∈ P(f n ), i, j = 1, . . . , N. (4.21)</formula><p>Remark 4.2. The discretization (4.18) is based on the assumption that the point set P(f n ) is uniformly distributed over the manifold such that the volume weight of each point is |M|/N . If P(f n ) is not uniformly distributed, PIM actually solves an elliptic equation with variable coefficients <ref type="bibr" target="#b8">[9]</ref>, where the coefficients are associated with the distribution.</p><p>Combining the PIM within Algorithm 1, finally we get the algorithm in LDMM, Algorithm 2. In Algorithm 2, the number of split Bregman iterations is set to be 1 to simplify the computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with nonlocal methods.</head><p>At first sight, LDMM is similar to nonlocal methods. But actually, they are very different. First, LDMM is based on minimizing the dimension of the patch manifold. The dimension of the patch manifold can be used as a general regularization in image processing. In this sense, LDMM is more systematic than nonlocal methods.</p><p>The other important difference is that the formulation of LDMM is continuous, while the nonlocal methods use a graph-based approach. In a graph-based approach, a weighted graph is constructed which links different pixels x, y over the image with a weight w(x, y). On this graph, the discrete gradient is defined, ∀x, y, ∇ w f (x, y) = w(x, y)(f (y)f (x)). <ref type="bibr">(5.1)</ref> Typically, in the nonlocal method, the following optimization problem is solved:</p><formula xml:id="formula_58">min f ∈R m×n 1 2 y -Φf 2 + λJ w(f ) (f ), (5.2)</formula><p>where J w (f ) is a regularization term related with the graph. The most frequently used J w (f ) are the L 2 energy Compute the weight matrix W = (w ij ) from P(f n ), where i, j = 1, . . . , N and N = |P(f n )| is the total number of points in P(f n ),</p><formula xml:id="formula_59">J w (f ) = x,y w(x, y)(f (x) -f (y)) 2 1/2 (5.3) and the NLV J w (f ) = x y w(x, y)(f (x) -f (y))</formula><formula xml:id="formula_60">w ij = R t (x i , x j ), wij = Rt (x i , x j ), x i , x j ∈ P(f n ), i, j = 1, . . . , N.</formula><p>And assemble the matrices L, W , and W as follows:</p><formula xml:id="formula_61">L = D -W , W = (w i,j</formula><p>), W = ( wi,j ), i, j = 1, . . . , N.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>Solve the following linear systems:</p><formula xml:id="formula_62">(L + μ W )U = μ W V , where V = P(f n ) -d n . 4:</formula><p>Update f by solving a least-squares problem:</p><formula xml:id="formula_63">f n+1 = arg min f ∈R m×n λ y -Φf 2 2 + μ U -P(f ) + d n 2 F . 5: Update d n , d n+1 = d n + U -P(f n+1</formula><p>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6: end while</head><p>The nonlocal methods provide powerful tools in image processing and were widely used in many problems. However, from the continuous point of view, the graph-based approach has an intrinsic drawback. Figure <ref type="figure" target="#fig_5">2</ref>(d) shows one example of subsample image recovery computed with an L 2 energy norm. The image of Barbara (Figure <ref type="figure" target="#fig_5">2(a)</ref>) is subsampled. Only 10% of the pixels are retained at random (Figure <ref type="figure" target="#fig_5">2(b)</ref>). It is clear that in the recovered image, some pixels are not consistent with their neighbors. From the zoomed-in image, it is easy to see that these pixels are just the retained pixels. This phenomenon shows that in the graph-based approach, the value at the retained pixels does not spread to their neighbors properly. Compared with the graph-based approach, the result given by the LDMM method is much better (Figure <ref type="figure" target="#fig_5">2(c)</ref>).</p><p>This phenomenon can be explained by using a simple model problem, the Laplace-Beltrami equation with Dirichlet boundary condition,   where M is a smooth manifold embedded in R d and ∂M is its boundary. Suppose the point cloud X = {x 1 , . . . , x N } samples the manifold M and B ⊂ X samples the boundary ∂M.</p><formula xml:id="formula_64">∆ M u(x) = 0, x ∈ M, u(x) = g(x), x ∈ ∂M,<label>(5.5</label></formula><p>Using the graph-based method, the solution of the Dirichlet problem (5.5) is approximately obtained by solving the following linear system:</p><formula xml:id="formula_65">     N j=1 w(x i , x j )(u(x i ) -u(x j )) = 0, x i ∈ X\B, u(x i ) = g(x i ), x i ∈ B.</formula><p>(5.6)</p><p>In the PIM, we know that the Dirichlet problem can be approximated by an integral equation,</p><formula xml:id="formula_66">   1 t M (u(x) -u(y))R t (x, y)dy -2 ∂M ∂u(y) ∂n Rt (x, y)dτ y = 0, x ∈ M, u(x) = g(x), x ∈ ∂M.<label>(5.7)</label></formula><p>Comparing these two approximations, (5.6) and (5.7), we can see that in the graph-based method, the boundary term, -2 ∂M ∂u(y)</p><p>∂n Rt (x, y)dτ y , is dropped. However, it is easy to check that this term is not small. Since the boundary term is dropped, the boundary condition is not enforced correctly in the graph-based method.</p><p>Another difference between LDMM and the nonlocal methods is that the choice of patch is more flexible in LDMM. In nonlocal methods, for each pixel there is a patch and the patch has to be centered around this pixel. In LDMM, we only require that patches have the same size and cover the whole image. This feature gives us more freedom to choose the patch.</p><p>6. Numerical results. In the numerical simulations, the weight function we used is the Gaussian weight</p><formula xml:id="formula_67">R t (x, y) = exp - x -y 2 σ(x) 2 . (6.1)</formula><p>σ(x) is chosen to be the distance between x and its 20th nearest neighbor. make the weight matrix sparse, the weight is truncated to the 50 nearest neighbors.</p><p>The patch size is 10 × 10 in the denoising and inpainting examples and is 20 × 20 in the superresolution examples.</p><p>For each point in X, the nearest neighbors are obtained by using an approximate nearest neighbor (ANN) search algorithm. We use a k-d tree approach as well as an ANN search algorithm to reduce the computational cost. The linear system in Algorithm 2 is solved by GMRES.</p><p>PSNR, defined as follows, is used to measure the accuracy of the results:</p><formula xml:id="formula_68">PSNR(f, f * ) = -20 log 10 ( f -f * /255), (6.2)</formula><p>where f * is the ground truth.</p><p>6.1. Inpainting. In the inpainting problems, the pixels are removed from the original image and we want to recover the original image from the remaining pixels. The corresponding operator, Φ, is</p><formula xml:id="formula_69">(Φf )(x) = f (x), x ∈ Ω, 0,</formula><p>x / ∈ Ω, <ref type="bibr">(6.3)</ref> where Ω ⊂ {0, . . . , m} × {0, . . . , n} is the region where the image is retained. The pixels outside of Ω are removed. In our simulations, Ω is selected at random.</p><p>We assume that the original images do not have noise. In this noise-free case, the parameter λ in the least-squares problem in Algorithm 2 is set to be ∞. Then, the least-squares problem can be solved as</p><formula xml:id="formula_70">f n+1 (x) = f (x), x ∈ Ω, (P * P) -1 (P * (U + d n )), x /</formula><p>∈ Ω, <ref type="bibr">(6.4)</ref> where P * is the adjoint operator of P. Notice that P * P is a diagonal operator. Thus f n+1 can be solved explicitly without inverting a matrix.</p><p>The initial guess of f is obtained by filling the missing pixels with random numbers satisfying Gauss distribution, N (µ 0 , σ 0 ), where µ 0 is the mean of Φf and σ 0 is the standard deviation of Φf . The parameter μ = 0.5.</p><p>In our simulations, the original images are subsampled. Only 10% of the pixels are retained. The retained pixels are selected at random. From these 10% subsampled images, the LDMM method is employed to recover the original images. The images we used in the test Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php are shown in Figure <ref type="figure" target="#fig_6">3</ref>. The reconstructed images are shown in Figure <ref type="figure">4</ref>. In this example, we compare the performance of LDMM with NLTV (1.6), TV, and BPFA <ref type="bibr" target="#b17">[18]</ref>. As we can see in Figure <ref type="figure">4</ref>, the LDMM gives the best recovery. LDMM recovers the image very well in both the cartoon part and the texture part. NLTV and TV work well in the cartoon part, but the reconstruction of the texture is not good. BPFA has problems recovering sharp edges, which makes the results visually less comfortable, although PSNR given by BPFA is pretty good.</p><p>In NLTV, we use the nonlocal gradient (1.3) to discretize the gradient operator, which may introduce inconsistency as we have pointed out in section 5. In Figure <ref type="figure" target="#fig_8">5</ref>, we show zoomed-in recovered images given by NLTV. It can be clearly seen that there are many inconsistent pixels in the reconstructed images. If we zoom in on the recovered images of Lena and peppers, we can also see a similar phenomenon. 6.2. Superresolution. Superresolution corresponds to the recovery of a high-definition image from a low-resolution image. Usually, the low-resolution image is obtained by filtering and subsampling from a high-resolution image:</p><formula xml:id="formula_71">Φf = (f * h) ↓ k , (6.5)</formula><p>where h is a low-pass filter, and ↓ k is the downsampling operator by a factor k along each axis. Here, we consider a simple case in which the filter h is not applied, i.e., Φf = (f ) ↓ k . ( This downsample problem can be seen as a special case of subsample. However, in this problem, the pixels are retained over regular grid points, which makes the recovery much more difficult than that in the random subsample problem considered in the previous example.</p><formula xml:id="formula_73">(Φf )(x) = f (x), x ∈ Ω, 0, x / ∈ Ω, (6.7) where Ω = {1, k + 1, 2k + 1, . . .} × {1, k + 1, 2k + 1, . . .}.</formula><p>Here, we also assume the measure is noise-free and use the same formula (6.4) to update the image f . The parameter μ = 0.5 in the simulation. The initial guess is obtained by bicubic interpolation.  Figure <ref type="figure" target="#fig_9">6</ref> shows the results for three images. The downsample rate is set to be 8. In terms of PSNR, compared with bicubic interpolation, the improvement in LDMM is not substantial. However, the images given by LDMM are visually more pleasing than those given by bicubic since the edges are reconstructed much better. The results of LDMM and NLTV are very close, except the edges in LDMM are a little sharper than those in NLTV.</p><p>6.3. Denoising. In the denoising problem, the operator Φ is the identity operator. In this case, the least-squares problem in Algorithm 2 has a closed form solution: f n+1 = (λ Id + μP * P) -1 (λy + μP * (U + d n )), <ref type="bibr">(6.8)</ref> where P * is the adjoint operator of P.</p><p>In the denoising tests, Gaussian noise is added to the original images. The standard deviation of the noise is 100. Parameters are μ = 0.5 and λ = 0.2 in the simulation.</p><p>Figure <ref type="figure" target="#fig_11">7</ref> shows the results obtained by LDMM, NLTV, and BM3D <ref type="bibr" target="#b3">[4]</ref>. In all the denoising tests, BM3D gives best results in terms of PSNR. The results of LDMM are better than NLTV except in the image of peppers, which is a piecewise smooth image and fits the NLTV model very well.</p><p>In image denoising, we have to admit that the results of our current LDMM model are not as good as those obtained in BM3D. In our subsequent paper, we find that using the semilocal patch could improve the denoising results <ref type="bibr" target="#b14">[15]</ref>.</p><p>At the end of this section, we want to make some remarks on the computational speed of LDMM. As shown in this section, LDMM gives very good results for inpainting, superresolution, and denoising problems. On the other hand, the computational cost of LDMM is relatively high. For the example of <ref type="bibr">Barbara (256 × 256)</ref>   denoising problem, LDMM spends about 9 mins (about 25 mins in BPFA). Both of the tests are run with MATLAB code on a laptop equipped with CPU Intel i7-4900, 2.8 GHz. In this paper, we have not optimized the numerical method for speed. There are many possibilities to speed up. For instance, the nearest neighbors may be searched over a local window rather than over the whole image. We may also exploit the freedom to choose the index set Θ in (1.7). First, Θ is set to be a coarse set such that the number of patches is small and computation is fast. After several iterations, the result is passed to a fine grid to refine the result. Using this idea, preliminary tests show the computational time is reduced to around 5 mins for Barbara (256 × 256) inpainting, while PSNR of the recovered image is 24.55 dB (24.74 dB in Figure <ref type="figure">4</ref>). These procedures will be investigated further in our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion.</head><p>In this paper, we proposed a novel LDMM for image processing. In the LDMM, instead of the image itself, we study the patch manifold of the image. Many studies reveal that the patch manifold has low dimensional structure for many classes of images. In the LDMM, we just use the dimension of the patch manifold as the regularization to recover the original image from the partial information. The PIM also plays a very important role. It gives a correct way to solve the Laplace-Beltrami equation over the point cloud. In this paper, we show the performance of the LDMM in subsample, downsample, and denoising problems.   LDMM gives very good results especially in subsample problems. On the other hand, the dimension of the manifold can be used as a general regularization not only in the image processing problem. In some sense, LDMM is a generalization of the low-rank model. We are now working on applying LDMM to matrix completion, hyperspectral image processing, and large-scale computational physics problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Diagram of patch set P (black points), trivial parameterization (red curve), and patch manifold M.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 .</head><label>1</label><figDesc>LDMM Algorithm-continuous version. Require: Initial guess of the image f 0 , d 0 = 0. Ensure: Restored image f . 1: while not converge do 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>and C t is</head><label></label><figDesc>ds and Rt (x, y)= C t R |x -y| 2 4t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 4 . 1 .</head><label>41</label><figDesc>If u ∈ C 3 (M) is a function on M, then we have for any x ∈ M, r(u) L 2 (M) = O(t 1/4), (4.16) where r(u) = M ∆u(y) Rt (x, y)dy + 1 t M (u(x)u(y))R t (x, y)dy -2 ∂M ∂u(y) ∂n R t (x, y)dτ y .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Subsampled image recovery of Barbara based on LDMM and nonlocal method: (a) original image; (b) subsampled image (10% pixels are retained at random); (c) recovered image by LDMM; (d) recovered image by graph Laplacian. The bottom row shows the zoomed-in image of the red box enclosed area.</figDesc><graphic coords="15,71.29,218.33,102.47,102.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Original image in the tests of subsample restorations.</figDesc><graphic coords="17,157.40,234.19,131.06,131.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Downloaded 10 /</head><label>10</label><figDesc>16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Restored image from 10% subsample image by NLTV.</figDesc><graphic coords="19,71.25,98.47,216.68,216.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Example of image superresolution with downsample rate k = 8.</figDesc><graphic coords="20,164.91,315.93,83.01,83.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>original</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Example of image denoising with standard deviation σ = 100.</figDesc><graphic coords="21,164.91,419.77,83.01,83.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>8 )</head><label>8</label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Let α i , i = 1, . . . , d, be the coordinate functions on M, i.e.,</figDesc><table><row><cell>α i</cell><cell></cell></row><row><cell>(3.6)</cell><cell>d.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>(f n+1 , α n+1 1</cell><cell>, . . . , α n+1 d</cell></row><row><cell cols="2">compute the coordinate functions α n+1 i</cell><cell>, i = 1, . . . , d, and f n+1 ,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>. 4 )</head><label>4</label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php • Update d k+1 ,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>,</head><label></label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php where H u (y) is the Hessian matrix of u at y. Note that M yx n R t (x, y)dy = O(t n/2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php We can rewrite (4.18) in the matrix form:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Algorithm 2. LDMM algorithm. Require: Initial guess of the image f 0 , d 0 = 0. Ensure: Restored image f .</figDesc><table><row><cell></cell><cell>1/2</cell><cell></cell></row><row><cell>(5.4)</cell><cell>2</cell><cell>.</cell></row></table><note><p>1: while not converge do 2:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>)</head><label></label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>. 6 )</head><label>6</label><figDesc>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</figDesc><table><row><cell>10% subsample</cell><cell>LDMM</cell><cell>NLTV</cell><cell>TV</cell><cell>BPFA</cell></row><row><cell></cell><cell>24.74 dB</cell><cell>23.35 dB</cell><cell>22.33 dB</cell><cell>23.44 dB</cell></row><row><cell></cell><cell>28.54 dB</cell><cell>27.75 dB</cell><cell>26.30 dB</cell><cell>27.38 dB</cell></row><row><cell></cell><cell>25.49 dB</cell><cell>23.66 dB</cell><cell>23.21 dB</cell><cell>24.72 dB</cell></row><row><cell></cell><cell>32.12 dB</cell><cell>28.38 dB</cell><cell>28.07 dB</cell><cell>30.41 dB</cell></row></table><note><p><p>4.</p>Examples of subsampled image recovery.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>in the inpainting problem, LDMM needs about 18 mins, while BPFA needs about 15 mins. For Barbara (512 × 512) in the</figDesc><table><row><cell>original</cell><cell>downsample</cell><cell>Bi-cubic</cell><cell>NLTV</cell><cell>LDMM</cell></row><row><cell></cell><cell></cell><cell>23.53 dB</cell><cell>24.76 dB</cell><cell>25.25 dB</cell></row><row><cell></cell><cell></cell><cell>21.82 dB</cell><cell>22.21 dB</cell><cell>22.42 dB</cell></row><row><cell></cell><cell></cell><cell>26.45 dB</cell><cell>26.63 dB</cell><cell>26.48 dB</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded 10/16/17 to 131.172.36.29. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. http://www.siam.org/journals/siims/10-4/M105868.html</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The work of the authors was supported by DOE-SC0013838 and NSF DMS-1118971. The work of the second author was partially supported by NSFC grants 11371220 and 11671005.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of image denoising algorithms, with a new one</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="490" to="530" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neighborhood filters and PDE&apos;s</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the local behavior spaces of natural images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ishkhanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zomorodian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3d transformdomain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonlocal linear image regularization and supervised segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="595" to="630" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonlocal operators with applications to image processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1005" to="1028" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The split bregman method for l1-regularized problems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="323" to="343" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The nonlinear statistics of high-contrast patches in natural images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="83" to="103" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convergent point integral method for isotropic elliptic equations on point cloud</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="874" to="905" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Point integral method for solving Poisson-type equations on manifolds from point clouds with convergence guarantees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="228" to="258" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="460" to="489" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image processing with non-local spectral bases</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="703" to="730" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Manifold models for signals and images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="248" to="260" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization of the weighted nonlocal Laplacian in low dimensional manifold model</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10915-017-0549-x</idno>
		<ptr target="https://doi.org/10.1007/s10915-017-0549-x" />
	</analytic>
	<monogr>
		<title level="j">J. Sci. Comput</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convergence of the point integral method for Poisson equation on point cloud</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Res. Math. Sci</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<idno>MathSciDoc:1609.19003</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="130" to="144" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
