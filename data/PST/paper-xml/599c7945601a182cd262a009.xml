<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative and Discriminative Text Classification with Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
							<email>dyogatama@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
							<email>lingwang@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
							<email>pblunsom@google.com</email>
						</author>
						<title level="a" type="main">Generative and Discriminative Text Classification with Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We empirically characterize the performance of discriminative and generative LSTM models for text classification. We find that although RNN-based generative models are more powerful than their bag-of-words ancestors (e.g., they account for conditional dependencies across words in a document), they have higher asymptotic error rates than discriminatively trained RNN models. However we also find that generative models approach their asymptotic error rate more rapidly than their discriminative counterparts-the same pattern that Ng &amp; Jordan (2001) proved holds for linear classification models that make more naïve conditional independence assumptions. Building on this finding, we hypothesize that RNNbased generative classification models will be more robust to shifts in the data distribution. This hypothesis is confirmed in a series of experiments in zero-shot and continual learning settings that show that generative models substantially outperform discriminative models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural network models used in natural language processing applications are usually trained discriminatively. This strategy succeeds for many applications when training data is abundant and the data distribution is stable. Unfortunately, neural networks require a lot of training data, and they tend to generalize poorly when the data distribution shifts (e.g., new labels, new domains, new tasks). In this paper, we explore using generative models to obtain improvements in sample complexity and ability to adapt to shifting data distributions.</p><p>While neural networks are traditionally used as discriminative models <ref type="bibr" target="#b9">(Ney, 1995;</ref><ref type="bibr" target="#b13">Rubinstein &amp; Hastie, 1997)</ref>, their flexibility makes them well suited to estimating class priors and class-conditional observation likelihoods. We focus on a simple NLP task-text classification-using discriminative and generative variant models based on a common neural network architecture ( §2). These models use an LSTM <ref type="bibr" target="#b3">(Hochreiter &amp; Schmidhuber, 1997)</ref> to process documents as sequences of words. In the generative model, documents are generated word by word, conditioned on a learned class embedding; in the discriminative model the LSTM "reads" the document and uses its hidden representation to model the class posterior. In contrast to previous generative models for text classification, ours can model unbounded (conditional) dependencies among words in each document.</p><p>We demonstrate empirically that our discriminative model obtains a lower asymptotic error rate than its generative counterpart, but it approaches this rate more slowly ( §3.4). This behavior is precisely the pattern that <ref type="bibr" target="#b10">Ng &amp; Jordan (2001)</ref> proved will hold in general for generative and discriminative linear models. Finding the same pattern with our models is somewhat surprising since our generative models are substantially more powerful than the linear models analyzed in that work (e.g., they model conditional dependencies among input features), and because their theoretical analysis relied heavily on linearity. arXiv:1703.01898v2 [stat.ML] 26 May 2017</p><formula xml:id="formula_0">x 1 x 2 x 3 h 1 h 2 h 3 p(y | x) y X (a) Discriminative hsi x 1 x 2 h 1 h 2 h 3 x 3 p(x1 | x&lt;1, y)</formula><p>x Encouraged by this result, we turn to learning problems where good sample complexity is crucial for success and explore whether generative models might be preferable to discriminative ones. We first consider the single-task continual learning setting in which the labels (classes) are introduced sequentially, and we can only learn from the newly introduced examples ( §3.5). Discriminative models are known to suffer from catastrophic forgetting when learning sequentially from examples from a single class at a time, and specialized techniques are actively being developed to minimize this problem <ref type="bibr" target="#b14">(Rusu et al., 2016;</ref><ref type="bibr" target="#b6">Kirkpatrick et al., 2017;</ref><ref type="bibr" target="#b2">Fernando et al., 2017)</ref>. Generative models, on the other hand, are a more natural fit for this kind of setup since the maximization of the training objective for a new class can be decoupled from other classes more easily (e.g., parameters of a naïve Bayes classifier can be estimated independently for each class). In order to compare discriminative and generative models more fairly, we use a generative model that shares many parameters across classes and evaluate its performance in this setting.</p><p>Finally, we compare the performance of discriminative and generative LSTM language models for zero-shot learning, where we construct a semantic label space that is fixed during training based on an auxiliary task ( §3.6). We investigate whether learning to map documents onto this semantic space (discriminative training) or learning to generate from points in the semantic space (generative training) is better. Here, we find substantial benefits for generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>Inputs to a text classification system are a document x = {x 1 , x 2 , . . . , x T }, where T is its length in words, and it will predict a label y ∈ Y. We compare discriminative and generative text classification models. Discriminative models are trained to distinguish the correct label among possible choices. Given a collection of labeled documents {(x i , y i )} N i=1 , these models are trained to maximize the conditional probability of the labels given the documents:</p><formula xml:id="formula_1">N i=1 log p(y i | x i ).</formula><p>Generative models, on the other hand, are trained to maximize the joint probability of labels and documents under the following factorization:</p><formula xml:id="formula_2">N i=1 log p(x i , y i ) = N i=1 log p(x i | y i )p(y i ).</formula><p>When predictions are made, Bayes' rule is used to compute p(y | x).</p><p>In both models, we represent a word x by a D-dimensional embedding x ∈ R D . Figure <ref type="figure" target="#fig_0">1</ref> shows an illustration of our models, and we describe them in details in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Discriminative Model</head><p>Our discriminative model uses LSTM with "peephole" connections to encode a document and build a classifier on top of the encoder by using the average of the LSTM hidden representations as the document representation. Specifically, given an input word embedding x t , we compute its hidden representation h t ∈ R E with LSTM as follows:</p><formula xml:id="formula_3">i t = σ(W i [x t ; h t−1 ; c t−1 ] + b i ) f t = σ(W f [x t ; h t−1 ; c t−1 ] + b f ) c t = f t c t−1 + i t tanh(W c [x t ; h t−1 ] + b c ) o t = σ(W o [x t ; h t−1 ; c t ] + b o ) h t = o t tanh(c t ),</formula><p>where [u; v] denotes vector concatenation. We then add a softmax layer on top of this LSTM, so the probability of predicting a label y ∈ Y is:</p><formula xml:id="formula_4">p(y | x) ∝ exp(( 1 T T t=0 h t )v y + b y )</formula><p>, where V ∈ R E×|Y| is the softmax parameters and b ∈ R |Y| is the bias. We use a simple average of LSTM hidden representations since in our preliminary experiments it works better than using the last hidden state h T , and it is computationally much cheaper for long documents (hundreds of words) than attention-based models. Importantly, this model is trained discriminatively to maximize the conditional probability of the label given the document: p(y | x; W, V).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Models</head><p>Our generative model is a class-based language model, shown in Figure <ref type="figure" target="#fig_0">1</ref>. Here, we similarly compute hidden representation h t with LSTM. Additionally, we also have a label embedding matrix V ∈ R E×|Y| . We use the chain rule to factorize the probability p(x | y) into a sequential prediction: p(x | y) = T t=1 p(x t | x &lt;t , y). To predict the word x t , we concatenate the LSTM's hidden representation x &lt;t (called h t ) with the label embedding v y and add a softmax layer over vocabulary with parameters U and class-specific bias parameters b y : p(x t | x &lt;t , y) ∝ exp(u xt [h t ; v y ] + b y,xt ). We designate this model "Shared LSTM", since it shares some parameters across classes (i.e., the word embedding matrix, LSTM parameters W, and softmax parameters U). This model's novelty owes to the fact that there is a single conditional model that shares parameters whose behavior is modulated by the given label embedding, whereas in traditional generative classification models, each label has an independent LM associated with it, such as the generative n-gram language classification models in <ref type="bibr" target="#b11">Peng &amp; Schuurmans (2003)</ref>.</p><p>In addition to the above model, we also experiment with a class-based generative language model where there is no shared component among classes (i.e., every class has its own word embedding, LSTM, and softmax parameters). One benefit of this approach is that training can be parallelized across classes, although the resulting model has larger number of parameters. We denote this model by "Independent LSTMs".</p><p>Note that the underlying LSTM of both our generative models is similar to the discriminative model, except that it is trained to maximize the joint probability p(y, x; W, V, U) = p(x | y; W, V, U)p(y). In terms of the number of parameters, these generative models have extra parameters U that are needed to predict words (we can view the label embedding matrix as a substitute for the softmax parameter in the discriminative case). For prediction, we compute ŷ = argmax y∈Y p(x | y; W, V, U)p(y) using the empirical relative frequency estimate of p(y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>We use publicly available datasets from <ref type="bibr" target="#b18">Zhang et al. (2015)</ref> to evaluate our models (http: //goo.gl/JyCnZq). They are standard text classification datasets that include news classification, sentiment analysis, Wikipedia article classification, and questions and answers categorization. Table <ref type="table" target="#tab_0">1</ref> shows descriptive statistics of datasets used in our experiments. For each dataset, we randomly hold 5,000 examples from the original training set to be used as our development set. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>In addition to our generative vs. discriminative models in §2 and baselines from previous work on these datasets, we also compare with the following generative models: Naïve Bayes neural network. Last, we also design a naïve Bayes baseline where p(x t | y) is modeled by a feedforward neural network (in our case, we use a two layer neural network). This is an extension of the naïve Bayes baseline, where we replace the class-conditional count-based unigram language model with a class-conditional vector-based unigram language model.</p><formula xml:id="formula_5">Naïve</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation Details</head><p>In all our experiments, we set the word embedding dimension D and the LSTM hidden dimension E to 100.<ref type="foot" target="#foot_0">1</ref> For the generative model, the dimension of the class embedding is also set to 100. We train our model using AdaGrad <ref type="bibr" target="#b1">(Duchi et al., 2012)</ref> and tune the learning rate on development sets. We also use the development sets to decide when to stop training based on classification accuracy as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sample Complexity and Asymptotic Errors</head><p>Ng &amp; Jordan (2001) theoretically and empirically show that generative linear models reach their (higher) asymptotic error faster than discriminative models (naïve Bayes classifier vs. logistic regression). While it is difficult to derive the theoretical properties of expressive recurrent neural network models such as ours, we empirically evaluate the performance of these models.</p><p>Table <ref type="table" target="#tab_1">2</ref> summarizes our results on the full datasets, along with results from previous work on these datasets. Our discriminative LSTM model is competitive with other discriminative models based on logistic regression <ref type="bibr" target="#b18">(Zhang et al., 2015;</ref><ref type="bibr" target="#b5">Joulin et al., 2016)</ref> or convolutional neural networks <ref type="bibr" target="#b18">(Zhang et al., 2015;</ref><ref type="bibr" target="#b16">Xiao &amp; Cho, 2016;</ref><ref type="bibr" target="#b0">Conneau et al., 2016)</ref>. All of the generative models have lower classification accuracies. These results agree with <ref type="bibr" target="#b10">Ng &amp; Jordan (2001)</ref> that discriminative models have lower asymptotic errors than generative models.</p><p>Comparing various generative models, we can see that the generative LSTM models are generally better than baseline generative models with stronger independence assumptions (i.e., naïve Bayes, Kneser-Ney Bayes, and naïve Bayes neural network). Our results suggest that LSTM is an effective method to capture dependencies among words in a document. We also compare the two generative LSTM models: shared LSTM and independent LSTMs. The results are roughly similar.</p><p>Next, we evaluate our models with varying training size. For each of our six datasets, we randomly choose 5, 20, 100, and 1000 examples per class. We train the models on these smaller datasets and report results in Figure <ref type="figure" target="#fig_2">2</ref>. Our results show that the generative shared LSTM model outperforms the discriminative model in almost all cases in the small-data regime on all datasets except one (AG News). Among generative models, the generative LSTM model still achieves better classification accuracies compared to naïve Bayes and Kneser-Ney Bayes models, even in the small-data regime.</p><p>While it is difficult to analyze the theoretical sample complexity of deep recurrent models, we see this collection of results as an empirical support that generative nonlinear models have lower sample complexity than their discriminative counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Continual learning</head><p>Our next set of experiments investigate properties of discriminative and generative LSTM models to adapt to data distribution shifts. An example of data distribution shift is when new classes are introduced to the models. In the real-world setting, being able to detect emergence of a new class  Setup. We consider the setup where models are presented with examples from each class sequentially (single-task continual learning).<ref type="foot" target="#foot_1">2</ref> Here, each model has to learn information from newly introduced examples to be able to correctly classify documents into this new class, but they cannot train from previously seen classes while doing so. Table <ref type="table" target="#tab_3">3</ref> summarizes our results and we discuss them in details in the followings.</p><p>Discriminative LSTM. We first investigate the performance of the LSTM discriminative model. Discriminative models are known to suffer from catastrophic forgetting-where the models overtrain on the newly introduced class and fail to retain useful information from previous classes-in this setup. In these experiments, every time we see examples from a new class, we update all parameters of the model with information the new examples. However, even after extensive tuning of learning rate and freezing some components of the network,<ref type="foot" target="#foot_2">3</ref> we were unable to avoid catastrophic forgetting. We observe that since the model is trained to discriminate among possible classes, when it only sees examples from a single class for tens or hundreds of iterations, it adjusts its parameters to always predict the new class. Of course, in theory, there might be an oracle learning rate that would prevent catastrophic forgetting while still acquiring enough knowledge about the newly introduced classes to update the model. However, we find that in practice it is very difficult to discover these learning rate values, especially for a reasonably large LSTM model that takes high-dimensional input such as a long news article. Even for the same learning rate value, the development set performance varies widely across multiple training runs. Note that since this model is trained discriminatively,  it is also not trivial make use of information from unlabeled data to pretrain some components of the model, except for the word embedding matrix. A promising method to prevent catastrophic forgetting in discriminative models is elastic weight consolidation <ref type="bibr" target="#b6">(Kirkpatrick et al., 2017)</ref>. However, the method requires computing a Fisher information matrix, and it is not clear how to compute it efficiently for complex models such as LSTM on GPUs. Generative LSTM. Next, we consider the generative independent LSTMs model. Parameter estimations of some generative models, such as the naïve Bayes and this independent LSTMs, can be naturally decoupled across classes. As a result, these models can easily incorporate information from newly introduced examples from a new class. Every time a new class is introduced, we simply learn a new model of p(x | y new ; W ynew , v ynew , U ynew ) and ∀y ∈ Y, update p(y). One possible drawback of this approach is that the size of the model grows with the number of classes.</p><p>Last, we experiment with the generative shared LSTM model. Our training procedure for this model is as follows. We first train the LSTM language model part on a large amount of unlabeled data. <ref type="foot" target="#foot_3">4</ref>When training the language model on unlabeled data, we remove the class-specific bias component b y and set the class embedding v y to be a random vector ỹ with a bounded norm ṽy 2 ≤ 1. After we pretrain the shared components, we freeze their parameters and tune the class embedding v y as well as the class-specific softmax bias b y on the labeled training data. The benefit of this trainingcompared to having a separate LSTM model for each class-is that it is faster to train (in the presence of examples from a new class). Given a new class y, we only need to learn two vectors: v y and b y . We can see from results in Table <ref type="table" target="#tab_3">3</ref> and Figure <ref type="figure" target="#fig_3">3</ref> that the generative shared LSTM model trained with this procedure approaches the performance of its equivalent model that can see examples from all classes, and performs competitively with the generative independent LSTMs model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Zero-shot learning</head><p>Our last set of experiments compare the performance of discriminative and generative LSTM language models for zero-shot learning, where the label embedding space is fixed based on an auxiliary task. Humans can acquire new concepts and learn relations among these concepts from an external task, and use this knowledge effectively across multiple tasks. In these experiments, we use datasets where the class labels are semantically meaningful concepts (e.g., science, sports, business-instead of star ratings from 1 to 5).</p><p>Setup. We remove labels of documents from one of the classes, but we provide the models with knowledge about the classes from external sources in the form of class embeddings V. For example, when labels are words (e.g. science, sports, business, etc.), we construct a semantic space by learning the label embeddings v y using standard word embedding techniques for all y ∈ Y. In order to do this, we use pretrained GloVe word embedding vectors <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>. <ref type="foot" target="#foot_4">5</ref> In cases where the class labels consist of more than one words (e.g., society and culture), we choose one word from the labels (e.g., society). At test time, we see examples from all classes and evaluate precision and recall of the hidden class, as well as the overall accuracy on all classes.</p><p>Discriminative LSTM. The model learns from the labeled data to place documents in the semantic space, such that embeddings of documents are close to embeddings of their respective labels. In practice, we fix the softmax parameters V and learn an embedding of the document to maximize exp(( 1 T T t=0 h t )v y ), where h t is the LSTM hidden state for word t in the document. In our experiments, this model never predicts the hidden class (zero precision and recall). Our results show  <ref type="table" target="#tab_4">4</ref>. We can see that for most hidden classes, the generative model achieves good performance. For example, on the AG News dataset, the model performs reasonably well for any of the hidden classes. For a more difficult dataset where the overall accuracy is not very high such as Yahoo, the precision of the hidden class is lower, and as a result the recall also suffers. Nonetheless, the model is still able to achieve reasonable overall accuracy in some cases (recall that the accuracy of this model trained on the full dataset without any hidden class is 69.3).</p><p>Of course, if we include predicted hidden class examples to the training set of a discriminative model, it can also achieve good performance on all classes. However, the main point is that the discriminative LSTM model never predicts the hidden classes without any training data. Two-class zero-shot learning. We also perform experiments with the generative LSTM model when we hide two classes on the AG News dataset and show the results in Table <ref type="table" target="#tab_5">5</ref>. In this case, the model does not perform as well since the precision of predicting hidden classes drops significantly, introducing too much noise in the training data. However, the model is still able to learn some useful  <ref type="bibr" target="#b8">(Morin &amp; Bengio, 2005)</ref>, noise contrastive estimation <ref type="bibr" target="#b7">(Mnih &amp; Teh, 2012)</ref>, sampled softmax <ref type="bibr" target="#b4">(Jean et al., 2015)</ref>, or one-vs-each approximation <ref type="bibr" target="#b15">(Titsias, 2017)</ref>. However, even with these approximations, discriminative models are still much faster.</p><p>Data likelihood. In generative models, we can easily compute the probability of a document by marginalizing over classes: p(x) = y∈Y p(x | y)p(y). Since there is no explicit model of p(x) in discriminative models, obtaining it would require a separate language model training. We explore whether we can use p(x) as an indicator of the presence of a new (unknown) class. We use the AG News corpus and train the generative LSTM model on examples from only 3 labels (recall that there are 4 labels in this corpus). We compute p(x) for all documents in the test set and show the results in Figure <ref type="figure" target="#fig_4">4</ref>. We can see that examples from the class where there is no training data (i.e., class 0 and class 1 in the top and bottom figures, respectively) tend to have lower marginal likelihoods than examples from classes observed in the training data. In practice, we can use this observation to see whether there is data distribution shifts and we need to update parameters of our models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have compared discriminative and generative LSTM-based text classification models in terms of sample complexity and asymptotic error rates. We showed that generative models are better than their discriminative counterparts in small-data regime, empirically extending the (theoretical) results of <ref type="bibr" target="#b10">Ng &amp; Jordan (2001)</ref> from linear to nonlinear models. Formal characterization of the generalization behavior of complex neural networks is difficult, with findings from convex problems failing to account for empirical facts about generalization <ref type="bibr" target="#b17">Zhang et al. (2017)</ref>. As such, this result is remarkable for being one domain in which generalization behavior of simpler models transfers to more complex models.</p><p>We also investigated their properties in the continual and zero-shot settings. Our collection of results showed that generative models are more suitable in these settings and they were able to obtain comparable performance to generative models trained on the full datasets in the standard setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrations of our discriminative (left) and generative (right) LSTM models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Bayes classifier. A simple count-based unigram language model that uses naïve Bayes assumption to factorize p(x | y) = T t=1 p(x t | y). Kneser-Ney Bayes classifier. A more sophisticated count-based language model that uses trigrams and Kneser-Ney smoothing p(x | y) = T t=1 p(x t | x t−1 , x t−2 , y). Similar to the naïve Bayes classifier, we construct one language model per class and predict by computing: ŷ = argmax y∈Y p(x | y)p(y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracies of generative and discriminative models with varying training size. and train on examples from a new class that shows up at a later time without having to retrain the model on the entire dataset is extremely attractive, especially in cases where we have a very large dataset and model. We focus on how well these models learn from new classes in this (sub)section and discuss detection of data distribution shifts in more details in §4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Classification accuracies on the AG News dataset for the generative LSTM models as we introduce class 0, 1, 2, and 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Log likelihood of test data p(x) from the generative LSTM model on the AG News dataset when training data only includes three classes. In the top plot, we exclude training examples from class 0, whereas in the bottom plot we exclude training examples class 1. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Descriptive statistics of datasets used in our experiments.</figDesc><table><row><cell>Name</cell><cell>#Train #Dev</cell><cell cols="2"># Test # Classes</cell></row><row><cell>AGNews</cell><cell>115,000 5,000</cell><cell>7,600</cell><cell>4</cell></row><row><cell>Sogou</cell><cell cols="2">445,000 5,000 60,000</cell><cell>5</cell></row><row><cell>Yelp</cell><cell cols="2">645,000 5,000 50,000</cell><cell>5</cell></row><row><cell>Yelp Binary</cell><cell>555,000 5,000</cell><cell>7,600</cell><cell>2</cell></row><row><cell>DBPedia</cell><cell cols="2">555,000 5,000 70,000</cell><cell>14</cell></row><row><cell>Yahoo</cell><cell cols="2">1,395,000 5,000 60,000</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of results on the full datasets.</figDesc><table><row><cell>Models</cell><cell cols="6">AGNews Sogou Yelp Bin Yelp Full DBPed Yahoo</cell></row><row><cell>Naïve Bayes</cell><cell>90.0</cell><cell>86.3</cell><cell>86.0</cell><cell>51.4</cell><cell>96.0</cell><cell>68.7</cell></row><row><cell>Kneser-Ney Bayes</cell><cell>89.3</cell><cell>94.6</cell><cell>81.8</cell><cell>41.7</cell><cell>95.4</cell><cell>69.3</cell></row><row><cell>MLP Naïve Bayes</cell><cell>89.9</cell><cell>76.1</cell><cell>73.6</cell><cell>40.4</cell><cell>87.2</cell><cell>60.6</cell></row><row><cell>Discriminative LSTM</cell><cell>92.1</cell><cell>94.9</cell><cell>92.6</cell><cell>59.6</cell><cell>98.7</cell><cell>73.7</cell></row><row><cell>Generative LSTM-independent comp.</cell><cell>90.7</cell><cell>93.5</cell><cell>90.0</cell><cell>51.9</cell><cell>94.8</cell><cell>70.5</cell></row><row><cell>Generative LSTM-shared comp.</cell><cell>90.6</cell><cell>90.3</cell><cell>88.2</cell><cell>52.7</cell><cell>95.4</cell><cell>69.3</cell></row><row><cell>bag of words (Zhang et al., 2015)</cell><cell>88.8</cell><cell>92.9</cell><cell>92.2</cell><cell>58.0</cell><cell>96.6</cell><cell>68.9</cell></row><row><cell>fastText (Joulin et al., 2016)</cell><cell>92.5</cell><cell>96.8</cell><cell>95.7</cell><cell>63.9</cell><cell>98.6</cell><cell>72.3</cell></row><row><cell>char-CNN (Zhang et al., 2015)</cell><cell>87.2</cell><cell>95.1</cell><cell>94.7</cell><cell>62.0</cell><cell>98.3</cell><cell>71.2</cell></row><row><cell>char-CRNN (Xiao &amp; Cho, 2016)</cell><cell>91.4</cell><cell>95.2</cell><cell>94.5</cell><cell>61.8</cell><cell>98.6</cell><cell>71.7</cell></row><row><cell>very deep CNN (Conneau et al., 2016)</cell><cell>91.3</cell><cell>96.8</cell><cell>95.7</cell><cell>64.7</cell><cell>98.7</cell><cell>73.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Continual learning results. Shared and Ind.-Gen are the generative shared and independent models respectively (see text for details).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AG News</cell></row><row><cell>Datasets AG News Yelp Full Yelp Binary DBPedia Yahoo</cell><cell>Shared-Gen Ind.-Gen Disc. 90.2 90.7 40.5 51.4 52.7 20.0 86.4 90.0 57.2 95.7 94.8 8.3 68.5 70.5 10.0</cell><cell>% accuracy</cell><cell>90 92 94 96 98 100</cell><cell cols="2">gen LSTM --shared gen LSTM --independent</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>new class</cell><cell>2</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Zero shot learning results on four datasets. Hidden class indicates the class that is not included in the training data. We show precision and recall on test data for the hidden class, as well as accuracy for examples from all classes. The model learns to generate from points in the label semantic space using the labeled documents. The model may infer how to generate a document about politics without ever having seen such an example in the training data. Similar to the discriminative case, we fix V, which in this case plays the role of class embeddings, and train other parts of the model on all training data except examples from the hidden class (we use the generative shared LSTM since naturally the generative independent LSTMs cannot be used in this experiment). We observe on the development set that this model is able to predict examples from the unseen class with high precision, but very low recall (≈ 1%). We design a self-training algorithm that add predicted hidden class examples from the development set to the training set and allow the model to train on these predicted examples. We show the results in Table</figDesc><table><row><cell>Dataset</cell><cell>Hidden Class</cell><cell cols="2">Prec. Recall Acc.</cell></row><row><cell></cell><cell>world</cell><cell>94.4</cell><cell>77.8 87.8</cell></row><row><cell>AG News</cell><cell>sports business</cell><cell>95.7 84.9</cell><cell>83.3 85.5 60.1 83.9</cell></row><row><cell></cell><cell>science and tech</cell><cell>92.0</cell><cell>54.3 83.2</cell></row><row><cell></cell><cell>sports</cell><cell>95.0</cell><cell>80.5 87.4</cell></row><row><cell></cell><cell>finance</cell><cell>24.2</cell><cell>0.7 73.1</cell></row><row><cell>Sogou</cell><cell>entertainment</cell><cell>90.2</cell><cell>78.8 86.6</cell></row><row><cell></cell><cell>automobile</cell><cell>42.9</cell><cell>0.7 72.1</cell></row><row><cell></cell><cell>science and tech</cell><cell>99.7</cell><cell>58.7 85.6</cell></row><row><cell></cell><cell>society and culture</cell><cell>42.8</cell><cell>7.9 64.9</cell></row><row><cell></cell><cell>science and math</cell><cell>48.3</cell><cell>9.8 63.2</cell></row><row><cell></cell><cell>health</cell><cell>26.3</cell><cell>0.4 61.8</cell></row><row><cell></cell><cell>education and reference</cell><cell>23.5</cell><cell>3.8 65.2</cell></row><row><cell>Yahoo</cell><cell>computers and internet sports</cell><cell>45.4 52.9</cell><cell>3 60.8 52.9 64.6</cell></row><row><cell></cell><cell>business and finance</cell><cell>43.6</cell><cell>17.3 66.2</cell></row><row><cell></cell><cell>entertainment and music</cell><cell>44.9</cell><cell>2.3 63.2</cell></row><row><cell></cell><cell>family and relationships</cell><cell>8.3</cell><cell>0.05 62.5</cell></row><row><cell></cell><cell>politics and government</cell><cell>48.6</cell><cell>10.4 62.1</cell></row><row><cell></cell><cell>company</cell><cell>98.9</cell><cell>46.6 93.3</cell></row><row><cell></cell><cell>educational institution</cell><cell>99.2</cell><cell>49.5 92.8</cell></row><row><cell></cell><cell>artist</cell><cell>88.3</cell><cell>4.3 90.3</cell></row><row><cell></cell><cell>athlete</cell><cell>96.5</cell><cell>90.1 94.6</cell></row><row><cell></cell><cell>office holder</cell><cell>0</cell><cell>0 89.1</cell></row><row><cell></cell><cell>mean of transportation</cell><cell>96.5</cell><cell>74.3 94.2</cell></row><row><cell>DBPedia</cell><cell>building natural place</cell><cell>99.9 98.9</cell><cell>37.7 92.1 88.2 95.4</cell></row><row><cell></cell><cell>village</cell><cell>99.9</cell><cell>68.1 93.8</cell></row><row><cell></cell><cell>animal</cell><cell>99.7</cell><cell>68.1 93.8</cell></row><row><cell></cell><cell>plant</cell><cell>99.2</cell><cell>76.9 94.3</cell></row><row><cell></cell><cell>album</cell><cell>0.03</cell><cell>0.001 88.8</cell></row><row><cell></cell><cell>film</cell><cell>99.4</cell><cell>73.3 94.5</cell></row><row><cell></cell><cell>written work</cell><cell>93.8</cell><cell>26.5 91.3</cell></row><row><cell cols="4">that while discriminative training of an expressive model such as LSTM on high dimensional text</cell></row><row><cell cols="4">data produces a reliable predictor of seen classes, the resulting model overfits to the seen classes.</cell></row><row><cell>Generative LSTM.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Zero-shot learning results with two hidden class on the AG News dataset. We show P0 (P1) and R0 (R1) that indicate precision and recall for hidden class one (two), as well as overall accuracy. In terms of training and inference time, discriminative models are much faster. For example, for our smallest (biggest) dataset that contains 115,000 (1,395,000) training examples, it takes approximately two hours (two days) to get good generative models, whereas training the discriminative models only takes approximately 20 minutes (6 hours). The main drawback of generative models in NLP applications is in the softmax computation, since it has to be done over an entire vocabulary set. In many cases, such as in our experiments, the size of the vocabulary is in the order of hundreds of thousands. There are approximate methods to speed up this softmax computation, such as via hierarchical softmax</figDesc><table><row><cell>Classes</cell><cell>P0</cell><cell>R0</cell><cell>P1</cell><cell>R1 Acc.</cell></row><row><cell>world+sports</cell><cell>43.2</cell><cell cols="3">3.4 54.7 90.2 67.2</cell></row><row><cell>world+business</cell><cell cols="3">55.4 75.6 25.9</cell><cell>2.2 67.6</cell></row><row><cell>world+science/tech</cell><cell>40.5</cell><cell cols="3">5.7 38.7 47.8 61.1</cell></row><row><cell>sports+business</cell><cell cols="3">62.3 80.7 48.3</cell><cell>6.6 67.6</cell></row><row><cell>sports+science/tech</cell><cell cols="3">66.2 85.5 66.8</cell><cell>6.7 67.6</cell></row><row><cell>business+science/tech</cell><cell cols="3">43.6 62.1 59.0</cell><cell>1.9 63.3</cell></row><row><cell cols="5">information since the overall accuracy is still higher than 50% (the accuracy of models that are only</cell></row><row><cell cols="5">trained on two classes without any zero-shot learning of the hidden classes).</cell></row><row><cell>4 Discussion</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Computational complexity.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We also experimented with setting both D and E to 50 and 300, they resulted in comparable performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Here, we focus on a single-task continual learning setup, although the idea can be generalized to a multitask setting as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">For example, we experiment with fixing the word embedding matrix and train all other components and fixing the LSTM language model component after seeing the first class and only train the softmax parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">In practice, we train on the corresponding training dataset without using document labels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><surname>Holger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Resesarch</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pathnet: Evolution channels gradient descent in super neural networks</title>
		<author>
			<persName><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><surname>Banarse</surname></persName>
		</author>
		<author>
			<persName><surname>Dylan</surname></persName>
		</author>
		<author>
			<persName><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><surname>Zwols</surname></persName>
		</author>
		<author>
			<persName><surname>Yori</surname></persName>
		</author>
		<author>
			<persName><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Kyunghyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL-IJCNLP</title>
				<meeting>of ACL-IJCNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><surname>Edouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><surname>Razvan</surname></persName>
		</author>
		<author>
			<persName><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><surname>Joel</surname></persName>
		</author>
		<author>
			<persName><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><surname>Kieran</surname></persName>
		</author>
		<author>
			<persName><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><surname>John</surname></persName>
		</author>
		<author>
			<persName><surname>Ramalhoa</surname></persName>
		</author>
		<author>
			<persName><surname>Tiago</surname></persName>
		</author>
		<author>
			<persName><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName><surname>Agnieszka</surname></persName>
		</author>
		<author>
			<persName><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName><surname>Demis</surname></persName>
		</author>
		<author>
			<persName><surname>Clopath</surname></persName>
		</author>
		<author>
			<persName><surname>Claudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<title level="m">Overcoming catastrophic forgetting in neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast and simple algorithm for training neural probabilistic language models</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName><surname>Whye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
				<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS</title>
				<meeting>of AISTATS</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the probabilistic interpretation of neural network classifiers and discriminative training criteria</title>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On discriminative vs. generative classifiers: A comparison of logistic regression and naive Bayes</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining naive Bayes and n-gram language models for text classification</title>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECIR</title>
				<meeting>of ECIR</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
				<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative vs informative learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Koray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<title level="m">Progressive neural networks</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One-vs-each approximation to softmax for scalable estimation of probabilities</title>
		<author>
			<persName><forename type="first">Michalis</forename><forename type="middle">K</forename><surname>Titsias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient character-level document classification by combining convolution and recurrent layers</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Samy</surname></persName>
		</author>
		<author>
			<persName><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
				<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS</title>
				<meeting>of NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
