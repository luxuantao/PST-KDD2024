<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confronting the Challenge of Quality Diversity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Pugh</surname></persName>
							<email>jpugh@eecs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Computer Science Division</orgName>
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Soros</surname></persName>
							<email>lsoros@eecs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Computer Science Division</orgName>
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Szerlip</surname></persName>
							<email>pszerlip@eecs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Computer Science Division</orgName>
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kenneth</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
							<email>kstanley@eecs.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of EECS</orgName>
								<orgName type="department" key="dep2">Computer Science Division</orgName>
								<orgName type="institution">University of Central Florida Orlando</orgName>
								<address>
									<postCode>32816</postCode>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Confronting the Challenge of Quality Diversity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">374EAAEB545797EE4D1007E01D1B2128</idno>
					<idno type="DOI">10.1145/2739480.2754664</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>I.2.6 [Artificial Intelligence]: Learning</term>
					<term>I.2.6 [Software Engineering]: Metrics-complexity measures, performance measures novelty search</term>
					<term>non-objective search</term>
					<term>quality diversity</term>
					<term>neuroevolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In contrast to the conventional role of evolution in evolutionary computation (EC) as an optimization algorithm, a new class of evolutionary algorithms has emerged in recent years that instead aim to accumulate as diverse a collection of discoveries as possible, yet where each variant in the collection is as fit as it can be. Often applied in both neuroevolution and morphological evolution, these new quality diversity (QD) algorithms are particularly well-suited to evolution's inherent strengths, thereby offering a promising niche for EC within the broader field of machine learning. However, because QD algorithms are so new, until now no comprehensive study has yet attempted to systematically elucidate their relative strengths and weaknesses under different conditions. Taking a first step in this direction, this paper introduces a new benchmark domain designed specifically to compare and contrast QD algorithms. It then shows how the degree of alignment between the measure of quality and the behavior characterization (which is an essential component of all QD algorithms to date) impacts the ultimate performance of different such algorithms. The hope is that this initial study will help to stimulate interest in QD and begin to unify the disparate ideas in the area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In evolutionary computation (EC), evolution has conventionally played the role of an optimization algorithm, wherein the performance of the algorithm is judged by how quickly and how close it comes to reaching an objective <ref type="bibr" target="#b5">[6]</ref>. The idea that evolution can act as an efficient objective opti-Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. mizer is inspired by the feats of evolution in nature, where it appears to have optimized a myriad of complex yet highly functional forms. While some niche areas of EC deviate from this objective-driven paradigm (such as open-ended evolution in artificial life <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>), it has nevertheless broadly encompassed even newer research areas such as generative and developmental systems (GDS <ref type="bibr" target="#b24">[25]</ref>) and neuroevolution <ref type="bibr" target="#b7">[8]</ref>.</p><p>One potential downside of this emphasis on objective optimization is that it has pitted EC as a direct competitor to virtually the entire field of machine learning, which similarly focuses largely on the problem of efficient optimization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref>. While one consequence of this similar emphasis is a view among some in machine learning that EC is an inferior optimization algorithm, even if that view is arguably uninformed, it still leaves evolution in the position of seemingly "just another optimization algorithm."</p><p>Yet evolution in nature is clearly not only about optimization. It is notable not for solving one particular problem, but many problems, and in uncountably diverse ways. In this sense, evolution is not a mere alternative to (for example) backpropagation <ref type="bibr" target="#b21">[22]</ref> or support vector machines <ref type="bibr" target="#b2">[3]</ref>, which are not designed to generate a vast diversity of high-quality solutions. (As explained later, this kind of evolutionary diversity is also not the same conceptually as a multi-objective Pareto front <ref type="bibr" target="#b6">[7]</ref> either.) This incongruity between EC in practice and its natural inspiration suggests a misalignment between the conventional application of EC and its full potential as a genuinely unique paradigm for search that can separate it from much of machine learning.</p><p>Hints of this broader interpretation of evolution within EC have begun to emerge with non-objective algorithms like novelty search (NS) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>, which searches for novelty without an explicit objective. NS has in turn inspired a succession of variant algorithms that elaborate on the idea of a search through novelty alone <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref>]. Yet even while this new generation of evolutionary algorithms begins to deviate from the traditional objective paradigm, such algorithms are still predominantly evaluated based on their ability to reach a particular objective <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. That is, while they may not function internally through the compass of the objective, externally they are often still treated merely as alternatives to objective-driven optimization algorithms.</p><p>However, some researchers have begun to take non-objective search in a different direction. They have instead embraced the potential for evolution to collect quality diversity (QD) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>, which means searching for as many possible variations (with respect to a chosen characterization) as possible, but where each such variation is as good as it can possibly be (with respect to a separate measure of quality). QD is particularly intriguing as an application of EC because it is explicitly not what most algorithms in machine learning are suited to achieving, yet nevertheless well-aligned with the achievements of evolution in nature. In this way QD is an opportunity for EC to shine as an essential option for the kinds of problems that require accumulating a multitude of diverse yet high-quality alternatives.</p><p>In this spirit, the aim of this paper is to begin to introduce a framework for understanding and relating QD algorithms. While the literature analyzing the nuances of algorithms in EC and elsewhere for the purpose of optimization is vast, almost no studies to date investigate the tradeoffs among different approaches to QD. This paper begins such an enterprise by introducing a domain designed explicitly to analyze approaches to QD in the context of neuroevolution, and then compares several leading algorithms in that new framework, including one new variant algorithm called MAP-Elites+Novelty, elaborated from the original MAP-Elites algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17]</ref>. The result is fresh insight into the consequences for different approaches to QD of varying the degree of alignment of the behavior characterization (which is essential to all such algorithms so far) with the notion of quality in the domain: some algorithms perform best when such alignment is high, while others excel without it. Because QD is such a promising new direction for EC with many possible future applications (as discussed in the next section), this initial investigation of algorithms in this area helps to lay the groundwork for an emerging new field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>Early hints of a science of QD within EC first appeared in the 1990s with work on multimodal function optimization (MMFO); a good review is provided by Mahfoud <ref type="bibr" target="#b14">[15]</ref>. MMFO aims to discover as many high-scoring (though not necessarily global) optima within a fitness landscape as possible. While MMFO spawned a number of important diversitypreservation mechanisms (mainly focusing on genetic diversity), a shift more than ten years later towards methods focused on behavioral diversity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> revived interest in the potential for diversity to transform the application of EC. This new wave of algorithms, starting with novelty search <ref type="bibr" target="#b11">[12]</ref>, differed from the older work on MMFO in particular in its focus on discovering the full spectrum of possible behaviors within a particular domain, often represented by evolved neural networks. While much work in this area centered on the possibility that a side effect of seeking behavioral diversity is sometimes the discovery of very high performers, another view that emerged is that the diversity of discoveries itself is a desirable byproduct.</p><p>Unlike in multi-objective optimization <ref type="bibr" target="#b6">[7]</ref>, where the aim is to collect a Pareto front of performance trade-offs, these diversity-collecting algorithms focus instead on sampling the full behavior space as comprehensively as possible without concern for objective performance. In some cases a collection of diverse behaviors on its own offers direct practical value; for example the recent divergent discriminative feature accumulation (DDFA) algorithm <ref type="bibr" target="#b27">[28]</ref> accumulates a vast collection of diverse feature detectors that are then aggregated into a neural network for later training in classification problems.</p><p>However, for many applications a collection of diverse behaviors on its own is not enough because the ideal outcome would be that each variant is not only unique but also as highperforming as it can be. For example, merely discovering that self-ambulating pogo sticks exist in the morphology and behavior space of virtual creatures is not as good as discovering the best possible self-ambulating pogo sticks, in addition to the best possible behaviors of all the other viable morphologies that exist <ref type="bibr" target="#b10">[11]</ref>. Note that this notion of QD is not the same as finding the highest-fitness subset of all conceivable creatures because even the very best pogo stick may have significantly lower fitness than e.g. quadrupeds and bipeds. However, the point is that we still want to find that pogo stick and see how well it can do. The diversity components in QD also still differ from the separate objectives in multi-objective optimization because individual diversity components include no intrinsic ranking -for example, all morphologies in the space of creatures are equally of interest as long as they can walk at all. To empower evolution to collect QD, a new breed of algorithms have appeared in recent years that augment algorithms like novelty search to push each unique variant towards its best possible performance, opening up new applications. For example, Lehman and Stanley <ref type="bibr" target="#b10">[11]</ref> introduced the novelty search with local competition (NSLC) algorithm to evolve a wide diversity of successful ambulating creatures in a single run. Similarly, Szerlip and Stanley <ref type="bibr" target="#b26">[27]</ref> collect a diversity of high-performing behaviors in a Sodarace-inspired two-dimensional creature domain. In an intriguing foreshadowing of future applications, Cully and Mouret <ref type="bibr" target="#b3">[4]</ref> show that QD once collected can be aggregated into a useful behavioral repertoire, in this case a collection of diverse robot motion skills.</p><p>Cully et al. <ref type="bibr" target="#b4">[5]</ref> later introduced an alternative algorithm for collecting QD called MAP-Elites, which showcased yet another intriguing application of behavioral diversity: a collection of ambulation strategies becomes a source of means to alter ambulation in the event of damage. Yet another application of QD is demonstrated by Nguyen et al. <ref type="bibr" target="#b19">[20]</ref>, who generate with MAP-Elites a diverse collection of high-quality spoof images that fool deep networks. This breadth of early applications of such QD algorithms suggests an important new research area with many applications yet to be realized.</p><p>The aim of this paper is to begin to understand the factors that contribute to successfully collecting QD under different conditions. A common component of all modern QD algorithms is the behavior characterization (BC), which formalizes the behavior of a candidate individual in the population and is used to determine its eligibility for selection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>. However, different approaches to QD treat the BC in different ways, and the consequences of these choices are currently poorly understood. For example, while novelty favors individuals with the most novel BC, MAP-Elites discretizes the behavior space into unique bins from which (when occupied) selection is equally probable. This paper focuses in particular on the trade-offs created by such differing treatments and begins to explore the possibility of hybridizations that may combine the benefits of different approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENT</head><p>To effectively tease out the differences between competing quality diversity (QD) algorithms, a test domain is needed with both (1) a concrete measure of quality, and (2) a diversity of legitimate solutions. No standard such benchmark domain yet exists in the area of QD, and thus one of the contributions of this paper is to propose one suited specifically to the concerns of QD.</p><p>The first experiments with novelty search <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> introduced a domain called HardMaze that has since become popular within the literature because of its explicit visual depiction of deception within a fitness landscape. The catch in HardMaze is that for a robot to navigate to the goal point of the maze it is necessary first to navigate away from the goal multiple times along the path. Additionally, there is an easily-reachable cul-de-sac that is close to the goal in terms of Euclidean distance but is actually farther away from the goal in terms of distance along the correct path. Importantly, searching for behavioral novelty alone (i.e. novelty search) in the HardMaze solves the maze much faster than a conventional objective-based search wherein fitness is the Euclidean distance to the goal point at the end of a trial (which, unlike novelty search, is easily drawn into the cul-de-sac). Thus, in HardMaze, Euclidean distance serves as a concrete measure of quality. However, HardMaze only contains one correct path through the maze (i.e. the diversity of legitimate solutions is limited), diminishing its utility for testing QD algorithms that seek many solutions.</p><p>Instead of HardMaze, in this paper, we augment the idea of a maze navigation domain that uses Euclidean distance as a concrete measure of quality to also encompass a diversity of possible solutions to the maze. This new domain, QD-Maze (figure <ref type="figure" target="#fig_1">1</ref>), still contains several deceptive traps (albeit less difficult than those in HardMaze: all algorithms tend to find at least one solution) while also allowing many possible "correct paths" to the goal, thus allowing an explicit diversity of paths to the goal.</p><p>Aside from the change in map configuration, the setup for QD-Maze is identical to HardMaze. A robot agent is given a fixed time to move around the maze before the trial ends and the agent's quality is scored according to the Euclidean distance between its final position and the goal point (the score is maximum if the goal point is reached). Reaching the goal point prematurely ends the trial. Agents are equipped with a set of six rangefinders, five of which are equally spaced across the front-facing 180 degrees and the sixth facing the rear. Additionally, four pie slice sensors (front, right, rear, and left -facing) detect the direction of the goal by activating the pie slice facing the goal with maximal value. In all experiments in this paper, agents are driven by evolved neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Behavior Characterizations (BCs)</head><p>The nature of the "diversity of solutions" available for discovery within the search depends heavily on how behaviors are characterized. In the original HardMaze experiments <ref type="bibr" target="#b11">[12]</ref>, agent behaviors are characterized according to their final position at the end of the trial (EndpointBC). This BC is notable for its strong alignment with the implicit objective of a maze. In other words, the neighborhood of behaviors around any particular trajectory endpoint generally share a similar level of quality, thereby aligning behavior with a notion of quality. Under the pressure of a strongly-aligned BC, searching for novelty alone may often be enough to find not only diversity, but also high-quality solutions to the domain, without even the need to explicitly optimize each behavioral neighborhood independently. To investigate the effect on different approaches to QD of varying the degree Under FullTrajectoryBC, the agent's position is sampled at three points throughout its trial: one-third, two-thirds, and at the end of the trial, and all three (x, y) points are concatenated together to form the six-dimensional BC vector. This BC is less strongly aligned with the notion of quality than EndpointBC because only the final sampled point has a strong correlation to the final score within the domain. Even less aligned again is HalfTrajectoryBC, wherein the agent's position is sampled at three points throughout only the first half of its trial. The second half of the trial leaves sufficient time for an agent to reach the goal from any location within the maze, rendering HalfTrajectoryBC even less aligned with the objective than FullTrajectoryBC. Highlighting this point, in theory, every behavior under HalfTrajecto-ryBC can potentially achieve a maximum quality score (i.e. ultimately solving the maze).</p><p>The final and least-aligned BC, DirectionBC, samples the direction the robot is facing on each tick (north, east, south, or west) and divides the duration of the trial into five equal-length time slices. The BC then consists of five dimensions, each corresponding to the direction that the agent was facing most often during each of the five time slices (north = 0.125, east = 0.375, south = 0.625, west = 0.875). This characterization is almost completely agnostic to the notion of quality because the goal point of the maze can be approached from any direction and the agent is free to wander around the maze and face any direction at any time. Under DirectionBC, every possible behavior in the behavior space can potentially achieve a maximum score on the objective of reaching the goal and there is little to no correlation between quality and behavioral neighborhoods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">QD Algorithms</head><p>A variety of algorithms can collect QD in a given domain, some designed explicitly to do so and some more as a side effect. In this paper, we compare the performance of three existing algorithms (novelty search, novelty search with local competition, and MAP-Elites), the latter two designed explicitly for QD, and a new variant of MAP-Elites (MAP-Elites+Novelty). A conventional fitness-based search (which is not expected to excel at QD) also serves as a baseline for performance.</p><p>To normalize comparisons across the algorithms as much as possible, the SharpNEAT <ref type="bibr" target="#b9">[10]</ref> distribution of the NEAT algorithm <ref type="bibr" target="#b23">[24]</ref> serves as the underlying neuroevolution platform for all of them. Therefore, the neural networks are able to evolve increasing complexity, a central feature of NEAT. However, except in the pure fitness-driven baseline runs, to minimize potentially confounding factors, the conventional genetic diversity component of NEAT (also called speciation) is excluded. That way, the diversity facet of QD is exclusively focused on behavior, which is the focus of the majority of recent innovation in non-objective search algorithms and has exhibited significantly more impact on performance in such algorithms than genetic diversity <ref type="bibr" target="#b18">[19]</ref>. Furthermore, because of preliminary experimental evidence that generational evolution can lead to instability or "jumping around the behavior space" in non-objective search populations, all algorithms are run in steady state mode, which means that a subset of the population (in this case 32 individuals evaluated in parallel) is replaced at a time instead of a whole generation (which is similar to the rtNEAT <ref type="bibr" target="#b25">[26]</ref> variant of NEAT). In total five algorithms are compared:</p><p>The classic novelty search (NS) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref> algorithm only searches for behavioral diversity with no notion of quality. It rewards behaviors that are maximally distant from previously discovered behaviors. However, if the BC is aligned with a notion of quality, then it is possible that NS without any special augmentation can itself collect QD effectively, a possibility thereby investigated in this experiment.</p><p>NS is augmented to explicitly seek QD in the novelty search plus local competition (NSLC) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27]</ref> algorithm. In this variant, a multi-objective formulation implemented through NSGA-II <ref type="bibr" target="#b6">[7]</ref> casts classic novelty as one objective and local competition (LC), which means a quality rank relative only to those with a similar BC, as another. The LC component encourages unique behaviors to improve in quality within their local behavioral neighborhoods, thereby aligning the search explicitly with the notion of QD. The LC approach contrasts with the idea of combining novelty with a global competition objective (NSGC), which has been previously shown less effective than NSLC for discovering a diversity of solutions within a search space <ref type="bibr" target="#b10">[11]</ref> because global competition explicitly prunes out diversity, and is therefore not considered in this paper.</p><p>Unlike NS and NSLC, the recent MAP-Elites (ME) algorithm <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref> divides the behavior space of the BC into discrete bins. Each bin then remembers the fittest (elite) genome ever to occupy it, and only one genome occupies any given bin at one time. Thus the elites within each bin capture the notion of quality and the whole set of bins capture the notion of diversity. In the classic formulation of ME, selection is very simple: the chance of producing an offspring is equal for each filled bin. Thus search effort tends to be spread uniformly across the known behavior space (rather than explicitly biased by novelty). One technical limitation of ME is that the BC must be constrained to a low number of dimensions because the total number of bins grows exponentially with BC dimensionality; for this reason and to keep the comparisons fair, none of the BCs in this paper includes more than six dimensions.</p><p>While the simplicity of selection in ME is appealing, its effect is that evolution is more likely to concentrate resources in regions of the behavior space already filled (i.e. because that is where most of the bins are occupied). This approach proved effective in ME applications so far, but it is easy to augment ME with pressure to concentrate effort on more novel regions by adding novelty pressure. In this MAP-Elites+Novelty (ME-Nov) variant tested for the first time here, the probability of selecting an occupied bin for reproduction is proportional to its novelty, creating a stronger pressure towards diversity than in the original ME. However, once all the bins are filled in ME-Nov, it collapses back to an approximation of regular ME. The question is whether the added initial pressure towards novelty might enhance ME's drive towards QD. Also, while ME has effectively filled most bins in experiments published so far <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>, in future domains in which some bins require very advanced solutions and therefore are unlikely all to be filled, the additional novelty pressure in ME-Nov could prove instrumental in covering as much of the space as possible.</p><p>Finally, a regular fitness-based search (fitness) (implemented as NEAT <ref type="bibr" target="#b23">[24]</ref>) simply searches for objectively superior performance (though with NEAT's conventional genetic speciation fitness-sharing mechanism to give it at least some hope of maintaining diversity). This variant, which is susceptible to the trap of deception <ref type="bibr" target="#b13">[14]</ref> and has no behavioral diversity mechanism, helps to highlight the need for specialized approaches to the problem of QD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MAP-Elites as a QD assessment mechanism</head><p>To compare the ability of competing QD algorithms to collect quality diversity, a quantitative measure of quality diversity is needed. Interestingly, the MAP-Elites style of discretized behavior space can be collected for all variant algorithms, even those that are not MAP-Elites. As long as there are not too many dimensions in the BC (as is true in BCs in this paper), the best behavior found for every bin in the behavior space (regardless of through which method it was found) can simply be stored for later analysis.</p><p>For this purpose, for each BC, the behavior space is discretized into an appropriately small number of bins (i.e. small enough that MAP-Elites itself can be run efficiently and with a breeding pool of the same order of magnitude as the size of the breeding population in the NS, NSLC, and fitness-based algorithmic treatments): 900 bins for EndpointBC, 729 for FullTrajectoryBC and HalfTrajectoryBC, and 1,024 for Di-rectionBC. For each algorithmic treatment, all individuals are checked for eligibility in this MAP-Elites-style grid as they are evaluated. The performance (QD score) of each algorithm at any given point during the run is then measured as the total fitness across all filled grid bins within the QD collection grid (where higher fitness or quality means a final robot position closer to the goal). Throughout a run, algorithms therefore improve their QD score by either (1) discovering higher quality solutions for existing bins (increasing quality) or (2) discovering more bins (increasing diversity). To achieve the highest QD scores (which is possible in this domain), an algorithm must therefore excel with respect to both quality and diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental setup</head><p>Each of the methods (NS, NSLC, ME, ME-Nov, fitness) was combined with each of the behavior characterizations (EndpointBC, FullTrajectoryBC, HalfTrajectoryBC, Direc-tionBC) to comprehensively test the effects of both factors on QD. Except for the DirectionBC variants, each combination was evaluated over 20 runs capped at 250,000 evaluations. Because the DirectionBC variants converge to stable scores significantly more slowly (collecting QD across such a poorly-aligned BC is harder), they were capped at 750,000 evaluations. The population size was 500 for NS, NSLC, and fitness; NS and NSLC also keep an associated novelty archive of maximum size 2,500, after which the least novel is removed when a new entry is added. For MAP-Elites, as is standard the population at any given time was the size of the number of occupied bins. To enable parallel evaluation, steady state evolution is implemented in batches of 32 offspring at one time. Following the settings in the original novelty search experiments <ref type="bibr" target="#b13">[14]</ref>, the probabilities for mutating connections was 60%, for adding connections was 10%, and for adding neurons was 0.5%. Evolved networks are constrained to be strictly feedforward. All other settings follow standard SharpNEAT 1.0 defaults <ref type="bibr" target="#b9">[10]</ref>. Source code for the entire experimental setup is available at http://eplex.cs.ucf.edu/uncategorised/software#QD Performance for each method and BC combination was measured according to the total fitness across all filled grid bins (Section 3.3). Results were averaged across twenty runs for each of the 20 method-BC combinations (a total of 400 separate runs were performed). Additionally, pairwise comparisons were performed after every 160 evaluations to determine when exactly there are significant differences between methods. Tukey's test (with p &lt; 0.05) was used to establish significance instead of the Student's t-test to account for the statistical problem of multiple pairwise comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>Figures 2 through 5 show for each BC the total quality diversity (QD) discovered so far by each method over the course of 20 averaged runs for each method. Higher QD is better. Fitness runs, which are included only as a baseline, are unsurprisingly significantly below all other runs. Among the other four methods, a yellow bar at the top of each graph highlights the period during which significant differences are observed between the best and worst performing methods among those four. Note that in terms of both QD and significance testing, numerical comparisons are only meaningful between methods within the same BC because the total possible QD score depends on the BC. The main result is that the ranking of methods depends on the alignment of the BC with the notion of quality, suggesting that choosing a QD method for a particular problem can potentially become a principled decision in the future, though some methods are more generally robust across different BCs than others.</p><p>With the EndpointBC (figure <ref type="figure" target="#fig_3">2</ref>), which is highly aligned with the notion of quality (i.e. closeness to the endpoint), the novelty search-based methods (NS and NSLC) significantly outperform all other methods for most of the run, with NS ending significantly (though only slightly) above NSLC. ME-Nov also significantly outperforms original ME, though is still significantly below NS and NSLC. (It is important to note that the differences between the methods  In this performance comparison and others in this paper, the average QD (taken over 20 runs) for each variant method is shown across a run. The yellow strip at the top indicates the period during which there is a significant difference at least between the top and bottom method (exlcuding fitness, which is always significantly worse than all other methods). The method labels are color coded to match with their respective curves and are shown from top to bottom in the their rank order during the period of significance. Note that because of the large QD scale, sometimes significant differences exist even when not visually apparent. For EndpointBC, NS performs best, followed by NSLC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality Diversity</head><p>as they converge near the end of the run can be difficult to perceive in the figures, but the QD scale is actually vast, so significant difference actually do persist in some cases where it is difficult to perceive visually.) The fact that novelty alone (NS) does so well highlights the fact that when the BC is aligned with fitness, additional sophisticated machinery designed to simultaneously reward quality and diversity is potentially unnecessary (perhaps depending on the difficulty of the domain), and in such situations novelty alone becomes a powerful force for QD.</p><p>However, as the alignment of the BC with quality reduces, the story changes. In the FullTrajectoryBC (figure <ref type="figure" target="#fig_5">3</ref>), which is still fairly aligned with the notion of quality (i.e. because trajectories that lead to the goal are high quality), NS, NSLC, and ME-Nov are tied (i.e. not significantly different) throughout most of the run. For this BC, only ME performs significantly below the other methods. Thus NS retains its utility as a top choice, but no longer holds an advantage over NSLC or ME-Nov. ME is not well suited to BCs with high alignment because it is agnostic with respect to novelty, leading to a relative delay compared to novelty-driven approaches (including ME-Nov) in exploiting promising areas of behavior space.</p><p>The HalfTrajectoryBC (figure <ref type="figure" target="#fig_6">4</ref>) aligns with quality even less, which leads to an interesting phenomenon seen only with this BC: After a very brief period at the start of the run, all methods effectively perform the same (although ME is still a bit below, the difference is not significant). In effect, HalfTrajectoryBC hits just the right level of misalignment that the advantages and disadvantages of respective methods become a wash. Surprisingly, even NS alone remains competitive, despite its lack of an explicit quality component.   rection only, aligns least with quality because it is largely detached from trajectory. For example, agents can arbitrarily spin around whenever they want without impacting their trajectory. Note that such low alignment is not unprecedented in serious applications. For example, an algorithm might search for diversity in terms of the height and mass of virtual creatures, which is hardly predictive of their quality as walkers <ref type="bibr" target="#b10">[11]</ref>. The results with DirectionBC suggest that such a complete loss of alignment can significantly upend the utility of NS as a QD mechanism. In this scenario, NS drops to the bottom of the ranking, performing significantly below other approaches (each of which have quality-seeking components). Furthermore, while NSLC exceeds NS significantly (showing that the LC component of NSLC can indeed provide an advantage in QD), both ME and ME-Nov significantly outperform NSLC. In fact, ME alone effectively ties with ME-Nov, suggesting that in cases of high misalignment, the advantage of the novelty pressure is eclipsed by the advantage of a strong elitist mechanism that pushes towards higher quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visualizing Behavior Spaces</head><p>Recall that for all variant search algorithms, the highestperforming genomes are collected in a discretized behavior space "grid" (inspired by the MAP-Elites grid). Collecting elite behaviors in this way enables a convenient visualization of each algorithm's ability to collect QD within the domain. The visualization technique, proposed by Cully et al. <ref type="bibr" target="#b4">[5]</ref>, uses two dimensions to display an entire behavior space grid by nesting two-dimensional grids within other two-dimensional grids, each two-dimensional grid representing two dimensions of the behavior space. As an example, figures 6 through 8 depict the final state of the QD collection grid at the end of one typical run of Fitness, ME, and NS in the FullTrajecto-ryBC. These visualizations reveal that: (1) Fitness (figure <ref type="figure">6</ref>) and MAP-Elites (figure <ref type="figure" target="#fig_9">7</ref>) exhibit a diminished ability to fill the grid for this BC (corresponding to a diminished pressure towards diversity) while the highest performing algorithm (on FullTrajectoryBC), NS (figure <ref type="figure" target="#fig_10">8</ref>), is able to fill the grid almost entirely after 250,000 evaluations. The visualizations also expose (2) the relatively strong alignment of the behavior space with the fitness measure through the smooth gradient in the outer two dimensions from low fitness at the top of the grid towards high fitness at the bottom-middle of the grid.</p><p>This paper is also accompanied by a website where sample interactive behavior space visualizations (including the ability to browse through the discovered behaviors within each bin) are available for all 20 method-BC combinations (including the three shown here): http://eplex.cs.ucf.edu/QD/GECCO-15/compare.html</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>The main insight to emerge from the experiment concerns the two key forces operating in QD algorithms: the push for novelty and the push for quality. If the BC, which supplies the main push for novelty, is sufficiently aligned with a notion of quality, then novelty-based approaches are most effectively empowered. In contrast, if the BC is orthogonal to a notion of quality, a strong push for quality (as in the elitism of ME) becomes the more instrumental factor in QD performance. The apparent ability under some BCs for NS alone to succeed, even without any explicit push for quality, and ME alone in other BCs to succeed without any explicit Figure <ref type="figure">6</ref>: Fitness in the FullTrajectoryBC. In this example grid, the six-dimensional behavior space (FullTra-jectoryBC) (discretized into three bins per dimension for a total of 729 bins) is visually depicted as a series of nested two-dimensional grids (each of which are 3 Ã— 3). The color of each grid box corresponds to the quality of the solution found by the search algorithm after 250,000 evaluations: yellow corresponds to low quality, dark red to high quality, and white to unfilled bins. Fitness finds very few of the possible behaviors for this BC. push for novelty, highlights just how profoundly this apparent principle applies. Yet it also appears that augmenting these most simple methods with a component to cover what they lack, e.g. augmenting NS with LC and ME with novelty, carries generally little risk. In fact, in some cases it improves their results, especially when they are being applied in their respectively less ideal alignment circumstances, hinting that aiming for "best of both worlds" is not unrealistic and may even be attainable for future QD algorithms yet unrealized.</p><p>However, the results in this initial study should be qualified by the observation that the QD-Maze domain is relatively easy; all methods except for fitness eventually find diverse solutions. Because more ambitious applications of QD in the future are likely significantly more challenging (imagine e.g. much more challenging parallel mazes), a natural question is whether the results here will extend to future experiments. While of course further empirical research is necessary and this study is only a first step, there is still good reason to believe that the results reported here are at least somewhat general: First, even in much harder domains, as the length of runs approaches infinity, all methods are likely eventually to find most QD. The experiment here is simply shorter because the domain is less difficult. Yet the stratification of different methods observed during the course of these short runs can conceivably emerge similarly in more challenging domains, though simply over longer timescales. Second, the fact that the ranking of methods in QD-Mazes matches well our expectations for each method (i.e. that those more reliant on a separate measure of quality do better when the BC is poorly aligned with fitness) further supports the credibility of the results.</p><p>Of course, there are some other considerations outside QD  Of the five compared algorithms, NS performs the best under the FullTrajectoryBC (featuring relatively high alignment between the BC and the objective) because it focuses exclusively on pursuing diversity. This conclusion is supported in the QD collection grid by almost all bins being filled (i.e. non-white).</p><p>performance alone that may arise in specific domains. For example, because it divides behavior space into discrete bins, ME (and ME-Nov) becomes increasingly difficult to apply to higher-dimensional BCs, an issue not present for NS or NSLC. Yet beyond its use as a method, the ME grid also turns out an excellent mechanism for collecting and measuring QD results, even for methods that are not ME. We anticipate that the results of future QD algorithms even unrelated to ME will ultimately often be visualized and quantified by placing them within in a classic ME grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>The aim of this paper was to help to unify the emerging quality diversity (QD) research area by beginning to introduce benchmarks and principles for analyzing and contrasting methods in the area. For this purpose a new domain with many parallel solutions called the QD-Maze was introduced, and several QD methods (including a new variant called MAP-Elites+Novelty) were compared under different behavior characterizations in QD-Maze. The results begin to establish initial principles for understanding QD, namely that the alignment of the behavior characterization with the notion of quality significantly influences which QD methods will be most effective in a particular setup. This insight gives hope that an even broader understanding of QD can be reached in the future, thereby helping to expand the reach and impact of evolutionary computation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>GECCO ' 15 ,</head><label>15</label><figDesc>July 11-15, 2015, Madrid, Spain c 2015 ACM. ISBN 978-1-4503-3472-3/15/07. . . $15.00 DOI: http://dx.doi.org/10.1145/2739480.2754664</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Quality diversity maze (QD-Maze). Individuals start at the point S and the highest quality solutions navigate to point G. While the maze is deceptive enough to challenge objective-based algorithms, it also contains a variety of solutions and thus serves as a benchmark for combining quality with diversity.</figDesc><graphic coords="3,335.94,53.80,200.84,172.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: EndpointBC (very high alignment).In this performance comparison and others in this paper, the average QD (taken over 20 runs) for each variant method is shown across a run. The yellow strip at the top indicates the period during which there is a significant difference at least between the top and bottom method (exlcuding fitness, which is always significantly worse than all other methods). The method labels are color coded to match with their respective curves and are shown from top to bottom in the their rank order during the period of significance. Note that because of the large QD scale, sometimes significant differences exist even when not visually apparent. For EndpointBC, NS performs best, followed by NSLC.</figDesc><graphic coords="5,338.45,53.80,210.86,138.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Finally, DirectionBC (figure5), which tracks facing di-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FullTrajectoryBC (high alignment). For this BC, which is slightly less aligned with quality than EndpointBC, NS, NSLC, and ME-Nov are effectively tied, with ME significantly behind.</figDesc><graphic coords="6,75.94,53.80,210.37,138.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: HalfTrajectoryBC (modest alignment). All the methods except fitness are tied for the vast majority of the run for this BC, which is even less aligned than FullTra-jectoryBC.</figDesc><graphic coords="6,75.44,284.27,210.86,138.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: DirectionBC (low alignment). With an almost complete lack of alignment in this BC, ME and ME-Nov tie for first place, and NS trails far behind.</figDesc><graphic coords="6,336.43,53.80,212.89,140.18" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: MAP-Elites in the FullTrajectoryBC. Compared to Fitness (figure6), ME discovers far more QD under FullTrajectoryBC. Some bins remain unfilled (white), corresponding to the lack of pressure towards diversity within the ME algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Novelty Search in the FullTrajectoryBC.Of the five compared algorithms, NS performs the best under the FullTrajectoryBC (featuring relatively high alignment between the BC and the objective) because it focuses exclusively on pursuing diversity. This conclusion is supported in the QD collection grid by almost all bins being filled (i.e. non-white).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the National Science Foundation under grant no. IIS-1421925. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving and still passing the alife test: Component-normalised activity statistics classify evolution in geb as unbounded</title>
		<author>
			<persName><forename type="first">A</forename><surname>Channon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="173" to="181" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Behavioral repertoire learning in robotics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 15th Annual Conf. on Genetic and Evolutionary Computation (GECCO &apos;13)</title>
		<meeting>of the 15th Annual Conf. on Genetic and Evolutionary Computation (GECCO &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cully</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarapore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
		<idno>abs/1407.3501</idno>
		<title level="m">Robots that can adapt like natural animals</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Evolutionary Computation: A Unified Perspective</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: NSGA-II</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neuroevolution: From architectures to learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>DÃ¼rr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mattiussi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="47" to="62" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generic behaviour similarity measures for evolutionary swarm robotics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Annual Conference on Genetic and Evolutionary Computation (GECCO &apos;13)</title>
		<meeting>the Fifteenth Annual Conference on Genetic and Evolutionary Computation (GECCO &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="199" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Green</surname></persName>
		</author>
		<ptr target="http://sharpneat.sourceforge.net/" />
		<title level="m">SharpNEAT homepage, 2003-2006</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evolving a diversity of virtual creatures through novelty search and local competition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Thirteenth Annual Conf. on Genetic and Evolutionary Computation (GECCO &apos;11)</title>
		<meeting>of the Thirteenth Annual Conf. on Genetic and Evolutionary Computation (GECCO &apos;11)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting open-endedness to solve problems through the search for novelty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 11th Intl. Conf. on Artificial Life (Alife XI)</title>
		<meeting>of the 11th Intl. Conf. on Artificial Life (Alife XI)<address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revising the evolutionary computation abstraction: Minimal criteria novelty search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Annual Conf. on Genetic and Evolutionary Computation (GECCO &apos;10)</title>
		<meeting>of the 12th Annual Conf. on Genetic and Evolutionary Computation (GECCO &apos;10)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="103" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Abandoning objectives: Evolution through the search for novelty alone</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="223" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Niching Methods for Genetic Algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mahfoud</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-05">May 1995</date>
			<pubPlace>Urbana, IL</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Illuminating search spaces by mapping elites</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming the bootstrap problem in evolutionary robotics using behavioral diversity</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doncieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Congress on Evolutionary Computation (CEC-2009)</title>
		<meeting>the IEEE Congress on Evolutionary Computation (CEC-2009)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Encouraging behavioral diversity in evolutionary robotics: An empirical study</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Mouret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Doncieux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="133" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno>abs/1412.1897</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Avida: A software platform for research in computational evolutionary biology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ofria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Wilke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="229" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Division blocks and the open-ended evolution of development, form, and behavior</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spector</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Annual Conference on Genetic and Evolutionary Computation (GECCO &apos;07)</title>
		<meeting>the Ninth Annual Conference on Genetic and Evolutionary Computation (GECCO &apos;07)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="316" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A taxonomy for artificial embryogeny</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Life</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="93" to="130" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Real-time neuroevolution in the NERO video game</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation Special Issue on Evolutionary Computation and Games</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Indirectly encoded sodarace for artificial life</title>
		<author>
			<persName><forename type="first">P</forename><surname>Szerlip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Artificial Life (ECAL-2013)</title>
		<meeting>the European Conference on Artificial Life (ECAL-2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="218" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning through divergent discriminative feature accumulation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Szerlip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Pugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-2015)</title>
		<meeting>of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-2015)<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
