<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving reservoirs using intrinsic plasticity</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2008-01-26">26 January 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Benjamin</forename><surname>Schrauwen</surname></persName>
							<email>benjamin.schrauwen@ugent.be</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Systems</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Sint-Pietersnieuwstraat 41</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marion</forename><surname>Wardermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Systems</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Sint-Pietersnieuwstraat 41</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Verstraeten</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Systems</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Sint-Pietersnieuwstraat 41</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jochen</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Neuroinformatics Group</orgName>
								<orgName type="institution">Bielefeld University</orgName>
								<address>
									<addrLine>Universita Â¨tsstraX e 25</addrLine>
									<postCode>33615</postCode>
									<settlement>Bielefeld</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dirk</forename><surname>Stroobandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Information Systems</orgName>
								<orgName type="institution">Ghent University</orgName>
								<address>
									<addrLine>Sint-Pietersnieuwstraat 41</addrLine>
									<postCode>9000</postCode>
									<settlement>Gent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving reservoirs using intrinsic plasticity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-01-26">26 January 2008</date>
						</imprint>
					</monogr>
					<idno type="MD5">653D98B4D1F9E9926657D5C8562EF1CE</idno>
					<idno type="DOI">10.1016/j.neucom.2007.12.020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Reservoir computing</term>
					<term>Intrinsic plasticity</term>
					<term>Information maximization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The benefits of using intrinsic plasticity (IP), an unsupervised, local, biologically inspired adaptation rule that tunes the probability density of a neuron's output towards an exponential distribution-thereby realizing an information maximization-have already been demonstrated. In this work, we extend the ideas of this adaptation method to a more commonly used non-linearity and a Gaussian output distribution. After deriving the learning rules, we show the effects of the bounded output of the transfer function on the moments of the actual output distribution. This allows us to show that the rule converges to the expected distributions, even in random recurrent networks. The IP rule is evaluated in a reservoir computing setting, which is a temporal processing technique which uses random, untrained recurrent networks as excitable media, where the network's state is fed to a linear regressor used to calculate the desired output. We present an experimental comparison of the different IP rules on three benchmark tasks with different characteristics. Furthermore, we show that this unsupervised reservoir adaptation is able to adapt networks with very constrained topologies, such as a 1D lattice which generally shows quite unsuitable dynamic behavior, to a reservoir that can be used to solve complex tasks. We clearly demonstrate that IP is able to make reservoir computing more robust: the internal dynamics can autonomously tune themselvesirrespective of initial weights or input scaling-to the dynamic regime which is optimal for a given task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In machine learning, feed-forward structures like artificial neural networks or kernel methods have been studied extensively for the processing of non-temporal input. These architectures are well understood in many respects because of their feed-forward and non-dynamic nature. Many realworld tasks, however, are temporal in nature, such as prediction (weather, dynamic systems, financial data), system control or identification, adaptive filtering, noise reduction, gait generation, planning or movement control in robotics, and vision and speech (recognition, processing, production).</p><p>Feed-forward approaches usually incorporate the temporal domain by means of extending the input to incorporate a finite time window. This technique is motivated by the famous Takens theorem <ref type="bibr" target="#b38">[39]</ref>, which states that the (hidden) state of the dynamical system can be reconstructed using an adequate delayed embedding. This explicit embedding converts the temporal problem into a spatial one. Disadvantages of this approach are the artificially introduced time horizon, the need for many parameters when a long delay is introduced, and the artificial way to represent time in the spatial domain.</p><p>A possible solution to this problem is adding recurrent connections to the feed-forward architecture and to transfer the respective learning algorithms into the spatiotemporal domain. Then the goal is to train all the weights in a fully (or sparsely) connected recurrent neural network based on a standard error minimization approach. This is the purpose of the extension of back-propagation to back-propagation-through-time for recurrent networks by Werbos <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> (which was later rediscovered in <ref type="bibr" target="#b26">[27]</ref>).</p><p>Theoretically, RNNs are very powerful tools for solving complex temporal machine learning tasks. Nonetheless, the application of these rules to real-world problems is not always feasible due to the high computational training costs and slow convergence <ref type="bibr" target="#b8">[9]</ref> and while it is in principle possible to achieve state-of-the-art performance, this is often suitable only for experts in the field <ref type="bibr" target="#b24">[25]</ref>. Another significant and principal problem for recurrent learning algorithms is the so-called fading gradient, which means that the error gradient vanishes or gets distorted when it is propagated many time steps backward, such that only short time series examples are usable for training. One possible solution to this is a specially constructed long short term memory (LSTM) architecture <ref type="bibr" target="#b29">[30]</ref>, which nonetheless does not always outperform time delayed neural networks.</p><p>A way to train recurrent structures while reducing the computational burden and circumventing the fading gradient problem has independently been discovered by Jaeger <ref type="bibr" target="#b10">[11]</ref> as the echo state network (ESN) and Maass <ref type="bibr" target="#b19">[20]</ref> as the liquid state machine (LSM). The idea is to train only parts of the network using a simple classification/regression technique and leave large parts of the network fixed. We refer to such an approach as reservoir computing (RC). Recent advances in the overall field have also been published in <ref type="bibr" target="#b32">[33]</ref>.</p><p>The ESN consists of a randomly connected recurrent network of analog neurons-the reservoir-that is driven by a (one-or multi-dimensional) temporal input signal. On the output level, the activations of the entire network are treated as high-dimensional spatio-temporal input features for a linear classification/regression algorithm-the linear readout. The ESN was introduced as an improved way to use the computational power of RNNs without training of the internal weights. From a formal viewpoint, the reservoir acts as a complex non-linear dynamic filter that transforms the input signals using a high-dimensional temporal mapping, not unlike the operation of an explicit, temporal kernel function. It is even possible to solve several classification tasks on a single input signal simultaneously by adding multiple readouts to one reservoir.</p><p>The LSM by Maass <ref type="bibr" target="#b19">[20]</ref> was originally presented as a general framework to perform real-time computation on temporal signals, but most of its realizations are based on an abstract cortical micro-column model. Here a 3D structured locally connected network of spiking neurons is randomly created using biologically inspired parameters and excited by external input spike trains. The responses from all neurons are projected to the next cortical layer where the actual training is performed. This projection is usually modeled using a simple linear regression function.</p><p>A different line of research by Steil <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29]</ref> showed that the online version of a current state-of-the-art learning rule for RNNs (Atiya-Parlos recurrent learning <ref type="bibr" target="#b1">[2]</ref>), which is derived as a classical error minimization method, leads to similar weight adaptation dynamics as were proposed by Jaeger and Maass: the output weights are trained and adapt quickly while internal weights are changing on a much slower time-scale and in a highly coupled way. These changes can even be ignored if the weights are well scaled initially. This shows that RC is both an attractive idea because of the simplicity of its training scheme and theoretically motivated from an approximation to an error minimizing learning algorithm.</p><p>Several successful applications of RC to both synthetic data and real world engineering applications have been reported in the literature. The former include dynamic pattern classification <ref type="bibr" target="#b11">[12]</ref>, autonomous sine generation <ref type="bibr" target="#b10">[11]</ref> or the computation of highly non-linear functions on the instantaneous rates of spike trains <ref type="bibr" target="#b18">[19]</ref>. In robotics, LSMs have been used to control a simulated robot arm <ref type="bibr" target="#b15">[16]</ref>, to model an existing robot controller <ref type="bibr" target="#b3">[4]</ref>, to perform object tracking and motion prediction <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, event detection <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b9">10]</ref>, or several applications in the Robocup competitions (mostly motor control) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28]</ref>. ESNs have been used in the context of reinforcement learning <ref type="bibr" target="#b5">[6]</ref>. Also, applications in the field of digital signal processing (DSP) have been quite successful, such as speech recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b31">32]</ref> or noise modeling <ref type="bibr" target="#b14">[15]</ref>. In <ref type="bibr" target="#b25">[26]</ref>, an application in brain-machine interfacing is presented. And finally, the use of reservoirs for chaotic time series generation and prediction have been reported in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35]</ref>. In many areas such as chaotic time series prediction and isolated digit recognition, RC techniques already outperform stateof-the-art approaches. A striking case is demonstrated in <ref type="bibr" target="#b14">[15]</ref> where it is possible to predict the Mackey-Glass chaotic time series several orders of a magnitude farther in time than with classical techniques.</p><p>Various ways of constructing reservoir topologies and weight matrices have been described (see <ref type="bibr" target="#b41">[42]</ref> for a brief overview), but for ESNs, one usually creates a network with a certain sparsity and assigns random weights drawn from a normal or uniform distribution or from a discrete set. Next, the weight matrix is globally scaled to set the spectral radius (largest absolute eigenvalue) to a certain value. While Jaeger proposes an optimal spectral radius of around 0:9 for certain small-scale problems, this is not generally applicable <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b43">44]</ref>. Optimization of a reservoir for applications is typically based on experience and heuristics and partly on a brute-force search of the parameter space. Moreover, the variance of the performance across different reservoirs with the same spectral radius is still quite substantial, which is clearly undesirable. Obviously, a computationally simple way to adapt the reservoirs to the task at hand without requiring a full parameter sweep or hand tuning based on experience would be welcome.</p><p>In this line, it was recently shown <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b37">38]</ref> that the performance of off-line and on-line learning for ESN networks with fermi transfer functions can be improved by using an unsupervised and local adaptation rule based on information maximization, called intrinsic plasticity (IP). This rule was first introduced in <ref type="bibr" target="#b39">[40]</ref> as a formal model of processes known in neurobiology as homeostatic plasticity.</p><p>Since this kind of plasticity is quasi omnipresent in biological neurons it is natural to investigate its formal counterpart in combination with standard artificial network learning algorithms. In this context, Triesch has also shown that in combination with Hebbian learning IP can drastically change the behavior of Hebbian networks toward finding heavy-tail directions in arbitrary input distributions <ref type="bibr" target="#b40">[41]</ref>. The interplay between these two unsupervised adaptation rules was further investigated in <ref type="bibr" target="#b16">[17]</ref>. There, it was shown that the combination of IP and STDP enhances the robustness of the network against small perturbations and aids the networks in discovering temporal patterns present in the input signals. The results from that work suggest that a fundamental underlying link between IP, STDP, and unsupervised information maximization exists, and that these rules operating strictly on the local neuronlevel succeed in steering the dynamics of the entire network toward a computationally desirable regime.</p><p>In this contribution, we further investigate different versions of IP learning extending and generalizing the idea. Whereas previous work on IP has focused on the fermi transfer function and an exponential target distribution, we derive the corresponding formalism here for the hyperbolic tangent transfer function and a Gaussian output distribution. We also derive a formalism to compute the target means and variances the IP learning rule is supposed to converge to. Simulations show that in practice these targets are reached surprisingly well despite the interference between neurons introduced by the recurrency in the network.</p><p>The simple, local IP rule effectively makes reservoirs significantly more robust: it empowers the reservoirs with the ability to autonomously and robustly adapt their internal dynamics, irrespective of initial weight setting, input scaling or topology, to a dynamic regime which is suited for the given task. The rule is purely input driven, and adapts the reservoir in an unsupervised way.</p><p>This work is organized as follows. We first derive the IP learning rule and show how it performs in practice in Section 2. Next, in Section 3, we show the effect of using the different IP rules to RC on three benchmark applications of different types. Section 4 demonstrates that IP can also help to use very sparse, structured reservoir topologies such as 1D lattices. We conclude and present future work in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Derivation of generalized IP</head><p>IP as introduced in [40] models a well-known phenomenon observed in a variety of biological neuron models called homeostatic plasticity: biological neurons tend to autonomously adapt to a fixed average firing rate for physiological reasons. In <ref type="bibr" target="#b39">[40]</ref> it was shown that such a mechanism, when the neurons have an exponential output firing rate distribution, are effectively maximizing information. While the biological mechanisms are not yet known precisely, it is very plausible that every single neuron tries to balance the conflicting requirements of maximizing its information transmission while at the same time obeying constraints on its energy expenditure. IP formalizes these hypotheses by assuming the following three principles:</p><p>(1) information maximization: the output of the neuron should contain as much information on the input as possible. This is achieved by maximizing the entropy of the output firing rates; (2) constraints on the output distributions: these are first of all the limited output range of the neuron <ref type="bibr" target="#b0">[1]</ref>, but can be also the limited energy available <ref type="bibr" target="#b2">[3]</ref>; (3) adapt the neurons intrinsic parameters: a biological neuron is only able to adjust its internal excitability, and not the individual synapses (see <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b6">[7]</ref> for reference of this common effect in biological neurons).</p><p>The original work of Triesch assumes a constraint on the mean of the output distribution and derives a gradient descent learning rule from these principles for fermi nonlinearities. In this work, we extend the formalism and study the effects on two types of transfer functions:</p><p>(1) the fermi function:</p><formula xml:id="formula_0">y Â¼ f Ã°xÃ Â¼ 1 1 Ã¾ expÃ°ÃxÃ</formula><p>(2) and the hyperbolic tangent:</p><formula xml:id="formula_1">y Â¼ f Ã°xÃ Â¼ tanhÃ°xÃ Â¼ expÃ°xÃ Ã expÃ°ÃxÃ expÃ°xÃ Ã¾ expÃ°ÃxÃ .</formula><p>The output of the fermi non-linearity is in the range Â½0; 1, while the output of the tanh function is in the range Â½Ã1; 1. We define intrinsic parameters for these non-linearities by adding a gain a and a bias b:</p><formula xml:id="formula_2">f gen Ã°xÃ Â¼ f Ã°ax Ã¾ bÃ.</formula><p>Following Triesch's original line of argument the information maximization principle corresponds to a maximization of the entropy of the output distribution of each neuron. In combination with the second principle this leads to maximum entropy (ME) distributions with certain fixed moments. The ME distribution for a given mean (first moment) and support in the interval Â½0; 1 is the exponential distribution. Likewise, the ME distribution for a given mean and standard deviation with support in Â½Ã1; 1 is the Gaussian. Therefore, to target ME output distribution in formal neurons, we use for the first moment case fermi neurons having a positive output range Â½0; 1 and for the second order case tanh neurons with output range Â½Ã1; 1. We express the amount the empirical output distribution differs from the desired ME distribution using the Kullback-Leibler divergence:</p><formula xml:id="formula_3">D KL Ã° p; pÃ Â¼ Z pÃ°yÃ log pÃ°yÃ pÃ°yÃ dy,</formula><p>where pÃ°yÃ is the actual probability density of the neuron's output activity and pÃ°yÃ the desired probability density function. The learning rule for Fermi neurons and targeted exponential output distribution</p><formula xml:id="formula_4">p exp Ã°yÃ Â¼ 1 m exp Ã y m</formula><p>was derived in <ref type="bibr" target="#b39">[40]</ref> and is given here for the sake of completeness. The neuron's gain a and bias b are updated at each time step with the following factors:</p><formula xml:id="formula_5">Db Â¼ Z 1 Ã 2 Ã¾ 1 m y Ã¾ y 2 m , Da Â¼ Z a Ã¾ Dbx.</formula><p>To extend the formalism to include the second moment, we need to minimize the Kullback-Leibler divergence to a desired Gaussian distribution</p><formula xml:id="formula_6">p norm Ã°yÃ Â¼ 1 s ffiffiffiffiffi ffi 2p p exp Ã Ã°y Ã mÃ 2 2s 2 .</formula><p>Following the line of reasoning by Triesch, this yields for the Kullback-Leibler divergence:</p><formula xml:id="formula_7">D KL Ã° p; pÃ Â¼ Z pÃ°yÃ log pÃ°yÃ 1=Ã° ffiffiffiffiffi ffi 2p p sÃe ÃÃ°yÃmÃ 2 =2s 2 ! dy Â¼ Z pÃ°yÃ logÃ° pÃ°yÃÃ dy Ã¾ Z pÃ°yÃ y 2 2s 2 dy Ã Z pÃ°yÃ 2my 2s 2 dy Ã¾ C Â¼ E logÃ° px Ã°xÃÃ Ã log qf gen qx Ã¾ 1 2s 2 y 2 Ã m s 2 y Ã¾ C,</formula><p>where C are constant terms, the relation p y Ã°yÃqy Â¼ p x Ã°xÃqx is used, y Â¼ f gen Ã°xÃ Â¼ tanhÃ°ax Ã¾ bÃ and p x Ã°xÃ represents the actual probability density of the input to the neuron. Deriving this w.r.t. for instance the parameter b yields:</p><formula xml:id="formula_8">qD KL qb Â¼ qE qb logÃ° px Ã°xÃÃ Ã log qf gen qx Ã¾ 1 2s 2 y 2 Ã m s 2 y Â¼ E 0 Ã q 2 f =qx 2 qf =qx Ã¾ 1 2s 2 qy 2 qb Ã m s 2</formula><p>qy qb .</p><p>In this case,</p><formula xml:id="formula_9">q 2 f qx 2 qf qx Ã1 Â¼ Ã2yÃ°1 Ã y 2 Ã 1 Ã y 2 Â¼ Ã2y,</formula><p>so this yields:</p><formula xml:id="formula_10">qD KL qb Â¼ E Ã m s 2 Ã¾ y s 2 Ã°2s 2 Ã¾ 1 Ã y 2 Ã¾ myÃ .</formula><p>Similar steps also yield the derivative w.r.t. a:</p><formula xml:id="formula_11">qD KL qa Â¼ E Ã mx s 2 Ã¾ xy s 2 Ã°2s 2 Ã¾ 1 Ã y 2 Ã¾ myÃ Ã 1 a</formula><p>.</p><p>From this, we get the following on-line learning rule with stochastic gradient descent using a learning rate Z:</p><formula xml:id="formula_12">Db Â¼ Ã Z Ã m s 2 Ã¾ y s 2 Ã°2s 2 Ã¾ 1 Ã y 2 Ã¾ myÃ , Da Â¼ Z a Ã¾ Dbx.</formula><p>The learning rule for the Gaussian/tanh combination bears similarity to the exponential/fermi rule. In particular, the relation between the Da and Db terms is identical (in fact, this relation is independent of the desired distribution or non-linearities).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Effects of bounded activation values</head><p>We now want to evaluate whether the IP learning rules are able to converge to the desired output distributions. There is however a problem due to the bounded nature of the output of the neurons we use: they are unable to output arbitrary values. The non-linearities we consider in this contribution are bounded to Â½0; 1 for fermi and Â½Ã1; 1 for tanh. Due to these bounds, when we impose a certain constraint on the moments of the infinite distribution, these moments will not be accurately approximated by a learning rule controlling a neuron with finite output range. The effects of these bounds on the moments of the actual output distribution can be computed as follows.</p><p>A probability density function truncated to Â½k; l can be derived from its infinite-support version by renormalization:</p><formula xml:id="formula_13">pÃ°yÃ Â¼ pÃ°yÃHÃ°y Ã kÃHÃ°l Ã yÃ R l k pÃ°yÃ dy ,</formula><p>where HÃ°xÃ represents the Heaviside function, which is 0 when xo0 and 1 otherwise. Given the ''desired'' moments of the distribution p which we use to train, we can now calculate the actual moments that are reached. The general derivation of these relations is quite complex, so we will only give the explicit relation for the mean on the exponential distribution and the fermi neuron. The exponential distribution truncated to Â½0; 1 is given by pexp Ã°yÃ Â¼ p exp Ã°yÃHÃ°y Ã kÃHÃ°l Ã yÃ R 1 0 p exp Ã°yÃ dy Â¼ 1=m expÃ°Ãy=mÃHÃ°y Ã kÃHÃ°l Ã yÃ R 1 0 1=m expÃ°Ãy=mÃ dy Â¼ expÃ°Ãy=mÃHÃ°y Ã kÃHÃ°l Ã yÃ mÃ°expÃ°1=mÃ Ã 1Ã expÃ°Ã1=mÃ .</p><p>From this we can calculate the actual mean:</p><formula xml:id="formula_14">mexp Â¼ Z y pexp Ã°yÃ dy Â¼ m expÃ°1=mÃ Ã m Ã 1 expÃ°1=mÃ Ã 1 .</formula><p>We did this also for the standard deviation and for the Gaussian distribution. The theoretical results can be seen in Fig. <ref type="figure">1</ref>.</p><p>The degree to which these moments differ from the infinite ones depends on how much of the probability mass of the infinite distribution is cutoff at k and l, as is shown by an example in Fig. <ref type="figure">2</ref>.For small values of m and s with respect to k and l, p is a good approximation of p. But for larger values, p approximates a uniform distribution, which has a mean of Ã°l Ã¾ kÃ=2 and a variance of</p><formula xml:id="formula_15">Ã°Ã°l Ã k Ã¾ 1Ã 2 Ã 1Ã=12.</formula><p>Experimental validation of these theoretical results can be seen in Fig. <ref type="figure">3</ref>. The setup used for these experiments is the one described in the following section. We see that the theoretical prediction for the mean of the exponential distribution is a good fit. The second moment is however </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian distribution</head><p>Fig. <ref type="figure">3</ref>. These plots show the theoretically derived relation between the desired moments and the actual moments (denoted with a dashed line, and can also be seen in Fig. <ref type="figure">1</ref>) of the exponential and Gaussian distribution, and the experimentally measured moments (full lines). The left plot shows the mean for the exponential distribution, the middle plot shows the variance for the exponential distribution, and the right plot shows the variance for the Gaussian distribution. The mean for the Gaussian distribution was set to 0. less well predicted. This is mainly due to the ''edge effect'' of the non-linearity. At the edges of the non-linearity's interval, increasingly high input values are needed to reach the edges of the interval. Since the IP learning rule converges, the a values remain bounded. The input to the non-linearity is thus also bounded and the outer edges of the non-linearity can thus never be reached. This can, for example, be seen in Fig. <ref type="figure">2</ref>. For the Gaussian, we also see a good correspondence, but which gets worse for increasing variance. Here again, edge effects start influencing the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Convergence in the presence of recurrency</head><p>Since all the derived rules are based on assuming independence of the neurons' input distributions, it is not self-evident that in a recurrent network, where neurons are coupled, the learning rules will succeed in driving the neurons toward the desired output distributions. Furthermore, each transfer function has only two degrees of freedom. Thus, it is worth investigating to what degree IP manages to approximate the desired output distributions in the presence of interfering recurrency. For the single neuron case, Triesch <ref type="bibr" target="#b39">[40]</ref> has already demonstrated that IP is able to adjust the bias and gain in a non-recurrent single neuron for approximating an exponential function.</p><p>Fig.</p><p>shows the theoretical and actual probability densities for both the fermi/exponential and tanh/Gaussian case, for two different settings of the desired moments. The estimation has been done by scaling a histogram with 200 bins of all neuron outputs over 1000 time steps such that its mass equals 1 (more details on the experimental setup can be found in the next section). The results show that even in the case of using IP in a highly connected recurrent network, we get a very good approximation of the theoretical distributions, with notable deviations only near the bounds of the nonlinearity (which is expected).</p><p>It has already qualitatively been shown in a previous publication by Steil (such as <ref type="bibr" target="#b35">[36]</ref>) that even though the IP learning rule actually only tunes the temporal distributions of single neurons, when they are coupled together in a recurrent network, the same distribution can be perceived spatially (even on a single time step if the reservoir is large enough)! A sparse temporal distribution thus results in a sparse spatial code. This ergodicity has yet never been proven theoretically to be always reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we will study the impact of both exponential and Gaussian IP on the performance of RC on several benchmark applications. The standard way to optimize a reservoir is by setting the spectral radius of the interconnection matrix to a given value. This method has however some drawbacks such as that the spectral radius is only a good measure for stability/dynamic regime if the network is not driven by input <ref type="bibr" target="#b22">[23]</ref>. There is furthermore a quite large variance on the performance when creating several random reservoirs with the same spectral radius. And finally, the spectral radius stability measure cannot be used on for example fermi nodes, or very constrained topologies. We will show that IP can help with regard to all three issues.</p><p>The three benchmarks we use in this section to evaluate the performance of the different IP rules are chosen to span a wide range of desired features we expect from reservoirs. First we use a task which purely evaluates the short-term memory performance of the system. We do this using the memory capacity (MC) introduced by Jaeger in <ref type="bibr" target="#b11">[12]</ref>. The input of the reservoir consists of a temporal signal uÃ°tÃ which is drawn from a uniform distribution in the interval Â½Ã0:8; 0:8. The outputs consist of an infinite number of outputs y s Ã°tÃ which try to reconstruct the delayed input uÃ°t Ã sÃ for s Â¼ 0 . . . 1: In practice, we take the number of outputs twice as high as the size of the reservoir which is a good approximation since the recall performance will drop drastically when we try to predict more time steps back in time than there are nodes in the network. The total MC is defined as the normalized correlation between the outputs and their associated delayed input:</p><formula xml:id="formula_16">MC Â¼ X 1 sÂ¼0 h á»¹s Ã°tÃuÃ°t Ã sÃi t hÃ° á»¹s Ã°tÃ Ã h á»¹s Ã°tÃi t Ã 2 i t hÃ°uÃ°tÃ Ã huÃ°tÃi t Ã 2 i t .</formula><p>Note that it was already shown theoretically in <ref type="bibr" target="#b11">[12]</ref> that for this task linear nodes perform optimally and they alone are capable of attaining the theoretical upper bound of MC Â¼ N with N the size of the reservoir, and this with a spectral radius very close to 1.</p><p>Next we use a widely used benchmark for time-seriesprediction techniques, a 30th-order NARMA system introduced in <ref type="bibr" target="#b1">[2]</ref>. The equation of this system is</p><formula xml:id="formula_17">yÃ°t Ã¾ 1Ã Â¼ 0:2yÃ°tÃ Ã¾ 0:04yÃ°tÃ X 29 jÂ¼0 yÃ°t Ã jÃ Ã¾ 1:5uÃ°t Ã 29ÃuÃ°tÃ Ã¾ 0:001,</formula><p>where uÃ°tÃ is uniformly drawn from the interval Â½0; 0:5. This is a difficult, highly non-linear task which requires a relatively long memory. Performance is evaluated using the normalized root mean square Error (NRMSE) which is equal to</p><formula xml:id="formula_18">NRMSE Â¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi hÃ° á»¹Ã°tÃ Ã yÃ°tÃÃ 2 i t hÃ°yÃ°tÃ Ã hyÃ°tÃi t Ã 2 i t s ,</formula><p>where yÃ°tÃ is the desired output and á»¹Ã°tÃ is the actual output.</p><p>As a last task, we use a real-world classification task where the goal is to perform isolated digits recognition from speech, for which state-of-the-art classification performance was already demonstrated using RC <ref type="bibr" target="#b42">[43]</ref>. We use almost the same setup as in <ref type="bibr" target="#b42">[43]</ref>: a subset of the TI46 speech corpus is used, where the samples are first transformed into a frequency decomposition using a choclear ear model, which is then resampled 128 times so the reservoir can cope better with the dynamics (for a detailed elaboration on this, we refer to <ref type="bibr" target="#b30">[31]</ref>). The classification is performed by training 10 readout functions, each representing one of the spoken digits. The reservoir has to classify the spoken digit at every time step, but the final classification is made by taking the winnertake-all of the temporal mean of all the ten readouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental settings</head><p>In this work we will use the following ESN setup. The reservoir states at time step t, xÃ°tÃ 2 R NÃ1 , are calculated by</p><formula xml:id="formula_19">xÃ°0Ã Â¼ 0, xÃ°t Ã¾ 1Ã Â¼ fÃ°W res</formula><p>res xÃ°tÃ Ã¾ W res inp uÃ°tÃ; a; bÃ, where N is the number of network nodes, W res res 2 R NÃN is the matrix of network weights, fÃ°:Ã is the vector-valued version of the generalized transfer function f Ã°:Ã, a; b 2 R N are the vectors of gain and bias parameters in fÃ°:Ã, M is the number of external inputs to the network at each time step, uÃ°tÃ 2 R MÃ1 the external input at time step t and the weights connecting these to the reservoir nodes W res inp 2 R NÃM . Note that above equation can be rewritten as</p><formula xml:id="formula_20">xÃ°t Ã¾ 1Ã Â¼ fÃ° Å´res res xÃ°tÃ Ã¾ Å´res inp uÃ°tÃ Ã¾ b; 1; 0Ã, Å´res res Â¼ diagÃ°aÃW res res , Å´res inp Â¼ diagÃ°aÃW res inp .</formula><p>This technique allows us to compare reservoirs that have been adapted by IP directly with networks which have not been adapted. We will call the spectral radius of Å´res res after applying IP, the effective spectral radius.</p><p>The output of a reservoir is calculated as a linear regression of the reservoir state at every time step:</p><formula xml:id="formula_21">yÃ°t Ã¾ 1Ã Â¼ W out res xÃ°t Ã¾ 1Ã Ã¾ b out .</formula><p>It is possible to add linear terms from the input to the output, but in the experiments in this work we did not choose to do this because we want to evaluate the influence of IP on the reservoir, and adding the linear terms can cloud the effects of IP.</p><p>We train a memoryless readout in a one-shot fashion for the complete dataset. This can be written as where V has to be optimized, given some loss function. We use a squared loss function and solve this using a least squares minimization with a regularization term to keep the linear regressor from over-fitting, which is often called ridge regression:</p><formula xml:id="formula_22">V Â¼ arg min V Ã°AV Ã BÃ 2 Ã¾ lkVk.</formula><p>This minimization can be efficiently calculated using this matrix solution:</p><formula xml:id="formula_23">V Â¼ Ã°A T A Ã¾ lIÃ Ã1 A T B.</formula><p>The l parameter is the regularization parameter which we determine using a five-fold cross-validation scheme with grid search on an alternating validation set. All the results in this paper are the test results after performing five-fold cross-validation on the total dataset. Some parameters of the network are kept fixed over all experiments. Network size is always 100-this could have been optimized, but we care here just for comparison between the two techniques. The W res res matrix is always created by initializing all the weights uniformly distributed between Ã1 and 1, which is then scaled to a given spectral radius, whereas the input weights are set with equal probability to Ã0:1 or 0:1. The IP parameters a and b are initialized to 1 and 0, respectively.</p><p>All results in plots and tables are averages and standard deviations over 30 experiments, i.e., for each training setup, 30 experiments were conducted. For training the linear readout, the first 100 time steps of the reservoir dynamics for every presented example were disregarded to allow the network to ''warm up'' (forget its initial state) sufficiently.</p><p>For pre-training a reservoir, the IP rule is applied with a learning rate of 0.0005 for 100 000 time steps (equal to 10 epochs when we use 10 time series, used for the crossvalidation, each consisting of 1000 time steps). To check whether IP has had sufficient time to adapt after this time, we verified that a and b had converged to small regions and compared the expected probability density with the one estimated from the reservoir's output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>The three benchmark experiments are now evaluated using different experimental setups. We first use the standard way of scaling reservoirs by ranging over the spectral radius. We do this for both fermi and tanh nodes. Next, we evaluate the use of IP for fermi nodes with an exponential distribution (ranging over m), and tanh nodes with a Gaussian distribution (ranging over s). For the Gaussian distribution we only use m Â¼ 0 since for other values, experiments show a considerable performance drop.</p><p>The results are shown in Fig. <ref type="figure" target="#fig_2">4</ref>, where the first row shows the results for the memory task, the second row shows the results for the NARMA task, and the bottom row shows the result for the isolated speech. The columns show the use of ranging the spectral radius for the tanh node type, exponential IP for fermi nodes, and Gaussian IP for tanh nodes. The results for ranging the spectral radius for the fermi nodes are not shown since the performance of this  IP is better than ranging the spectral radius, both in average performance as in standard deviation (except for one case), denoted between brackets. Some tasks perform better with a Gaussian distribution, others with an exponential distribution. Bold values denotes the best performance.</p><p>reservoir type is very poor. This is expected since the spectral radius is not a good measure for the dynamic regime when using fermi nodes. The best values for all settings are summarized in Table <ref type="table" target="#tab_0">1</ref> where the results of ranging the spectral radius for the fermi nodes are added as a reference. Memory: The best achievable MC of reservoirs without IP is quite different for networks using tanh nodes and those using fermi nodes (see Table <ref type="table" target="#tab_0">1</ref>). Note that for MC, higher is better. While tanh-networks can have a capacity of up to 31, fermi-networks have little more than half that memory, 17. In tanh-networks, the drop in performance for spectral radii smaller than optimal is not drastic. When using IP, the largest change in results can be found in fermi-networks. While the best achievable MC increases by one-fourth, i.e. 4, the variance of the results for one and the same parameter setting, i.e. m, gets very small, only 0.77, contrasting to setting the spectral radius, where this was 1.48. In tanh-networks, the difference between using spectral radius scaling and IP pre-adaptation was not as pronounced. The best MC achievable there, also increased by using IP from 29.78 to 31.31, but the variance is a little bit worse if using IP. Furthermore, the performance drop for suboptimal parameter settings is equally pronounced for s as for spectral radii.</p><p>NARMA: With tanh-networks, the best error achieved was 0.52, at a spectral radius of 0.95. For spectral radii of smaller 0.9 or larger than 1, performance drops considerably. When using Gaussian IP, the performance improves considerably, up to 10%. But, in contrast to the memory task, where the performance drop looks qualitative similar for changes in spectral radius and s, the optimal value of s is very small, and the performance decreases steadily for increasing its value. Fermi-node networks perform very poorly on this task. When using exponential IP, the performance slightly increases, but is still considerably worse than tanh nodes. Interestingly, if m was chosen larger than 0.3, its setting did not yield any differences in the results, and the variance drops to very small values of 0.019. Speech: Using IP slightly improves both the average and standard deviation of the performance for both exponential and Gaussian distributions. For this task, the exponential distribution seems to perform best. Another effect is the extreme drop in performance when ranging the spectral radius to high values, where the reservoir becomes unstable. When using IP, this instability is never reached.</p><p>When using fermi neurons, and imposing an exponential distribution, the optimal settings for m differ for the three different tasks. Increasing m increases the linear memory present in the network, i.e. the best performance could be observed for m Â¼ 0:1. In NARMA, the opposite was true: Up to m Â¼ 0:1, the variance of results was too high to be usable.</p><p>But when using tanh-neurons and imposing a Gaussian distribution, for s, the optimal setting was almost the same for both tasks, besides the performance drop in MC for s Â¼ 0:1, smaller settings were better.</p><p>An interesting fact can be discovered if comparing optimal spectral radius and the effective spectral radius of the optimal s: where the s was optimal, the effective spectral radius was equal to the optimal spectral radius of a network without IP (see Fig. <ref type="figure">5</ref>). Notice the relatively small variance of the relation between moments and effective spectral radius. Imposing certain output distributions on the nodes of the reservoir is thus actually a precise way of controlling the dynamics in the reservoir. Note that the effective spectral radius for fermi nodes cannot at all be related to those of tanh neurons, which are normally used. For tanh neurons, we see that when varying s over it complete range, we actually vary the effective spectral radius in its most important range: between 0.8 and 1.1.</p><p>Note that IP and spectral radius influence each other in two ways. Firstly, the initial weight matrix size can alter the learning behavior of IP, because the adjustment factors of the intrinsic parameters by the rule depend on the amount of activity present in the network. Secondly, changing the gain of the transfer function corresponds to scaling all incoming weights, and therefore changing the spectral radius of the weight matrix. Thus, the effective spectral </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian distribution</head><p>Fig. <ref type="figure">5</ref>. In these plots we show the relation between the mean and standard deviation of the exponential and Gaussian distribution, respectively, and the effective spectral radius which is attained after pre-training the reservoir using IP. These plots are generated from the MC task, but look almost identical for the other tasks. This shows that there is a clear relation between moments and effective spectral radius, which is task independent. radius after applying IP will be different from the one the network was initialized to.</p><p>It is known that the MC is highest for linear reservoirs that have a spectral radius close to 1 <ref type="bibr" target="#b11">[12]</ref>. We see a similar effect when using IP: the best performance is attained for Gaussian distributions with small variance, where to bulk of the dynamics is thus in the linear part of the tanh nonlinearity. The optimal s Â¼ 0:2 which we can clearly relate to an effective spectral radius of 1 in Fig. <ref type="figure">5</ref>. The NARMA task also seems to prefer reservoirs that for the most part operate in their linear regime. This can be explained due to the relatively large memory that is needed to solve the 30thorder NARMA task. The speech task on the other hand appears to be a task that can take advantage of the nonlinearities when operating the bulk of the dynamics in the non-linear part of the fermi neurons due to the exponential distribution.</p><p>Note that for fermi-node networks, only IP with exponential target distribution is studied here. When not pre-adapting fermi networks, the performance is always very bad. But, when using IP pre-adaptation, fermi neurons can already get the best performance for the speech task. The bad performance on the memory and NARMA task might suggest that the fermi neurons are intrinsically flawed. But when pre-adapting fermi nodes with Gaussian IP, which is possible, but not thoroughly investigated in this work, the results seem to be qualitative similar to the ones achieved with tanh-node networks. The above results thus mainly relate to the distributions, and not to the node types.</p><p>Note that in previous work of Steil <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> a good performance was achieved for fermi nodes with exponential IP on a related NARMA task. This was however accomplished by additional features in the overall reservoir architecture that were deliberately left out in this work to be better able to discern the actual influence of the reservoir: not only the current time step was fed to the reservoir, but also a time delayed version of the input, and both these inputs were also directly fed to the linear output. Since the NARMA task depends on a long input history, and has a large linear component, these two features allow the reservoir to perform better. The optimal value for m in this setup is actually quite low, which suggests that the reservoir only has to solve the non-linear part of the NARMA task, while the linear, time delayed connection takes care of the linear part. When the reservoir has to solve all these parts by itself, we see that the exponential distribution is not the best option since it is not able to generate a long enough memory of its input, as is suggested by the results on the MC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Constrained topologies</head><p>When constructing reservoirs for RC, in most cases completely random and sparse connectivity matrices are used. These type of topologies have very good reservoir properties because the dynamic regime can be easily changed by globally scaling all the weights up or down. Due to this scaling, the dynamic regime can be precisely and progressively varied from very damped to highly unstable dynamics. The topologies are however not easy to implement on, for example, a planar substrate such as a silicon chip.</p><p>When we look at very constrained topologies such as 1D and 2D lattices, which are the easiest topologies to implement on planar substrates, they behave very badly as reservoirs: for example, a 1D lattice where the input is only fed to one of the nodes (see Fig. <ref type="figure">6</ref>) will have a very sudden transition from order (only the node which is fed with the input is active, and the activity of all the other nodes is smaller in orders of magnitude) to a wildly oscillating reservoir when you scale up the weights. The boundary of stability is very narrow in this case. These topologies can thus not easily be used in the usual sense when just globally scaling the weights.</p><p>We will now demonstrate that imposing distributions on the reservoir dynamics is the key to using these constrained topologies as reservoirs, and this in a very robust and reproducible way. We repeated the MC and NARMA task with the same settings as in the previous section, but now using the constrained topology shown in Fig. <ref type="figure">6</ref> which is a 1D lattice consisting of 50 neurons which are only connected to their nearest neighbors. For this experiment we only look at tanh nodes and IP with a Gaussian distribution. The results can be seen in Fig. <ref type="figure" target="#fig_3">7</ref>. We did not do this experiment for the speech task, since this task has multiple inputs (one input for each frequency component in the cochlear model), and in this experiment we only want to evaluate if IP is able to create usable dynamics in a ring topology where only a single neuron initially has some activity.</p><p>We first evaluate the performance of this ring topology when just scaling the spectral radius. For both the MC and the NARMA task, scaling the spectral radius performs very poorly. Especially when scaling up the reservoir for ARTICLE IN PRESS Fig. <ref type="figure">6</ref>. Example ring topology, where each node is connected to its nearest-neighbors, and the input is only fed to a single neuron.</p><p>the NARMA task, we see a drastic increase in error and variance of error, which is due to the unstable regime which is reached.</p><p>When using Gaussian IP to pre-adapt this ring topology, we see an increase in performance for both the MC and NARMA task. This clearly demonstrates that IP is very capable of enforcing a desired dynamic regime on a reservoir, even if its topology is very constrained.</p><p>Using this IP rule thus allows to pre-adapt simple, constrained topologies prior to hardware implementation. Due to the pre-adaptation, these very sparsely and regularly connected reservoirs can be actually used to perform tasks in an efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and future work</head><p>We have presented and derived a reservoir adaptation rule that maximizes the information content of the reservoir states based on a constraint on the mean and variance of the state distribution. This results in exponential and normal distributed reservoir states. We have shown that IP learning is capable of driving the neurons in larger networks to the theoretically derived renormalized mean and variance of the desired exponential and Gaussian output distributions. We have evaluated reservoir performance and effective spectral radius on several benchmarks with very different characteristics.</p><p>The results show that for the three tasks we always get an improvement in performance when pre-adapting the reservoir using IP. We also see a steady decrease in variance, which shows that the spread on the performance when creating random reservoirs can be decreased using a very simple local, unsupervised learning rule which is only used to pre-adapt the reservoir weights.</p><p>For tasks that mainly need linear responses (as the memory task and NARMA), the Gaussian distribution performs best, while on tasks that are non-linear (like the speech recognition), the exponential distribution performs best.</p><p>Another important effect of IP is that it makes it possible to use node types and topologies which normally perform very poorly as reservoir. When for example training fermi nodes with Gaussian distribution (which was only briefly investigated), we get the same results as when using tanh distribution. The very special ring-topology can, through the use of IP, also be used as a real reservoir. This has far reaching consequences when we implement reservoirs in hardware: a reservoir can be built using hardware friendly nearest neighbour connections, and can still be effectively used as reservoir.</p><p>As future work we plan to extend the idea of information maximization to different output distributions, more parameters that can be trained and other node types. We will especially focus on distributions with a high kurtosis since these lead to sparse codes. It has been shown that sparse codes increase signal-to-noise ratio, and improve the detection of coincidences <ref type="bibr" target="#b7">[8]</ref>. Distributions such as the Laplace distribution which have a kurtosis of 3 will be investigated. The Gaussian distribution investigated in this work only has a kurtosis of 0. We will further show that IP can be an enabler for using very constrained topologies in RC, as already hinted in this work. Extending the work to spiking neurons will also show this for LSM-type reservoirs which are currently very difficult to tune, but by using IP can be controlled using a single parameter. A clear link between information maximization, and the influence of it on dynamics must also be further studied (why is there a clear link between parameters of the distribution and for example the spectral radius, which is a measure of stability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ARTICLE IN PRESS</head><p>The idea of autonomously regulated robustness of dynamics is a powerful new concept. It was shown that a certain dynamic regime in reservoirs leads to good performance for a given task. The IP rule allows the reservoirs to autonomously perceive and adapt their dynamics to this specific regime, irrespective of disturbances, initial weights or input scaling. Reservoirs were already robust in the sense that the performance variance for random reservoirs is small. This robustness even improves when adding IP, since reservoirs can now autonomously form the correct dynamic properties. In future work, this idea of robust computing using selfregulating dynamic systems will be further investigated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig.1. Statistics of the finite distributions compared to that of infinite ones. The left figure shows mean and standard deviation of the finite exponential distribution against m of the infinite distribution. They are identical for small m (linear relation), but then saturate. In the middle and right figures, the standard deviation and mean of the finite normal distribution against variance of the infinite distribution for several settings of m is shown. The s of finite and infinite distributions is equal for small s values and then saturates. The mean should stay constant, but goes to zero for large s values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. Results for all three benchmarks for tanh with spectral radius ranging (left column), exponential IP for fermi nodes (middle column), and Gaussian IP for tanh nodes (right column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results for a ring topology, on the left for ranging across the spectral radius without IP, on the right for adjusting the desired standard deviation with IP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Best results for the three different benchmarks</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Fermi-specrad</cell><cell>Tanh-specrad</cell><cell>Fermi-exp. IP</cell><cell>tanh-Gauss. IP</cell></row><row><cell>MC</cell><cell>17.41 (1.48)</cell><cell>29.78 (1.87)</cell><cell>20.32 (0.77)</cell><cell>31.31 (1.93)</cell></row><row><cell>NARMA</cell><cell>0.77 (0.012)</cell><cell>0.52 (0.050)</cell><cell>0.74 (0.019)</cell><cell>0.46 (0.042)</cell></row><row><cell>Speech</cell><cell>0.070 (0.018)</cell><cell>0.069 (0.015)</cell><cell>0.060 (0.012)</cell><cell>0.069 (0.015)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>B. Schrauwen et al. / Neurocomputing 71 (2008) 1159-1171</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work partially funded the Research Foundation-Flanders project G.0317.05, and the Photonics@be Interuniversity Attraction Poles program (IAP 6/10), initiated by the Belgian State, Prime Minister's Services, Science Policy Office.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His research interests include the study of reservoir dynamics and unsupervised reservoir adaptation rules for engineering applications. Dirk Stroobandt is a member of AIG (Alumni Ghent), KVIV (Royal Flamish Engineering Association), IEEE, and ACM.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Could information theory provide an ecological theory of sensory processing?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Atick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="251" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">New results on recurrent network training: unifying the algorithms and accelerating convergence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Parlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">697</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Responses of neurons in primary and inferior temporal visual cortices to natural scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Baddeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C A</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sengpiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Wakeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Rolls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings in Biological Sciences</title>
		<meeting>in Biological Sciences</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="1775" to="1783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training networks of biological realistic spiking neurons for real-time robot control</title>
		<author>
			<persName><forename type="first">H</forename><surname>Burgsteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Engineering Applications of Neural Networks</title>
		<meeting>the 9th International Conference on Engineering Applications of Neural Networks<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On learning with recurrent spiking neural networks and their applications to robot control with real-world devices</title>
		<author>
			<persName><forename type="first">H</forename><surname>Burgsteiner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Graz University of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling reward functions for incomplete state representations via echo state networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks<address><addrLine>Montreal, Quebec</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Plasticity in single neuron and circuit computations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Destexhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">431</biblScope>
			<biblScope unit="page" from="789" to="795" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What is the goal of sensory coding?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="559" to="601" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Perspectives on learning with recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks (ESANN)</title>
		<meeting>the European Symposium on Artificial Neural Networks (ESANN)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to ground fact symbols in behavior-based robots</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hertzberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>SchoÂ¨nherr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th European Conference on Artificial Intelligence</title>
		<meeting>the 15th European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="708" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The &apos;&apos;echo state&apos;&apos; approach to analysing and training recurrent neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<idno>148</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>German National Research Center for Information Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report GMD Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Short term memory in echo state networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<idno>152</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>German National Research Center for Information Technology</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report GMD Report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive nonlinear system identification with echo state networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="593" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reservoir riddles: suggestions for echo state network research (extended abstract</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1460" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harnessing nonlinearity: predicting chaotic systems and saving energy in wireless telecommunication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Movement generation and control with generic neural microcircuits</title>
		<author>
			<persName><forename type="first">P</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BIO-AUDIT</title>
		<meeting>BIO-AUDIT</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="16" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fading memory and times series prediction in recurrent networks with different forms of plasticity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pipa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Triesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="312" to="322" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new approach towards vision suggested by biologically realistic neural microcircuit models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Legenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Biologically Motivated Computer Vision</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 2nd Workshop on Biologically Motivated Computer Vision<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fading memory and kernel properties of generic cortical microcircuit models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>NatschlaÂ¨ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">4-6</biblScope>
			<biblScope unit="page" from="315" to="330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time computing without stable states: a new framework for neural computation based on perturbations</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>NatschlaÂ¨ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2531" to="2560" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A model for real-time computation in generic neural microcircuits</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>NatschlaÂ¨ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="229" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Velocity control of an omnidirectional robocup player with recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oubbati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Buchheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Robocup Symposium</title>
		<meeting>the Robocup Symposium</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="691" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Analysis and design of echo state networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Ozturk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="111" to="138" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>PloÂ¨ger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Arghir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>GuÂ¨nther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hosseiny</surname></persName>
		</author>
		<title level="m">Echo state networks for mobile robot modeling and control</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="157" to="168" />
		</imprint>
	</monogr>
	<note>RoboCup 2003: Robot Soccer World Cup VII</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neurocontrol of nonlinear dynamical systems with Kalman filter trained recurrent networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Puskorius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="279" to="297" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning mappings in brain machine interfaces with echo state networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carmena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolelis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the 2005 IEEE International Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="233" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Learning internal representations by error propagation, Parallel Distributed Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Echo state networks used for motor control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>PloÂ¨ger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 IEEE International Conference on Robotics and Automation</title>
		<meeting>the 2005 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1953" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Analyzing the weight dynamics of recurrent learning algorithms</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">D</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="5" to="23" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The introduction of time-scales in reservoir computing, applied to isolated digits recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Defour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimum mean squared error time series classification using an echo state network prediction model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Skowronski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Circuits and Systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Special issue on echo state networks and liquid State machines</title>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Backpropagation-decorrelation: online recurrent learning with O(N) complexity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks (IJCNN)</title>
		<meeting>the International Joint Conference on Neural Networks (IJCNN)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="843" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Memory in backpropagation-decorrelation O(N) efficient online recurrent learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Online stability of backpropagation-decorrelation recurrent learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="642" to="650" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Several ways to solve the mso problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks (ESANN)</title>
		<meeting>the European Symposium on Artificial Neural Networks (ESANN)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="489" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Online reservoir adaptation by intrinsic plasticity for backpropagation-decorrelation and echo state learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="353" to="364" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Detecting strange attractors in turbulence</title>
		<author>
			<persName><forename type="first">F</forename><surname>Takens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamical Systems and Turbulence</title>
		<title level="s">Lecture Notes in Mathematics</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Rand</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A gradient rule for the plasticity of a neuron&apos;s intrinsic excitability</title>
		<author>
			<persName><forename type="first">J</forename><surname>Triesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks (ICANN)</title>
		<meeting>the International Conference on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Synergies between intrinsic and synaptic plasticity mechanisms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Triesch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="885" to="909" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A unifying comparison of reservoir computing methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>D'haene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stroobandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="391" to="403" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Isolated word recognition using a liquid state machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stroobandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th European Symposium on Artificial Neural Networks (ESANN)</title>
		<meeting>the 13th European Symposium on Artificial Neural Networks (ESANN)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="435" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Reservoir-based techniques for speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stroobandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the World Conference on Computational Intelligence</title>
		<meeting>the World Conference on Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1050" to="1053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Isolated word recognition with the liquid state machine: a case study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Verstraeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stroobandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Campenhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="521" to="528" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Intrinsic plasticity for reservoir learning algorithms</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wardermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Steil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks (ESANN)</title>
		<meeting>the European Symposium on Artificial Neural Networks (ESANN)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="513" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Beyond regression: new tools for prediction and analysis in the behavioral sciences</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Applied Mathematics, Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The other side of the engram: experiencedriven changes in neuronal intrinsic excitability</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Linden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="885" to="900" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
