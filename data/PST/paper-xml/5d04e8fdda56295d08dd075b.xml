<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuzhe</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Guo</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dina</forename><surname>Katabi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhi</forename><surname>Xu</surname></persName>
						</author>
						<title level="a" type="main">ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>State-of-the-art deep neural networks (NNs) are vulnerable to adversarial examples <ref type="bibr" target="#b32">(Szegedy et al., 2013)</ref>. By adding small human-indistinguishable perturbation to the inputs, an adversary can fool neural networks to produce incorrect outputs with high probabilities. This phenomena raises increasing concerns for safety-critical scenarios such as the self-driving cars where NNs are widely deployed.</p><p>An increasing body of research has been aiming to either generate effective perturbations, or construct NNs that are robust enough to defend against such attacks. Currently, many effective algorithms exist to craft these adversarial examples, but defense techniques seem to be lagging behind. 1 MIT CSAIL, Cambridge, MA, USA. Correspondence to: Yuzhe Yang &lt;yuzhe@mit.edu&gt;, Zhi Xu &lt;zhixu@mit.edu&gt;.</p><p>Proceedings of the 36 th International Conference on Machine <ref type="bibr">Learning, Long Beach, California, PMLR 97, 2019.</ref> Copyright 2019 by the author(s).</p><p>For instance, the state-of-the-art defense can only achieve less than 50% adversarial accuracy for ∞ perturbations on datasets such as CIFAR-10 ( <ref type="bibr" target="#b26">Madry et al., 2017)</ref>. Under recent strong attacks, most defense methods have shown to break down to nearly 0% accuracy <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>.</p><p>As adversarial perturbations are carefully generated structured noise, a natural conjecture for defending against them is to destroy their structure. A naive approach for doing so would randomly mask (i.e., zero out) pixels in the image. While such method can eliminate the adversarial structure within the noise through random information drop, it is almost certain to fail since it equally destroys the information of the original image, making NN inference even worse.</p><p>However, this naive starting point raises an interesting suggestion: instead of simply applying a random mask to the images, a preferable method should also reconstruct the images from their masked versions. In this case, the random masking destroys the crafted structures, but the reconstruction recovers the global structures that characterize the objects in the images. Images contain some global structures. An image classified as cat should have at least a cat as its main body. Humans use such global structure to classify images. In contrast the structure in adversarial perturbation is more local and defies the human eye. If both training and testing are performed under the same underlying global structures (i.e., there is no distributional shift in training and testing), the network should be generalizable and robust. If the reconstruction can successfully maintain the underlying global structure, the masking-and-reconstruction pipeline can redistribute the carefully constructed adversarial noises to non-adversarial structures.</p><p>In this paper, we leverage matrix estimation (ME) as our reconstruction scheme. ME is concerned with recovering a data matrix from noisy and incomplete observations of its entries, where exact or approximate recovery of a matrix is theoretically guaranteed if the true data matrix has some global structures (e.g., low rank). We view a masked adversarial image as a noisy and incomplete realization of the underlying clean image, and propose ME-Net, a preprocessing-based defense that reverts a noisy incomplete image into a denoised version that maintains the underlying global structures in the clean image. ME-Net realizes adversarial robustness by using such denoised global-structure preserving representations.</p><p>We note that the ME-Net pipeline can be combined with different training procedures. In particular, we show that ME-Net can be combined with standard stochastic gradient descent (SGD) or adversarial training, and in both cases improves adversarial robustness. This is in contrast with many preprocessing techniques which cannot leverage the benefits of adversarial training <ref type="bibr" target="#b9">(Buckman et al., 2018;</ref><ref type="bibr" target="#b31">Song et al., 2018;</ref><ref type="bibr" target="#b19">Guo et al., 2017)</ref>, and end up failing under the recent strong white-box attack <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>.</p><p>We provide extensive experimental validation of ME-Net under the strongest black-box and white-box attacks on established benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet, where ME-Net outperforms state-ofthe-art defense techniques. Our implementation is available at: https://github.com/YyzHarry/ME-Net.</p><p>We summarize our contributions as follows:</p><p>• We are the first to leverage matrix estimation as a general pipeline for image classification and defending against adversarial attacks.</p><p>• We show empirically that ME-Net improves the robustness of neural networks under various ∞ attacks: 1. ME-Net alone significantly improves the state-of-theart results on black-box attacks; 2. Adversarially trained ME-Net consistently outperforms the state-of-the-art defense techniques on whitebox attacks, including the strong attacks that counter gradient obfuscation <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>. Such superior performance is maintained across various datasets: CIFAR-10, MNIST, SVHN and Tiny-ImageNet.</p><p>• We show additional benefits of ME-Net such as improving generalization (performance on clean images).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ME-Net</head><p>We first describe the motivation and high level idea underlying our design. We then provide the formal algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Design Motivation</head><p>Images contain noise: even "clean" images taken from a camera contain white noise from the environment. Such small, unstructured noise seems to be tolerable for modern deep NNs, which achieve human-level performance. However, the story is different for carefully constructed noise. Structured, adversarial noise (i.e., adversarial examples) can easily corrupt the NN results, leading to incorrect prediction from human's perspective. This means that to achieve robustness to adversarial noise, we need to eliminate/reduce the crafted adversarial structure. Of course, while doing so, we need to maintain the intrinsic structures in the image that allow a human to make correct classifications.</p><p>We can model the problem as follows: An image is a superposition of: 1) intrinsic true structures of the data in the scene, 2) adversarial carefully-structured noise, and 3) nonadversarial noise. Our approach is first to destroy much of the crafted structure of the adversarial noise by randomly masking (zeroing out) pixels in the image. Of course, this process also increases the overall noise in the image (i.e., the non-adversarial noise) and also negatively affects the underlying intrinsic structures of the scene. Luckily however there is a well-established theory for recovering the underlying intrinsic structure of data from noisy and incomplete (i.e., masked) observations. Specifically, if we think of an image as a matrix, then we can leverage a well-founded literature on matrix estimation (ME) which allows us to recover the true data in a matrix from noisy and incomplete observations <ref type="bibr" target="#b10">(Candès &amp; Recht, 2009;</ref><ref type="bibr" target="#b23">Keshavan et al., 2010;</ref><ref type="bibr" target="#b12">Chatterjee et al., 2015)</ref>. Further, ME provides provable guarantees of exact or approximate recovery of the true matrix if the true data has some global structures (e.g., low rank) <ref type="bibr" target="#b16">(Davenport &amp; Romberg, 2016;</ref><ref type="bibr" target="#b14">Chen &amp; Chi, 2018)</ref>. Since images naturally have global structures (e.g., an image of a cat, has a cat as a main structure), ME is guaranteed to restore the intrinsic structures of the clean image.</p><p>Another motivation for our method comes from adversarial training, where an NN is trained with adversarial examples.</p><p>Adversarial training is widely adopted to increase the robustness of neural networks. However, recent theoretical work formally argues that adversarial training requires substantially more data to achieve robustness <ref type="bibr" target="#b30">(Schmidt et al., 2018)</ref>. The natural question is then how to automatically obtain more data, with the purpose of creating samples that can help robustness. Our masking-then-reconstruction pipeline provides exactly one such automatic solutions. By using different random masks, we can create variations on each image, where all such variations maintain the image's underlying true global structures. We will see later in our results that this indeed provides significant gain in robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Matrix Estimation Pipeline</head><p>Having described the intuition underlying ME-Net, we next provide a formal description of matrix estimation (ME), which constitutes the reconstruction step in our pipeline.</p><p>Matrix Estimation. Matrix estimation is concerned with recovering a data matrix from noisy and incomplete observations of its entries. Consider a true, unknown data matrix M ∈ R n×m . Often, we have access to a subset Ω of entries from a noisy matrix X ∈ R n×m such that E[X] = M . For example, in recommendation system, there are true, unknown ratings for each product from each user. One often observes a subset of noisy ratings if the user actually rates the product online. Technically, it is often assumed that each entry of X, X ij , is a random variable independent of the others, which is observed with probability p ∈ (0, 1] (i.e., missing with probability 1 − p). The theoretical question is then formulated as finding an estimator M , given noisy, incomplete observation matrix X, such that M is "close" to M . The closeness is typically measured by some matrix norm, || M − M ||, such as the Frobenius norm.</p><p>Over the years, extensive algorithms have been proposed. They range from simple spectral method such as universal singular value thresholding (USVT) <ref type="bibr" target="#b12">(Chatterjee et al., 2015)</ref>, which performs SVD on the observation matrix X and discards small singular values (and corresponding singular vectors), to convex optimization based methods, which minimize the nuclear norm <ref type="bibr" target="#b10">(Candès &amp; Recht, 2009)</ref>, i.e.:</p><p>min</p><formula xml:id="formula_0">M ∈R n×m || M || * s.t. Mij ≈ X ij , ∀ (i, j) ∈ Ω,<label>(1)</label></formula><p>where || M || * is the nuclear norm of the matrix (i.e., sum of the singular values). To speed up the computation, the Soft-Impute algorithm <ref type="bibr" target="#b27">(Mazumder et al., 2010)</ref> reformulates the optimization using a regularization parameter λ ≥ 0: min</p><formula xml:id="formula_1">M ∈R n×m 1 2 (i,j)∈Ω Mij − X ij 2 + λ|| M || * . (2)</formula><p>In this paper, we view ME as a reconstruction oracle from masked images, rather than focusing on specific algorithms.</p><p>The key message in the field of ME is: if the true data matrix M has some global structures, exact or approximate recovery of M can be theoretically guaranteed <ref type="bibr" target="#b10">(Candès &amp; Recht, 2009;</ref><ref type="bibr" target="#b12">Chatterjee et al., 2015;</ref><ref type="bibr" target="#b14">Chen &amp; Chi, 2018)</ref>. This strong theoretical guarantee serves as the foundation for employing ME to reconstruct structures in images. In the literature, the most studied global structure is low rank. Latent variable models, where each row i and each column j are associated with some features u i ∈ R r and v j ∈ R r and M ij = f (u i , v j ) for some function f , have also been investigated <ref type="bibr" target="#b12">(Chatterjee et al., 2015;</ref><ref type="bibr" target="#b7">Borgs et al., 2017)</ref>. To some extent, both could be good models for images.</p><p>Empirical Results. Before closing, we empirically show that images have strong global structures (i.e., low rank).</p><p>We consider four datasets: MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. We perform SVD on each image and compute its approximate rank, which is defined as the minimum number of singular values necessary to capture at least 90% of the energy in the image. Fig. <ref type="figure" target="#fig_0">1</ref> plots the histogram and the empirical CDF of the approximate ranks for each dataset.</p><p>As expected, images in all datasets are relatively low rank. Specifically, the vast majority of images in MNIST, CIFAR-10, and SVHN have a rank less than 5. The rank of images in Tiny-ImageNet is larger but still significantly less than the image dimension (∼10 vs. 64). This result shows that images tend to be low-rank, which implies the validity of using ME as our reconstruction oracle to find global structures.</p><p>Next, we show in Fig. <ref type="figure" target="#fig_1">2</ref> the results of ME-based reconstruction for different masks. Evidently, the global structure (the gate in the image) has been maintained even when p, the probability of observing the true pixel, is as low as 0.3. This shows that despite random masking we should be able to reconstruct the intrinsic global image structure from the masked adversarial images. Our intuition is that humans use such underlying global structures for image classification, and if we can maintain such global structures while weakening other potentially adversarial structures, we can force both training and testing to focus on human recognizable structures and increase robustness to adversarial attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model</head><p>We are now ready to formally describe our technique, which we refer as ME-Net. The method is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref> and summarized as follows: • ME-Net Training: Define a mask as an image transform in which each pixel is preserved with probability p and set to zero with probability 1 − p. For each training image X, we apply n masks with probabilities {p 1 , p 2 , . . . , p n }, and obtain n masked images {X (<ref type="foot" target="#foot_0">1</ref>) , X (2) , . . . , X (n) }.</p><p>An ME algorithm is then applied to obtain reconstructed images { X(1) , X(2) , . . . , X(n) }. We train the network on the reconstructed images { X(1) , X(2) , . . . , X(n) } as usual via SGD. Alternatively, adversarial training can also be readily applied in our framework.</p><p>• ME-Net Inference: For each test image X, we randomly sample a mask with probability p = 1 n n i=1 p i , i.e., the average of the masking probabilities during training. The masked image is then processed by the same ME algorithm used in training to obtain X. Finally, X is fed to the network for prediction.</p><p>Note that we could either operate on the three RGB channels separately as independent matrices or jointly by concatenating them into one matrix. In this paper, we take the latter approach as their structures are closely related. We provide additional details of ME-Net in Appendix A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Evaluation</head><p>We evaluate ME-Net empirically under ∞ -bounded attacks and compare it with state-of-the-art defense techniques.</p><p>Experimental Setup: We implement ME-Net as described in Section 2. For each attack type, we compare ME-Net with state-of-theart defense techniques for the attack under consideration. For each technique, we report accuracy as the percentage of adversarial examples that are correctly classified. 1 As common in prior work <ref type="bibr" target="#b26">(Madry et al., 2017;</ref><ref type="bibr" target="#b9">Buckman et al., 2018;</ref><ref type="bibr" target="#b31">Song et al., 2018)</ref>, we focus on robustness against ∞bounded attacks, and generate adversarial examples using standard methods such as the CW attack <ref type="bibr" target="#b11">(Carlini &amp; Wagner, 2017)</ref>, Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b18">(Goodfellow et al., 2015)</ref>, and Projected Gradient Descent (PGD) which is a more powerful adversary that performs a multi-step variant of FGSM <ref type="bibr" target="#b26">(Madry et al., 2017)</ref>.</p><p>Organization: We first perform an extensive study on CIFAR-10 to validate the effectiveness of ME-Net against black-box and white-box attacks. We then extend the results to other datasets such as MNIST, SVHN, and Tiny-ImageNet. We also provide additional supporting results in Appendix C, D, E, F, G and J. Additional hyper-parameter studies, such as random restarts and different number of masks, can be found in Appendix I, H and K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Black-box Attacks</head><p>In black-box attacks, the attacker has no access to the network model; it only observes the inputs and outputs. We evaluate ME-Net against three kinds of black-box attacks: • Decision-based attack: We apply the newly proposed Boundary attack <ref type="bibr" target="#b8">(Brendel et al., 2017)</ref> which achieves better performance than transfer-based attacks. We apply 1000 attack steps to ensure convergence.</p><p>• Score-based attack: We also apply the state-of-the-art SPSA attack <ref type="bibr" target="#b33">(Uesato et al., 2018)</ref> which is strong enough to bring the accuracy of several defenses to near zero. We use a batch-size of 2048 to make the SPSA strong, and leave other hyper-parameters unchanged.</p><p>As in past work that evaluates robustness on CIFAR-10  The vectors right before the softmax layer are projected to a 2D plane using t-SNE <ref type="bibr" target="#b25">(Maaten &amp; Hinton, 2008)</ref>.  ( <ref type="bibr" target="#b26">Madry et al., 2017;</ref><ref type="bibr" target="#b9">Buckman et al., 2018)</ref>, we use the standard ResNet-18 model in <ref type="bibr" target="#b20">(He et al., 2016)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">White-box Attacks</head><p>In white-box attacks, the attacker has full information about the neural network model (architecture and weights) and defense methods. To evaluate robustness against such whitebox attacks, we use the BPDA attack proposed in <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>, which has successfully circumvented a number of previously effective defenses, bringing them to near 0 accuracy. Specifically, most defense techniques rely on preprocessing methods which can cause gradient masking for gradient-based attacks, either because the preprocessing is not differentiable or the gradient is useless. BPDA addresses this issue by using a "differentiable approximation" for the backward pass. As such, until now no preprocessing method is effective under white-box attacks. In ME-Net, the backward pass is not differentiable, which makes BPDA the strongest white-box attack.  Table <ref type="table" target="#tab_4">3</ref> shows a comparison of the performance of various preprocessing methods against the BPDA white-box attack. We compare ME-Net with three preprocessing defenses, i.e., the PixelDefend method <ref type="bibr" target="#b31">(Song et al., 2018)</ref>, the Thermometer method <ref type="bibr" target="#b9">(Buckman et al., 2018)</ref>, and the total variation (TV) minimization method <ref type="bibr" target="#b19">(Guo et al., 2017)</ref>. The results in the table for <ref type="bibr" target="#b31">(Song et al., 2018;</ref><ref type="bibr" target="#b9">Buckman et al., 2018)</ref> are directly taken from <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>. Since the TV minimization method is not tested on CIFAR-10, we implement this method using the same setting used with ME-Net. The table shows that preprocessing alone is vulnerable to the BPDA white-box attack, as all schemes perform poorly under such attack. Interestingly however, the table also shows that ME-Net's preprocessing is significantly more robust to BPDA than other preprocessing methods.</p><p>We attribute this difference to that ME-Net's preprocessing step focuses on protecting the global structures in images.</p><p>Next we report the results of white-box attacks on schemes that use adversarial training. One key characteristic of ME-Net is its orthogonality with adversarial training. Note that many preprocessing methods propose combining adversarial training, but the combination actually performs worse than adversarial training alone <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>. Since ME-Net's preprocessing already has a decent accuracy under the strong white-box attacks, we envision a further improvement when combining with adversarial training. We compare ME-Net against two baselines: we compare against <ref type="bibr" target="#b26">(Madry et al., 2017)</ref>, which is the state-of-the-art in defenses against white-box attacks. We also compare with the Thermometer technique in <ref type="bibr" target="#b9">(Buckman et al., 2018)</ref> In contrast, the Thermometer method that also uses preprocessing plus adversarial training cannot survive the strong white-box adversary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Evaluation with Different Datasets</head><p>We evaluate ME-Net on MNIST, SVHN, CIFAR-10, and Tiny-ImageNet and compare its performance across these datasets. For space limitations, we present only the results for the white-box attacks. We provide results for black-box attacks and additional attacks in Appendix C, D, E, and F.</p><p>For each dataset, we use the network architecture and parameters commonly used in past work on adversarial robustness to help in comparing our results to past work. For MNIST, we use the LeNet model with two convolutional layers as in <ref type="bibr" target="#b26">(Madry et al., 2017)</ref>. We also use the same attack parameters as total perturbation scale of 76.5/255 (0.3), and step size 2.55/255 (0.01). Besides using 40 and 100 total attack steps, we also increase to 1000 steps to further strengthen the adversary. For ME-Net with adversarial training, we follow their settings to use 40 steps PGD during training. We use standard ResNet-18 for SVHN and CIFAR-10, and DenseNet-121 for Tiny-ImageNet, and set attack parameters as follows: total perturbation of 8/255 (0.031), step size of 2/255 (0.01), and with up to 1000 total attack steps. Since in <ref type="bibr" target="#b26">(Madry et al., 2017)</ref> the authors did not examine on SVHN and Tiny-ImageNet, we follow their methods to retrain their model on these datasets. We use 7 steps PGD for adversarial training. We keep all the training hyperparameters the same for ME-Net and <ref type="bibr" target="#b26">(Madry et al., 2017)</ref>.</p><p>Fig. <ref type="figure" target="#fig_8">6</ref> shows the performance of ME-Net on the four datasets and compares it with <ref type="bibr" target="#b26">(Madry et al., 2017)</ref>, a state-of-the-art defense against white-box attacks. We plot both the result of a pure version of ME-Net, and ME-Net with adversarial training. The figure reveals the following results. First, it shows that ME-Net with adversarial training outperforms the state-of-the-art defense against white-box attacks. Interestingly however, the gains differ from one dataset to another. Specifically, ME-Net is comparable to <ref type="bibr" target="#b26">(Madry et al., 2017)</ref> on MNIST, provides about 8% gain on CIFAR-10 and Tiny-ImageNet, and yields 23% gain on SVHN.</p><p>We attribute the differences in accuracy gains across datasets to differences in their properties. MNIST is too simple (single channel with small 28×28 pixels), and hence ME-Net and <ref type="bibr" target="#b26">(Madry et al., 2017)</ref> both achieve over 90% accuracy. The other datasets are all more complex and have 3 RGB channels and bigger images. More importantly, Fig. <ref type="figure" target="#fig_0">1</ref> shows that the vast majority of images in SVHN have a very low rank, and hence very strong global structure, which is a property that ME-Net leverages to yield an accuracy gain of 23%. CIFAR-10 and Tiny-ImageNet both have relatively low rank images but not as low as SVHN. The CDF shows that 90% of the images in CIFAR have a rank lower than 5, whereas 90% of the images in Tiny-ImageNet have a rank below 10. When taking into account that the dimension of Tiny-ImageNet is twice as CIFAR (64×64 vs. 32×32), one would expect ME-Net's gain on these datasets to be comparable, which is compatible with the empirical results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Evaluation against Adaptive Attacks</head><p>Since ME-Net provides a new preprocessing method, we examine customized attacks where the adversary takes advantage of knowing the details of ME-Net's pipeline. We propose two kinds of white-box attacks: 1) Approximate input attack: since ME-Net would preprocess the image, this adversary attacks not the original image, but uses the exact preprocess method to approximate/reconstruct an input, and attacks the newly constructed image using the BPDA procedure <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>  the gradient, and then projects the gradient to the low-rank space of the image iteratively, i.e., it projects on the space constructed by the top few singular vectors of the original image, to construct the adversarial noise. Note that these two attacks are based on the BPDA white-box attack which has shown most effective against preprocessing. Table <ref type="table" target="#tab_7">5</ref> shows the results of these attacks, which demonstrates that ME-Net is robust to these adaptive white-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Comparison of Different ME Methods</head><p>Matrix estimation (ME) is a well studied topic with several established ME techniques. The results in the other sections are with the Nuclear Norm minimization algorithm <ref type="bibr" target="#b10">(Candès &amp; Recht, 2009)</ref>. Here we compare the performance of three ME methods: the Nuclear Norm minimization algorithm, the Soft-Impute algorithm <ref type="bibr" target="#b27">(Mazumder et al., 2010)</ref>, and the universal singular value thresholding (USVT) approach <ref type="bibr" target="#b12">(Chatterjee et al., 2015)</ref>.</p><p>We train ME-Net models using different ME methods on CIFAR-10 with ResNet-18. We apply transfer-based PGD black-box attacks with 40 attack steps, as well as whitebox BPDA attack with 1000 attack steps. We compare the complexity, generalization and adversarial robustness of these methods. More details can be found in Appendix H.   different ME methods is slightly different, as more complex algorithms may gain better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Improving Generalization</head><p>As a preprocessing method, ME-Net also serves as a data augmentation technique during training. We show that besides adversarial robustness, ME-Net can also improve generalization (i.e., the test accuracy) on clean data. We distinguish between two training procedures: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Due to the large body of work on adversarial robustness, we focus on methods that are most directly related to our work, and refer readers to the survey <ref type="bibr" target="#b4">(Akhtar &amp; Mian, 2018)</ref> for a more comprehensive and broad literature review.</p><p>Adversarial Training. Currently, the most effective way to defend against adversarial attacks is adversarial training, which trains the model on adversarial examples generated by different kinds of attacks <ref type="bibr" target="#b26">(Madry et al., 2017;</ref><ref type="bibr" target="#b32">Szegedy et al., 2013;</ref><ref type="bibr" target="#b18">Goodfellow et al., 2015)</ref>. Authors of <ref type="bibr" target="#b26">(Madry et al., 2017)</ref> showed that training on adversarial examples generated by PGD with a random start can achieve state-ofthe-art performance on MNIST and CIFAR-10 under ∞ constraint. One major difficulty of adversarial training is that it tends to overfit to the adversarial examples. Authors in <ref type="bibr" target="#b30">(Schmidt et al., 2018)</ref> thus demonstrated and proved that much more data is needed to achieve good generalization under adversarial training. ME-Net can leverage adversarial training for increased robustness. Further its data augmentation capability helps improving generalization.</p><p>Preprocessing. Many defenses preprocess the images with a transformation prior to classification. Typical preprocessing includes image re-scaling <ref type="bibr" target="#b34">(Xie et al., 2018)</ref>, discretization <ref type="bibr">(Chen et al., 2018)</ref>, thermometer encoding <ref type="bibr" target="#b9">(Buckman et al., 2018)</ref>, feature squeezing <ref type="bibr" target="#b35">(Xu et al., 2017)</ref>, image quilting <ref type="bibr" target="#b19">(Guo et al., 2017)</ref>, and neural-based transformations <ref type="bibr" target="#b31">(Song et al., 2018;</ref><ref type="bibr" target="#b29">Samangouei et al., 2018)</ref>. These defenses can cause gradient masking when using gradientbased attacks. However, as shown in <ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>, by applying the Backward Pass Differentiable Approximation (BPDA) attacks designed for obfuscated gradients, the accuracy of all of these methods can be brought to near zero. ME-Net is the first preprocessing method that remains effective under the strongest BPDA attack, which could be attributed to its ability to leverage adversarial training.</p><p>Matrix Estimation. Matrix estimation recovers a data matrix from noisy and incomplete samples of its entries. A classical application is recommendation systems, such as the Netflix problem <ref type="bibr" target="#b6">(Bell &amp; Koren, 2007)</ref>, but it also has richer connections to other learning challenges such as graphon estimation <ref type="bibr" target="#b3">(Airoldi et al., 2013;</ref><ref type="bibr" target="#b7">Borgs et al., 2017)</ref>, community detection (Abbe &amp; Sandon, 2015b;a) and time series analysis <ref type="bibr" target="#b2">(Agarwal et al., 2018)</ref>. Many efficient algorithms exist such as the universal singular value thresholding approach <ref type="bibr" target="#b12">(Chatterjee et al., 2015)</ref>, the convex nuclear norm minimization formulation <ref type="bibr" target="#b10">(Candès &amp; Recht, 2009)</ref> and even non-convex methods <ref type="bibr" target="#b22">(Jain et al., 2013;</ref><ref type="bibr" target="#b15">Chen &amp; Wainwright, 2015;</ref><ref type="bibr" target="#b17">Ge et al., 2016)</ref>. The key promise is that as long as there are some structures underlying the data matrix, such as being low-rank, then exact or approximate recovery can be guaranteed. As such, ME is an ideal reconstruction scheme for recovering global structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced ME-Net, which leverages matrix estimation to improve the robustness to adversarial attacks. Extensive experiments under strong black-box and white-box attacks demonstrated the significance of ME-Net, where it consistently improves the state-of-the-art robustness in different benchmark datasets. Furthermore, ME-Net can easily be embedded into existing networks, and can also bring additional benefits such as improving standard generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The approximate rank of different datasets. We plot the histogram (in red) and the empirical CDF (in blue) of the approximate rank for images in each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. An example of how ME affects the input images. We apply different masks and show the reconstructed images by ME.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. An illustration of ME-Net training and inference process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3. During training, for each image we randomly sample 10 masks with different p values and apply matrix estimation for each masked image to construct the training set. During testing, we sample a single mask with p set to the average of the values used during training, apply the ME-Net pipeline, and test on the reconstructed image. Unless otherwise specified, we use the Nuclear Norm minimization method (Candès &amp; Recht, 2009) for matrix estimation. We experiment with two versions of ME-Net: the first version uses standard stochastic gradient descent (SGD) to train the network, and the second version uses adversarial training, where the model is trained with adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Vanilla under adv. attack. (b) ME-Net under adv. attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Class separation under black-box adversarial attack.The vectors right before the softmax layer are projected to a 2D plane using t-SNE<ref type="bibr" target="#b25">(Maaten &amp; Hinton, 2008)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, intra-class (b) Black-box adv. attack.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The empirical CDF of the distance within and among classes. We quantitatively show the intra-class and inter-class distances between vanilla model and ME-Net on clean data and under black-box adversarial attacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure6. White-box attack results on different datasets. We compare ME-Net with<ref type="bibr" target="#b26">(Madry et al., 2017)</ref> under PGD or BPDA attack with different attack steps up to 1000. We show both the pure ME-Net without adversarial training, and ME-Net with adversarial training. For Tiny-ImageNet, we report the Top-1 adversarial robustness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Transfer-based attack: A copy of the victim network is trained with the same training settings. We apply CW, FGSM and PGD attacks on the copy network to generate black-box adversarial examples. We use the same attack parameters as in<ref type="bibr" target="#b26">(Madry et al., 2017)</ref>: total perturbation ε of 8/255 (0.031), step size of 2/255 (0.01). For PGD attacks, we use 7, 20 and 40 steps. Note that we only consider the strongest transfer-based attacks, i.e., we use white-box attacks on the independently trained copy to generate black-box examples.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>CIFAR-10 black-box results under transfer-based attacks. We compare ME-Net with state-of-the-art defense methods under both SGD and adversarial training.</figDesc><table><row><cell>Since most defenses experimented only with transfer-based</cell></row><row><cell>attacks, we first compare ME-Net to past defenses under</cell></row><row><cell>transfer-based attacks. For comparison, we select a state-</cell></row><row><cell>of-the-art adversarial training defense (Madry et al., 2017)</cell></row><row><cell>and a preprocessing method (Buckman et al., 2018). We</cell></row><row><cell>compare these schemes against ME-Net with standard SGD</cell></row><row><cell>training. The results are shown in Table 1. They reveal that</cell></row><row><cell>even without adversarial training, ME-Net is much more</cell></row><row><cell>robust than prior work to black-box attacks, and can improve</cell></row><row><cell>accuracy by 13% to 25%, depending on the attack.</cell></row><row><cell>To gain additional insight, we look at the separation between</cell></row><row><cell>different classes under black-box transfer-based attack, for</cell></row><row><cell>the vanilla network and ME-Net. Fig. 4(a) and 4(b) show</cell></row><row><cell>the 2D projection of the vectors right before the output layer</cell></row><row><cell>(i.e., softmax layer), for the test data in the vanilla model and</cell></row><row><cell>ME-Net. The figures show that when the vanilla model is</cell></row><row><cell>under attack, it loses its ability to separate different classes.</cell></row></table><note>. In training ME-Net, we experiment with different settings for p. We report the results for p ∈ [0.8, 1] below, and refer the reader to the Appendix for the results with other p values. In contrast, ME-Net can sustain clear separation between classes even in the presence of black-box attack.To further understand this point, we compute the Euclidean distance between classes and within each class. Fig.5 plots</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>CIFAR-10 extensive black-box results. We show significant adversarial robustness of ME-Net under different strong black-box attacks.</figDesc><table /><note>the empirical CDFs of the intra-class and inter-class distance between the vectors before the output layer, for both the vanilla classifier and ME-Net. The figure shows results for both clean data and adversarial examples. Comparing ME-Net (in red) with the vanilla classifier (in blue), we see that ME-Net both reduces the distance within each class, and improves the separation between classes; further this result applies to both clean and adversarial examples. Overall, these visualizations offer strong evidence supporting the improved robustness of ME-Net. Finally, we also evaluate ME-Net under other strong blackbox attacks.Table 2 summarizes these results demonstrating that ME-Net consistently achieves high robustness under different black-box attacks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>White-box attack against pure preprocessing schemes. We use PGD or BPDA attacks in white-box setting. Compared to other pure preprocessing methods, ME-Net can increase robustness by a significant margin. *Data from<ref type="bibr" target="#b5">(Athalye et al., 2018)</ref>.</figDesc><table><row><cell>For white box attacks, we distinguish two cases: defenses</cell></row><row><cell>that use only preprocessing (without adversarial training),</cell></row><row><cell>and defenses that incorporate adversarial training. All de-</cell></row><row><cell>fenses that incorporate adversarial training, including ME-</cell></row><row><cell>Net, are trained with PGD with 7 steps.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>White-box attack results for adversarial training. We use 1000 steps PGD or BPDA attacks in white-box setting to ensure the results are convergent. ME-Net achieves state-of-the-art white-box robustness when combined with adversarial training.the attacker, we allow it to use the strongest possible attack, i.e., it uses BPDA with 1000 PGD attack steps to ensure the results are convergent. Note that previous defenses (including the state-of-the-art) only consider up to 40 steps. Table 4 summarizes the results. As shown in the table, ME-Net combined with adversarial training outperforms the state-of-the-art results under white-box attacks, achieving a 52.8% accuracy with ResNet and a 55.1% accuracy with WideResNet.</figDesc><table><row><cell>Network</cell><cell>Method</cell><cell>Type</cell><cell cols="2">Steps Accuracy</cell></row><row><cell></cell><cell>Madry</cell><cell>Adv. train</cell><cell>1000</cell><cell>45.0%</cell></row><row><cell>ResNet-18</cell><cell>ME-Net</cell><cell cols="2">Prep. + Adv. train 1000</cell><cell>52.8%</cell></row><row><cell></cell><cell>Madry</cell><cell>Adv. train</cell><cell>1000</cell><cell>46.8%</cell></row><row><cell>WideResNet</cell><cell cols="3">Thermometer Prep. + Adv. train 1000</cell><cell>12.3%</cell></row><row><cell></cell><cell>ME-Net</cell><cell cols="2">Prep. + Adv. train 1000</cell><cell>55.1%</cell></row></table><note>, which like ME-Net, combines a preprocessing step with adversarial training. For all compared defenses, adversarial training is done using PGD with 7 steps. We also use BPDA to approximate the gradients during the backward pass. For our comparison we use ResNet-18 and its wide version since they were used in past work on robustness with adversarial training. As for</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. 2) Projected BPDA attack: since ME-Net focuses on the global structure of an image, this adversary aims to attack directly the main structural space of the image. Specifically, it uses BPDA to approximate</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell cols="3">Steps Approx. Input Projected BPDA</cell></row><row><cell>ME-Net</cell><cell cols="2">Pure Adversarial 1000 1000</cell><cell>41.5% 62.5%</cell><cell>64.9% 74.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Results of ME-Net against adaptive white-box attacks on CIFAR-10. We use 1000 steps PGD-based BPDA for the two newly proposed attacks, and report the accuracy of ME-Net.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>shows the results of our comparison. The table shows that all the three ME methods are able to improve the original standard generalization, and achieve almost the same test accuracy. The nuclear norm minimization algorithm takes much longer time and more computation power. The Soft-Impute algorithm simplifies the process but still requires certain computation resources, while the USVT approach is much simpler and faster. The performance of</figDesc><table><row><cell>Method</cell><cell cols="4">Complexity Clean Black-box White-box</cell></row><row><cell>Vanilla</cell><cell>−</cell><cell>93.4%</cell><cell>0.0%</cell><cell>0.0%</cell></row><row><cell>ME-Net -USVT</cell><cell>Low</cell><cell>94.8%</cell><cell>89.4%</cell><cell>51.9%</cell></row><row><cell>ME-Net -Soft-Imp.</cell><cell>Medium</cell><cell>94.9%</cell><cell>91.3%</cell><cell>52.3%</cell></row><row><cell>ME-Net -Nuc. Norm</cell><cell>High</cell><cell>94.8%</cell><cell>91.0%</cell><cell>52.8%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>Comparisons between different ME methods. We report the generalization and adversarial robustness of three ME-Net models using different ME methods on CIFAR-10. We apply transfer-based 40 steps PGD attack as black-box adversary, and 1000 steps PGD-based BPDA as white-box adversary.</figDesc><table><row><cell>Method</cell><cell>Training</cell><cell cols="4">MNIST CIFAR-10 SVHN Tiny-ImageNet</cell></row><row><cell>Vanilla</cell><cell>Pure</cell><cell>98.8%</cell><cell>93.4%</cell><cell>95.0%</cell><cell>66.4%</cell></row><row><cell>ME-Net</cell><cell>Pure</cell><cell>99.2%</cell><cell>94.9%</cell><cell>96.0%</cell><cell>67.7%</cell></row><row><cell>Madry</cell><cell>Adversarial</cell><cell>98.5%</cell><cell>79.4%</cell><cell>87.4%</cell><cell>45.6%</cell></row><row><cell cols="3">ME-Net Adversarial 98.8%</cell><cell>85.5%</cell><cell>93.5%</cell><cell>57.0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Generalization performance on clean data. For each dataset, we use the same network for all the schemes. ME-Net improves generalization for both adversarial and non-adversarial training. For Tiny-ImageNet, we report the Top-1 accuracy.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">To be consistent with literature, we generate adversarial examples from the whole dataset and use all of them to report accuracy.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank the anonymous reviewers for their helpful comments in revising the paper. We are grateful to the members of NETMIT and CSAIL for their insightful discussions and supports. Zhi Xu is supported by the Siemens FutureMakers Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Community detection in general stochastic block models: Fundamental limits and efficient algorithms for recovery</title>
		<author>
			<persName><forename type="first">Abbe</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="670" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recovering communities in the general stochastic block model without knowing the parameters</title>
		<author>
			<persName><forename type="first">Abbe</forename></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="page" from="676" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Model agnostic time series analysis via matrix estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Amjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS performance evaluation review</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic blockmodel approximation of a graphon: Theory and consistent estimation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="692" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00553</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.00420" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">2018. July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lessons from the netflix prize challenge</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<idno type="DOI">10.1145/1345448.1345465</idno>
		<ptr target="http://doi.acm.org/10.1145/1345448.1345465" />
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<idno type="ISSN">1931-0145</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="79" />
			<date type="published" when="2007">2007</date>
			<publisher>December</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Thy friend is my friend: Iterative collaborative filtering for sparse matrix estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4715" to="4726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04248</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Thermometer encoding: One hot way to resist adversarial examples</title>
		<author>
			<persName><forename type="first">J</forename><surname>Buckman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">S18S</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exact matrix completion via convex optimization</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations of Computational mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">717</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix estimation by universal singular value thresholding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="214" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving adversarial robustness by data-specific discretization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<idno>CoRR, abs/1805.07816</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Harnessing structures in big data via guaranteed low-rank matrix estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08397</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03025</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An overview of low-rank matrix recovery from incomplete observations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06422</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Matrix completion has no spurious local minimum</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2973" to="2981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6572" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00117</idno>
		<title level="m">Countering adversarial images using input transformations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Low-rank matrix completion using alternating minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Netrapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanghavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the forty-fifth annual ACM symposium on Theory of computing</title>
				<meeting>the forty-fifth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="665" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matrix completion from noisy entries</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Keshavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2057" to="2078" />
			<date type="published" when="2010-07">Jul. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Certified robustness to adversarial examples with differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lecuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atlidakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Geambasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03471</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral regularization algorithms for learning large incomplete matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2287" to="2322" />
			<date type="published" when="2010-08">Aug. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Logit pairing methods can fool gradientbased attacks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Trost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Defensegan: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarially robust generalization requires more data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1804.11285" />
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName><surname>Pixeldefend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05666</idno>
		<title level="m">Adversarial risk and the dangers of evaluating against weak attacks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Feature squeezing: Detecting adversarial examples in deep neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01155</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
