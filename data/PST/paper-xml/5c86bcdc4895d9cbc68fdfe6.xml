<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuous Probability Distribution Prediction of Image Emotions via Multi-Task Shared Sparse Regression</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
							<email>schzhao@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Hongxun</forename><surname>Yao</surname></persName>
							<email>h.yao@hit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<email>gaoyue@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<email>rrji@xmu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Bertini</forename><forename type="middle">S</forename><surname>Marco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">G</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><surname>Ding</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Cognitive Science</orgName>
								<orgName type="department" key="dep2">School of Information Science and Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Continuous Probability Distribution Prediction of Image Emotions via Multi-Task Shared Sparse Regression</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">857D6C3E6DFD9E0668DE3BD06C1EA4FC</idno>
					<idno type="DOI">10.1109/TMM.2016.2617741</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2016.2617741, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 1 received January 18, 2016; revised April 30, 2016 and August 9, 2016; accepted October 5, 2016. Date of publication October 15, 2016; date of current version October 15, 2016. This work was supported by the This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2016.2617741, IEEE Transactions on Multimedia IEEE TRANSACTIONS ON MULTIMEDIA 3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image emotion</term>
					<term>probability distribution</term>
					<term>valence-arousal</term>
					<term>Gaussian mixture model</term>
					<term>shared sparse regression</term>
					<term>multi-task learning london</term>
					<term>stormy</term>
					<term>dramatic weather</term>
					<term>… Emotion category: fear Sentiment: negative Valence: 4.1956 Arousal: 4.4989 Dominance: 4.8378 Comments from different viewers Personalized emotions Wow</term>
					<term>that is fantastic...it looks so incredible</term>
					<term>….. That sky is amazing. Emotion: awe</term>
					<term>Sentiment: positive V: 7.121 A: 4.479 D: 6.635 Yup a fave for me as well. Exciting drama at its best. Emotion: excitement</term>
					<term>Sentiment: positive V: 7.950 A: 6.950 D: 7.210 Hey</term>
					<term>it really frightened me! My little daughter just looked scared. Emotion: fear</term>
					<term>Sentiment: negative V: 2.625 A: 5.805 D: 3.625</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Previous works on image emotion analysis mainly focused on predicting the dominant emotion category or the average dimension values of an image for affective image classification and regression. However, this is often insufficient in various realworld applications, as the emotions that are evoked in viewers by an image are highly subjective and different. In this paper, we propose to predict the continuous probability distribution of image emotions which are represented in dimensional valencearousal space. We carried out large-scale statistical analysis on the constructed Image-Emotion-Social-Net dataset, on which we observed that the emotion distribution can be well modelled by a Gaussian mixture model. This model is estimated by an expectation-maximization algorithm with specified initializations. Then we extract commonly used emotion features at different levels for each image. Finally, we formalize the emotion distribution prediction task as a shared sparse regression (SSR) problem and extend it to multi-task settings, named multi-task shared sparse regression (MTSSR), to explore the latent information between different prediction tasks. SSR and MTSSR are optimized by iteratively reweighted least squares. Experiments are conducted on the Image-Emotion-Social-Net dataset with comparisons to three alternative baselines. The quantitative results demonstrate the superiority of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(c) are the expected emotion labels that we assign to the uploader using the keywords in (b) in red. (d) are the comments to (a) from different viewers and the personalized emotion labels that we obtain using the keywords in red. (e) illustrates the differences, where the hollow points are the perceived emotion labels in VA space, the blue square and magenta diamond points are the target average VA scores by the traditional affective image regression methods using different strategies of obtaining labels, while the contour lines of GMM are the target emotion distribution by the proposed method.</p><p>hand, affective gap can be defined as "the lack of coincidence between the measurable signal properties, commonly referred to as features, and the expected affective state in which the user is brought by perceiving the signal" <ref type="bibr" target="#b3">[4]</ref>. On the other hand, subjective evaluation refers to the fact that "people may have different evoked emotions on the same image due to the difference of social and cultural backgrounds" <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>.</p><p>Existing work mainly focused on finding features that can express emotions better to bridge the affective gap. To this end, such methods fall into the traditional classification or regression scenarios, trying to assign the dominant emotion category or the average dimension values to an image. However, predicting only the dominant emotion is insufficient in many applications, as the emotions that are evoked in different viewers by an identical image are highly subjective and different <ref type="bibr" target="#b6">[7]</ref>, due to various contextual factors, such as social and cultural influence <ref type="bibr" target="#b4">[5]</ref>. For example, an image of stormy weather (Figure <ref type="figure" target="#fig_0">1</ref> (a)) may evoke feelings of excitement to some observers who like photographing of marvellous phenomenon, but fear in others who are afraid of thunder. Under such a circumstance, it is natural to take the subjective evaluation into account. This refers to predicting the personalized emotion perceptions for user-centric computing and predicting the probability distribution of image emotions for image-centric computing. To the best of our knowledge, there are few works tackling the subjective evaluation challenge on predicting the personalized perceptions as well as the probability distribution of image emotions.</p><p>In this paper, we make an initial attempt to predict the continuous probability distribution of image emotions represented in dimensional valence-arousal space, as shown in Figure <ref type="figure" target="#fig_0">1</ref>. To accomplish this task, we construct a large-scale personalized image emotion dataset, named Image-Emotion-Social-Net, with images fully automatically crawled from Flickr. Related lexicon-based text emotions are viewed as personalized perceptions of image emotions. By the statistical analysis of personalized emotion perceptions on the Image-Emotion-Social-Net dataset, we observe that the valencearousal emotion labels can be well represented by a Gaussian mixture model (GMM), i.e. a mixture of bidimensional Gaussian distributions. The expectation-maximization (EM) algorithm with specified initializations is then used to estimate the parameters of GMM. Subsequently, the emotion distribution prediction is formalized as a shared sparse regression (SSR) problem. SSR is then extended to multi-task setteings as multitask shared sparse regression (MTSSR), which can predict the emotion distribution for multiple images simultaneously. Iteratively reweighted least squares is adopted to optimize SSR and MTSSR. Commonly used visual emotion features of three different levels are extracted. We conduct experiments on the constructed Image-Emotion-Social-Net dataset to demonstrate the effectiveness of the proposed method.</p><p>The contributions of this paper are three-fold: 1. Different from traditional methods for affective image regression, we propose to predict the continuous probability distribution of image emotions, which can be viewed as an initial attempt to tackle the subjective evaluation challenge. 2. (Multi-task) shared sparse regression together with three baseline methods are presented as learning models to predict the continuous probability distribution of image emotions. Iteratively reweighted least squares is used to optimize (MT)SSR. 3. We construct and release a large-scale emotion distribution dataset containing 18,700 images. The statistical analysis on this dataset further demonstrates the necessity of subjective evaluation. The experimental results on this dataset demonstrate that the proposed (MT)SSR outperforms several state-of-the-art methods. One preliminary version on continuous probability distribution prediction of image emotions was first introduced in our previous work <ref type="bibr" target="#b8">[9]</ref>. In this paper we have improvements in four aspects: <ref type="bibr" target="#b0">(1)</ref> we perform a more comprehensive survey of related works; <ref type="bibr" target="#b1">(2)</ref> we provide the detailed dataset construction process and more statistical analysis; <ref type="bibr" target="#b2">(3)</ref> we improve the methods of SSR and MTSSR by adding constraints on the sparse coefficients to ensure that in predicted GMM the mixing coefficients sum to 1 and the covariance matrixes are positive definite; and (4) we conduct more comparative experiments and enrich the analysis of the results.</p><p>The remainder of this paper is organized as follows: Section II reviews related work. The motivation of this paper, including the constructed Image-Emotion-Social-Net dataset and its related statistical analysis, and the problem definition of emotion distribution prediction are described in Section III. We present the feature extraction and the emotion distribution prediction models, including the proposed (MT)SSR and three baseline methods in Section IV and V, respectively. Experimental evaluation and analysis are given in Section VI, followed by the conclusion and future work in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Image emotion and sentiment analysis. To analyze emotions from a given image, there are two widely used models: categorical emotion states (CES) and dimensional emotion space (DES). CES methods model emotions as one of a few basic categories <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b16">[17]</ref>, while DES methods employ 3-D or 2-D space to represent emotions, such as valence-arousaldominance (VAD) <ref type="bibr" target="#b17">[18]</ref>, natural-temporal-energetic <ref type="bibr" target="#b18">[19]</ref> and valence-arousal (VA) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b12">[13]</ref>. VAD is the most widely used DES, where valence represents the pleasantness of a stimulus ranging from happy to unhappy, arousal represents the intensity of emotion provoked by a stimulus ranging from excited to calm, while dominance represents the degree of control exerted by a stimulus ranging from controlled to in control <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. Specifically, image emotion is often called image sentiment for binary classification (positive or negative) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Accordingly, related work on image emotion analysis can be classified into three different tasks: affective image classification <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, regression <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref> and retrieval <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Discrete probability distribution has been preliminarily investigated in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> based on CES models. We model image emotions using dimensional valence-arousal representations to predict continuous probability distributions.</p><p>From a feature's view point, visual features are designed and extracted of different levels, i.e., the different aspects or extends of image representations <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>. Low-level holistic features including Wiccest features and Gabor features were extracted to classify image emotions in <ref type="bibr" target="#b11">[12]</ref>. Machajdik et al. <ref type="bibr" target="#b10">[11]</ref> extracted features inspired from psychology and art theory, such as color, texture and composition. Lu et al. <ref type="bibr" target="#b12">[13]</ref> investigated the computability of emotion through shape features. Zhao et al. <ref type="bibr" target="#b1">[2]</ref> proposed to extract more interpretable mid-level emotion features based on principlesof-art, such as balance, contrast, harmony and variety. Visual sentiment ontology and detectors are proposed to detect highlevel adjective noun pairs based on large-scale social multimedia data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Yuan et al. <ref type="bibr" target="#b22">[23]</ref> used mid-level scene attributes for binary sentiment classification. Simple social correlation features are explored for emotion classification of social network images <ref type="bibr" target="#b15">[16]</ref>. As a special case of image emotion, facial expression recognition is also widely studied in recent years <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b35">[36]</ref>. Tkalcic et al. <ref type="bibr" target="#b36">[37]</ref> proposed to obtain the affective labels of images based on users' facial emotion expressions. We extract commonly used emotion features of three different levels <ref type="bibr" target="#b25">[26]</ref> to test their performances for emotion distribution prediction.</p><p>Based on the extracted features, state-of-the-art methods tried to assign a dominant emotion category or the average dimension values to an image for affective image classification and regression with CES and DES models, respectively. The commonly used models are based on machine learning methods, such as Naive Bayes <ref type="bibr" target="#b10">[11]</ref>, support vector machine (SVM) or support vector regression (SVR) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, sparse learning <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>, multi-graph learning <ref type="bibr" target="#b25">[26]</ref> and convolutional neural network (regression) (CNN(R)) <ref type="bibr" target="#b5">[6]</ref>. We present shared sparse regression for emotion distribution prediction and extend it to multi-task settings.</p><p>Note that affective content analysis has also been widely studied based on other types of input data, such as text <ref type="bibr" target="#b37">[38]</ref>, speech <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, music <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b43">[44]</ref> and videos <ref type="bibr" target="#b44">[45]</ref>- <ref type="bibr" target="#b48">[49]</ref>.</p><p>Probability distribution prediction. In many applications of machine learning, it would be more reasonable and useful to predict the probability distribution for a target variable rather than simply the most likely value for that variable <ref type="bibr" target="#b49">[50]</ref>. Probability distribution prediction has been studied in some areas, such as surf height <ref type="bibr" target="#b49">[50]</ref>, user behavior <ref type="bibr" target="#b50">[51]</ref> and spike events <ref type="bibr" target="#b51">[52]</ref>. According to probability theory, there are typically two types of probability distributions: discrete probability distribution and continuous probability distribution. Generally, the distribution prediction task can be formalized as a regression problem. For different emotion representation models, the distribution prediction varies slightly. For CES, the task aims to predict the discrete probability of different emotion categories, the sum of which is equal to 1. CNNR and shared sparse learning are recently used to predict the discrete probability distribution of image emotions in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b6">[7]</ref>. For DES, the task usually transfers to predict the parameters of specified continuous probability distribution, the form of which should be firstly decided, such as Gaussian distribution and exponential distribution. In this paper, we focus on the latter one, i.e., predicting the continuous probability distributions of image emotions.</p><p>Sparse learning and multi-task learning. Sparse learning represents the target variable as a sparsely linear combination of a set of basis functions and is widely used in many areas, such as face recognition <ref type="bibr" target="#b52">[53]</ref>, visual classification <ref type="bibr" target="#b53">[54]</ref> and emotion analysis <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Meanwhile, in many real-world applications, some classification/regression/clustering tasks may be related to each other <ref type="bibr" target="#b54">[55]</ref>. For example, in the prediction of therapy outcome, the tasks of predicting the effectiveness of several combinations of drugs are related <ref type="bibr" target="#b55">[56]</ref>. Traditional single-task learning methods solve these tasks independently, which ignores the task relatedness. Learning these tasks simultaneously by extracting and utilizing appropriate shared information across different tasks has been empirically <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> as well as theoretically <ref type="bibr" target="#b58">[59]</ref> proved to often significantly improve performances relative to single-task learning. A survey on multi-task learning can be referred to <ref type="bibr" target="#b54">[55]</ref>. By combining sparse learning and multi-task learning, Wang et al. <ref type="bibr" target="#b59">[60]</ref> proposed sparse multitask regression for brain imaging identification. Similarly, we present multi-task shared sparse regression but with different optimization functions (using 0 -norm instead of 1 -norm) and constraints for emotion distribution prediction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM DESCRIPTION</head><p>In this section, we introduce the dataset (Image-Emotion-Social-Net<ref type="foot" target="#foot_0">1</ref> ) on emotions of social images and describe the problem definition of continuous distribution prediction of image emotions.</p><p>A. The Image-Emotion-Social-Net Dataset 1) Dataset Construction: We downloaded 21,066,920 images from Flickr with 2,060,357 users belonging to 264,683 groups. Each image is associated with the metadata, such as the title, tags, taken time and location if available. Each user is associated with the personal information, the contact list and the group list they joined in. As how to measure emotions is still far from consensus in research community <ref type="bibr" target="#b60">[61]</ref>, we defined emotions using both categorical and dimensional representations. For CES, we used the 8 categories rigorously defined in psychology <ref type="bibr" target="#b61">[62]</ref>, including 4 negative and 4 positive emotions. To get the ground truth labels, we adopted keywords based searching strategy as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. Tens of keywords for each emotion category are obtained from a public synonym searching site<ref type="foot" target="#foot_1">2</ref> and are manually verified, with examples shown in Table <ref type="table">I</ref>. Expected emotions of the image uploaders are firstly considered. The keywords are searched from the title, tags and descriptions given by the uploaders. The emotion category with the most frequent keywords is set as the ground truth of expected emotions from the uploaders, as shown in Figure <ref type="figure" target="#fig_0">1 (b)</ref> and<ref type="figure">(c</ref>).</p><p>As we focus on personalized emotion perception, we then searched from all the comments of related images to get the actual emotion labels of each viewer. We removed the images if the searched title, tags or descriptions contain negation adjacent and prior to the target keywords, such as "I am not happy". Similarly, we also removed the comments with negation adjacent and prior to the target keywords. Note that the labels of an image for a specific user are allowed to have different emotion categories (such as fear, disgust) but must have only one sentiment (positive or negative). Then we computed the average value of valence, arousal and dominance as ground truth for dimensional emotion representation based on recently published VAD norms of 13,915 English lemmas <ref type="bibr" target="#b20">[21]</ref>, as shown in Figure <ref type="figure" target="#fig_0">1 (d)</ref>. Besides, we also gave the sentiment categories (positive or negative). We combined the expected emotions and actual emotions of all involved images for each  If one user is the uploader of an image, then the emotion of the metadata text (title, tags and descriptions) is the personalized emotion of this user, which is also the expected emotion that is expected to evoke in other viewers by this user. If one user is a viewer of an image, then the emotion of the comment is the personalized emotion of this user.</p><p>2) Dataset Validation: To validate the quality of the dataset, we did a crowdsourcing experiment on discrete emotions. For each emotion category, we randomly selected 200 images with associated titles, tags and descriptions for expected emotions, and 200 comments with corresponding images for personalized emotions. 5 graduate students (3 males, 2 females) were invited to judge whether the text was used to express the assigned emotions of related images. To facilitate this judgement, they were asked simple question like "Do you think that the text is used to express excitement for this image?", and they just needed to choose YES or NO. The result is shown in Figure <ref type="figure">3</ref>. We can find that for both expected and personalized emotions, on average more than 88% of emotion labels receive at least 3 Yes's, which verifies the quality of the constructed dataset. In such cases, the expected emotion labels are 3.5% more accurately assigned than personalized emotions. To assess the inter-rater agreement, we also calculate the Fleiss' kappa <ref type="foot" target="#foot_2">3</ref> of the 5 annotators. The average Fleiss' kappa (the standard deviation) for the 8 emotion categories of expected emotions and personalized emotions are 0.2297 (0.0748) and 0.3224 (0.1411), respectively.</p><p>3) Statistics of Dataset: The distribution of images per emotion category is shown in Table <ref type="table" target="#tab_1">II</ref>, where the first four columns represent the number of images in each of the 8 emotions; while the last column is the number of images with binary sentiments. We can find that the number of negative emotions is relatively small. The distribution of valence, arousal (without showing dominance here) is illustrated in Figure <ref type="figure">4</ref>(a), which looks like a petal or heart, similar to the emotion space in <ref type="bibr" target="#b19">[20]</ref>. The user distribution based on involved images is shown in Figure <ref type="figure">4(b)</ref>.</p><p>Totally we select 18,700 images with more than 20 VA labels each for the experiments on emotion distribution pre-diction. The distribution of emotion numbers for these images is shown in Figure <ref type="figure">2</ref> We can find that the emotion perceptions of different users are truly subjective and personalized.</p><p>We also analyze the relation between the expected and personalized emotions. For each of the images with more than 20 labels, we compute the Euclidean distances between personalized emotions and expected emotion in VA space, and average all the distances. The histogram of the average VA distance is shown in Figure <ref type="figure">5(a)</ref>. For CES, we count the proportion of personalized emotions that are different from expected emotion for each image. The histogram of different emotion proportions is illustrated in Figure <ref type="figure">5(b)</ref>. It is clear that there exists great inconsistency between expected and personalized emotions. Some image examples with high different emotion proportions are shown in Figure <ref type="figure" target="#fig_6">7</ref>.</p><p>4) Challenging Tasks: The challenging tasks that can be performed by researchers on this dataset include, but not limited to, the following aspects:</p><p>• Image-centric emotion analysis. For each image, we can predict the dominant or expected emotion category like traditional affective image classification. Besides, we can predict the emotion distribution of each image, taking the normalized emotion proportion as the ground truth. • User-centric emotion prediction. For each user, we can predict her personalized emotion perception of some specific images. The above two tasks can be extended to regression tasks, all of which can be done using visual, social, temporal and the combination of all features. • Emotion related data mining and applications. This dataset contains visual and social information to support research on emotion influence mining, social advertising and affective image retrieval, etc. For different tasks, the roles of expected emotions and actual emotions are different. For image-centric expected emotion analysis, only the expected emotions can be used. For imagecentric dominant emotion analysis or emotion distribution analysis, the expected emotions can be viewed as one type of actual emotions. For user-centric emotion prediction, the expected emotions can also be viewed as one type of actual emotions, but only for the uploaders of related images.</p><p>In this paper, we focus on the image-centric emotion distribution analysis, trying to predict the continuous probability distribution of image emotions when perceived by large quantity of viewers, including the image uploaders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Problem Definition</head><p>From Figures <ref type="figure">6(a</ref> Based on these observations, we define the distribution of VA emotion labels as a GMM by</p><formula xml:id="formula_0">p(x; θ) = L l=1 π l N (x|µ l , Σ l ),<label>(1)</label></formula><p>where x = (v, a) is pair-wise VA emotion labels, µ l and Σ l are the mean vector and covariance matrix of the lth Gaussian component, while π l is the mixing coefficient, which satisfies π l ≥ 0 and L l=1 π l = 1. In this paper, the number of Gaussian components is 2, i.e. L = 2 and θ</p><formula xml:id="formula_1">= (µ 1 , Σ 1 , µ 2 , Σ 2 , π 1 , π 2 ).</formula><p>It should be noted that the number of Gaussian components L can be easily enlarged if more personalized emotion labels are obtained.</p><p>The EM algorithm is used to estimate the parameters of GMM. Specifically, the initializations are obtained by firstly partitioning the VA labels into two clusters based on whether valence is greater than 5, the typical boundary of positive and negative sentiments <ref type="bibr" target="#b20">[21]</ref>, and then computing the mean vector µ l and covariance matrix Σ l of each cluster. The mixing coefficients are set as the proportions of related VA labels in each cluster to the total labels. In experiment, the EM algorithm is converged in 6.28 steps on average without overfitting. Some estimated results of GMM and detailed parameter values are shown in Figure <ref type="figure">6(c</ref>).</p><p>Suppose we have</p><formula xml:id="formula_2">N training images f 1 , • • • , f N , the emo- tion distributions are p 1 (x; θ 1 ), • • • , p N (x; θ N ), where θ n = (µ n1 , Σ n1 , µ n2 , Σ n2 , π n1 , π n2 ) are the parameters of the nth emotion distribution (n = 1, • • • , N ). Similarly, suppose we have M test images g 1 , • • • , g M with ground truth emo- tion distributions q 1 (x; ϑ 1 ), • • • , q M (x; ϑ M ), where ϑ m (m = 1, • • • , M</formula><p>) are the distribution parameters. Then our goal is to predict the emotion distribution parameters ϑ m based on {f n , θ n } N n=1 for each g m . That is</p><formula xml:id="formula_3">f : ({fn, θn} N n=1 , gm) → ϑm.<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXTRACTED EMOTION FEATURES</head><p>As shown in <ref type="bibr" target="#b25">[26]</ref>, there are various types of features that may contribute to the perceptions of image emotions. Similar to <ref type="bibr" target="#b25">[26]</ref>, we extract commonly used emotion features of different levels and generalities for each image, including low-level GIST <ref type="bibr" target="#b62">[63]</ref> and elements-of-art <ref type="bibr" target="#b10">[11]</ref>, mid-level attributes <ref type="bibr" target="#b63">[64]</ref> and principles-of-art <ref type="bibr" target="#b1">[2]</ref>, and high-level ANPs <ref type="bibr" target="#b0">[1]</ref> and expressions <ref type="bibr" target="#b29">[30]</ref>.</p><p>Low-level features suffer from the difficulty of easy interpretation and the link to emotions is weak <ref type="bibr" target="#b25">[26]</ref>. In this paper, we just extract GIST as generic feature, one of the most commonly used features, for its relatively powerful description ability of visual phenomena in a scene perspective <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>.</p><p>We extract special features derived from elements of art, including color and texture <ref type="bibr" target="#b10">[11]</ref>. Low-level color features include mean saturation and brightness, vector based mean hue, emotional coordinates (pleasure, arousal and dominance)   . Image examples with high different proportions (&gt;0.9) between personalized and expected emotions, which indicates that the personalized emotion perceptions greatly differ from the expected emotions, probably due to different responses to the visual semantics (see the metadata and comments in Fig. <ref type="figure" target="#fig_0">1</ref>).</p><p>based on brightness and saturation, colorfulness and color names. Low-level texture features include Tamura texture, Wavelet textures and gray-level co-occurrence matrix (GLCM) based texture <ref type="bibr" target="#b10">[11]</ref>.</p><p>Mid-level features are more semantic, more interpretable and have stronger link to emotions than low-level features <ref type="bibr" target="#b1">[2]</ref>. Recently, attribute based representation has been widely studied for its intuitive interpretation and cross-category generalization property in visual recognition domain <ref type="bibr" target="#b63">[64]</ref>- <ref type="bibr" target="#b65">[66]</ref>. We extract 102 dimensional attributes which are commonly used by humans to describe scenes as mid-level generic features. As in <ref type="bibr" target="#b63">[64]</ref>, the attributes can be classified into five types: materials (mental), surface properties (dirty), functions or affordances (reading), spatial envelop attributes (cluttered) and object presence (flowers). GIST features and SVM implemented in Liblinear toolbox 4 are used to train attribute classifiers based on 14,340 images in SUN database <ref type="bibr" target="#b62">[63]</ref>.</p><p>Features inspired from principles of art, including balance, contrast, harmony, variety, gradation, and movement are extracted as mid-level special features <ref type="bibr" target="#b1">[2]</ref>. These artistic principles are used to arrange and orchestrate artistic elements in art theory for describing specific semantics and emotions and are proved to have stronger link to emotions than elements. Please refer to <ref type="bibr" target="#b1">[2]</ref> for detailed implementations.</p><p>High-level features are the detailed semantic contents contained in images. People can easily understand the emotions conveyed in images by recognizing the semantics. Concepts described by 1,200 adjective noun pairs (ANPs) <ref type="bibr" target="#b0">[1]</ref> are extracted as generic features. The ANPs are detected by a large detector library SentiBank <ref type="bibr" target="#b0">[1]</ref>, which is trained on about 500k images downloaded from Flickr using various low-level features, including GIST, color histogram, LBP descriptor, attribute, etc. Liblinear SVM is used as classifier by early fusion. Finally, we obtain a 1,200 dimensional vector describing the probability that each ANP is detected.</p><p>Motivated by the conclusion that facial expressions may determine the emotions of the images containing faces <ref type="bibr" target="#b25">[26]</ref>, we also extract 8 kinds of facial expressions (anger, contempt, disgust, fear, happiness, sadness, surprise, neutral) <ref type="bibr" target="#b66">[67]</ref> as high-level special features. Compositional features of local Haar appearance features are built by a minimum error based optimization strategy, which are embedded into an improved AdaBoost algorithm <ref type="bibr" target="#b29">[30]</ref>. Trained on CK+ database <ref type="bibr" target="#b66">[67]</ref>, the method performs well even on low intensity expressions <ref type="bibr" target="#b29">[30]</ref>. Face detection is firstly conducted using the Viola-Jones  algorithm <ref type="bibr" target="#b67">[68]</ref> to decide whether an image contains faces. Finally, we can get a 8 dimensional vector, each of which represents the proportion of related facial expressions.</p><p>The extracted features are abbreviated as GIST, Elements, Attributes, Principles, ANP and Expressions with dimension 512, 48, 102, 165, 1200 and 8, respectively, as summarized in Table <ref type="table" target="#tab_4">III</ref>. Please refer to <ref type="bibr" target="#b25">[26]</ref> for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EMOTION DISTRIBUTION PREDICTION ALGORITHMS</head><p>The emotion distribution prediction task of Eq. ( <ref type="formula" target="#formula_3">2</ref>) can be viewed as a regression problem. We detail the proposed MTSSR method together with several baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline A: Global Weighting</head><p>The idea of global weighting (GW) is simple and direct. The emotion distribution parameters θ n (n = 1, • • • , N ) of all training images are considered as basis functions. The test distribution parameter ϑ m is computed by weighting all the basis functions as follows</p><formula xml:id="formula_4">ϑm = N n=1 snθn N n=1 sn ,<label>(3)</label></formula><p>where s n = exp(-d(g m , f n )/σ) is the similarity between images g m and f n , d(•, •) is a specified distance function, while σ is set as the average distance of all the training images. In experiment, the Euclidean distance is used for d(•, •) and each θ n , ϑ m is reshaped as a column vector for convenience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline B: K-Nearest Neighbor Weighting</head><p>Different from GW, K-nearest neighbor weighting (KNNW) just weighs K instead of all basis functions by selecting the top K most similar training images. Suppose the top</p><formula xml:id="formula_5">K largest similarities in [s 1 , • • • , s N ] are s t1 , • • • , s t K ,</formula><p>then the test parameter ϑ m estimated by K-nearest neighbor weighting is computed by</p><formula xml:id="formula_6">ϑm = K k=1 st k θt k K k=1 st k .<label>(4)</label></formula><p>When K == N , KNNW turns to GW.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline C: Support Vector Regression</head><p>Support vector regression (SVR) aims to find support vectors which lie on the maximum margin hyperplanes in feature space and contribute to predictions. Training SVR means solving</p><formula xml:id="formula_7">min 1 2 wi 2, s.t. θni -wi, fn -b ≤ , wi, fn + b -θni ≤ ,<label>(5)</label></formula><p>where the target value θ ni is the ith component of θ n (n = 1, • • • , N ), the inner product plus intercept w i , f n + b is the prediction for that sample, and is a free parameter that serves as a threshold. After optimization, we can predict ϑ mi by ϑ mi = w i , g m + b. We use the LIBSVM toolbox <ref type="foot" target="#foot_3">5</ref> with linear kernel (for fast speed) to implement SVR for emotion distribution prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Algorithm D: Shared Sparse Regression</head><formula xml:id="formula_8">Suppose F = [f 1 , • • • , f N ], Θ = [θ 1 , • • • , θ N ].</formula><p>The basic idea of shared sparse regression (SSR) is that g m and ϑ m can be written in terms of bases F and Θ respectively, but with shared sparse coefficients φ m . That is</p><formula xml:id="formula_9">gm = Fφm and ϑm = Θφm,<label>(6)</label></formula><p>where φ m is obtained by</p><formula xml:id="formula_10">φ * m = argmin φm Fφm -gm 2 + η φm 0, s.t. φm ≥ 0 and φm 1 = 1,<label>(7)</label></formula><p>where η is a regularization coefficient that controls the relative importance of the regularization term and the sum-of-squares error term. The constraints φ m ≥ 0 and φ m 1 = 1 ensure that the predicted mixing coefficients in Eq. ( <ref type="formula" target="#formula_0">1</ref>) sum to 1 and that the covariance matrixes are positive definite. In practice, η is decided by cross validation. By iteratively reweighted least squares (IRLS) <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>, the objective function of Eq. ( <ref type="formula" target="#formula_10">7</ref>) can be reduced to the following quadratic function with respect to φ m J (φm)</p><formula xml:id="formula_11">Fφm -gm 2 + η N n=1 |φm,n| p p (0 ≤ p ≤ 1) Fφm -gm 2 + η N n=1 1 |φm,n| 2-p + ε |φn| 2 = φ T m (F T F + ηΓm)φm -2g T m Fφm,<label>(8)</label></formula><p>where ε &gt; 0 is introduced to avoid division by zero, Γ m is a diagonal matrix with</p><formula xml:id="formula_12">Γ m (n, n) = 1 |φ m,n | 2-p + ε</formula><p>. The optimization problem Eq. ( <ref type="formula" target="#formula_11">8</ref>) can now be easily solved by off-the-shelf optimization methods. In experiment, p → 0. The learning procedure is summarized in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Algorithm E: Multi-Task Shared Sparse Regression</head><p>GM, KNNW and SSR model one test image each time, while SVR predicts one target value each time. They do not explore the latent correlation between different prediction tasks by jointly combining them together. That is, they ignore the task relatedness. Multi-task shared sparse regression (MTSSR) utilizes this information. Different related tasks are learnt simultaneously by extracting and utilizing appropriate shared information across tasks. Compared with SSR, MTSSR argues that the regression performance can be improved by taking advantage of the feature group structure <ref type="bibr" target="#b59">[60]</ref>. </p><formula xml:id="formula_13">Suppose G = [g 1 , • • • , g M ], Ω = [ ϑ 1 , • • • , ϑ M ],</formula><p>Φ ∈ R N ×M is obtained by solving the following convex optimization problem</p><formula xml:id="formula_15">min Φ FΦ -G 2 + η1 Φ 0 + η2 Φ 2,1, s.t. φm ≥ 0 and φm 1 = 1, for m = 1, 2, • • • , M,<label>(10)</label></formula><p>where η 1 and η 2 are regularization coefficients, similar to η in Eq. ( <ref type="formula" target="#formula_10">7</ref>), while • 2,1 denotes the 2,1 -norm of a matrix</p><formula xml:id="formula_16">Φ 2,1 = N n=1 M m=1 φ 2 n,m .</formula><p>The constraints φ m ≥ 0 and φ m 1 = 1 for each m ensure that for each test image, the predicted mixing coefficients in Eq. ( <ref type="formula" target="#formula_0">1</ref>) sum to 1 and that the covariance matrixes are positive definite.</p><p>Sparse multi-task regression was previously proposed in <ref type="bibr" target="#b59">[60]</ref> for brain imaging identification, which aims to optimize</p><formula xml:id="formula_17">min Φ FΦ -G 2 + η1 Φ 1 + η2 Φ 2,1.<label>(11)</label></formula><p>The difference between Eq. ( <ref type="formula" target="#formula_15">10</ref>) and Eq. ( <ref type="formula" target="#formula_17">11</ref>) is that the proposed MTSSR utilizes 0 -norm instead of 1 -norm and is optimized with constraints. Please note that though mathematically similar, Eq. ( <ref type="formula" target="#formula_17">11</ref>) is not suitable for continuous emotion distribution prediction, since it cannot guarantee the predicted covariance matrix is positive definite. In such cases, the solution of Eq. ( <ref type="formula" target="#formula_17">11</ref>) violates the basic bidimensional  Gaussian distribution assumption as in Section III-B.</p><p>We employ the iteratively reweighted least squares <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref> to optimize Eq. <ref type="bibr" target="#b9">(10)</ref>. The components of Eq. ( <ref type="formula" target="#formula_15">10</ref>) are transformed by</p><formula xml:id="formula_18">Φ 0 n,m |φn,m| p p n,m φ 2 n,m |φn,m| 2-p + ε ,<label>(12)</label></formula><formula xml:id="formula_19">Φ 2,1 = n m φ 2 n,m n m φ 2 n,m m φ 2 n,m + ε ,<label>(13)</label></formula><p>where 0 ≤ p ≤ 1. Let ϕ n,m = 1/(|φ n,m | 2-p + ε) and ψ n = 1/ m φ 2 n,m + ε, then the objective function of Eq. ( <ref type="formula" target="#formula_15">10</ref>) is transformed to</p><formula xml:id="formula_20">J (Φ) m Fφm -gm 2 + n,m (η1ϕn,m + η2ψn)φ 2 n,m = m Fφm -gm 2 + m φ T m Wmφm,<label>(14)</label></formula><p>where W m is a diagonal matrix with W m (n, n) = η 1 ϕ n,m + η 2 ψ n . min J (Φ) is a quadratic programming problem, which can be easily solved by off-the-shelf optimization methods. In experiment, p → 0. The learning procedure is summarized in Algorithm 2.</p><p>VI. EXPERIMENTS To evaluate the effectiveness of the proposed method for continuous distribution prediction of dimensional image emotions, we carried out experiments on 18,700 images which are selected from the Image-Emotion-Social-Net dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Criteria</head><p>The Kullback-Leibler divergence and the log likelihood (abbreviated as KL and LLH) are used as the evaluation metric. As a classical measure of distance between distributions, the KL divergence of the predicted distribution q m (x; ϑ m ) from the ground truth distribution q m (x; ϑ m ) is defined as  In practice, KL(q m || q m ) is approximated by a finite sum of the points {s 1 , • • • , s S } sampled following distribution q m (x; ϑ m ) by</p><formula xml:id="formula_21">KL(qm|| qm) 1 S S n=1</formula><p>ln qm(sn; ϑm) -ln qm(sn; ϑm) . ( <ref type="formula">16</ref>)</p><p>KL measures the amount of information lost when q m (x; ϑ m ) is used to approximate q m (x; ϑ m ). Its value is equal to the expected number of extra bits required to code samples from q m (x; ϑ m ) using a code optimized for q m (x; ϑ m ) rather than the code optimized for q m (x; ϑ m ) <ref type="foot" target="#foot_4">6</ref> . KL ≥ 0 and lower value indicates better performance, with equality if, and only if the predicted distribution q m (x; ϑ m ) is equal to the ground truth distribution q m (x; ϑ m ).</p><p>The log likelihood metric is computed based on the actual VA labels {x m1 , • • • , x mRm } by </p><p>Higher LLH represents better performance, indicting that the predicted distribution can more accurately fit the actual labels 7 .</p><p>In experiments, we employ 5-fold (noted as A to E) cross validation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Each run, one fold is selected for testing and the other four folds are used for training. The parameters in our method are selected from the training data. For example, we first select A as the test set. Then we split the data in B to E into 5 folds again, and a new 5-fold cross validation is conducted to select the best parameters based on the average KL. The selected parameters are used to test A. We computed the average KL, LLH and the standard deviation of the 5 runs. To better explain the validation process, we also reported the performances of 1 run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Discussions</head><p>1) On the Influence of K in KNNW: Firstly, we investigated the influence of K in KNNW on the performance of emotion distribution prediction (K = 1, 5, 10, 20, 50, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000). When K = 1, KNNW refers to nearest neighbor weighting (NNW). The results are illustrated in Figure <ref type="figure" target="#fig_10">8</ref>. The results of NNW are given in numerical values for better illustration. It is clear to see that (1) for each feature, NNW performs worst, meaning 2) On the Influence of η in SSR: Secondly, we evaluated the influence of the regularization parameter η in the proposed SSR on emotion distribution prediction (η = 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10). The results of the average KL divergence and log likelihood are shown in Figure <ref type="figure">9</ref>. Generally, with the decrease of η, the performance becomes better. When η decreases to O(10 -4 ), the performance turns to be stable. η = 0.0005, 0.0001, 0.0001, 0.0001, 0.0001 and 0.0005 perform best for features GIST, Elements, Attributes, Principles, ANP and Expressions, respectively.</p><p>3) On the Influence of η 1 , η 2 in MTSSR: Finally, the influences of the regularization parameters η 1 , η 2 in the proposed MTSSR are validated, with results shown in Figure <ref type="figure" target="#fig_0">10</ref> and Figure <ref type="figure" target="#fig_0">11</ref>. For clarity, η 1 = 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5 are plotted with η 2 = 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 100(1000). From these results, we can find that (1) generally, with the decrease of η 1 , the performance becomes better; when η 1 decreases to O(10 -4 ), the performance turns to be stable, which is similar to SSR; (2) for each η 1 , with the increase of η 2 , the performance firstly becomes better and then turns worse, meaning that there exists the best η 2 . So we can conclude that selecting proper η 2 can indeed improve the performance of emotion distribution prediction, which indicates the significance of the multi-task learning settings.</p><p>4) On Different Methods and Features: We compared the performance of the proposed method with the three baselines on different features. The average KL and LLH and the standard deviation are illustrated in Figure <ref type="figure" target="#fig_12">12</ref>, while the statistical significance test is shown in Table <ref type="table" target="#tab_8">IV</ref>.</p><p>From these results, we can find that (1) KL and LLH are dependent on both the features and the models; they are relatively consistent to measure the performance of distribution prediction; (2) for all the features except Expressions, the proposed MTSSR model significantly outperforms the three baselines and SSR under 95% confidence interval, which demonstrates the effectiveness of MTSSR in emotion distribution prediction; (3) MTSSR outperforms SSR with an per-   Though not very obvious from the contour lines, the predicted distributions of the proposed (MT)SSR are more similar to the ground truth distribution than the three baselines, which can be clearly observed by comparing the values of the distribution parameters. The predicted results of all methods, especially GW and KNNW, tend to be close to the average values of the parameters, caused by the assumption that the test parameters can be linearly represented by the training parameters with positive coefficients, which ensures the positive definiteness of covariance matrixes. But in such cases, the smallest or largest parameters cannot be well predicted, since they cannot be well linearly represented by the training parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we proposed to predict the continuous probability distribution of image emotions represented in VA space, which can be viewed as an initial attempt to measure the subjective evaluation of human emotion perceptions. We presented (multi-task) shared sparse regression as the learning model and optimized it by iteratively reweighted least squares. Besides, different levels of emotion features were extracted and three baseline algorithms were provided. Experiments conducted on the Image-Emotion-Social-Net dataset corroborated the effectiveness of the proposed method. The predicted emotion distribution can be explored in many applications, such as affective image retrieval and emotion transfer.</p><p>For further studies, we will consider exploring social related factors <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b70">[71]</ref>, such as social correlations, known locations and personal interests, for emotion distribution prediction. Consistently combining and fusing multi-modal features <ref type="bibr" target="#b71">[72]</ref> in MTSSR may further improve the prediction performance, which is also worth studying. In addition, we will try deep learning for emotion distribution prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The differences between traditional affective image regression and the proposed emotion distribution prediction. (a) is the uploaded image to Flickr. (b) are the title, tags and description given by the uploader to (a).(c) are the expected emotion labels that we assign to the uploader using the keywords in (b) in red. (d) are the comments to (a) from different viewers and the personalized emotion labels that we obtain using the keywords in red. (e) illustrates the differences, where the hollow points are the perceived emotion labels in VA space, the blue square and magenta diamond points are the target average VA scores by the traditional affective image regression methods using different strategies of obtaining labels, while the contour lines of GMM are the target emotion distribution by the proposed method.</figDesc><graphic coords="1,498.49,253.91,63.14,52.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig.2. The statistics of CES and DES of the 18,700 images in the Image-Emotion-Social-Net dataset. We can find that only a tiny proportion of images convey just 1 emotion and the STD of valence for most images is larger than 1.5, which demonstrates the subjectiveness of emotion perceptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Dataset statistics results on VAD distribution and user distribution. (a) is consistent with traditional emotion space [20]. (b) approximatively follows a typical Gaussian distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a). The histogram of valence and arousal standard deviations (STD) are shown in Figure 2(b) and Figure 2(c), while the histogram of the correlation coefficients is shown in Figure 2(d). Some image examples and related personalized emotion labels are shown in Figure 6(a) and 6(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) and 6(b), we have the following observations:<ref type="bibr" target="#b0">(1)</ref> The emotions evoked by an image in different viewers are truly subjective and different; Just assigning the average dimensional values of valence and arousal to an image is obviously not enough; (2) Though highly different, the perceived emotions follow certain distributions, which can be clearly grouped into two clusters, corresponding to the positive and negative sentiments; In each cluster, the VA emotion values are relatively stable; (3) The VA emotion labels can be well modeled by a mixture of two bidimensional Gaussian distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Image examples in the Image-Emotion-Social-Net dataset ,4.4291,0.1273,0.0404,0.1428,0.9016 4.0821,4.0438,0.1894,-0.2413,0.5202,0,4.1874,0.0541,0.0183,0.0823,0.4912 3.6645,5.0830,0.3453,-0.3074,0.7874,0,4.3461,0.1160,0.1373,0.2138,0.3889 4.0661,4.2914,0.2947,-0.3983,0.9904,0.6111 (c) The estimated GMM using specified EM algorithm ,4.3311,0.2159,0.1255,0.2747,0.5957 3.8305,4.6911,0.4510,-0.4428,0.8755,0.4043 KL=0.3519,LLH=-1,4.3321,0.2170,0.1263,0.2757,0.5970 3.8324,4.6883,0.4510,-0.4421,0.8729,0,4.3320,0.2160,0.1255,0.2759,0.5963 3.8303,4.6944,0.4507,-0.4452,0.8786,0.4037 KL=0.2564,LLH=-0,4.3301,0.2168,0.1261,0.2785,0.5950 3.8302,4.6972,0.4509,-0.4467,0.8816,0.4050 KL=0.2699,LLH=-0,4.3300,0.2151,0.1260,0.2803,0.5946 3.8318,4.7016,0.4497,-0.4500,0.8830,0.4054 KL=0.4875,LLH=-1,4.3302,0.2175,0.1265,0.2795,0.5951 3.8299,4.6980,0.4513,-0.4471,0.8824,0.4049 KL=0.5642,LLH=-1.3329 (d) The predicted GMM by GW using ANP features ,4.3503,0.1905,0.1088,0.2386,0.5719 3.8253,4.7027,0.4259,-0.4078,0.8085,0,4.3166,0.2125,0.1310,0.2686,0.5872 3.8070,4.6824,0.5053,-0.3674,0.8466,0,4.3098,0.2315,0.1457,0.3162,0.5994 3.8244,4.7484,0.4519,-0.4422,0.8914,0.4006 KL=0.512,LLH=-1.3166 (e) The predicted GMM by KNNW (K = 300) using ANP features ,4.3880,0.2046,0.1281,0.2477,0.6232 3.8778,4.6308,0.4027,-0.4026,0.7957,0,4.3382,0.2021,0.1280,0.2510,0.6024 3.8194,4.6799,0.4416,-0.4585,0.8704,0,4.2966,0.1958,0.1146,0.2317,0.5788 3.8253,4.6536,0.4189,-0.4320,0.8392,0,4.3344,0.2027,0.1271,0.2562,0.6201 3.8608,4.6904,0.4232,-0.4984,0.9036,0,4.3120,0.1945,0.1269,0.2386,0.5767 3.8296,4.6579,0.4324,-0.4264,0.8325,0,4.3157,0.2226,0.1363,0.2635,0.5867 3.8150,4.6539,0.4466,-0.4307,0.8422,0.4127 KL=0.5836,LLH=-1.3208 (f) The predicted GMM by SVR using Elements features ,4.3478,0.1773,0.0745,0.2234,0.6625 3.9225,4.6675,0.4566,-0.4892,0.8838,0.3375 KL=0.2175,LLH=-0,4.3886,0.1260,0.0730,0.1492,0.6646 3.7291,4.8768,0.3686,-0.4444,0.8674,0.3354 KL=0.2907,LLH=-0,4.4069,0.1860,0.1330,0.2633,0.6359 3.9108,4.7054,0.5227,-0.4669,0.9219,0.3641 KL=0.2124,LLH=-0,4.3751,0.1657,0.0497,0.1985,0.6529 3.8527,4.7908,0.4569,-0.5144,0.9702,0.3471 KL=0.1398,LLH=-0,4.3914,0.1241,0.0771,0.1918,0.6330 3.7679,4.8992,0.4813,-0.6618,1.1124,0.3670 KL=0.2457,LLH=-0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Fig.7. Image examples with high different proportions (&gt;0.9) between personalized and expected emotions, which indicates that the personalized emotion perceptions greatly differ from the expected emotions, probably due to different responses to the visual semantics (see the metadata and comments in Fig.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 : 4 φ</head><label>14</label><figDesc>Learning procedure for Shared Sparse Regression Input: Training examples (F, Θ), test image gm, error threshold γ, regularization coefficient η, max-epochs E, stability parameter ε Output: Predicted emotion distribution parameter ϑm for gm 1 Initialize φ (e) m ← argmin φ (e-1)T m (F T F+ηΓ (e) m )φm -2g T m Fφ (e-1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>MTSSR jointly predicts Ω for G by letting the test features and the target values share the same coefficients Φ on training data F and Θ as follows G = FΦ and Ω = ΘΦ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 ϕ 4 Φ</head><label>34</label><figDesc>m (n, n) = η1ϕn,m + η2ψn;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The influence of K in KNNW on continuous emotion distribution prediction using different features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. Performance comparison between the proposed method and the three baselines on emotion distribution prediction using different features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 6(d) to Figure 6(h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. The influence of η 1 , η 2 in MTSSR on continuous emotion distribution prediction using different features on KL divergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II IMAGE</head><label>II</label><figDesc>NUMBERS OF CATEGORICAL EMOTIONS.</figDesc><table><row><cell>amusement</cell><cell>awe</cell><cell cols="2">contentment excitement</cell><cell>positive</cell></row><row><cell>270,748</cell><cell>328,303</cell><cell>181,431</cell><cell>115,065</cell><cell>1,016,186</cell></row><row><cell>anger</cell><cell>disgust</cell><cell>fear</cell><cell>sadness</cell><cell>negative</cell></row><row><cell>29,844</cell><cell>20,962</cell><cell>55,802</cell><cell>57,476</cell><cell>362,400</cell></row></table><note><p>user. This process resulted in a dataset containing 1,012,901 images uploaded by 11,347 users and 1,060,636 comments on these images commented by 106,688 users. We chose 7723 active users with more than 50 involved images. Finally we obtained 1,434,080 emotion labels of three types, including 8 emotion categories, 2 sentiment categories and continuous values of valence, arousal and dominance. All the involved images of one user are labelled with sentiment categories and VAD values, while a tiny proportion of them are not assigned with the emotion categories if no keyword is found.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III SUMMARY</head><label>III</label><figDesc>OF THE EXTRACTED FEATURES OF DIFFERENT LEVELS. '#' INDICATES THE DIMENSION OF EACH FEATURE.</figDesc><table><row><cell cols="2">Levels Generality</cell><cell>Short Description</cell><cell>#</cell></row><row><cell>Low</cell><cell>Generic</cell><cell>GIST features [63] [64]</cell><cell>512</cell></row><row><cell></cell><cell>Special</cell><cell>Color and texture [11]</cell><cell>48</cell></row><row><cell>Mid</cell><cell>Generic</cell><cell>Attributes [64] [23]</cell><cell>102</cell></row><row><cell></cell><cell>Special</cell><cell>Principles-of-art [2]</cell><cell>165</cell></row><row><cell>High</cell><cell>Generic</cell><cell cols="2">Adjective noun pairs [1] 1200</cell></row><row><cell></cell><cell>Special</cell><cell>Facial expressions [30]</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2016.2617741, IEEE Transactions on Multimedia Training examples (F, Θ), test images G, error threshold γ, regularization coefficient η1, η2, max-epochs E, stability parameter ε Output: Predicted emotion distribution parameters Ω for G 1 Initialize Φ (0) ; 2 for e ← 1 to E do</figDesc><table><row><cell>IEEE TRANSACTIONS ON MULTIMEDIA Algorithm 2: Learning procedure for Multi-Task Shared</cell></row><row><cell>Sparse Regression</cell></row><row><cell>Input:</cell></row></table><note><p>1520-9210 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>using only one training image with the most similar features to the test image to predict the emotion distribution is insufficient; (2) the best K is dependent on the extracted features and is almost consistent on KL and LLH for each feature; (3) K = 4000, 500, 1000, 400, 300 and 3000 perform best for features GIST, Elements, Attributes, Principles, ANP and Expressions, respectively. These best Ks are selected as baselines for comparison with the proposed (MT)SSR.</figDesc><table><row><cell>KL</cell><cell>GIST Principles</cell><cell cols="2">Elements ANP</cell><cell cols="3">Attributes Expressions</cell><cell>LLH</cell><cell>GIST Principles</cell><cell>Elements ANP</cell><cell cols="2">Attributes Expressions</cell></row><row><cell>1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-1.0</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-1.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-1.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.4</cell><cell cols="2">0.0001 0.001 0.01</cell><cell>0.1</cell><cell>1</cell><cell>10</cell><cell></cell><cell>-1.3</cell><cell cols="2">0.0001 0.001 0.01 0.1</cell><cell>1</cell><cell>10</cell><cell></cell></row><row><cell></cell><cell cols="4">(a) KL divergence</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Log likelihood</cell><cell></cell></row><row><cell cols="12">Fig. 9. The influence of η in SSR on continuous emotion distribution</cell></row><row><cell cols="6">prediction using different features.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>that</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV STATISTICAL</head><label>IV</label><figDesc>SIGNIFICANCE TEST OF MTSSR COMPARED WITH THE THREE BASELINES AND SSR MEASURED BY P-VALUE (×10 -3 ). KL and 38.8% on KL and 1.4%, 2.7%, 1.8%, 1.8%, 3.2% and 10.2% on LLH for the six kinds of features respectively; this superiority benefits from the exploration of latent information between different tasks; (4) the best features are ANP, ANP, Elements, Principles and ANP for GW, KNNW, SVR, SSR and MTSSR, respectively; Generally, the low-level generic features perform the worst, which indicates that they cannot represent image emotions well because of the largest "affective gap"; More interpretable mid-level and the high-level features have stronger link to image emotions, which is consistent with the conclusions in<ref type="bibr" target="#b25">[26]</ref>; (5) though simple, GW and KNNW outperform SVR on average in emotion distribution prediction; (6) the best KL divergence of all the methods are still larger than 0,4, indicating that the emotion distribution prediction is a challenging task and that current methods still cannot model this task accurately.Using related best features, we show some detailed prediction results of different methods in</figDesc><table><row><cell></cell><cell></cell><cell cols="2">KL divergence</cell><cell></cell><cell></cell><cell cols="2">Log likelihood</cell><cell></cell></row><row><cell></cell><cell>GW</cell><cell>KNNW</cell><cell>SVR</cell><cell>SSR</cell><cell>GW</cell><cell>KNNW</cell><cell>SVR</cell><cell>SSR</cell></row><row><cell>GIST</cell><cell>2.80</cell><cell>2.79</cell><cell>1.16</cell><cell>17.95</cell><cell>16.28</cell><cell>16.64</cell><cell>9.55</cell><cell>8.22</cell></row><row><cell>Elem</cell><cell>1.76</cell><cell>2.35</cell><cell>1.07</cell><cell>0.45</cell><cell>12.97</cell><cell>15.84</cell><cell>9.96</cell><cell>0.12</cell></row><row><cell>Attr</cell><cell>2.58</cell><cell>2.51</cell><cell>1.09</cell><cell>1.11</cell><cell>19.98</cell><cell>20.27</cell><cell>13.00</cell><cell>4.00</cell></row><row><cell>Prin</cell><cell>2.17</cell><cell>2.78</cell><cell>1.14</cell><cell>1.90</cell><cell>11.35</cell><cell>12.87</cell><cell>8.73</cell><cell>0.42</cell></row><row><cell>ANP</cell><cell>2.90</cell><cell>5.09</cell><cell>0.20</cell><cell>0.13</cell><cell>18.41</cell><cell>25.00</cell><cell>1.30</cell><cell>0.01</cell></row><row><cell>Expr</cell><cell>1.71</cell><cell>1.79</cell><cell>0.72</cell><cell>5.33</cell><cell>9.76</cell><cell>10.45</cell><cell>6.74</cell><cell>14.65</cell></row><row><cell cols="9">formance improvement of 4.4%, 4.9%, 3.6%, 2.0%, 6.7% on</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://sites.google.com/site/schzhao/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.thesaurus.com/browse/synonym/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://en.wikipedia.org/wiki/Fleiss%27_kappa</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://www.csie.ntu.edu.tw/~cjlin/libsvm/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring principles-of-art features for image emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object-based visual sentiment concept analysis and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="367" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting moods from pictures and sounds: Towards truly personalized tv</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="100" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aesthetics and emotions in images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fedorovskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A mixed bag of emotions: Model, predict, and transfer emotion distributions</title>
		<author>
			<persName><forename type="first">K.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="860" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting discrete probability distribution of image emotions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2459" to="2463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting personalized emotion perceptions of social images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting continuous probability distribution of image emotions in valence-arousal space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="879" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image retrieval by emotional semantics: A study of emotional space and feature extraction</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>W.-N. Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3534" to="3539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-K</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On shape and the computability of emotions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suryanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Adams</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Context-aware affective images classification based on bilayer sparse representation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Quantitative study of individual emotional states in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C M</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="144" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can we understand van gogh&apos;s mood? learning to infer affects from images in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="857" to="860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How do your friends on social media disclose your emotions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three dimensions of emotion</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schlosberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A connotative space for supporting movie affective recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1356" to="1370" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Affective video content representation and modeling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Q</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="154" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 english lemmas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1207" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing and predicting sentiment of images on the social web</title>
		<author>
			<persName><forename type="first">S</forename><surname>Siersdorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Minack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hare</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="715" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sentribute: image sentiment analysis from a mid-level perspective</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcdonough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Workshop on Issues of Sentiment Discovery and Opinion Mining</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">User interest and social influence based emotion prediction for individuals</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="785" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Emotionally representative image discovery for social events</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">177</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Affective image retrieval via multi-graph learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1025" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning mid-level features for recognition</title>
		<author>
			<persName><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2559" to="2566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting object boundaries using low-, mid-, and high-level information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1055" to="1067" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning predictable and discriminative attributes for visual recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3783" to="3789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring facial expressions with compositional features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2638" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video indexing and recommendation based on affective analysis of viewers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1473" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Static and dynamic 3d facial expression recognition: A comprehensive survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sandbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="683" to="697" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Speaking effect removal on emotion recognition from facial expressions based on eigenface conversion</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1732" to="1744" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video classification and recommendation based on affective analysis of viewers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="101" to="110" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Affective body expression perception and recognition: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kleinsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bianchi-Berthouze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="33" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic facial expression recognition using features of salient facial patches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Happy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Routray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Affective labeling in a content-based recommender system for images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tkalcic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Odic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kosir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="391" to="400" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Opinion mining and sentiment analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Emotion recognition from speech: a review</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Koolagudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Speech Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning salient features for speech emotion recognition using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Machine recognition of music emotion: A review</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">40</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Quantitative study of music listening behavior in a social and affective context</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1304" to="1315" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Emotional accompaniment generation system based on harmonic progression</title>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1469" to="1479" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Emotion based image musicalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flexible presentation of videos based on affective content analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modelling</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="368" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cavva: Computational affective video-in-video advertising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yadati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="23" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Corpus development for affective video indexing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1075" to="1089" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Using audio-derived affective offset to enhance tv recommendation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Shepstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1999" to="2010" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video affective content analysis: a survey of state of the art methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="410" to="430" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Predicting probability distributions for surf height using an ensemble of mixture density networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cumulative probability distribution model for evaluating user behavior prediction algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Social Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="385" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Impact of spike train autostructure on probability distribution of joint spike events</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pipa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grün</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Vreeswijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1123" to="1163" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual classification with multitask joint sparse representation</title>
		<author>
			<persName><forename type="first">X.-T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4349" to="4360" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Malsar: Multi-task learning via structural regularization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Arizona State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-task learning for hiv therapy screening</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bogojeska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="56" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Robust multi-task feature learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="895" to="903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1817" to="1853" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sparse multi-task regression and feature selection to identify brain imaging predictors for memory performance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Risacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Saykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="557" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">What are emotions? and how can they be measured?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Information</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="695" to="729" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Emotional category data on images from the international affective picture system</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Mikels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Fredrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lindberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maglio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Reuter-Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sun attribute database: Discovering, annotating, and recognizing scene attributes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Designing category-level attributes for discriminative visual recognition</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="771" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Start from scratch: Towards automatically identifying, modeling, and naming visual attributes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Robust real-time face detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Iteratively reweighted algorithms for compressive sensing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="3869" to="3872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Preconditioning for accelerated iteratively reweighted least squares in structured sparsity reconstruction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2713" to="2720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Multimedia social event detection in microblog</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="269" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Collective matrix factorization hashing for multimodal data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2075" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
