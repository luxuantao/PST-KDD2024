<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context and Hierarchy in a Probabilistic Image Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ya</forename><surname>Jin</surname></persName>
							<email>yajin@dam.brown.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Division of Applied Mathematics</orgName>
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stuart</forename><surname>Geman</surname></persName>
							<email>geman@dam.brown.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Division of Applied Mathematics</orgName>
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Context and Hierarchy in a Probabilistic Image Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">81824BBF909CB4E224A2A6ED91671F96</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is widely conjectured that the excellent ROC performance of biological vision systems is due in large part to the exploitation of context at each of many levels in a part/whole hierarchy. We propose a mathematical framework (a "composition machine") for constructing probabilistic hierarchical image models, designed to accommodate arbitrary contextual relationships, and we build a demonstration system for reading Massachusetts license plates in an image set collected at Logan Airport. The demonstration system detects and correctly reads more than 98% of the plates, with a negligible rate of false detection. Unlike a formal grammar, the architecture of a composition machine does not exclude the sharing of sub-parts among multiple entities, and does not limit interpretations to single trees (e.g. a scene can have multiple license plates, or no plates at all). In this sense, the architecture is more like a general Bayesian network than a formal grammar. On the other hand, unlike a Bayesian network, the distribution is non-Markovian, and therefore more like a probabilistic context-sensitive grammar. The conceptualization and construction of a composition machine is facilitated by its formulation as the result of a series of non-Markovian perturbations of a "Markov backbone." 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>By the theory of nonparametric inference, essentially any classification or estimation problem can be solved, more or less automatically, from a sufficiently rich and sufficiently lengthy sequence of examples. This was already well known within the statistics community in the early 1980's, by which time several elegant approaches had been explored, including, for example, kernal estimation (e.g. <ref type="bibr" target="#b26">[28]</ref>), k-nearest-neighbor classification (e.g. <ref type="bibr" target="#b9">[10]</ref>), and Grenander's method of sieves <ref type="bibr">[22]</ref>. The problem of overfitting (a.k.a. controlling variance, model selection) was already front and center, resulting in the development of various analytic and practical methodologies (e.g. bounds derived from the Vapnik-Cervonenkis dimensionality <ref type="bibr" target="#b40">[42]</ref>, cross validation and generalized cross validation <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b43">45]</ref>, and information-based measures of complexity <ref type="bibr" target="#b28">[30]</ref>).</p><p>The availability of these remarkable tools invites a tabula rasa approach to the problem of computer vision. In principle, it is possible to formulate the object recognition problem in terms of a search for one or more decision surfaces in a high-dimensional image representation, and in principle it is possible to solve the problem, as well as it can possibly be solved, by nonparametric estimation. Indeed, recent advances in learning theory (e.g. Vapnik <ref type="bibr" target="#b41">[43]</ref>, Freund &amp; Schapire <ref type="bibr" target="#b13">[14]</ref>), coupled with relentless advances in computing technology, have rendered this approach practical for certain applications, such as the recognition of isolated hand-written digits.</p><p>More ambitious vision problems require more in the way of a priori structure, dictated by the need to control variance (over-fitting) and practical limitations in the size of any manageable set of training data. Yet a priori structure means a priori bias (cf. <ref type="bibr" target="#b17">[18]</ref>), and the search for an appropriate class of models, embodying the right structure for unconstrained vision problems, is therefore critically important.</p><p>Here we propose a structure based upon the dual principles of hierarchy and reusability. Several observations argue for this general approach:</p><p>Feature and Part Sharing. Reusability is a common theme in computer vision. Indeed, the notion of a feature itself, such as a Gabor filter, or a locally invariant Sift feature <ref type="bibr" target="#b25">[27]</ref>, is already based upon the assumption that, from scene to scene, the same feature will participate in the representation of any one of a multitude of different entities. Biederman <ref type="bibr" target="#b3">[4]</ref> and others have argued that a possibly small number of reusable parts might be sufficient to compose the large ensemble of shapes and objects that are in the repertoire of human vision. Empirical evidence for sharing comes from studies of the diminishing numbers of new parts that are needed to represent objects in a sequential learning task (Krempp et al. <ref type="bibr" target="#b22">[24]</ref>), as well as from the successes of multiple-object recognition systems built on a common substrate of lower-level parts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context.</head><p>It is often observed that segmentation can be ambiguous, if not impossible, in the absence of the contextual information provided through recognition. Similarly, reliable edge and boundary detection is notoriously difficult when attempted in a purely bottom-up framework, without more global contextual constraints that help to disambiguate, for example, texture, shadow, and occlusion boundaries (cf. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b36">38]</ref>). By little more than their nature, hierarchical models (as in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b31">33]</ref>), embody multi-level contextual constraints.</p><p>Efficient Representation. <ref type="bibr">Barlow [3]</ref> proposed suspicious coincidences as a possible principle for discovering meaningful groupings, such as the grouping of features into parts, parts into objects, or objects into scenes. A new label, "tree", "telephone", "desk", makes for a more efficient representation by virtue of "explaining" an otherwise "suspicious coincidence" in the arrangement of features and parts. Much earlier, Laplace <ref type="bibr" target="#b23">[25]</ref> made a similar observation, arguing that a likelihood principle was sufficient to provide a gradient towards meaningful grouping. These notions of grouping are closely related to the notion of efficient representation, in that the introduction of a label for an otherwise unlikely grouping of parts amounts to an enhanced encoding and a shorter description length (as discussed for example by Bienenstock et al. <ref type="bibr" target="#b4">[5]</ref>). By this connection, hierarchical description is a close cousin of Rissanen's Minimum Description Length principle <ref type="bibr" target="#b28">[30]</ref>.</p><p>Biology. Fodor and Pylyshyn <ref type="bibr" target="#b11">[12]</ref> have questioned the biological relevance of the (nonparametric-type) learning algorithms employed in most neural network models. They argue that these models lack a fundamental feature of human cognition -they are not compositional. The principle of compositionality holds that humans perceive and organize information as a syntactically constrained hierarchy of reusable parts. The prototypical formulation was introduced by Chomsky <ref type="bibr" target="#b7">[8]</ref> as a system of formal grammars. Indeed, language itself is the prototypical compositional system, with evident hierarchy, syntax, and reusability. In the visual world, physical objects and scenes decompose naturally into a hierarchy of meaningful and generic parts, and it is perhaps no coincidence that there is an apparent hierarchical structure in the ventral visual pathways of the more highly evolved visual systems <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b39">41]</ref>.</p><p>In §2, following the formulation proposed in <ref type="bibr" target="#b16">[17]</ref>, we develop a prior probability model on hierarchically organized image interpretations ("composition machine"). We begin with a Markov structure, in the spirit of a Bayesian net-work, and later perturb this distribution in order to achieve greater selectivity. An application to licence-plate reading is explored in §3, and some conclusions and speculation are offered in §4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model: Composition Machine</head><p>Composition systems are generative, probabilistic, image models that embody a hierarchy of part/whole relationships. Generative probabilistic models include Bayesian networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b34">36]</ref>, linear and nonlinear filtering <ref type="bibr" target="#b10">[11]</ref>, Markov random fields <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b44">46]</ref>, and probabilistic contextfree grammars <ref type="bibr" target="#b18">[19]</ref>. Compositional systems are distinct from these models in that they are non-Markovian. On the one hand this makes computation substantially more difficult, but on the other hand, non-Markovian models are more selective and thereby, in principle, capable of smaller type II error probabilities (probabilities of false alarms). Markov Backbone. Figure <ref type="figure" target="#fig_0">1</ref> depicts the 'Markov backbone', which is a generative, hierarchical model equipped with a Markov structure on a directed acyclic graph. Starting at the bottom, the image pixels are represented by a one-dimensional string of nodes, corresponding to a onedimensional slice through the two-dimensional pixel array. Hidden (model) variables are associated with twodimensional sheets of nodes that sit "above" the image array; these variables are called bricks (as in Lego bricks) to emphasize their re-usability across legitimate configurations. The layer of bricks that sit immediately above the image array are called terminal bricks, and as we shall see, are associated with local image filters.</p><p>Bricks represent semantic variables, like edges, strokes, junctions, shapes, and various parts and objects. Assignments will vary from application to application; Figure <ref type="figure" target="#fig_1">2</ref> indicates the assignments for the application to license-plate reading. Each sheet of bricks comprises a regular sublatice of each type of variable, so that the third layer above the image, in Figure <ref type="figure" target="#fig_1">2</ref>, is interspersed with "generic-letter bricks", "generic-number bricks", and "L-junction bricks".</p><p>A brick can be on or off. An on brick selects one subset of "children bricks" from an allowed collection of such subsets drawn from the layers below. The possible sets of children are depicted with broken ovals in Figure <ref type="figure" target="#fig_0">1</ref>. <ref type="foot" target="#foot_1">2</ref> We refer to these selections as "compositions", which is exactly what they are in the sense that the selected children are composed, as "parts", into the "object" represented by the on brick. Put another way, the on brick is instantiated by the selected children. An image interpretation, corresponding to a semantic labeling of a scene, is a subgraph of on bricks, each substantiated by an allowed set of children bricks, which themselves must be on. Such subgraphs are called complete. See Figure <ref type="figure" target="#fig_0">1</ref>, where an example is highlighted with colored nodes and bold arrows. The set of all interpretations is denoted I.</p><p>The state of a brick, say the brick β ∈ B, is a random variable, x β ∈ {0, 1, . . . , n β }, with x β = 0 representing off, and x β = 1, 2, . . . , n β representing the selected set of children in Figure <ref type="figure" target="#fig_0">1</ref>. The pixels themselves (actually, their grey levels) are represented by a vector of intensities, y.</p><p>The Bayesian framework has two components: a prior distribution, here on the set of interpretations, I, and a conditional data model, meaning a probability distribution on y for each I ∈ I. As mentioned earlier, we start with a Markovian distribution on I. Each brick β ∈ B is assigned a probability vector ( β 0 , β 1 , . . . , β n β ). In terms of these parameters, the probability P (I) of an interpretation (i.e. a complete subgraph) I is</p><formula xml:id="formula_0">P (I) = β∈B ( β x β ) β∈B(I) (1 -β 0 ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where B is the set of all bricks and B(I), the "below set", is the set of all on bricks that are not roots of the (directed) subgraph I. One way to verify that I∈I P (I) = 1 is by a thought experiment: choose, independently and according to the respective probability vectors, the states of the bricks in the top layer. Next choose, also independently, the states of the bricks in the penultimate layer, using again the respective probability vectors, except that selected children of on bricks are conditioned to themselves be on. Continue downward, finally choosing the states of the terminal bricks.</p><p>The procedure selects a complete subgraph I ∈ I according to the distribution P (I) and establishes the Markov property with respect to the directed acyclic graph represented by nodes and arrows in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Perturbation -the Compositional Distribution. One way to assess a Bayesian model, in this case the "Markov backbone" defined in Equation <ref type="formula" target="#formula_0">1</ref>, is to examine samples. The upper panel in Figure <ref type="figure" target="#fig_2">3</ref> is a random sample from the set of instantiations of a "4-digit-string" brick of the fourth layer (counting from the bottom) of the composition machine for reading plates depicted in Figure <ref type="figure" target="#fig_1">2</ref>. The black and white pixels represent the filters associated with the states of the selected terminal bricks in the instantiation.</p><p>(The filters themselves are reusable parts of characterssee discussion of data models below.) The evident poor fit of the subtrees (numerals and parts of numerals) is a signature of the Markov property. Whereas the distribution accommodates the basic structures of interest, the coverage is too broad. This works against selectivity, and hence ROC performance, in a recognition system. One approach to the coverage problem is through expanded state spacesthe state of a brick can be elaborated to include detailed positional information about its instantiation. This solution, very much analogous to adopting attribute grammars in computational linguistics, can be short-sighted since the potential number of relevant and interacting attributes (position, size, stroke width, color, etc.) is potentially unmanageable in a Markov system. We have chosen instead to treat each attribute with a non-Markov perturbation, starting with the Markov backbone. Briefly, the derivation is as follows: Associate with each brick β ∈ B a (possibly vector-valued) attribute function a β (I), which measures the "fit" among the "parts" that instantiate β, as it appears in the particular interpretation I ∈ I. If β is a "4-digit-string" brick, specifically, then a β (I) returns the relative coordinates of the four numerals that instantiate β in the interpretation I. Similarly, each character brick, and each numeral in particular, has an associated attribute function that computes the relative coordinates of the particular parts that are composed into that character in a particular interpretation. A "compositional distribution" is built from a Markov backbone (Equation <ref type="formula" target="#formula_0">1</ref>) and a pair of probability distributions, p c β ("composed") and p 0 β ("null"), on each attribute a β . The former, composed distribution, captures regularities of the arrangements (i.e. instantiations) of the children bricks, given that they are parts of the object represented by β; the latter, null distribution, is the attribute distribution in the absence of the non-Markovian term. The set of relative coordinates of the three parts that make up the '0' in the upper panel of Figure <ref type="figure" target="#fig_2">3</ref> is an example of an attribute, and the particular arrangement of the parts in the figure is a sample from the corresponding null distribution.</p><p>In a compositional distribution, the null attribute distributions are compared to their composed counterparts: given I ∈ I,</p><formula xml:id="formula_2">P (I) ∝ β∈B ( β x β ) β∈B(I) (1 -β 0 ) β∈A(I) p c β (a β (I)) p 0 β (a β (I))<label>(2)</label></formula><p>where A(I), the "above set", is the set of non-terminal on (active) bricks. The proportionality sign (∝) can be replaced with equality (=) if, at the introduction of each attribute function, a β , care is taken to ensure that p 0 β (a β ) is exactly the current ("unperturbed") conditional distribution on a β given x β &gt; 0. In general, it is not practical to compute an exact null distribution and P must be re-normalized.</p><p>The effect on coverage of the perturbation can be seen by comparing the upper and lower panels in Figure <ref type="figure" target="#fig_2">3</ref>. For each non-terminal brick β, the denominator, p 0 β (a β ), was approximated by assuming that in the absence of an explicit constraint, the prior distribution on a β is the one consistent with independent instantiations of the children. The numerator, p c β (a β ), was constructed to encourage regularity in the relative positions of character parts, and of characters, in composing characters and strings, respectively. The upper panel is a sample instantiation from the Markov backbone; the lower panel is a sample instantiation from the full compositional distribution. Samples from the full compositional distribution can be computed (at considerable computational cost) through a variant of importance sampling. Conditional Data Models. The data model connects interpretations to the grey-level image, and completes the Bayesian framework. In the license-plate-reading demonstration system, we have assumed that the data distribution, conditioned on an interpretation, is a function only of the states of the terminal bricks:</p><formula xml:id="formula_3">P ( y|I) = P ( y|{x β : β ∈ T })</formula><p>where T ⊆ B is the set of terminal, or bottom-row, bricks.</p><p>Good performance in most image analysis applications requires some degree of photometric invariance. In the context of a probability model, the notion of invariance is closely connected to the statistical notion of sufficiency. The following data model, employed in the demonstration system, is an example of the application of sufficiency to invariance. As remarked earlier, the terminal bricks in the demonstration system represent reusable parts of alphanumeric characters. The states of the terminal bricks code the local position of the represented part. Some of the parts can be more-or-less clearly discerned from the upper-hand (Markov) panel in Figure <ref type="figure" target="#fig_2">3</ref>. The zero and the eight are each made of three parts whereas the four and the five are each made of two parts. The black portion of each "part filter" represents image locations that are expected to be dark, relative to the locations represented by the white portion of the filter. The rank sum R (cf. Lehmann <ref type="bibr" target="#b24">[26]</ref>) of the intensities of the corresponding "black" pixels, among the union of intensities of black and white pixels, is a convenient statistic that is demonstrably invariant to all monotone transformations of the image histogram. We model pixel grey levels by assuming that their distribution depends only on R (R is sufficient), and we model R with an exponential probability distribution, thereby promoting small rank sums cor-responding to dark-on-light characters. Pixels that are not referenced by any active terminal brick are modeled as uniformly and independently distributed. More details on this data model can be found in <ref type="bibr" target="#b19">[20]</ref>. Parsing. The a posteriori distribution on interpretations, given a particular image as represented by y, is P (I| y) = P ( y|I)P (I) P ( y) ∝ P ( y|I)P (I)</p><p>The interpretation I corresponds to a full semantic analysis of the scene -an explicit labeling of every pixel, either as background or as participating in one or more particular hierarchies, each of which instantiates a brick of a specific type. From I one reads off the locations and identifications of license plates, strings of characters, characters, lines, parts of characters, etc., as they may be found throughout the image. In short, I represents a semantic and syntactic parsing of the scene with respect to the variables embodied in the composition machine.</p><p>Ideally we would make exact computations under P (I| y). Perhaps we would compute the probability that a scene contains a license plate along with the most likely reading of the plate, or perhaps we would compute the most likely parse of the entire scene. Unfortunately, these and other functionals of the posterior distribution are intractable (indeed, NP-hard). We are forced to explore the computationally feasible alternatives.</p><p>Motivated by the observation that the states of lowerlevel bricks (e.g. terminal bricks) represent coarse hypotheses in the set I (the set of all interpretations) <ref type="foot" target="#foot_2">3</ref> , and the good computational performance of coarse-to-fine (ctf) vision systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b42">44]</ref>, not to mention the optimality of ctf search under some conditions <ref type="bibr" target="#b5">[6]</ref>, we have explored parsing methodologies that start with a bottom-up pass for indexing into a set of likely interpretations.</p><p>Consider again the license plate application and the composition machine depicted in Figure <ref type="figure" target="#fig_1">2</ref>. In the bottom-up pass the computation is launched by evaluating, via a local likelihood ratio test, the evidence for every state of every terminal brick. Each state signals a part at a particular location. A threshold is adjusted so that very few, if any, actual parts are missed, resulting in a large number of "false positive" detections of parts of characters and plate boundaries.</p><p>This same sequence of likelihood ratio tests and conservative thresholds can then be used, in turn, to elicit possible activities among next-level bricks, this time based upon the already-computed collection of possible terminal-brick states. Recursive, bottom-to-top, application of the procedure generates a large list of possible parts and objects, each corresponding to a consistent subgraph within the compositional architecture, and each equipped with a measure of fitness based on a likelihood ratio. The list includes local interpretations that are largely redundant, differing only in the fine detail of positioning, as well as others that are mutually inconsistent. This is the index set, a set of candidate parts and objects that we next employ in a simple greedy algorithm to compute a full-blown parse.</p><p>The best candidate, as measured by likelihood ratio, is selected to seed the parse. Conditioned on the selected candidate (which itself is a sub-graph in the compositional architecture, and hence a parse), we choose next, from all consistent candidates in the remaining index set, the one that most increases the likelihood of the parse when combined with the already-selected candidate. The pair of consistent sub-graphs defines a new parse with higher likelihood. The list optimization procedure continues until there are no further additions from the index set that improve likelihood. The process can be repeated, n times, by seeding the k'th parse with the k'th best candidate from the index set, and finally choosing the best (most likely) parse among the n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Demonstration: Reading License Plates</head><p>The approach is Bayesian: given an image, and given a composition machine with the semantic variables listed in Figure <ref type="figure" target="#fig_1">2</ref> and the architecture outlined in §2, we look for a high-likelihood interpretation, I ∈ I. The presence and identity of license plates are then read off by visiting toplayer active bricks (see Figure <ref type="figure" target="#fig_1">2</ref>) and their instantiations (subtrees) -which, in particular, include the license-plate numbers.</p><p>Data. A set of 458 images (each containing 494×652 pixels) were collected and supplied by the Visics Corporation. Some images contained plates from other states with other fonts and syntaxes, and in some cases the entire plate was not imaged. The experiments were confined to the 385 images that contained human-readable standard-syntax Massachusetts license plates. A typical image is shown in Figure <ref type="figure">5</ref>, and a collage of plates from multiple images is shown in Figure <ref type="figure" target="#fig_3">4</ref>. There is only a small amount of rotation and variation in scale across the image set.</p><p>Performance. Interpretations commonly include character parts, characters, and even strings of characters, at multiple locations throughout the scene. The bottom panel of Figure <ref type="figure">5</ref> shows the top 25 "objects" (complete subtrees) that participate in a full-blown parse of the top-panel image. The full parse includes hundreds to thousands of additional annotations. Nevertheless, the system, starting with its first implementation and including the implementation reported on here, has never produced a false detection at the licenseplate level. This is regardless of whether it is run on scenes with multiple plates (as in Figure <ref type="figure">6</ref>) or no plates at all. The reading rate for characters contained in the license-plate ID's is about 99.5%, and above 98% for the ID's themselves (an ID is misread if any of its characters are misread). Search Strategies. Bottom-up seeding, as described in §2 is slow, even if candidates are heavily pruned during the bottom-up (indexing) pass. Although it is indeed a coarseto-fine exploration of I, the overwhelming majority of the calculations of likelihood are unnecessary in that they could be eliminated, before the fact, if the goal were to find instances of a particular object (e.g. find and read license plates).</p><p>These observations suggest a more efficient ctf strategy: traverse the bricks associated with the objects of interest (top-level bricks in the license-plate system). For each brick, perform a depth-first search for an instantiation. Lower-level bricks might be visited multiple times. Hence, for each brick, a list of instantiations is maintained and re-used every time that brick appears in the computation. Computation passes immediately to the terminal bricks, and the search remains coarse-to-fine in the sense discussed in §2. Yet many of the terminal bricks, indeed the vast majority, are never visited. Furthermore, the algorithm admits easily to multi-threading or implementation on a multi-processor system.</p><p>A simplified version of depth-first search was implemented. Top-level (license-plate) bricks are instantiated by a pair of bricks: a license-plate number (chosen from the penultimate layer) and a license-plate boundary (chosen from the third layer from the top). For each top-layer brick, the possible children among the license-plate boundaries were first explored. Although there were some falsepositive boundaries (one is seen in Figure <ref type="figure">6</ref>), only a small fraction of the image needed to be further explored for the corresponding license number. The result was a many-fold improvement in computation speed with no loss in performance. It is likely that a fully implemented depth-first search would further improve computational efficiency. Observations. How important is the non-Markovian perturbation? It is straight-forward to run the composition machine with and without the perturbation term. What is more, the states of intermediate bricks signal detections of intermediate structures (such as characters, strings, and boundaries), and can therefore be assessed, in and of themselves, by their recognition performance.</p><p>We consistently find a substantial drop in performance, at all levels of recognition, from characters up to license plates, when running the Markov backbone in place of the full compositional (non-Markovian) system. For example, although we have not run the full data set under the Markov backbone, a random sample points to a substantial drop in detection performance, from the current 98+% of correctly read plates to something closer to 90%, as well as the appearance of some false detections at the license-plate level.</p><p>A different kind of experiment bears on the justification of hierarchical structure, per se. As formulated in §2, an in-terpretation amounts to an annotation of a scene in terms of a multitude of parts and objects. (See Figure <ref type="figure">5</ref> for the top 25 parts and objects participating in a particular interpretation.) Consider now a highly simplified version of the licenseplate composition machine, consisting of only the bottom two layers. The system can be used to detect characters in images. An alternative use of the character models embodied in the compositional structure would be to test at each location for the presence of a particular character, against the alternative that neither the character nor any part of the character is present. In other words, an all-or-none test instead of a test for character against the compound alternative of background or part(s). (We restrict ourselves to the character layer because the all-or-none test is nearly computationally prohibitive.) We find that recognition performance, as measured for example by the ROC curve, suffers substantially when we force an all-or-none decision. We will have more to say about this observation shortly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Concluding Remarks</head><p>The machine was "built by hand," but possibly some of it could be inferred directly from data. For example, it is not difficult to imagine parameter estimation schemes that would employ labeled or unlabeled data to statistically adjust the brick-based probabilities ( β 0 , β 1 , . . . , β n β -see §2), or the relational distributions (p c β and p 0 β ) that govern the attribute likelihood ratios. On the other hand, learning the architecture itself, including the selection of bricks, children sets, and attribute functions, is an enormously challenging problem. We have little to say on this matter except to speculate that such a system would probably have to be inferred bottom-up, one layer at a time, perhaps based upon the principle of "suspicious coincidences" articulated by Barlow in his theory of unsupervised learning <ref type="bibr" target="#b2">[3]</ref>.</p><p>We believe that there is an important connection between reusability and the persistent gap between human and machine performance in vision. As every practitioner knows, the computer vision problem would be far easier if "background" could be reasonably modeled as some kind of sim- ple, stationary, stochastic process, such as white noise or a simple Markov random field. Instead, real scenes are typically filled with structure, and structured backgrounds have a way of conspiring to look surprisingly like the objects of interest, at least as seen by artificial vision systems. We would argue that this is a manifestation of the compositional nature of the visual world, and that it is the source of the poor performance of artificial vision systems, relative to biological vision systems, when operating near the zeromissed-detection end of the ROC curve. Backgrounds and foregrounds are made of the same stuff -the same reusable parts. This would suggest that false detections occur predominantly at locations that share parts with the objects of interest, and it would argue strongly for compositional scene interpretation, whereby these locations can be labeled as parts without forcing an artificial distinction between object and background.</p><p>Consistent with these observations, we have been careful to define an interpretation as any complete subgraph (see §2), including multiple trees rooted at multiple levels. As mentioned earlier, experiments that artificially limit interpretations to include, say, either a full character, on the one hand, or no part of a character on the other, result in inferior ROC performance. In essence, by building compositional representations for the objects of interest, we equip these same objects with effective background models, namely their proper subtrees.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Vertical slice through a "composition machine". Each row extends to a two-dimensional sheet of "bricks". See text for details.</figDesc><graphic coords="2,308.86,300.66,212.60,159.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Semantic hierarchy for plate-reading application. All told, there are about 500,000 bricks. See text for details.</figDesc><graphic coords="3,50.11,72.11,236.20,177.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Samples from Markov backbone (upper panel, '4850') and compositional distribution (lower panel, '8502').</figDesc><graphic coords="4,74.73,225.80,146.79,115.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Extracted plate region of sample images</figDesc><graphic coords="6,339.63,308.87,183.44,90.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 5 .</head><label>65</label><figDesc>Figure 6. Test image and its parse with license and boundary objects</figDesc><graphic coords="7,132.85,81.94,149.22,106.95" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Supported by Army Research Office contract DAAD19-02-1-0337, National Science Foundation grant DMS-0427223, and National Science Foundation grant IIS-0423031 as part of the NSF/NIH Collaborative Research in Computational Neuroscience Program.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For simplicity of the figure, children are depicted as residing exclusively in the layer immediately below. In fact, children can reside at any level below a parent. As an example -see Figure2-a plate-boundary brick from the third layer from the top composes with a license-number brick from the second layer from the top to instantiate a license-plate brick in the top layer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A given low-level brick participates (is active) in many more interpretations than a given high-level brick.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We wish to thank the Visics Corporation for supplying the license-plate data, and Eric Hopkins, President of Visics, for a great deal of information on the state of the art in license-plate reading.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A coarse-to-fine strategy for multi-class shape detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>IEEE Trans. PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pop: Patchwork of parts models for object recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trouve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Chicago</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What is the computational goal of the neocortex?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Barlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-Scale Neuronal Theories of the Brain</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Davis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recognition-by-components: A theory of human image understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compositionality, mdl priors, and object recognition</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Potter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Mozer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical testing designs for pattern recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1155" to="1202" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Class specific top downsegmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="110" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Aspects of the Theory of Syntax</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial priors for part-based recognition using statistical models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pattern classification and scene analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deterministic and stochastic approaches to nonlinear filtering</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mceneaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Control Signals Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="109" to="142" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionism and cognitive architecture: a critical analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fodor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pylyshyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3" to="71" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning lowlevel vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Jour. of Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="25" to="47" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discussion of three papers regarding the asymptotic consistency of boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Syntactic Methods in Pattern Recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neocognitron: A hierarchical neural network capable of visual pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="119" to="130" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the formulation of a composition machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Division of Applied Mathematics, Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks and the bias/variance dilemma</title>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probability and statistics in computational linguistics, a brief review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical foundations of speech and language processing</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coarse-to-fine search and rank-sum statistics in object recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Manbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcclure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Division of Applied Mathematics</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Brown University</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Composition systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly of Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">LX</biblScope>
			<biblScope unit="page" from="707" to="736" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to parse images</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 12</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="463" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sequential learning of reusable parts for object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krempp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amit</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Johns Hopkins University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Esssai philosophique sur les probabilités</title>
		<author>
			<persName><forename type="first">P</forename><surname>Laplace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1965">1965</date>
			<publisher>Translation of Truscott and Emory</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Nonparametrics: Statistical Methods Based on Ranks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lehmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Holden-Day, Inc</publisher>
			<pubPlace>San Francisco</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On estimation of a probability density function and mode</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Models of object recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="page" from="1199" to="1204" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic complexity and modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1080" to="1100" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="860" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Noticing familiar objects in real world scenes: The role of temporal cortical neurons in natural vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sheinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Logothetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2001">1340 V1350. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatial random tree grammars for modeling hierarchal structure in images</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pollak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004-05">May 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bayesian reasoning on qualitative descriptions from images and speech</title>
		<author>
			<persName><forename type="first">G</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sagerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="155" to="172" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-validatory choice and assessment of statistical predictors (with discussion)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J.R. Statist. Soc., B</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="111" to="147" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image modeling with position-encoding dynamic trees</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="859" to="871" />
			<date type="published" when="2003-07">July 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sharing features: efficient boosting procedures for multiclass object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="762" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image parsing: unifying segmentation, detection and recognition</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. of Computer Vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Sequence-seeking and counter streams: A computational model for bi-directional information flow in the visual cortex</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral Cortex</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visual features of intermediate complexity and their use in classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vidal-Naquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Information processing in the primate visual system: an integrated systems perspective</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Essen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Felleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">255</biblScope>
			<biblScope unit="page" from="419" to="423" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Estimation of Dependences Based on Empirical Data</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982">1982</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">The Nature of Statistical Learning Theory</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust real time object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Jour. Comp. Vis</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of some smoothing problems and the method of generalized cross-validation for solving them</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Statistics</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Krishnaiah</surname></persName>
		</editor>
		<imprint>
			<publisher>North Holland</publisher>
			<date type="published" when="1977">1977</date>
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning in gibbsian feilds: How accurate and how fast can it be?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1001" to="1006" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
