<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Amy</forename><forename type="middle">X</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haoran</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Moses</surname></persName>
						</author>
						<title level="a" type="main">Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1101/2020.09.04.283929</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretrained embedding representations of biological sequences which capture meaningful properties can alleviate many problems associated with supervised learning in biology. We apply the principle of mutual information maximization between local and global information as a selfsupervised pretraining signal for protein embeddings. To do so, we divide protein sequences into fixed size fragments, and train an autoregressive model to distinguish between subsequent fragments from the same protein and fragments from random proteins. Our model, CPCProt, achieves comparable performance to state-of-the-art selfsupervised models for protein sequence embeddings on various downstream tasks, but reduces the number of parameters down to 0.9% to 8.9% of benchmarked models. Further, we explore how downstream assessment protocols affect embedding evaluation, and the effect of contrastive learning hyperparameters on empirical performance. We hope that these results will inform the development of contrastive learning methods in protein biology and other modalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to improved sequencing technologies, the size of protein databases have seen exponential growth over the past decades <ref type="bibr" target="#b9">(Consortium, 2019)</ref>. However, the cost and time associated with obtaining labels for supervised problems on these proteins presents an important challenge. In response, recent works look towards self-supervised pretraining for obtaining informative fixed-length embeddings from amino acid sequences. Given input sequences, a loss is minimized that does not rely on labels related to the quantity of inter- †Work done at the University of Toronto and Vector Institute. 1 insitro 2 Department of Computer Science, University of Toronto 3 Vector Institute for Artificial Intelligence 4 Department of Cell and Systems Biology, University of Toronto. Correspondence to: Alan Moses &lt;alan.moses@utoronto.ca&gt;.</p><p>est (e.g. function or structure), but derived from the data itself. This circumvents the need to obtain experimental or expert-annotated labels.</p><p>Figure <ref type="figure">1</ref>. Downstream performance on protein-related tasks obtained by finetuning embeddings from pretrained models, plotted against the number of parameters in the pretrained model. Orange stars denote our model, and blue crosses denote methods benchmarked in <ref type="bibr" target="#b42">Rao et al. (2019)</ref>. ρ denotes Spearman's correlation for regression tasks, and "Acc" indicates accuracy.</p><p>Self-supervised learning based on contrastive tasks aim simply to tell apart aspects of the data, as opposed to generative tasks, such as inpainting <ref type="bibr" target="#b38">(Pathak et al., 2016)</ref> or autoencoding <ref type="bibr" target="#b26">(Kingma &amp; Welling, 2013)</ref>, which train models to generate part or all of the data. A major advantage of contrastive learning, in principle, is that no complicated decoding of the latent space is required <ref type="bibr" target="#b37">(Oord et al., 2018)</ref>. Moving model capacity to the encoder to obtain embeddings with high predictive power is appealing for several reasons. First, they can be portable for computational biologists without the computational resources to train large models. Second, collapsing sequence data into an informative latent space help with clustering and leveraging similarity information between proteins to learn more about protein mechanisms and function.</p><p>Recent works demonstrate self-supervised pretraining of protein sequences can yield embeddings which implicitly capture properties such as phylogenetic, fluorescence, pairwise contact, structural, and subcellular localization. However, many of these works directly take embedding techniques from natural language processing (NLP) tasks <ref type="bibr" target="#b59">(Yang et al., 2018a;</ref><ref type="bibr" target="#b44">Riesselman et al., 2019;</ref><ref type="bibr" target="#b45">Rives et al., 2019;</ref><ref type="bibr" target="#b3">Alley et al., 2019;</ref><ref type="bibr" target="#b20">Heinzinger et al., 2019;</ref><ref type="bibr" target="#b14">Elnaggar et al., 2019;</ref><ref type="bibr" target="#b4">Armenteros et al., 2019;</ref><ref type="bibr" target="#b33">Madani et al., 2020;</ref><ref type="bibr" target="#b15">Elnaggar et al., 2020)</ref>. Presumably, using a more biologicallymotivated proxy task will yield better insights and performance on biological data. Some methods incorporate biological information such as protein-protein interactions <ref type="bibr" target="#b36">(Nourani et al., 2020)</ref>, or structured labels from SCOP <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019)</ref> and PDB <ref type="bibr" target="#b18">(Gligorijevic et al., 2019)</ref>; however, high-quality curation of these labels circle back to the need for expensive experiments.</p><p>Recently, there has been growing interest in using a mutual information (MI) maximization objective for obtaining selfsupervised representations <ref type="bibr" target="#b55">(Tschannen et al., 2019)</ref>. At its core, biological sequences prescribe a set of information for function, structure, etc.; the ideas of information content and error-correcting codes seem therefore to be natural fits for modelling the evolutionary and functional constraints which drove protein sequences towards its currently observed complexity. Indeed, information theoretic principles have been explored in computational biology since the the 1970s <ref type="bibr" target="#b17">(Gatlin et al., 1972)</ref>, and subsequently applied to many modelling problems in biological sequence analysis <ref type="bibr" target="#b48">(Roman-Roldan et al., 1996;</ref><ref type="bibr" target="#b56">Vinga, 2014)</ref> and molecular biology <ref type="bibr" target="#b1">(Adami, 2004)</ref>, such as sequence logo visualization <ref type="bibr" target="#b51">(Schneider &amp; Stephens, 1990)</ref>, transcription factor binding site discovery <ref type="bibr" target="#b52">(Stormo, 2000)</ref>, structure prediction of protein loops <ref type="bibr" target="#b29">(Korber et al., 1993)</ref>, and evolutionary conservation of sequence features <ref type="bibr" target="#b41">(Rao et al., 1979;</ref><ref type="bibr" target="#b40">Pritišanac et al., 2019)</ref>. Using the information content of protein sequence patches as a pretraining objective better aligns with underlying biological principles as compared to directly lifting methods from NLP to biology. If patches capture motifs, unusual structural elements, regions of unusual amino acid composition, parts of catalytic sites, etc., the model must capture this implicit information in its latent representation of the protein during self-supervised pretraining.</p><p>In this work, we present CPCProt, which maximize mutual information between context and local embeddings by minimizing a contrastive loss. On protein-related downstream benchmarks <ref type="bibr" target="#b42">(Rao et al., 2019)</ref>, CPCProt achieves comparable results, despite using 0.9% the number of pretraining parameters of the largest model <ref type="bibr" target="#b3">(Alley et al., 2019)</ref> and 8.9% of the number of parameters of the smallest neural network model <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019)</ref>, as illustrated in Figure <ref type="figure">1</ref>. We further explore difficulties in using downstream performance as a means to assess protein embeddings, and explore the effect of contrastive learning parameters, to motivate the development of similar methods in protein biology and other modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>The InfoMax optimization principle aims to find a mapping g (constrained by a function class G), which maps from input X to output g(X), such that the Shannon mutual information between the pair is maximized <ref type="bibr" target="#b30">(Linsker, 1988)</ref>:</p><formula xml:id="formula_0">max g∈G I(X; g(X)).</formula><p>(1)</p><p>Conceptually, therefore, the training task of finding a desirable encoder which maps input sequences to output embeddings is performing InfoMax optimization.</p><p>It is possible to use a variational approach to estimate a bound on the mutual information between continuous, highdimensional quantities <ref type="bibr" target="#b12">(Donsker &amp; Varadhan, 1983;</ref><ref type="bibr" target="#b35">Nguyen et al., 2010;</ref><ref type="bibr" target="#b2">Alemi et al., 2016;</ref><ref type="bibr" target="#b6">Belghazi et al., 2018;</ref><ref type="bibr" target="#b37">Oord et al., 2018;</ref><ref type="bibr" target="#b39">Poole et al., 2019)</ref>. Recent works capture this intuition to yield self-supervised embeddings in the modalities of imaging <ref type="bibr" target="#b37">(Oord et al., 2018;</ref><ref type="bibr" target="#b22">Hjelm et al., 2018;</ref><ref type="bibr" target="#b5">Bachman et al., 2019;</ref><ref type="bibr" target="#b53">Tian et al., 2019;</ref><ref type="bibr" target="#b21">Hénaff et al., 2019;</ref><ref type="bibr">Löwe et al., 2019;</ref><ref type="bibr" target="#b19">He et al., 2019;</ref><ref type="bibr" target="#b8">Chen et al., 2020;</ref><ref type="bibr" target="#b54">Tian et al., 2020;</ref><ref type="bibr" target="#b57">Wang &amp; Isola, 2020</ref>), text <ref type="bibr" target="#b46">(Rivière et al., 2020;</ref><ref type="bibr" target="#b37">Oord et al., 2018;</ref><ref type="bibr" target="#b28">Kong et al., 2019)</ref>, and audio <ref type="bibr">(Löwe et al., 2019;</ref><ref type="bibr" target="#b37">Oord et al., 2018)</ref>, with high empirical downstream performance.</p><p>The general formulation is: given input X, define {X (1) , X (2) } as two different "views" of X (e.g. patches of an image, or representations of different sequence timesteps), and encoders {g 1 , g 2 } which encode {X (1) , X (2) } respectively. The goal is to find encoder mappings which maximize the mutual information between the outputs, and can be shown to lower bound the InfoMax objective in Equation 1 <ref type="bibr" target="#b55">(Tschannen et al., 2019)</ref>:</p><formula xml:id="formula_1">max g1∈G1,g2∈G2 I (g 1 (X (1) ); g 2 (X (2) )) (2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>We describe CPCProt, which applies the InfoNCE loss introduced by the Contrastive Predictive Coding (CPC) method <ref type="bibr" target="#b37">(Oord et al., 2018)</ref> to protein sequences.</p><p>(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.</p><p>The copyright holder for this preprint this version posted September 6, 2020. ; https://doi.org/10.1101/2020.09.04.283929 doi: bioRxiv preprint Self-Supervised Contrastive Learning of Protein Representations By Mutual Information Maximization</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Predictive Coding and InfoNCE</head><p>Here, we formalize the InfoNCE loss for mutual information maximization in the language of protein sequences. The CPC method introduces a lower-bound estimator for the unnormalized mutual information between two continuous quantities. It is named as such as it stems from the noise-contrastive estimation (NCE) method <ref type="bibr" target="#b19">(Gutmann &amp; Hyvärinen, 2010)</ref>. NCE fits logistic regression parameters to distinguish data from noise (i.e. contrastively), in order to parameterize models in high dimensions, and InfoNCE directly adapts this contrastive task to estimate mutual information instead.</p><p>Define g 1 and g 2 from Equation 2 to be an encoder and autoregressor respectively, which we denote as g enc and g ar . Further, define x as an input protein sequence, z as the latent embedding produced by g enc (x), and c as the long-range protein sequence "context", as summarized by the autoregressor g ar (z). At a given position t (indexed in the latent space, i.e. for z), we compute the InfoNCE mutual information estimation I N CE (z t+k ; c t ) for k ∈ {1, 2, . . . , K}, for a batch of N samples. The estimation is performed by minimizing the loss:</p><formula xml:id="formula_2">L t+k = −E log exp(f (z t+k , c t )) exp(f (z t+k , c t )) + N −1 j=1 exp(f (z j , c t ))</formula><p>In other words, in each batch of N samples, we have a single sample z t+k drawn jointly with c t from p(z t+k , c t ). Then, following the NCE method, we draw N − 1 "fake" samples from the noise distribution p(z ) to create a set of {z j } N −1 j=1 . In practice, the expectation is taken over multiple batches. This objective is a contrastive task, using a cross-entropy loss which encourages a critic, f , to correctly identify the single "real" sample of z t+k . Minimizing this loss provides an unnormalized lower-bound estimate on the true MI, I N CE (z t:(t+K) ; c t ) <ref type="bibr" target="#b37">(Oord et al., 2018)</ref>.</p><p>From the representation learning perspective, we obtain embeddings which maximize the mutual information between the protein "context" (e.g. long-range contacts) and the local embedding space (e.g. local secondary structure). Next-token prediction as a self-supervised proxy task has previously been successfully used to learn representations of proteins <ref type="bibr" target="#b3">(Alley et al., 2019)</ref>, and both methods share the underlying intuition that capturing latent information between current and future positions in the protein sequence may richly capture a down-sampled or "patched" version of the input amino acid sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CPCProt</head><p>We adapt a strategy from image experiments <ref type="bibr" target="#b37">(Oord et al., 2018;</ref><ref type="bibr" target="#b21">Hénaff et al., 2019;</ref><ref type="bibr" target="#b22">Hjelm et al., 2018;</ref><ref type="bibr" target="#b5">Bachman et al., 2019)</ref> to protein sequences, in which the input x (zeropadded up to the longest sequence in a batch) is divided into fixed-length patches. Each patch is passed into an encoder to output a single pooled embedding for the patch. These embeddings are then concatenated into the latent embedding z; that is, the length of z becomes L z = sequence length patch length .</p><p>Here, a patch length of 11 is selected, such that it is long enough to capture some local structural information and also results in a reasonable L z on our pretraining dataset. We start with some t min to allow c t to gain some degree of sequence context in calculations of the loss. We then calculate I N CE t+k for every t ∈ {t min , t min + 1, ..., L z − K} and k ∈ {1, 2, ..., K}. Patching reduces the number of InfoNCE calculations necessary), and ensures that the inputs used to create z 1 , z 2 , ..., z k do not overlap, to reduce triviality of the task. A schematic detailing the method is illustrated in Figure <ref type="figure" target="#fig_0">2</ref>.</p><p>The final loss minimized in each batch is the average of calculated L t+k for all values of t and k, i.e.: Architecture In <ref type="bibr" target="#b37">Oord et al. (2018)</ref>, it is shown a simple architecture can achieve good performance with NLP tasks;</p><formula xml:id="formula_3">L = 1 Lz−K−tmin 1 K Lz−K t=tmin K k=1 L t+k<label>(3)</label></formula><p>(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.</p><p>The copyright holder for this preprint this version posted September 6, 2020. ; https://doi.org/10.1101/2020.09.04.283929 doi: bioRxiv preprint we design a simple convolutional encoder with increasing number of filters in the top layers, to keep a lightweight number of connections.</p><p>The CPCProt encoder consists of embedding layer to 32 hidden dimensions, followed by 64 filters of length 4, 64 filters of length 6, and 512 filters of length 3. Filter lengths are designed such that the output of an 11 amino acid patch has an output length of one. ReLU activation is added to the output of each layer, and normalization is added channelwise, to avoid using batch normalization, since negative samples are drawn in-batch (see Section 3.2).</p><p>In addition, we also report results using a larger encoder. Both CPCProt GRU large and CPCProt LSTM uses an embedding layer to 64 hidden dimensions, followed by 128 filters of length 4, 256 filters of length 4, and 512 filters of length 3; in the final layer, CPCProt GRU large uses 1024 filters of length 3, whereas CPCProt LSTM uses 2048 filters.</p><p>For both CPCProt and CPCProt GRU large , a single-layer GRU is used as the autoregressor, while CPCProt LSTM uses a two-layer LSTM. To avoid information leakage about later sequence locations in the context vector, we only use unidirectional autoregressors. All autoregressors use the same number of hidden dimensions as encoder output.</p><p>Distinguishing Noise Samples To encourage the encoder to learn a richer embedding, and to mitigate the need to train separate critics for each position of k, we modify the bilinear critic used in <ref type="bibr" target="#b37">Oord et al. (2018)</ref>, and instead use a parameterless dot product critic for f <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>. Rather than using a memory bank <ref type="bibr" target="#b58">(Wu et al., 2018;</ref><ref type="bibr" target="#b53">Tian et al., 2019;</ref><ref type="bibr" target="#b19">He et al., 2019)</ref>, we draw "fake" samples from p(z) and p(c) using other z t+k and c t from other samples in the same batch <ref type="bibr" target="#b8">(Chen et al., 2020)</ref>. That is, the diagonal of the output of the dot-product critic is the "correct pairing" of z (i) t+k and c (i) t at a given t and k, and the softmax is computed using all off-diagonal entries to draw the N − 1 "fake" samples from the noise distribution p(z ).</p><p>Additional Pretraining Details We use t min = 1 and choose K = 4 (that is, 44 amino acids away). Sequences are zero-padded up to the longest sequence in that batch, and protein domains are truncated to a maximum of 550 amino acids due to memory constraints. Sequences shorter than the minimum length needed to perform the NCE calculation is discarded. Since k is 1-indexed, the minimum training sequence length is (t min + 1 + K) × patch length. For our default models, this minimum length is (1+1+4)×11 = 66 amino acids. A large K incentives the model to pick up more information during training, but decreases computational efficiency. Furthermore, because we throw away sequences which are too short to complete the NCE loss at t min + K, a large choice of t min and K results in discarding more data during training.</p><p>For CPCProt, we use a batch size of 64, trained for 19 epochs with a constant learning rate of 1e-4. For CPCProt LSTM , a batch size of 1024 was used, trained for 18 epochs with an initial learning rate of 1e-3 and decayed by γ = 0.85 at each epoch. For CPCProt GRU large , a batch size of 1024 is used with a constant learning rate of 1e-4, trained for 24 epochs. All models use the Adam optimizer with β 1 = 0.9, β 2 = 0.999, and = 1e-8 <ref type="bibr" target="#b25">(Kingma &amp; Ba, 2014)</ref>. Convergence was determined as no improvement in the validation contrastive loss for 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Data and Downstream Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pretraining</head><p>Pretraining Data We pretrain our models using protein domain sequences from the Pfam database <ref type="bibr" target="#b13">(El-Gebali et al., 2019)</ref>. To compare only the effectiveness of the InfoNCE objective, we pretrain using the same data splits as used for downstream benchmarks <ref type="bibr" target="#b42">(Rao et al., 2019)</ref>, which holds out six Pfam clans as the test set. 5% of the remaining sequences are used for validation, and the remaining 32,207,059 Pfam amino acid sequences are used for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating Embeddings for Hyperparameter Selection</head><p>We examine three evaluations for pretraining hyperparameter selection: (1) Performance on the validation dataset on downstream tasks (see Section 4.2); (2) contrastive accuracy on the pretraining validation data; and (3) Pfam family prediction using a 1-nearest-neighbor (1NN) classifier on the pretraining validation data. The latter two evaluation metrics are explored to avoid overfitting to benchmark datasets <ref type="bibr" target="#b43">(Recht et al., 2019)</ref>. Though in principle, the contrastive accuracy on heldout pretraining data is sufficient for hyperparameter selection, we were concerned that the contrastive task is relatively local, and may fail to assess how well embeddings have captured the global context.</p><p>The 1NN classification task is a direct measure of the ability for embeddings to cleanly separate Pfam domains in the latent space, and requires no parameter tuning or additional labels for evaluation. For this task, the dataset consists of sequences from the 50 Pfam families with the most sequences in the pretraining validation dataset, subsampled to 120 sequences per family for class balance. 70% of this embeddings is used to populate the 1NN classifier, and 30% of the sequences are used at the classification phase. A t-SNE of CPCProt embeddings colored by the 50 families is shown in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>For the contrastive task (i.e. the self-supervised pretraining task), we keep the ratio of negative-to-positive samples consistent and use a batch size of 512 for all models for this validation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream Evaluation Protocols</head><p>To understand properties of the embeddings, we examine performance on various supervised downstream tasks, derived from Tasks Assessing Protein Embeddings (TAPE) <ref type="bibr" target="#b42">(Rao et al., 2019)</ref>, using three different downstream evaluation models: a small neural network head, linear models, and k-nearest-neighbours (kNN).</p><p>Neural Network<ref type="foot" target="#foot_0">1</ref> Finetuning Head Evaluation For better consistency with benchmark protein-related tasks, we use the same default downstream architectures as provided by the authors in the tape-proteins package, version 0.4 <ref type="bibr" target="#b42">(Rao et al., 2019)</ref>. In line with benchmark methods, we allow backpropagation through the embeddings during finetuning. For consistency, we do not tune the architecture of the neural network heads. A grid search is conducted on the learning rate, batch size, and number of finetuning epochs, and test set results are reported for the model with the best performance on the validation set for a given task.</p><p>Using a non-linear neural network headwith end-to-end finetuning presumably yields better performance on downstream tasks, and offers a better evaluation of capabilities which can be derived from the embeddings, for those working directly with the biological tasks assessed in this work.</p><p>Linear and Nearest-Neighbours Downstream Evaluation Following previous work in self-supervised learning, we also assess embeddings using a linear model <ref type="bibr" target="#b37">(Oord et al., 2018;</ref><ref type="bibr" target="#b21">Hénaff et al., 2019;</ref><ref type="bibr" target="#b19">He et al., 2019;</ref><ref type="bibr" target="#b5">Bachman et al., 2019;</ref><ref type="bibr" target="#b53">Tian et al., 2019;</ref><ref type="bibr">2020)</ref>. Note that as compared to the neural network finetuning head evaluation, we use static embeddings extracted from the model without end-to-end optimization. For classification tasks, we use a logistic regression model, while for regression tasks, we use a linear regression model. For logistic regression models, we report the best result from conducting a simple grid search over the inverse regularization strength C ∈ {1, 0.01, 0.0001}.</p><p>In addition, we evaluate separation in the latent space using a kNN model with a grid search over the number of neighbours k ∈ {1, 5, 10}. Due to computational considerations, we do not use the kNN for secondary structure prediction.</p><p>Evaluating using a simple classifier assesses several desirable embedding properties: (1) limiting the modelling capacity given to the downstream models is a more direct assessment of the powers of the embedding methods themselves;</p><p>(2) linear/logistic regression and kNN is more suitable than a neural network for downstream tasks where data availability is a constraint, as is the case for many biological use cases;</p><p>(3) logistic regression has a convex loss whose optimization is relative more straight-forward than the non-convex training of a neural network. Similarly, a kNN consists simply of a look-up at test time. This arguably removes some ambiguity regarding if differences in performance should be attributed to lack of convergence or improper hyperparameter tuning when finetuning using a neural network head; and (4) as we note in Section 4.3, there exist minor architectural differences between the neural network architectures released in tape-proteins and the original benchmarks in <ref type="bibr" target="#b42">Rao et al. (2019)</ref>. In conjunction with the neural network head finetuning results, we hope that a more holistic picture of the embeddings can be provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Downstream Evaluation Tasks</head><p>Remote Homology Remote sequence homologs share conserved structural folds but have low sequence similarity. The task is a multi-class classification problem, consisting of 1195 classes, each corresponding to a structural fold. Since global context from across the Pfam domain is important, we use the final position of the autoregressor output, c.</p><p>Data from the SCOP 1.75 database <ref type="bibr" target="#b16">(Fox et al., 2013)</ref> is used. Each fold can be sub-categorized into superfamilies, and each superfamily sub-categorized into families. The training, validation, and test set splits are curated in <ref type="bibr" target="#b23">Hou et al. (2018)</ref>; test sets examines three levels of distribution shift from the training dataset. In the "Family" test set, proteins in the same fold, superfamily, and family exists in both the training and testing datasets (i.e. no distribution shift). The "Superfamily" test set holds out certain families within superfamilies, but sequences with overlap with training dataset at the superfamily level. Finally, the "Fold" test set also holds out certain superfamilies within folds. Note that severe class imbalance exists for this task, as 433 folds in the training dataset only contains one sample.</p><p>For evaluation using a neural network head, the classification architecture is a multi-layer perceptron (MLP) with one hidden layer of 512 units. Note that results in benchmarked models also train a simple dense layer to obtain an attention vector before calculating an attention-weighted mean.</p><p>Secondary Structure Secondary structure is a sequenceto-sequence task evaluating the embeddings' ability to capture local information <ref type="bibr" target="#b42">(Rao et al., 2019)</ref>. We report threeclass accuracy (Q3), following the DSSP labeling system <ref type="bibr" target="#b24">(Kabsch &amp; Sander, 1983)</ref>. Each input amino acid is mapped to one of three labels ("helix", "strand", or "other"), and accuracy is the percentage of correctly-labelled positions. To obtain the embedding, we use a sliding input window to obtain z with the same length as the input sequence, and then use c as the embedding to incorporate global context.</p><p>Classification results are presented on three datasets: (1) TS115, consisting of 115 protein sequences <ref type="bibr" target="#b60">(Yang et al., 2018b)</ref>; (2) CB513, consisting of 513 protein regions from 434 proteins <ref type="bibr" target="#b10">(Cuff &amp; Barton, 1999)</ref>; and (3) free-modelling targets from the 2016 CASP12 competition, consisting of 21 protein sequences <ref type="bibr" target="#b0">(Abriata et al., 2018;</ref><ref type="bibr" target="#b34">Moult et al., 2018)</ref>.</p><p>For training these supervised classifiers, the same validation and filtered training datasets as NetSurf-2.0 is used, where sequences with greater than 25% sequence similarity as the three test set sequences were removed from the training set <ref type="bibr" target="#b27">(Klausen et al., 2019)</ref>.</p><p>For evaluation using a neural network head, the classification architecture in tape-proteins is a convolutional architecture with 512 filters of size 5 and 3 in layers one and two, respectively. The original benchmarks use a higher capacity NetSurfP model <ref type="bibr" target="#b27">(Klausen et al., 2019)</ref>, with two convolutional layers followed by two bidirectional LSTM layers and a linear output layer.</p><p>Fluorescence The fluorescence task is a protein engineering task which evaluates how fine-trained local genotypic changes can be captured to predict phenotypic expression, as measured by native fluorescence. The regression task is to predict the log-intensity of a mutant GFP sequence. Since this task is more sensitive to local than global information, we apply a mean-pool along the sequence dimension of the encoder output, z.</p><p>The data is from a Deep Mutational Scan (DMS) experiment from <ref type="bibr" target="#b49">Sarkisyan et al. (2016)</ref>, which measures fluorescence from derivative genotypes of the green fluorescent protein avGFP. Data splits are curated in <ref type="bibr" target="#b42">Rao et al. (2019)</ref>. Training and validation data are in a Hamming distance 3 neighborhood from the original protein, while the test data exhibits larger distribution shift and is from the Hamming distance 4-15 neighborhood.</p><p>For evaluation using a neural network head, tape-proteins uses the same MLP architecture as described in the remote homology task. The original benchmarks in <ref type="bibr" target="#b42">Rao et al. (2019)</ref> compute an trainable attention-weighted mean prior to classification.</p><p>Stability Stability is a protein engineering task which measures the most extreme concentration for which a protein can maintain its structure. This is a regression task to predict a stability score of proteins generated by de novo design. Since this task is also sensitive to fine-grained local effects, we use the mean along the encoder output z as a pooled embedding.</p><p>The data is from <ref type="bibr" target="#b47">Rocklin et al. (2017)</ref>, which measures the stability of proteins generated by parallel DNA synthesis, consisting of sequences from four protein topologies: ααα, βαββ, αββα, ββαββ. The stability score is the difference between the measured EC 50 of the actual protein and its predicted EC 50 in its unfolded state. Here, EC 50 is the protease concentration at which 50% of cells pass the characterization threshold; note that it is measured on a log 10 scale. Data splits are curated in <ref type="bibr" target="#b42">Rao et al. (2019)</ref>, such that the test set consists of seventeen 1-Hamming distance neighbourhoods from the training and validation datasets. A visualization of this test split is shown in Figure <ref type="figure" target="#fig_2">4</ref>.</p><p>For evaluation using a neural network head, as with remote homology and fluorescence, we use the provided MLP architecture, while the original benchmarks compute an trainable attention-weighted mean prior to classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Downstream Tasks Evaluation</head><p>We evaluate the quality of the learned embeddings using the downstream tasks described in Section 4. As motivated in Section 4.2, we use different heads for downstream evaluation. In Table <ref type="table" target="#tab_0">1</ref>, we report results using neural network finetuning heads, and in Table <ref type="table" target="#tab_1">2</ref>-4, downstream results using simple linear and/or kNN models are reported.</p><p>We also evaluate two variant CPCProt models, selected during hyperparameter tuning by metrics which do not depend on downstream tasks, to avoid benchmark overfitting, and to examine the effect of increasing the number of parameters on downstream performance, as motivated in Section 4.1. The CPCProt LSTM variant achieves the highest performance on the contrastive task, while the CPCProt GRU large variant achieves the highest performance on the 1NN Pfam family prediction task. Results on these tasks are reported in Appendix A. For all figures, however, we focus on our smallest 1.7M CPCProt model. as baselines on most tasks; however, we use only 0.9% of the number of embedding parameters of the largest model <ref type="bibr" target="#b3">(Alley et al., 2019)</ref> and 8.9% of the number of embedding parameters of the smallest neural network <ref type="bibr" target="#b7">(Bepler &amp; Berger, 2019)</ref> (Tables <ref type="table" target="#tab_4">1-5</ref>; Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># of</head><p>For the fluorescence task, CPCProt achieves higher ρ and lower MSE than other models for both a neural network finetuning head and most linear regression and kNN evaluations (Tables <ref type="table" target="#tab_4">1, 5</ref>; Appendix B). For the secondary structure and stability tasks, CPCProt achieves comparable performance with other neural network models, with a fraction of the number of parameters (Tables <ref type="table" target="#tab_2">1, 3</ref>, 4).</p><p>For remote homology, when using a neural network for downstream evaluation, the difference in Top-1 accuracy between CPCProt and other neural network models is more drastic (Table <ref type="table" target="#tab_0">1</ref>); however, when using logistic regression and kNN for downstream classification, this gap in accuracy decreases (Table <ref type="table" target="#tab_1">2</ref>). As noted in Section 4.3, this is a highly class-imbalanced and multi-class task, and sometimes a oneshot problem for some classes in the Fold test set. This may contribute to the failure of all embeddings to generalize to (which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission.</p><p>The copyright holder for this preprint this version posted September 6, 2020. ; https://doi.org/10.1101/2020.09.04.283929 doi: bioRxiv preprint the Fold test set even in the presence of labels, as well as the variability in changes to model performance when switching from a neural network to a simpler classifier (Tables <ref type="table" target="#tab_1">1, 2</ref>). As expected, as holdout test set distributions shift more from the training data, performance deteriorates for all models (Tables <ref type="table" target="#tab_1">1, 2</ref>). Downstream Assessment is Inconsistent Using Different Models For the purpose of using downstream tasks primarily as a means to evaluate the quality of embeddings, changing the downstream model used sometime result in preferring different models (Tables <ref type="table" target="#tab_1">1, 2</ref>, 3, 4, 5), as does using a different performance metric (i.e. MSE versus Spearman's ρ for regression tasks) (Tables <ref type="table" target="#tab_4">4, 5</ref>, Appendix B). For example, though CPCProt appear to generalize poorly to the Fold test set for the remote homology task relative to baselines using a neural network classifier head (Table <ref type="table" target="#tab_0">1</ref>), it in fact outperforms both BERT and UniRep when using a kNN classifier (Table <ref type="table" target="#tab_1">2</ref>). For fluorescence and stability, which must capture fine-grained local effects, using a simple linear regression or kNN model seem to better differentiate performances of UniRep, BERT, and CPCProt variants (Tables <ref type="table" target="#tab_4">4, 5</ref>).</p><p>For stability and fluorescence, performance improves when using a finetuned neural network head for most embeddings, potentially reflecting the non-linear interactions needed to map local sequence effects to protein engineering measurements (Tables <ref type="table" target="#tab_4">1, 4, 5</ref>). In contrast, for secondary structure, finetuning with a higher capacity MLP actually decreases Q3 accuracy. For example, for the CASP12 test set, using a linear model instead of a MLP increases Q3 accuracy by 0.07 to 0.12 for BERT, UniRep, and CPCProt variants (Tables 1, 3). Note that the CASP12 test set consists only of 12 sequences, and the secondary structure training dataset is also the smallest out of the tasks compared (Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">CPCProt Qualitatively Captures Protein Stability Information without Supervised Training</head><p>As a motivating example to further "attribute responsibility" of downstream performance measurements to the pretrained model and finetuning procedures, we leverage the ability for t-SNE to preserve local neighbour information in high dimensions, and choose to examine the test set curated by <ref type="bibr" target="#b42">Rao et al. (2019)</ref> of the stability data from <ref type="bibr" target="#b47">Rocklin et al. (2017)</ref>.</p><p>The data was heldout to be from seventeen 1-Hamming distance neighbourhoods away from seventeen candidate proteins. Seventeen local structures cleanly emerge in the t-SNE embeddings from the raw one-hot encoded sequences alone (Figure <ref type="figure" target="#fig_2">4</ref>). However, within each cluster of sequences derived from the same candidate protein, sequences close in stability are not assigned local neighbour relationships. After embedding using CPCProt, proteins close in stability measurements are also also close in t-SNE embedding space within each cluster, despite having never seen labels corresponding to stability during training. After updating embeddings via supervised end-to-end finetuning with stability labels using a MLP head, some new local structures emerge within clusters, though the ability for similarly stable proteins to cluster near each other do not improve drastically. This qualitatively suggests that the metrics presented in Table 1 can be reasonably attributed to the ability for CPCProt to capture implicit information about protein stability, despite not having seen any labels from stability experiments during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">More Contrastive Learning Negative Samples Does Not Improve Performance</head><p>Theoretically, the InfoNCE estimator requires a large number of negative samples to reduce variance <ref type="bibr" target="#b39">(Poole et al., 2019)</ref>. Further, as formalized in <ref type="bibr" target="#b57">Wang &amp; Isola (2020)</ref>, in the limit of infinite negative samples, the InfoMax objective has the desirable property of directly optimizing for the properties of alignment (similar samples are mapped to similar locations in the latent space) and uniformity (features preserve maximal information).</p><p>The standard practice, therefore, is that increasing the number of samples help performance, even at great computational costs. This has indeed yielded good results empirically <ref type="bibr" target="#b22">(Hjelm et al., 2018;</ref><ref type="bibr" target="#b53">Tian et al., 2019;</ref><ref type="bibr" target="#b19">He et al., 2019;</ref><ref type="bibr" target="#b5">Bachman et al., 2019;</ref><ref type="bibr" target="#b58">Wu et al., 2018;</ref><ref type="bibr" target="#b8">Chen et al., 2020)</ref>, However, in <ref type="bibr" target="#b50">Saunshi et al. (2019)</ref>, it is theoretically and empirically shown that larger number of negative samples may decrease downstream performance under certain conditions. As previous literature points out <ref type="bibr" target="#b55">(Tschannen et al., 2019;</ref><ref type="bibr" target="#b57">Wang &amp; Isola, 2020)</ref>, this empirical and theoretical disjoint in how the number of negative samples affect performance render the success of using contrastive losses for representation learning more mysterious.</p><p>In Figure <ref type="figure" target="#fig_3">5</ref>, we compare performance on the 1NN Pfam family prediction task (Section 4.1) for models pretrained using different numbers of negative samples. Our results corroborate the ideas in <ref type="bibr" target="#b50">Saunshi et al. (2019)</ref> that downstream performance does not empirically improve with the number of negative samples.</p><p>We also further explore the effect of choosing K. A larger K necessitates that the model must learn information about amino acid patches further away given the context at position t. We train six different models with different settings of K, keeping all other hyperparameters consistent. On the 1NN Pfam family prediction task, asking the model to learn information about further away patches (i.e. larger K) decrease downstream performance. This may be in part due to the fact that a larger K increases the minimum sequence length needed, and results in more sequences discarded. For example, at K = 12, only Pfam sequences larger than 154  amino acids would be seen during training, which impacts performance when classifying shorter Pfam sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Importance of Using a Per-k Critic</head><p>One major difference between CPC and other InfoMax methods is that the information maximization signal is formulated as an ordered autoregressive task <ref type="bibr" target="#b22">(Hjelm et al., 2018)</ref>, and uses a different critic for each position of k. In Figure 6, the accuracy on the contrastive task at each position of t + k for each k ∈ {1, 2, 3, 4} (averaged across all t) is visualized.</p><p>As expected, it becomes more difficult distinguish latent embeddings further away from the position from which the context is derived. The ability for the final model to solve this contrastive task is compared with a random model. This also illustrates the non-triviality of the contrastive task, which the model has learned to improve on after pretraining has converged. A validation batch size of 1024 is used, and the random model achieves roughly the expected accuracy of 1 N = 1 1024 ≈ 0.00098.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Relationship Between Pretrained Model Size and Downstream Performance Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure">1</ref> show that there is no clear connection between increasing the number of parameters (in the pretrained model only) and downstream performance, contrary to the philosophy behind 567 million parameter NLP-inspired models for protein representations <ref type="bibr" target="#b15">(Elnaggar et al., 2020)</ref>. This is true even for variants of CPCProt, which were trained using the same self-supervised objective.</p><p>The finding that CPCProt achieves comparable results</p><p>with less parameters may be a reflection of the overparameterization of existing protein embedding models in the field, or of a unique benefit conferred by the contrastive training. In any case, these results show that simply porting large models from NLP to proteins is not an efficient use of computational resources, and we encourage the community to further explore this relationship. As compared to currently-available protein embedding models, we note the suitability of CPCProt for downstream use-cases where model size is a key concern.</p><p>Difficulties in Quantitatively Assessing Protein Embeddings Though we explore using the pretraining objective (i.e. the contrastive accuracy) and a simple downstream task which assesses the capturing of global information (i.e. the 1NN Pfam family prediction task) to select hyperparameters for pretraining, we find that these performance metrics do not correlate with downstream performance for all tasks. This illustrates a difficulty of selecting pretraining hyperparameters for embeddings.</p><p>Furthermore, as explored in Section 5.1, even with the availability of downstream task benchmarks and data, it is difficult to quantitatively assess embedding performance, as results differ when using different models and performance metrics (i.e. MSE vs ρ). In some cases, where downstream accuracy is not the goal, it may be better use an embedding which outperforms other models when assessed by a linear downstream model and has presumably has captured more information, despite having a lower performance overall when compared to another model that provides better initialization for an neural network finetuning head.</p><p>Moreover, it is difficult to attribute quantitative performance on downstream tasks to the information captured by the embedding, or to the supervised finetuning procedures. We attempt to analyze this qualitatively in Figure <ref type="figure" target="#fig_2">4</ref>. This is made more difficult by the inconsistency in whether if encoder weights should be frozen during training. While some works in contrastive learning freeze the encoder during finetuning <ref type="bibr" target="#b57">(Wang &amp; Isola, 2020)</ref>, which makes sense as a means to directly evaluate the embeddings, large NLP embedding models such as BERT typically update parameters end-toend <ref type="bibr" target="#b11">(Devlin et al., 2018)</ref>, as do protein models inspired by these NLP models <ref type="bibr" target="#b42">(Rao et al., 2019)</ref>, which makes sense as a means to evaluate the ability for these embeddings to achieve optimal performance for a specific task of interest.</p><p>Thus, we hope to highlight that downstream tasks should not be a definitive method to evaluate the goodness of an embedding. However, they may be good proxies to examine specific desiderata regarding global and local information or out-of-distribution generalization, when evaluated using a consistent protocol. Given the diversity of biological use cases, focusing on capturing crucial global information for one use case (e.g. long-range contacts) could be counter to another use case focused on highly localized effects (e.g. variant effect prediction). Quantitative metrics of downstream performance should guide the choice of pretrained embeddings in a case-by-case manner.</p><p>Limitations of CPC and In-Batch Negative Sampling Though we apply the CPC method to proteins, there are other representation learning methods which fall under the InfoMax training scheme, which may also be applied to proteins. Further, the choice of negative samples to approximate the marginal distribution p(z ) affects the tightness of the bound <ref type="bibr" target="#b55">(Tschannen et al., 2019;</ref><ref type="bibr" target="#b39">Poole et al., 2019;</ref><ref type="bibr" target="#b50">Saunshi et al., 2019)</ref>. We choose negative samples in-batch for computational efficiency, but a more rigorous examination of the most apt negative sampling strategy for the protein modality is needed. As noted in <ref type="bibr" target="#b55">Tschannen et al. (2019)</ref>, maximizing MI does not always result in higher performance, and the empirical performance of recent InfoMax works may be due more to tuning the contrastive task and architecture. In fact, other works empirically demonstrate a U-shaped relationship between I N CE and downstream linear evaluation performance <ref type="bibr" target="#b54">(Tian et al., 2020)</ref>. The relationship between information maximization and good empirical results remains to be investigated, and we hope that our empirical examination of selecting the number of negative samples and the size of the local embedding window can extend beyond the protein modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this work, we introduce CPCProt, which achieves comparable downstream performance as existing protein embedding models, at a fraction of the number of parameters. We further compare the effects of using different pretraining evaluation metrics and downstream models for evaluating embeddings on protein-related tasks, and find that there is poor consistency in how models compare against one another, illustrating the difficulty in defining the "goodness" of an embedding for biological use cases. In addition, we empirically evaluate hyperparameters affecting contrastive training for representation learning, and empirically corroborate the theoretical result that additional negative samples do not always improve performance <ref type="bibr" target="#b50">(Saunshi et al., 2019)</ref>.</p><p>In academic and industry settings where model size is a key concern, CPCProt may be a desirable embedding method as compared to other currently available methods, as it fits easily on a single GPU, and achieves results in the same neighborhood of performance as larger pretrained models. We hope that this work can inform the development of other embedding models for biological sequences, as well as contrastive representation learning methods beyond the protein sequence modality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Input protein sequences are divided into "patches" of 11 amino acids. Each patch is encoded into a single vector though genc, and all encodings are concatenated to form z. gar is an autoregressor that aggregates local information, and produce c, a context vector which summarizes the global context. Amino acid sequences are zero-padded to the longest sequence in the batch, with remaining amino acids not long enough for a patch discarded. For a given batch, the loss is the average of the InfoNCE estimate I t+k for all t ∈ {tmin, tmin + 1, ..., Lz − K} and k ∈ {1, 2, ..., K}.In this example batch, tmin = 1, Lz = 6, and K = 4.</figDesc><graphic url="image-2.png" coords="3,301.59,395.95,245.70,161.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. CPCProt embedding t-SNE of the 50 largest Pfam families in the validation dataset, using the final position of the context vector. Note that while colours denote different families, proximity in the continuous color space do not correspond to any intrinsic similarities between families.</figDesc><graphic url="image-3.png" coords="5,119.79,67.06,105.29,104.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. t-SNE visualization of proteins which are all 1-Hamming distance away from one of seventeen candidate proteins. Colors denote stability measurement on a log 10 scale. The data corresponds to test set curated in the TAPE benchmarks (Rao et al., 2019) for the stability dataset from Rocklin et al. (2017).</figDesc><graphic url="image-4.png" coords="9,29.34,67.06,538.22,164.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Downstream performance on a simple 1NN Pfam family classification task, using embedding models pretrained with different hyperparameters. For all plotted models, the default hyperparameters are LR=0.0001, batch size=1024, and K=4, using the same architecture as the 1.7M parameter CPCProt model. For reference, the performance of a randomly initialized CPCProt model and the 182M parameter UniRep model is also plotted. (A) Examination of the effect of batch size. Since we draw N − 1 negative samples in-batch, batch size is used as a proxy for the number of negative samples. (B) Examining the choice of K, that is, the number of "future" encoded amino acid patches which the model must learn to distinguish, using the context at that position.</figDesc><graphic url="image-5.png" coords="9,76.14,302.91,444.59,234.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Comparison of accuracy on the contrastive task on the validation set, for a randomly initialized and the final 1.7M parameter CPCProt model, reported for each position of t + k (averaged over all t ∈ {tmin, tmin + 1, ..., Lz − K}). A random model achieves the expected accuracy of 1 N = 1 1024 ≈ 0.00098, illustrating the difficulty of the pretraining task.</figDesc><graphic url="image-6.png" coords="10,55.44,67.06,234.00,220.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,295.74,244.77,257.39,236.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Embedding performance by downstream task using the default neural network finetuning head, compared against Tasks Assessing Protein Embeddings (TAPE) benchmarks<ref type="bibr" target="#b42">(Rao et al., 2019)</ref>. See Appendix C for dataset sizes by task.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Embedding Parameters</cell><cell></cell><cell>Remote Homology</cell><cell></cell><cell></cell><cell>Secondary Structure</cell><cell></cell><cell>Stability Fluorescence</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Fold Superfamily Family CB513 CASP12 TS115</cell></row><row><cell>Unirep</cell><cell></cell><cell cols="2">182M</cell><cell>0.23</cell><cell>0.38</cell><cell>0.87</cell><cell>0.73</cell><cell>0.72</cell><cell>0.77</cell><cell>0.73</cell><cell>0.67</cell></row><row><cell>BERT</cell><cell></cell><cell>92M</cell><cell></cell><cell>0.21</cell><cell>0.34</cell><cell>0.88</cell><cell>0.73</cell><cell>0.71</cell><cell>0.77</cell><cell>0.73</cell><cell>0.68</cell></row><row><cell>ResNet</cell><cell></cell><cell>48M</cell><cell></cell><cell>0.17</cell><cell>0.31</cell><cell>0.77</cell><cell>0.75</cell><cell>0.72</cell><cell>0.78</cell><cell>0.73</cell><cell>0.21</cell></row><row><cell>LSTM</cell><cell></cell><cell>44M</cell><cell></cell><cell>0.26</cell><cell>0.43</cell><cell>0.92</cell><cell>0.75</cell><cell>0.70</cell><cell>0.78</cell><cell>0.69</cell><cell>0.67</cell></row><row><cell>Bepler et al.</cell><cell></cell><cell>19M</cell><cell></cell><cell>0.17</cell><cell>0.20</cell><cell>0.79</cell><cell>0.73</cell><cell>0.70</cell><cell>0.76</cell><cell>0.64</cell><cell>0.33</cell></row><row><cell>One Hot</cell><cell></cell><cell>0</cell><cell></cell><cell>0.09</cell><cell>0.08</cell><cell>0.39</cell><cell>0.69</cell><cell>0.68</cell><cell>0.72</cell><cell>0.19</cell><cell>0.14</cell></row><row><cell>CPCProt</cell><cell></cell><cell>1.7M</cell><cell></cell><cell>0.12</cell><cell>0.12</cell><cell>0.48</cell><cell>0.69</cell><cell>0.70</cell><cell>0.73</cell><cell>0.65</cell><cell>0.68</cell></row><row><cell cols="2">CPCProt GRU large</cell><cell>8.4M</cell><cell></cell><cell>0.13</cell><cell>0.14</cell><cell>0.52</cell><cell>0.70</cell><cell>0.70</cell><cell>0.73</cell><cell>0.65</cell><cell>0.68</cell></row><row><cell cols="2">CPCProt LSTM</cell><cell>71M</cell><cell></cell><cell>0.11</cell><cell>0.11</cell><cell>0.47</cell><cell>0.68</cell><cell>0.66</cell><cell>0.70</cell><cell>0.68</cell><cell>0.68</cell></row><row><cell></cell><cell></cell><cell cols="4">Remote Homology</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fold</cell><cell cols="3">Superfamily Family</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">LR kNN LR kNN LR kNN</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UniRep</cell><cell cols="5">0.08 0.06 0.18 0.11 0.48 0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT</cell><cell cols="5">0.20 0.11 0.30 0.24 0.76 0.74</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CPCProt</cell><cell cols="5">0.14 0.12 0.13 0.10 0.50 0.51</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">CPCProt GRU large 0.13 0.12 0.14 0.10 0.50 0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">CPCProt LSTM 0.14 0.11 0.15 0.12 0.52 0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Downstream evaluation using logistic regression and kNN k-nearest-neighbours models (Top-1 accuracy).</figDesc><table><row><cell></cell><cell cols="3">Secondary Structure</cell></row><row><cell></cell><cell cols="3">CB513 CASP12 TS115</cell></row><row><cell></cell><cell>LR</cell><cell>LR</cell><cell>LR</cell></row><row><cell>UniRep</cell><cell>0.66</cell><cell>0.80</cell><cell>0.70</cell></row><row><cell>BERT</cell><cell>0.72</cell><cell>0.82</cell><cell>0.77</cell></row><row><cell>CPCProt</cell><cell>0.61</cell><cell>0.80</cell><cell>0.68</cell></row><row><cell>CPCProt GRU large</cell><cell>0.62</cell><cell>0.80</cell><cell>0.69</cell></row><row><cell>CPCProt LSTM</cell><cell>0.62</cell><cell>0.80</cell><cell>0.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Downstream evaluation using logistic regression models.</figDesc><table><row><cell cols="2">Top-3 (Q3) accuracy is reported.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Stability</cell></row><row><cell></cell><cell>LR</cell><cell></cell><cell>kNN</cell></row><row><cell></cell><cell>MSE</cell><cell>ρ</cell><cell>MSE</cell><cell>ρ</cell></row><row><cell>UniRep</cell><cell cols="4">0.21 0.62 0.24 0.57</cell></row><row><cell>BERT</cell><cell cols="4">0.36 0.39 0.23 0.49</cell></row><row><cell>CPCProt</cell><cell cols="4">0.34 0.55 0.18 0.51</cell></row><row><cell cols="5">CPCProt GRU large 0.31 0.62 0.18 0.52</cell></row><row><cell>CPCProt LSTM</cell><cell cols="4">0.22 0.62 0.19 0.54</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Downstream evaluation using linear regression and kNN models for the protein engineering task of stability (MSE and Spearman's ρ).</figDesc><table><row><cell></cell><cell cols="3">Fluorescence</cell></row><row><cell></cell><cell>LR</cell><cell></cell><cell>kNN</cell></row><row><cell></cell><cell>MSE</cell><cell>ρ</cell><cell>MSE</cell><cell>ρ</cell></row><row><cell>UniRep</cell><cell cols="4">1.32 0.55 1.66 0.37</cell></row><row><cell>BERT</cell><cell cols="4">1.15 0.52 1.75 0.46</cell></row><row><cell>CPCProt</cell><cell cols="4">1.13 0.54 1.82 0.49</cell></row><row><cell cols="5">CPCProt GRU large 0.81 0.63 1.84 0.50</cell></row><row><cell>CPCProt LSTM</cell><cell cols="4">0.85 0.67 1.80 0.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Downstream evaluation using linear regression and kNN models for the protein engineering task of fluorescence (MSE and Spearman's ρ).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In this work, we use the more general term "neural network finetuning" as opposed to "MLP finetuning" in the contrastive representation learning literature, as one of the protein-related downstream tasks is a sequence-to-sequence task which uses a CNN head.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements Amy X. Lu was funded by the NSERC Canada Graduate Scholarships-Master's award. Alan M. Moses holds a Tier II Canada Research Chair. Maryzeh Ghassemi is funded in part by Microsoft Research, a CIFAR AI Chair at the Vector Institute, a Canada Research Council Chair, and an NSERC Discovery Grant. Computing resources for this work was provided by the Vector Institute. The authors thank Nick Bhattacharya, Alex X. Lu, Adam Riesselman, Denny Wu, and Kevin Yang for insightful input.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Assessment of hard target modeling in casp12 reveals an emerging role of alignment-based contact prediction methods</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Abriata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Tamò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peraro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proteins: Structure, Function, and Bioinformatics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="97" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Information theory in molecular biology</title>
		<author>
			<persName><forename type="first">C</forename><surname>Adami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics of Life Reviews</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="22" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00410</idno>
		<title level="m">Deep variational information bottleneck</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequence-only deep representation learning</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">589333</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language modelling for biological sequencescurated datasets and baselines</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J A</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15509" to="15519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning protein sequence embeddings using information from structure</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bepler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08661</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Uniprot: a worldwide hub of protein knowledge</title>
		<author>
			<persName><forename type="first">U</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D506" to="D515" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="519" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Asymptotic evaluation of certain markov process expectations for large time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Donsker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Varadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">iv. Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="212" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">S</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L L</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Piovesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C E</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Finn</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gky995</idno>
		<ptr target="https://academic.oup.com/nar/article/47/D1/D427/5144153" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<idno type="ISSN">0305-1048</idno>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D427" to="D432" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end multitask learning, from protein language to protein features without alignments</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">864405</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Prottrans: Towards cracking the language of life&apos;s code through self-supervised deep learning and high performance computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhowmik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06225</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structural classification of proteins-extended, integrating scop and astral data and classification of new structures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Chandonia</surname></persName>
		</author>
		<author>
			<persName><surname>Scope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D304" to="D309" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Information theory and the living system</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Gatlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Columbia University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structure-based function prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gligorijevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Renfrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kosciolek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vatanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Berenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Fisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Xavier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">786236</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010. 2019</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Momentum contrast for unsupervised visual representation learning</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling the language of life-deep learning protein sequences</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">614313</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepsf: deep convolutional neural network for mapping protein sequences to folds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1295" to="1303" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen-bonded and geometrical features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolymers: Original Research on Biomolecules</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Netsurfp-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jespersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Soenderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O A</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D M</forename><surname>D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08350</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Covariation of mutations in the v3 loop of human immunodeficiency virus type 1 envelope protein: an information theoretic analysis</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Korber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Farber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Lapedes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="7176" to="7180" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-organization in a perceptual network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Linsker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="105" to="117" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Putting an end to end-to-end: Gradient-isolated learning of representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Löwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Veeling</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3033" to="3045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Progen: Language modeling for protein generation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Eguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03497</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Critical assessment of methods of protein structure prediction (CASP)-Round XII</title>
		<author>
			<persName><forename type="first">J</forename><surname>Moult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schwede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tramontano</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25415</idno>
		<ptr target="http://doi.wiley.com/10.1002/prot.25415" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="7" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep representation learning of proteins based on siamese networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Nourani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Mchardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mofrad</surname></persName>
		</author>
		<author>
			<persName><surname>Tripletprot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06922</idno>
		<title level="m">On variational bounds of mutual information</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Entropy and information within intrinsically disordered protein regions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pritišanac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Vernon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Moses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Forman</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">662</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The information content of dna and evolution</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of theoretical biology</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="803" to="807" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Evaluating protein transfer learning with tape</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9686" to="9698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet?</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accelerating protein design using autoregressive generative models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Riesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-E</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Kollasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Manglik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">757252</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">622803</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised pretraining transfers well across languages</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rivière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Mazaré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02848</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houliston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lemak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mulligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="issue">6347</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Application of information theory to dna sequence analysis: a review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Roman-Roldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bernaola-Galvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1187" to="1194" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local fitness landscape of the green fluorescent protein</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sarkisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bolotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Usmanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Mishin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Sharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Ivankov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">G</forename><surname>Bozhanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Soylemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="issue">7603</biblScope>
			<biblScope unit="page">397</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequence logos: a new way to display consensus sequences</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="6097" to="6100" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dna binding sites: representation and discovery</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Stormo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="23" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Information theory applications for biological sequence analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="389" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learned protein embeddings for machine learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Bedbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="2642" to="2648" />
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sixty-five years of the long march in protein secondary structure prediction: the final stretch?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="482" to="494" />
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
