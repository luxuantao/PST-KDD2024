<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multilingual Unsupervised Sentence Simplification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-05-01">1 May 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
							<email>louismartin@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
							<email>angelafan@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">LORIA</orgName>
								<address>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Éric</forename><surname>De La Clergerie</surname></persName>
							<email>eric.delaclergerie@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
							<email>abordes@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
							<email>benoit.sagot@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multilingual Unsupervised Sentence Simplification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-05-01">1 May 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2005.00352v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress in Sentence Simplification has been hindered by the lack of supervised data, particularly in languages other than English. Previous work has aligned sentences from original and simplified corpora such as English Wikipedia and Simple English Wikipedia, but this limits corpus size, domain, and language. In this work, we propose using unsupervised mining techniques to automatically create training corpora for simplification in multiple languages from raw Common Crawl web data. When coupled with a controllable generation mechanism that can flexibly adjust attributes such as length and lexical complexity, these mined paraphrase corpora can be used to train simplification systems in any language. We further incorporate multilingual unsupervised pretraining methods to create even stronger models and show that by training on mined data rather than supervised corpora, we outperform the previous best results. We evaluate our approach on English, French, and Spanish simplification benchmarks and reach stateof-the-art performance with a totally unsupervised approach. We will release our models and code to mine the data in any language included in Common Crawl.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text Simplification is the task of reducing the complexity of the vocabulary and sentence structure of text while retaining its original meaning, with the goal of improving readability and understanding. Simplification has a variety of important societal applications, for example increasing accessibility for those with cognitive disabilities such as aphasia <ref type="bibr" target="#b12">(Carroll et al., 1998)</ref>, dyslexia <ref type="bibr" target="#b42">(Rello et al., 2013)</ref>, and autism <ref type="bibr" target="#b16">(Evans et al., 2014)</ref>, or for nonnative speakers <ref type="bibr" target="#b40">(Paetzold and Specia, 2016)</ref> and children with reading difficulties <ref type="bibr" target="#b20">(Gala et al., 2020)</ref>. Research has mostly focused on English simplification, where corpora with source texts and their Figure <ref type="figure">1</ref>: Sentence Simplification Models for Any Language using Unsupervised Data. Sentences from the web are used to create a large scale index that allows mining millions of paraphrases. Subsequently, we finetune pretrained models augmented with controllable mechanisms on the paraphrase corpora to achieve sentence simplification models in any language. associated simplified texts exist and can be automatically aligned, such as English Wikipedia and Simple English Wikipedia or the Newsela corpus <ref type="bibr" target="#b57">(Xu et al., 2015)</ref>. However, such data is difficult to find in languages other than English, and the need for manually simplified corpora limits the incorporation of large quantities of raw text data.</p><p>In this work, we focus on leveraging unsupervised data to train sentence simplification systems in multiple languages. As sentence simplification is a special case of paraphrasing, we present a general technique to create training corpora in any language by mining pairs of paraphrased sequences from the web. Using controllable text generation mechanisms proposed in <ref type="bibr" target="#b37">Martin et al. (2020)</ref> we then train simplification models by controlling attributes such as length, lexical complexity, and syntactic complexity. Mining paraphrases as a superset of sentence simplifications proves better than mining for simplifications directly. Datasets mined for simplifications would necessitate several heuristics on the type of simplifications that should be mined; we show that a model able to adjust during training on a dataset mined with fewer assumptions can reach better experimental results.</p><p>These automatically created corpora benefit from large quantities of data in various languages available online. We apply this technique to mine English, French, and Spanish paraphrase training data. On English, we show that the quality of the models trained with mined data is similar to that of models trained with automatically aligned English Wikipedia and Simple English Wikipedia.</p><p>Subsequently, we use multilingual pretraining with BART and mBART <ref type="bibr" target="#b33">(Lewis et al., 2019;</ref><ref type="bibr" target="#b35">Liu et al., 2020)</ref> to further incorporate unsupervised data in our sentence simplification models. This unsupervised pretraining leverages large quantities of data freely available on the web to create high quality generative models. By using pretrained models and finetuning on our mined paraphrase datasets, we are able to achieve state of the art results with no supervised data in multiple languages. In summary, we make the following contributions:</p><p>• We propose a mining procedure to create large paraphrase corpora in multiple languages, to train sentence simplification systems.</p><p>• We incorporate recent advances in generative pretraining and controllable generation to achieve or match the state of the art in English with 42.65 SARI on ASSET and 40.85 SARI on TURKCORPUS datasets with a completely unsupervised approach. We show that we can further improve by more than 1 SARI point by incorporating supervised data.</p><p>• We reach state of the art in multiple languages using only mined data, alleviating the need for language-specific supervised corpora.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Sentence Simplification in multiple languages Data-driven methods have been predominant approach in English Sentence Simplification in the recent years, requiring large supervised training corpora of complex-simple aligned sentences <ref type="bibr" target="#b56">(Wubben et al., 2012;</ref><ref type="bibr" target="#b58">Xu et al., 2016;</ref><ref type="bibr" target="#b59">Zhang and Lapata, 2017;</ref><ref type="bibr" target="#b60">Zhao et al., 2018;</ref><ref type="bibr" target="#b37">Martin et al., 2020)</ref>. Methods have relied on using the Wikipedia edit history <ref type="bibr" target="#b9">(Botha et al., 2018)</ref>, or more notably on English Wikipedia and Simple English Wikipedia with automatic alignment of sentences from similar articles for the creation of such corpora <ref type="bibr" target="#b62">(Zhu et al., 2010;</ref><ref type="bibr" target="#b13">Coster and Kauchak, 2011;</ref><ref type="bibr" target="#b55">Woodsend and Lapata, 2011;</ref><ref type="bibr" target="#b28">Kauchak, 2013;</ref><ref type="bibr" target="#b59">Zhang and Lapata, 2017)</ref>. Using Wikipedia and automatic alignments has been shown to have various flaws compared to professional simplifications from news articles such as in the Newsela dataset <ref type="bibr" target="#b57">(Xu et al., 2015)</ref>.</p><p>There are fewer professional simplifications, and these often come with restrictive licenses that hinder widespread usage and reproducibility. Multiple efforts have explored simplification in other languages such as Brazilian Portuguese <ref type="bibr" target="#b0">(Aluísio et al., 2008)</ref>, Spanish <ref type="bibr" target="#b43">(Saggion et al., 2015;</ref><ref type="bibr" target="#b47">Štajner et al., 2015)</ref>, Italian <ref type="bibr" target="#b10">(Brunato et al., 2015;</ref><ref type="bibr" target="#b51">Tonelli et al., 2016)</ref>, Japanese <ref type="bibr" target="#b21">(Goto et al., 2015;</ref><ref type="bibr" target="#b26">Kajiwara and Komachi, 2018;</ref><ref type="bibr" target="#b27">Katsuta and Yamamoto, 2019)</ref>, and French <ref type="bibr" target="#b20">(Gala et al., 2020)</ref>, but these lack of a large supervised training corpus such as what is available in English. <ref type="foot" target="#foot_0">1</ref> In this work, we show that a method trained on large corpora automatically mined from raw text in any language can reach state of the art results in all languages.</p><p>Unsupervised Simplification When parallel data is not available or not used, sentence simplification systems rely on unsupervised simplification techniques, often based on techniques from machine translation. The prevailing approach is to split a raw monolingual corpora into two disjoint sets of complex and simple sentences with readability metrics to train unsupervised models. <ref type="bibr" target="#b26">Kajiwara and Komachi (2018)</ref> train statistical machine translation models on unsupervised alignment, based on word embeddings, of English Wikipedia. Other methods remove the alignment process and train with the two disjoint sets with auto-encoders <ref type="bibr" target="#b49">(Surya et al., 2019;</ref><ref type="bibr" target="#b61">Zhao et al., 2020)</ref>, unsupervised statistical machine translation for Japanese <ref type="bibr" target="#b27">(Katsuta and Yamamoto, 2019)</ref>, and back-translation in Spanish and Italian <ref type="bibr" target="#b4">(Aprosio et al., 2019)</ref>. The performance of such unsupervised methods are however often below their supervised counterparts. In our work we remove the need for separating the raw monolingual corpora in two disjoint sets and instead mine paraphrases directly, and train models with stateof-the-art performance.</p><p>Mining Previous work on the unsupervised creation of corpora from noisy text has focused on mining parallel data for machine translation systems. Works have leveraged document retrieval <ref type="bibr" target="#b38">(Munteanu and Marcu, 2005)</ref>, language models <ref type="bibr" target="#b11">(Buck and Koehn, 2016;</ref><ref type="bibr" target="#b31">Koehn et al., 2018</ref><ref type="bibr" target="#b30">Koehn et al., , 2019))</ref>, and embedding space alignment <ref type="bibr" target="#b6">(Artetxe and Schwenk, 2019)</ref> to create large corpora <ref type="bibr" target="#b50">(Tiedemann, 2012;</ref><ref type="bibr">Schwenk et al., 2019a,b)</ref>. We focus on paraphrasing for sentence simplifications, which presents new challenges for unsupervised mining. Unlike machine translation, where the same sentence should be identified in two languages, we develop a method to identify varied simplifications of sentences. Mining in translation has leveraged heuristics such as similar length, but paraphrases, and most specifically simplifications, have a wide array of surface forms, including multiple sentences, different vocabulary usage, and removal of content from more complex sentences.</p><p>Previous work in unsupervised paraphrasing has learned to align sentences from various corpora <ref type="bibr" target="#b7">(Barzilay and Lee, 2003)</ref> with a variety of different objective functions <ref type="bibr" target="#b34">(Liu et al., 2019)</ref>. Techniques for large-scale mining have been used to create large paraphrase corpora <ref type="bibr" target="#b54">(Wieting and Gimpel, 2018)</ref>, but this has not been applied to multiple languages or the task of sentence simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We describe how to create sentence simplification models in multiple languages without supervised data. We mine a large quantity of paraphrases from the Common Crawl using libraries such as LASER <ref type="bibr" target="#b5">(Artetxe et al., 2018)</ref> and faiss <ref type="bibr" target="#b23">(Johnson et al., 2019)</ref>. Then, we show how we leverage controllable generation mechanisms and unsupervised pretraining in addition to unsupervised data mining to train high quality simplification models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mining Paraphrases in Multiple Languages</head><p>Challenges in Mining Simplifications Progress in creating simplification models in multiple languages has been hindered by lack of supervised training data, namely pairs of complex sentences matched with their simplified forms. Existing models often use automatically aligned sentences from Wikipedia and Simple English Wikipedia, but this approach is not scalable and does not exist for other languages. Further, simplifications exist in many possible forms that depends on the target audience it is aimed for -a simple sequence is not always only shorter, but could be split into multiple sentences, use less complex vocabulary, include less detail, and so on. These simplification guidelines are not uniquely defined, and even if they could be used to heuristically mine corpora, rules for different languages might wildly differ and prevent large-scale mining in multiple languages. We confirm experimentally in Section 5.3 that mining simplifications does not achieve better results than the more general mining of paraphrases.</p><p>Mining Paraphrases as Generalized Simplifications Thus, in this work, we focus on creating strong simplification models from paraphrase data.</p><p>Paraphrases can be seen as strict generalization of simplifications. To create models that can simplify after being trained on paraphrases, we leverage advancements in controllable text generation. Control models have been used in a variety of settings to adjust length <ref type="bibr" target="#b18">(Fan et al., 2018)</ref>, bias <ref type="bibr" target="#b15">(Dinan et al., 2019)</ref>, and style <ref type="bibr" target="#b46">(See et al., 2019)</ref>. We use the controllable sentence simplification model ACCESS <ref type="bibr" target="#b37">(Martin et al., 2020)</ref>, which learns to control length, amount of paraphrasing, lexical complexity and syntactic complexity to train on mined paraphrase corpora and dynamically create simplifications at inference time. We then scale our unsupervised paraphrase mining approach to create automatically aligned paraphrase corpora in different languages.</p><p>Sequences Extraction Sentence Simplification consists of multiple rewriting operations, some of which spanning over multiple sentences (e.g. sentence splitting or sentence fusion). To allow for these types of operations to be represented in our data, we extract sequences of multiple sentences from raw documents. As we detail in Section 4.1 these sequences are further filtered to remove noisy text with too much punctuation, or low language model probability for example. In the following these series of multiple consecutive sentences are termed sequences.</p><p>We extract these sequences from CCNET <ref type="bibr" target="#b53">(Wenzek et al., 2019)</ref>. CCNET is an extraction of Common Crawl,<ref type="foot" target="#foot_1">2</ref> an open source snapshot of the web, that has been split into different languages using fasttext language identification <ref type="bibr" target="#b25">(Joulin et al., 2017)</ref> and various language modeling filtering techniques to identify high quality, clean sentences. For English and French, we extract 1 billion sequences from CCNET. For Spanish we extract 650 millions sequences, the maximum for this language in CCNET after filtering out noisy text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Creating a Sequence Index Using Embeddings</head><p>To automatically mine our paraphrase corpora, we first compute n-dimensional sentence embeddings Query For insulation, it uses foam-injected polyurethane which helps ensure the quality of the ice produced by the machine. It comes with an easy to clean air filter. Mined It has polyurethane for insulation which is foam-injected. This helps to maintain the quality of the ice it produces. The unit has an easy to clean air filter.</p><p>Query Here are some useful tips and tricks to identify and manage your stress. Mined Here are some tips and remedies you can follow to manage and control your anxiety.</p><p>Query As cancer cells break apart, their contents are released into the blood. Mined When brain cells die, their contents are partially spilled back into the blood in the form of debris.</p><p>Query The trail is ideal for taking a short hike with small children or a longer, more rugged overnight trip. Mined It is the ideal location for a short stroll, a nature walk or a longer walk.</p><p>Query Thank you for joining us, and please check out the site. Mined Thank you for calling us. Please check the website. for each of our sequences using the LASER toolkit <ref type="bibr" target="#b5">(Artetxe et al., 2018)</ref>. LASER is a multilingual sentence embedding model that is trained to map sentences of similar meaning to the same location in the embedding space. We use the faiss librairy <ref type="bibr" target="#b23">(Johnson et al., 2019)</ref> to create an index with all these sentence embeddings. faiss indexes are a type of data structure that can store a large amount of vectors and provide a fast and efficient interface for searching nearest neighbors within the index.</p><p>Mining Paraphrases For each language, after the billion-scale index is created, we use the same 1 billion sequences as queries to identify potential paraphrases in the index. Each sequence is queried against the index and returns a set of top-k nearest neighbor sequences according to the semantic LASER embedding space using L2 distance<ref type="foot" target="#foot_2">3</ref> . These nearest neighbors are candidate paraphrases of the query sentence. We apply additional filters to remove poor quality alignments where the sequences are not paraphrases, for example when they are almost identical, when they are contained in one another, or when they were extracted from two consecutive and overlapping sliding windows of the same original document. Table <ref type="table" target="#tab_0">1</ref> displays some examples of the resulting mined paraphrases in English while Table <ref type="table" target="#tab_1">2</ref> reports statistics of the mined corpora in English, French and Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Simplifying with ACCESS</head><p>We train our models on paraphrase data to produce simplifications by leveraging advancements in controllable text generation, specifically applying the ACCESS control mechanism <ref type="bibr" target="#b37">(Martin et al., 2020)</ref>.</p><p>Training with Control Tokens At training time, the model is provided with control tokens that give oracle information on the target sequence, such as the amount of compression of the target sequence relative to the source sequence. For example, when the target sequence is 80% of the size of the original sequence, we provide the &lt;NumChars 80%&gt; control token. This encourages the model to rely on the oracle control tokens. At inference time it can then control the generation by selecting a given target control token value.</p><p>Choosing Control Values at Inference In our experiments, We want the generations to best reflect the simplification typologies of each dataset.</p><p>To this end, we select the control values that achieve the best SARI on the validation set using hyperparameter search, and keep those values fixed for all sentences in the test set. We repeat this process for each evaluation dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Leveraging Unsupervised Pretraining</head><p>Unsupervised pretraining has recently demonstrated large improvements on generative tasks, by using sequence-to-sequence models as denoising auto-encoders on large quantities of raw data  <ref type="bibr" target="#b37">(Martin et al., 2020)</ref>. Replace-only Levenshtein similarity only considers replace operations in the traditional Levenshtein similarity and assigns 0 weights to insertions and deletions. <ref type="bibr" target="#b33">(Lewis et al., 2019;</ref><ref type="bibr" target="#b35">Liu et al., 2020)</ref> by training with noising functions such as span-based masking <ref type="bibr" target="#b17">(Fan et al., 2019a;</ref><ref type="bibr" target="#b24">Joshi et al., 2020)</ref> or shuffling sentence order. We leverage these pretrained models to further extend our unsupervised approach to text simplification. For English, we finetune pretrained generative model BART <ref type="bibr" target="#b33">(Lewis et al., 2019)</ref> on our newly created mined training corpora. BART is a pretrained sequence-to-sequence model that can be seen as a generalization of other recent pretrained models such as BERT <ref type="bibr" target="#b14">(Devlin et al., 2018)</ref> and GPT2 <ref type="bibr">(Radford et al.)</ref>.</p><p>For non-English languages, we use its multilingual generalization MBART <ref type="bibr" target="#b35">(Liu et al., 2020)</ref>. MBART was pretrained using the BART objective on 25 languages and showed large performance gains for low-resource machine translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head><p>We assess the performance of our approach on three languages: English, French, and Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Mining Details</head><p>Sequence Extraction We only consider documents from the HEAD split in CCNET-this represents the third of the data with the best perplexity using a language model. We extract sequences from raw documents using the NLTK sentence tokenizer <ref type="bibr" target="#b8">(Bird and Loper, 2004</ref>) and generate all possible sequences of adjacent sentences with lengths between 10 and 300 characters. We further filter these sequences to remove those with low probability according to a 3-gram Kneser-Ney language model trained with kenlm <ref type="bibr" target="#b22">(Heafield, 2011)</ref>. We also filter sequences with too much punctuation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase Mining</head><p>We compute LASER embeddings of dimension 1024 and reduce dimensionality with a 512 PCA followed by random rotation. We further compress them using 8 bit scalar quantization. The compressed embeddings are then stored in a faiss inverted file index with 32,768 cells (nprobe=16). These embeddings are used to mine pairs of paraphrases. We return the top-8 nearest neighbors, and keep those with L2 distance lower than 0.05 and relative distance compared to other top-8 nearest neighbors lower than 0.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrases Filtering</head><p>The resulting paraphrases are filtered to remove almost identical paraphrases by enforcing a case-insensitive character-level Levenshtein distance <ref type="bibr" target="#b32">(Levenshtein, 1966)</ref> greater or equal to 20%. We remove paraphrases that come from the same document to avoid aligning sequences that overlapped each other in the original text. We also remove paraphrases where one of the sequence is contained in the other. We further filter out any sequence that is present in our evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training details</head><p>We implement our models with fairseq <ref type="bibr" target="#b39">(Ott et al., 2019)</ref>. All our models are Transformers <ref type="bibr" target="#b52">(Vaswani et al., 2017)</ref> based on BART Large , keeping the optimization procedure and hyperparameters fixed <ref type="bibr" target="#b33">(Lewis et al., 2019)</ref>. We either randomly initialize weights for the standard sequence-tosequence experiments or initialize with pretrained BART for the BART experiments. For controllable generation, we use the open-source ACCESS implementation <ref type="bibr" target="#b36">(Martin et al., 2018)</ref>. We use the same control parameters as the original paper, namely length, Levenshtein similarity, lexical complexity, and syntactic complexity. <ref type="foot" target="#foot_3">4</ref>In all our experiments, we report scores averaged over 5 random seeds evaluated on the test set with 95% confidence intervals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>In addition to that of previous work, we report results for several basic baselines, which are useful for languages where previous work does not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity baseline</head><p>The original sequence is the simplification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Truncation baseline</head><p>The original sequence is truncated by keeping the first 80% of words. It proves to be a strong baseline in practice, as measured by standard text simplification metrics.</p><p>Pivot baseline We use machine translation to provide a baseline for languages for which no simplification corpora are available. The complex non-English sentence is translated to English, simplified with our best English simplification system, and then translated back into the source language. For French and Spanish translation, we use CCMATRIX <ref type="bibr" target="#b45">(Schwenk et al., 2019b)</ref> to train Transformer models with LayerDrop <ref type="bibr" target="#b19">(Fan et al., 2019b)</ref>. We use the BART+ACCESS model trained on MINED+ WIKILARGE as the English simplification model. While pivoting creates potential errors from inaccurate or unnatural translations, recent improvements of neural translation systems on high resource languages nevertheless makes this a strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reference</head><p>We report reference scores for English because multiple references are available for each original sentence. We compute these scores in a leave-one-out scenario where each reference is evaluated against all the others and then scores are averaged over all references.<ref type="foot" target="#foot_4">5</ref> 4.4 Evaluation Metrics SARI Sentence simplification is commonly evaluated with SARI <ref type="bibr" target="#b58">(Xu et al., 2016)</ref>, which compares model generated simplifications with the source sequence and gold references. It averages F1 scores for addition, keep, and deletion operations. We compute SARI with the EASSE<ref type="foot" target="#foot_5">6</ref> simplification evaluation suite (Alva-Manchego et al., 2019)<ref type="foot" target="#foot_6">7</ref> .</p><p>BLEU We report BLEU scores for completeness, but these should be carefully interpreted. They have been found to correlate poorly with human judgments of simplicity <ref type="bibr" target="#b48">(Sulem et al., 2018)</ref>. Furthermore, the identity baseline achieves very high BLEU scores on some datasets (e.g. 92.81 on AS-SET or 99.36 on TURKCORPUS), which underlines the weaknesses of this metric.</p><p>FKGL Finally, we report readability scores using the Flesch-Kincaid Grade Level (FKGL) <ref type="bibr" target="#b29">(Kincaid et al., 1975)</ref>, a linear combination of sentence lengths and word lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation Data</head><p>English To evaluate our models in English, we use ASSET (Alva-Manchego et al., 2020a) and TURKCORPUS <ref type="bibr" target="#b58">(Xu et al., 2016)</ref>. TURKCORPUS (sometimes known as WIKILARGE evaluation set) and ASSET were created using the same 2000 valid and 359 test source sentences. TURKCOR-PUS contains 8 reference simplifications per source sentence and ASSET contains 10 references per source. TURKCORPUS's simplifications mostly consist in small lexical paraphrases. ASSET is a generalization of TURKCORPUS with a more varied set of rewriting operations. ASSET simplifications are also considered simpler than simplifications in TURKCORPUS by human judges.</p><p>French For French, we use the ALECTOR dataset <ref type="bibr" target="#b20">(Gala et al., 2020)</ref> for evaluation. ALEC-TOR is a collection of literary (tales, stories) and scientific (documentary) texts along with their manual document-level simplified versions. These documents were extracted from material available to French primary school pupils.</p><p>Most of these documents were simplified line by line, each line consisting of a few sentences. For each original document, we align each line that contains less than 6 sentences with the closest line in its simple counterpart, using the LASER embedding space. The resulting alignments are split into validation and test by randomly sampling the documents for the validation (450 samples) and rest for test (416 samples).</p><p>Spanish For Spanish we use the SIMPLEXT Corpus from <ref type="bibr" target="#b43">(Saggion et al., 2015)</ref>. The SIM-PLEXT Corpus is a set of 200 news articles that scores from previous systems that we compare to. We do so by using the system predictions provided by the respective authors, and available in EASSE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>History Landsberg prison, which is in the town's western outskirts, was completed in 1910. Simplified The Landsberg prison, which is near the town, was built in 1910.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>In 2004, Roy was selected as the greatest goaltender in NHL history by a panel of 41 writers, coupled with a simultaneous fan poll. Simplified In 2004, Roy was chosen as the greatest goaltender in NHL history by a group of 41 writers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>The name "hornet" is used for this and related species primarily because of their habit of making aerial nests (similar to the true hornets) rather than subterranean nests. Simplified The name "hornet" is used for this and related species because they make nests in the air (like the true hornets) rather than in the ground.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Nocturnes is an orchestral composition in three movements by the French composer Claude Debussy. Simplified Nocturnes is a piece of music for orchestra by the French composer Claude Debussy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>This book by itself is out of print having been published along with nine short stories in the collection The Worthing <ref type="bibr">Saga (1990)</ref>. Simplified This book by itself is out of print. It was published along with nine short stories in 1990.</p><p>Table <ref type="table">4</ref>: Examples of Generated Simplifications. We show simplifications generated by our best unsupervised model: BART+ACCESS trained on mined data only. Bold highlights differences between original and simplified.</p><p>were manually simplified by trained experts for people with learning disabilities. We split the dataset by documents into a valid (460 samples) and test set (449 samples). SIMPLEXT Corpus contains simplifications with a high editing ratio, where the original sentence is strongly rephrased and compressed compared to other evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>We now assess the quality of our mined data and the improvements brought up by unsupervised pretraining for simplifying English, French and Spanish.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">English Simplification</head><p>We compare models trained on our mined corpus of 1.2 million English paraphrase pairs (see Table <ref type="table" target="#tab_1">2</ref>) with models trained with the standard simplification dataset WIKILARGE. We also compare to other state-of-the-art supervised models <ref type="bibr" target="#b56">(Wubben et al., 2012;</ref><ref type="bibr" target="#b59">Zhang and Lapata, 2017;</ref><ref type="bibr" target="#b60">Zhao et al., 2018;</ref><ref type="bibr" target="#b37">Martin et al., 2020)</ref> and the only previously published unsupervised model <ref type="bibr" target="#b49">(Surya et al., 2019)</ref>.</p><p>Using Seq2Seq Models on Mined Data When training a Transformer sequence-to-sequence model (Seq2Seq) on WIKILARGE compared to the mined corpus, we find that our models trained on the mined data perform better by 5.32 SARI on ASSET and 2.25 SARI on TURKCORPUS (see Table <ref type="table" target="#tab_2">3</ref>). It is surprising that a model trained solely on a paraphrase corpus might achieve such good results on simplification benchmarks. Multiple works have shown that simplification models suffered from not making enough modifications to the source sentence and showed that forcing models to rewrite the input was beneficial <ref type="bibr" target="#b56">(Wubben et al., 2012;</ref><ref type="bibr" target="#b37">Martin et al., 2020)</ref>. We speculate that this might explain why our sequence-to-sequence Using only unsupervised data, we are able to achieve a +2.52 SARI improvement over the previous state of the art on ASSET<ref type="foot" target="#foot_7">8</ref> . We achieve similar results on TURKCORPUS with 40.85 SARI with no supervision (average over 5 random seeds) versus the previous state of the art of 41.38 SARI (best seed selected on the validation set by the authors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples of Simplifications</head><p>Various examples from our totally unsupervised simplification system are shown in Table <ref type="table">4</ref>. Examining the simplifications, we see reduced sentence length, sentence splitting of a complex sentence into multiple shorter sentences, and the use of simpler vocabulary. For example, the word cultivation is changed into growing and aerial nets is simplified into nests in the air. Additional simplifications include the removal of less important content and removal of content within parentheses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">French and Spanish Simplification</head><p>Our unsupervised approach to text simplification can be applied to any language provided enough data can be mined. As for English, we first create a corpus of paraphrases composed of 1.4 million sentences in French and 1.0 million sentences in Spanish (see Table <ref type="table" target="#tab_1">2</ref>). We evaluate the quality of our mined corpus in Table <ref type="table" target="#tab_3">5</ref>. Unlike for English, where supervised training data has been created using Simple English Wikipedia, no such datasets exist for French or Spanish simplification. We compare to several baselines, namely the identity, truncation and pivot baselines.</p><p>Using Seq2Seq Models on Mined data Compared to our baselines, training a Transformer sequence-to-sequence model on our mined data achieves stronger results in French and stronger results in Spanish except for the pivot baseline.</p><p>Adding MBART and ACCESS To incorporate multilingual pretraining, we use MBART. MBART was trained on 25 languages rather than the English BART. Similar to what we observed in English, we achieve the best results by combining MBART, ACCESS, and training on mined data. It outperforms our strongest baseline by +8.25 SARI in French but seems to lag behind in Spanish.</p><p>As shown in the English results in Table <ref type="table" target="#tab_2">3</ref>, MBART also suffers a small loss in performance of 1.54 SARI compared to its monolingual English counterpart BART, probably due to the fact that it handles 25 languages instead of one. Using a monolingual version of BART trained for French or Spanish would perform even better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics Weaknesses in Spanish</head><p>For Spanish, the pivot baseline achieves the highest SARI, but a very low BLEU of 0.94. Qualitative examination of the pivot simplifications showed that the generated Spanish simplifications had very few words in common with the gold standard references -hence the low BLEU-although these predictions were correct simplifications of the source.</p><p>The SIMPLEXT Corpus contains highly rewritten and compressed simplifications (average compression ratio of 56% compared to &gt; 85% for eval- uation datasets in other languages). And the SARI metric reserves 33% of the score to rewarding "correct" word deletions, which might affect most of the source words in this dataset. As a confirmation, a dummy system that deletes all words from the source (i.e. returns an empty simplification), achieves a SARI score of 33.33 and BLEU of 0. This finding questions the use of SARI in settings with high rates compression and rewriting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablations</head><p>Mining Simplifications vs. Paraphrases In this work, we mined paraphrases to train simplification models. We also considered directly mining simplifications using simplification heuristics.</p><p>In order to mine a simplification dataset for comparison, we followed the same procedure of querying 1 billion sequences on an index of 1 billion sequences. We then kept only pairs that either contained sentence splits, were compressed by reducing sequence length, or that had simpler vocabulary (based on word frequencies). We also removed the paraphrase constraint that enforced sentences to be different enough (Levenshtein distance greater than 20%). We tuned these heuristics to optimize SARI scores on the validation set. The resulting dataset is composed of 2.7 million simplification pairs.</p><p>In Table <ref type="table" target="#tab_4">6</ref>, we show that sequence-to-sequence models trained on paraphrases achieve better performance. A similar trend exists with BART and ACCESS, justifying the simpler approach of mining paraphrases instead of simplifications.</p><p>How Much Mined Data Do You Need? Our proposed method leverages the large quantity of sentences in many languages available on the web to mine millions of paraphrases. We investigate the importance of a scalable mining approach that can create million-sized training corpora for sentence simplification. In number of mined pairs, SARI drastically improves, indicating that efficient mining at scale is critical to achieve state of the art performance. Unlike human-created training sets, unsupervised mining with LASER and faiss allows for even larger datasets, and enables this for multiple languages.</p><p>Improvements from Pretraining and Controllable Simplification Unlike previous approaches to text simplification, we use pretraining to train our simplification systems. In a qualitative examination, we found the main improvement from pretraining is increased fluency and meaning preservation in the output generations. For example, in Table <ref type="table">9</ref>, the model trained with ACCESS only substituted culturally akin with culturally much like whereas when using BART+ACCESS it was simplified into the more fluent closely related.</p><p>While our simplification models trained on mined data see several million sentences, pretraining methods are typically trained on billions of sentences. We combine pretrained models with controllable simplification, which enhances the performance of the simplification system in addition to allowing them to adapt to different type of simplified text given the needs of the end audience.</p><p>Table <ref type="table" target="#tab_7">8</ref> highlights that while both the BART pretrained model and the ACCESS controllable simplification mechanism bring some improvement over standard sequence-to-sequence in terms of SARI, they work best in combination and boost the performance by +4.62 SARI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>They are culturally akin to the coastal peoples of Papua New Guinea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCESS</head><p>They're culturally much like the Papua New Guinea coastal peoples. BART+ACCESS They are closely related to coastal people of Papua New Guinea</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>Orton and his wife welcomed Alanna Marie Orton on July 12, 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCESS</head><p>Orton and his wife had been called Alanna Marie Orton on July 12. BART+ACCESS Orton and his wife gave birth to Alanna Marie Orton on July 12, 2008.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original</head><p>He settled in London, devoting himself chiefly to practical teaching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACCESS</head><p>He set up in London and made himself mainly for teaching. BART+ACCESS He settled in London and devoted himself to teaching.</p><p>Table <ref type="table">9</ref>: Influence of BART on Simplifications. We display some examples of generations that illustrate how BART improves the fluency and meaning preservation of generated simplifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an unsupervised approach to text simplification using controllable generation mechanisms and pretraining in combination with large scale mining of paraphrases from the web. This approach is language agnostic and achieves stateof-the-art results, even above previously published results of supervised systems, on three languages: English, French, and Spanish. In future work, we plan to investigate how to scale this approach to more languages and types of simplifications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Density of several text features in WIKILARGE and our mined data. The WordRank ratio is a measure of lexical complexity reduction<ref type="bibr" target="#b37">(Martin et al., 2020)</ref>. Replace-only Levenshtein similarity only considers replace operations in the traditional Levenshtein similarity and assigns 0 weights to insertions and deletions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of Mined Paraphrases. Paraphrases, although sometimes not preserving the entire meaning, display various rewriting operations, such as lexical substitution, compression or sentence splitting.</figDesc><table><row><cell></cell><cell>Type</cell><cell cols="2"># Sequence # Avg. Tokens</cell></row><row><cell></cell><cell></cell><cell>Pairs</cell><cell>per Sequence</cell></row><row><cell cols="2">WIKILARGE Supervised</cell><cell>296,402</cell><cell>21.7 (orig.)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>16.0 (simp.)</cell></row><row><cell>English</cell><cell>Mined</cell><cell>1,194,945</cell><cell>22.3</cell></row><row><cell>French</cell><cell>Mined</cell><cell>1,360,422</cell><cell>18.7</cell></row><row><cell>Spanish</cell><cell>Mined</cell><cell>996,609</cell><cell>22.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Statistics of mined paraphrase training corpora compared to standard supervised WIKILARGE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Unsupervised Sentence Simplification for English. We display SARI, BLEU and FKGL on TURK-CORPUS and ASSET English evaluation datasets. It is particularly famous for the cultivation of kiwifruit. Simplified It is famous for growing the kiwifruit.</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell>ASSET</cell><cell></cell><cell></cell><cell>TURKCORPUS</cell><cell></cell></row><row><cell></cell><cell></cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell></row><row><cell>PBMT-R (Wubben et al., 2012)</cell><cell>PWKP (Wikipedia)</cell><cell>34.63</cell><cell>79.39</cell><cell>8.85</cell><cell>38.04</cell><cell>82.49</cell><cell>8.85</cell></row><row><cell>UNTS (Surya et al., 2019)</cell><cell>UNSUP. DATA</cell><cell>35.19</cell><cell>76.14</cell><cell>7.60</cell><cell>36.29</cell><cell>76.44</cell><cell>7.60</cell></row><row><cell cols="2">Dress-LS (Zhang and Lapata, 2017) WIKILARGE</cell><cell>36.59</cell><cell>86.39</cell><cell>7.66</cell><cell>36.97</cell><cell>81.08</cell><cell>7.66</cell></row><row><cell>DMASS-DCSS (Zhao et al., 2018)</cell><cell>WIKILARGE</cell><cell>38.67</cell><cell>71.44</cell><cell>7.73</cell><cell>39.92</cell><cell>73.29</cell><cell>7.73</cell></row><row><cell>ACCESS (Martin et al., 2020)</cell><cell>WIKILARGE</cell><cell>40.13</cell><cell>75.99</cell><cell>7.29</cell><cell>41.38</cell><cell>76.36</cell><cell>7.29</cell></row><row><cell>Identity Baseline</cell><cell>-</cell><cell>20.73</cell><cell>92.81</cell><cell>10.02</cell><cell>26.29</cell><cell>99.36</cell><cell>10.02</cell></row><row><cell>Truncate Baseline</cell><cell>-</cell><cell>29.85</cell><cell>84.94</cell><cell>7.91</cell><cell>33.1</cell><cell>88.82</cell><cell>7.91</cell></row><row><cell>Reference</cell><cell>-</cell><cell cols="4">44.87±0.36 68.95±1.33 6.49±0.15 40.04±0.3</cell><cell cols="2">73.56±1.18 8.77±0.08</cell></row><row><cell>Seq2Seq</cell><cell>WIKILARGE</cell><cell cols="6">32.71±1.55 88.56±1.06 8.62±0.34 35.79±0.89 90.24±2.52 8.63±0.34</cell></row><row><cell>Seq2Seq</cell><cell>MINED</cell><cell cols="6">38.03±0.63 61.76±2.19 9.41±0.07 38.06±0.47 63.70±2.43 9.43±0.07</cell></row><row><cell>BART+ACCESS</cell><cell>WIKILARGE</cell><cell cols="2">43.63±0.71 76.28±4.3</cell><cell cols="4">6.25±0.42 42.62±0.27 78.28±3.95 6.98±0.95</cell></row><row><cell>MBART+ACCESS</cell><cell>MINED</cell><cell>41.11±0.7</cell><cell cols="5">77.22±2.12 7.18±0.21 39.40±0.54 77.05±3.02 8.65±0.4</cell></row><row><cell>BART+ACCESS</cell><cell>MINED</cell><cell cols="6">42.65±0.23 66.23±4.31 8.23±0.62 40.85±0.15 63.76±4.26 8.79±0.3</cell></row><row><cell>BART+ACCESS</cell><cell cols="6">WIKILARGE + MINED 44.15±0.56 72.98±4.27 6.05±0.51 42.53±0.36 78.17±2.2</cell><cell>7.60±1.06</cell></row><row><cell>Original</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>±0.37 20.68 ±0.74 3.92 ±0.24 34.53 ±1.83 0.94 ±1.37 5.94 ±2.02 Seq2Seq MINED 39.25 ±0.64 41.79 ±1.47 3.63 ±0.17 25.66 ±0.52 6.00 ±0.17 15.89 ±0.11 MBART+ACCESS MINED 41.73 ±0.67 45.83 ±2.55 3.15 ±0.16 28.56 ±1.61 6.90 ±0.32 12.62 ±0.8 Unsupervised Text Simplification in French and Spanish. We display SARI, BLEU and FKGL on the French evaluation dataset ALECTOR and the Spanish evaluation dataset SIMPLEXT Corpus.</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell cols="2">ALECTOR (French)</cell><cell cols="3">SIMPLEXT Corpus (Spanish)</cell></row><row><cell></cell><cell></cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell></row><row><cell>Identity Baseline</cell><cell>-</cell><cell>26.16</cell><cell>64.39</cell><cell>3.73</cell><cell>6.08</cell><cell>8.16</cell><cell>17.72</cell></row><row><cell>Truncate Baseline</cell><cell>-</cell><cell>33.44</cell><cell>50.32</cell><cell>2.29</cell><cell>18.24</cell><cell>7.65</cell><cell>14.69</cell></row><row><cell cols="4">Pivot Baseline 33.48 model trained on paraphrases achieves these re--</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">sults. Furthermore, when generating a paraphrase,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">our model might assign higher probabilities to fre-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">quent words which might naturally operate lexical</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">simplifications to some extent.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Adding BART and ACCESS We use mined</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">data to finetune BART and add the simplification-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">based generative control from ACCESS. When</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">trained on the mined English data and WIKI-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">LARGE, BART+ACCESS performs the best in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">terms of SARI on ASSET (44.15). On TURK-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">CORPUS the best results are achieved by training</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">BART+ACCESS on WIKILARGE (42.62 SARI).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Mining Simplifications vs. Paraphrases. Comparison of models trained on our mined paraphrase corpora or on our mined simplification corpora.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ASSET (English)</cell><cell></cell></row><row><cell>Model</cell><cell>Data</cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell></row><row><cell>Seq2Seq</cell><cell cols="4">Simpl. 32.46±0.59 87.07±0.28 8.30±0.32</cell></row><row><cell>Seq2Seq</cell><cell>Para.</cell><cell cols="3">38.03±0.63 61.76±2.19 9.41±0.07</cell></row><row><cell cols="5">BART+ACCESS Simpl. 42.44±0.73 60.18±10.2 7.18±0.68</cell></row><row><cell cols="2">BART+ACCESS Para.</cell><cell cols="3">42.65±0.23 66.23±4.31 8.23±0.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 ,</head><label>7</label><figDesc>we analyze the performance of training our best model on English, a combination of BART and ACCESS, on different amounts of mined data. By increasing the ±0.33 91.91 ±0.11 9.92 ±0.02 10K 31.51 ±0.79 89.19 ±0.52 9.67 ±0.09 100K 41.25 ±0.76 84.09 ±1.68 7.75 ±0.15 1.2M (full) 42.65 ±0.23 66.23 ±4.31 8.23 ±0.62</figDesc><table><row><cell># Training</cell><cell></cell><cell>ASSET (English)</cell><cell></cell></row><row><cell>Samples</cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell></row><row><cell>1K</cell><cell>24.58</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Importance of Large Scale Mining. We finetune BART+ACCESS on varying amounts of mining.</figDesc><table><row><cell></cell><cell></cell><cell>ASSET (English)</cell><cell></cell></row><row><cell>Model</cell><cell>SARI</cell><cell>BLEU</cell><cell>FKGL</cell></row><row><cell>Seq2Seq</cell><cell cols="3">38.03±0.63 61.76±2.19 9.41±0.07</cell></row><row><cell>ACCESS</cell><cell cols="3">39.66±0.36 61.09±1.06 8.31±0.24</cell></row><row><cell>BART</cell><cell cols="3">39.73±0.20 65.00±1.66 9.26±0.09</cell></row><row><cell cols="4">BART+ACCESS 42.65±0.23 66.23±4.31 8.23±0.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Relative Importance of Pretraining and AC-CESS. We display SARI, BLEU and FKGL scores on the ASSET evaluation dataset of models trained on our mined paraphrase data.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">More detail can be found in the survey by<ref type="bibr" target="#b3">Alva-Manchego et al. (2020b)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://commoncrawl.org</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The original LASER implementation uses cosine similarity instead of L2 distance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We modify the Levenshtein similarity parameter to only consider replace operations, by assigning a 0 weight to insertions and deletions. This change helps decorrelate the Levenshtein similarity control token from the length control token and produced better results in preliminary experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">To avoid creating a discrepancy in terms of number of references between this setting where we leave one reference out and when we evaluate the models with all references, we compensate by duplicating one of the other references at random so that the total number of references is unchanged.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://github.com/feralvam/easse</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">We use the latest version of SARI implemented in EASSE which fixes bugs and inconsistencies from the traditional implementation of SARI. As a consequence, we also recompute</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">It should be noted that the previous state of the art by<ref type="bibr" target="#b37">Martin et al. (2020)</ref> was achieved before the publication of the ASSET dataset. ACCESS, the authors' controllable sentence simplification system might have performed better, were it finetuned for the ASSET dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was partly supported by Benot Sagot's chair in the PRAIRIE institute, funded by the French national agency ANR as part of the "Investissements davenir" programme under the reference ANR-19-P3IA-0001.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards brazilian portuguese automatic text simplification systems</title>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Sandra M Aluísio</surname></persName>
		</author>
		<author>
			<persName><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><forename type="middle">G</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><forename type="middle">Pm</forename><surname>Maziero</surname></persName>
		</author>
		<author>
			<persName><surname>Fortes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM symposium on Document engineering</title>
				<meeting>the eighth ACM symposium on Document engineering</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Asset: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th annual meeting of the Association for Computational Linguistics (ACL 2020)</title>
				<meeting>the 58th annual meeting of the Association for Computational Linguistics (ACL 2020)<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note>Benot Sagot, and Lucia Specia. To appear</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04567</idno>
		<title level="m">Easse: Easier automatic sentence simplification evaluation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-driven sentence simplification: Survey and benchmark</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="87" />
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural text simplification in low-resource conditions using weak supervision</title>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Palmero Aprosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattia A Di</forename><surname>Gangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</title>
				<meeting>the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised statistical machine translation</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01272</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to paraphrase: an unsupervised approach using multiple-sequence alignment</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference of the North American Chapter</title>
				<meeting>the 2003 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics on Human Language Technology</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">NLTK: The natural language toolkit</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Interactive Poster and Demonstration Sessions</title>
				<meeting>the ACL Interactive Poster and Demonstration Sessions<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="214" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">A</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09468</idno>
		<title level="m">Learning to split and rephrase from wikipedia edit history</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design and annotation of the first italian corpus for text simplification</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Brunato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felice</forename><surname>Dell'orletta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Venturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simonetta</forename><surname>Montemagni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 9th Linguistic Annotation Workshop</title>
				<meeting>The 9th Linguistic Annotation Workshop</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="31" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Findings of the WMT 2016 bilingual document alignment shared task</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-2347</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
				<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="554" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Practical simplification of english newspaper text to assist aphasic readers</title>
		<author>
			<persName><forename type="first">John</forename><surname>Carroll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvonne</forename><surname>Canning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siobhan</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tait</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology</title>
				<meeting>the AAAI-98 Workshop on Integrating Artificial Intelligence and Assistive Technology</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences using wikipedia</title>
		<author>
			<persName><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop on monolingual text-to-text generation</title>
				<meeting>the workshop on monolingual text-to-text generation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Queens are powerful too: Mitigating gender bias in dialogue generation</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Urbanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.03842</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An evaluation of syntactic simplification rules for people with autism</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><surname>Orasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iustin</forename><surname>Dornescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)</title>
				<meeting>the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="131" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using local knowledge graph construction to scale Seq2Seq models to multidocument inputs</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="4186" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Controllable abstractive summarization</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-2706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Neural Machine Translation and Generation</title>
				<meeting>the 2nd Workshop on Neural Machine Translation and Generation<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11556</idno>
		<title level="m">Reducing transformer depth on demand with structured dropout</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Alector: A parallel corpus of simplified french texts with alignments of misreadings by poor and dyslexic readers</title>
		<author>
			<persName><forename type="first">Núria</forename><surname>Gala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anaïs</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludivine</forename><surname>Javourey-Drevet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">C</forename><surname>Ziegler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Resources and Evaluation for Language Technologies (LREC)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Japanese news simplification: Task design, data set construction, and analysis of simplified text. Proceedings of MT Summit XV</title>
		<author>
			<persName><forename type="first">Isao</forename><surname>Goto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadashi</forename><surname>Kumano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kenlm: Faster and smaller language model queries</title>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth workshop on statistical machine translation</title>
				<meeting>the sixth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="187" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Text simplification without simplified corpora</title>
		<author>
			<persName><forename type="first">Tomoyuki</forename><surname>Kajiwara</surname></persName>
		</author>
		<author>
			<persName><surname>Komachi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Natural Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="223" to="249" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving text simplification by corpus expansion with unsupervised learning</title>
		<author>
			<persName><forename type="first">Akihiro</forename><surname>Katsuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhide</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Asian Language Processing (IALP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="216" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improving text simplification language modeling using unsimplified text data</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st annual meeting of the association for computational linguistics</title>
				<meeting>the 51st annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1537" to="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><surname>Robert P Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Findings of the wmt 2019 shared task on parallel corpus filtering for lowresource conditions</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Pino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
				<meeting>the Fourth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Findings of the WMT 2018 shared task on parallel corpus filtering</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huda</forename><surname>Khayrallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><forename type="middle">L</forename><surname>Forcada</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6453</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</title>
				<meeting>the Third Conference on Machine Translation: Shared Task Papers<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="726" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions, and reversals</title>
		<author>
			<persName><surname>Vladimir I Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet physics doklady</title>
				<imprint>
			<date type="published" when="1966">1966</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="707" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">Marjan Ghazvininejad,. 2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Xianggen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fandong</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03588</idno>
		<title level="m">Unsupervised paraphrasing by simulated annealing</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08210</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reference-less quality estimation of text simplification systems</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Humeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Emmanuel</forename><surname>Mazaré</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-7005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA)</title>
				<meeting>the 1st Workshop on Automatic Text Adaptation (ATA)<address><addrLine>Tilburg, the Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
	<note>Éric de La Clergerie, Antoine Bordes, and Benoît Sagot</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Éric de la Clergerie, and Antoine Bordes</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Sagot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Language Resources and Evaluation</title>
				<meeting>the 12th International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Controllable sentence simplification</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving machine translation performance by exploiting non-parallel corpora</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Dragos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Munteanu</surname></persName>
		</author>
		<author>
			<persName><surname>Marcu</surname></persName>
		</author>
		<idno type="DOI">10.1162/089120105775299168</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="477" to="504" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-4009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
				<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
	<note>Minnesota</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised lexical simplification for non-native speakers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gustavo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Paetzold</surname></persName>
		</author>
		<author>
			<persName><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simplify or help?: text simplification strategies for people with dyslexia</title>
		<author>
			<persName><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility</title>
				<meeting>the 10th International Cross-Disciplinary Conference on Web Accessibility</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Making it simplext: Implementation and evaluation of a text simplification system for spanish</title>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luz</forename><surname>Rello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biljana</forename><surname>Drndarevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Accessible Computing (TACCESS)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05791</idno>
		<title level="m">Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno>arXiv:1911</idno>
		<title level="m">Ccmatrix: Mining billions of high-quality parallel sentences on the web</title>
				<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08654</idno>
		<title level="m">What makes a good conversation? how controllable attributes affect human judgments</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic text simplification for spanish: Comparative evaluation of various simplification strategies</title>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Štajner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horacio</forename><surname>Saggion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference recent advances in natural language processing</title>
				<meeting>the international conference recent advances in natural language processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Semantic structural evaluation for text simplification</title>
		<author>
			<persName><forename type="first">Elior</forename><surname>Sulem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omri</forename><surname>Abend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Rappoport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="685" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised neural text simplification</title>
		<author>
			<persName><forename type="first">Sai</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Laha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1198</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2058" to="2068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Parallel Data, Tools and Interfaces in OPUS</title>
		<author>
			<persName><forename type="first">Jrg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
				<meeting>the Eight International Conference on Language Resources and Evaluation (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simpitiki: a simplification corpus for italian</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Palmero Aprosio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Saltori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Italian Conference on Computational Linguistics</title>
				<meeting>the Third Italian Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ParaNMT-50M: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations</title>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1042</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to simplify sentences with quasi-synchronous grammar and integer programming</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Woodsend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="409" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName><forename type="first">Antal</forename><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Problems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="283" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Sentence simplification with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="584" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Integrating transformer and paraphrase rules for sentence simplification</title>
		<author>
			<persName><forename type="first">Sanqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andi</forename><surname>Saptono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bambang</forename><surname>Parmanto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1355</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3164" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Semi-supervised text simplification with back-translation and asymmetric denoising autoencoders</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A monolingual tree-based translation model for sentence simplification</title>
		<author>
			<persName><forename type="first">Zhemin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Delphine</forename><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010)</title>
				<meeting>the 23rd International Conference on Computational Linguistics (Coling 2010)<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Beijing</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1353" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
