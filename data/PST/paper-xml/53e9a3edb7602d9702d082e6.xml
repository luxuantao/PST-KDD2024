<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classification of Short Texts by Deploying Topical Annotations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Daniele</forename><surname>Vitale</surname></persName>
							<email>d.vitale@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
							<email>ferragina@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
							<email>scaiella@di.unipi.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">University of Pisa</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classification of Short Texts by Deploying Topical Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">087AA2C9F7C967C92A058F7887A1D619</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach to the classification of short texts based on two factors: the use of Wikipedia-based annotators that have been recently introduced to detect the main topics present in an input text, represented via Wikipedia pages, and the design of a novel classification algorithm that measures the similarity between the input text and each output category by deploying only their annotated topics and the Wikipedia link-structure. Our approach waives the common practice of expanding the feature-space with new dimensions derived either from explicit or from latent semantic analysis. As a consequence it is simple and maintains a compact intelligible representation of the output categories. Our experiments show that it is efficient in construction and query time, accurate as state-of-the-art classifiers (see e.g. Phan et al. WWW '08), and robust with respect to concept drifts and input sources.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text Categorization (TC) is the problem of labeling natural language texts with one or more thematic categories drawn from a predefined set. It is one of the most important research fields in Information Retrieval (IR), and its solutions are at the core of several applications ranging from the automatic cataloging of newspaper pages and web pages, the management of incoming emails, as well as to the annotation of genomic sequences <ref type="bibr" target="#b14">[15]</ref>. The majority of existing text classifiers represent a text as a bag of words (shortly, BOW), and then use machine learning techniques over vectors in a high-dimensional space whose features are reals derived from the occurrence frequencies of those words. Many learning methods, such as k-nearest neighbors (k-NN), Naive Bayes, maximum entropy, and support vector machines (SVM), have been used over BOW to solve a lot of classification problems achieving satisfactory results (see e.g. <ref type="bibr" target="#b14">[15]</ref>).</p><p>In the last decade, however, the explosion of applications in the field of ecommerce, social networks, search engines, instant messaging and information publishing, introduced a new challenging scenario in which texts are very short (a few tens of terms), sparse and poorly written. These texts do not provide enough word co-occurrence or shared context to obtain accurate results via classic similarity text measures, as the ones hinging onto the BOW method, so their performance becomes quite limited in this new scenario.</p><p>Recently, several studies tried to overcome these limitations inspired by the observation that when humans approach the task of text categorization, they interpret the specific wording of the document in the much larger context of their background knowledge and experience. This has been technically implemented by using external knowledge bases to endow the algorithms with the breadth of knowledge available to humans. Examples are DMOZ <ref type="bibr" target="#b4">[5]</ref>, Wikipedia (see e.g. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12]</ref>) or even the whole Web <ref type="bibr" target="#b12">[13]</ref>. Two main approaches have been followed to deploy this encyclopedic knowledge: one computes similarity scores between texts based on the results returned by a search engine <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>; the other one enriches the BOW representation with new dimensions representing topics detected in the input texts. In both cases the classification performance is significantly better than the one obtainable with previously known approaches, and today LSAbased approaches (e.g. <ref type="bibr" target="#b11">[12]</ref>) are the state-of-the-art.</p><p>Our work belongs to this second line of research, in that it deploys an externalknowledge base (namely Wikipedia) to derive the topics which occur in the texts to be classified, but it diverts from the known approaches in many respects that we are going to comment below. First of all, we detect the topics occurring in the input texts by using a recent set of IR-tools, called topic annotators <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. These tools are efficient and accurate in identifying meaningful sequences of terms in a input text and in linking them to pertinent Wikipedia pages representing their underlying topics. Among all known annotators we decided to use Tagme <ref type="bibr" target="#b3">[4]</ref> because it is the state-of-the-art for processing short texts. There are several positive issues deriving from this choice, if compared to the stateof-the-art classifiers based on LSA-methods <ref type="bibr" target="#b11">[12]</ref>. First, Tagme does an explicit semantic analysis of the input texts by manipulating in a principled and controlled way manifest topics grounded in Wikipedia, rather than the latent topics derived by using LSA. Second, Tagme is efficient in distilling the used knowledge from the entire Wikipedia, unlike LSA-approaches that need to restrict the analysis to a limited set of training data, because of their high time complexity. Finally, Tagme hinges on very few parameters and this helps to improve its generalization performance, unlike the LSA-approaches that use many parameters that need an extensive tuning and an "appropriate universal" training set <ref type="bibr" target="#b11">[12]</ref>. The main specialty of our proposal is that we do not use the detected topics (Wikipedia pages) to expand the BOW-space or to enrich the content of the input texts, as done before. Rather, we characterize the output categories via a set of top topics, derived from the annotation of Tagme over the training samples. The selection is based on a ranking function that mimics the classical tf-idf scheme here adapted to work on topics vs categories, rather than terms vs documents. The key advantage of this topical-representation of output categories, compared to LSA and derivatives, is that the top topics are grounded in Wikipedia pages and thus can be interpreted easily.</p><p>Our final contribution is the design of a single-label classifier that is based on a novel similarity measure between the set of top topics of a category and the set of topics individuated by Tagme in the input text. This similarity measure takes into account the link-structure of Wikipedia by extending the measure proposed in <ref type="bibr" target="#b10">[11]</ref> for a pair of topics to a pair of sets of topics. This way our classifier deploys not only the content of Wikipedia pages but also their interlinked structure, which has been proved in the last years to be much useful to detect semantic similarity among short texts (see e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>).</p><p>We have tested our approach over 3 datasets, one composed of search-result snippets and made available by <ref type="bibr" target="#b11">[12]</ref>, the other two datasets are composed of tweets and news and were created by ourselves. On the first dataset (snippets) our classifier yields a performance comparable to the one achieved by the best approach based on pLSA <ref type="bibr" target="#b11">[12]</ref>; but, unlike pLSA, our classifier does not need a time-consuming training phase. On the other two datasets (news and tweets) our classifier improves significantly known approaches, based on BOW and machine learning tools, when few training examples are available and it shows to be robust with respect to the concept-drift problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several studies in the last years tried to handle the problem of classifying short texts. They can be grouped in two main approaches: the first one proposed similarity distances specifically tailored to work on short texts, the second one focused on enriching the BOW representation by generating new features derived from external-knowledge bases, such as Wikipedia.</p><p>Sahami and Heilman <ref type="bibr" target="#b12">[13]</ref> proposed a similarity kernel function between short texts based on the results returned for a web-search composed according to the two texts to be compared. This function can then be used in any kernelbased machine learning algorithm, such as k-NN. This method (and other similar ones, e.g. <ref type="bibr" target="#b1">[2]</ref>) is mainly term-based and time consuming, because it needs to query repeatedly a search engine, and thus it does not fit well with applications managing a stream of short texts such as the ones mentioned in the introduction.</p><p>Zelikovitz and Hirsh <ref type="bibr" target="#b18">[19]</ref> suggested to find similarity between two documents that can be related but don't share words, one from the training set and the other from the test set, by leveraging their similarity to other unlabeled but longer documents. In a later work, Zelikovitz et al. <ref type="bibr" target="#b19">[20]</ref> used techniques of Transductive Latent Semantic Analysis, a variant of LSI, to expand the training set with unlabeled test examples. This idea has been further developed and improved in 2008 by Phan et al. <ref type="bibr" target="#b11">[12]</ref>. They presented a framework for discovering hidden topics based on LDA, a variant of pLSA, and then added these topics to the BOW-representation of short texts, which were then classified using a maximum entropy learning method. This approach offers high accuracy over two specific datasets consisting of few MBs of training data (one composed of about 12k websearch results and the other one consisting of 50k medical abstracts). Nonetheless it presents two main limitations: it needs the tuning of many parameters and a high time complexity, so it takes days of computing time even if it is restricted to work on a reduced "universal dataset" that must be properly built (see <ref type="bibr" target="#b11">[12]</ref>).</p><p>Gabrilovich and Markovitch <ref type="bibr" target="#b5">[6]</ref> (see also <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>) proposed to perform a semantic analysis based on manifest topics grounded in Wikipedia pages, rather than latent topics used by LSA. So they expanded the BOW-representation of short texts with new dimensions representing topics detected in the input texts to be classified and modeled by Wikipedia pages. The mapping between each text and the Wikipedia topics is achieved through a feature generator which acts like a retrieval engine. It receives a short text and outputs the most relevant Wikipedia pages which are related to that text. The titles of these pages are further filtered and those with high discriminative capacity are used as additional features to expand the BOW-representation of the corresponding input texts. The expanded feature vectors are then classified using SVM. <ref type="bibr" target="#b5">[6]</ref> showed significant improvements with respect to learning methods based on the plain BOW. Our proposal is also based on Wikipedia pages as topics, but we detect these topics via modern annotators, such as Tagme <ref type="bibr" target="#b3">[4]</ref>, and we use these topics to provide a novel text/category representation rather than deploying them to expand the BOWspace. These annotators are efficacious in disambiguating polysemous terms and in relating synonymous terms in short texts, so we argue that they should be more powerful than just searching Wikipedia for related pages, as done in <ref type="bibr" target="#b5">[6]</ref>. Moreover, the experimental results in <ref type="bibr" target="#b10">[11]</ref> show that expanding the BOW-space with possibly noisy topics is much more time consuming, induces a complicated feature post-selection step, and it is less effective than just directly comparing Wikipedia pages by means of the inter-linked structure of the Wikipedia graph, as we do in our paper.</p><p>Recently Sriram et al. <ref type="bibr" target="#b15">[16]</ref> proposed an algorithm specifically designed to classify Twitter messages in a set of five categories: news, events, opinions, deals and private messages. The classification task was based on eight features which were domain specific. A more general framework for Twitter messages is described in <ref type="bibr" target="#b6">[7]</ref>. The authors proposed to map tweets to Wikipedia pages and then to calculate a distance between them by measuring the overlap of their Wikipedia categories. But it is well known that the category graph of Wikipedia is "haphazard, redundant, incomplete, and inconsistent" <ref type="bibr" target="#b9">[10]</ref>; moreover the searches of <ref type="bibr" target="#b6">[7]</ref> between single words and titles of Wikipedia pages may possibly miss a multi-term topic.</p><p>The softwares used in the above papers are missing, so we will use only the results reported in <ref type="bibr" target="#b11">[12]</ref> for which the dataset is available. Another contribution of our paper is to make publicly available two large datasets (tweets and news) and the software of our classifier for future (repeatable) comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Topic Annotators</head><p>A recent line of research <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> has started to successfully address the problem of providing a semantic contextualization of texts by detecting short and meaningful sequences of terms into them and then link each sequence to a relevant and pertinent Wikipedia page (aka topic). These links solve efficaciously synonymy and polysemy issues, because the identified Wikipedia pages unambiguously represent the specific topics denoted by those sequences of terms given the context offered by the input text. As an example, let us consider the following text fragment: "US president issues Libya ultimatum". These topic annotators are able to detect "US president", "Libya" and "ultimatum" as meaningful phrases to be linked with the topics represented by the Wikipedia pages dealing with the President of the United States, the nation of Libya and the threat to declare war, respectively. This contextualization is very powerful because it may help in detecting the semantic similarity of texts not sharing terms, which is one of the limitations of the classical similarity measures based on the BOW-models. Indeed, consider this text fragment: "Barack Obama says Gaddafi may wait out military assault". It would be difficult to detect the tight relationship between this one and the previous text by using classical similarity measures based on word matches, tf-idf or co-occurrences. On the contrary, the concepts associated to the input texts by topic annotators might allow one to discover easily this connection by taking into account the Wikipedia link-structure (more later).</p><p>The disambiguation task performed by these annotators also prevents errors due to ambiguous words. For example consider the following two similar texts: "the paparazzi photographed the star" and "the astronomer photographed the star". A word-based approach would find hard to figure out their wide topicdistance. Topic annotators instead would link the word "star" in the first fragment to the Wikipedia page entitled "Celebrity" and, in the second fragment, to the page "Star"(intended as the astronomical object). And since these two pages (topics) are far in the Wikipedia graph, an algorithm could easily spot the semantic distance between the two phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Topic-Based Classifier</head><p>Unlike previous works that aimed for expanding the BOW-representation with features extracted from external knowledge bases (see Section 2), consisting of either explicit <ref type="bibr" target="#b5">[6]</ref> or latent topics <ref type="bibr" target="#b11">[12]</ref>, our classifier relies only on the topics identified by Tagme in the input texts and on the connectivity among the corresponding Wikipedia pages in the underlying Wikipedia graph. As an example, recall the previous two phrases about US President and Obama. Previous classifiers would relate these phrases by expanding "Barack Obama" and "US President" with all possible related topics, hoping to find the common topic "President of the United States" among them. Conversely, we process the two phrases with Tagme and thus annotate the segments "Barack Obama" and "US President" with their corresponding Wikipedia pages. At this point we can detect on-the-fly the strong relationship between those two texts by identifying the proximity in the Wikipedia graph of these two Wikipedia pages.</p><p>Starting from these promising considerations, we have designed our shorttext classifier to work as follows. We annotate all training samples with Tagme and then characterize each output category via a set of top topics chosen by a combination of their frequency in the training samples and their diversification.</p><p>At query time, the input text is annotated by Tagme and its set of topics is compared against the set of top-topics of each category, searching for the most similar one. The similarity is computed by extending in a principled way the measure devised in <ref type="bibr" target="#b10">[11]</ref> for just a pair of topics to a pair of sets of topics taking advantage of the Wikipedia link-structure. Clearly our classifier does not rely on any learning method-such as k-NN, Naive Bayes, or SVM. An extensive set of experiments will show in Section 5 that this classification framework is efficient, accurate and robust in that it can be applied successfully to news, tweets and snippets, and because it can address the concept-drift problem <ref type="bibr" target="#b13">[14]</ref> without the need to change significantly its parameters and structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Training Phase</head><p>Let C be the set of known categories, we use Tagme to process all training samples and, for each category c ∈ C, we denote by T c the whole set of topics identified in the samples labeled as c. Then we apply a properly defined (see below) ranking function to each topic in T c , and finally select the top-k topics that will be used to characterize the category c in the subsequent testing phase. In the rest of this paper we will denote by T k c the set of top-k topics selected for the category c. As the experiments will show in the next Section 5, the only parameter we have to control in this process is k.</p><p>The key issue for implementing the training phase is therefore the design of the ranking function which helps in selecting the top-k topics in T c . This function is inspired by the well-known tf-idf schema, here transposed to work in the context of topics vs categories, rather than terms vs documents. For each category c ∈ C, we define: rank c (t) = f req(t, c) × log |C| C(t) , where f req(t, c) is the number of training documents belonging to category c and annotated with topic t by Tagme, whereas C(t) is the number of categories whose samples have been annotated with t (hence t ∈ T c ). We can paraphrase this formula by saying that the first part depends on the topic frequency, whereas the second part depends on the inverse category frequency of t. The idea is to penalize topics that are very common among categories and to rank higher topics that are the most discriminative for the category c.</p><p>The best k topics in T c are selected in accordance to rank c and they define the classification model for the category c. Parameter k has to be chosen carefully because it affects the time efficiency of the classification process and its accuracy. A large k could include too generic topics, while a small k could reduce the generalization performance of the model. The value of this parameter will be analysed in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Classification Phase</head><p>Given an input short text d, we use Tagme to detect the set of topics mentioned in it, say T d . We then compute a classification score for each category c ∈ C by comparing in a proper way the topics discovered by Tagme in d, hence T d , and the top-k topics modeling category c, hence T k c . The key issue here is how to "compare" these two sets of topics in order to establish how much related are the short text d and the category c. Looking carefully at the problem we are indeed required to compare two sets of nodes (pages of Wikipedia) living into a much larger graph (Wikipedia graph), so we could adopt any distance function based on the structure and inter-linkedness of that graph.</p><p>There have been many proposals of distances between Wikipedia nodes (see e.g <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17]</ref>). The most effective one is currently the Wikipedia Link-based Measure (WLM) proposed by Milne and Witten in <ref type="bibr" target="#b10">[11]</ref> and extensively tested in <ref type="bibr" target="#b9">[10]</ref>. Technically, WLM computes the relatedness between two topics as a function of the size of the intersection between their ingoing stars in the Wikipedia graph. In other words, the relatedness is estimated by taking into account the number of simple 2-long paths connecting the two compared nodes (for details see <ref type="bibr" target="#b10">[11]</ref>). This measure finds its theoretical roots in the Google Similarity Distance of <ref type="bibr" target="#b2">[3]</ref>. WLM is both cheaper, more robust and more accurate than other known measures: cheaper because Wikipedia's extensive textual content may be ignored in its calculation (cfr. <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6]</ref>), more robust because it computes the volume of short paths connecting the compared nodes rather than their shortest-path distance (cfr. <ref type="bibr" target="#b16">[17]</ref>), and more accurate because it is more closely tied to the manually defined semantics of the resource.</p><p>In our context the comparison between a category c with an input text d boils down to the comparison of two sets of topics, namely T k c and T d . We therefore extend the WLM measure by computing the sum of the relatedness between each pair of topics, one annotated in d and the other one from the top-k topics of category c. Each term of this sum is weighted to take into account the fact that topics in d do not have the same importance in characterizing the "category" of this text. We have chosen as weight the score assigned by Tagme to each annotation in d, called ρ-score in <ref type="bibr" target="#b3">[4]</ref>. This value measures the importance and reliability of an annotation with respect to all other annotated topics in the input text d, and thus reasonably quantifies how much an annotation "can say" about the category of that input text.</p><p>Formally, the classification score of the input text d into the category c has been defined as follows:</p><formula xml:id="formula_0">CSV c (d) = t ∈T d ρ(d, t ) × ⎛ ⎝ t ∈T k c rel(t , t ) ⎞ ⎠</formula><p>where ρ(d, t ) is the ρ-score assigned by Tagme to the annotation t detected in d and rel(t , t ) is the WLM-measure of <ref type="bibr" target="#b10">[11]</ref> applied to the two Wikipedia pages denoting the topics t and t . The category with the highest score is the classification of d. Note that our classifier differs significantly from known approaches commented in Section 2, because (1) it deploys for the first time the recently proposed topic annotators (here we used Tagme) to detect topics within the input texts to be classified, and, more importantly, because (2) it uses these detected topics not to augment the BOW-representation or the text content of the training and test samples, but rather to directly compare the input text and the output categories via the novel similarity function CSV c (d) which strongly deploys the connectivity of the Wikipedia graph and no learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We evaluated our classifier over three datasets composed of snippets of web pages returned by a search engine, short news extracted from RSS feeds, and microblogging messages taken from Twitter. The first dataset, called Snippets, was released by <ref type="bibr" target="#b11">[12]</ref> and it is composed of 12k (10k of training and 2k of test) snippets returned by a web-search engine. Snippets have length of about 13 terms (on average) and are labeled with 8 categories. This dataset was built to limit the cooccurrence of terms between training and testing samples, and thus to emphasize the performance of semantic-based classifiers as the ones experimented in <ref type="bibr" target="#b11">[12]</ref>. It must be said that <ref type="bibr" target="#b11">[12]</ref> removed stop and rare words from the snippets: this is clearly dis-advantageous for our approach because, unlike the approaches based on pLSA or ESA, Tagme operates at multi-words level and thus could miss some annotations. Nonetheless, to our knowledge, this is the only dataset of short texts available to the community<ref type="foot" target="#foot_0">1</ref> and thus we used it to compare our classifier with respect to the state-of-the-art proposal of <ref type="bibr" target="#b11">[12]</ref>.</p><p>The second dataset, called News, is composed of 32k short texts drawn from the RSS feeds of three newspapers: nytimes, reuters and usatoday. We built it by gathering all news stories published by these editors from March 2011 until June 2011, and took their title and the short abstract, if available, accounting for about 20 terms on average. We derived the category of each news from the taxonomy of its publishing web-site. However, since the three taxonomies are different, we have identified seven common categories: World, US, Science and Technology, Sport, Business, Health, Entertainment.</p><p>The last dataset, called Tweets, is composed of about 7k messages that we gathered from Twitter in July 2011 according to the following process: (a) we downloaded from the public stream of Twitter only the messages that contained more than 10 alphabetic chars and a link to some popular web-site of news<ref type="foot" target="#foot_1">2</ref> ; (b) we parsed the linked news-page in order to find the category in which that page is classified by the web-site itself; (c) we labeled the Twitter message with this category, limiting to the seven common categories listed above. Here we postulate that the classification label assigned to each tweet is correct because the tweet probably contains a comment or a description of the linked news-story and thus can be categorized in the same category of that linked news. We argue that this dataset is hard to classify because of the well-known poor textual composition of tweets, which amounts to an average of 8 words in our dataset.</p><p>We created the training and test sets by splitting in two halves both News and Twitter datasets according to the publishing date. We performed several kinds of experiments over these three datasets. The first experiment aimed at evaluating the robustness of the classifier by varying the size of the training set: we trained 10 different classifiers on an increasing number of training samples ranging from 10% to 100% of the overall training data. For each size, we computed the accuracy of the classification process with respect to the test set (of about 16k for News and 3.5k for Tweets). We call this test VarTrain. The second experiment was aimed at evaluating the robustness of classifiers with respect to the concept-drift problem <ref type="bibr" target="#b13">[14]</ref>, because the meaning of some concepts could change over the time thus defeating the classification model built in advance (this test is called ConceptDrift). This issue is important when dealing with time-related sources like the ones generating our datasets News and Tweets. Thus we divided these two datasets in timely-based partitions, so we used the first week of the training set of News (1054 news) to train the model, and then we generated 7 tests (of about 2300 news on average) by dividing the test set of news in a weekly-basis. Similarly for Tweets, we used the first two days to train the model (738 news), then we discarded the next two days, and finally we generated 6 tests (of about 890 tweets on average) by dividing the remaining tweets on a two-days basis. As can be seen in both cases we introduced a time-gap between the training set and the first test set, in order to limit overlapping topics. For all experiments we reported the classification accuracy. All three datasets are available at http://acube.di.unipi.it/datasets. Parameter Tuning. Our approach relies on just one parameter: the number k of top-topics selected to model each category (see Section 4). For each experiment, we identify the best value of k in the range 10-50 by using a small portion of the training set (about 10%) as validation set. Our experiments showed that, for all datasets and all tests, the best k is small and ranges from 15 to 30 (for higher values the accuracy remains constant or gets slightly worse).</p><p>Results for Snippets. This dataset was built by the authors of <ref type="bibr" target="#b11">[12]</ref>, so its train/test parts are predefined. As pointed out above the available data miss part of the original texts so it is unfavorable for our approach. Nevertheless Fig. <ref type="figure">1</ref> shows that our classifier comes very close to the accuracy reported in <ref type="bibr" target="#b11">[12]</ref> <ref type="foot" target="#foot_2">3</ref> . More importantly our classifier can be trained in just 30mins on a commodity PC, whereas <ref type="bibr" target="#b11">[12]</ref>'s approach requires several hours of computing time over a cluster of PCs. Finally our classifier is faster to be queried because of the compactedness of text and category representations, and the peculiarity of our similarity distance which depends only on the structure of the Wikipedia graph.</p><p>Results for News. Since <ref type="bibr" target="#b11">[12]</ref>'s system is unavailable, we compared our approach with classifiers based on machine-learning algorithms over the BOW representation: typically they use Bayesian approach (Naive Bayes), tree classifier (C4.5), Support Vector Machine (SVM) or Maximum Entropy (MaxEnt). The features for the BOW-approaches were weighted following the tf-idf schema. The VarTrain test, reported in Fig 2 .a, shows an impressive accuracy when classifying new data with a very small number of training samples. Our approach resulted also very accurate for the concept-drift problem, as shown in Fig 2 .b. In conclusion, our approach is better when the training set is small with respect to the test set. This can be explained by the fact that this dataset is maybe easy for machine-learning tools since news were collected in a short time window, thus their set of features may be homogeneous. This is different from Snippets, which was created to limit the co-occurrence of terms between the training and the test documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results for</head><p>Tweets. This dataset is challenging because the tweets are very short, noisy, and not always in English. However, as shown in Fig. <ref type="figure">3</ref>, our approach yields an impressive improvement of classification accuracy with respect to the classic BOW-based approaches for both tests. Notice that the overall performance is lower than the one achieved by all classifiers over the other datasets, underlying the fact that this dataset is very difficult because of tweets features. Also in this case, our classifier is better for smaller training sets and this feature Fig. <ref type="figure">3</ref>. Evaluation over Twitter dataset. Topical is our proposal. On the y-axis is reported the accuracy. On the x-axis of (a) is reported the size of the training set, while on the x-axis of (b)are reported the 6 test sets chronologically ordered.</p><p>is particularly useful in the context of Twitter where is reasonable to argue that the topics of discussion change frequently, so it's more difficult to individuate a representative training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We presented a novel approach to the classification of short texts which resulted very accurate, efficient, simple and providing a compact representation of categories/texts. Known semantic-based classifiers (see e.g. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>) are instead more complex and time-consuming for the training and the classification phases. Our experiments showed also that our approach is robust with respect to the input source and the concept-drift problem. There are some other issues that we plan to investigate in the near future. The accuracy of our algorithm depends on the quality of the annotation produced by Tagme <ref type="bibr" target="#b3">[4]</ref> and on the relatedness measure introduced by <ref type="bibr" target="#b10">[11]</ref>: even though they are shown to be effective, it is worth considering a deeper tuning phase of these tools, or designing variations given the specific TC-task in hand.</p><p>Finally, we note that the literature is missing of repeatable experiments because the software implementing the recent solutions of <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> is not available. We foresee to re-implement and make available these two approaches, evaluate them over our publicly-available datasets, and thus provide a standard benchmark for future proposals.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Evaluation over News dataset. Topical is our proposal. On the y-axis is reported the accuracy. On the x-axis of (a) is reported the size of training set, while on the x-axis of (b) are reported the 7 test sets chronologically ordered.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Evaluation over Snippet dataset. Topical is our proposal. On the y-axis is reported the accuracy, on the x-axis is reported the size of training set.</figDesc><table><row><cell>ϴϱй</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ϳϱй</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ϲϱй</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ϱϱй</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ϰϱй</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ϭŬ</cell><cell>ϮŬ</cell><cell>ϯŬ</cell><cell>ϰŬ</cell><cell>ϱŬ</cell><cell>ϲŬ</cell><cell>ϳŬ</cell><cell>ϴŬ</cell><cell>ϵŬ ϭϬŬ</cell></row><row><cell></cell><cell></cell><cell cols="2">DĂǆŶƚ</cell><cell></cell><cell></cell><cell></cell><cell cols="2">dKW/ &gt;</cell></row><row><cell></cell><cell></cell><cell cols="2">WŚĂŶ Ğƚ Ăů͘</cell><cell></cell><cell></cell><cell></cell><cell>^sD</cell><cell></cell></row><row><cell>Fig. 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We also considered the datasets by<ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> but those texts are composed of an unordered sequence of stemmed terms. Thus they cannot be processed by the topic annotators presented in Sect. 3 because they need the full-words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p><ref type="bibr" target="#b1">2</ref> We consider links to cnn.com, huffingtonpost.com, nytimes.com and reuters.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Performances of this system and MaxEnt (the baseline in<ref type="bibr" target="#b11">[12]</ref>) are extracted from<ref type="bibr" target="#b11">[12]</ref> because the software is unavailable. Hence we could not check accuracy of this classifier on our two other datasets.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work has been supported in part by MIUR PRIN MadWeb, MIUR FIRB Linguistica '06, and a Google Faculty Award 2010.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Clustering Short Texts using Wikipedia</title>
		<author>
			<persName><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="787" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Measuring semantic similarity between words using Web Search engines</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ishizuka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Google similarity distances</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vitanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="370" to="383" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">TAGME: On-the-fly annotation of short text fragments (by Wikipedia entities)</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Scaiella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ACM CIKM</publisher>
			<biblScope unit="page" from="1625" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature generation for text categorization using world knowledge</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conference on A.I</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1048" to="1053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wikipedia-based Semantic Interpretation for Natural Language Processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="443" to="498" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discovering Context: Classifying Tweets through a Semantic Transform Based on Wikipedia</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Genc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sakamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Nickerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FAC 2011</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Schmorrow</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Fidopiastis</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6780</biblScope>
			<biblScope unit="page" from="484" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust Disambiguation of Named Entities in Text</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fürstenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="782" to="792" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collective annotation of Wikipedia entities in web text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mining meaning from Wikipedia</title>
		<author>
			<persName><forename type="first">O</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="716" to="754" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An effective, low-cost measure of semantic relatedness obtained from Wikipedia links</title>
		<author>
			<persName><forename type="first">D</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop on Wikipedia and Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to Classify Short and Sparse Text &amp; Web with Hiddent Topics from Large-scale Data Collections</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Houriguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A web-based kernel function for measuring the similarity of short text snippets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Heilman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="377" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Schlimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Graner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Beyond Incremental Processing: Tracking Concept Drift</title>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="502" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Machine learning in automated text categorization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Short text classification in twitter to improve information filtering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fuhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ferhatosmanoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="841" to="842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WikiRelate! Computing Semantic Relatedness Using Wikipedia</title>
		<author>
			<persName><forename type="first">M</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ponzetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="page" from="1419" to="1424" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards effective short text deep classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Haofen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1143" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving short-text classification using unlabeled data for classification problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zelikovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hirsh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1191" to="1198" />
		</imprint>
		<respStmt>
			<orgName>ICML</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Transductive Learning for Short-Text Classification problems using Latent Semantic Indexing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zelikovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJPRAI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="146" to="163" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
