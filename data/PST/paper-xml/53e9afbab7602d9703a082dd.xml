<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harvesting Visual Concepts for Image Search with Complex Queries</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
							<email>nieliqiang@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">National University of Singapore</orgName>
								<orgName type="institution" key="instit2">Hefei University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harvesting Visual Concepts for Image Search with Complex Queries</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">154469FD8D17000AA922A39B29A6F8A0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.3.3 [Information Search and Retrieval]: Retrieval models; H.3.5 [Information Systems]: Information Storage and Retrieval Complex Image Query</term>
					<term>Photo-based QA</term>
					<term>News Visualization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of image reranking to boost retrieval performance has been found to be successful for simple queries. It is, however, less effective for complex queries due to the widened semantic gap. This paper presents a scheme to enhance web image reranking for complex queries by fully exploring the information from simple visual concepts. Given a complex query, our scheme first detects the noun-phrase based visual concepts and crawls their top ranked images from popular image search engines. Next, it constructs a heterogeneous probabilistic network to model the relatedness between the complex query and each of its crawled images. The network seamlessly integrates three layers of relationships, i.e., the semantic-level, cross-modality level as well as visual-level. These mutually reinforced layers are established among the complex query and its involved visual concepts, by harnessing the contents of images and their associated textual cues. Based on the derived relevance scores, a new ranking list is generated. Extensive evaluations on a real-world dataset demonstrate that our model is able to characterize the complex queries well and achieve promising performance as compared to the state-of-the-art methods. Based on the proposed scheme, we introduce two applications: photo-based question answering and textual news visualization. Comprehensive experiments well validate the proposed scheme.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>With the exponential growth of image contents available online, searching for images is becoming an indispensable ac-tivity in people's daily lives. Meanwhile, web image retrieval has attracted world-wide research attentions and achieved great success for simple textual queries <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b11">13,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b29">31]</ref>. As the web surfers get increasingly savvy and specific with their search behaviors, the queries tend to be more complex and sophisticated. This phenomenon is consistent with the report from Hitwise [1] in late 2009 : the average query length is getting longer from 2007. In addition, long queries are becoming more and more popular in various media search applications, such as multimedia question answering <ref type="bibr" target="#b22">[24]</ref>, text illustration <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b9">11]</ref>, and known item search <ref type="bibr" target="#b6">[8]</ref>.</p><p>A complex image query is defined as a natural language query comprising several inter-related concepts. One example is the query "a baby with an apple lying on the bed". Here there are three concepts: baby, apple and bed. These concepts are linked by internal relationships: baby with an apple, both baby and apple are on the bed. It is obvious that complex queries can express specific information needs more precisely than the shorter ones. However, current commercial web search engines do not, in general, perform well with verbose queries, especially for image retrieval<ref type="foot" target="#foot_0">1</ref> . This is due to the following reasons. First, compared to simple queries, long ones frequently consist of more concepts, which further widen the semantic gap between the textual queries and the visual contents. Second, a complex query usually depicts the intrinsic semantic relationships among its constituent visual concepts. These kind images have loose coupled relationships with the surrounding textual descriptions, causing poor text-based search performance. Third, while there are abundant positive samples and query logs for simple queries, the positive samples are rare for complex queries. This makes learning based model less effective. Therefore, it is not surprising that the returned images are often incorrectly ranked for complex queries.</p><p>Visual reranking techniques can drastically improve the traditional text-based image search results. The existing approaches generally fall into two categories. One is pseudo relevance feedback (PRF) based <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b18">20]</ref>. They treat a significant fraction of the top images as pseudo-positive examples and collect some bottom images as pseudo-negative examples. They then either learn a classifier or cluster the images to perform reranking. But, for complex queries, relevant samples are usually rare or not ranked at the top of the result list. This severely limits the ability to select pseudo positive and negative training samples. The other category is graph based <ref type="bibr" target="#b30">[32,</ref><ref type="bibr" target="#b10">12,</ref><ref type="bibr" target="#b28">30]</ref> that propagates the initial ranking information over the whole graph until convergence. However, for complex query, many irrelevant images are frequently distributed in high ranked positions initially. These irrelevant images can hardly be pushed down by the graphbased methods, since they often have low similarities with other irrelevant images in the lower ranked positions <ref type="bibr" target="#b20">[22]</ref>. Consequently new approaches towards image reranking for complex queries are highly desired.</p><p>To tackle this problem, we hypothesize that the search results of a complex query are less visually consistent and coherent than those retrieved by each of its constituent visual concept; and the latter characterize the former's partial features in terms of both semantics and visual exemplars. An example illustrating the assumption is intuitively demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>. Based on this assumption, we explore the information cues from visual concepts to enhance Web image reranking for complex queries. Specifically, we propose a scheme, which contains two main components as shown in Figure <ref type="figure" target="#fig_2">2</ref>. The first component identifies the involved visual concepts by leveraging lexical and corpus-dependent knowledge, and collects the top relevant datapoints from popular image search engines. The second component constructs a heterogeneous probabilistic network to model the relevance between the complex query and each of its retrieved images. This network comprises three subnetworks, each representing a layer of relationship, including: (a) the underlying relationship among image pairs, (b) the cross-modality relationship between the image and the visual concept<ref type="foot" target="#foot_1">2</ref> , and (c) the high-level semantic relationship between visual concept and the complex query<ref type="foot" target="#foot_2">3</ref> . The three layers are strongly connected by a probabilistic model. The layers mutually reinforce each other to facilitate the estimation of relevance scores for new reanking list generation. Most importantly, the whole process is unsupervised and can be extended to handle large-scale data.</p><p>Based on the proposed scheme, we introduce two potential application scenarios of web image reranking for complex queries: photo-based question answering (PQA) and textual news visualization (TNV) <ref type="bibr" target="#b15">[17]</ref>. PQA is a sub-branch of multimedia question answering <ref type="bibr" target="#b22">[24]</ref>, aiming to answer questions with precise image information, which provides answer seekers with better multimedia experience. TNV is to complement the textual news with context associated images, which may better draw the readers' attention or help them grasp the textual information quickly. By conducting experiments on the real-world datasets, we demonstrate that our proposed scheme yields significant gains in reranking performance for complex queries, and achieves fairly satisfactory results for these two applications.</p><p>The main contributions of this research are: 1. To the best of our knowledge, this is the first that targets web images reranking for complex queries from the probabilistic perspective. This work unravels the unreliable initial ranking list problem of the existing image reranking approaches for complex queries. 2. It proposes a heuristic approach to detect noun-phrase based visual concepts from complex query, instead of just treating individual terms as possible concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">It proposes a heterogeneous probabilistic network to</head><p>automatically estimate the relevance score of each image, which jointly couples three layer relationships, spanning from semantic level to visual level. This is different from the conventional complex query modelling approaches <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b14">16,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b0">2]</ref> that either require human interactions or consider the query terms independently and neglect the connections among them.</p><p>The remainder is organized as follows. Sections 2 and 3 respectively review the related work and briefly introduce the reranking scheme. Sections 4 and 5 introduce visual concept detection and the proposed heterogeneous probabilistic network, respectively. Experimental results and analysis are presented in Section 6, followed by the applications in Section 7. Finally, Section 8 contains our remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Complex Queries in Text Search</head><p>Several recent research efforts have been conducted for improving long query performance in text-based information retrieval. These efforts can be broadly categorized into automatic query term re-weighting <ref type="bibr" target="#b4">[6,</ref><ref type="bibr" target="#b3">5,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b5">7]</ref> and query reduction <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b2">4]</ref> approaches.</p><p>It has been found that assigning appropriate weights to query concepts has significant positive effects on retrieval performance <ref type="bibr" target="#b4">[6]</ref>. Bendersky and Croft <ref type="bibr" target="#b3">[5]</ref> developed and evaluated a technique that assigns weights to the identified key concepts in the verbose query, and observed improved retrieval effectiveness. Lease et al. <ref type="bibr" target="#b13">[15]</ref> presented a regression framework to estimate term weights based on knowledge from past queries. A novel method beyond unsupervised estimation of concept importance was proposed in <ref type="bibr" target="#b5">[7]</ref>, which weights the query concept using a parameterized combination of diverse importance features.</p><p>Pruning the complex query to retain only the important terms is also recognized as one crucial dimension to improve search performance. Kumaran and Allan <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b12">14]</ref> proposed an interactive query induction approach, by presenting the users with the top 10 ranked sub-queries along with corresponding top ranking snippets. The tabbed interface allows the user to click on each sub-query to view the associated snippet, and select the most promising one as their new query. A more practical approach was proposed in <ref type="bibr" target="#b2">[4]</ref>   the reduced versions of the original query that were obtained by dropping one single term at a time. It can be incorporated into existing web search engines' architectures without requiring modifications to the underlying search algorithms.</p><p>Though great success has been achieved for complex query processing in text search domain, these techniques cannot be directly applied to the general media domain due to the different modalities between the query and search results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complex Queries in Media Search</head><p>Some research efforts have been conducted on modelling complex queries in media search. For example, Aly et al. <ref type="bibr" target="#b0">[2]</ref> proposed fusion strategies to model combined semantic concepts by simply aggregating the search results from their constituent primitive concepts. However, such approach fails to characterize complex queries as it overlooks the mutual relationships among different aspects of complex queries. Image search by concept map was proposed in <ref type="bibr" target="#b31">[33]</ref>. It presents a novel interface to enable users to indicate the spatial distribution among semantic concepts. However, the input model is not consistent with the current popular search engines and the concept-relationship is not limited to spacial arrangement. Yuan et al. <ref type="bibr" target="#b33">[35]</ref> explored how to utilize the plentiful but partially related samples, as well as the users' feedbacks, to learn complex queries in interactive concept-based video search. This work gracefully compensates the insufficient relevant samples. Further, Yuan <ref type="bibr" target="#b34">[36]</ref> moved one step beyond primitive concepts and proposed a higher-level semantic descriptor named "concept bundle" to enhance video search of complex queries. But these two works are supervised. Recently, harvesting social images for bi-concept search was proposed in <ref type="bibr" target="#b14">[16]</ref> to retrieve images in which two concepts are co-occurring. However, it is unable to handle multiple concepts.</p><p>Overall, literature regarding complex queries in media search is still relatively sparse, and the existing approaches either view the query terms independently or require intensive human interactions. Differring from the existing works, our approach models the complex queries automatically, and jointly considers the relationships between concepts and the complex queries from high-level to low-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">WEB IMAGE RERANKING SCHEME</head><p>As aforementioned, a complex query Q comprises several visual and abstract concepts as well as their intrinsic relations. As shown in the left part of Figure <ref type="figure" target="#fig_2">2</ref>, we first perform visual concepts selection, since they have strong description in images. Supposing T visual concepts C = {q 1, q2, . . . , qT } are detected. The T visual concepts are then regarded as simple queries to a commercial search engine and retrieve a collection of images D = {(x 1, y1), (x2, y2), . . . , (xL, yL)}.</p><p>Here the image</p><formula xml:id="formula_0">x i (xi ∈ R d ) is crawled using simple visual concept y i (yi ∈ C). Complex query Q has an ordered image list X = {(x L+1, xL+2, . . . , xL+N }.</formula><p>Our target is to explore the visual concepts and their partial relations to enhance the image relevance estimation with respect to the given complex query, i.e., Score(Q, x u), u = L + 1, . . . , L + N . Based on these relevance scores, a new refined ranking list will be generated.</p><p>To estimate the relevance score, we propose a heterogeneous probabilistic network as displayed in the middle part of Figure <ref type="figure" target="#fig_2">2</ref>, which is inspired by the KL-divergence measure <ref type="bibr" target="#b1">[3]</ref>. It is composed of several dissimilar sub-networks, which provide probabilistic estimations from different angles. But the constituents are of a conglomerate mass, strongly connected by a probabilistic model. It is formally formulated as,</p><formula xml:id="formula_1">Score(Q, x u) = - qc∈Q P (qc|Q) × log P (qc|xu)<label>(1)</label></formula><p>where P (q c|Q) measures the importance of a visual concept q c given the complex query Q, i.e., the high level semantic relatedness between a visual concept and the complex query. The second term in Eq.( <ref type="formula" target="#formula_1">1</ref>) can be further decomposed as,</p><formula xml:id="formula_2">P (qc|xu) = L i=1 P (qc|xi) × P (xi|xu)<label>( 2 )</label></formula><p>where P (q c|xi) involves two different modalities, specifically, the high level concept and the low level visual content; while P (x i|xu) measures the underlying visual relatedness of image pairs. The above formulation intuitively reflects that our proposed heterogeneous probabilistic network comprises three sub-networks, representing three different relationship layers: semantic level, cross-modality level and visual level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VISUAL CONCEPT DETECTION</head><p>In this paper, a visual concept is defined as a noun phrase depicting a concrete entity with a visual form. Beyond visual concepts, complex queries tend to contain several redundant chunks. These redundant chunks have grammatical meaning for communication between humans to help understand the key concepts <ref type="bibr" target="#b24">[26]</ref>, but are hard to model visually. One example is the query, "find images describing the moment the astronaut getting out of the cabin". In this query, only "the astronaut" and "the cabin" have high correspondence with the visual contents, while the use of other chunks may bring unpredictable noise to the image reranking method. Therefore, to differentiate the visual content related chunks from unrelated ones, we propose a heuristic framework for visual concept detection as illustrated in Figure <ref type="figure" target="#fig_3">3</ref>. A central resource in this framework is an automatically constructed visual vocabulary. Now given a complex query, we extract its constituent visual concepts as follows:</p><p>1. We segment a given complex query Q into several chunks using the openNLP 4 tool. 2. For each chunk, we match it against our constructed visual vocabulary. If any of its terms matches a term in our visual vocabulary, the chuck is classified as a visual concept. This detected visual concept is used as a simple query to retrieve the top ranked images and their surrounding texts for reranking purpose. 3. We construct a flexible vocabulary containing visual related words, by leveraging the lexical and corpusdependent knowledge. Specifically, we collect all the noun terms from our dataset utilizing the Part-Of-Speech Tagger 5 , and remove stop words from the noun set. For each selected noun word, we traverse along its hypernyms path in the WordNet, until one of the five predefined high-level categories is reached. They are "color", "thing", "artifact", "organism", and "natural phenomenon". These 5 categories cover almost all the key concepts in our dataset. The noun words that match to these 5 categories are recognized as visual related. This approach is analogous to <ref type="bibr" target="#b16">[18]</ref>.</p><p>Compared to the conventional single-word based visual concept definition <ref type="bibr" target="#b16">[18,</ref><ref type="bibr" target="#b29">31]</ref>, the noun-phrase based definition is able to incorporate a lot of adjunct terms, such as "a red apple", which carries additional color cue for "apple".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">HETEROGENEOUS NETWORK</head><p>In this section, we will discuss in greater detail each component of our proposed heterogeneous probabilistic network,  namely, semantic relatedness estimation, visual relatedness estimation and cross-modality relatedness estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Semantic Relatedness Estimation</head><p>Different concepts play different roles in the given complex query, and concept weighting <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b13">15,</ref><ref type="bibr" target="#b5">7]</ref> has been studied for decades to quantify their importances. However, these conventional methods are developed for long query in text search domain; few of them take the visual information into consideration. Instead, our approach estimates the semantic relatedness in image search by linearly integrating multifaceted cues, i.e., visual analysis, external resource analysis as well as surrounding text analysis.</p><p>First, from the perspective of underlying visual analysis, we respectively denote X c and X to be the set of images retrieved by the visual concept q c and complex query Q. Their relatedness can be defined as,</p><formula xml:id="formula_3">V (q c, Q) = 1 |Xc| × |X | x i ∈Xc,x j ∈X K(xi, xj)<label>(3)</label></formula><p>K(•, •) is the Gaussian similarity function, defined as,</p><formula xml:id="formula_4">K(x i, xj) = exp(- ||xi -xj|| 2 σ 2 ) (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where the radius parameter, σ, is simply set as the median of the Euclidean distances of all related image pairs. Second, actually the visual concepts detected from the same complex query are usually not independent. For example, for the complex image query "a lady driving a red car on the road", the semantical relationship between "a red car" and "the road" is relatively high. Inspired by Google distance <ref type="bibr" target="#b8">[10]</ref>, we estimate the inter-concepts relatedness based on the frequency of their co-occurrence by exploring the Flickr image resource as the largest publicly available multimedia corpus,</p><formula xml:id="formula_6">NGD(q c, qj) = max(log f (qc), log f (qj)) -log f (qc, qj) log M -min(log f (qc), log f (qj)) (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where M is the total number of images retrieved from Flickr, roughly estimated as 5 billion. f (q c) and f (qj) are respectively the numbers of hits for search concepts q c and qj, and f (q c, qj) is the number of web images on which both q c and qj co-occur. Note that we define NGD(qc, qj) = 0, if q c = qj. Then the relatedness between qc and the given complex query Q is:</p><formula xml:id="formula_8">G(qc, Q) = 1 T q j ∈C NGD(qc, qj) ( 6 )</formula><p>where T is the number of visual concepts detected from Q. This estimation can be viewed as exploring the external web image knowledge to weight the visual concepts. Third, we estimate the semantic relatedness by using the surrounding text-matching score. For each complex query Q, we first merge all surrounding textual information of its retrieved images, such as tag, title, description, etc, into a single document. The same operation is then conducted for all the T detected visual concepts, resulting in T documents. We then parse the T +1 documents using the OpenNLP tool. All nouns and adjectives are selected as salient words, since they are observed to be more descriptive and informative than verbs or adverbs. Based on these salient words, the tf-idf scores <ref type="bibr" target="#b34">[36]</ref> are computed to represent the semantic relatedness between a visual concept q c and the given complex query Q, denoted as T (q c, Q).</p><p>Finally, we linearly combine these three measures as,</p><formula xml:id="formula_9">P (q c|Q) = α1V (qc, Q) + α2G(qc, Q) + α3T (qc, Q) (7)</formula><p>where α i is the fusing weight with sum being 1. They are selected based on a training set comprising 20 complex queries, which are randomly sampled from our constructed complex query collection. We tune the weights to the values that optimize the average NDCG@50 with grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visual Relatedness Estimation</head><p>To explore the visual relationship between images, we perform Markov random walk over a K nearest neighbour graph to propagate the relatedness among images. The vertices of the graph are the L + N images and the undirected edges are weighted with pair-wise similarity. We use W to denote the similarity matrix and W ij , its (i, j)-th element, indicates the similarity between x i and xj. Typically, it is estimated as</p><formula xml:id="formula_10">W ij = K(xi, xj) if xj ∈ NK (xi) or xi ∈ NK (xj) 0 otherwise<label>(8)</label></formula><p>where N K (xi) denotes the index set for the K nearest neighbours of image x i computed by Euclidean distance. Noting that W ii is set as 1, so that self-loop is included.</p><p>Denoting A as the one step transition matrix. Its element A iu indicates the probability of the transition from node i to node u and is computed directly from the related weights,</p><formula xml:id="formula_11">A iu = Wiu j Wij (9)</formula><p>The L1 normalization of each row turns A into a stochastic transition matrix. Then the probability of a random walk, which initially starts from node i, and stops at node u after t steps, can be denoted as</p><formula xml:id="formula_12">P (x u(t)|xi(0)) = [A t ]iu<label>(10)</label></formula><p>Based on Eq.( <ref type="formula" target="#formula_12">10</ref>), we can simply evaluate the probability that the walker starts from x i at time 0 given that it ends at x u at time t, with the reasonable assumption that the starting node is uniformly chosen,</p><formula xml:id="formula_13">P (xi(0)|xu(t)) = P (xu(t)|xi(0)) × P (xi(0)) P (xu(t))<label>(11)</label></formula><p>= P (xu(t)|xi(0))</p><formula xml:id="formula_14">j P (xu(t)|xj(0)) = [A t ]iu j [A t</formula><p>]ju Since visual concepts and complex query are associated with the starting and ending images, respectively, Eq.( <ref type="formula" target="#formula_2">2</ref>) can be rewritten as</p><formula xml:id="formula_15">P (q c|xu) = L i=1 P (qc|xi) × P (xi(0)|xu(t))<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cross-Modality Relatedness Estimation</head><p>As mentioned above, P (qc|xi) in Eq.( <ref type="formula" target="#formula_2">2</ref>) measures the relatedness between two different modalities, the high-level concept and the low-level visual information. We now present two techniques to link these two modalities: kernel density estimation approach (KDE) <ref type="bibr" target="#b17">[19]</ref> and normalizing relatedness cross concepts (NRCC) <ref type="bibr" target="#b27">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">KDE Approach</head><p>For each image xu retrieved by Q, P (qc) is identical and P (x i) is assumed to be uniform. Therefore Eq.( <ref type="formula" target="#formula_15">12</ref>) can be restated as,</p><formula xml:id="formula_16">P (q c|xu) ∝ L i=1 P (xi|qc) × P (xi(0)|xu(t))<label>(13)</label></formula><p>where P (x i|qc) is the probability density function, representing the relevance of an image to the given visual concept. KDE approach is utilized to perform the estimation. We use X c to denote the set of images retrieved by the visual concept q c, the KDE approach measures P (xi|qc) as</p><formula xml:id="formula_17">P (xi|qc) = 1 |Xc| x j ∈Xc K(xi, xj)<label>(14)</label></formula><p>The above equation can be intuitively interpreted as follows: q c and each of its retrieved images in Xc can respectively be viewed as a family and family members. Then the closeness of an unknown image to this family is estimated by averaging the soft voting from all family members.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">NRCC Approach</head><p>The drawback of the KDE approach is that it does not take the underlying associations among visual concepts belonging to the same complex query into consideration. To compensate for this limitation, we formally define P (q c|xi) as,</p><formula xml:id="formula_18">P (q c|xi) = 1 Zi x j ∈Xc P (xj(0)|xi(t))<label>(15)</label></formula><p>where Z u is a normalizing factor, and formulated as,</p><formula xml:id="formula_19">Zi = qc∈C x j ∈Xc P (xj(0)|xi(t))<label>(16)</label></formula><p>As its formulation implies, this approach is named as normalizing relatedness cross concepts (NRCC), which has been preliminarily studied in <ref type="bibr" target="#b27">[29]</ref>.</p><p>Compared to the KDE approach, by regarding C as a community with several families, the relatedness between the given image x i and a family qc, is determined not only by the family members x j in qc, but also other community families in C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Discussions</head><p>To further study the impact of the number of transitions t on visual relatedness, we first define the stationary probability vector π of the stochastic transition matrix A that does not change under the power of A. Mathematically, it is expressed as, πA = π <ref type="bibr" target="#b15">(17)</ref> The Perron-Frobenius theorem <ref type="bibr" target="#b25">[27]</ref> ensures every stochastic matrix has such vectors; and for a matrix with strictly positive entries, this vector is unique. It can be computed by observing that for any i,</p><formula xml:id="formula_20">lim t→∞ [A t ]iu = πu (<label>18</label></formula><formula xml:id="formula_21">)</formula><p>where πu is the u-th element of the row vector π. It implies that when t → ∞, the probability of being in a state u is independent of the initial state i. Namely, all the starting points become indistinguishable. In the other limiting case, when t = 1, we utilize only the neighbourhood graph, which will be totally influenced by K.</p><p>The local neighbourhood size K should be large enough to guarantee a singly connected graph. Meanwhile, K should be sufficiently small to avoid introducing more edges between the relevant and irrelevant samples, which may degrade the reranking performance drastically. However, too small a K will miss the "correct" edges between the relevant samples, resulting in the weakening of the key consistency.</p><p>The computational complexity of our approach mainly comes from two parts: feature extraction and transition matrix iteration. The former is the most computationally expensive step, but can be handled off-line. The cost of the latter scales as O(d(L + N ) 2 + t(L + N ) 2 ), where d is the 1428-dimension features, and t is the number of transitions in dozens level. Since we only use the top results, L + N is usually in the order of thousands. Thus the computational cost is very low. In our experiments, the process can be completed in less than 1 second if we do not take the feature extraction part into account (3.4GHz and 8G memory).</p><p>It is worthy emphasizing that our proposed scheme can also be applied for other image repositories, even for those images without surrounding texts by simply ignoring the last term in Eq.( <ref type="formula">7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head><p>We collected a large real-world dataset from WikiAnswers, which contains 1, 944, 492 unique QA pairs and covers a wide range of topics, including entertainment, life, education, etc. Based on this dataset, we constructed a visual word vocabulary, from which 100 most frequent visual words are selected. We then issued these terms into Google Image and selected 50 suggested complex queries according to our definition. Some representative samples are listed in Table <ref type="table" target="#tab_2">1</ref>. For each complex query and its embedded visual concepts, the top 500 images are crawled from Google Image. Baby with an apple lying in the bed 5</p><p>A lady driving a red car on the street 6</p><p>A cowboy riding a horse at sundown 7</p><p>Lions attacking zebras on the grassland 8</p><p>A man walking his dog in the park 9</p><p>A lady wears sunglasses on the sea beach 10</p><p>Comparison between white iphone and black iphone</p><p>To obtain the relevance ground truth of each image, we conduct a manual labelling procedure. Five human annotators were involved in the process. Each image was labelled to be very relevant (score 2), relevant (score 1) or irrelevant (score 0) with respect to the given query. We performed a voting to establish the final relevance level of each image. For the cases that there were two classes having the same number of ballots, a discussion was carried out among the labelers to decide the final ground truths.</p><p>To represent the content of each image, we extracted the following features:</p><p>1. We used the difference of Gaussians to detect keypoints in each image and extracted their SIFT descriptors. By building a visual codebook of size 1000 based on K-means, we obtained a 1000-dimensional bag-ofvisual-words histogram for each image.</p><p>2. We further extracted 428-dimensional global visual features, including 225-dimensional block-wise color moments based on 5-by-5 fixed partition of the image; 128-dimensional wavelet texture; and 75-D edge direction histogram. When it comes to reranking performance evaluation, we adopted NDCG@n as our metric,</p><formula xml:id="formula_22">NDCG@n = rel 1 + n i=2 rel i log 2 i IDCG (<label>19</label></formula><formula xml:id="formula_23">)</formula><p>where reli is the relevance score of the i-th image in the ranked list, IDCG is the normalizing factor that makes NDCG@n being 1 for a perfect ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">On Visual Concept Detection</head><p>Following our heuristic rules stated in Section 3, we first selected all the noun terms from WikiAnswers dataset the by Stanford Log-linear Part-Of-Speech Tagger. We filtered out the stop words from the noun set. We then went through the WordNet 3.0<ref type="foot" target="#foot_3">6</ref> hypernym hierarch within 10 steps, from bottom to top, to identify each selected word's hypernyms, until one of the five predefined high-level categories are matched: "color", "thing", "artifact", "organism", and "natural phenomenon". As shown in <ref type="bibr" target="#b16">[18]</ref>, these 5 categories cover a substantial part of many frequently used concepts in computer vision and multimedia domains. In this way, we constructed a visual word vocabulary containing 12, 812 noun entries. Table <ref type="table" target="#tab_3">2</ref> illustrates their distribution statistics over the 5 categories in our vocabulary.  From the selected 50 complex queries, 205 chucks are detected by OpenNLP, among which 119 chucks are manually voted as visual concepts by 5 volunteers. As mentioned previously, for each term in a given chuck, we search it in our constructed visual vocabulary, and this chuck will be categorized as a visual concept if at least one term is matched. Table <ref type="table" target="#tab_4">3</ref> illustrates the confusion matrix obtained by our proposed visual concept detection. We can see that our approach achieves fairly good performance, i.e., 89.27%. The misclassification results mainly come from some chunks that are product names not archived in the WordNet, such as "iphone", "ipad", etc, and also from some verb chunks that have visual content descriptive attribute, such as "wear", etc. In our further work, we will broaden our visual word dictionary by incorporating product name list to boost our classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">On Query Performance Analysis</head><p>We first conducted experiment to evaluate the retrieval effectiveness of the current dominant image search engines for simple and complex queries, respectively.</p><p>The selected 50 queries and their 119 involved visual concepts are regarded as complex queries and simple queries, respectively. Figure <ref type="figure">4</ref> displays the average search performance comparison. From the figure we can see that the search results of simple queries remarkably outperform those based on complex queries. And along with the increase of NDCG-depth n, the average performance of complex queries drops at a faster rate. This observation partially verifies our hypothesis that: compared to complex queries, the search results of simple queries are more visually consistent and coherent. Also, it reveals the fact that the current search engines, especially the image search engine, do not perform well for complex queries, even though great success has been achieved for simple queries. So effective image reranking for complex queries is highly desirable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">On Reranking Performance Comparison</head><p>To demonstrate the effectiveness of our proposed approach, we comparatively evaluate the following unsupervised reranking methods:</p><p>• RW: Random walk reranking <ref type="bibr" target="#b10">[12]</ref> is a typical graphbased reranking method jointly exploiting both initial ranking result and visual similarity between images. The stationary probability of random walk is used to compute the final relevance scores. (Baseline 1)</p><p>• PRF: Pseudo-Relevance Feedback <ref type="bibr" target="#b32">[34]</ref>. A support vector machine (SVM) classifier is trained to perform the reranking based on the assumption that the topranked images for each query are more relevant than the low-ranked results in general. (Baseline 2)</p><p>• Proposed KDE: Our proposed probabilistic reranking approach with cross-modalities relatedness estimation by KDE method.</p><p>• Proposed NRCC: Our proposed probabilistic reranking approach with cross-modalities relatedness estimation by NRCC method.</p><p>For each method mentioned above, the involved parameters are carefully tuned, and the parameters with the best performances are used to report the final comparison results. The experimental results are illustrated in Figure <ref type="figure">5</ref>. It can be observed that our proposed approaches are consistently and substantially better than the current publicly disclosed state-of-the-art web image reranking algorithms across all evaluated NDCGs. From this figure, we can also observe that the improvements over the initial ranking result from RW and PRF are much slighter, especially for NDCGs with smaller n. The main reason is that they both have problems of unreliable initial ranking list, which frequently exists in complex query search. In contrast, our proposed scheme for complex queries is more robust, since it tends not to be affected too much by the initial ordering of images.</p><p>Further, it is observed that the proposed NRCC stably outperforms the proposed KDE approach. This is due to the fact that NRCC takes the relationship between visual concepts in the same complex query into consideration, while   KDE assumes that these visual concepts are independent, when estimating the relatedness between a visual concept and a given image. Figure <ref type="figure" target="#fig_6">6</ref> illustrates the top 10 images before and after reranking for different reranking approaches for the complex query "soldiers holding American flag on the mountain". Obviously, our proposed approaches obtain the most satisfying results.</p><p>However, after examining the performance of each query, it is observed that our scheme fails to handle some queries that have explicit spatial or action relationship constraints between visual concepts. Examples include "a butterfly on the left top of the flower" and "a man listening to a mobile phone". We will integrate this kind information into our scheme for general complex queries in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">On the Sensitivity of Parameters</head><p>As discussed above, both the number of transitions t and local neighbour size K are important parameters in our method. In this section, we further conduct experiments to investigate the effect of these parameters based on the proposed NRCC. We first perform grid search with step size 1, to seek the t and K with optimal reranking performance. 28 and 304 are located for t and K, respectively.</p><p>The NDCG@50-t curve is presented in Figure <ref type="figure" target="#fig_7">7</ref> with K fixed as 304. As illustrated, the performance increases with t growing and arrives at a peak at a certain t, then the performance sharply decreases, and finally becomes relatively constant. This result is consistent with our previous analysis that when t tends towards infinite, all the starting points become indistinguishable. Similarly, Figure <ref type="figure" target="#fig_8">8</ref> shows the NDCG@50-K curve with t fixed as 28, where the performance varies according to different K. With the gradual increase of K, more relevant samples are connected to each other, and "incorrect" edges between the relevant samples and irrelevant samples are potentially introduced. From Figure <ref type="figure" target="#fig_8">8</ref>, it can be observed that NDCG@50 obtains the peak performance at K = 304, which is a trade-off value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">APPLICATIONS</head><p>In this section, we introduce two potential application scenarios of image reranking for complex queries: photo-based question answering and textual news visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Photo-based Question Answering</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Application Scenario</head><p>Community question answering (cQA) services have gained great popularity over the past decades <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b7">9]</ref>, which encourage askers to post their specific questions on any topic and obtain answers provided by other participants. It also facilitates general users to seek information from the large repository of well-answered questions. However, existing cQA forums, such as Yahoo!Answers, Answerbag, MetaFilter, usually support only textual answers, which are not in- Even when the answer is described by several very long sentences in Yahoo!Answers, it is still hard for users to grasp the appearance differences. Here it reflects the fact that a picture is worth a thousand words. However, noting that not all the QA pairs prefer image answers. Textual answer is sufficient when it comes to the quantity-type questions, such as "what is the population in China". Also video answers will be much more lively and interesting for procedure-oriented questions, such as "how to assemble a computer". Actually this is the so-called multimedia question answering <ref type="bibr" target="#b7">[9]</ref>, a rising topic in media search domain.</p><p>In this paper, we only focus on the QA pairs which may be better explained with images. However, as stated in <ref type="bibr" target="#b22">[24]</ref>, the queries generated from the textual QA pairs are usually very verbose and complex, not supported well by the current commercial image search engines. Based on our proposed approach, we develop a photo-based QA system, which automatically complements the original textual answers with relevant web images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Experiments</head><p>To demonstrate the effectiveness of the PQA system, we conducted the experiment on 1000 non-conversational QA pairs, selected from Yahoo!Answers dataset <ref type="bibr" target="#b19">[21]</ref>, which contains 4, 483, 032 QA pairs. For each QA pair, five volunteers were invited to vote whether it can provide users with better experience by adding images instead of using purely texture descriptions. Around 260 QA pairs were selected. We then directly employed the method in <ref type="bibr" target="#b22">[24]</ref> to generate a most informative query from each QA pair. Our statistics are shown in Table <ref type="table" target="#tab_5">4</ref>, which show that more than 53% of queries contain two or more visual concepts.</p><p>Accordingly, a query-aware reranking approach is proposed to select the top 10 relevant images. To be specific, if the query is simple, i.e., containing only one visual concept, then the RW <ref type="bibr" target="#b10">[12]</ref> will be used directly. On the other hand, if the query is complex, we employ the proposed NRCC. We compare our proposed approach with the following methods.</p><p>• Naive Search: Simply perform image search with each query on Google Image without reranking. • Naive Fusion: Simply perform image search with each visual concept in the generated complex query, and then fuse the results. Figure <ref type="figure">9</ref> shows the comparison of these three methods. It can be observed that our query-aware reranking approach outperforms the other two methods remarkably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Textual News Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Application Scenario</head><p>"Every picture tells a story" suggests to us the essence of visual communication via pictures. This phrase is also consistent with our common sense, i.e., pictures in textual news always facilitate and expedite our understanding, especially for elderly and juvenile. Meanwhile, searching the image database in order to provide several meaningful and illustrative pictures to their textual news is a routine task for news writers. However, the pictures contained in news documents are usually very few as shown in Table <ref type="table" target="#tab_6">5</ref>. which shows that more than 46% news documents do not contain any pictures. The statistical result is based on the experimental dataset. To assist news readers and news writers, we propose a scheme to automatically seek relevant web images that best contextualize the content of news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Experiments</head><p>We directly used the news dataset in <ref type="bibr" target="#b15">[17]</ref>, crawled from ABCNews.com, BBS.co.uk, CNN.com and GoogleNews; it contains up to 48, 429 unique documents after duplicate removal. To save manual labelling efforts, we randomly select 100 news documents from the whole data set for evaluation. It is observed that most of the news articles are fairly long, and it is not an easy task to extract descriptive queries. So we simply regard the expert generated titles of the news documents as complex queries due to their obvious summarizing attribute.</p><p>Further, it is observed that more than 43% of titles contain at least one person-related visual concept. So we propose to employ query dependent image representations for reranking. Specifically, let X c and X the set of images retrieved by the visual concept q c and complex query Q, respectively; and q c is predicted as person related query by the method in <ref type="bibr" target="#b9">[11]</ref>. Then for each image in X c and X , we performed face detection. We extracted the 256-dimensional Local Binary Pattern (LBP) features <ref type="bibr" target="#b26">[28]</ref> from the largest face region for any x i in Xc; and the same features are extracted for all the detected faces for any x u in X . The similarity between xi and xu is then computed as,</p><formula xml:id="formula_24">W iu = max x∈Ou K(xi, x)<label>(20)</label></formula><p>where Ou is the set of LBP features extracted from the faces in image x u. Other image pair similarity is the same as previously introduced. We call this the query-aware presentation method.</p><p>To demonstrate the effectiveness of our proposed queryaware image presentation method, we compare it with the query independent unified image presentation method as described earlier, i.e., all the images are presented by the combination of bag-of-visual-words and global features. The result is presented in Figure <ref type="figure" target="#fig_0">10</ref>, which shows that our queryaware image presentation is better than query-independent image presentation approach, even though both of them are based on our same reranking principles. The inial ranking performance reflects lower search performance. This is because the news titles generally contain some redundant terms, which overwhelm the key concepts and potentially confuse the search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we proposed a scheme to rerank web images for complex queries, which is robust to the unreliable initial ranking list. For a given complex query, it first detects the noun-phrase based visual concepts and collects their relevant images simultaneously. It then constructs a heterogeneous probabilistic network to estimate the image relevance score, which consists of three mutual reinforced sub-networks. These sub-networks represent different relationship layers, spanning from semantic level to visual level, which are established among the complex query and its detected visual concepts, by harnessing the content of images and their associated textual information. Based on these relevance scores, a new ranking list is generated. The experimental results showed that our scheme is significantly better than the other existing state-of-the-art approaches. We also introduced two application scenarios, which can benefit from our scheme, namely photo-based question answering and textual news visualization. A limitation of current work is that it ignores the relationships explicitly described by the complex query, which have no uniform patterns and are notoriously hard to model. We will integrate this kind information into our scheme for general complex queries in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Image retrieval results comparison. The search results of a complex query are less visually consistent than those retrieved by its constituent visual concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Photo-based Question AnsweringTextual News Visualization Other Potential Applications</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the proposed web image reranking scheme for complex queries. It contains two components, i.e., visual concept detection and relevance estimation. This scheme facilitates many applications, including photo-based question answering, textual news visualization and others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An illustration of visual concepts detection from a given complex query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Retrieval performance comparison between complex queries and their belonging primitive visual concepts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Illustrative results for complex query "soldiers holding American flag on the mountain" based on different reranking approaches. Our proposed approaches obtain the most satisfying results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The performance with different t when K is fixed as 304.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The performance with different K when t is fixed as 28.constant. This result is consistent with our previous analysis that when t tends towards infinite, all the starting points become indistinguishable.Similarly, Figure8shows the NDCG@50-K curve with t fixed as 28, where the performance varies according to different K. With the gradual increase of K, more relevant samples are connected to each other, and "incorrect" edges between the relevant samples and irrelevant samples are potentially introduced. From Figure8, it can be observed that NDCG@50 obtains the peak performance at K = 304, which is a trade-off value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure 9: The average performance comparison of Photo-based QA System.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 : The representative complex queries gener- ated based on our corpus and Google Image sugges- tion. Here we do not illustrate all the queries due to limited space.</head><label>1</label><figDesc></figDesc><table><row><cell>ID</cell><cell>Complex Query</cell></row><row><cell>1</cell><cell>President Obama and troops</cell></row><row><cell>2</cell><cell>Women swimming in pool</cell></row><row><cell>3</cell><cell>Soldiers holding American flag on the mountain</cell></row><row><cell>4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 : The distribution of visual words over five predefined high-level categories.</head><label>2</label><figDesc></figDesc><table><row><cell>Visual Category</cell><cell cols="2">Visual Words # Percentage</cell></row><row><cell>Color</cell><cell>159</cell><cell>1.24%</cell></row><row><cell>Thing</cell><cell>919</cell><cell>7.17%</cell></row><row><cell>Artifact</cell><cell>4219</cell><cell>32.93%</cell></row><row><cell>Organism</cell><cell>7214</cell><cell>56.31%</cell></row><row><cell>Natural phenomenon</cell><cell>301</cell><cell>2.35%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : The confusion matrix of visual concept de- tection results. The prediction accuracy is</head><label>3</label><figDesc>89.27%.</figDesc><table><row><cell>`````````P</cell><cell>rediction</cell><cell cols="2">Class Visual Concepts</cell><cell>Non Visual Concepts</cell></row><row><cell cols="2">Visual Concepts</cell><cell></cell><cell>102</cell><cell>5</cell></row><row><cell cols="3">Non Visual Concepts</cell><cell>17</cell><cell>81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 : The distribution of visual concepts embed- ded in the generated queries for photo-based QA.</head><label>4</label><figDesc></figDesc><table><row><cell>One</cell><cell>Two</cell><cell>More Than Two</cell></row><row><cell>Visual Concepts</cell><cell>Visual Concepts</cell><cell>Visual Concepts</cell></row><row><cell>46.15%</cell><cell>38.85%</cell><cell>15.0%</cell></row><row><cell cols="3">tuitive for many questions, such as the question "what is the</cell></row><row><cell cols="3">difference between alligators and crocodiles".</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : The distribution of the number of pictures involved in news documents.</head><label>5</label><figDesc></figDesc><table><row><cell>Without Any Picture</cell><cell cols="2">One Pictures Two Pictures</cell><cell>&gt; Two Pictures</cell></row><row><cell>46.15%</cell><cell>38.85%</cell><cell>15.0%</cell><cell>38.85%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A study in<ref type="bibr" target="#b26">[28]</ref> shows that a failed image query tends to be longer than the average successful query, which indicates longer queries' higher specificity of contents and also reveals the limitations of current web image search engines for complex queries.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The underlying visual associations among visual concepts are also integrated.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The semantic associations among visual concepts are also considered in this layer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>http://wordnet.princeton.edu/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">ACKNOWLEDGEMENTS</head><p>This work was supported by NUS-Tsinghua Extreme Search project under the grant number: R-252-300-001-490.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Building detectors to support searches on combined semantic concepts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hiemstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ordelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIR Workshop</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Query expansion using term relationships in language models for information retrieval</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bruza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploring reductions for long web queries</title>
		<author>
			<persName><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discovering key concepts in verbose queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning concept importance using a weighted dependence model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parameterized concept weighting in verbose queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Known-item search by nus</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>-J. Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIST TRECVID</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">From text question-answering to multimedia qa on web-scale media resources</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<editor>LS-MMRM</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The google similarity distance</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Cilibrasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M B</forename><surname>Vitanyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Assisted news reading with automated illustration</title>
		<author>
			<persName><forename type="first">D</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Correia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Video search reranking through random walk over document-level context graph</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In MM</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A case for shorter queries, and helping users create them</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Effective and efficient user interaction for long queries</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regression rank: Learning to meet the opportunity of descriptive queries</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lease</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harvesting social images for bi-concept search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Worring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">News contextualization with geographic and visual information</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image retagging</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Tag ranking</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to video search rerank via pseudo preference feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to rank answers on large online qa collections</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mihai Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robust visual reranking via sparsity and ranking constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Morioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning the semantics of multimedia queries and concepts from a small number of examples</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Teši</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ć</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multimedia answering: enriching text qa with media information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>-J. Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Oracle in image search: A content-based approach to performance prediction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Query term ranking based on dependency parsing of verbose queries</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Perron-Frobenius theorem: some of its applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An analysis of failed queries for web image retrieval</title>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Science</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Partially labeled classification with markov random walks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bayesian video search reranking</title>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">80 million tiny images: A large data set for nonparametric object and scene recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards a relevant and diverse search of social images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image search by concept map</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimedia search with pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVR</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Utilizing related samples to enhance interactive concept-based video search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>TMM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning concept bundles for video search with complex queries</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
