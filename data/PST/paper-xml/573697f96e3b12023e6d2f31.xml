<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EESEN: END-TO-END SPEECH RECOGNITION USING DEEP RNN MODELS AND WFST-BASED DECODING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Gowayyed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">EESEN: END-TO-END SPEECH RECOGNITION USING DEEP RNN MODELS AND WFST-BASED DECODING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recurrent neural network</term>
					<term>connectionist temporal classification</term>
					<term>end-to-end ASR</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting contextindependent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic speech recognition (ASR) has traditionally leveraged the hidden Markov model/Gaussian mixture model (HMM/GMM) paradigm for acoustic modeling. HMMs act to normalize the temporal variability, whereas GMMs compute the emission probabilities of HMM states. In recent years, the performance of ASR has been improved dramatically by the introduction of deep neural networks (DNNs) as acoustic models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>. In the hybrid HMM/DNN approach, DNNs are used to classify speech frames into clustered context-dependent (CD) states (i.e., senones). On a variety of ASR tasks, DNN models have shown significant gains over the GMM models. Despite these advances, building a state-of-the-art ASR system remains a complicated, expertise-intensive task. First, acoustic modeling typically requires various resources such as dictionaries and phonetic questions. Under certain conditions (e.g., in low-resource lan-guages), these resources may be unavailable, which restricts or delays the deployment of ASR. Second, in the hybrid approach, training of DNNs still relies on GMM models to obtain (initial) frame-level labels. Building GMM models normally goes through multiple stages (e.g., CI phone, CD states, etc.), and every stage involves different feature processing techniques (e.g., LDA, fMLLR, etc.). Third, the development of ASR systems highly relies on ASR experts to determine the optimal configurations of a multitude of hyper-parameters, for instance, the number of senones and Gaussians in the GMM models.</p><p>Previous work has made various attempts to reduce the complexity of ASR. In <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref>, researchers propose to flatstart DNNs and thus get ride of GMM models. However, this GMM-free approach still requires iterative procedures such as generating forced alignments and decision trees. Meanwhile, another line of work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b12">12]</ref> has focused on end-to-end ASR, i.e., modeling the mapping between speech and labels (words, phonemes, etc.) directly without any intermediate components (e.g., GMMs). On this aspect, Graves et al. <ref type="bibr" target="#b13">[13]</ref> introduce the connectionist temporal classification (CTC) objective function to infer speech-label alignments automatically. This CTC technique is further investigated in <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b14">14]</ref> on large-scale acoustic modeling tasks. Although showing promising results, research on end-to-end ASR faces two major obstacles. First, it is challenging to incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b10">10]</ref> has successfully constrained search paths with lexicons. However, how to integrate word-level language models efficiently still is an unanswered question <ref type="bibr" target="#b10">[10]</ref>. Second, the community lacks a shared experimental platform for the purpose of benchmarking. End-to-end systems described in the literature differ not only in their model architectures but also in their decoding methods. For example, <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b8">[8]</ref> adopt two distinct versions of beam search for decoding CTC models. These setup variations hamper rigorous comparisons not only across endto-end systems, but also between the end-to-end and existing hybrid approaches.</p><p>In this paper, we resolve these issues by presenting and publicly releasing our Eesen framework. Acoustic modeling in Eesen is viewed as a sequence-to-sequence learning problem. We exploit deep recurrent neural networks (RNNs) <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16]</ref> as the acoustic models, and the Long Short-Term Memory (LSTM) units <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b21">20]</ref> as the RNN building blocks. Using the CTC objective function, Eesen simplifies acoustic modeling into learning a single RNN over pairs of speech and context-independent (CI) label sequences. A distinctive feature of Eesen is a generalized decoding method based on weighted finite-state transducers (WFSTs). In this method, individual components (CTC labels, lexicons and language models) are encoded into WFSTs, and then composed into a comprehensive search graph. The WFST representation provides a convenient way of handling the CTC blank label and enabling beam search during decoding. Our experiments with the Wall Street Journal (WSJ) benchmark show that Eesen results in superior performance than the existing end-to-end ASR pipelines <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>. The WERs of Eesen are on a par with strong hybrid HMM/DNN baselines. Moreover, the application of CI modeling targets allows Eesen to speed up decoding and reduce decoding memory usage. Eesen is released as an open-source project<ref type="foot" target="#foot_0">1</ref> , and will undertake continuous expansion and optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE EESEN FRAMEWORK: MODEL TRAINING</head><p>Acoustic models in Eesen are deep bidirectional RNNs trained with the CTC objective function <ref type="bibr" target="#b13">[13]</ref>. We describe the model structure in Section 2.1, and restate key points of CTC training in Section 2.2. Section 2.3 presents some practical considerations emerging from our GPU implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Deep Bidirectional Recurrent Neural Networks</head><p>Compared to the standard feedforward networks, RNNs have the advantage of learning complex temporal dynamics on sequences. Given an input sequence X = (x 1 , ..., x T ), a recurrent layer computes the forward sequence of hidden states</p><formula xml:id="formula_0">− → H = ( − → h 1 , ..., − → h T ) by iterating from t = 1 to T : − → h t = σ( − → W hx x t + − → W hh − → h t−1 + − → b h )<label>(1)</label></formula><p>where − → W hx is the input-to-hidden weight matrix, − → W hh is the hidden-to-hidden weight matrix. In addition to the inputs x t , the hidden activation h t−1 from the previous time step are fed to influence the hidden outputs at the current time step. In a bidirectional RNN, an additional recurrent layer computes the backward sequence of hidden outputs ← − H from t = T to 1:</p><formula xml:id="formula_1">← − h t = σ( ← − W hx x t + ← − W hh ← − h t−1 + ← − b h )<label>(2)</label></formula><p>Our acoustic model is a deep architecture, in which we stack multiple bidirectional recurrent layers. At each frame t, the concatenation of the forward and backward hidden outputs</p><formula xml:id="formula_2">[ − → h t , ← − h t ]</formula><p>from the current layer are treated as inputs into the next recurrent layer. Learning of RNNs can be done using back-propagation through time (BPTT). In practice, training RNNs to learn long-term temporal dependency can be difficult due to the vanishing gradients problem <ref type="bibr" target="#b22">[21]</ref>. To overcome this issue, we apply the LSTM units <ref type="bibr" target="#b18">[17]</ref> as the building blocks of RNNs. LSTM contains memory cells with self-connections to store the temporal states of the network. Also, multiplicative gates are added to control the flow of information. Fig. <ref type="figure" target="#fig_0">1</ref> depicts the structure of the LSTM units we use. The blue curves represent peephole connections <ref type="bibr" target="#b23">[22]</ref> that link the memory cells to the gates to learn precise timing of the outputs. The computation at the time step t can be formally written as follows. We omit the → arrow for uncluttered formulation.</p><formula xml:id="formula_3">i t = σ(W ix x t + W ih h t−1 + W ic c t−1 + b i ) (3a) f t = σ(W f x x t + W f h h t−1 + W f c c t−1 + b f ) (3b) c t = f t c t−1 + i t φ(W cx x t + W ch h t−1 + b c ) (3c) o t = σ(W ox x t + W oh h t−1 + W oc c t + b o ) (3d) h t = o t φ(c t ) (3e)</formula><p>where i t , o t , f t , c t are the outputs of the input gate, output gate, forget gate and memory cells respectively. The W .x weight matrices connect the inputs with the units, whereas the W .h matrices connect the previous memory cell states with the units. The W .c terms are diagonal weight matrices for peephole connections. Also, σ is the logistic sigmoid nonlinearity, and φ is the hyperbolic tangent nonlinearity. The computation of the backward LSTM layer can be represented similarly. In this work, we use a purely LSTM-based architecture as the acoustic model. However, combing LSTMs with other network structures, e.g., time-delay <ref type="bibr" target="#b24">[23,</ref><ref type="bibr" target="#b25">24]</ref> or convolutional neural networks <ref type="bibr" target="#b26">[25,</ref><ref type="bibr" target="#b20">19]</ref>, is straightforward to achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training with Connectionist Temporal Classification</head><p>Unlike in the hybrid approach, the RNN model in our Eesen framework is not trained using frame-level labels with respect to the cross-entropy (CE) criterion. Instead, following <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b10">10]</ref>, we adopt the CTC objective <ref type="bibr" target="#b13">[13]</ref> to automatically learn the alignments between speech frames and their label sequences (e.g., phonemes or characters). Assume that the label sequences in the training data contain K unique labels. Normally K is a relatively small number, e.g., around tional blank label ∅, which means no labels being emitted, is added to the labels. For simplicity of formulation, we denote every label using its index in the label set. Given an utterance X = (x 1 , ..., x T ), its label sequence is denoted as z = (z 1 , ..., z U ). In our implementation, we always index the blank as 0. Therefore z u is an integer ranging from 1 to K. The length of z is constrained to be no greater than the length of the utterance, i.e., U ≤ T . CTC aims to maximize ln P r(z|X), the log-likelihood of the label sequence given the inputs, by optimizing the RNN model parameters.</p><p>The final layer of the RNN is a softmax layer which has K +1 nodes that correspond to the K +1 labels (including ∅). At each frame t, we get the output vector y t whose k-th element y k t is the posterior probability of the label k. However, since the labels z are not aligned to the frames, it is difficult to evaluate the likelihood of z given the RNN outputs. To bridge the RNN outputs with label sequences, an intermediate representation, the CTC path, is introduced in <ref type="bibr" target="#b13">[13]</ref>. A CTC path p = (p 1 , ..., p T ) is a sequence of labels at the frame level. It differs from z in that the CTC path allows occurrences of the blank label and repetitions of non-blank labels. The total probability of the CTC path is decomposed into the probability of the label p t at each frame:</p><formula xml:id="formula_4">P r(p|X) = T t=1 y pt t (4)</formula><p>The label sequence z can then be mapped to its corresponding CTC paths. This is a one-to-multiple mapping because multiple CTC paths can correspond to the same label sequence. For example, both "A A ∅ ∅ B C ∅" and "∅ A A B ∅ C C" are mapped to the label sequence "A B C". We denote the set of CTC paths for z as Φ(z). Then, the likelihood of z can be evaluated as a sum of the probabilities of its CTC paths:</p><formula xml:id="formula_5">P r(z|X) = p∈Φ(z) P r(p|X)<label>(5)</label></formula><p>However, summing over all the CTC paths is computationally intractable. A solution is to represent the possible CTC paths compactly as a trellis. To allow blanks in CTC paths, we add "0" (the index of ∅) to the beginning and the end of z, and also insert "0" between every pair of the original labels in z. The resulting augmented label sequence l = (l 1 , ..., l 2U +1 ) is leveraged in a forward-backward algorithm for efficient likelihood evaluation. Specifically, in a forward pass, the variable α u t represents the total probability of all CTC paths that end with label l u at frame t. As with the case of HMMs <ref type="bibr" target="#b27">[26]</ref>, α u t can be recursively computed from α u t−1 and α u−1 t−1 . Similarly, a backward variable β u t carries the total probability of all CTC paths that starts with label l u at t and reaches the final frame T . The likelihood of the label sequence z can then be computed as:</p><formula xml:id="formula_6">P r(z|X) = 2U +1 u=1 α u t β u t (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where t can be any frame 1 ≤ t ≤ T . The objective ln P r(z|X) now becomes differentiable with respect to the RNN outputs y t . We define an operation on the augmented label sequence Υ(l, k) = {u|l u = k} that returns the elements of l which have the value k. The derivative of the objective with respect to y k t can be derived as:</p><formula xml:id="formula_8">∂ ln P r(z|X) ∂y k t = 1 P r(z|X) 1 y k t u∈Υ(l,k) α u t β u t<label>(7)</label></formula><p>These errors are back-propagated through the softmax layer and further into the RNN to update the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">GPU Implementation</head><p>We implement the training of the RNN models on GPU devices. To fully exploit the capacity of GPUs, multiple utterances are processed at a time in parallel. This parallel processing speeds up model training by replacing matrix-vector multiplication over single frames with matrix-matrix multiplication over multiple frames. Within a group of parallel utterances, we pad every utterance to the length of the longest utterance in the group. These padding frames are excluded from gradients computation and parameter updating. For further acceleration, the training utterances are sorted by their lengths, from the shortest to the longest. The utterances in the same group then have approximately the same length, which minimizes the number of padding frames. To ensure training stability, the gradients of RNN parameters are clipped to the range of <ref type="bibr">[-50, 50</ref>]. CTC learning is also expensive because the forward and backward vectors (α t and β t ) have to be computed sequentially, either from t = 1 to T or from t = T to 1. Like in RNNs, our implementation of CTC also processes multiple utterances at the same time. Moreover, at a specific frame t, the elements of α t (and β t ) are independent and thus can be computed in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">THE EESEN FRAMEWORK: DECODING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Decoding with WFSTs</head><p>Previous work has introduced a variety of methods <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b10">10]</ref> to decode CTC-trained models. These methods, however, either fail to integrate word-level language models <ref type="bibr" target="#b10">[10]</ref> or achieve the integration under constrained conditions (e.g., nbest list rescoring in <ref type="bibr" target="#b6">[6]</ref>). In this work, we propose a generalized decoding approach based on WFSTs <ref type="bibr" target="#b28">[27,</ref><ref type="bibr" target="#b29">28]</ref>. A WFST is a finite-state acceptor (FSA) in which each transition has an input symbol, an output symbol and a weight. A path Fig. <ref type="figure">2</ref>. A toy example of the grammar (language model) WFST. The arc weights are the probability of emitting the next word when given the previous word. The node 0 is the start node, and the double-circled node is the end node.</p><p>through the WFST takes a sequence of input symbols and emits a sequence of output symbols. Our decoding method represents the CTC labels, lexicons and language models as separate WFSTs. Using highly-optimized FST libraries such as OpenFST <ref type="bibr" target="#b30">[29]</ref>, we can fuse the WFSTs efficiently into a single search graph. Building of the individual WFSTs is described as follows. Although exemplified in the scenario of English, the same procedures hold for other languages.</p><p>Grammar. A grammar WFST encodes the permissible word sequences in a language/domain. The WFST shown in Fig. <ref type="figure">2</ref> represents a toy language model which permits two sentences "how are you" and "how is it". The WFST symbols are the words, and the arc weights are the language model probabilities. With this WFST representation, CTC decoding in principle can leverage any language models that can be converted into WFSTs. Following conventions in the literature <ref type="bibr" target="#b29">[28]</ref>, the language model WFST is denoted as G.</p><p>Lexicon. A lexicon WFST encodes the mapping from sequences of lexicon units to words. Depending on what labels our RNN has modeled, there are two cases to consider. If the labels are phonemes, the lexicon is a standard dictionary as we normally have in the hybrid approach. When the labels are characters, the lexicon simply contains the spellings of the words. A key difference between these two cases is that the spelling lexicon can be easily expanded to include any out-of-vocabulary (OOV) words. In contrast, expansion of the phoneme lexicon is not so straightforward. It relies on some grapheme-to-phoneme rules/models, and is potentially subject to errors. The lexicon WFST is denoted as L. Fig. <ref type="figure">3</ref> and 4 illustrate these two cases of building L.</p><p>For the spelling lexicon, there is another complication to deal with. With characters as CTC labels, we usually insert an additional space character between every pair of words, in order to model word delimiting in the original transcripts. During decoding, we allow the space character to optionally appear at the beginning and end of a word. This complication can be handled easily by the WFST shown in Fig. <ref type="figure">4</ref>.</p><p>Token. The third WFST component maps a sequence of frame-level CTC labels to a single lexicon unit (phoneme or character). For a lexicon unit, its token WFST is designed to subsume all of its possible label sequences at the frame level. Therefore, this WFST allows occurrences of the blank label ∅, as well as repetitions of any non-blank lables. For exam-Fig. <ref type="figure">3</ref>. The WFST for the phoneme-lexicon entry "is IH Z". The "&lt;eps&gt;" symbol means no inputs are consumed or no outputs are emitted. Fig. <ref type="figure">4</ref>. The WFST for the spelling of the word "is". We allow the word to optionally start and end with the space character "&lt;space&gt;". ple, after processing 5 frames, the RNN model may generate 3 possible label sequences "AAAAA", "∅ ∅ A A ∅", "∅ A A A ∅". The token WFST maps all these 3 sequences into a singleton lexicon unit "A". Fig. <ref type="figure">5</ref> shows the WFST structure for the phoneme "IH". We denote the token WFST as T .</p><p>Search Graph. After compiling the three individual WF-STs, we compose them into a comprehensive search graph. The lexicon and grammar WFSTs are firstly composed. Two special WFST operations, determinization and minimization, are performed over the composition of them, in order to compress the search space and thus speed up decoding. The resulting WFST LG is then composed with the token WFST, which finally generates the search graph. Overall the oder of the FST operations is:</p><formula xml:id="formula_9">S = T • min(det(L • G))<label>(8)</label></formula><p>where •, det and min denote composition, determinization and minimization respectively. The search graph S encodes the mapping from a sequence of CTC labels emitted on speech frames to a sequence of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Posterior Normalization</head><p>When decoding the hybrid DNN models, we need to scale the states posteriors from the DNNs using states priors. The priors are usually estimated from the forced alignments of the training data. During decoding of the CTC-trained models, we adopt a similar procedure. Specifically, we run the final Fig. <ref type="figure">5</ref>. An example of the token WFST which depicts the phoneme "IH". We allow the occurrences of the blank label "&lt;blank&gt;" and the repetitions of the non-blank label "IH".</p><p>RNN model over the training set for a propagation pass. Labels with the largest posteriors are picked as the frame-level alignments, from which priors of the labels are estimated. However, this method does not perform well in our experiments. Part of the reason is that the softmax-layer outputs from a CTC-trained model display a highly peaky distribution <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>. That is, a majority of the frames have the blank as their labels. The activation of the non-blank labels only appears in a narrow region along the time axis. This causes the prior estimates to be dominated by the count of the blank. Alternatively, we propose to estimate more robust label priors from the label sequences in the training data. As mentioned in Section 2.2, the label sequences actually used by CTC training are the augmented label sequences, which insert the blank at the beginning, at the end, and between every label pair in the original label sequences. We compute the priors from the augmented label sequences (e.g., "∅ IH ∅ Z ∅"), instead of the original ones (e.g., "IH Z"), through simple counting. In our experiments, this simple method gives better recognition accuracy than both the aforementioned framealignment method and also the proposal described in <ref type="bibr" target="#b14">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>The experiments are conducted on the Wall Street Journal (WSJ) corpus that can be obtained from LDC under the catalog numbers LDC93S6B and LDC94S13B. Data preparation gives us 81 hours of transcribed speech, from which we select 95% as the training set and the remaining 5% for cross validation. As discussed in Section 2, we apply deep RNNs as the acoustic models. Inputs of the RNNs are 40-dimensional filterbank features together with their first and second-order derivatives. The features are normalized via mean subtraction and variance normalization on the speaker basis.</p><p>Initial values of the model parameters are randomly drawn from a uniform distribution with the range [−0.1, 0.1]. The model is trained with BPTT, in which the errors are backpropagated from CTC. Utterances in the training set are sorted by their lengths, and 10 utterances are processed in parallel at a time. The error rate of the hypothesized labels is monitored to determine learning rates. The hypothesized labels are formed by firstly picking the frame-level labels (the label with the largest probability at every frame), and then removing blanks and label repetitions. The label error rate (LER) can be obtained in the same manner as WER, i.e., computing the edit distance between the hypothesized labels and the reference. We adopt a decaying "newbob" learning rate schedule based on LERs. Specifically, the learning rate starts from 0.00004 and remains unchanged until the drop of LER on the validation set between two consecutive epochs falls below 0.5%. Then the learning rate is decayed by a factor of 0.5 at each of the subsequent epochs. The whole learning process terminates when the LER fails to decrease by 0.1% between two successive epochs.</p><p>Our decoding follows the WFST-based approach in Section 3. After posterior normalization, the acoustic model scores need to be scaled down. The scaling factor lies between 0.5 and 0.9, and its optimal value is decided empirically. We apply the WSJ standard pruned trigram language model in the ARPA format (which we will consistently refer to as standard). To be consistent with previous work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref>, we report our results on the eval92 set. Our experimental setup has been released together with Eesen, which enables the readers to reproduce our numbers easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Phoneme-based Systems</head><p>We explore the optimal RNN configurations on the phonemebased systems. When phonemes are taken as CTC labels, we employ the CMU dictionary<ref type="foot" target="#foot_1">2</ref> as the lexicon. Due to the lack of forced alignments, CTC training cannot handle multiple pronunciations for the same word. For every word, we only keep its first pronunciation in the lexicon and remove all the other alternatives. From the lexicon, we extract 72 labels including phonemes, noise marks and the blank. Our bestperforming model has 4 bi-directional LSTM layers. At each layer, both the forward and the backward sub-layers contain 320 memory cells. Model training ends up to reach the LER (phone error rate in this setting) of 8.8% on the validation set. On the eval92 testing set, the Eesen end-to-end system finally achieves the WER of 7.87%, with both the lexicon and the language model used in decoding. When only the lexicon is used, our decoding behaves similarly as the beam search in <ref type="bibr" target="#b6">[6]</ref>. In this case, the WER rises quickly to 26.92%. This obvious degradation reveals the effectiveness of our decoding approach in integrating language models.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows a comparison between Eesen and a hybrid HMM/DNN system. The hybrid system is constructed by following the standard Kaldi recipe "s5" <ref type="bibr" target="#b29">[28]</ref>. Inputs of the DNN model are 11 neighboring frames of filterbank features. The DNN has 6 hidden layers and 1024 units at each layer. This DNN model contains slightly more parameters (9.2 vs 8.5 million) than the Eesen RNN model. Parameters of the DNN are initialized with restricted Boltzmann machines (RBMs) that are pre-trained in a greedy layerwise fashion <ref type="bibr" target="#b31">[30]</ref>. The DNN is fine-tuned to optimize the CE objective with respect to 3421 senones. For fair evaluations, we decode the DNN model using the original lexicon, rather than the expanded lexicon used by the Kaldi recipe. Also, the language model released From Table <ref type="table" target="#tab_0">1</ref>, we observe that the performance of the Eesen system is still behind the hybrid HMM/DNN system. Our recent developments of Eesen reveal that CTCtrained models can outperform the existing hybrid approach on larger-sized datasets, e.g., Switchboard. Interested readers may refer to the Eesen repository for the updates.</p><p>A major advantage of Eesen compared with the hybrid approach is the decoding speed. The acceleration comes from the drastic reduction of the number of states, i.e., from thousands of senones to tens of phonemes/characters. To verify this, Table <ref type="table" target="#tab_1">2</ref> compares the decoding speed of the Eesen and the hybrid HMM/DNN systems under their best decoding settings. From their real-time factors, we observe that decoding in Eesen is 3.2× faster than that of HMM/DNN. Also, the decoding graph (TLG) in Eesen is significantly smaller than the graph (HCLG) used by HMM/DNN, which saves the disk space for storing the graphs. We apply the same RNN architecture discussed in Section 4.2 to modeling characters. We take the word list from the CMU dictionary as our vocabulary, ignoring the word pronunciations. CTC training deals with 59 labels including letters, digits, punctuation marks, etc. Table <ref type="table">3</ref> shows that with the standard language model, the character-based system gets the WER of 9.07%. CTC experiments in past work <ref type="bibr" target="#b6">[6]</ref> have adopted an expanded vocabulary, and re-trained the language model using text data released together with the WSJ corpus. For fair comparison, we follow the identical configuration. OOV words that occur at least twice in the language model training texts are added to the vocabulary. A new trigram language model is built (and then pruned) with the language model training texts. Under this setup, the WER of the Eesen character-based system is reduced to 7.34%.</p><p>Table <ref type="table">3</ref> lists the results of end-to-end ASR systems that have been reported in the previous work <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b8">8]</ref> and on the same dataset. Our Eesen framework outperforms both <ref type="bibr" target="#b6">[6]</ref> and <ref type="bibr" target="#b8">[8]</ref> in terms of WERs on the testing set. It is worth pointing out that the 8.7% WER reported in <ref type="bibr" target="#b6">[6]</ref> is obtained not in a purely end-to-end manner. Instead, the authors of <ref type="bibr" target="#b6">[6]</ref> generate a nbest list of hypotheses from a hybrid DNN model, and apply the CTC model to rescore the hypotheses candidates. Our Eesen numbers, in contrast, come from a completely end-toend pipeline, without any intervention from GMM or hybrid DNN models. Table <ref type="table">3</ref>. Performance of the character-based Eesen system using different vocabularies and language models, and its comparison with results presented in previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System</head><p>Vocabulary </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>In this work, we present our Eesen framework to build end-toend ASR systems. Eesen exploits deep RNNs as the acoustic models and CTC as the training objective function. We train the RNN models in a single step, and thus are able to reduce the complexity of ASR system development. The WFSTbased decoding enables efficient and effective incorporation of lexicions and language models. Because of its open-source property, Eesen can serve as a shared benchmark platform for research on end-to-end ASR.</p><p>In our future work, we plan to further improve the WERs of Eesen systems via more advanced learning techniques (e.g., expected transcription loss in <ref type="bibr" target="#b6">[6]</ref>) and alternative decoding approach (e.g., dynamic decoders <ref type="bibr" target="#b32">[31]</ref>). Also, we are interested to apply Eesen to various languages <ref type="bibr" target="#b33">[32,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34]</ref> and different types (e.g., noisy, far-field) of speech, and investigate how end-to-end ASR performs under these conditions. Moreover, due to the removal of GMMs, acoustic modeling in Eesen cannot leverage speaker adapted front-ends. We will study new speaker adaptation <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b37">36]</ref> and adaptive training <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38]</ref> techniques for the CTC models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575. This research was performed as part of the Speech Recognition Virtual Kitchen project, which is supported by the United States National Science Foundation under grant number CNS-1305365. This work was partially funded by Facebook, Inc. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Facebook, Inc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A memory block of LSTM.</figDesc><graphic url="image-1.png" coords="2,360.22,72.00,153.76,95.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of the phoneme-based Eesen system, and its comparison with the hybrid HMM/DNN system built with Kaldi. "#Param" means the number of parameters.</figDesc><table><row><cell>Model</cell><cell>LM</cell><cell cols="2">#Param WER%</cell></row><row><cell>Eesen RNN</cell><cell>lexicon</cell><cell>8.5M</cell><cell>26.92</cell></row><row><cell>Eesen RNN</cell><cell>trigram</cell><cell>8.5M</cell><cell>7.87</cell></row><row><cell cols="2">Hybrid HMM/DNN trigram</cell><cell>9.2M</cell><cell>7.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparisons of decoding speed between the phoneme-based Eesen system and the hybrid HMM/DNN system. "RTF" refers to the real-time factor in decoding. "Graph Size" means the size of the decoding graph in terms of megabytes.</figDesc><table><row><cell>Model</cell><cell cols="2">RTF Graph Size</cell></row><row><cell>Eesen RNN</cell><cell>0.64</cell><cell>263</cell></row><row><cell cols="2">Hybrid HMM/DNN 2.06</cell><cell>480</cell></row><row><cell>4.3. Character-based Systems</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/yajiemiao/eesen</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">http://www.speech.cs.cmu.edu/cgi-bin/cmudict</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Audio, Speech, and Language Processing</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="30" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature engineering in context-dependent deep neural networks for conversational speech transcription</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Workshop on</title>
		<imprint>
			<biblScope unit="page" from="24" to="29" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GMM-free DNN training</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="5639" to="5643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Asynchronous, online, GMM-free training of a context dependent acoustic model for speech recognition</title>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Bacchiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association (INTERSPEECH). ISCA</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
				<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deepspeech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubho</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Awni Y Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.2873</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent nn: First results</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lexicon-free conversational speech recognition with neural networks</title>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics</title>
				<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end attention-based large vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philemon</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04395</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Listen, attend and spell</title>
		<author>
			<persName><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
				<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning acoustic frame labeling for speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francoise</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="4280" to="4284" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid speech recognition with deep bidirectional LSTM</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="273" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Franc ¸oise Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association (INTERSPEECH). ISCA</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional, long short-term memory, fully connected deep neural networks</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On speaker adaptation of long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association (INTER-SPEECH)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning precise timing with LSTM recurrent networks</title>
		<author>
			<persName><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="115" to="143" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Phoneme recognition using time-delay neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiyuki</forename><surname>Hanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiyohiro</forename><surname>Shikano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
				<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="328" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A novel objective function for improved phoneme recognition using time-delay neural networks</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>John B Hampshire</surname></persName>
		</author>
		<author>
			<persName><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="216" to="228" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for large-scale speech tasks</title>
		<author>
			<persName><forename type="first">Tara</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuvana</forename><surname>Ramabhadran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><surname>Lawrence R Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Silovský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselý</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding (ASRU)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName><forename type="first">Cyril</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Implementation and Application of Automata</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee-Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A one-pass decoder based on polymorphic linguistic context assignment</title>
		<author>
			<persName><forename type="first">Hagen</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Fügen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding</title>
				<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="214" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Distributed learning of multilingual DNN feature extractors using GPUs</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<publisher>IN-TERSPEECH</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving languageuniversal feature extraction with deep maxout and convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association (INTERSPEECH). ISCA</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition for chinese mandarin using long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<publisher>INTER-SPEECH</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Speaker adaptation of context dependent deep neural networks</title>
		<author>
			<persName><forename type="first">Hank</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="7947" to="7951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptation of contextdependent deep neural networks for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Kaisheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards speaker adaptive training of deep neural network acoustic models</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
				<imprint>
			<publisher>INTER-SPEECH</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>ISCA</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Speaker adaptive training of deep neural network acoustic models using i-vectors</title>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1938" to="1949" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>TASLP)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
