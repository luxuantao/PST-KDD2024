<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmentations in Graph Contrastive Learning: Current Methodological Flaws &amp; Towards Better Practices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Puja</forename><surname>Trivedi</surname></persName>
							<email>pujat@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ekdeep</forename><surname>Singh Lubana</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujun</forename><surname>Yan</surname></persName>
							<email>yujunyan@umich.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
							<email>yqyang@berkeley.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
							<email>dkoutra@umich.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Augmentations in Graph Contrastive Learning: Current Methodological Flaws &amp; Towards Better Practices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Neural Networks</term>
					<term>Contrastive Learning</term>
					<term>Data Augmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph classification has a wide range of applications in bioinformatics, social sciences, automated fake news detection, web document classification, and more. In many practical scenarios, including web-scale applications, where labels are scarce or hard to obtain, unsupervised learning is a natural paradigm but it trades off performance. Recently, contrastive learning (CL) has enabled unsupervised computer vision models to compete well against supervised ones. Theoretical and empirical works analyzing visual CL frameworks find that leveraging large datasets and domain relevant augmentations is essential for framework success. Interestingly, graph CL frameworks report high performance while using orders of magnitude smaller data, and employing domain-agnostic augmentations (e.g., node or edge dropping, feature perturbations) that can corrupt the graphs' underlying properties.</p><p>Motivated by these discrepancies, we seek to determine: (i) why existing graph CL frameworks perform well despite weak augmentations and limited data; and (ii) whether adhering to visual CL principles can improve performance on graph classification tasks. Through extensive analysis, we identify flawed practices in graph data augmentation and evaluation protocols that are commonly used in the graph CL literature, and propose improved practices and sanity checks for future research and applications. We show that on small benchmark datasets, the inductive bias of graph neural networks can significantly compensate for the limitations of existing frameworks. In case studies with relatively larger graph classification tasks, we find that commonly used domain-agnostic augmentations perform poorly, while adhering to principles in visual CL can significantly improve performance. For example, in graph-based document classification, which can be used for better web search, we show task-relevant augmentations improve accuracy by 20%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-based tasks such as graph similarity search, anomaly detection, and clustering have traditionally relied on hand-crafted graph features <ref type="bibr" target="#b2">[3]</ref>. However, leveraging recent advances in graph neural networks (GNNs), such tasks can now be fulfilled by learning effective graph representations automatically <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11]</ref>. Indeed, graph representation learning has started playing an integral role in various scientific and web applications, such as graph-based similarity search for web documents <ref type="bibr" target="#b15">[16]</ref>, fake news detection by classifying their propagation patterns <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref>, and analysis of the activity in web and social network communities (e.g., discussion threads on Reddit, code repository networks on Github) <ref type="bibr" target="#b41">[42]</ref>.</p><p>While GNNs have been successful in a variety of (semi-)supervised learning settings, in most practical scenarios, labels are scarcely available or hard to obtain. For example, web pages are seldom assigned with labels which summarize their contents; labeling fake news can be time-consuming; and drugs require prohibitively expensive wet lab experiments or analysis to determine their toxicity <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b73">74]</ref>. Unsupervised learning is a natural paradigm for such label-scarce settings, but its performance often lags behind that of supervised learning.</p><p>Recently, contrastive learning (CL) has revolutionized representation learning in computer vision, enabling unsupervised models to outperform their supervised counterparts on several vision tasks <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b19">20]</ref>. Such visual contrastive learning (VCL) frameworks maximize similarity between augmentations of an input (positive views), while minimizing similarity to other samples in the batch (negative views). Several theoretical and empirical works have attributed the success of VCL to two key principles: (i) training on large, high quality datasets <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> and (ii) leveraging strong, arXiv:2111.03220v1 [cs.LG] 5 Nov 2021 task-relevant augmentations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b72">73]</ref>. Indeed, VCL frameworks routinely use 1K-8K samples in a batch, necessitating large datasets, to ensure that enough negative samples are available to train stably <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b19">20]</ref>. Furthermore, by leveraging strong data augmentations, these frameworks introduce invariance to properties that are irrelevant to downstream task performance, preserve taskrelevant properties and prevent the model from learning shortcuts that may lead to brittle representations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Inspired by the success of VCL and motivated by label scarcity in graph applications, graph contrastive learning (GCL) has become an increasingly popular unsupervised learning paradigm for extracting useful representations from non-euclidean data <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b65">66]</ref>. Almost invariably, these frameworks rely on domain-agnostic graph augmentations (DAGA), as demonstrated in Fig. <ref type="figure" target="#fig_0">1</ref>, to instantiate their self-supervision task and achieve seemingly strong performance on benchmark datasets <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b64">65]</ref>. Interestingly, though, these frameworks significantly deviate from the aforementioned key principles known to be necessary for success in VCL. For example, by relying on domain-agnostic graph augmentations, these frameworks can corrupt task-relevant information, leading to false positive or invalid samples (see Fig. <ref type="figure" target="#fig_0">1</ref>). Furthermore, these works routinely benchmark their performance using graph classification datasets that contain &lt;10K samples and only two classes. In contrast, the smallest benchmarks used in VCL contain 60K samples with 10 to 100 classes (e.g., CIFAR-10 and CIFAR-100 <ref type="bibr" target="#b29">[30]</ref>).</p><p>In this work, we seek to identify the core mechanisms that enable GCL methods to perform seemingly well, despite significantly deviating from the well-grounded principles that underpin the success of visual CL. We demonstrate that flawed graph data augmentation practices and evaluation protocols combine to obfuscate the actual discriminative power of representations learned using GCL. To address these shortcomings, we leverage our analysis and the core motivations of contrastive learning to introduce several inexpensive strategies that can allow practitioners to ascertain the strength or weakness of their models' learned representations. Finally, we show that in practically complex settings, abiding by visual CL principles is not only important, but necessary, to achieve good performance. Our main contributions are summarized as follows:</p><p>‚Ä¢ Analysis of limitations of graph data augmentations:</p><p>We revisit commonly-used augmentations in GCL, and demonstrate that domain-agnostic strategies are weak augmentations that (i) produce out-of-distribution or invalid views, and (ii) lead models to learn weakly discriminative features that have both high inter-and intra-class similarity. We also systematically show across architectures and datasets that the inductive bias of randomly-initialized GNNs mitigates weak augmentations and other framework shortcomings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES &amp; RELATED WORK</head><p>We first introduce visual and graph CL frameworks and discuss the importance of large datasets and strong, task-relevant augmentations in the success of visual CL. For more details, see Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive Learning (CL)</head><p>Intuitively, CL frameworks learn representations by maximizing similarity between positive or "similar" samples and minimizing similarity between "dissimilar" or negative samples. Formally, let D = {ùë• 1 , . . . ùë• ùëõ } be a dataset consisting of samples ùë• ùëñ ‚àà X, the data domain. Let ùëì be an encoder such that ùëì (ùë• ùëñ ) = ùíõ ‚àà R ùëë and T : X ‚Üí X be a function that augments ùë• ùëñ to construct a positive pair of data {ùë• ùëñ , ùë• ùëó = T (ùë• ùëñ )}. Given a batch of size ùëÅ , similarity function sim : (R ùëë , R ùëë ) ‚Üí [0, 1], temperature parameter ùúè, and {ùë• ùëñ , ùë• ùëó }, representations are learned using the normalized temperature-scaled cross entropy (NT-XENT) loss <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b52">53]</ref>:</p><formula xml:id="formula_0">‚Ñì ùëñ,ùëó = ‚àí log exp sim ùíõ ùëñ , ùíõ ùëó /ùúè 2ùëÅ ùëò=1 1 [ùëò‚â†ùëñ ] exp (sim (ùíõ ùëñ , ùíõ ùëò ) /ùúè) .<label>(1)</label></formula><p>The numerator encourages the representations of ùë• ùëñ , ùë• ùëó to be similar, while the denominator encourages negative pairs (ùëò ‚â† ùëñ) to be dissimilar. Correspondingly, models learn to perform instance discrimination where each sample defines its own class and the augmented sample also belongs to this class.</p><p>The role of large, high-quality datasets. Optimizing Equation (1) is equivalent to learning an estimator for the mutual information shared between two augmented views, where the quality of the estimation is upper bounded by batch size <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b52">53]</ref>. While recent work finds better mutual information estimates do not necessarily entail better performance <ref type="bibr" target="#b51">[52]</ref>, empirically, large batch sizes are required to effectively train negative-sample frameworks <ref type="bibr" target="#b6">[7]</ref>. BYOL <ref type="bibr" target="#b14">[15]</ref> and SimSiam <ref type="bibr" target="#b7">[8]</ref> are recently proposed positive-sample-only frameworks that use, respectively, an exponentially moving average target network and Siamese encoder networks with a stop gradient operation to eliminate negative samples. Such frameworks are also more suited to limited-resource situations because they can train with smaller batch sizes, but may have more difficulty training and weaker performance than negative-sample counterparts.</p><p>The role of augmentations. State-of-the-art VCL frameworks <ref type="bibr">[6-8, 15, 20]</ref> define T through aggressive data augmentation, such as cropping and color jittering, and solarization. Good augmentations are shown to preserve task-relevant information, while simultaneously minimizing irrelevant information across views <ref type="bibr" target="#b50">[51]</ref>. By optimizing Eq. 1, the model learns invariance to properties altered by augmentations, leading to more generalizable representations. For example, the augmentations used by SimCLR have been shown to introduce "occlusion invariance", which is useful in classification tasks where objects may be obscured <ref type="bibr" target="#b39">[40]</ref>. Notably, augmentation strategies are not universal; occlusion invariance is not beneficial for semantic segmentation tasks, where view-point invariance represents a better prior on relevant information <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Contrastive Learning (GCL)</head><p>In this paper, we focus on three state-of-the-art CL frameworks for graph classification that represent different methodological perspectives: GraphCL <ref type="bibr" target="#b65">[66]</ref>, InfoGraph <ref type="bibr" target="#b45">[46]</ref> and MVGRL <ref type="bibr" target="#b18">[19]</ref>. GraphCL mirrors SimCLR, using a shared GNN encoder that generates representations of augmented graph views. Similar to DIM <ref type="bibr" target="#b20">[21]</ref> and SwAV <ref type="bibr" target="#b5">[6]</ref>, InfoGraph directly maximizes the mutual information between sampled subgraphs (local) and pooled graph (global) representations. Meanwhile, MVGRL mirrors CMC <ref type="bibr" target="#b49">[50]</ref> and contrasts multiple views of a graph.</p><p>Graph data augmentation. The discrete, non-euclidean nature of graphs not only makes it difficult to determine what properties are relevant for a task, but also how to preserve this information. As we discuss in Appendix F, the few existing works on graph data augmentations <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b70">71]</ref> are expensive and use labels. Therefore, existing graph CL frameworks leverage three main strategies to generate views: feature or topological perturbation (GraphCL), sampling (InfoGraph), and/or diffusion processes (MVGRL). We focus on the domain-agnostic perturbation and sampling basedstrategies introduced by GraphCL, as these are more popular in recent frameworks <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66]</ref>, more similar to the composable augmentations used in VCL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b14">15]</ref>, inexpensive to apply, and do not require dual view encoders. Figure <ref type="figure" target="#fig_0">1</ref> illustrates an example of these augmentations. Domain-agnostic graph augmentations or DAGA. An extensive empirical study on the benefits of DAGA in GCL <ref type="bibr" target="#b65">[66]</ref> demonstrated that composition of augmentations improves performance, augmentation utility is dataset dependent, and variable augmentation strength (i.e., number of nodes/edges/features modified or size of sampled subgraph) for creating a more difficult task is beneficial. A critical assumption behind DAGA is that limiting the augmentation strength to a fraction of the overall graph does not significantly alter the task-relevant information in the augmented graph. In this work, we revisit this assumption to show that it does not hold for many datasets and discuss the negative implications when training with poorly augmented graphs. Moreover, while it is obvious that context-aware graph augmentations (CAGA) that are sensitive to the task domain should improve performance, it is not clear how to create such augmentations. We not only extensively demonstrate the benefits of such augmentations, but also discuss a pipeline on how to design them in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REVISITING AUGMENTATIONS &amp; EVALUATION IN GCL: LIMITATIONS</head><p>In this section, we investigate how GCL differs from successful VCL practices and the effects of this departure. We find three key limitations of domain-agnostic graph augmentations: The strong inductive bias of randomly-initialized GNN models is the hidden mechanism that mitigates methodological and augmentations deficiencies of GCL frameworks.</p><p>We discuss these in more detail in the following subsections. In our analysis, we focus on commonly used graph classification datasets (Table <ref type="table" target="#tab_1">1</ref>) <ref type="bibr" target="#b34">[35]</ref>. Official implementations for GraphCL<ref type="foot" target="#foot_0">1</ref> , InfoGraph<ref type="foot" target="#foot_1">2</ref> , and MVGRL<ref type="foot" target="#foot_2">3</ref> are used. We consider the base encoder architecture used by <ref type="bibr" target="#b65">[66]</ref> and report results with graph convolutional layers from GIN <ref type="bibr" target="#b61">[62]</ref> (original implementation), PNA <ref type="bibr" target="#b9">[10]</ref>, SAGE <ref type="bibr" target="#b16">[17]</ref>, GAT <ref type="bibr" target="#b54">[55]</ref>, and GCN <ref type="bibr" target="#b25">[26]</ref>. See Appendix B for details on the training setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">(L1) Domain-agnostic graph augmentations destroy task-relevant information</head><p>Given the importance of data augmentation in the success of VCL, several recent works have sought to understand properties of successful data augmentation as well as the role of data augmentation while learning representations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b72">73]</ref>. Intuitively, data augmentation should expand the original training set with samples that are sufficiently similar, thereby preserving task-relevant information, while also sufficiently dissimilar, so as to prevent trivially easy positive pairs. This intuition can be quantified with two metrics, affinity and diversity <ref type="bibr" target="#b31">[32]</ref>. Affinity measures the distribution shift between the augmented sample distribution and the original sample distribution. Diversity quantifies how difficult it is to learn from augmented samples instead of only training samples. While augmentations that best improve performance optimize for both affinity and diversity <ref type="bibr" target="#b31">[32]</ref>, it is not clear that domainagnostic graph augmentations also optimize for both metrics. For example, molecular graph classification is a commonly-used benchmark when evaluating GCL frameworks. However, as noted in Fig. <ref type="figure" target="#fig_0">1</ref>, just a few node/edge/feature perturbations suffice to invalidate a molecule or significantly alter its function. Augmented data is sufficiently diverse, but it is not clear if creating invalid molecule samples also leads to low affinity or out-of-distribution samples.  Hypothesis. We argue that while it is not expected that accuracy on augmented data will match that of clean data, augmented accuracy should be nontrivial if augmentations are indeed informationpreserving <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Experimental setup. We measure affinity as follows: (i) train a supervised PNA network on the original training data, (ii) generate an augmented dataset by using random node/subgraph dropping at 20% of the graph size, as suggested by <ref type="bibr" target="#b65">[66]</ref> and (iii) evaluate on clean and augmented training data separately. The difference between clean and augmented accuracy is defined as affinity and quantifies the distribution shift induced by augmentations <ref type="bibr" target="#b31">[32]</ref>. Meanwhile, diversity is measured through the difference in loss when training on original versus augmented data only.</p><p>Results. In Table <ref type="table" target="#tab_2">2</ref>, we see a considerable difference between clean and augmented accuracy across datasets. This implies low affinity, indicating large distribution shift, and confirms that domainagnostic graph augmentations destroy task-relevant information.</p><p>In terms of diversity, we find that training on augmented data is unstable. Both these properties indicate that the current augmentation practices generate invalid and out-of-distribution data, which in fact hurt downstream performance <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">(L2) Domain-agnostic augmentations induce weak discriminability</head><p>Recall that the NT-XENT loss in Eq. ( <ref type="formula" target="#formula_0">1</ref>) maximizes the cosine similarity between representations of positive pairs while simultaneously minimizing the cosine similarity amongst representations of negative examples. However, Limitation (L1) points out that domain-agnostic graph augmentations can destroy task-relevant information and lead to corrupted positive pairs, i.e. pairs that no longer share task-relevant information. This implies that representation similarity will be maximized for samples that are not semantically similar, and that the resulting representations may not be discriminative with respect to classes-i.e., intra-class samples may not have higher similarity than inter-class samples, counter to what is expected.</p><p>Hypothesis. If a model has learned discriminative representations, intra-class cosine similarity (on-diagonal blocks in Figure <ref type="figure" target="#fig_1">2</ref>) should be closer to 1 (high similarity) while inter-class similarity (off-diagonal blocks) should be closer to 0 (low similarity).</p><p>Experimental setup. We measure the discriminative power of representations learned using GCL as follows: given models trained using GraphCL, InfoGraph and MVGRL, we extract representations for the entire dataset. Then, we calculate cosine similarity between all representation pairs. In Fig. <ref type="figure" target="#fig_1">2</ref>, we plot the cosine similarity between representations (sorted by class label), such that the upper left and lower right quadrants correspond to the similarity between same-class representations. Representation similarity from an untrained model is also included. Results on additional datasets can be found in Appendix C.</p><p>Table <ref type="table">3</ref>: Inductive Bias on Benchmark Datasets. Following the same evaluation protocol as <ref type="bibr" target="#b45">[46]</ref>, we generate embeddings from an untrained N-Layer GIN encoder and perform classification using LinearSVC. Results for GraphCL and InfoGraph are reported from <ref type="bibr" target="#b65">[66]</ref>. Best accuracy is in bold; other models whose accuracy, including standard deviation, fall within standard deviation of the best accuracy are underlined. We see across all datasets that untrained models have a strong inductive bias. On PROTEINS, DD and MUTAG models perform very competitively with trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Random Results. MVGRL (Fig. <ref type="figure" target="#fig_1">2d</ref>) and InfoGraph (Fig. <ref type="figure" target="#fig_1">2c</ref>) do not encounter corrupted positive pairs as they use diffusion-based views and direct mutual information maximization over sampled subgraphs, respectively. Correspondingly, MVGRL learns representations with high intra-class similarity and low inter-class similarity, as desired. InfoGraph has high intra-class similarity, but representation similarity is higher than MVGRL between inter-class samples. We note that the randomly initialized model (Fig. <ref type="figure" target="#fig_1">2a</ref>) has higher absolute similarity for all representations than trained methods, but intra-class similarity is higher than inter-class similarity. However, GraphCL, which trains on corrupted pairs (Limitation (L1)), has low intra-class similarity as seen in the upper-left quadrant (Fig. <ref type="figure" target="#fig_1">2b</ref>). This implies that the model has not learned features that capture the semantic similarity between the samples of the same class.</p><p>Proposed evaluation practice. Given that contrastive learning frameworks directly optimize the similarity between representations, we argue that plotting the representation similarity can serve as a simple sanity check for practitioners to ensure that the model has indeed learned representations as expected.</p><p>Having established that DAGA leads to harmful distribution shifts and GraphCL has poorly-discriminative representations, next, we investigate whether other factors are bolstering GCL performance. Specifically, we discuss the role of random inductive bias and identify flaws in current GCL evaluation practices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">(L3) Strong inductive bias of random models reduces GCL inefficiencies</head><p>As noted in Limitation (L2), randomly-initialized models produce representations that are already discriminative without any training (Fig. <ref type="figure" target="#fig_1">2a</ref>). While the strength of inductive bias of GNNs in (semi-) supervised settings has been noted before <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b68">69]</ref>, we aim to better contextualize the performance of GCL frameworks by conducting a systematic analysis of the inductive bias of GNNs, using several datasets and architectures.</p><p>Empirical setup. For all datasets but DEEZER and GITHUB-SGZR, we use the same GIN encoder as GraphCL and Infograph. The GIN layer is replaced with a PNA layer for DEEZER and GITHUB-SGZR and we report results after training this model with GraphCL and InfoGraph. See Appendix C for more details.</p><p>Results. As shown in Table <ref type="table">3</ref>, a randomly-initialized, untrained model is able to able to achieve non-trivial performance on several benchmark datasets. Given that unsupervised representation learning is often a form of pretraining and representations can be of poorer quality when trained with DAGA (see Figure <ref type="figure" target="#fig_1">2b</ref>), it becomes difficult to justify to the expense of GCL if performance does not surpass that of a randomly-initialized model. Furthermore, we note that the strong inductive bias may have partially mitigated the negative effects of DAGA (Limitations (L1)-(L2)).</p><p>Proposed evaluation practices. We argue that randomly-initialized models are a non-trivial baseline for GCL frameworks and should be included in results. While a recent work <ref type="bibr" target="#b62">[63]</ref> includes randomlyinitialized baselines as part of its results, this practice is far from being the norm.</p><p>Furthermore, other changes to GCL evaluation that are necessary. As mentioned in Sec. 2, optimizing NT-XENT is equivalent to learning an estimator of the mutual information shared between two views, whose upper bound is limited by the batch size. Given the limited size of popular benchmark datasets (Table <ref type="table">3</ref>), it becomes difficult for negative-sample GCL frameworks using NT-XENT, such as GraphCL, to learn reliable mutual information estimates.</p><p>Moreover, negative samples in NT-XENT are through the other samples in the batch. However, popular benchmark datasets are binary classification problems. Therefore, in a balanced binary dataset, half the sample in any given batch will share the label of the augmented views but will be treated as negative samples. While it has been noted that popular graph classification datasets are problematic <ref type="bibr" target="#b12">[13]</ref>, here, we show that evaluating on these datasets in an unsupervised learning setting with negative-sample GCL frameworks is fundamentally flawed and this practice must be discontinued. This matter is of some urgency as this common evaluation protocol is becoming more prevalent <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b64">65]</ref>. We note that positive-sample-only frameworks, such as SimSiam <ref type="bibr" target="#b7">[8]</ref> and BYOL <ref type="bibr" target="#b14">[15]</ref>, are appropriate for binary classification tasks; they do not rely on negative samples and are generally robust to smaller batch-sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Summary of Proposed Evaluation Practices</head><p>We summarize the practices that we hope will be adopted in future graph CL research:</p><p>‚Ä¢ Given that domain-agnostic graph augmentations can destroy task-relevant information and harm the model's ability to learn discriminative representations, there is need for designing context-aware graph augmentations (Sec. 4). ‚Ä¢ Random models have strong inductive bias and should be reported as baselines. ‚Ä¢ Irrespective of accuracy, small graph datasets must not be used when evaluating negative-sample GCL frameworks. ‚Ä¢ There is a need for more rigorous evaluation that looks beyond accuracy, such as the aformentioned measures of representation quality, i.e., affinity, diversity, and cos. similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">BENEFITS &amp; DESIGN OF CONTEXT-AWARE GRAPH AUGMENTATIONS</head><p>The previous section demonstrates that augmentation and evaluation deficiencies have enabled GCL to perform well despite deviating from principles followed by successful VCL frameworks. In this section, through two case studies, we correct the aforementioned flaws, exemplify the benefits of adhering to VCL principles, and outline a process for designing context-aware graph augmentations. Our broad strategy for designing such augmentations is to leverage domain knowledge in the abstracted modality, in this case images or natural language, to identify augmentations that will (i) preserve task-relevant information, (ii) break view symmetry, and (iii) introduce semantically meaningful invariance.</p><p>Context-aware Augmentations. Various data, such as documents, propagation patterns, molecules, maps, and point-clouds, can be naturally represented as graphs. GNNs can then be used to learn representations to complete downstream tasks. For example, documents can be represented as word co-occurrence graphs (Sec. 4.1) and images can be represented as super-pixel nearestneighbor graphs (Sec. 4.2). In this section, we demonstrate how to convert domain knowledge or known context-aware augmentations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b58">59]</ref> in the abstracted data modality (natural language and images) to augmentations that are effective in the graph space.</p><p>Evaluation Protocol. The document classification task uses a binary dataset that contains 10k samples. Therefore, we use positivesample-only frameworks for it. Super-pixel MNIST classification has precedence as a benchmark for evaluating the performance of GNNs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>, and corrects many limitations addressed in Sec. 3. Specifically, the dataset (70k samples) is larger than standard graph benchmarks, and contains 10 classes, enabling the appropriate use of negative-sample frameworks. All models are trained in an unsupervised setting and we report ùëò-NN accuracy on representations obtained from the trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Case Study 1: Document Classification</head><p>Automated classification of web documents is key for the organization of the Web and for improved web search experience. Among the different ways of classifying documents, we cast document classification as a graph classification problem which has been shown to be promising <ref type="bibr" target="#b15">[16]</ref> as it captures not only the content but also the structure of the documents.</p><p>Dataset &amp; Task. Following <ref type="bibr" target="#b35">[36]</ref>, we convert the Subjectivity document classification dataset <ref type="bibr" target="#b37">[38]</ref> into co-occurrence graphs, where nodes represent words, edges indicate that two words have cooccurred within the same window (e.g. window size 2 and 4), and node features are word2vec <ref type="bibr" target="#b32">[33]</ref> embeddings. An example of a cooccurrence graph mapping to a document is shown in Fig. <ref type="figure" target="#fig_2">3a</ref>. The dataset contains both objective movie summaries and subjective movie reviews, and the task is to predict subjectivity.</p><p>Setup of GNN models. We use a Message Passing Attention Network <ref type="bibr" target="#b35">[36]</ref> as the encoder, and a 2-layer MLP as the predictor. The representation dimension is 64, and models are trained using Adam <ref type="bibr" target="#b24">[25]</ref> with LR=5e-4. Additional training details are given in Appendix D. We report results with the original GCN layer used by <ref type="bibr" target="#b35">[36]</ref>, as well as with the GraphSAGE <ref type="bibr" target="#b16">[17]</ref> and GIN <ref type="bibr" target="#b61">[62]</ref> layers replacing it. Given the task is binary classification, we use only-positive-sample frameworks, SimSiam <ref type="bibr" target="#b7">[8]</ref> and BYOL <ref type="bibr" target="#b14">[15]</ref>. Domain-Agnostic Graph Augmentations. We conduct an informal grid search to select which DAGA and augmentation strength to use. Among node, edge, and subgraph dropping at {5%, 10%, 20%} of text length, we find generating both views using subgraph dropping (10%) performs the best. Generating one view with subgraph dropping (10%) and the other with node-dropping (10%) performs second best.</p><p>Context-Aware Graph Augmentations. Several NLP augmentations have been proposed in past work <ref type="bibr" target="#b58">[59]</ref>: synonym replacement, random word insertion, random word swapping and random word deletion, where the augmentation strength is determined by the sentence length. These augmentations can help preserve task-relevant information, introduce invariances that are useful for the downstream task (e.g., invariance to the occasional dropped word), and break view symmetry in the natural language modality.</p><p>The above-mentioned augmentations can be directly translated into co-occurrence graph augmentations as follows: Synonym Replacement is equivalent to replacing node features of the selected word (node) with the closest word2vec embedding. Random Insertion can be approximated in the co-occurence graph by (i) creating a new node with a randomly selected word2vec embedding and (ii) duplicating the connections of an existing node. Random Deletion can be represented by (i) randomly removing a node from the co-occurrence graph and (ii) rewiring the modified graph to connect neighbors of the removed node. Random Swap is equivalent to swapping the features of two nodes. We provide more detailed explanations in the appendix. Unlike these context-aware augmentations, we note that domain-agnostic subgraph and node dropping does not rewire the co-occurence graph.</p><p>The mapping between natural language and co-occurrence graph augmentations suggests that translating abstracted modality augmentations to graph augmentations is a viable strategy for designing context-aware augmentations augmentations. One can also understand these augmentations as direct perturbation of the datagenerating process <ref type="bibr" target="#b72">[73]</ref>.  <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b69">70]</ref>, where words are treated as nodes, edges indicate co-occurrence in a sliding window, and nodes features are word2vec embeddings <ref type="bibr" target="#b32">[33]</ref>. As shown in (b), we perform synonym replacement (purple) and random word insertion (green) to augment sentences without losing task-relevant information <ref type="bibr" target="#b58">[59]</ref>. In (c), we show random node (word) deletion (red) in the co-occurrence graph.</p><p>Our results show that natural language space augmentations improve classification accuracy substantially over baseline graph augmentations, which are more prone to altering sentence sentiment. Results. As shown in Table <ref type="table" target="#tab_4">4</ref>, context-aware augmentations perform considerably better (up to +20%) than baseline graph augmentations over two window sizes <ref type="bibr">(2 and 4)</ref>. Given that we use the default hyperparameters suggested by <ref type="bibr" target="#b35">[36]</ref>, it is possible that BYOL has not yet converged for all augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Study 2: Super-pixel Classification</head><p>Our second case study is based on super-pixel MNIST classification, a standard benchmark for evaluating GNN performance <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Dataset &amp; Task. We follow the established protocols in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> to create super-pixel representations of MNIST, where each image is represented as a K-nearest neighbors graph between super-pixels (homogeneous patches of intensity). Nodes map to super-pixels, node features are super-pixel intensity and position, and edges are connections to ùëò neighbors. An example is shown in Fig. <ref type="figure" target="#fig_3">4</ref>.</p><p>Setup of GNN models. The following architecture is used for experiments: the encoder is 5-layer GIN architecture similar to <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b12">[13]</ref>. The predictor is a 2-layer MLP and there is no projector. Models are trained for 80 epochs, using Adam <ref type="bibr" target="#b24">[25]</ref> with LR=1e-3, and the representation dimension is set to 110. The models are trained in an unsupervised setting using SimSiam <ref type="bibr" target="#b7">[8]</ref> and Sim-CLR <ref type="bibr" target="#b6">[7]</ref>. More training details are in Appendix E. While composing augmentations is known to improve performance on vision tasks, we avoid it here in order to fairly compare to graph baselines, which only consider a single augmentation. Node dropping, a DAGA, significantly alters graph topology and it is unclear if task-relevant information is preserved.</p><p>Colorizing perturbs node features but preserves graph topology. Because classification does not rely upon color, colorizing preserves task-relevant information in both image and graph space while also breaking view symmetry.</p><p>Domain-Agnostic Graph Augmentations. Following <ref type="bibr" target="#b65">[66]</ref>, we apply random node dropping at 20% of the graph size to obtain both samples in the positive pair.</p><p>Context-Aware Graph Augmentations. While geometric image augmentations <ref type="bibr" target="#b6">[7]</ref>, such as horizontal flipping and rotating, generally preserve task-relevant information and introduce semantically meaningful invariance, they cannot break view symmetry in GCL frameworks. GNNs are permutation invariant. Therefore, the representations of a pair of flipped images will be similar as their corresponding super-pixel graph representations are equivalent up to node reordering. On the other hand, augmentations such as cropping may result in qualitatively different super-pixel graphs. Here, it is unclear if the super-pixel graph obtained after augmentation preserves task-relevant information, even if cropping is information preserving with respect to the original image. Therefore, it is not trivial to identify successful augmentations in the abstracted domain that will also be successful in graph space.</p><p>Given the difficulty of identifying augmentations that perturb super-pixel graph topology but also preserve task-relevant information, we focus on image space augmentations that lead to modified node features in the super-pixel graph. Specifically, we select random colorizing as the context-aware augmentation as it (i) preserves task-relevant information as color is not relevant property when classifying digits, (ii) it breaks view symmetry because the node features of augmented samples are different and (iii) introduces a harmless invariance to colorization. We briefly note that augmentations are generally selected to introduce invariances that are useful to the downstream task. For example, cropping results in occlusion invariance, which is useful for classification tasks where objects are often partially covered <ref type="bibr" target="#b39">[40]</ref>. We take a complementary approach where augmentations introduce harmless information (color) and the model learns to ignore it. This can be a useful strategy when it is difficult to clearly identify potentially useful invariances for a downstream task.</p><p>Results. In Table <ref type="table" target="#tab_5">5</ref>, we observe that training with an informationpreserving, context-aware augmentation (colorizing) improves accuracy for both SimSiam and SimCLR. While BYOL generally performs more poorly than SimSiam and SimCLR, colorizing is still within standard deviation of DAGA. Composing augmentations with colorizing would likely further improve performance but we leave such ablations to future work. Lastly, we note that randomlyinitialized models have 37.79% accuracy, indicating that super-pixel data is a useful benchmark for future GCL evaluation <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Summary</head><p>These case studies on challenging datasets demonstrate how to leverage domain knowledge in the abstracted modality to identify context-aware aware augmentations that are applicable to GCL. We show that is not trivial to directly translate from common abstract modality augmentations (geometric image augmentations) to feasible context-aware graph augmentations and discuss criteria for selecting successful augmentations. In Section 4.1, we show how domain knowledge can be used to modify DAGA to achieve considerably improved performance. Additionally, these case studies correct flawed evaluation practices to use negativesample frameworks only when there are multiple classes and report random baselines. Given that augmentation utility is dependent on the downstream task <ref type="bibr" target="#b39">[40]</ref> and graph data augmentations are difficult <ref type="bibr" target="#b70">[71]</ref> due to the non-euclidean, discrete nature of graphs, we outline a process, instead of a prescription, for designing contextaware graph augmentations that practitioners can adapt to their own settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we discuss limitations in the evaluation and design of existing instance-discrimination GCL frameworks, and introduce new improved practices, including the design of context-aware augmentations. First, through our analysis, we show domain-agnostic graph augmentations do not preserve task-relevant information and lead to weakly discriminative representations. Next, we demonstrate that benchmark graph classification datasets are not appropriate for evaluating GCL frameworks as (i) random baselines perform non-trivially against models trained through unsupervised learning, (ii) limited batch sizes leads to unreliable mutual information estimates in negative sample frameworks, and (iii) training on binary classification datasets leads to class collisions in negativesample frameworks. We also show that the strong inductive bias of random models mitigates framework inefficiencies. While we acknowledge the community is moving toward larger and more extensive benchmarks <ref type="bibr" target="#b12">[13]</ref>, we emphasize that it is fundamentally incorrect to continue evaluating GCL on legacy graph classification benchmarks for the aforementioned reasons. Furthermore, on two case studies with practically complex tasks, we show how to use domain knowledge to design information-preserving, context-aware augmentations and achieve significant improvements over training with domain-agnostic graph augmentations. In summary, GCL is an exciting new direction, especially for web applications where unlabeled datasets can be readily scraped, and our work can inform the evaluation of new methods as well as help practitioners design context-aware augmentations.  Method Augmentations BGRL <ref type="bibr" target="#b48">[49]</ref> Edge Dropping, Attr. Masking GCA <ref type="bibr" target="#b71">[72]</ref> Edge Dropping, Attr. Masking (both weighted by centrality) GCC <ref type="bibr" target="#b40">[41]</ref> RWR Subgraph Extraction of Ego Network GraphCL <ref type="bibr" target="#b65">[66]</ref> Node Dropping, Edge Adding/Dropping, Attr. Masking, Subgraph Extraction MVGRL <ref type="bibr" target="#b18">[19]</ref> PPR Diffusion + Sampling SelfGNN <ref type="bibr" target="#b23">[24]</ref> Attr. Splitting, Attr. Standardization + Scaling, Local Degree Profile, Paste + Local Degree Profile augmentations are random node dropping (at 20% and 30%). The context aware augmentation is random colorizing, performed using Scikit-Image ( <ref type="bibr" target="#b53">[54]</ref>). As discussed in the main text, colorizing can be represented as transformation on node features as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F RELATED WORK</head><p>Graph Data Augmentation. While intuitive augmentations that preserve task-relevant information exist for images and natural language, such augmentations do not exist for graphs due to their discrete, non-euclidean nature. For molecular graphs, modifying a single change can completely alter the original properties. In light of these difficult, graph data augmentation is less explored.</p><p>[71] train a neural edge predictor to increase homophily by adding edges between nodes expected to be of the same class and break edges between nodes of expected dissimilar classes. However, this approach is expensive and not applicable to graph classification. <ref type="bibr" target="#b27">[28]</ref> focus on feature augmentations because it is easier than designing information preserving topological transformations. They add adversarial perturbations to node features as augmentations. In contrastive learning scenarios, labels are not available and cannot be used for the adversarial perturbation, so the proposed approach is not directly applicable. Graph Self-Supervised Learning. Several paradigms for self-supervised learning in graphs have been recently explored, including the use of pre-text tasks, multi-tasks, and unsupervised learning. See <ref type="bibr" target="#b30">[31]</ref> for an up-to-date survey. Graph pre-text tasks are often reminiscent of image in-painting tasks <ref type="bibr" target="#b67">[68]</ref>, and seek to complete masked graphs and/or node features ( <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b66">67]</ref>). Other successful approaches include predicting graph level or property level properties during pre-training or part of regular training to prevent over-fitting <ref type="bibr">([22]</ref>). These tasks often must be carefully selected to avoid negative transfer between tasks. Many unsupervised approaches have also been proposed. <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56]</ref> draw inspiration from <ref type="bibr" target="#b20">[21]</ref> and maximize the mutual information between global and local representations. MVGRL ( <ref type="bibr" target="#b18">[19]</ref>) contrasts different views at multiple granularities similar to <ref type="bibr" target="#b52">[53]</ref>. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b71">72]</ref> use augmentations to generate views for contrastive learning. See Table F for a summary of the augmentations used. We note that random corruption, sampling or diffusion based approaches often do not preserve task relevant information or introduce meaningful invariances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: [Left] Domain-Agnostic Graph Augmentations introduced in [66]: attribute masking, edge perturbation (deletion/addition in red/green), node dropping and subgraph sampling. [Right] False Positive Samples. Acidic molecule Phenol and basic molecule Analine are structurally similar, differing at only one edge (chemical bond) and the corresponding node feature (element). Domain-agnostic graph augmentations can inadvertently generate this pair as a positive view, resulting in similar representations for semantically dissimilar entities.</figDesc><graphic url="image-1.png" coords="1,317.96,219.36,240.25,120.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Representational Similarity. We compute the cosine similarity between all-pairs of representations for MUTAG using three graph CL frameworks as well as a randomly-initialized model. The upper left and lower right regions (indicated by green lines) shows intra-class similarity. GraphCL destroys information, indicated by low similarity. MVGRL, which uses diffusionbased views, learns representations that have high intraclass similarity and low interclass similarity, as desired. InfoGraph, which directly maximizes mutual information between local/global representations, preserves high intraclass similarity, and has moderate interclass similarity. These results indicate that false positive examples, generated through domain agnostic graph augmentations, negatively impact GraphCL's training.</figDesc><graphic url="image-2.png" coords="4,73.84,98.67,89.32,89.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Augmentations for Document Classification: Documents are represented as co-occurrence graphs<ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b69">70]</ref>, where words are treated as nodes, edges indicate co-occurrence in a sliding window, and nodes features are word2vec embeddings<ref type="bibr" target="#b32">[33]</ref>. As shown in (b), we perform synonym replacement (purple) and random word insertion (green) to augment sentences without losing task-relevant information<ref type="bibr" target="#b58">[59]</ref>. In (c), we show random node (word) deletion (red) in the co-occurrence graph. Our results show that natural language space augmentations improve classification accuracy substantially over baseline graph augmentations, which are more prone to altering sentence sentiment.</figDesc><graphic url="image-10.png" coords="7,64.64,83.72,149.81,131.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Augmentations for Super-pixel Based Classification.Node dropping, a DAGA, significantly alters graph topology and it is unclear if task-relevant information is preserved. Colorizing perturbs node features but preserves graph topology. Because classification does not rely upon color, colorizing preserves task-relevant information in both image and graph space while also breaking view symmetry.</figDesc><graphic url="image-14.png" coords="8,53.80,161.87,108.11,55.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>assess the benefits of proposed augmentations and frameworks.‚Ä¢ Case studies with strong augmentations: In two case studies on different data modalities, we demonstrate how to leverage simple domain knowledge to develop strong, task-relevant augmentations that are feasible for GCL. Our systematic process leads to augmentations that outperform generic augmentations in classification accuracy by up-to 20% in the web task of graph-based document classification.</figDesc><table><row><cell>‚Ä¢ Identification of methodological flaws &amp; better prac-</cell></row><row><cell>tices: We contextualize recent theoretical work in visual self-</cell></row><row><cell>supervised learning to identify problematic practices in GCL:</cell></row><row><cell>(i) the use of small datasets and (ii) training with negative-</cell></row><row><cell>sample frameworks on binary classification datasets. Our</cell></row></table><note>results indicate future works must report untrained baselines to accurately represent method performance, and avoid using negative-sample frameworks for binary classification. Furthermore, we provide carefully-designed sanity checks for practitioners to</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset Description Standard augmentation practices are neither strong nor taskrelevant, and are susceptible to destroying task-relevant information (e.g., the underlying graphs' properties). (L2) The existing GCL evaluation protocol is flawed and can lead to weakly discriminative representations. (L3)</figDesc><table><row><cell>Name</cell><cell cols="5">Graphs Classes Avg. Nodes Avg. Edges Domain</cell></row><row><cell>IMDB-BINARY [64]</cell><cell>1000</cell><cell>2</cell><cell>19.77</cell><cell>96.53</cell><cell>Social</cell></row><row><cell>REDDIT-BINARY [64]</cell><cell>2000</cell><cell>2</cell><cell>429.63</cell><cell>497.75</cell><cell>Social</cell></row><row><cell>GOSSIPCOP [45]</cell><cell>5464</cell><cell>2</cell><cell>55.48</cell><cell>54.51</cell><cell>News</cell></row><row><cell>DEEZER [42]</cell><cell>9629</cell><cell>2</cell><cell>23.49</cell><cell>65.25</cell><cell>Social</cell></row><row><cell>GITHUB SGZR [42]</cell><cell>12725</cell><cell>2</cell><cell>113.79</cell><cell>234.64</cell><cell>Social</cell></row><row><cell>MUTAG [29]</cell><cell>188</cell><cell>2</cell><cell>17.93</cell><cell cols="2">19.79 Molecule</cell></row><row><cell>PROTEINS [5]</cell><cell>1113</cell><cell>2</cell><cell>39.06</cell><cell cols="2">72.82 Bioinf.</cell></row><row><cell>DD [44]</cell><cell>1178</cell><cell>2</cell><cell>284.32</cell><cell cols="2">715.66 Bioinf.</cell></row><row><cell>NCI1 [58]</cell><cell>4110</cell><cell>2</cell><cell>29.87</cell><cell cols="2">32.30 Molecule</cell></row><row><cell>(L1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Augmentation</figDesc><table><row><cell>Dataset</cell><cell cols="2">Clean Train Acc. Aug. Train Acc.</cell></row><row><cell>MUTAG</cell><cell>90.14 ¬± 1.36</cell><cell>37.67 ¬± 1.48</cell></row><row><cell>PROTEINS</cell><cell>70.70 ¬± 4.30</cell><cell>56.54 ¬± 8.11</cell></row><row><cell>NCI1</cell><cell>75.55 ¬± 4.60</cell><cell>60.15 ¬± 0.069</cell></row><row><cell>DD</cell><cell>84.06 ¬± 8.81</cell><cell>65.41 ¬± 14.87</cell></row><row><cell>REDDIT-BINARY</cell><cell>85.56 ¬± 3.21</cell><cell>50.56 ¬± 0.09</cell></row><row><cell>IMDB-BINARY</cell><cell>70.93 ¬± 0.046</cell><cell>50.11 ¬± 0.384</cell></row><row><cell>GOSSIPCOP</cell><cell>98.047 ¬± 0.37</cell><cell>96.03 ¬± 1.57</cell></row></table><note>Affinity. Affinity<ref type="bibr" target="#b31">[32]</ref>, measured by the difference between original and augmented accuracy of a supervised model, captures how much the data distribution has changed as a result of augmentation. We see that DAGA leads to low affinity. This is expected for molecular datasets, where it is easy to create invalid molecules, and is also true for some social network datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Document Classification. We use subgraph dropping (S) at 10% of sentence length and node-dropping (N) at 10% of sentence length as baseline graph augmentations. For text space augmentations, we stochastically apply synonym replacement (5%), random insertion (5%), random swapping (5 %) and random deletion (10 %). Random Acc. w/ window-size = 2 is 58.46 ¬± 1.97. Random Acc. w/ window-size = 4 is 63.93 ¬± 0.045.</figDesc><table><row><cell></cell><cell>GCN</cell><cell></cell><cell>SAGE</cell><cell></cell><cell>GIN</cell><cell></cell></row><row><cell>Augmentation</cell><cell cols="2">SimSiam Acc. BYOL Acc.</cell><cell cols="2">SimSiam Acc. BYOL Acc.</cell><cell cols="2">SimSiam Acc. BYOL Acc.</cell></row><row><cell>S. v. S (ws =2)</cell><cell>69.41 ¬± 7.28</cell><cell>62.98 ¬± 3.12</cell><cell>59.17 ¬± 8.36</cell><cell>67.17 ¬± 2.70</cell><cell>55.67 ¬± 4.61</cell><cell>65.02 ¬± 2.00</cell></row><row><cell>S vs. N (ws =2)</cell><cell>57.84 ¬± 4.31</cell><cell>65.78 ¬± 8.22</cell><cell>56.74 ¬± 1.70</cell><cell>63.77 ¬± 2.90</cell><cell>58.2 ¬± 8.24</cell><cell>74.26 ¬± 3.80</cell></row><row><cell>Context-Aware (ws = 2)</cell><cell>83.65 ¬± 2.31</cell><cell>78.12 ¬± 2.73</cell><cell cols="2">81.28 ¬± 2.54 78.23 ¬± 4.53</cell><cell cols="2">80.37 ¬± 4.07 77.79 ¬± 0.09</cell></row><row><cell>S vs. S (ws = 4)</cell><cell>61.76 ¬± 5.12</cell><cell>66.38 ¬± 2.29</cell><cell>54.68 ¬± 1.53</cell><cell>67.37 ¬± 1.11</cell><cell>54.71 ¬± 3.00</cell><cell>66.18 ¬± 2.34</cell></row><row><cell>S vs. N (ws = 4)</cell><cell>55.38 ¬± 1.99</cell><cell>68.311.88</cell><cell>59.23 ¬± 8.03</cell><cell>70.6 ¬± 4.85</cell><cell>53.31 ¬± 1.36</cell><cell>66.59 ¬± 1.57</cell></row><row><cell>Context-Aware (ws = 4)</cell><cell cols="2">81.12 ¬± 3.97 74.05 ¬± 5.465</cell><cell cols="2">80.67 ¬± 10.36 75.65 ¬± 5.54</cell><cell cols="2">75.30 ¬± 15.61 76.55 ¬± 7.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Super-pixel Classification. We perform unsupervised training and report the KNN Accuracy using domain agnostic graph augmentations (Node Dropping, abbrv. ND) and context aware graph augmentations (Colorize). Context aware augmentations improve performance. Accuracy of randomly initialized model is 37.79 ¬± 0.03.</figDesc><table><row><cell>Aug.</cell><cell cols="2">SimSiam Acc. SimCLR Acc.</cell><cell>BYOL Acc.</cell></row><row><cell>ND (20%)</cell><cell>66.30 ¬± 0.33</cell><cell cols="2">68.56 ¬± 0.16 65.32 ¬± 0.95</cell></row><row><cell>ND (30%)</cell><cell>61.30 ¬± 0.48</cell><cell>68.07 ¬± 0.37</cell><cell>61.87 ¬± 1.03</cell></row><row><cell>Colorize</cell><cell>68.95 ¬± 1.20</cell><cell cols="2">73.67 ¬± 0.10 64.42 ¬± 2.385</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Inductive Bias: Additional results. ¬± 0.003 0.73 ¬± 0.007 0.74 ¬± 0.004 0.75 ¬± 0.004 0.75 ¬± 0.003 NCI1 0.76 ¬± 0.004 0.75 ¬± 0.001 0.75 ¬± 0.002 0.78 ¬± 0.008 0.79 ¬± 0.007 DD 0.78 ¬± 0.002 0.77 ¬± 0.012 0.78 ¬± 0.003 0.79 ¬± 0.007 0.76 ¬± 0.003 REDDIT-B 0.52 ¬± 0.005 0.51 ¬± 0.003 0.52 ¬± 0.005 0.92 ¬± 0.002 0.80 ¬± 0.062 IMDB-B 0.54 ¬± 0.001 0.57 ¬± 0.016 0.58 ¬± 0.008 0.71 ¬± 0.011 0.62 ¬± 0.070 ¬± 0.003 0.85 ¬± 0.009 0.84 ¬± 0.003 0.81 ¬± 0.032 0.85 ¬± 0.013 PROTEINS 0.74 ¬± 0.002 0.74 ¬± 0.005 0.74 ¬± 0.006 0.74 ¬± 0.007 0.74 ¬± 0.005 NCI1 0.76 ¬± 0.009 0.75 ¬± 0.004 0.76 ¬± 0.002 0.78 ¬± 0.004 0.70 ¬± 0.040 DD 0.78 ¬± 0.005 0.77 ¬± 0.006 0.79 ¬± 0.001 0.79 ¬± 0.003 0.76 ¬± 0.005 REDDIT-B 0.52 ¬± 0.005 0.53 ¬± 0.004 0.52 ¬± 0.012 0.75 ¬± 0.004 -IMDB-B 0.51 ¬± 0.004 0.51 ¬± 0.009 0.50 ¬± 0.005 0.51 ¬± 0.007 -</figDesc><table><row><cell cols="2">GraphSAGE 3 Layer</cell><cell>4 Layer</cell><cell>5 Layer</cell><cell>GraphCL</cell><cell>InfoGraph</cell></row><row><cell>MUTAG</cell><cell cols="5">0.85 ¬± 0.005 0.85 ¬± 0.006 0.85 ¬± 0.005 0.82 ¬± 0.040 0.85 ¬± 0.005</cell></row><row><cell>PROTEINS</cell><cell cols="5">0.73 ¬± 0.004 0.73 ¬± 0.003 0.74 ¬± 0.005 0.75 ¬± 0.002 0.74 ¬± 0.008</cell></row><row><cell>NCI1</cell><cell cols="5">0.74 ¬± 0.003 0.75 ¬± 0.006 0.73 ¬± 0.011 0.78 ¬± 0.000 0.79 ¬± 0.002</cell></row><row><cell>DD</cell><cell cols="5">0.77 ¬± 0.006 0.78 ¬± 0.002 0.78 ¬± 0.005 0.80 ¬± 0.008 0.77 ¬± 0.010</cell></row><row><cell>REDDIT-B</cell><cell cols="4">0.85 ¬± 0.014 0.83 ¬± 0.016 0.83 ¬± 0.005 -</cell><cell>0.66 ¬± 0.137</cell></row><row><cell>IMDB-B</cell><cell cols="4">0.66 ¬± 0.012 0.81 ¬± 0.008 0.81 ¬± 0.008 -</cell><cell>-</cell></row><row><cell>PNA</cell><cell>3 Layer</cell><cell>4 Layer</cell><cell>5 Layer</cell><cell>GraphCL</cell><cell>InfoGraph</cell></row><row><cell>MUTAG</cell><cell cols="5">0.88 ¬± 0.011 0.88 ¬± 0.010 0.89 ¬± 0.009 0.86 ¬± 0.023 0.90 ¬± 0.014</cell></row><row><cell>PROTEINS</cell><cell cols="5">0.74 ¬± 0.003 0.74 ¬± 0.012 0.74 ¬± 0.005 0.74 ¬± 0.007 0.74 ¬± 0.003</cell></row><row><cell>NCI1</cell><cell cols="5">0.67 ¬± 0.008 0.68 ¬± 0.011 0.68 ¬± 0.010 0.78 ¬± 0.008 0.77 ¬± 0.019</cell></row><row><cell>DD</cell><cell cols="5">0.76 ¬± 0.014 0.76 ¬± 0.002 0.76 ¬± 0.008 0.80 ¬± 0.008 0.76 ¬± 0.006</cell></row><row><cell>REDDIT-B</cell><cell cols="5">0.90 ¬± 0.003 0.88 ¬± 0.014 0.89 ¬± 0.010 0.92 ¬± 0.006 0.92 ¬± 0.006</cell></row><row><cell>IMDB-B</cell><cell cols="5">0.72 ¬± 0.007 0.68 ¬± 0.011 0.68 ¬± 0.010 0.71 ¬± 0.009 0.71 ¬± 0.009</cell></row><row><cell>GCN</cell><cell>3 Layer</cell><cell>4 Layer</cell><cell>5 Layer</cell><cell>GraphCL</cell><cell>InfoGraph</cell></row><row><cell>MUTAG</cell><cell cols="5">0.85 ¬± 0.003 0.85 ¬± 0.004 0.85 ¬± 0.005 0.82 ¬± 0.013 0.85 ¬± 0.003</cell></row><row><cell cols="2">PROTEINS 0.74 GAT 3 Layer</cell><cell>4 Layer</cell><cell>5 Layer</cell><cell>GraphCL</cell><cell>InfoGraph</cell></row><row><cell>MUTAG</cell><cell>0.84</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Document Classification: We use the same augmentations as in Table4. Text-to-Graph augmentations perform synonym replacement as modifying node features.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Selected GCL Frameworks and brief description of augmentations.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/Shen-Lab/GraphCL</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/fanyun-sun/InfoGraph</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://github.com/kavehhassani/mvgrl</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://github.com/jasonwei20/eda-nlp</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://www.dgl.ai</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported by NSF Grant No. IIS1845491, Army Young Investigator Award No. W9-11NF1810397, and Adobe, Amazon, Facebook, and Google faculty awards. We thank Jay Thiagarajan and Mark Heimann for helpful discussions on the project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A LIMITATION (L1):DESTROYING TASK RELEVANT INFORMATION</head><p>In Sec. 3.1, we show that domain agnostic graph augmentations are susceptible to destroying task relevant information. Experimental Setup: We use the same backbone Graph Isomorphism Network ( <ref type="bibr" target="#b61">[62]</ref>) as InfoGraph ( <ref type="bibr" target="#b45">[46]</ref>) and GraphCL ( <ref type="bibr" target="#b65">[66]</ref>) when training all dataset but GOSSIPCOP. The following training configuration is used: Number of Layers = 3, Learning Rate = 0.01, Number of Epochs = 30, Batch-Size = 32, Optimizer = Adam. Models are trained on a Nvidia Tesla K80 GPU. A batch-norm layer is included between the output of the backbone and cross entropy layer. For augmentations, we follow <ref type="bibr" target="#b65">[66]</ref> and stochastically apply node dropping at 20% of graph size and subgraph dropping at 20% of graph size. For GOSSIPCOP, the encoder is based off the implementation in PyTorch Geometric <ref type="bibr" target="#b13">[14]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B LIMITATION (L2): WEAKLY DISCRIMINATIVE REPRESENTATIONS</head><p>In Sec. 3.2, we show that domain agnostic graph augmentations can lead to weakly discriminative representations, where intra-class and inter-class representational similarity is high. Experimental Setup: The following training configuration is used in this experiment, including the additional results below: 3-layer GIN model with hidden dimension, learning rate, and epochs trained of (32, NA, NA) for RAND (Random Initialization), (512,0.001,20) for InfoGraph ( <ref type="bibr" target="#b45">[46]</ref>), and (32,0.01,20) for GraphCL ( <ref type="bibr" target="#b65">[66]</ref>). The Adam optimizer ( <ref type="bibr" target="#b24">[25]</ref>) is used to train all models. A Nvidia Tesla K80 GPU is used to train all models. Results for MVGRL ( <ref type="bibr" target="#b18">[19]</ref>) are not included as we consistently witnessed Out-Of-Memory errors during training (we use a 12-GB GPU). Results are reported over 3 seeds. Note, for the Deezer and Github datasets, we used PNA convolutional layers instead of GIN as training was unstable with GIN.</p><p>Additional Results: Fig. <ref type="figure">5a</ref> includes additional results for PRO-TEINS, NCI1 and DD datasets. Given that the cosine similarity between positive samples is directly maximized by the contrastive loss, the low representational similarity indicates the model has not learned well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C LIMITATION (L3): RANDOM INDUCTIVE BIAS</head><p>In Sec. 3.3, we show that the random inductive bias of GNNs can be quite strong on commonly used benchmarks. Experimental Setup: For all datasets, excluding DEEZER and GITHUB-SGZR, we report results from GraphCL <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr" target="#b45">[46]</ref>. We use the same GIN encoder as GraphCL when reporting the performance of randomly initialized models for these datasets. The results on DEEZER and GITHUB-SGZR are reported using a modified version of the GraphCL GIN encoder where the GIN layer is replaced with a PNA Layer. We then train the modified model according to the default of GraphCL and InfoGraph. On GITHUB-SGZRS, Info-Graph training time on exceeds eights hours using a NVIDIA Tesla P100. Further, we find that the inductive bias of GNNs is strong across different architectures (GraphSAGE, PNA, GCN, and GAT). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DOCUMENT CLASSIFICATION</head><p>In Sec. 4.1, we demonstrate the benefits of using context-aware augmentations on a graph-based text classification task. Experimental Setup:We use the model, code base and default settings of <ref type="bibr" target="#b35">[36]</ref>. Models are trained using Adam: lr = 0.001, weightdecay = 1e-4 and cosine scheduler (T=8). We use the code 4 and augmentations by <ref type="bibr" target="#b58">[59]</ref>. Synonym replacement, random deletion, random insertion and random swapping are applied at 5%, 10%, 5%, 5% of sentence length respectively. We generate an augmented version of each sentence for every training epoch. For domain agnostic augmentations, we apply random node dropping (10%) to generate one view. The other view is generated by applying random node dropping (10%) or subgraph dropping (10%).</p><p>As noted in Sec. 4.1, natural language augmentations can be directly approximated in graph space. We provide proof of concept using the synonym replacement augmentation. In Table <ref type="table">.</ref> 7, results are reported for a model trained with synonym replacement and graph space equivalent, node swapping at 5%. This model achieves comparable accuracies to the original context aware augmentations. We suspect that synonym replacement is crucial for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E SUPER-PIXEL CLASSIFICATION</head><p>In Sec. 4.2, we demonstrate the benefits of using context-aware augmentations via a case study on MNIST superpixel classification.</p><p>Experimental Setup: 50K images are used for training, 10K for validation, and 10K for testing. We follow the same procedure as <ref type="bibr" target="#b12">[13]</ref> to convert images to superpixel graphs: SLIC ( <ref type="bibr" target="#b0">[1]</ref>) is used to extract superpixels from the image. Then, a ùëò-nearest neighbor graph is constructed between the superpixels. Node features are RGB values and (ùë•, ùë¶) coordinates of superpixels. Classification is performed using three contrastive learning frameworks: SimSiam ([8]), SimCLR ( <ref type="bibr" target="#b6">[7]</ref>), and BYOL ( <ref type="bibr" target="#b14">[15]</ref>). The same hyper-parameters and architecture are used for all frameworks. Specifically, we use a 5-Layer GIN model closely following <ref type="bibr" target="#b12">[13]</ref>. This model is converted from DeepGraphLibrary 5 to Pytorch Geometric ( <ref type="bibr" target="#b13">[14]</ref>). The following hyper-parameters are used: LR=5e-4, Hidden-Dim =110, Epochs=80, Batch-size = 128. The Adam ( <ref type="bibr" target="#b24">[25]</ref>) Optimizer is used for training. The projector is a 2-layer MLP. The predictor is a 2-layer MLP. Predictor hidden dimension is 1028. Bottleneck dimension is 128. Results are reported over 3 seeds. Domain agnostic graph</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC Superpixels Compared to State-of-the-Art Superpixel Methods</title>
		<author>
			<persName><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Appu</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur√©lien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>S√ºsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Investigating the Role of Negatives in Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Jordan</forename><forename type="middle">T</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surbhi</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network similarity via multiple social theories</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Berlingerio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Social Networks Analysis and Mining</title>
				<editor>
			<persName><forename type="first">Jon</forename><forename type="middle">G</forename><surname>Rokne</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1439" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rumor Detection on Social Media with Bi-Directional Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="549" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Sch√∂nauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Thirteenth International Conference on Intelligent Systems for Molecular Biology</title>
				<meeting>Thirteenth International Conference on Intelligent Systems for Molecular Biology</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/chen20j.html" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring Simple Siamese Representation Learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2020. CoRR abs/2011.10566. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An Empirical Study of Training Self-Supervised Vision Transformers</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2104.02057" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021. IEEE, 9726-9735</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Principal Neighbourhood Aggregation for Graph Nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">User Preference-aware Fake News Detection</title>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichao</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>G√≥mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al√°n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Benchmarking Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Prakash Dwivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast Graph Representation Learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bootstrap Your Own Latent -A New Approach to Self-Supervised Learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altch√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>√Åvila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R√©mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Web Page Classification Based on Graph Neural Network</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baojiang</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovative Mobile and Internet Services in Ubiquitous Computing</title>
				<editor>
			<persName><forename type="first">Leonard</forename><surname>Barolli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kangbin</forename><surname>Yim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsing-Chung</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="188" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Continual Learning for Fake News Detection from Social Media</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanika</forename><surname>Karunasekera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Leckie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2021 -30th International Conference on Artificial Neural Networks, series = Lecture Notes in Computer Science, volume = 12892</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="372" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Contrastive Multi-View Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khas</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00975" />
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comprehensive Study on Molecular Supervised Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Doyeong</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soojung</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchan</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung-Hoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanseok</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyeol</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seongok</forename><surname>Ryu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Inf. Model</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Self-supervised Graph Neural Networks without explicit negative sampling</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zekarias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarunas</forename><surname>Kefato</surname></persName>
		</author>
		<author>
			<persName><surname>Girdzijauskas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Understanding Attention and Generalization in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">FLAG: Adversarial Data Augmentation for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Subgraph Matching Kernels for Attributed Graphs</title>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph Self-Supervised Learning: A Survey</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00111</idno>
		<ptr target="https://arxiv.org/abs/2103.00111" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Affinity and Diversity: Quantifying Mechanisms of Data Augmentation</title>
		<author>
			<persName><forename type="first">Gontijo</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvia</forename><forename type="middle">J</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><forename type="middle">D</forename><surname>Smullin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Dyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tom√°s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damon</forename><surname>Mannion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06673[cs.SI]</idno>
		<title level="m">Fake News Detection on Social Media using Geometric Deep Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">TUDataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<ptr target="www.graphlearning.io" />
	</analytic>
	<monogr>
		<title level="m">ICML 2020 Workshop on Graph Representation Learning and Beyond</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Message Passing Attention Networks for Document Understanding</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning</title>
		<author>
			<persName><forename type="first">Kento</forename><surname>Nozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Issei</forename><surname>Sato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On Variational Bounds of Mutual Information</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.06922" />
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Demystifying Contrastive Self-Supervised Learning: Invariances, Augmentations and Dataset Biases</title>
		<author>
			<persName><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Karate Club: An API Oriented Open-source Python Framework for Unsupervised Learning on Graphs</title>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</title>
				<meeting>the 29th ACM International Conference on Information and Knowledge Management (CIKM &apos;20)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3125" to="3132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Almost Free Inductive Embeddings Out-Perform Trained Graph Neural Networks in Graph Classification in a Range of Benchmarks</title>
		<author>
			<persName><forename type="first">Vadeem</forename><surname>Safronov</surname></persName>
		</author>
		<ptr target="https://towardsdatascience.com/almost-free-inductive-embeddings-out-" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
	<note>performtrained-graph-neural-networks-in-graph-classification-651ace368bc1</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weisfeiler-Lehman Graph Kernels</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jan Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">FakeNewsNet: A Data Repository with News Content, Social Context, and Spatiotemporal Information for</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Mahudeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Studying Fake News on Social Media. Big Data</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">InfoGraph: Unsupervised and Semi-supervised Graph-Level Representation Learning via Mutual Information Maximization</title>
		<author>
			<persName><forename type="first">Fan-Yun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adversarial Graph Augmentation to Improve Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Susheel</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Neville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.05819</idno>
		<ptr target="https://arxiv.org/abs/2106.05819" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Petar Velickovic, and Michal Valko. 2021. Bootstrapped Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R√©mi</forename><surname>Munos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bootstrapped Representation Learning on Graphs</title>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Thakoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R√©mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06514</idno>
		<ptr target="https://arxiv.org/abs/2102.06514" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contrastive Multiview Coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">What Makes for Good Views for Contrastive Learning?</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On Mutual Information Maximization for Representation Learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">K</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Representation Learning with Contrastive Predictive Coding</title>
		<author>
			<persName><forename type="first">A√§ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>St√©fan Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Sch√∂nberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran√ßois</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuelle</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName><surname>Gouillart</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Tony Yu, and the scikit-image contributors. 2014. scikit-image: image processing in Python</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li√≤</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
		<ptr target="https://openreview.net/forum?id=rklz9iAcKQ" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Julius Von K√ºgelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luigi</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><surname>Locatello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04619</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Comparison of Descriptor Spaces for Chemical Compound Retrieval and Classification</title>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDM.2006.39</idno>
		<ptr target="https://doi.org/10.1109/ICDM.2006.39" />
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Towards Understanding the Feature Learning Process of Self-supervised Contrastive Learning</title>
		<author>
			<persName><forename type="first">Zixin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">What Should Not Be Contrastive in Contrastive Learning</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=CZ8Y3NzuVzO" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
				<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">How Powerful are Graph Neural Networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Selfsupervised Graph-level Representation Learning with Local and Global Structure</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep Graph Kernels</title>
		<author>
			<persName><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2783258.2783417</idno>
		<ptr target="https://doi.org/10.1145/2783258.2783417" />
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning Automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR, 12121-12132</idno>
		<ptr target="http://proceedings.mlr.press/v139/you21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
				<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<date type="published" when="2021-07-24">2021. 18-24 July 2021</date>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
	<note>Virtual Event (Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">When Does Self-Supervision Help Graph Convolutional Networks? CoRR abs</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09136</idno>
		<ptr target="https://arxiv.org/abs/2006.09136" />
		<imprint>
			<date type="published" when="2006">2020. 2006. 2020</date>
			<biblScope unit="page">9136</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Generative Image Inpainting with Contextual Attention</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07892</idno>
		<ptr target="http://arxiv.org/abs/1801.07892" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Comparing stars: On approximating graph edit distance</title>
		<author>
			<persName><forename type="first">Zhiping</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">Kh</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueli</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhen</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Data Augmentation for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><forename type="middle">J</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Graph Contrastive Learning with Adaptive Augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Roland</forename><forename type="middle">S</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<title level="m">Contrastive Learning Inverts the Data Generating Process</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Prioritizing network communities</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rok</forename><surname>Sosiƒç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<publisher>KNN Acc. (BYOL) KNN Acc</publisher>
		</imprint>
	</monogr>
	<note>Augmentation (SimSiam)</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<idno>84.67 ¬± 1.57 77.96 ¬± 2.04</idno>
		<title level="m">Graph Space (ws = 4)</title>
				<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
