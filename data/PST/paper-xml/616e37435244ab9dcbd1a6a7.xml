<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONTROL PREFIXES for Text Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-15">15 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jordan</forename><surname>Clive</surname></persName>
							<email>jordan.clive19@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>London, UK</roleName><forename type="first">Kris</forename><forename type="middle">Cao</forename><surname>Deepmind</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marek</forename><surname>Rei</surname></persName>
							<email>marek.rei@imperial.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONTROL PREFIXES for Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-15">15 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.08329v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompt learning methods adapt pre-trained language models to downstream applications by using a task-specific prompt together with the input. Most of the current work on prompt learning in text generation relies on a shared dataset-level prompt for all examples in the dataset. We extend this approach and propose a dynamic method, CONTROL PREFIXES, which allows for the inclusion of conditional input-dependent information in each prompt. CONTROL PREFIXES is at the intersection of prompt learning and controlled generation, empowering the model to have finer-grained control during text generation. The method incorporates attribute-level learnable representations into different layers of a pre-trained transformer, allowing for the generated text to be guided in a particular direction. We provide a systematic evaluation of the technique and apply it to five datasets from the GEM benchmark for natural language generation (NLG). We present state-of-the-art results on several data-to-text datasets, including WebNLG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, approaches in text generation have been dominated by adapting one large-scale, pre-trained language model (PLM) to various downstream tasks. Such adaptation is often performed via finetuning, which necessitates updating and storing all of the parameters, resulting in multiple new language models (LMs), one for each task. This poses a considerable deployment challenge as the scale of PLMs continues to climb from millions to billions of parameters. Moreover, full fine-tuning has been shown to be unnecessarily profligate through overwriting natural language understanding (NLU) that could otherwise be shared among tasks <ref type="bibr" target="#b41">(Peters et al., 2019)</ref>; it has also been shown that fine-tuned networks do not deviate substantially from the pretrained one in parameter space <ref type="bibr" target="#b1">(Aghajanyan et al., 2020;</ref><ref type="bibr" target="#b43">Radiya-Dixit and Wang, 2020)</ref>, implying the existence of parameter efficient alternatives.</p><p>Many researchers have sought to alleviate these issues by using fixed-LM techniques, where all the parameters of the base LM always remain unchanged. An ever-growing subsection of these methods can be classed under prompt learning, where language models are adapted to downstream tasks with the aid of a prompt accompanying the input. A recent survey on prompt learning <ref type="bibr" target="#b29">(Liu et al., 2021a)</ref>, however, notes the dearth of research exploring dynamic prompts, which are inputdependent. This work considers dynamic prompts, and is inspired by how traditional controlled generation methods utilize controllable attributes to generate target sentences with desired qualities. Existing controlled generation techniques either aim to generate text with specific target qualities, independent of overall task performance or are methods that have the benefit of updating not only the attributelevel parameters, but adjusting, at the same time, all the LM parameters.</p><p>We propose the dynamic prompting method CONTROL PREFIXES, which extends prefix-tuning. The prefix-tuning method integrates static taskspecific prompts at every layer of a model, adding only 0.1-2% additional parameters to the base LM. With CONTROL PREFIXES we aim to preserve the fixed-LM property, while also allowing datapointspecific attributes to act as guidance signals at the input-level. This is done by employing modular control prefixes, which change alongside the input according to the guidance signal. Operating together with the static prompt parameters, these dynamic prompts can steer the frozen PLM to extend finer-grained control. The chosen attributes can provide additional information about the input, for example the domain of a data-to-text tripleset, or it can specify some aspect of the desired output, such as the target length for text simplification.</p><p>We evaluate our method on an array of text generation tasks, leveraging additional input-level in-formation specific to each dataset. Our results show that our fixed-LM architecture outperforms previous approaches, usually based on fine-tuning, according to the WebNLG <ref type="bibr" target="#b13">(Gardent et al., 2017)</ref>, <ref type="bibr">DART (Radev et al., 2020)</ref> and E2E Clean <ref type="bibr" target="#b12">(Du≈°ek et al., 2019)</ref> data-to-text datasets. In addition, our method attains higher human-assessed performance than existing systems for summarization. This work establishes that the parameters learnt, corresponding to similar labels of a single attribute, share properties. We also demonstrate that zeroshot learning with CONTROL PREFIXES can be effective for conditioning on input-level information previously unseen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Prompt Learning Prompt learning <ref type="bibr" target="#b29">(Liu et al., 2021a;</ref><ref type="bibr" target="#b49">Sun et al., 2021;</ref><ref type="bibr" target="#b46">Schick and Schutze, 2021)</ref> is a nascent field, instigated by the arrival of GPT-3 <ref type="bibr" target="#b5">(Brown et al., 2020)</ref>, involving task-specific adaptation of large LMs via prepending an instruction. Several successive works <ref type="bibr" target="#b31">(Logeswaran et al., 2020;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b25">Lester et al., 2021)</ref> employ prompt-embedding tuning, which trains continuous embeddings prepended to the input embeddings. Li and Liang (2021) discovered that prefix-tuning was more effective than prompt-embedding tuning for text generation. In prefix-tuning, additional trainable key-value pairs, which are fixed across all examples, are used to augment the left context in every attention computation. Therefore, the prompt has constituents at every layer rather than being confined to steer the frozen LM only through the input as in embedding tuning.</p><p>Controlled generation A complementary field to prompt learning is controlled generation, which aims to incorporate various types of guidance (e.g. length specifications <ref type="bibr" target="#b22">(Kikuchi et al., 2016)</ref> or highlighted phrases <ref type="bibr" target="#b15">(Grangier and Auli, 2018)</ref> beyond the input text into the generation model. <ref type="bibr" target="#b19">Johnson et al. (2016)</ref> successfully trained a multilingual translation model with control tokens to encode each language. <ref type="bibr" target="#b20">Keskar et al. (2019)</ref> pre-trained a 1.63B parameter model, also alongside conditional control tokens, and demonstrated these learnt to govern style, content, and task-specific behaviour. However, these examples are undesirable in not being fixed-LM techniques-the whole underlying LM can adapt alongside the control tokens.</p><p>Alternatives exist, such as plug-and-play perturbations of the LM hidden states towards a target at-tribute <ref type="bibr" target="#b37">(Nguyen et al., 2016;</ref><ref type="bibr" target="#b10">Dathathri et al., 2020)</ref>. These methods are fixed-LM and are able to control target qualities such as sentiment and topic. However, they are slow at inference time due to requiring multiple passes for a single batch. The shift in conditional probability can also lead to text degeneration <ref type="bibr" target="#b17">(Holtzman et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic prompts</head><p>There have been few works exploring dynamic prompts <ref type="bibr" target="#b29">(Liu et al., 2021a;</ref><ref type="bibr" target="#b51">Tsimpoukelli et al., 2021)</ref>, which are inputdependent. Perhaps most similar to our work is work by <ref type="bibr" target="#b54">Yu et al. (2021)</ref>, who use an attribute alignment function to form dynamic prompts. Unlike our work, the prompt does not have a static component and aims to generate text with specific target attributes, independent of task performance. With CONTROL PREFIXES, the intention is to also maximize task-specific performance, which is why we maintain a large static prompt component to specify the task itself.</p><p>Auxiliary scaffold tasks Incorporating auxiliary scaffold tasks via multitask learning has been previously used for improving span-labeling and text classification <ref type="bibr" target="#b50">(Swayamdipta et al., 2018;</ref><ref type="bibr" target="#b9">Cohan et al., 2019)</ref>. <ref type="bibr" target="#b7">Cachola et al. (2020)</ref> demonstrate that control tokens can be used to effectively incorporate scaffold tasks alongside the main task for BART LARGE . Inspired by this form of data augmentation, we apply a similar procedure with CONTROL PREFIXES when training on DART, a dataset formed from an accumulation of heterogeneous sub-datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONTROL PREFIXES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>This work considers sequence-to-sequence tasks where the objective is to model the conditional probability P (Y | X) with X and Y representing the tokenized input and output sequences respectively. For example, in summarization, X is an article and Y is a short target summary.</p><p>To model P (Y | X), this paper adopts T5-large <ref type="bibr" target="#b44">(Raffel et al., 2020)</ref> or BART LARGE <ref type="bibr" target="#b26">(Lewis et al., 2020)</ref> as the underlying pre-trained LM with parameters œÜ; and as we consider fixed-LM methods, œÜ always remains frozen. These models are Transformer encoder-decoder models where decoding proceeds auto-regressively. Let us denote d to represent the hidden state dimension and L the number of layers. We use (E, Dc, Dm) to de-1 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0</p><formula xml:id="formula_0">C e Z F 3 D Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt;</formula><p>General Task Prefix (400k -8M params) P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0    </p><formula xml:id="formula_1">C e Z F 3 D Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 2 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y</formula><formula xml:id="formula_2">Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y</formula><formula xml:id="formula_3">Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y</formula><formula xml:id="formula_4">Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y</formula><formula xml:id="formula_5">Y Z G v / j V G 0 Q s C b l C J q k x X c + N 0 U + p R s E k n x V 6 i e E x Z R M 6 4 l 1 L F Q 2 5 8 d P F u T N y Y Z U B G U b a l k K y U H 9 P p D Q 0 Z h o G t j O k O D a r 3 l z 8 z + s m O L z x U 6 H i B L l i y 0 X D R B K M y P x 3 M h C a M 5 R T S y j T w t 5 K 2 J h q y t A m V L A h e K s v r 5 N W t e J d V a o P 1 V L t L o s j D 2 d w D p f g w T X U 4 B 4 a 0 A Q G E 3 i G V 3 h z Y u f F e X c</formula><p>+ l q 0 5 J 5 s 5 h T 9 w P n 8 A U f e O 5 Q = = &lt; / l a t e x i t &gt; C A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d p 8 I + 3 A y s / I d W I z T g j 7 N a T x L X s E <ref type="table" target="#tab_7">+ l q 0 5 J 5 s 5 h T 9 w P n 8 A U f e O 5</ref>  <ref type="table" target="#tab_7">4 B w u 4 A o 8 u I E 6 3 E M T W s B g A s / w C m 9 O 4 r w 4 7 8 7 H s</ref>  <ref type="table" target="#tab_5">u u j e t + O 7 m t 7 Z 3 d v f x + 4 e D w 6 P i k e H r W 1 l</ref>  <ref type="table" target="#tab_5">u u j e t + O 7 m t 7 Z 3 d v f x + 4 e D w 6 P i k e H r W 1 l</ref> </p><formula xml:id="formula_6">= " &gt; A A A B 7 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t E R p L D G R j w Q u Z G 9 Z Y M P e 3 m V 3 z o R c + B E 2 F h p j 6 + + x 8 9 + 4 w B U K v m S S l / d m M j M v i K U w 6 L r f T m 5 j c 2 t 7 J 7 9 b 2 N s / O D w q H p + 0 T J R o x p s s k p H u B N R w K R R v o k D J O 7 H m N A w k b w e T + t x v P 3 F t R K Q e c R p z P 6 Q j J Y a C U b R S u 1 7 u p 7 e z c r 9 Y c i v u A m S d e B k p Q Y Z G v / j V G 0 Q s C b l C J q k x X c + N 0 U + p R s E k n x V 6 i e E x Z R M 6 4 l 1 L F Q 2 5 8 d P F u T N y Y Z U B G U b a l k K y U H 9 P p D Q 0 Z h o G t j O k O D a r 3 l z 8 z + s m O L z x U 6 H i B L l i y 0 X D R B K M y P x 3 M h C a M 5 R T S y j T w t 5 K 2 J h q y t A m V L A h e K s v r 5 N W t e J d V a o P 1 V L t L o s j D 2 d w D p f g w T X U 4 B 4 a 0 A Q G E 3 i G V 3 h z Y u f F e X c</formula><formula xml:id="formula_7">Q = = &lt; / l a t e x i t &gt; C C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L I J i s c L g h W N Q d 8 b o 2 O K S D y c K D o = " &gt; A A A B 7 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B l M Y x n B f E B y h L 3 N X r J k b + / Y n R P C k R 9 h Y 6 G I r b / H z n / j J r l C E x 8 M P N 6 b Y W Z e k E h h 0 H W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l T q M y y B q z y q B U d q v u A m S d e D k p Q 4 7 m o P T V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n x X 7 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F u T N y a Z U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D G / 9 T K g k R a 7 Y c l G Y S o I x m f 9 O h k J z h n J q C W V a 2 F s J G 1 N N G d q E i j Y E b / X l d d K u V b 3 r a u 2 h V q 7 f 5 X E U</formula><formula xml:id="formula_8">n X D y W f O 4 A + c z x 9 V A 4 7 n &lt; / l a t e x i t &gt; C B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 6 a Q c a O G r 1 e 7 o H x F + i h R Q 5 B w I t g = " &gt; A A A B 7 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T Q W G I i H w l c y N 4 y w I a 9 v c v u n g m 5 8 C N s L D T G 1 t 9 j 5 7 9 x g S s U f M k k L + / N Z G Z e E A</formula><formula xml:id="formula_9">G i G L Z Y J C L V D a h G w S W 2 D D c C u 7 F C G g Y C O 8 G 0 s f A 7 T 6 g 0 j + S j m c X o h 3 Q s + Y g z a q z U a Z Q H a X 1 e H h R L b s V d g m w S L y M l y N A c F L / 6 w 4 g l I U r D B N W 6 5 7 m x 8 V O q D G c C 5 4 V + o j G m b E r H 2 L N U 0 h C 1 n y 7 P n Z M r q w z J K F K 2 p C F L 9 f d E S k O t Z 2 F g O 0 N q J n r d W 4 j / e b 3 E j O 7 8 l M s 4 M S j Z a t E o E c R E Z P E 7 G X K F z I i Z J Z Q p b m 8 l b E I V Z c Y m V L A h e O s v b 5 J 2 t e L d V K o P 1 V K t n s W R h w u 4 h G v w 4 B Z q c A 9 N a A G D K T z D K 7 w 5 s f P i v D s f q 9 a c k 8 2 c w x 8 4 n z 9 T f Y 7 m &lt; / l a t e x i t &gt; C B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 6 a Q c a O G r 1 e 7 o H x F + i h R Q 5 B w I t g = " &gt; A A A B 7 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T Q W G I i H w l c y N 4 y w I a 9 v c v u n g m 5 8 C N s L D T G 1 t 9 j 5 7 9 x g S s U f M k k L + / N Z G Z e E A</formula><formula xml:id="formula_10">G i G L Z Y J C L V D a h G w S W 2 D D c C u 7 F C G g Y C O 8 G 0 s f A 7 T 6 g 0 j + S j m c X o h 3 Q s + Y g z a q z U a Z Q H a X 1 e H h R L b s V d g m w S L y M l y N A c F L / 6 w 4 g l I U r D B N W 6 5 7 m x 8 V O q D G c C 5 4 V + o j G m b E r H 2 L N U 0 h C 1 n y 7 P n Z M r q w z J K F K 2 p C F L 9 f d E S k O t Z 2 F g O 0 N q J n r d W 4 j / e b 3 E j O 7 8 l M s 4 M S j Z a t E o E c R E Z P E 7 G X K F z I i Z J Z Q p b m 8 l b E I V Z c Y m V L A h e O s v b 5 J 2 t e L d V K o P 1 V K t n s W R h w u 4 h G v w 4 B Z q c A 9 N a A G D K T z D K 7 w 5 s f P i v D s f q 9 a c k 8 2 c w x 8 4 n z 9 T f Y 7 m &lt; / l a t e x i t &gt; C A &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d p 8 I + 3 A y s / I d W I z T g j 7 N a T x L X s E = " &gt; A A A B 7 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t E R p L D G R j w Q u Z G 9 Z Y M P e 3 m V 3 z o R c + B E 2 F h p j 6 + + x 8 9 + 4 w B U K v m S S l / d m M j M v i K U w 6 L r f T m 5 j c 2 t 7 J 7 9 b 2 N s / O D w q H p + 0 T J R o x p s s k p H u B N R w K R R v o k D J O 7 H m N A w k b w e T + t x v P 3 F t R K Q e c R p z P 6 Q j J Y a C U b R S u 1 7 u p 7 e z c r 9 Y c i v u A m S d e B k p Q Y Z G v / j V G 0 Q s C b l C J q k x X c + N 0 U + p R s E k n x V 6 i e E x Z R M 6 4 l 1 L F Q 2 5 8 d P F u T N y Y Z U B G U b a l k K y U H 9 P p D Q 0 Z h o G t j O k O D a r 3 l z 8 z + s m O L z x U 6 H i B L l i y 0 X D R B K M y P x 3 M h C a M 5 R T S y j T w t 5 K 2 J h q y t A m V L A h e K s v r 5 N W t e J d V a o P 1 V L t L o s j D 2 d w D p f g w T X U 4 B 4 a 0 A Q G E 3 i G V 3 h z Y u f F e X c + l q 0 5 J 5 s 5 h T 9 w P n 8 A U f e O 5 Q = = &lt; / l a t e x i t &gt; C C &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X L I J i s c L g h W N Q d 8 b o 2 O K S D y c K D o = " &gt; A A A B 7 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N Y i J Y h b t Y a B l M Y x n B f E B y h L 3 N X r J k b + / Y n R P C k R 9 h Y 6 G I r b / H z n / j J r l C E x 8 M P N 6 b Y W Z e k E h h 0 H W / n Y 3 N r e 2 d 3 c J e c f / g 8 O i 4 d H L a N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l T q M y y B q z y q B U d q v u A m S d e D k p Q 4 7 m o P T V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n x X 7 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F u T N y a Z U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D G / 9 T K g k R a 7 Y c l G Y S o I x m f 9 O h k J z h n J q C W V a 2 F s J G 1 N N G d q E i j Y E b / X l d d K u V b 3 r a u 2 h V q 7 f 5 X E U 4 B w u 4 A o 8 u I E 6 3 E M T W s B g A s / w C m 9 O 4 r w 4 7 8 7 H s n X D y W f O 4 A + c z x 9 V A 4 7 n &lt; / l a t e x i t &gt; C B &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D 6 a Q c a O G r 1 e 7 o H x F + i h R Q 5 B w I t g = " &gt; A A A B 7 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T Q W G I i H w l c y N 4 y w I a 9 v c v u n g m 5 8 C N s L D T G 1 t 9 j 5 7 9 x g S s U f M k k L + / N Z G Z e E A u u j e t + O 7 m t 7 Z 3 d v f x + 4 e D w 6 P i k e H r W 1 l G i G L Z Y J C L V D a h G w S W 2 D D c C u 7 F C G g Y C O 8 G 0 s f A 7 T 6 g 0 j + S j m c X o h 3 Q s + Y g z a q z U a Z Q H a X 1 e H h R L b s V d g m w S L y M l y N A c F L / 6 w 4 g l I U r D B N W 6 5 7 m x 8 V O q D G c C 5 4 V + o j G m b E r H 2 L N U 0 h C 1 n y 7 P n Z M r q w z J K F K 2 p C F L 9 f d E S k O t Z 2 F g O 0 N q J n r d W 4 j / e b 3 E j O 7 8 l M s 4 M S j Z a t E o E c R E Z P E 7 G X K F z I i Z J Z Q p b m 8 l b E I V Z c Y m V L A h e O s v b 5 J 2 t e L d V K o P 1 V K t n s W R h w u 4 h G v w 4 B Z q c A 9 N a A G D K T z D K 7 w 5 s f P i v D s f q 9 a</formula><p>c k 8 2 c w x 8 4 n z 9 T f Y 7 m &lt; / l a t e x i t &gt; hX, Y, Gi &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K l 0 Z E 5 h y B P x a b Y I S 6 w e K y t h R S 5 0   note the three classes of attention present in each layer: self-attention in the encoder (E), decoder cross-attention (Dc) and decoder masked-attention (Dm). For an attention computation in the l-th layer, the query, key and value matrices are denoted Q l ‚àà R N √ód , and K l , V l ‚àà R M √ód , where N is the number of tokens in the series relating to queries, and M is the number of tokens in the series relating to keys and values.</p><formula xml:id="formula_11">= " &gt; A A A C C n i c b V A 9 S w N B E N 2 L 3 / H r 1 N J m N Q g W I d x F Q c u g h Z Y K x k R y I e x t 5 p I l e 3 v H 7 p w Q Q m o b / 4 q N h S K 2 / g I 7 / 4 2 b m E I T H w w 8 3 p t h Z l 6 Y S m H Q 8 7 6 c 3 N z 8 w u L S 8 k p + d W 1 9 Y 9 P d 2 r 4 1 S a Y 5 V H k i E 1 0 P m Q E p F F R R o I R 6 q o H F o Y R a 2 D s f + b V 7 0 E Y k 6 g b 7 K T R j 1 l E i E p y h l V r u X i A h w k A y 1 Z F A 6 0 V 6 V 6 Q X g R a d L g Z 6 L L b c g l f y x q C z x J + Q A p n g q u V + B u 2 E Z z E o 5 J I Z 0 / C 9 F J s D p l F w C c N 8 k B l I G e + x D j Q s V S w G 0 x y M X x n S A 6 u 0 a Z R o W w r p W P 0 9 M W C x M f 0 4 t J 0 x w 6 6 Z 9 k b i f 1 4 j w + i 0 O R A q z R A U / 1 k U Z Z J i Q k e 5 0 L b Q w F H 2 L W F c C 3 s r 5 V 2 m G U e b X t 6 G 4 E + / P E t u y y X / q F S + P i 5 U z i Z x L J N d s k 8 O i U 9 O S I V c k i t S J Z w 8 k C f y Q l 6 d R + f Z e X P e f 1 p z z m R m h / y B 8 / E N 9 4 q Z z g = = &lt; / l a t e x i t &gt; hX, Y i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G H F Y X s G n t U w f 3 + v O 1 n e a Y 9 T / v 0 Q = " &gt; A A A C B 3 i c b V D L S g N B E J y N r x h f U Y + C D A b B g 4 T d K O g x 6 M V j B P O Q b A i z k 9 5 k y O z s M t M r h J C b F 3 / F i w d F v P o L 3 v w b J 4 + D J h Y 0 F F X d d H c F i R Q G X f f b y S w t r 6 y u Z d d z G 5 t b 2 z v 5 3 b 2 a i V P N o c p j G e t G w A x I o a C K A i U 0 E g 0 s C i T U g / 7 1 2 K 8 / g D Y i V n c 4 S K A V s a 4 S o e A M r d T O H / o S Q v Q l U 1 0 J t H F K 7 3 0 t u j 3 0 9 U R p 5 w t u 0 Z 2 A L h J v R g p k h k o 7 / + V 3 Y p 5 G o J B L Z k z T c x N s D Z l G w S W M c n 5 q I G G 8 z 7 r Q t F S x C E x r O P l j R I + t 0 q F h r G 0 p p B P 1 9 8 S Q R c Y M o s B 2 R g x 7 Z t 4 b i / 9 5 z R T D y 9 Z Q q C R F U H y 6 K E w l x Z i O Q 6 E d o Y G j H F j C u B b 2 V s p 7 T D O O N r q c D c G b f 3 m R 1 E p F 7 6 x Y u j 0 v l K 9 m c W T J A T k i J 8 Q j F 6 R M b k i F V A k n j + S Z</formula><formula xml:id="formula_12">Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y</formula><formula xml:id="formula_13">Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J O E + x E d K h E K R t F K 9 9 V m t V + u u D V 3 D r J K v J x U I E e z X / 7 q D W K W R l w h k 9 S Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intuition</head><p>We believe having fixed PLM parameters that capture broad natural language understanding, shared task-specific parameters which specify the task itself, and attribute-level parameters which integrate input-level information has a range of benefits. The general task-specific parameters, which channel the frozen LM to carry out the overall task, can themselves adapt to modular control prefixes which change according to the guidance signal, for each input X. This demarcation of parameters enables fine-grained control to be extended to aid performance on a downstream task. CONTROL PREFIXES is, therefore, able to leverage input-level information while being a fixed-LM, parameter efficient<ref type="foot" target="#foot_0">1</ref> method. For this work, we only consider attributes as guidance signal which are made up of discrete labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Description</head><p>The idea is to have a general task prefix P Œ∏ ("taskspecific parameters"), as in prefix-tuning which remains static, and train at the same time C Œ∏ ("attribute-level parameters"): a set of prefixes that change depending on the input. This requires attribute-level information or guidance G, to indicate which control prefixes to be used while processing X.<ref type="foot" target="#foot_1">2</ref> Let us consider the parallel corpus Z = X j , Y j , G j j=1,..,N , where G j indicates all the conditional attribute-level information for the sample j. The goal is to optimize through gradient descent the final inference parameters, Œ∏, whilst the underlying œÜ parameters of the pre-trained LM remain frozen:</p><formula xml:id="formula_14">Œ∏ * = arg max Œ∏ N j=1 log p Y j | X j , G j ; P Œ∏ , C Œ∏ , œÜ .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>General Prefix</head><p>For each attention class (E, Dc, Dm), a distinct prefix of key-value pairs is learnt, P = {P 1 , . . . , P L }, where P l ‚àà R œÅ√ó2d ‚àÄl ‚àà {1, . . . , L}. P ‚àà R œÅ√ó2dL and œÅ is the prompt length, i.e. the number of additional key-value pairs in each attention computation. In prefix-tuning<ref type="foot" target="#foot_2">3</ref> , for an attention computation in the l-th layer, K l and V l are augmented to become</p><formula xml:id="formula_15">K l = [P l,K ; K l ] , V l = [P l,V ; V l ]<label>(2)</label></formula><p>where K l , V l ‚àà R (œÅ+M )√ód . The overall general prefix, parameterized by Œ∏, is P Œ∏ = P E , P Dc , P Dm , where P Œ∏ ‚àà R œÅ√ó6dL .</p><p>Control Prefixes Let us consider one attribute with R possible labels<ref type="foot" target="#foot_3">4</ref> , such as the news domain of an article (e.g. sport, technology etc.), C Œ∏ = {C Œ∏,1 , . . . , C Œ∏,R }, where C Œ∏,r ‚àà R œÅc√ó6dL , ‚àÄr ‚àà {1 . . . .R}. C Œ∏,r represents the control prefix learnt for the r-th attribute label and the parameter œÅ c denotes the control prompt length for this particular attribute. Let A be a function which returns the corresponding control prefix for the attribute label indicated by G. In CONTROL PREFIXES the K l and V l are augmented to become</p><formula xml:id="formula_16">K l = [A(G) l,K ; P l,K ; K l ] , V l = [A(G) l,V ; P l,V ; V l ]<label>(3)</label></formula><p>where K l , V l ‚àà R (œÅc+œÅ+M )√ód .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shared Re-parameterization</head><p>As in Li and Liang (2021), optimization is stabilized by an increase in the trainable parameters. However, rather than one network, we use three distinct two-layered large feed-forward neural networks for each attention class, applied row-wise. For each attention class (E, Dc, Dm), P = MLP( P ) where P ‚àà R œÅ√ód is smaller than the matrix P ‚àà R œÅ√ó2dL , and each MLP has an intermediate dimension k which we set to 800. The distinct MLPs and each P are parameterized by training parameters Œ∏; thus, Œ∏ is a function of Œ∏ and |Œ∏| &lt; | Œ∏|. Once training is complete, the final Œ∏ parameters can be saved for use at inference and the re-parameterization parameters dispensed with.</p><p>As described for the general prefix, P Œ∏ , each control prefix, C Œ∏,r , comprises three constituents for each attention class:</p><formula xml:id="formula_17">C Œ∏,r = C E r , C Dc r , C Dm r .</formula><p>The re-parameterization of C Œ∏,r occurs in exactly the same manner as P Œ∏ , sharing the same MLP E , MLP Dc and MLP Dm . When using a disjoint set of re-parameterizations for the control prefixes, learning becomes unstable and performance degrades. <ref type="foot" target="#foot_4">5</ref>Recent work by <ref type="bibr" target="#b6">Buhai et al. (2020)</ref> show that over-parameterization can smooth the optimization landscape. With this in mind, the three distinct re-parameterizations compel each prefix element to coordinate control for the particular attention class. For example, the rows of P E and C E r lie in a vector space better coordinated for moderating the processing of the input sequence X than P Dm and C Dm r . This is due to being formed from the shared mapping MLP E .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets, Guidance and Metrics</head><p>Data-to-text The objective of data-to-text generation is to produce fluent text from structured input, viz. a tripleset (a set of subject-predicateobjects). As in Li and Liang (2021), we elect to evaluate on the data-to-text datasets DART and WebNLG. However, we implement prefix-tuning for T5-large rather than GPT-2; for T5-large provides a much stronger baseline and enables comparison with state-of-the-art (SOTA) systems. <ref type="foot" target="#foot_5">6</ref> Results are also reported on E2E Clean, a dataset solely focused on the restaurant domain. We use the official evaluation scripts and report a selection of BLEU <ref type="bibr" target="#b38">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b24">(Lavie and Agarwal, 2007)</ref>, and TER <ref type="bibr" target="#b49">(Snover et al., 2006)</ref> for each dataset. <ref type="foot" target="#foot_6">7</ref>WebNLG contains triplesets from DBPedia <ref type="bibr" target="#b4">(Auer et al., 2007)</ref>. The test set is divided into two partitions: Seen, which contains 10 DBpedia categories present in the training set, and Unseen, which covers 5 categories never seen during training. <ref type="foot" target="#foot_7">8</ref> These categories, such as Airport or Food are used as guidance signal in our experiments (indicated by A 1 in Table <ref type="table" target="#tab_0">1</ref>); our approach for unseen categories is discussed in ¬ß6.2.</p><p>Providing the category explicitly as guidance with CONTROL PREFIXES may enable inductive biases relating to properties of triples belonging to a specific WebNLG category to be captured more effectively. This idea is encouraged by studies where there is a clear disparity in performance on different categories between different model types <ref type="bibr" target="#b35">(Moryossef et al., 2019;</ref><ref type="bibr" target="#b8">Castro Ferreira et al., 2020)</ref>.</p><p>DART is an open-domain, multi-source corpus, with six sources: internal and external human annotation of both Wikipedia tables and WikiSQl; as well as the two existing datasets WebNLG and E2E Clean. Radev et al. (2020) revealed fine-tuning T5-large on the WebNLG dataset with only the human annotated portion of DART achieves SOTA performance, whilst using the whole DART dataset is not as effective. Nevertheless, this inspired the idea of using the six DART sub-dataset sources as a controllable attribute-represented by A 2 in Table <ref type="table" target="#tab_0">1</ref>-as a data augmentation strategy.</p><p>Simplification We use WikiLarge <ref type="bibr" target="#b56">(Zhang and Lapata, 2017)</ref> as training data and evaluate on the two benchmarks TurkCorpus <ref type="bibr" target="#b53">(Xu et al., 2016)</ref> and ASSET <ref type="bibr" target="#b3">(Alva-Manchego et al., 2020)</ref>. Both benchmarks are composed of the same 2000 validation source and 359 test source sentences. Martin et al. ( <ref type="formula">2020</ref>) introduced 'BART LARGE with AC-CESS', which is a fine-tuned BART LARGE model trained alongside control tokens to condition on four simplification-specific attributes, such as the length compression ratio (the length of the target sequence relative to the source sequence). We use the same controllable attributes in this work to directly compare with <ref type="bibr" target="#b34">Martin et al. (2020)</ref> (Table <ref type="table" target="#tab_1">2</ref>). The control ratios are discretized into bins of fixed-width 0.05, capped to a maximum ratio of 2. At inference time, once the model has been trained with these oracle controls, the control ratios are set to desired values by tuning on the respective validation set.</p><p>We report the non-learned metrics SARI <ref type="bibr" target="#b53">(Xu et al., 2016)</ref> and FKGL <ref type="bibr" target="#b23">(Kincaid et al., 1975)</ref>. <ref type="foot" target="#foot_8">9</ref>Unlike previous studies, we also use the machinelearned Q&amp;A metric QuestEval <ref type="bibr">(Scialom et al., 2021)</ref> to assess our text simplification models.</p><p>Summarization As in Li and Liang (2021), we report results on the XSum dataset <ref type="bibr" target="#b36">(Narayan et al., 2018)</ref> using BART LARGE . XSum comprises 226,711 British Broadcasting Corporation (BBC) articles coupled with their single-sentence summaries-where each sample corresponds to a unique URL. The URL contains information on whether the sub-directory is from the BBC Sport or BBC News page (A 1 in Table <ref type="table">3</ref>), and further subdirectory information (A 2 in Table <ref type="table">3</ref>, where A 2 has 40 labels), for example ('sport', 'formula1') or ('news', 'science'). The motivation for using this as guidance is that different sub-directories are likely to share properties relating to how the information is presented; journalists are also usually confined to one domain. In addition to reporting the cus-tomary ROUGE scores <ref type="bibr" target="#b28">(Lin, 2004)</ref>, we submit our CONTROL PREFIXES model outputs to the GENIE external human evaluation framework <ref type="bibr" target="#b21">(Khashabi et al., 2021)</ref>-where 300 instances are assessed across 5 intrinsic dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Architecture and Hyper-parameters</head><p>All implementations in this study are built on top of the Transformers library <ref type="bibr" target="#b52">(Wolf et al., 2020)</ref>. As T5 has relative position biases, we set these in all layers pertaining to offsets where the key is part of a prefix to zero. For BART LARGE we adapt the original implementation <ref type="bibr" target="#b27">(Li and Liang, 2021)</ref>. For the data-to-text datasets, we follow Ribeiro et al. ( <ref type="formula">2020</ref>) and linearize the triples, prepending the special tokens &lt;H&gt;, &lt;R&gt;, and &lt;T&gt; before the subject, predicate, and object of an individual triple. <ref type="foot" target="#foot_9">10</ref> We also prepend "translate Graph to English: " to every input <ref type="bibr" target="#b44">(Raffel et al., 2020)</ref>.</p><p>The general prompt length and each control prompt length are architecture-specific parameters that we vary to try and maximize performance on the validation set. We use gradient accumulation across batches to maintain an effective batch size above 64, a linear learning rate scheduler for all models and beam-search decoding. The hyperparameters we consider are principally the learning rate and the optimizer: AdamW <ref type="bibr" target="#b32">(Loshchilov and Hutter, 2017)</ref> or AdaFactor <ref type="bibr" target="#b48">(Shazeer and Stern, 2018)</ref>. 11 We chose the checkpoint with the highest validation set score using BLEU for data-to-text, SARI for simplification and ROUGE-2 for summarization. For all tasks, we train our models on single Tesla V100-SXM2-16GB machines, with mixed precision for BART LARGE based models (fp16) and full precision for T5-large based models (fp32).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data-to-Text</head><p>For DART, both CONTROL PREFIXES (A 2 ) and prefix-tuning attain higher performance (Table <ref type="table" target="#tab_0">1</ref>) than the current SOTA-which is T5-large finedtuned <ref type="bibr" target="#b42">(Radev et al., 2020)</ref>-by 1.29 and 0.54 BLEU points respectively. This indicates CON-TROL PREFIXES can extend control of the frozen T5-large more effectively than prefix-tuning.</p><p>The SOTA for WebNLG is a T5-large model fine-tuned on WebNLG and the human annotated portion of <ref type="bibr">DART (Radev et al., 2020)</ref>. CONTROL PREFIXES achieves a 0.83 higher BLEU overall, and 1.33 on the Seen categories than this model. Notably, CONTROL PREFIXES (A 1 ) outperforms CONTROL PREFIXES (A 1 ,A 2 ) on the Seen component of the dataset, but does not generalize as well to the unseen categories. We argue this illustrates the benefit of using both controllable attributes. The prefix-tuning model with additional DART data, like the SOTA, is trained on only the human annotated portion and yields a minor performance increase of 0.05 BLEU compared to prefixtuning solely trained on WebNLG. We believe this indicates that for fine-tuning, training on a complementary type of additional data allows the PLM to maintain more NLU by not over-fitting a narrow distribution. Therefore the LM can generalize better. Whilst for prefix-tuning, much of this gain has already been realized by retaining the original frozen parameters.</p><p>The SOTA <ref type="bibr" target="#b16">(Harkous et al., 2020)</ref> for E2E Clean consists of a fine-tuned GPT-2 with a semantic fidelity classifier trained on additional generated data. CONTROL PREFIXES (A 2 ), which can leverage the heterogeneous DART datasets, outperforms the SOTA in terms of BLEU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Simplification</head><p>Table 2 reveals that prefix-tuning BART performs comparably to when we fine-tune BART. When comparing our CONTROL PREFIXES to fine-tuned 'BART LARGE with ACCESS' there is comparable performance in terms of SARI for ASSET, and better FKGL results. However on TurkCorpus, CON-TROL PREFIXES yields lower performance on average for SARI and FKGL. For text simplification, <ref type="bibr" target="#b34">Martin et al. (2020)</ref> indicate the gains from using the controllable attributes, as assessed by SARI and FKGL, are mostly due to being able to calibrate the length ratio, with validation and test sets being drawn from the same distribution, as opposed to the WikiLarge training distribution. We highlight the Gold Reference score for TurkCorpus, which produces inferior results for SARI and FKGL compared to both guided models. The Gold Reference result is computed via a leave-one-out scenario where each reference is evaluated against all others, and then an average is taken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Summarization</head><p>Our research is not solely focused on parameter efficiency, but more on the effectiveness of adapting an already parameter efficient, fixed-LM method (adding &lt;3% additional parameters). The only way to add parameters with prefix-tuning is to increase the prompt length. XSum is the only dataset considered where performance does not plateau when increasing prompt length<ref type="foot" target="#foot_11">14</ref> , so therefore we ensure CONTROL PREFIXES does not have more parameters than prefix-tuning to ensure a fair comparison. Table <ref type="table">3</ref>: Summarization results on XSum 13 . R-1, R-2 and R-L refer to ROUGE-1, ROUGE-2 and ROUGE-L. The human-assessed results are from the GENIE benchmark, where the 95% confidence intervals are computed with bootstrap re-sampling. Note the BART LARGE fine-tuned results for the human-assessed dimensions are transcribed from <ref type="bibr" target="#b21">Khashabi et al. (2021)</ref>, whilst the automatic metric results, indicated by * , are from <ref type="bibr" target="#b26">Lewis et al. (2020)</ref>. Prefixtuning and CONTROL PREFIXES (A 1 ,A 2 ) use BART LARGE as the fixed LM. A 1 refers to the BBC news/sport page attribute and A 2 the further sub-directory attribute.</p><p>Table <ref type="table">3</ref> shows that despite CONTROL PREFIXES underperforming fine-tuning according to automatic metrics, CONTROL PREFIXES attains higher human-assessed results. CONTROL PREFIXES also holds the highest overall human evaluation ranking on the GENIE platform (higher than T5-large and PEGASUS LARGE <ref type="bibr" target="#b55">(Zhang et al., 2019)</ref> fine-tuned). This study is limited in not being able to compare human-assessment of prefix-tuning, which yields slightly lower ROUGE scores than CONTROL PRE-FIXES, as participants of GENIE are limited to one submission. The confidence intervals indicate that this result is not necessarily definitive-but it at least highlights the problems with evaluation for XSum, and that the quality of generations in this domain is not captured fully with ROUGE. A sample size of 300 is typically much larger than that where authors construct their own evaluation <ref type="bibr" target="#b36">(Narayan et al. (2018)</ref>    <ref type="bibr" target="#b33">(Maaten and Hinton, 2008)</ref> visualizations of the length compression control prefixes learnt as part of our simplification CON-TROL PREFIXES model. <ref type="foot" target="#foot_12">15</ref> We plot only the decoder self-attention constituent of each control prefix (comprising multiple key-value pairs at each layer) as the length ratio directly concerns the target. <ref type="foot" target="#foot_13">16</ref> The relationship learnt by the control prefixes is very manifest-aided by the near uniform distribution of length ratios in the WikiLarge training dataset from 0 to 1.1. Fig. <ref type="figure" target="#fig_8">2</ref> establishes that for this simplistic attribute, different control prefixes corresponding to similar attribute labels (i.e., varying length ratios for the length attribute) share properties. Interestingly the decoder cross-attention of the control prefix is not as manifest. We believe this is due to BART LARGE being accustomed to the same crossattention key-value pairs in each layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Zero-shot Learning</head><p>We argue that even for more complicated attributes, such as the WebNLG category attribute, if the attribute labels are similar, the respective control prefixes will similarly guide both the general, taskspecific prefix parameters and the frozen LM parameters. Previous work has discussed the notion of task similarity <ref type="bibr" target="#b0">(Achille et al., 2019)</ref> for prompt learning methods <ref type="bibr" target="#b25">(Lester et al., 2021)</ref>; however, we argue prefixes concerning different labels of one attribute are more likely to overlap in terms of learnable properties than different tasks or whole datasets.</p><p>In the case of WebNLG, where although no examples of the unseen category are present during training, a textual label for the category exists. 17 This gives us some prior on the properties of the unseen categories, which we show is enough to successfully zero-shot transfer with control prefixes. For each WebNLG model with the category attribute, we map each category's textual label, including for the unseen categories, to a Glove embedding 18 <ref type="bibr" target="#b40">(Pennington et al., 2014)</ref>. Then for each unseen category, we map to the seen category with the highest cosine similarity in embedding space, and use that control prefix at inference for the corresponding unseen sample. For example, the control prefix for the seen category SportsTeam is used for 17 This was available to all competition participants. 18 Glove Common Crawl <ref type="bibr">(840B tokens, 2.2M vocab, cased, 300d vectors)</ref>.</p><p>examples relating to the unseen category Athlete. <ref type="foot" target="#foot_14">19</ref>Table <ref type="table" target="#tab_2">4</ref> shows a comparison of using an out-ofvocabulary (OOV) control prefix for each example with an unseen category, and the zero-shot transfer method for both WebNLG datasets<ref type="foot" target="#foot_15">20</ref> . The OOV control prefix is trained on a random 2% of the data for each accumulated batch. These results indicate that zero-shot transfer is more promising than a learned OOV representation. The result fundamentally depends on the WebNLG categories, and if similar textual labels pertain to similar triplesets that CONTROL PREFIXES can utilize. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We introduce CONTROL PREFIXES, a controlled generation technique, which integrates a taskspecific prompt alongside dynamic prompts to leverage additional input-level information. The method extends prefix-tuning, enabling the model to have finer-grained control over generated text, and assists in maximizing downstream task performance.</p><p>We demonstrate that CONTROL PREFIXES outperforms prefix-tuning, as well as existing approaches, on an array of natural language generation tasks. Our method attains state-of-theart results on several data-to-text datasets including WebNLG. This is despite learning &lt;2% additional parameters to the underlying LM parameters (which remain fixed). Additionally, our method holds the highest human evaluation ranking on the external platform GENIE for the summarization dataset XSum. Fig. <ref type="figure" target="#fig_10">4</ref> depicts the length compression ratio output distribution on the validation set for CONTROL PREFIXES, where a length control prefix of a specific attribute value (0.25,0.5,0.75,1.0) is specified. This clearly demonstrates CONTROL PREFIXES is capable of controlling the target length with respect to the input. Table <ref type="table" target="#tab_0">11</ref> displays example output generations with each of the (0.25,0.5,0.75,1.0) values specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Simplification Length Control</head><p>Fig. <ref type="figure">5</ref> is supplementary to ¬ß6.1, showing all constituents of the length compression control prefixes for all attribute values. In the WikiLarge training data, there are far fewer training samples where the simplified output is much longer than the complex, original input in WikiLarge. This explains why the representations are not as interpretable for values greater than 1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 QuestEval</head><p>The Gold Reference results for QuestEval 21 are higher for TurkCorpus compared to ASSET in Table 2. We argue this is because the test set gold references are on average 114 characters for TurkCorpus, as opposed to 98 for ASSET. Therefore, the ASSET references contain less information to answer the generated queries during QuestEval evaluation; and thus, there is lower performance. We argue this shows a limitation with using QuestEval as a reference-less metric for simplification-by favouring longer generations. 21 Although QuestEval can take references, the authors maintain that any improvement in correlation with human performance is very minor. The dimension represented on the x-axis is stretched from a 1:1 to 2:1 aspect ratio for labelling clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Prefix-tuning + Control Tokens</head><p>We propose another architecture 'prefix-tuning + control tokens', where all of the original LM parameters, œÜ, still remain fixed, including the embedding matrix. Control has to be exerted through the few control embeddings and prefix-tuning's ability to steer the frozen œÜ parameters through &lt; 2% additional parameters. We use this method to inform the model of the same discrete guidance information as in CONTROL PREFIXES, but with control tokens instead of control prefixes. <ref type="foot" target="#foot_16">22</ref> This alter-native method is less expressive than CONTROL PREFIXES, in much the same way as prefix-tuning is more expressive than prompt-embedding tuning. Prefix-tuning + control tokens also does not benefit from the shared re-parameterizations ( ¬ß3.3) that we argue allow for more effective demarcation of control of the fixed LM in each attention class subspace.</p><p>Table <ref type="table" target="#tab_7">9</ref> reveals that CONTROL PREFIXES outperforms prefix-tuning + control tokens on the data-totext and summarization datasets, while the results are both comparable to the Gold References on simplification datasets. This indicates that CONTROL PREFIXES is better able to integrate and leverage guidance signal at the input-level, whilst maintaining the fixed-LM property, than prefix-tuning + control tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Varying Prompt Length</head><p>The only way to add parameters with prefix-tuning is by increasing prompt length. Fig. <ref type="figure">6</ref> illustrates how performance saturation is observed-after a certain prompt length performance plateaus. Different datasets require varying prompt lengths to attain near maximum performance in a parameter search for prompt length. For the data-to-text datasets, near maximum performance (&gt;99% of the maximum validation score in the search) is reached with a prompt length of 1 or 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Qualitative Examples</head><p>For data-to-text, Table <ref type="table" target="#tab_0">13</ref> displays example CON-TROL PREFIXES output for WebNLG input belonging to unseen categories, along with the zero-shot procedure. Table <ref type="table" target="#tab_0">13</ref> depicts example CONTROL PREFIXES (A 1 ,A 2 ) output alongside prefix-tuning model output for WebNLG+ 2020 input. For simplification, Table <ref type="table" target="#tab_1">12</ref> compares the fixed-LM guided generations of CONTROL PREFIXES to the finetuned BART LARGE with ACCESS <ref type="bibr" target="#b34">(Martin et al., 2020)</ref>. For summarization, Table <ref type="table" target="#tab_3">15</ref> depicts cherrypicked CONTROL PREFIXES generated summaries for XSum input, alongside T5-large fine-tuned summaries that have higher ROUGE scores. This is to illustrate how CONTROL PREFIXES can achieve higher human assessment through GENIE than top-performing fine-tuned models, whilst attaining lower automatic metric scores. Figure <ref type="figure">6</ref>: Prefix-tuning results of a model parameter search on several datasets for the optimal prompt length per dataset. These results are for the metric monitored per task on the respective validation sets indicated in the legend. œÜ% denotes the % of additional parameters to the number of fixed-LM parameters required at inference time. The y-axis is a relative measure: the validation set performance as a % of the maximum attained in the parameter search.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 3 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 4 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 5 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " d p 8 I + 3 A y s / I d W I z T g j 7 N a T x L X s E = " &gt; A A A B 7 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t E R p L D G R j w Q u Z G 9 Z Y M P e 3 m V 3 z o R c + B E 2 F h p j 6 + + x 8 9 + 4 w B U K v m S S l / d m M j M v i K U w 6 L r f T m 5 j c 2 t 7 J 7 9 b 2 N s / O D w q H p + 0 T J R o x p s s k p H u B N R w K R R v o k D J O 7 H m N A w k b w e T + t x v P 3 F t R K Q e c R p z P 6 Q j J Y a C U b R S u 1 7 u p 7 e z c r 9 Y c i v u A m S d e B k p Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>v J I 3 5 8 l 5 c d 6 d j 2 l r x p n N 7 J M / c D 5 / A I 8 q m R 0 = &lt; / l a t e x i t &gt; Pre-trained Model (0.4B params) P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 1: High-level diagram contrasting prefix-tuning and CONTROL PREFIXES in the single-task setup for a PLM such as BART LARGE . The same single-task batch (examples 1,2,3,4 and 5) is considered for both setups. Left: Prefix-tuning has one general prefix P for all examples. Right: CONTROL PREFIXES utilizes additional attribute information at the input-level, G, in i). This conditional information is used in ii) to dictate which control prefix (C A , C B , C C ) to use for a particular example in a batch. This takes advantage of prefix-tuning's capacity to include different prefixes in one forward pass.</figDesc><graphic url="image-3.png" coords="3,160.64,145.93,89.14,50.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>use 50 and Dou et al. (2020) use 100).6 Analysis6.1 Visualizing Control Prefixes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 displays t-SNE<ref type="bibr" target="#b33">(Maaten and Hinton, 2008)</ref> visualizations of the length compression control</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 2</head><label>2</label><figDesc>Figure 2: t-SNE visualizations for the decoder selfattention constituent of the length compression control prefixes of the simplification model. Each circle represents a control prefix corresponding to each length ratio (bins of fixed width 0.05, from 0 to 1.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram illustrating the influence of different target length ratios on the actual length compression ratio output distribution for the simplification CONTROL PREFIXES model on the TurkCorpus validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure5: t-SNE visualizations for constituents of the length compression control prefixes learnt as part of the simplification CONTROL PREFIXES model. Each diagram depicts representations of control prefixes corresponding to each length value (41 bins of fixed width 0.05, from 0 to 2) for a particular attention mechanism. The dimension represented on the x-axis is stretched from a 1:1 to 2:1 aspect ratio for labelling clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Data-to-text test set results reported on the respective official evaluation scripts. œÜ% denotes the % of additional parameters to the number of fixed-LM parameters required at inference time. T5-large fine-tuned results for WebNLG are from Ribeiro et al. (2020), and for E2E Clean are calculated from public model outputs<ref type="bibr" target="#b14">(Gehrmann et al., 2021)</ref>. Several of the baseline results were only reported to the significant figures shown. A 1 signifies models trained with control prefixes for the WebNLG category attribute, and A 2 with control prefixes for the DART sub-dataset source attribute. For WebNLG, S, U and A refer to BLEU scores for the Seen, Unseen and All portions of the dataset. The DART results are reported on the official evaluation script for v1.1.1, the same version as the official leaderboard. A CONTROL PREFIXES model attains state-of-the-art results for each dataset 12 .</figDesc><table><row><cell></cell><cell>œÜ%</cell><cell></cell><cell>DART</cell><cell>œÜ%</cell><cell></cell><cell>WebNLG</cell><cell></cell><cell>œÜ%</cell><cell>E2E Clean</cell></row><row><cell></cell><cell></cell><cell cols="3">BLEU METEOR TER ‚Üì</cell><cell>S</cell><cell>U</cell><cell>A</cell><cell>BLEU METEOR</cell></row><row><cell>T5-large fine-tuned</cell><cell cols="2">100 50.66</cell><cell>40</cell><cell cols="5">43 100 64.89 54.01 59.95 100 38.74</cell><cell>37.4</cell></row><row><cell>SOTA</cell><cell cols="2">100 50.66</cell><cell>40</cell><cell cols="5">43 100 65.82 56.01 61.44 100</cell><cell>43.6</cell><cell>39</cell></row><row><cell>Prefix-tuning</cell><cell cols="2">1.0 51.20</cell><cell cols="6">40.62 43.13 1.0 66.95 55.39 61.73 1.0 43.66</cell><cell>39.0</cell></row><row><cell>CONTROL PREFIXES (A 1 )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">-1.4 67.32 55.38 61.94</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>+Data: DART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell cols="2">1.0 51.20</cell><cell cols="6">40.62 43.13 1.0 67.05 55.37 61.78 1.0 43.04</cell><cell>38.7</cell></row><row><cell>CONTROL PREFIXES (A 2 )</cell><cell cols="2">1.1 51.95</cell><cell cols="6">41.07 42.75 1.0 66.99 55.56 61.83 1.0 44.15</cell><cell>39.2</cell></row><row><cell>CONTROL PREFIXES (A 1 ,A 2 )</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="4">-1.4 67.15 56.41 62.27</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Simplification results on ASSET and TurkCorpus test sets. ‚Ä† This model is from<ref type="bibr" target="#b34">Martin et al. (2020)</ref>, where the authors fine-tuned BART LARGE model alongside control tokens for the four attributes. The CONTROL PREFIXES model is trained with control prefixes for these same four attributes. Prefix-tuning and CONTROL PREFIXES use BART LARGE as the fixed LM. The * denotes baseline results calculated in this study-the model outputs of<ref type="bibr" target="#b34">Martin et al. (2020)</ref> are publicly available. The BART LARGE with ACCESS and CONTROL PREFIXES model are the average test set results over 5 random seeds.</figDesc><table><row><cell></cell><cell></cell><cell>œÜ%</cell><cell></cell><cell cols="2">ASSET</cell><cell></cell><cell cols="3">TurkCorpus</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">SARI FKGL ‚Üì QuestEval</cell><cell cols="3">SARI FKGL ‚Üì QuestEval</cell></row><row><cell>Gold Reference</cell><cell></cell><cell>-</cell><cell>44.87</cell><cell cols="2">6.49</cell><cell>0.63  *</cell><cell>40.04</cell><cell>8.77</cell><cell>0.66  *</cell></row><row><cell cols="3">BART LARGE with ACCESS  ‚Ä† 100</cell><cell>43.63</cell><cell cols="2">6.25</cell><cell>0.64  *</cell><cell>42.62</cell><cell>6.98</cell><cell>0.66  *</cell></row><row><cell cols="2">BART LARGE fine-tuned</cell><cell cols="2">100 39.91  *</cell><cell cols="2">7.73  *</cell><cell cols="2">-39.55  *</cell><cell>7.73  *</cell><cell>-</cell></row><row><cell>Prefix-tuning</cell><cell></cell><cell>1.8</cell><cell>40.12</cell><cell cols="2">7.28</cell><cell>-</cell><cell>39.06</cell><cell>7.28</cell><cell>-</cell></row><row><cell>CONTROL PREFIXES</cell><cell></cell><cell>1.8</cell><cell>43.58</cell><cell cols="2">5.97</cell><cell>0.64</cell><cell>42.32</cell><cell>7.74</cell><cell>0.66</cell></row><row><cell>œÜ%</cell><cell>Human overall</cell><cell cols="2">Human conciseness</cell><cell>Human fluency</cell><cell cols="2">Human no-hallucination</cell><cell cols="2">Human informativeness</cell><cell>R-1</cell><cell>R-2</cell><cell>R-L</cell></row><row><cell cols="10">BART LARGE fine-tuned 45.14  Prefix-tuning 100 0.49 +0.03 ‚àí0.04 0.50 +0.03 ‚àí0.03 0.50 +0.03 ‚àí0.03 0.52 +0.03 ‚àí0.03 0.49 +0.03 ‚àí0.03 3.0 -----43.53</cell><cell>20.66</cell><cell>35.63</cell></row><row><cell cols="2">CONTROL PREFIXES (A 1 , A 2 ) 2.8 0.51 +0.03 ‚àí0.03</cell><cell cols="2">0.53 +0.02 ‚àí0.02</cell><cell>0.51 +0.03 ‚àí0.03</cell><cell></cell><cell>0.53 +0.03 ‚àí0.03</cell><cell>0.49 +0.03 ‚àí0.03</cell><cell cols="2">43.81</cell><cell>20.84</cell><cell>35.81</cell></row></table><note>* 22.27 * 37.25 *</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>A comparison of the performance on the Unseen portions for WebNLG test sets, with i) a single OOV Control Prefix used for all samples from unseen categories, or ii) the zero-shot transfer approach outlined, utilizing the available textual labels.</figDesc><table><row><cell></cell><cell cols="2">Unseen Component</cell></row><row><cell></cell><cell cols="2"># Examples # Categories BLEU</cell></row><row><cell>WebNLG</cell><cell>891</cell><cell>5</cell></row><row><cell>OOV Representation</cell><cell></cell><cell>56.35</cell></row><row><cell>Zero-shot</cell><cell></cell><cell>56.41</cell></row><row><cell>WebNLG+ 2020</cell><cell>896</cell><cell>3</cell></row><row><cell>OOV Representation</cell><cell></cell><cell>50.02</cell></row><row><cell>Zero-shot</cell><cell></cell><cell>50.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Detailed results on the DART test set to complement Table1. T5-large fine-tuned is the current SOTA<ref type="bibr" target="#b42">(Radev et al., 2020)</ref>. We report results on the official evaluation script for v1.1.1, the same version as the official leaderboard, available here: https://github.com/Yale-LILY/dart. *Results for this model were only reported to the significant figures shown. œÜ% denotes the % of additional parameters to the number of fixed-LM parameters required at inference time. .95 55.39 61.73 46.73 42.71 44.87 31.34 39.01 34.86 CONTROL PREFIXES (A 1 ) 1.4 67.32 55.38 61.94 46.78 42.77 44.92 30.96 39.01 34.65 .05 55.37 61.78 46.69 42.82 44.90 31.36 38.79 34.77 CONTROL PREFIXES (A 2 ) 1.0 66.99 55.56 61.83 46.67 42.87 44.91 31.37 38.53 34.65 CONTROL PREFIXES (A 1 ,A 2 ) 1.4 67.15 56.41 62.27 46.64 43.18 45.03 31.08 38.78 34.61</figDesc><table><row><cell></cell><cell></cell><cell cols="8">œÜ% BLEU METEOR TER ‚Üì BERTScore(F1)</cell></row><row><cell>T5-large fine-tuned*</cell><cell></cell><cell cols="2">100 50.66</cell><cell></cell><cell>40</cell><cell>43</cell><cell></cell><cell>0.95</cell></row><row><cell>Prefix-tuning</cell><cell></cell><cell cols="2">1.0 51.20</cell><cell cols="2">40.62</cell><cell>43.13</cell><cell></cell><cell>0.95</cell></row><row><cell cols="4">CONTROL PREFIXES (A 1 ) 1.1 51.95</cell><cell cols="2">41.07</cell><cell>42.75</cell><cell></cell><cell>0.95</cell></row><row><cell></cell><cell>œÜ%</cell><cell></cell><cell>BLEU</cell><cell></cell><cell></cell><cell>METEOR</cell><cell></cell><cell></cell><cell>TER ‚Üì</cell></row><row><cell></cell><cell cols="2">S</cell><cell>U</cell><cell>A</cell><cell>S</cell><cell>U</cell><cell>A</cell><cell>S</cell><cell>U</cell><cell>A</cell></row><row><cell>T5-large</cell><cell>100</cell><cell>-</cell><cell cols="3">64.89 54.01 59.95</cell><cell>46</cell><cell>43</cell><cell>44</cell><cell>34</cell><cell>41</cell></row><row><cell>SOTA</cell><cell cols="4">100 65.82 56.01 61.44</cell><cell>46</cell><cell>43</cell><cell>45</cell><cell>32</cell><cell>38</cell><cell>35</cell></row><row><cell cols="2">Prefix-tuning 1.0 66+Data: DART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>1.0 67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Detailed results on the WebNLG test set to complement Table1. S, U and A refer to the Seen, Unseen and All portions of the WebNLG dataset. Several of the baseline results were only reported to the significant figures shown.</figDesc><table><row><cell></cell><cell cols="6">œÜ% BLEU NIST METEOR R-L CIDEr</cell></row><row><cell>T5-Large</cell><cell cols="2">100 38.74</cell><cell>6.15</cell><cell>0.374</cell><cell>53.0</cell><cell>1.64</cell></row><row><cell>SOTA</cell><cell>100</cell><cell>43.6</cell><cell>-</cell><cell>0.39</cell><cell>57.5</cell><cell>2.0</cell></row><row><cell>Prefix-tuning</cell><cell cols="2">1.0 43.66</cell><cell>6.51</cell><cell>0.390</cell><cell>57.2</cell><cell>2.04</cell></row><row><cell>+Data: DART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell cols="2">1.0 43.04</cell><cell>6.46</cell><cell>0.387</cell><cell>56.8</cell><cell>1.99</cell></row><row><cell cols="3">CONTROL PREFIXES (A 2 ) 1.0 44.15</cell><cell>6.51</cell><cell>0.392</cell><cell>57.3</cell><cell>2.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Detailed results on the E2E Clean test set to complement Table1. Several of the baseline results were only reported to the significant figures shown.</figDesc><table><row><cell></cell><cell cols="5">œÜ% BLEU METEOR chrF++ TER ‚Üì BLEURT</cell></row><row><cell>T5-large*  ‚Ä†</cell><cell>100 51.74</cell><cell>0.403</cell><cell>0.669</cell><cell>0.417</cell><cell>0.61</cell></row><row><cell>Prefix-tuning</cell><cell>1.0 54.74</cell><cell>0.417</cell><cell>0.693</cell><cell>0.399</cell><cell>0.62</cell></row><row><cell>CONTROL PREFIXES (A 1 )</cell><cell>1.6 54.97</cell><cell>0.417</cell><cell>0.693</cell><cell>0.398</cell><cell>0.62</cell></row><row><cell>+Data: DART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CONTROL PREFIXES (A 2 )</cell><cell>1.0 54.92</cell><cell>0.418</cell><cell>0.695</cell><cell>0.397</cell><cell>0.62</cell></row><row><cell cols="2">CONTROL PREFIXES (A 1 ,A 2 ) 1.6 55.41</cell><cell>0.419</cell><cell>0.698</cell><cell>0.392</cell><cell>0.63</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>WebNLG+ 2020. The overall WebNLG+ 2020 test set results using the official evaluation script. *As the model outputs are publicly available, we are able to run evaluation to achieve the same precision. ‚Ä† Results from<ref type="bibr" target="#b39">Pasricha et al. (2020)</ref>, who before fine-tuning on the WebNLG+ data, further pre-train T5-large using a Mask Language Modelling objective (with 15% of the tokens masked) on the WebNLG corpus and a corpus of DBpedia. A 1 signifies models trained with control prefixes for the WebNLG category attribute, and A 2 with control prefixes for the DART sub-dataset source attribute.</figDesc><table><row><cell cols="3">DART WebNLG E2E Clean</cell><cell>ASSET</cell><cell cols="2">TurkCorpus</cell><cell>XSum</cell></row><row><cell></cell><cell>BLEU</cell><cell></cell><cell cols="3">SARI QuestEval SARI QuestEval</cell><cell>R-2</cell></row><row><cell>Prefix-tuning + Control Tokens 51.72</cell><cell>61.89</cell><cell cols="2">43.57 43.64</cell><cell>0.63 42.36</cell><cell>0.66 43.67</cell></row><row><cell>CONTROL PREFIXES 51.95</cell><cell>62.27</cell><cell cols="2">43.81 43.58</cell><cell>0.64 42.32</cell><cell>0.66 43.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Prefix-tuning + Control Tokens. Comparison of our best CONTROL PREFIXES model for each dataset with prefix-tuning + control tokens for the same attributes. The guided simplification models are the average test set results over 5 random seeds.</figDesc><table><row><cell>Model</cell><cell>Stage</cell><cell>L-rate</cell><cell>Opt</cell><cell>Warmup-steps</cell><cell>Epochs</cell><cell>Batch Size</cell><cell>Effective Batch</cell><cell>Beam Width</cell><cell>LN-Œ±</cell><cell>Min Target</cell><cell>Max Target</cell><cell>No Repeat Trigram</cell></row><row><cell>DART (T5-large)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>-</cell><cell>7e-5</cell><cell>Ada</cell><cell>2000</cell><cell>40</cell><cell>6</cell><cell>96</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>384</cell><cell>No</cell></row><row><cell>CONTROL PREFIXES (A1)</cell><cell>-</cell><cell>7e-5</cell><cell>Ada</cell><cell>2000</cell><cell>40</cell><cell>6</cell><cell>96</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>384</cell><cell>No</cell></row><row><cell>E2E Clean (T5-large)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>-</cell><cell>8e-5</cell><cell>Ada</cell><cell>2000</cell><cell>50</cell><cell>6</cell><cell>96</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>384</cell><cell>No</cell></row><row><cell>CONTROL PREFIXES (A2)</cell><cell>1 2</cell><cell>7e-5 5e-5</cell><cell>Ada Ada</cell><cell>2000 2000</cell><cell>30 50</cell><cell>6 6</cell><cell>96 96</cell><cell>5 5</cell><cell>1 1</cell><cell>0 0</cell><cell>384 384</cell><cell>No No</cell></row><row><cell>WebNLG (T5-large)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>-</cell><cell>7e-5</cell><cell>Ada</cell><cell>2000</cell><cell>30</cell><cell>6</cell><cell>96</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>384</cell><cell>No</cell></row><row><cell>CONTROL PREFIXES (A1)</cell><cell>-</cell><cell>7e-5</cell><cell>Ada</cell><cell>2000</cell><cell>40</cell><cell>6</cell><cell>96</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>384</cell><cell>No</cell></row><row><cell>+Data: DART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>-</cell><cell>7e-5</cell><cell>Ada</cell><cell>2000</cell><cell>40</cell><cell>6</cell><cell>96</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>384</cell><cell>No</cell></row><row><cell>CONTROL PREFIXES (A2)</cell><cell>1 2</cell><cell>7e-5 3e-5</cell><cell>Ada Ada</cell><cell>2000 2000</cell><cell>30 30</cell><cell>6 6</cell><cell>96 96</cell><cell>5 5</cell><cell>1 1</cell><cell>0 0</cell><cell>384 384</cell><cell>No No</cell></row><row><cell>CONTROL PREFIXES (A1, A2)</cell><cell>1 2</cell><cell>7e-5 3e-5</cell><cell>Ada Ada</cell><cell>2000 2000</cell><cell>30 30</cell><cell>6 6</cell><cell>96 96</cell><cell>5 5</cell><cell>1 1</cell><cell>0 0</cell><cell>384 384</cell><cell>No No</cell></row><row><cell>CONTROL PREFIXES (A1)</cell><cell>1 2</cell><cell>7e-5 3e-5</cell><cell>Ada Ada</cell><cell>2000 2000</cell><cell>30 30</cell><cell>6 6</cell><cell>96 96</cell><cell>5 5</cell><cell>1 1</cell><cell>0 0</cell><cell>384 384</cell><cell>No No</cell></row><row><cell>XSum (BARTLARGE )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>-</cell><cell>7e-5</cell><cell>AdamW</cell><cell>2000</cell><cell>40</cell><cell>8</cell><cell>128</cell><cell>6</cell><cell>1</cell><cell>10</cell><cell>60</cell><cell></cell></row><row><cell>CONTROL PREFIXES (A1, A2)</cell><cell>-</cell><cell>7e-5</cell><cell>AdamW</cell><cell>2000</cell><cell>40</cell><cell>8</cell><cell>128</cell><cell>6</cell><cell>1</cell><cell>10</cell><cell>60</cell><cell></cell></row><row><cell>ASSET &amp; TurkCorpus (BARTLARGE )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Prefix-tuning</cell><cell>-</cell><cell>5e-5</cell><cell>AdamW</cell><cell>2000</cell><cell>30</cell><cell>8</cell><cell>64</cell><cell>6</cell><cell>0.8</cell><cell>3</cell><cell>100</cell><cell></cell></row><row><cell>CONTROL PREFIXES)</cell><cell>-</cell><cell>4e-5</cell><cell>Ada</cell><cell>5000</cell><cell>30</cell><cell>8</cell><cell>64</cell><cell>6</cell><cell>1</cell><cell>3</cell><cell>100</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Hyper-parameters. Detailed hyper-parameters reporting for the models in this work. If the training procedure is multi-stage, each stage is indicated. L-rate is the learning rate, all learning follows a linear learning rate scheduler; Opt refers to the optimizer, Ada (Adafactor) or AdamW; Effective Batch = Batch size x # of gradient accumulation batches; LN-Œ± refers to the Œ± in length normalization during beam search.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the term parameter efficient to denote methods adding &lt;3% additional parameters to a fixed LM's parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We discuss cases where G is not present in ¬ß6.2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">There has been confusion in recent work concerning different forms of prefix-tuning (Li and Liang, 2021). For details and observations of the benefits (previously unremarked upon) conferred by key-value pair prefix-tuning, see Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">It is easy to see how the procedure can be generalized to multiple attributes; we use up to four attributes and varying control prompt lengths.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">This also results in a significant increase in the number of training parameters Œ∏. In contrast, with the methodology outlined, each additional control prefix relates to only an additional dœÅc training parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">BARTLARGE exhibits inferior performance to T5-large on data-to-text; for example, 9.7 BLEU points lower on WebNLG Unseen<ref type="bibr" target="#b45">(Ribeiro et al., 2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">Full results from the evaluation scripts including machinelearned metrics can be found in Appendix A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">  8  Every training category label can be seen in Appendix B, where we visualize control prefixes corresponding to each training category.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">We use the FKGL and latest version of SARI implemented in EASSE<ref type="bibr" target="#b2">(Alva-Manchego et al., 2019)</ref> which is used in<ref type="bibr" target="#b34">Martin et al. (2020)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9">The embeddings relating to these special tokens are the only embeddings we train, as our work is focused on fixed-LM methods.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10">Additional training data is permitted by the organizers of the E2E Clean and WebNLG datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_11">Interestingly we do not observe performance degradation as in other studies utilizing different forms of prefix-tuning<ref type="bibr" target="#b18">(Hu et al., 2021)</ref>. This is shown in Appendix G.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_12">A perplexity of 5 is used for all plots.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_13">Plots for the encoder and decoder cross-attention constituents can be seen found in Appendix E.16  The public GENIE leaderboard is available at https://leaderboard.allenai.org/ genie-xsum/submissions/public</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_14">Appendix H displays model output for WebNLG along with the zero-shot procedure.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_15">We also report results on WebNLG+ 2020<ref type="bibr" target="#b8">(Castro Ferreira et al., 2020)</ref>, the second official WebNLG competition, in Appendix B.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_16">Only the embeddings pertaining to the controllable attributes and the prefix are trained.</note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m M j M v S K Q w 6 L r f T m F t f W N z q 7 h d 2 t n d 2 z 8 o H x 6 1 T Z x q x l s s l r H u B N R w K R R v o U D J O 4 n m N A o k f w z G N z P / 8 Y l r I 2 L 1 g J</p><p>7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 3 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D</p><p>7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 4 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m</p><p>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt; 5 P &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 5 I M I q F y p 0 C e Z F 3 D Y g u V F o c X b 6 B 0 = " &gt; A A A B 6 n i c b V A 9 T w J B E J 3 D L 8 Q v 1 N J m I 5 h Y k T s s t C T a W G I U J I E L 2 V v 2 Y M P e 3 m V 3 z o R c + A k 2 F h p j 6 y + y 8 9 + 4 w B U K v m S S l / d m</p><p>r u c m 6 G d U o 2 C S T 0 u 9 1 P C E s j E d 8 q 6 l i k b c + N n 8 1 C k 5 s 8 q A h L G 2 p Z D M 1 d 8 T G Y 2 M m U S B 7 Y w o j s y y N x P / 8 7 o p h l d + J l S S I l d s s S h M J c G Y z P 4 m A 6 E 5 Q z m x h D I t 7 K 2 E j a i m D G 0 6 J R u C t / z y K m n X a 9 5 F r X 5 X r z S u 8 z i K c A K n c A 4 e X E I D b q E J L W A w h G d 4 h T d H O i / O u / O x a C 0 4 + c w x / I H z + Q N h 2 I 0 y &lt; / l a t e x i t &gt;</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Results</head><p>Additional results using the official evaluation scripts for the data-to-text datasets are reported in Tables <ref type="table">5,6</ref>,7 to supplement the results in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B WebNLG+ 2020 Results</head><p>As NLG is notoriously challenging to evaluate, this work assesses model performance on five of the eleven datasets comprising GEM <ref type="bibr" target="#b14">(Gehrmann et al., 2021)</ref>, a benchmark that intends to provide robust datasets and reproducible standards across an array of NLG tasks. The GEM datasets used are DART, E2E Clean, ASSET, TurkCorpus and WebNLG+ 2020.  WebNLG+ 2020 is not a component of DART-it was used for the second official WebNLG competition <ref type="bibr" target="#b8">(Castro Ferreira et al., 2020)</ref>. There are 16 training categories (the 15 categories from WebNLG, but with new examples), alongside 3 unseen categories. Table <ref type="table">8</ref> displays WebNLG+ 2020 results using the same model architectures as used for WebNLG. A similar pattern is revealed, in that CONTROL PREFIXES outperforms prefixtuning with CONTROL PREFIXES (A 1 ,A 2 ) as the top-performing model. This illustrates again the benefit of using both controllable attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Prefix-tuning</head><p>We make two previously unremarked upon observations of the benefits conferred by using the keyvalue pair prefix-tuning described in ¬ß3.3 compared to prefix-tuning involving augmenting the activations directly <ref type="bibr" target="#b18">(Hu et al., 2021)</ref> or promptembedding tuning of prompt length œÅ. i) The form discussed does not restrict the input length of the base LM. ii) The time complexity at inference time is reduced; for example, if we take a multi-head self-attention computation (M = N ), the time complexity at inference time is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Training Details</head><p>Table <ref type="table">10</ref> displays the hyper-parameters used when training the models reported in this paper. The CONTROL PREFIXES models with the DART subdataset source attribute (A 2 ) use DART as additional data and were trained in two stages: i) on DART, ii) solely on the downstream dataset. The WebNLG prefix-tuning model with DART data shown in Table <ref type="table">10</ref> uses only the human annotated portion of DART. The prefix-tuning models using all of the DART data for WebNLG and E2E Clean were similarly trained in two stages, with the same hyper-parameters as DART sub-dataset source attribute (A 2 ). Training prefix-tuning on all of DART as additional data for WebNLG yielded lower performance than with only the human-annotated DART portion, so was not reported in Table <ref type="table">1</ref>.</p><p>Decoding specific parameters were not tuned-we instead mirrored what the topperforming fine-tuned based system used for the particular LM and dataset. For example, a beam width of 5 as in <ref type="bibr" target="#b45">Ribeiro et al. (2020)</ref> for T5-large on all data-to-text datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASSET Corpus</head><p>Source: The Great Dark Spot is thought to represent a hole in the methane cloud deck of Neptune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Reference ‚Ä†</head><p>The Great Dark Spot represents a hole in the methane cloud of Neptune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTROL PREFIXES</head><p>It is thought that the Great Dark Spot is a hole in Neptune's methane cloud deck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARTLARGE with ACCESS</head><p>The Great Dark Spot looks like a hole in the methane cloud deck of Neptune.</p><p>Source: Fives is a British sport believed to derive from the same origins as many racquet sports.</p><p>Gold Reference ‚Ä† Fives is a British sport developed from the same origins as many racquet sports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTROL PREFIXES</head><p>Fives is a British sport. It is believed to have its origins in racquet sports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BARTLARGE with ACCESS</head><p>Fives is a British sport. It is thought to come from the same as many racquet sports.</p><p>Source: Nevertheless, Tagore emulated numerous styles, including craftwork from northern New Ireland, Haida carvings from the west coast of Canada (British Columbia), and woodcuts by Max Pechstein.</p><p>Gold Reference ‚Ä† Tagore copied many styles.   Triplesets are from WebNLG unseen categories and the zero-shot procedure is depicted using the textual category labels. As an example, for the unseen category Athlete, the closest Glove embedding belonging to a seen category label in embedding space is SportsTeam. Therefore the trained control prefix relating to SportsTeam is used for this example at inference time. Table <ref type="table">14</ref>: WebNLG+ 2020 generations: sources are shown in their linearized form as fed to the T5-large based models. The Source control prefix is highlighted, along with the final Category control prefix for the Prefixtuning model. The zero-shot procedure is depicted for the Unseen Category MusicalWork. The closest embedding belonging to a Seen category in embedding space is Artist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XSum news world</head><p>Kamal C Chavara was detained by the police in Kerala state on Sunday after the youth wing of the Hindu nationalist BJP lodged a complaint against him. Last month, the Supreme Court ruled that the anthem must be played in every cinema before a film is screened. Some 20 people have been held in Kerala and Tamil Nadu since then for remaining seated during the anthem. Also, India's colonial-era sedition law has been often used against students, journalists, writers and social activists and those critical of the government. Reports said that the BJP's youth wing lodged a complaint against a Facebook post by Mr Chavara which allegedly insulted the anthem. The post was apparently an excerpt from one of his books. Senior police official Sateesh Bino told the NDTV news channel that the writer-activist "is being questioned for his controversial post on the national anthem on Facebook" and had been charged with sedition. Earlier this month, 12 people were arrested at a cinema in Kerala, after they remained seated while the national anthem played. The cinemagoers, who were attending an international film festival, were later freed but they face charges of "failure to obey an order issued by a public servant, thereby causing obstruction or annoyance to others". And at a cinema in Chennai, eight people who did not stand for the anthem were assaulted and abused, police said.  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Task2vec: Task embedding for meta-learning</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00653</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6429" to="6438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Intrinsic dimensionality explains the effectiveness of language model fine-tuning</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<idno>CoRR, abs/2012.13255</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Alva-Manchego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04567</idno>
		<title level="m">Easse: Easier automatic sentence simplification evaluation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">ASSET: A dataset for tuning and evaluation of sentence simplification models with multiple rewriting transformations</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Emilio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alva-Manchego</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><surname>Scarton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beno√Æt</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno>CoRR, abs/2005.00481</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S√∂ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
				<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
				<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Empirical study of the benefits of overparameterization in learning latent variable models</title>
		<author>
			<persName><forename type="first">Rares-Darius</forename><surname>Buhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoni</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">TLDR: extreme summarization of scientific documents</title>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Cachola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno>CoRR, abs/2004.15011</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Ilinykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="55" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Structural scaffolds for citation intent classification in scientific publications</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Field</forename><surname>Cady</surname></persName>
		</author>
		<idno>CoRR, abs/1904.01608</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Plug and play language models: A simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Gsum: A general framework for guided neural abstractive summarization</title>
		<author>
			<persName><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>CoRR, abs/2010.08014</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic noise matters for neural natural language generation</title>
		<author>
			<persName><forename type="first">Ond≈ôej</forename><surname>Du≈°ek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Howcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-8652</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th International Conference on Natural Language Generation</title>
				<meeting>of the 12th International Conference on Natural Language Generation<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="421" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The WebNLG challenge: Generating text from RDF data</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Conference on Natural Language Generation</title>
				<meeting>the 10th International Conference on Natural Language Generation<address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="124" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tosin</forename><forename type="middle">P</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aremu</forename><surname>Anuoluwapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghavi</forename><surname>Khyathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miruna-Adriana</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaustubh</forename><forename type="middle">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Dhole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Garbacea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangfeng</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shailza</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mounica</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khyati</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saad</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Emiel Van Miltenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Nadeem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubungo</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salomey</forename><surname>Andre Niyongabo</surname></persName>
		</author>
		<author>
			<persName><surname>Osei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ankur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Raunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Diego</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sashank</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo√£o</forename><surname>Sedoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Shaikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<idno>CoRR, abs/2102.01672</idno>
		<title level="m">The GEM benchmark: Natural language generation, its evaluation and metrics</title>
				<editor>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nishant</forename><surname>Subramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Akhila</forename><surname>Yerukola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiawei</forename><surname>Zhou</surname></persName>
		</editor>
		<imprint>
			<publisher>Marco Antonio Sobrevilla Cabezudo</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">QuickEdit: Editing text &amp; translations by crossing words out</title>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="272" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Have your text and use it too! end-to-end neural datato-text generation with semantic fidelity</title>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Harkous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Groves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Saffari</surname></persName>
		</author>
		<idno>CoRR, abs/2004.06577</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno>CoRR, abs/1904.09751</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><forename type="middle">B</forename><surname>Vi√©gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR, abs/1611.04558</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno>ArXiv, abs/1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">GENIE: A leaderboard for human-in-the-loop evaluation of text generation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno>CoRR, abs/2101.06561</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Controlling output length in neural encoder-decoders</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1140</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kincaid</surname></persName>
		</author>
		<author>
			<persName><surname>Robert P Fishburne</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><forename type="middle">S</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><surname>Chissom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Statistical Machine Translation</title>
				<meeting>the Second Workshop on Statistical Machine Translation<address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Czech Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno>CoRR, abs/2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2021b. GPT understands, too. CoRR, abs/2103.10385</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Few-shot sequence learning with transformers</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno>CoRR, abs/2012.09543</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">√âric de la Clergerie, Antoine Bordes, and Beno√Æt Sagot</title>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno>CoRR, abs/2005.00352</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Multilingual unsupervised sentence simplification</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving quality and efficiency in plan-based neural data-to-text generation</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Moryossef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>CoRR, abs/1808.08745</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno>CoRR, abs/1612.00005</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.3115/1073083.1073135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</title>
				<meeting>the 40th Annual Meeting on Association for Computational Linguistics, ACL &apos;02</meeting>
		<imprint>
			<publisher>USA. Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">NUIG-DSI at the WebNLG+ challenge: Leveraging transfer learning for RDF-to-text generation</title>
		<author>
			<persName><forename type="first">Nivranshu</forename><surname>Pasricha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mihael Arcan</surname></persName>
		</author>
		<author>
			<persName><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
				<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="137" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">To tune or not to tune? adapting pretrained representations to diverse tasks</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4302</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th Workshop on Representation Learning for NLP</title>
				<meeting>the 4th Workshop on Representation Learning for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. RepL4NLP-2019</date>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">DART: open-domain structured data record to text generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrit</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinand</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiachun</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aadit</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangxiaokang</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Irwanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faiaz</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murori</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Tarabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/2007.02871</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How fine can fine-tuning be? learning efficient language models</title>
		<author>
			<persName><forename type="first">Evani</forename><surname>Radiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Dixit</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
				<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>Online. PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="2435" to="2443" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Investigating pretrained language models for graph-to-text generation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><surname>Schmitt</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Hinrich Sch√ºtze, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploiting clozequestions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schutze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">√âric Villemonte de la Clergerie, and Beno√Æt Sagot. 2021. Rethinking automatic evaluation in sentence simplification</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<idno>CoRR, abs/2104.07560</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno>CoRR, abs/1804.04235</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">ERNIE 3.0: Large-scale knowledge enhanced pretraining for language understanding and generation</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Snover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linnea</forename><surname>Micciulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyuan</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weibao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhizhou</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2107.02137</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Machine Transaltion in the Americas (AMTA 2006</title>
				<meeting>the Association for Machine Transaltion in the Americas (AMTA 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006. 2021</date>
		</imprint>
	</monogr>
	<note>A study of translation error rate with targeted human annotation</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Syntactic scaffolds for semantic structures</title>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Multimodal few-shot learning with frozen language models</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Tsimpoukelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Cabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<idno>CoRR, abs/2106.13884</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R√©mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Optimizing statistical machine translation for text simplification</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtney</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="401" to="415" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attribute alignment: Controlling text generation from pretrained language models</title>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR, abs/2103.11070</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/1912.08777</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10931</idno>
		<title level="m">Sentence simplification with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
