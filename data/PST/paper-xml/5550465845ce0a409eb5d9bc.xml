<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GentleRain: Cheap and Scalable Causal Consistency with Physical Clocks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiaqing</forename><surname>Du</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>SoCC &apos;14</addrLine>
									<postCode>03 -05 2014</postCode>
									<settlement>November, Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cȃlin</forename><surname>Iorgulescu</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>SoCC &apos;14</addrLine>
									<postCode>03 -05 2014</postCode>
									<settlement>November, Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>SoCC &apos;14</addrLine>
									<postCode>03 -05 2014</postCode>
									<settlement>November, Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
							<affiliation key="aff0">
								<address>
									<addrLine>SoCC &apos;14</addrLine>
									<postCode>03 -05 2014</postCode>
									<settlement>November, Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GentleRain: Cheap and Scalable Causal Consistency with Physical Clocks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AD07DCF592273E8B42A7DA889AE15EB</idno>
					<idno type="DOI">10.1145/2670979.2670983</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>C.2.4 [Computer Systems Organization]: Distributed Systems Causal Consistency</term>
					<term>Distributed Consistency</term>
					<term>Key Value Stores</term>
					<term>Geo-replication</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>GentleRain is a new causally consistent geo-replicated data store that provides throughput comparable to eventual consistency and superior to current implementations of causal consistency.</p><p>GentleRain uses a periodic aggregation protocol to determine whether updates can be made visible in accordance with causal consistency. Unlike current implementations, it does not use explicit dependency check messages, resulting in a major throughput improvement at the expense of a modest increase in update visibility. Furthermore, Gen-tleRain tracks causal consistency by attaching to updates scalar timestamps derived from loosely synchronized physical clocks. Clock skew does not cause violations of causal consistency, but may delay the visibility of updates. By encoding causality in a single scalar timestamp, GentleRain reduces storage and communication overhead for tracking causality.</p><p>We evaluate GentleRain using Amazon EC2, and demonstrate that it achieves throughput equal to about 99% of eventual consistency, and 120% better than previous implementations of causal consistency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Distributed data stores are a critical infrastructure component of many large-scale online services. To provide performance and availability, these data stores are often georeplicated. A critical decision in geo-replication is the choice of a consistency model. At one end, strong consistency <ref type="bibr" target="#b14">[16]</ref> provides simple semantics, but suffers from long latencies and does not tolerate network partitions. At the other end, eventual consistency provides excellent performance and tolerates partitions <ref type="bibr" target="#b27">[29]</ref>, but renders the programming model more complicated. Recent work also considers the possibility of multiple consistency models in simultaneous use <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>Causal consistency <ref type="bibr" target="#b0">[2]</ref> is an attractive model for constructing geo-replicated data stores. The causality relation is the transitive closure of the order of events within a single client and the reads-from relation between different clients <ref type="bibr" target="#b18">[20]</ref>. A causally consistent store guarantees that an update does not become visible until all its causal dependencies are visible. Causal consistency is attractive, because it avoids the long latencies and partition-intolerance associated with strong consistency, yet it also avoids some of the anomalies possible with eventual consistency.</p><p>Large data stores furthermore partition the data at each datacenter in order to scale to very large data sets. Recent papers <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b21">23,</ref><ref type="bibr" target="#b22">24]</ref> have shown how to implement causal consistency in a replicated partitioned data store without incurring the serialization bottleneck of going through a single log. Roughly speaking, both the client and the data store track causal dependencies. Updates are replicated asynchronously, and the causal dependencies recorded for an update are sent along with the update replication message. At a remote datacenter, the recipient verifies that all causal dependencies are present by sending dependency check messages to other partitions. Once these checks complete, the new version is installed. Unfortunately, the exchange of potentially many dependency check messages can cause throughput to degrade compared to eventual consistency.</p><p>GentleRain presents a different design for a causally consistent key value store with the following features:</p><p>• GentleRain eliminates dependency check messages for updates.</p><p>• GentleRain uses only a single physical timestamp to track dependencies.</p><p>These features distinguish GentleRain from other causally consistent key value stores such as COPS <ref type="bibr" target="#b21">[23]</ref>, which explicitly tracks individual dependencies, and Orbe <ref type="bibr" target="#b10">[12]</ref>, which tracks dependencies using dependency matrices. Eliminating dependency check messages altogether improves throughput in comparison to the other two systems. Using only a single timestamp allows for a far more concise representation of dependencies, and therefore a reduction in storage and communication overheads. GentleRain therefore provides a new and hitherto unexplored design point in the construction of causally consistent data stores.</p><p>While achieving better throughput and reducing storage and communication overhead, GentleRain incurs longer latencies in making updates remotely visible. Updates become visible immediately at the originating datacenter -as in other systems -but remotely they incur a slightly longer visibility delay.</p><p>The contributions of this paper are:</p><p>• The design and implementation of a causally consistent data store that has throughput comparable to eventual consistency, with a modest increase in remote update visibility latency</p><p>• An implementation of a causally consistent key-value store that encodes dependency information as a single scalar, both in terms of storage and transmission</p><p>• The evaluation of these contributions in the context of a key-value store geo-replicated on Amazon EC2</p><p>• An exploration of the tradeoff between the throughput and the remote update visibility latency of causally consistent data stores</p><p>The outline of the rest of this paper is as follows. Section 2 motivates our approach by showing the origin of the overheads of previous causally consistent data store implementations. Section 3 describes the system model. Section 4 presents GentleRain's core protocol. Section 5 provides a proof sketch and some discussion of various aspects of GentleRain's protocol. We evaluate the performance of Gen-tleRain in Section 6. We discuss related work in Section 7, and conclude in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>We demonstrate with a simple experiment the effect of dependency check messages on the throughput of current causal consistency implementations. To do so, we compare the throughput of causal and eventual consistency by an experiment in a distributed key-value store. The data store provides single-item read and write operations. It implements causal consistency as in COPS <ref type="bibr" target="#b21">[23]</ref> and Eiger <ref type="bibr" target="#b22">[24]</ref>, tracking only the nearest dependencies, because of the transitivity of causality. For eventual consistency, the implementation does not track dependencies and makes updates visible as soon as they arrive. A client reads a random item from each partition and updates one random item at a randomly selected partition. The update is then propagated to replicas at remote datacenters. This workload stretches the causal consistency implementation, since it creates dependencies across all partitions for each update operation. While worst-case, workloads with a large number of dependencies across different partitions may not be rare in real world applications <ref type="bibr" target="#b2">[4]</ref>. For instance, the default page of a user of Twitter or Facebook loads at least dozens of or even hundreds of different states. Any subsequent updates via the page, such as commenting on other users' posts, causally depends on all the displayed states.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the throughput of causal and eventual consistency. As more partitions are added to the system, the performance gap between the two becomes larger. For causal consistency, dependency check messages are the main reason for the performance degradation. We demonstrate this fact by performing the following additional experiments. In a first experiment, we remove from the code implementing causal consistency any sending or receiving of messages that check dependencies. In a second experiment, we leave the sending and receiving of dependency check messages in place, but we do not perform any computation or waiting as a result of receiving these messages. The resulting throughput is shown in Figure <ref type="figure" target="#fig_0">1</ref> by the curve "no remote dependency check" for the first experiment, and by the curve "fake remote dependency check" for the second experiment. Not sending or receiving any dependency check messages results in throughput almost identical to eventual consistency. Sending and receiving the messages, but not acting on them, leaves throughput at values comparable to causal consistency. The conclusion is then clear: if causal consistency is to achieve throughput comparable to eventual consistency, we must find a way to implement causal consistency without the exchange of dependency check messages between partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Definition and Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causality</head><p>Causality is the happens-before relationship between two events <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b18">20]</ref>. We denote causal order by . For two operations a and b, if a b, we say b depends on a or a is a dependency of b. a b if and only if one of the following three rules holds:</p><p>• Thread-of-execution. a and b are operations in a single thread of execution, and a happens before b.</p><p>• Reads-from. a is a write operation, b is a read operation, and b reads the value written by a.</p><p>• Transitivity. There is some other operation c such that a c and c b.</p><p>A version X of a data item x is causally dependent on a version Y of data item y if the write of X causally depends on the write of Y. A store is causally consistent if, when a certain version of a data item is visible to a client, then all of its causal dependencies are also visible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture and Interface</head><p>We assume a distributed key-value store that manages a large set of data items. The system is partitioned into N partitions and each partition is replicated by M replicas. A data item is assigned to a partition based on the hash value of its key. In a typical configuration, as shown in Figure <ref type="figure">2</ref>, the system runs at M different datacenters. Each datacenter runs N partitions. Hence, a full copy of the data is stored at each datacenter.</p><p>We assume a multiversion data store. An update operation creates a new version of an item. In addition to the actual value of the key, each version also stores some metadata, in order to track causality. The system periodically garbagecollects old versions of items.</p><p>A server is equipped with a physical clock, which provides monotonically increasing timestamps. Clocks are loosely synchronized by a time synchronization protocol, such as NTP <ref type="bibr">[1]</ref>. The correctness of our system does not depend on the synchronization precision. • val ← GET(key): The GET operation returns the value of the item identified by key.</p><p>• vals ← GET-SNAPSHOT keys : This operation returns the values of a group of items from a snapshot of the data store. A causally consistent snapshot satisfies the following property: For any two items x and y, if X, a version of x, and Y , a version of y, belong to the same consistent snapshot, then there does not exist X , another version of x, such that 1) X is created after X, and 2) X Y . The datastore is free to return a snapshot from any point in the past. It can therefore exclude values that have been already read by the client before executing the snapshot read.</p><p>• vals ← GET-ROTX keys : This operation provides a causally consistent read-only transaction <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref>. It has the same semantics as a snapshot but, in addition is constrained to include any values previously read by the client.</p><p>While the GET and PUT operations are relatively standard, the snapshot is a weaker form of the causally consistent readonly transactions provided by systems such as Orbe <ref type="bibr" target="#b10">[12]</ref> and COPS <ref type="bibr" target="#b21">[23]</ref>. Relaxing the requirement that previously seen updates be part of the snapshot returned by a causally consistent read-only transaction allows the snapshot to be implemented more efficiently than read-only transactions, while continuing to serve many of the same purposes. For example, consider the case of a photo album in a social network. A user might change the album properties to private and then add photos to it. A client, by using a snapshot read </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">GentleRain Protocol</head><p>The GentleRain protocol timestamps all updates with the physical clock value of the server where they originate (the source server). Concatenated with their partition and replica identifiers, these timestamps provide a total order on all updates. The protocol guarantees that this total order is consistent with the causal order of events.</p><p>We distinguish between, on the one hand, updates that have been received at a server, and, on the other hand, updates that have been made visible to clients. The protocol guarantees that updates are made visible only if that can be done in accordance with causal consistency. Local updates are always immediately visible. Updates originating at remote datacenters become visible when their update timestamp is smaller than the global stable time (GST), defined below. The use of physical clock values as update timestamps, rather than logical clock values, is instrumental in making sure that the global stable time makes suitable progress, and remote updates become visible in a short amount of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">States</head><p>Table <ref type="table">1</ref> provides a summary of the symbols used in the protocol.</p><p>Client States. A client c maintains for its session a dependency time DT c . This value is the maximum update timestamp of all items accessed so far by the client session. A client c also maintains in GST c the global stable time that it is aware of.</p><p>Algorithm 1 Client operations at client c</p><formula xml:id="formula_0">1: GET(key k) 2:</formula><p>send GETREQ k, GST c to server 3:</p><p>receive GETREPLY v, ut, gst 4:</p><p>DT c ← max(DT c , ut)</p><formula xml:id="formula_1">5:</formula><p>GST c ← max(GST c , gst) </p><formula xml:id="formula_2">[i] (i = m) from p i</formula><p>n , replica i of the same partition located at datacenter i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We define the local stable time LST m</head><p>n at a server p m n as the minimum element of its VV m n . Updates originating at any replica p i n of p m n with an update timestamp smaller than or equal to LST m n have been received at p m n . We define the global stable time GST m n at a server p m n as a lower bound on the minimum LST of all partitions within the same datacenter. Partitions in the same datacenter periodically compute GST, by means of a protocol described in Section 4.5. This quantity can vary across partitions but never exceeds the minimum LST across all partitions in the datacenter.</p><p>Item Version. For each item uniquely identified by its key, there exists a chain of one or more item versions. We represent an item version d as a tuple k, v, ut, sr . k is a unique key that identifies the item. v is the value of the item. ut is the update time, the creation time of the item at its source server. sr is the source replica, the replica id of the item's source server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Protocol</head><p>We now describe how our protocol executes GET and PUT operations from clients and replicates PUT operations while preserving causality. Algorithms 1 and 2 show the pseudocode of the protocol running at the client and server side, respectively.</p><p>GET. A client sends a GET request with an item key and its GST c to the server that serves the partition containing the item. The server first updates its GST if it is smaller than the client's. The server then obtains the latest version in the version chain of the requested item, which is either created by clients attached to the local datacenter or has an </p><formula xml:id="formula_3">LST m n ← min(VV m n ) 31: GST m n ← min({LST m k | 0 ≤ k ≤ N -1})</formula><p>update timestamp no greater than the partition's GST. Hence, a client always reads local updates without any delay and replicated updates from other datacenters once they are globally stable. The partition returns the item value, its update timestamp, and its GST back to the client. Upon receiving the reply, the client sets its dependency time DT c to the returned item update timestamp if the latter is larger than its current value. It also updates its global stable time GST c to the returned global stable time if the latter is larger than its current value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PUT.</head><p>A client sends a PUT request, PUTREQ k, v, DT c , which includes the item key, the update value, and the client's dependency time, to the server that manages the item. The server then checks that the client's dependency time is smaller than its physical clock time. In the highly unlikely case that it is not, it waits until the condition becomes true.</p><p>The server then updates the local element of its version vector (m th element of VV m n at server p m n ) with its physical clock time. It creates a new version of the item by assigning it a tuple that consists of the key, value, update time, and its replica id, and inserts the newly created item version in the version chain of the item. The server sends a reply with the update time of the newly created item version to the client. Upon receiving the reply, the client sets its dependency timestamp to the returned item update timestamp if the latter is larger than its current value.</p><p>Update replication. After a server executes a local update, it replicates it asynchronously by sending it in update timestamp order to its replicas at the other datacenters. When a server receives such an update replication message, it inserts the received item version in the corresponding version chain. However, this update is not visible to local clients until the partition's GST becomes larger than its update timestamp.</p><p>GST derivation. Each server periodically computes the GST, which is the minimum of the LSTs of all partitions within the same datacenter. We explain how to aggregate the LSTs of all partitions, compute the GST, and distribute it efficiently in Section 4.5.</p><p>Heartbeats. If a partition does not receive update requests from clients, GST may not increase. To solve this problem, a partition that does not have frequent local updates periodically (at time interval ∆) broadcasts its latest clock time to its replicas. It does so by piggybacking the clock time in the heartbeat messages used by failure detectors. Heartbeat messages and update replication messages are sent in order of increasing update timestamps and clock values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Reads of Multiple Items</head><p>GentleRain also supports snapshot reads and causally consistent read-only transactions as defined in Section 3.</p><p>GET-SNAPSHOT. Algorithm 3 shows the pseudo-code of snapshot reads. A client sends a snapshot read request, which includes a set of item keys and its GST to a coordinating partition, selected based on some load balancing algorithm. The coordinating partition first updates the partition's GST if it is smaller than the client's. It then initializes the snapshot timestamp of the snapshot read using the latest GST.</p><p>Using the snapshot timestamp, the snapshot read chooses a proper version of each requested item at the partition that manages the item. The latest item version with an update timestamp smaller than the snapshot timestamp is returned. The item value, update timestamp, and the GST of the accessed partition are sent back to the coordinating partition. </p><formula xml:id="formula_4">GST m n ← max(GST m n , gst) 11: vset ← φ , ut ← 0, gst ← 0 12: st ← GST m n 13:</formula><p>for each key k ∈ kset do run read-only transaction protocol in Eiger <ref type="bibr" target="#b22">[24]</ref> The coordinating partition returns the collected item values with the maximum update timestamp and the maximum GST back to the client. Upon receiving the reply, the client updates its dependency time and GST, as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GET-ROTX.</head><p>Algorithm 4 shows the pseudo-code of causally consistent read-only transactions. Such a transaction returns values that form a read snapshot, but in addition it must also guarantee that this snapshot includes any values previously seen by the client.</p><p>This latter condition may not hold in a situation in which a client reads a version of an item written by a client in the same datacenter, and then performs a snapshot read including that item. If the GST of the server hosting that item at the time of the snapshot is smaller than the update timestamp of that version, then the snapshot would return an earlier version. To avoid this scenario, our protocol for read-only transactions takes one of two approaches. On the one hand, if the dependency time of the client does not exceed the GST by more than some threshold α, then we simply block the transaction to allow the GST to advance past the dependency time of the client. We then execute the snapshot read protocol. On the other hand, if the dependency time is larger than the GST by an amount that exceeds the threshold α, then we fall back on using the protocol used in Eiger <ref type="bibr" target="#b22">[24]</ref> for causally consistent read-only snapshots. Compared with our snapshot read protocol, this protocol may require two rounds to terminate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Conflict Detection</head><p>A conflict happens when two causally unrelated updates to the same key are done at two different replicas. In Gen-tleRain, we detect such conflicts and handle them by calling up to the application that must then tell GentleRain (in a consistent manner at all servers) how to order the conflicting updates. To detect conflicts we use a similar technique to that used in COPS <ref type="bibr" target="#b21">[23]</ref>. Each update that needs to be replicated also carries the update time and source replica id of the previous version of the item in the version chain. Before applying a propagated update, a server checks whether the previous version in the chain is the same as the previous version attached to the incoming update. If these are different, then a conflict has occurred and the application must decide whether to retain the incoming version or drop it as it has been superseded by a later version. Conflict detection does not flag causally ordered updates to the same key as conflicting, because the causally previous update must have arrived at all replicas before a client can install a causally later version at the local replica.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Efficient Global Stable Time Derivation</head><p>Servers within the same datacenter periodically exchange their LSTs to compute their GST. If the number of partitions is large, exchanging LSTs by simply broadcasting them is too expensive and not scalable. Although, with broadcasting GST is derived and distributed to each partition in one round-trip network latency within a datacenter, the message complexity of broadcasting is O(N 2 ). If we assume there are a thousand servers and GST is computed every 10ms, each server sends and receives 100K messages per second, which is not affordable in practice. To efficiently derive the GST at each server, we build a tree over all servers in the same datacenter and compute an aggregate minimum using the tree. This process has been used to solve similar problems such as aggregating state from distributed graph computation <ref type="bibr" target="#b23">[25]</ref> or in sensor networks <ref type="bibr" target="#b13">[15]</ref>.</p><p>During initialization, all servers are given the fanout of the tree and a list of servers sorted based on their partition id. A server then finds its parent and children nodes from the list and sets up a TCP connection to each of them. Leaf nodes of the tree periodically push their LSTs to their parent nodes. Once an internal node receives LSTs from all its children, it computes the minimum and pushes it to its parent node.</p><p>The root node obtains the GST of the round and pushes it down the tree. The message complexity is O(N). Each round of GST computation takes 2 * log F N round trips in the datacenter, where F is the fanout of the tree.</p><p>Using a tree for GST computation brings message counts down to acceptable levels. Continuing the example above, a tree with fanout of five and depth of six can cover almost 20K nodes, well above one thousand servers in the example.</p><p>Computing the GST every 10ms means that each server only sends and receives one message every 10 ms on every one of its six links in the tree (five children and one parent). This works out to sending and receiving 600 messages per second rather than 100K messages per second.</p><p>At the same time, the tree enables computing and distributing the GST in a reasonable amount of time. This is important because a remote update becomes visible after it has been received by the local datacenter and the GST computation takes into account the increase in the LST corresponding to the timestamp of the arrived update. In the same example, if we assume that the link latency within the datacenter is 0.1ms then propagating values from the leaf to the root and back again represents the worst case for latency and works out to 1.2ms. In contrast, the latency between datacenters is usually of the order of about a 100 milliseconds. Therefore GST computation adds only a marginal delay to update propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Why Physical Clocks?</head><p>The GentleRain protocol continues to provide causal consistency if the loosely synchronized physical clocks are replaced with Lamport clocks (or vectors, such as in <ref type="bibr" target="#b0">[2]</ref> or Orbe <ref type="bibr" target="#b10">[12]</ref>) that are incremented on receiving updates from clients and replication messages from other servers. This, however, can cause clocks at different servers to move at very different rates if they also receive updates at different rates. Hence, the visibility of propagated updates from replicas can be arbitrarily delayed as GST always tracks the min-imum element across all version vectors in the datacenter. Using loosely synchronized physical clocks together with the aggregation protocol allows us to keep GentleRain's ability to scale across partitions and datacenters, while avoiding the problem of large differences between clocks at different partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Throughput vs. Latency Tradeoff</head><p>As we have shown in Section 2, in the current implementations of causal consistency, dependency check messages are a major source of overhead, and explain the difference in throughput between causal and eventual consistency. In GentleRain we have eliminated those dependency messages, and replaced them with a tree aggregation protocol for computing the minimum timestamp and (in the absence of regular updates at a partition) a heartbeat protocol. Both these protocols are parameterized by the time interval at which they are invoked. As we show in Section 6, with reasonable choices of such intervals, throughput is much better than current implementations of causal consistency and approximates that of eventual consistency.</p><p>This better throughput comes at the price of an increase in update visibility latency for remote updates. Local updates are visible immediately, both in current implementations and in GentleRain. In current implementations, the latency before an update becomes visible at a remote datacenter is the sum of the network travel time from the origin to the remote datacenter plus the time to exchange the dependency check messages, if any. Since the latter are local to a datacenter, we assume in first approximation that remote update visibility is equal to the travel time to the remote datacenter. In GentleRain, the latency before an update becomes visible at a remote datacenter is the sum of a number of factors. First, there is the network travel time from the furthest removed datacenter, because this is the longest time it takes for an update or a heartbeat with a particular value to arrive. Second, there is the clock skew between those two datacenters. Third, there is the interval at which the aggregation protocol runs, and, finally, in the absence of regular updates, there is the interval at which the heartbeat protocol runs. Assuming expected values for these component times, we can in first approximation conclude that the remote update visibility latency is equal to the network travel time to the furthest removed datacenter.</p><p>We argue that this is a reasonable tradeoff for increased throughput because the maximum latency between datacenters is usually under 270ms (for a satellite link) and this latency is tolerable for applications such as social networks.</p><p>If increased remote update visibility is a consideration, the GentleRain protocol can be modified to, instead of a single scalar, maintain a vector of size the number of datacenters as dependency information. Essentially, rather than computing LST m n , the minimum of VV m n , we maintain the entire vector, and use that to compute an element-wise minimum over all partitions in a datacenter. Similarly, a vector of dependencies is maintained by the client and with each data item, and exchanged between clients and servers. With this modified design, the computation and storage overhead increase, but the latency is reduced, in first approximation, to the network travel time to the originator of the update. We have not yet experimented with this modified design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Garbage Collection</head><p>We The computation of the safe garbage collection timestamp can be done efficiently using the same techniques as used for computing GST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Correctness</head><p>We provide an informal proof sketch that GentleRain provides causal consistency. First, local updates can be made visible locally, because they have read and written their dependencies locally, and so they must be visible. Second, for remote updates, we first demonstrate the correctness of Gen-tleRain in the absence of the heartbeat messages. This is done by demonstrating two supporting propositions, which are then used to derive the overall correctness argument. We conclude by showing that the heartbeat messages are correct optimizations.</p><p>Proposition 1: If an update u1 depends on an update u2, then u2.ut &lt; u1.ut.</p><p>By lines 4 and 8 of Algorithm 1, the client puts in the PU-TREQ the largest update timestamp value of any dependency it incurred as a result of a previous GET or PUT. By lines 6, 7, and 11 of Algorithm 2, the update timestamp of an update is always at least as large as the dependency time dt passed in the PUTREQ. It follows that the new update has an update timestamp larger than any of its dependencies.</p><p>Proposition 2: When, at some partition p m n , GST m n has a certain value T, then all partitions p m i (i = 0..N) have received all updates with update timestamp less than or equal to T. This is shown by contradiction. Suppose there is an update u originating in partition p j i ( j = m) with u.ut &lt; T , and that update has not been received by partition p m i . Since update replication messages arrive in update timestamp order, and by line 20 of Algorithm 2, VV m i [ j] &lt; T , thus LST m i &lt; T , and GST m n &lt; T , leading to a contradiction. Correctness in the absence of heartbeat messages.</p><p>We wish to show that the following proposition holds. Assume data item x resides at partition i and data item y resides at partition j. If a client receives as a result of a GET(x) from partition p m i a version X of x, and if that version X of x depends on the version Y of y, then if that same client performs a GET(y) to partition p m j , it receives in response a version of y created no earlier than Y, and that version is available from p m j without blocking. The phrase "no earlier than" is to be interpreted in terms of the total order imposed on all updates by the update timestamps (physical clock values followed by replica and partition identifiers to break ties).</p><p>Assume a client performs a GET(x) on partition p m i , and receives as a result a version X of x with update timestamp X.ut. Let T be the value of GST m i at p m i at the time the GET is performed. Then, it follows, by line 3 of Algorithm 2, that X.ut &lt; T , and, by line 5 of Algorithm 1, that T ≤ GST c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>If that version X of x depends on a version Y of y, then by</head><p>Proposition 1 it must be that Y.ut &lt; X.ut, and therefore also that Y.ut &lt; T . Then, by Proposition 2, the version of Y of y must have been received at p m j . Assume finally that the same client performs a GET(y) on partition p m j , then by line 2 of Algorithm 2, GST c ≤ GST m j , and thus also Y.ut &lt; T ≤ GST m j . By line 3 of Algorithm 2, it follows that GET(y) returns a version of y with update timestamp no smaller than Y.ut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correctness in the presence of heartbeat messages.</head><p>The heartbeat messages are an optimization that causes the GST values to move forward when there is no local update in a particular partition. We conclude by showing that this optimization is correct. A heartbeat message from p m i to p m n informs p m n of the fact that there are no updates from p m i with update timestamp smaller than the heartbeat value ct. All updates across different partitions and replicas are totally ordered by their update timestamps. By Proposition 2, all partitions have received all updates with update timestamp less than or equal to the GST. We assign the latest GST to the operation as its snapshot timestamp. Therefore, by choosing the item version with the largest timestamp that is not greater than the snapshot timestamp, we provide a snapshot that is causally consistent as defined in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correctness of read-only transactions.</head><p>By virtue of the fact that the update timestamp order is a total order that reflects the partial causal order, the value of the snapshot timestamp provides a value that divides all updates causally before and causally after that timestamp. By choosing the version with the largest timestamp value before the snapshot timestamp and delaying the transaction briefly (line 11 of Algorithm 4), we return versions that form a causally consistent snapshot as defined in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Failures and Laggards</head><p>GentleRain is designed to continue causally consistent key value service even in the presence of machine and network failures or in the presence of slow machines (laggards). In particular it never violates causal consistency in response to a query.</p><p>Machine failures, network partitions and machine slowdowns all have a single consequence in GentleRain: GST at one or more datacenters does not make adequate progress. By design GentleRain only delays the visibility of remote updates in such a case and not local updates, whose visibility is independent of GST . Geo-replicated datastores are usually built under the assumption of locality of traffic to a datacenter to better serve users in a geographical region. GentleRain isolates the exchange of information among clients local to a datacenter from the above mentioned problems.</p><p>Further, GentleRain assumes that servers in a datacenter are themselves backed up by strongly consistent replicas within the same datacenter. This means that GST computation can only be stalled on failure of a single server by the amount of time needed to switch over to another local replica. This leaves network failures both within and across datacenters. Datacenters themselves often contain redundancy in their networks, and this redundancy can be further improved by adequately designing the network topology <ref type="bibr" target="#b20">[22]</ref>. This leaves GentleRain only vulnerable to complete partitions between datacenters. If a datacenter gets disconnected from the rest, no remote updates can be visible at any datacenter due to GST not making progress. Such datacenter partitions are usually rare and in GentleRain do not impact visibility of local updates within a datacenter. However, we can handle such complete datacenter partitions by excluding the partitioned datacenter from the computation of GST. We also exclude any updates from the disconnected datacenter that have been seen after the GST value where it was deemed to be partitioned, to avoid the situation where a value has arrived without its dependencies. This exclusion is possible since we have a total order on updates given by the single scalar timestamp. This failure handling protocol requires us to maintain a group <ref type="bibr" target="#b15">[17]</ref> of connected datacenters. Designing and implementing this protocol to dynamically size the group of datacenters over which GST is computed is a focus for future work in GentleRain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>We evaluate GentleRain in terms of local latencies, throughput, and remote update visibility latency, by varying the workloads and the number of partitions. We compare Gen-tleRain to Eventual Consistency (EC) and a current implementation of causal consistency (COPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation and Setup</head><p>We implement GentleRain in C++ and use Google's Protocol Buffers for message serialization. We also implement eventual consistency (EC) and COPS in the same code base for performance comparison. The GentleRain implementation requires about 20k lines of code, compared to 21k lines for EC and COPS. Our implementation of COPS only tracks one-hop nearest dependencies. It does not provide read-only transactions, which requires tracking complete dependencies. In this mode, without support for transactions, the COPS protocol is identical to Eiger <ref type="bibr" target="#b22">[24]</ref>, which is a later protocol from the same authors. Also, this places COPS in a more favorable position for comparison with GentleRain.</p><p>The data store is a key-value store, partitioned over a group of servers using consistent hashing <ref type="bibr" target="#b16">[18]</ref>. The data set used in the experiments contains one million key-value pairs per partition, with the key size being eight bytes and the value size 64 bytes. The key-value store keeps all key-value pairs in main memory. A key points to a linked list that contains different versions of the same item. The operation log resides on disk. The system performs group commit to write multiple updates in one disk write. A PUT operation inserts a new version to the version chain of the updated item and adds a record to the operation log.</p><p>We run NTP to keep physical clocks synchronized <ref type="bibr">[1]</ref>. NTP can be configured to either change the clock value or change the clock frequency to catch up or fall back to a target. We configure it to change clock frequency, so that our physical clocks always move forward, a requirement for correctness in GentleRain.</p><p>We run all experiments on Amazon EC2 using c3.large instances running Ubuntu 12.04. Each server has two virtual CPU cores, 3.75 GB memory, and 2 x 16GB SSD storage. We replicate each partition at three EC2 datacenters: one on the US east coast (Virginia), one on the US west coast (Oregon), and one in Europe (Ireland). We measure the following average latencies between these datacenters: Virginia to Oregon 81.2ms, Oregon to Ireland 166.1ms, and Ireland to Virginia 87.5ms. Partitions in a datacenter form a binary tree for the purposes of GST computation. Unless stated otherwise, in all experiments below, the aggregation tree protocol to compute GST is run every 5 milliseconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Microbenchmarks</head><p>We first examine the throughput that can be achieved by a single partition server in all three systems. We launch enough clients to saturate the server. GET operations read a randomly selected item, and PUT operations update a randomly selected item. For comparison, we also measure the throughput of Echo operations, in which the server simply returns the arguments to the client.</p><p>As shown in Table <ref type="table" target="#tab_5">2</ref>, a GentleRain partition server can process Echo operations at about 65 Kops/sec, GET operations at about 58 Kops/sec, and PUT operations at about 30 Kops/sec. The results are similar for the other two systems.</p><p>The throughput of Echo indicates the message processing capability of our hardware. The throughput of GET is within 10% of the throughput of Echo, which indicates the speed of the hardware in terms of message handling. PUT operations are more expensive, primarily because of the cost of creating a new version. As the update value size increases, the throughput drops slightly due to the need for a larger memory copy. In all cases, the CPU is the bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Throughput</head><p>We examine the throughput of GentleRain in comparison to EC and COPS, for various workloads while varying the number of partitions.</p><p>In the first experiment, each client reads a randomly selected item from every partition and updates a randomly selected item at one partition. The write operation depends on the previous write and the read operations in between. Figure <ref type="figure" target="#fig_4">3</ref> shows the throughput of GentleRain, COPS, and EC. Gen-tleRain performs as well as EC for all number of partitions, and performs significantly better than COPS. At 32 partitions, both EC and GentleRain achieve a throughput of 1670 Kops/sec, while COPS achieves 748 Kops/sec. The difference between GentleRain and COPS stems from the fact that in this experiment, which is a worst case scenario for COPS, it needs to send dependency check messages to all partitions.  Dependency check messages are a source of overhead even with large updates, as Figure <ref type="figure" target="#fig_5">4</ref> shows where we replace the 64 byte updates with 1KB ones.</p><p>In the second experiment, each client updates a randomly selected item from each partition in a round-robin fashion. Each write depends only on the previous write. Figure <ref type="figure">5</ref> shows the throughput of GentleRain, COPS, and EC. The same general trend shows in these results, but the difference between COPS and GentleRain is smaller because only one dependency check message is required in COPS: at 32 partitions, GentleRain sustains 937 Kops/sec, while COPS sustains 717 Kops/sec.</p><p>In the third experiment, a client reads N randomly selected items from randomly selected partitions and writes one randomly selected item to each of M randomly selected partitions. We vary the ratio of N to M. Figure <ref type="figure">6</ref> shows the throughput results. Overall, GentleRain is close to the throughput of eventual consistency. GentleRain provides far better throughput than COPS for read-heavy workloads. This performance gap decreases as we move towards the updateheavy end, as COPS no longer needs to track or check as many dependencies for each update.  In the fourth experiment, a client issues snapshot reads. We vary the number of items a snapshot reads. The throughput results for GentleRain and EC (treating the reads as normal reads) are reported in Figure <ref type="figure" target="#fig_7">7</ref>. The cost of a causally consistent read-only transaction is almost the same as a snapshot read when the two-round read protocol of Eiger is not selected. Hence, we do not present its throughput numbers in the paper.</p><p>In our last throughput experiment, we evaluate the overhead of the GST computation, which requires partitions within the same datacenter to periodically exchange messages. Figure <ref type="figure" target="#fig_8">8</ref> shows throughput as a function of the interval at which the  GST computation is carried out for an experiment in which a client reads all partitions and updates one of them. Increasing the value of the interval by 256X causes an increase in throughput of only 1.15X, demonstrating that the throughput is relatively unaffected by the rate at which GST computation messages are exchanged in the same datacenter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Update Visibility Latency</head><p>We measure the update visibility latency by storing the physical time when an update is installed at its origin (in the value of its key-value pair), and subtracting it from the physical time when the update becomes visible at a remote datacenter.</p><p>The clock skew between clocks on different servers causes this measurement to be only an approximation of the update visibility latency, but it is a good approximation, because the clock skew is much smaller than the network travel times between datacenters.</p><p>Figure <ref type="figure" target="#fig_9">9</ref> shows a cumulative distribution of the latency before updates originating in Ireland and Virginia become visible in Oregon. The results confirm the discussion in Section 5. For EC and COPS the vast majority of the updates become visible at a time equal to the network travel time between the datacenters: about half of the updates originate in Ireland and about half in Virginia, and each half becomes visible in Oregon after the network travel time from their origin to their destination. For GentleRain the update visibility is roughly equal to the longest network travel time, in this case from Ireland to Oregon, plus the GST computation interval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Related Work</head><p>The choice of consistency for replicated systems has been the subject of much research, with models ranging from strong consistency <ref type="bibr" target="#b14">[16]</ref> to eventual consistency <ref type="bibr" target="#b27">[29]</ref> over various intermediate models such as causal consistency <ref type="bibr" target="#b0">[2,</ref><ref type="bibr" target="#b18">20]</ref>, and combinations of various models.</p><p>At one end, there are a number of systems that provide only eventual consistency, such as Dynamo <ref type="bibr" target="#b9">[11]</ref>. We have demonstrated that we can achieve the stronger semantics of causal consistency, with similar throughput, by allowing a modest increase in remote update visibility. At the other end, there are systems that provide strong consistency, such Spanner <ref type="bibr" target="#b8">[10]</ref> and MegaStore <ref type="bibr" target="#b3">[5]</ref>. While providing a simple programming model, such systems provably must exhibit much longer latencies <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b12">14]</ref>. Some recent systems employ multiple consistency models within a single system <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b26">28]</ref>.</p><p>The system chooses which consistency to use for what data item based on SLAs or ordering constraints imposed by the user.</p><p>The concept of causality in distributed systems was introduced by Lamport <ref type="bibr" target="#b18">[20]</ref>. Lamport also discussed the use of physical clocks instead of logical Lamport clocks. However, the physical clocks there were used as a mechanism to incorporate out-of-band causality beyond the causality propagated through message exchange. Consequently, those clocks required a tight enough synchronization bound to subsume any such causality. In contrast, physical clocks in GentleRain do not require any synchronization bounds to offer causal consistency. Rather, they are more similar to logical Lamport Clocks with the coupling to physical time made to ensure that clocks on different servers progress at a reasonably similar rate to aid update visibility.</p><p>Bayou <ref type="bibr" target="#b25">[27]</ref>, lazy replication <ref type="bibr" target="#b17">[19]</ref>, ISIS <ref type="bibr" target="#b6">[8]</ref>, causal memory <ref type="bibr" target="#b0">[2]</ref>, and PRACTI <ref type="bibr" target="#b4">[6]</ref> implement causal consistency, but all assume single-machine replicas and do not consider parti-tions. COPS was the first system to implement causal consistency in a partitioned replicated data store <ref type="bibr" target="#b21">[23]</ref>. The authors also introduce the concept of a causally consistent read-only transaction, and in a later system, Eiger <ref type="bibr" target="#b22">[24]</ref>, the concept of a causally consistent write-only transaction. Later systems along the same lines include ChainReaction <ref type="bibr" target="#b1">[3]</ref> and Orbe <ref type="bibr" target="#b10">[12]</ref>. In terms of maintaining causal consistency, all four systems (COPS, Eiger, ChainReaction, and Orbe) rely on maintaining detailed dependency information and explicit dependency check messages to verify that an update can be installed according to the rules of causal consistency. They also employ various optimizations to reduce the number of dependencies and the number of dependency check messages, by relying on the transitivity of causality and only tracking nearest dependencies <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b22">24]</ref> or by using a sparse representation of a dependency matrix <ref type="bibr" target="#b10">[12]</ref>. Nonetheless, their worst-case behavior remains linear in the number of partitions. In contrast, GentleRain needs only a single scalar to track dependencies, independent of any workload characteristics.</p><p>Our work has in part been inspired by the use of physical clocks to implement causal read-only transactions in Orbe <ref type="bibr" target="#b10">[12]</ref>. Physical clocks have also been used in many other distributed systems. In recent examples, Spanner implements serializable transactions with external consistency <ref type="bibr" target="#b14">[16]</ref> in a geographically replicated and partitioned data store <ref type="bibr" target="#b8">[10]</ref>. Unlike GentleRain, Spanner uses synchronized clocks with bounded uncertainty, called TrueTime, requiring access to GPS and atomic clocks. Clock-SI <ref type="bibr" target="#b11">[13]</ref> uses loosely synchronized clocks to provide snapshot isolation <ref type="bibr" target="#b5">[7]</ref> in a partitioned data store.</p><p>GentleRain supports only a mapping from variable length keys to values, and does not explicitly support more complex schemas such as column families <ref type="bibr" target="#b22">[24]</ref>. One could add support for columns and column families, if desired, by treating the key as multidimensional with one dimension percolumn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented a new protocol for implementing causal consistency for geo-replicated partitioned data stores. The protocol trades remote update visibility latency for improved throughput. We have demonstrated by means of measurements that for a variety of workloads GentleRain indeed provides very good throughput, close to eventual consistency, and considerably better than existing solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Throughput of a distributed data store with eventual and causal consistency. Each partition is replicated by three replicas. Clients read an item from each partition and update an item at one partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2 4 : 19 : 25 :send HEARTBEAT ct to p k n 27 : 28 :</head><label>2419252728</label><figDesc>Server operations at server p m n 1: upon receive GETREQ k, gst d from version chain of key k s.t. d.sr = m or d.ut ≤ GST m n send GETREPLY d.v, d.ut, GST m n to client 5: upon receive PUTREQ k, v, dt : d.ut ← VV m n [m] 12: set source replica: d.sr ← m 13: insert d to version chain of key k 14: send PUTREPLY d.ut to client 15: upon new version d created 16: for each server p k n , k ∈ {0..M -1}, k = m do 17: send REPLICATE d to p k n 18: upon receive REPLICATE d from p k n insert d to version chain of key d.k for each server p k n , k ∈ {0..M -1}, k = m do 26: upon receive HEARTBEAT ct from p k n VV m n [k] ← ct 29: upon every θ time 30:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 3 6 : 9 :</head><label>369</label><figDesc>Snapshot Reads 1: // at client c 2: GetSnapshot(keys kset) 3: send GETSNAPSHOT ks, GST c to server 4: receive GETSNAPSHOTREPLY vset, ut, gst 5: DT c ← max(DT c , ut) GST c ← max(GST c , gst) 7: return vset 8: // at partition p m n upon receive GETSNAPSHOT kset, gst 10:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>By line 28 of Algorithm 2, VV m n [i] = ct, and by line 30 of Algorithm 2 LST m n = min(VV m n ), the invariant is maintained that p m n has received all updates with update timestamp smaller than LST m n . Correctness of snapshot reads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Throughput of 1 to 32 partitions. A client reads an item from every partition and updates an item at one partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Throughput of 1 to 32 partitions. A client reads an item from every partition and updates an item at one partition with a 1KB value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Throughput of 1 to 32 partitions. A client updates an item at each partition in a round-robin fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Throughput of snapshot reads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Varying GST computation intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Visibility latency of remote updates replicated from Ireland and Virginia to Oregon</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Visibility latency of remote updates replicated from Ireland and Oregon to Virginia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Server Server Application Tier Datacenter Datacenter Datacenter Client Client Client</head><label></label><figDesc></figDesc><table><row><cell>Figure 2. System architecture. The data set is replicated by</cell></row><row><cell>multiple datacenters. Clients are co-located with the data</cell></row><row><cell>store in the datacenter and are used by the application tier</cell></row><row><cell>to access the data store.</cell></row><row><cell>Our distributed key-value store provides the following oper-</cell></row><row><cell>ations to the clients:</cell></row><row><cell>• PUT(key, val): A PUT operation assigns value val to an</cell></row><row><cell>item identified by key. If item key does not exist, the</cell></row><row><cell>system creates a new item with initial value val. If key</cell></row><row><cell>exists, then a new version storing val is created.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>briefly describe how to garbage-collect old item versions to keep the storage footprint small. Partitions within the same datacenter periodically exchange global snapshot timestamps of the oldest active snapshot read. If a partition does not have any active snapshot read, it sends out its GST. At each round of garbage collection, a partition chooses the minimum among the received timestamps as the safe garbage collection timestamp. With this timestamp, a partition scans the version chain of each item it stores. It only keeps the latest item version created before the safe garbage collection timestamp (if there is one) and the versions created after the timestamp. It removes all the other versions as these are not needed by active and future snapshot reads.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 .</head><label>2</label><figDesc>Throughput in Kops/sec for client operations on a single partition server without replication.</figDesc><table><row><cell>Op/Bytes</cell><cell cols="4">Echo/-GET/10 PUT/16 PUT/128</cell></row><row><cell>GentleRain</cell><cell>65.3</cell><cell>57.8</cell><cell>33.4</cell><cell>30.1</cell></row><row><cell>EC</cell><cell>65.3</cell><cell>59.2</cell><cell>33.9</cell><cell>30.4</cell></row><row><cell>COPS</cell><cell>65.3</cell><cell>58.4</cell><cell>33.6</cell><cell>30.2</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We would like to thank our shepherd Michael Kaminsky and the anonymous reviewers for their feedback that helped make the paper better.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causal memory: Definitions, implementation, and programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahamad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Hutto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="49" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chainreaction: a causal+ consistent datastore based on chain replication</title>
		<author>
			<persName><forename type="first">S</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leitão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Systems</title>
		<meeting>the European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bolton causal consistency</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Management of Data</title>
		<meeting>the Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="761" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Megastore: Providing scalable, highly available storage for interactive services</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Practi replication</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Belaramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dahlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nayate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yalagandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Networked Systems Design and Implementation</title>
		<meeting>the Symposium on Networked Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A critique of ANSI SQL isolation levels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Berenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>O'neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'neil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Management of Data</title>
		<meeting>the Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reliable communication in the presence of failures</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Birman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Joseph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="76" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards robust distributed systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Principles of Distributed Computing</title>
		<meeting>the Symposium on Principles of Distributed Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>abstract</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spanner: Google&apos;s globally-distributed database</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Corbett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fikes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Furman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gubarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hochschild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Operating Systems Design and Implementation</title>
		<meeting>the Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="251" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dynamo: Amazon&apos;s highly available keyvalue store</title>
		<author>
			<persName><forename type="first">G</forename><surname>Decandia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hastorun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kakulapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lakshman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pilchin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sivasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vosshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems Review</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Orbe: scalable causal consistency using dependency matrices and physical clocks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Cloud Computing</title>
		<meeting>the Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Clock-SI: Snapshot isolation for partitioned data stores using loosely synchronized clocks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elnikety</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Reliable Distributed Systems</title>
		<meeting>the Symposium on Reliable Distributed Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="173" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Brewer&apos;s conjecture and the feasibility of consistent, available, partition-tolerant web services</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lynch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGACT News</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="51" to="59" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scalable faulttolerant aggregation in large process groups</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Renesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Birman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Dependable Systems and Networks</title>
		<meeting>the Conference on Dependable Systems and Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linearizability: A correctness condition for concurrent objects</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Herlihy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="492" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Processor group membership protocols: specification, design and implementation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fakhouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Reliable Distributed Systems</title>
		<meeting>the Symposium on Reliable Distributed Systems</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Consistent hashing and random trees: distributed caching protocols for relieving hot spots on the world wide web</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lewin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Theory of Computing</title>
		<meeting>the Symposium on Theory of Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="654" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Providing high availability using lazy replication</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ladin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shrira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="360" to="391" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time, clocks, and the ordering of events in a distributed system</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lamport</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="558" to="565" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making geo-replicated systems fast as possible, consistent when necessary</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Preguic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Operating Systems Design and Implementation</title>
		<meeting>the Conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fault-tolerant engineered network</title>
		<author>
			<persName><forename type="first">V</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Halperin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Networked Systems Design and Implementation</title>
		<meeting>the Conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="399" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Don&apos;t settle for eventual: scalable causal consistency for widearea storage with cops</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Operating Systems Principles</title>
		<meeting>the Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stronger semantics for low-latency geo-replicated storage</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaminsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Andersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Networked Systems Design and Implementation</title>
		<meeting>the Conference on Networked Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="313" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pregel: A system for largescale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Management of Data</title>
		<meeting>the Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detection of mutual inconsistency in distributed systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Parker</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Popek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rudisin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Stoughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="240" to="247" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flexible update propagation for weakly consistent replication</title>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Spreitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Theimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Demers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems Review</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="288" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Consistency-based service level agreements for cloud storage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kotla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Aguilera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Abu-Libdeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Operating Systems Principles</title>
		<meeting>the Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Eventually consistent</title>
		<author>
			<persName><forename type="first">W</forename><surname>Vogels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="44" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
