<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sphinx: Enabling Privacy-Preserving Online Learning over the Cloud</title>
				<funder ref="#_eGW4qUN">
					<orgName type="full">Hong Kong RGC TRS</orgName>
				</funder>
				<funder ref="#_ZRqKQch #_ZjRF5aR">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_Bd7QcS6">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chaoliang</forename><surname>Zeng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenghang</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Di</forename><surname>Chai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junxue</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sphinx: Enabling Privacy-Preserving Online Learning over the Cloud</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/SP46214.2022.00066</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the growing complexity of deep learning applications, users have started to delegate their data and models to the cloud. Among these applications, online learning services, which involve both training and inference procedures, are widely deployed. To ensure privacy guarantee on the public cloud, researchers have proposed a plethora of privacy-preserving deep learning algorithms with different techniques, ranging from obfuscation mechanisms to cryptographic tools. However, none of them is applicable to online learning services. They either focus only on inference or training procedure while ignoring the other, or require non-colluding or trusted third parties.</p><p>In this paper, we present Sphinx, an efficient and privacypreserving online deep learning system without any trusted third parties. Sphinx strikes a balance between model performance, computational efficiency, and privacy preservation with systematical optimizations on both private inference and training protocols. At its core, Sphinx synthesizes homomorphic encryption and differential privacy reciprocally to maintain the model by keeping most of its parameters as plaintexts, enabling fast training and inference protocol designs. Meanwhile, by refining the homomorphic operation behaviors, Sphinx avoids most of the heavyweight homomorphic operations and minimizes the communication cost. As a result, Sphinx is able to reduce the training time significantly while achieving real-time inference without exposing user privacy. In our experiments, we find that compared to the pure homomorphic encryption solution, Sphinx is 35? faster for training and 4 orders of magnitude faster for inference, providing real-time inference response (0.05 seconds for MNIST and 0.08 seconds for CIFAR-10). Our experiments also demonstrate that Sphinx achieves promising model accuracy under a tight privacy budget (96% accuracy under = 2, ? = 10 -5 for MNIST) without a trusted data aggregator, and is more robust against practical reconstruction attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Although deep learning has become the fundamental infrastructure and core functionality for many applications, the computation requirement of decent training and fast inference on deep neural networks keeps increasing. To relieve the computation overhead, machine-learning-as-a-service (MLaaS) system is involved. The same trend happens in providing online learning services, which involves both training and inference procedures during the service time. For example, starting with a baseline model trained on a generic and large dataset such as ImageNet <ref type="bibr" target="#b0">[1]</ref>, the image classification service provider could provide inference services as well as fine-tuned personalized models for each user with their personal images.</p><p>Delegating the training and inference workloads to a public cloud inevitably raises privacy concerns. To solve this problem, previous wisdom has developed MLaaS systems with supports from various privacy-preserving techniques to protect user privacy (Section II). However, given the unique characteristic of online learning that requires both efficient inference and training procedures, none of the existing works is applicable to online learning services on deep neural networks. They either focus on providing inference service over encrypted data ([2]- <ref type="bibr" target="#b4">[5]</ref>) that assumes a plain public model on the server, or concentrate on the training procedure on encrypted models ([6]- <ref type="bibr" target="#b9">[10]</ref>) that leads to inefficient inference. Moreover, other potential solutions ( <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b18">[19]</ref>) make unrealistic assumptions about the threat model, such as multiple non-colluding servers or a trusted aggregator.</p><p>Therefore, we ponder a fundamental question: as demonstrated in Figure <ref type="figure" target="#fig_0">1</ref>, can we design an efficient and privacypreserving online learning framework without additional third party requirements, e.g., non-colluding servers or a trusted aggregator? Specifically, we focus on deep neural network, a widely used model in online learning services.</p><p>In this paper, we present Sphinx, an efficient and privacypreserving online deep learning system offering both efficient training and real-time inference services on deep neural networks without a trusted party. Sphinx protects both model and input data, thus allowing users to delegate training and inference tasks to cloud servers without exposing data privacy. Sphinx is applicable for feed-forward neural networks consisting of fully connected layers and convolutional layers.</p><p>To achieve the above features, Sphinx uses differential privacy to bridge privacy-preserving training and private inference solutions. It also introduces new hybrid privacypreserving protocols for training and inference phases, respectively, according to their specific characteristics and requirements. As far as we know, Sphinx is the first of its kind in the regime of privacy preserving deep learning to adopt different privacy-preserving protocols for both phases. To design privacy-preserving deep learning protocols by combining different primitives together, we have two-fold key technical challenges: i) how to design efficient and compatible training and inference protocols with different primitives respectively to fulfill their specific requirements, and ii) how to fully exploit the possible merits from the hybrid protocols, in terms of both privacy and accuracy, by deliberately designing the privacypreserving operation behaviors.</p><p>At its core, Sphinx divides each linear layer in deep neural networks into two parts: the linear component W and the bias component b. Sphinx encrypts all the bias components with homomorphic encryption (HE), and perturbs the linear components with differential privacy (DP). We show that this design enables our high throughput training and low latency inference protocols (Section V), and builds a reciprocal relation between DP and HE techniques from both theoretical (Section VI) and empirical (Section IX) perspectives.</p><p>To accelerate the training procedure under the HE scheme, Sphinx makes several system optimizations (Section VII). First, by deliberately designing the homomorphic arithmetic operation behaviors between features, gradients and model parameters, it avoids most of the expensive rescaling and relinearization operations for ciphertext multiplications. Second, Sphinx accelerates the encryption operation and reduces the ciphertext size, which further lowers the communication time between the client and server.</p><p>Sphinx combines the above insights and optimizations to offer a reasonable privacy-accuracy trade-off, efficient training, and real-time inference. In the experimental section (Section IX), we show that Sphinx achieves 35? less training time and 5? lower communication costs for both neural networks on MNIST and CIFAR-10 datasets than the pure HE method. For inference, Sphinx achieves real-time inference in the online phase (0.05s for MNIST and 0.08s for CIFAR-10), which is 4 orders of magnitude faster compared to the pure HE method. Moreover, given the same privacy budget, Sphinx achieves a similar model accuracy as the pure DP training algorithm DPSGD in <ref type="bibr" target="#b14">[15]</ref> (96% accuracy under = 2, ? = 10 -5 for MNIST) without a trustworthy server.</p><p>One line of criticism of DP-based algorithms is the huge gap between the upper bound of privacy loss and the realistic privacy leakage in adversarial scenarios <ref type="bibr" target="#b19">[20]</ref>. To verify the privacy guarantee provided by Sphinx, we evaluate the privacyaccuracy trade-off against current reconstruction attacks from gradients <ref type="bibr" target="#b20">[21]</ref> (Section IX-E). The result shows that even with similar privacy cost compared to DP solutions, Sphinx can achieve a significantly stronger defense against attacks in a practical scenario.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This section reviews existing privacy-preserving deep learning solutions that focused either on privacy-preserving inference or on privacy-preserving training but not both, which motivates our design of Sphinx. We provide a qualitative comparison between Sphinx and these works regarding the threat model and relative training and inference speeds approximated from their evaluation results in Table <ref type="table" target="#tab_1">I</ref>.</p><p>Privacy-Preserving Inference on Neural Networks A number of solutions focus on the inference procedure, where the server holds a well-trained model to provide predictions-as-aservice for clients ([2]- <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b27">[28]</ref>). In this setting, since the model is either public or proprietary to the service provider, it is stored and evaluated as plaintext. Thus, the forward propagation only involves matrix multiplications between the encrypted input data and unencrypted model parameters, on which lots of optimizations have been proposed. For instance, Gazelle <ref type="bibr" target="#b2">[3]</ref> introduced a hybrid secure inference algorithm, where the linear layers are evaluated with packed additively homomorphic encryption (PAHE) and the non-linear layers with garbled circuits. Based on Gazelle, Delphi <ref type="bibr" target="#b4">[5]</ref> improves the inference latency significantly by bringing forward the heavyweight cryptographic computations to the preprocessing phase and developing a planner based on neural architecture search (NAS) to search for the most efficient approximation model meeting the target accuracy goal.</p><p>However, these optimizations for private inference cannot be transferred into online learning setting, as the training algorithm needs to encrypt the model parameters and gradients to protect training data against the cloud. Our work, on the other hand, is able to offer training and inference services simultaneously in a privacy-preserving way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Privacy-Preserving Training on Neural Networks</head><p>Solutions focusing on privacy-preserving training on neural networks can be classified into centralized and distributed, depending on the number of servers required. Some of the centralized training methods ( <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>) adopt HE to encrypt both model weights and user data to outsource the training to the service provider. However, their training processes involve arithmetic operations between the encrypted neural network layers and encrypted features, which is an order of magnitude slower than the arithmetic operations between plaintexts and ciphertexts (Table <ref type="table" target="#tab_3">II</ref>). Furthermore, these solutions do not take inference into consideration and simply apply the timeconsuming forward phase in training for evaluation. Our work, however, provides a specific inference protocol based on lightweight secret sharing techniques, which is much faster, thus achieving real-time inference response. Some other centralized training solutions adopt DP-based algorithms to protect privacy information by introducing randomization in the algorithm ( <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>). For instance, Abadi et al. <ref type="bibr" target="#b14">[15]</ref> proposed differentially private stochastic gradient descent (DPSGD) with Gaussian noiseadditive mechanism and proved a tighter bound on the privacy guarantee. DP-FTRL improves DPSGD with tree aggregation trick to get rid of sampling and shuffling <ref type="bibr" target="#b18">[19]</ref>. Instead of perturbing the gradients, some works proposed to introduce randomization into the objective function <ref type="bibr" target="#b15">[16]</ref>, the intermediate activation features <ref type="bibr" target="#b16">[17]</ref>, and the labels <ref type="bibr" target="#b30">[31]</ref>. However, the trade-off between model performance and privacy level in DP-based algorithms has long been criticized in practice. Zhu at al. showed that to successfully defend privacy leakage from gradients, the model performance may drop significantly due to the noise added on gradients <ref type="bibr" target="#b32">[33]</ref>. As a result, to preserve model performance, the privacy constraint companies and researchers used can hardly protect individual data ( <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>). Also, while focusing on publishing perturbed models,   <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>). For instance, SecureML <ref type="bibr" target="#b10">[11]</ref> firstly proposed MPC alternatives to linear and non-linear functions to perform secure neural network training and inference. ABY3 <ref type="bibr" target="#b11">[12]</ref> introduced the three-party computation version of ABY in <ref type="bibr" target="#b38">[39]</ref> to improve efficiency, allowing switching between arithmetic, boolean, and Yao's GCs. SecureNN <ref type="bibr" target="#b12">[13]</ref> applied three-party and four-party secure computation protocols to accelerate neural network secure training. FLASH <ref type="bibr" target="#b22">[23]</ref> proposed a fourparty privacy-preserving deep learning framework to achieve the strongest security notion of guaranteed output delivery, which allows one of the servers to be an active adversary. However, all of them assume multiple non-colluding servers or honest majority setting where the majority of the servers are trustworthy, and thus are impractical under the MLaaS setting.</p><p>There are also a number of works focusing on privacypreserving federated learning, where multiple data owners collaboratively learn their local models without exposing user privacy ([40]- <ref type="bibr" target="#b51">[52]</ref>). In the federated learning setting, the clients maintain their data and update the model locally. For instance, Shokri at al. proposed distributed selective stochastic gradient algorithm (selective SGD) and applied DP mechanisms on the uploaded gradients <ref type="bibr" target="#b39">[40]</ref>. Meanwhile, Sinem Sav et al. employed HE to perform federated neural network learning across data owners with several cryptographic optimizations <ref type="bibr" target="#b47">[48]</ref>. In a federated learning setting, clients are the data owners as well as main executors for the heavy computations of model training rather than service providers. As we are focusing on the MLaaS setting where the clients outsource the heavy computations to the service provider with large amounts of computation power, these works are beyond the scope of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>We provide basic knowledge about the threat model, deep neural networks, and privacy-preserving techniques, including HE and DP, that underlie Sphinx's training and inference protocols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Threat Model</head><p>We consider a client-server model with two parties: client A and server B, where client A owns the data and server B provides training and inference services. In the training phase, A continuously provides collected input data, and B provides online training service to update the model. After following the privacy-preserving training protocol, B generates a welltrained model, which can be sent back to A or stored on the server to provide further secure inference service for the client. In the inference phase, the client provides the input data, and the service provider outputs the prediction result based on the trained model and the input data. During the training and inference procedures, B should not obtain any sensitive individual information from the input data. We consider a semi-honest server as in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>: it adheres to the protocol but is curious to infer privacy information from client A based on all the available information provided during the interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN Training and Inference</head><p>Deep neural networks (DNNs), as a family of machine learning models, adopt stacked artificial neural network layers to extract features from raw data at different abstract levels hierarchically. With stacked layers, deep learning models can approximate arbitrary complex functions after training on collected massive data, and is capable of performing classification and regression tasks with higher accuracy and robustness compared with classical machine learning models <ref type="bibr" target="#b52">[53]</ref>. Here, taking convolutional neural networks (CNNs) as an example, we focus on the abstraction of DNNs from a transformation perspective. A typical CNN processes a batch of input images through a sequence of neural network layers performing various transformations, including linear layer and non-linear layers. The CNN output can be probabilities for classification or prediction results for regression, based on the target task setting.</p><p>Linear layers. In a CNN, linear layers are of two types: convolutional (Conv) layers and fully-connected (FC) layers. Generally, input raw images are processed by several Conv layers to extract image features at different abstractions, which are then flattened and fed to the FC layers to generate classification or regression results. Despite the differences in terms of structure, implementation and functionality, they both offer affine transformations involving linear transformations and translations.</p><p>Non-linear layers. The non-linear layers perform non-linear transformations on the input features. The most common ones in CNNs are max-pooling functions and activation functions. While activation functions such as ReLU and Sigmoid generally perform non-linear functions on the input features element-wise, the max-pooling function takes small chunks of input images or feature maps to perform sub-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Homomorphic Encryption</head><p>In our work, we focus on the CKKS HE scheme proposed in <ref type="bibr" target="#b53">[54]</ref> and implemented in SEAL <ref type="bibr">[55]</ref>. CKKS is a leveled homomorphic encryption supporting a limited number of additions and multiplications on ciphertexts based on the ring learning with error (RLWE) problem. Here we only give a brief introduction to CKKS and some operations (e.g., lazy rescaling, lazy relinearization) involved in our optimization section. For more details, please refer to <ref type="bibr" target="#b53">[54]</ref>.</p><p>1) Mathematical Background: To encrypt a message m ? C N/2 , a vector of floating-point values, CKKS firstly encodes it into a plaintext r. The plaintext space is a polynomial ring r ? Z q [X]/(? 2N (X)), where ? M (X) is the M -th cyclotomic polynomial and q the modulo of polynomial coefficients. Then with the public key p, CKKS encrypts a plaintext r with the public key into a ciphertext consisting of a pair of polynomials (c 0 , c 1 ). To protect the preferred precision of messages during the encoding, CKKS multiplies the message by a scaling factor ? &gt; 0 to keep precision of 1/?.</p><p>CKKS supports addition and multiplication between ciphertexts. More concretely, given two ciphertexts ct 1 and ct 2 from messages m 1 and m 2 , we have Dec(ct 1 +ct 2 ) ? m 1 +m 2 and Dec(ct 1 * ct 2 ) ? m 1 * m 2 . Also, CKKS supports operations between ciphertexts and plaintexts, which is much cheaper than the corresponding operations between ciphertexts.</p><p>Rescaling After multiplication (with ciphertext or plaintext), the scaling factor of the output ciphertext turns to ? 2 and requires rescaling. CKKS performs rescaling by truncating a ciphertext into a smaller modulus q/?. For every ciphertext, the number of rescaling operation allowed is called the 'level' of the ciphertext (denoted by L in the paper), representing the number of multiplication/rescaling operations still allowed.</p><p>Relinearization After multiplication between two ciphertexts, the result ciphertext size will grow exponentially. To prevent it, CKKS performs relinearization to transform the ciphertext back to a pair of polynomials. <ref type="bibr" target="#b54">[56]</ref> presents this relinearization technique in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Differential Privacy</head><p>DP provides a theoretic privacy guarantee for randomized algorithms. It is a formal definition of privacy loss in the context of statistical and machine learning analysis. Through DP, data curators can release statistical results while still protecting individual information in the dataset. (</p><formula xml:id="formula_0">)<label>1</label></formula><p>Adjacent inputs mean any two datasets that differ on only one row. With this definition, DP mathematically guarantees that the result of a DP analysis almost makes no difference to the analysis, whether or not any individual's private information is included in the input. To fulfill its definition, DP mechanism injects random noise into the original data or the statistical results, so deleting a single record from the original dataset will only generate a negligible effect on the algorithm output. Thus, we can obtain meaningful statistical results with acceptable accuracy loss. These mechanisms usually involve a trade-off between model performance and privacy level, depending on the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMBINING DIFFERENTIAL PRIVACY AND HOMOMORPHIC ENCRYPTION</head><p>In this section, by diving into the characteristics of DP and HE, we make several key observations that motivate the combination of DP and HE in our design of Sphinx.</p><p>Differential privacy improves efficiency. As shown in Table <ref type="table" target="#tab_3">II</ref>, we observe that the homomorphic arithmetic operations (multiplication and addition) take much less time when one operator is plaintext. As training and inference over DNNs involve a lot of matrix multiplications, by introducing DP to protect model parameters as plaintexts, we can significantly reduce the computation overhead in the training procedure compared to the pure HE methods. Furthermore, we observe that DP enables the lightweight techniques used in private inference works ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>), which achieve low inference latency yet assume a public inference model in plaintext. In Sphinx, most of the model parameters are protected with DP mechanism and stored in plaintexts on the service provider. Thus, we can adopt the state-of-the-art cryptographic methods for private inference in Sphinx and significantly reduce the inference time in the online phase.</p><p>Secure computation improves privacy. We observe that we can achieve DP preservation without a trusted aggregator through secure computation schemes. Secure computation protocols ensure that the computation service provider executes the computation task without learning anything about the input data. For example, through HE, users can encrypt their data before sending them to the aggregator <ref type="bibr" target="#b55">[57]</ref>. HE allows the aggregator to perform secure aggregation to get meaningful statistical results, upon which the differentially private noiseadditive mechanism is applied. During the procedure, the individual privacy is protected by HE all the time against the aggregator. In this way, the combination of the two privacypreserving methods enables centralized differentially private   Masking defends attacks. We find that by hiding the bias parameters/gradients, we can effectively lower the risk of an attacker reconstructing data under DP mechanisms. Geiping et al. proved that to reconstruct the input of a biased fullyconnected layer uniquely from the network's gradients, the knowledge of the derivatives w.r.t both the bias and the activation layer are essential <ref type="bibr" target="#b56">[58]</ref>. By masking these gradients and activations against potential attackers, Sphinx effectively defends the reconstruction attacks under the same privacy level, as shown in Section IX-E. Based on the above observations, we propose Sphinx, a privacy-preserving online learning framework for DNNs by combining HE with DP. In Sphinx, all the linear layers in DNNs are divided into two parts: the linear components and the bias components. Sphinx encrypts the bias components with HE, and perturbs the linear components with DP. We note that this design builds a reciprocal relation between DP and HE techniques: V. DESIGN This section describes the design of Sphinx in detail. We illustrate the model architecture in Figure <ref type="figure" target="#fig_2">2</ref>. Given one input sample, the forward propagation operation for each convolutional or fully connected layer in feed-forward neural networks can be formalized as:</p><formula xml:id="formula_1">N = 2 12 L = 1 N = 2 13 L = 3 N = 2</formula><formula xml:id="formula_2">a i+1 = f (W i a i + b i ),<label>(2)</label></formula><p>where a i represents the input vector of the i-th layer (a 1 is the input data), W i a i + b i is the affine transformation of the input feature, and f denotes the composite function including operations such as pooling and non-linear activation functions. We separate the affine transformation into the linear part which involves W i , and the translation part that contains b i . We call</p><formula xml:id="formula_3">W = (W 1 , W 2 , ...W K ) the linear components and b = (b 1 , b 2 , ...b K )</formula><p>the bias components in the rest of the paper, where K denotes the number of linear layers in the deep neural networks. In this section, we will explain how to adopt HE and DP in the two components, respectively, to harvest the benefits from both techniques. The design can be adopted for convolutional neural networks (CNN) and multilayer perceptron (MLP). Here we focus on the convolutional neural networks containing convolutional layers, non-linear activations, pooling layers, and fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Phase</head><p>We observe from Table II that the homomorphic matrix multiplication between a ciphertext and a plaintext is much faster Computes the average gradients of bias components:</p><formula xml:id="formula_4">[g b ] ? 1 B ( j [? b L(x j )]); 9</formula><p>Updates the bias components:</p><formula xml:id="formula_5">[b t+1 ] ? [b t ] -? t [g b ];</formula><p>Sends [? W L(x j )] back to the client; Client:</p><formula xml:id="formula_6">Decrypts [? W L(x j )] with SK;</formula><p>Clips the gradient:</p><formula xml:id="formula_7">? W L(x j ) ? max(1, ? W L(x j ) 2 C</formula><p>) ;</p><p>Computes the average gradients of linear components:</p><formula xml:id="formula_8">g W ? 1 B ( j ? W L(x j ));</formula><p>Adds noise:</p><formula xml:id="formula_9">g W ? g W + N (0, ? 2 C 2 B 2 I) ;</formula><p>Sends g W back to the server; Server:</p><p>Update the linear components:</p><formula xml:id="formula_10">W t+1 ? W t -? t g W ;</formula><p>than that between two ciphertexts. To exploit such performance characteristic, for each layer, we extract and encrypt the bias component b with HE, leaving the linear component W perturbed with DP later. Here we show how this modification avoids most of the computationally intensive operations and improves training efficiency while still protecting the input data and model functionality against the server.</p><p>During the training procedure, the client encrypts the training data and then sends it to the server as the encrypted input of the first layer in the model. Given a ciphertext [u] and a plaintext v, the scalar multiplication result is also a ciphertext [u * v]. Thus, for every layer in the neural networks, the forward propagation operation can be formalized as</p><formula xml:id="formula_11">[a i+1 ] = f (W i [a i ] + [b i ]),<label>(3)</label></formula><p>where only scalar multiplication and homomorphic addition operation occur: the homomorphic matrix multiplication between ciphertexts is avoided with W i unencrypted.</p><p>The forward propagation continues layer by layer until it produces the cost function L(?). To minimize it using gradient descent, we need to adopt the backpropagation algorithm to calculate the gradients of the cost function ? ? L(?) with respect to the model parameters ?, including W from the linear components and b from the bias components. For one layer described in Equation <ref type="formula" target="#formula_2">2</ref>, starting from the gradient with respect to the output of the layer ? a i+1 L, the backpropagation computation consists of two parts: (i) computing the gradients on the current layer (? b i L and ? W i L), and (ii) propagating the gradients back to the previous layer (? a i L). As the propagated gradients are encrypted, the backpropagation operation in each layer can be formalized as follows:</p><formula xml:id="formula_12">[? b i L] = [? a i+1 L] f (W i [a i ] + [b i ]) [? W i L] = [a i ]([? a i+1 L] f (W i [a i ] + [b i ])) [? a i L] = W i ([? a i+1 L] f (W i [a i ] + [b i ])).<label>(4)</label></formula><p>We can see from the formulas that with the linear components W unencrypted, while the gradient calculation for each layer (? b i L and ? W i L) stays the same as that for the fully encrypted model, the homomorphic matrix multiplications between ciphertexts are avoided during the gradient propagation, i.e., calculating ? a i L.</p><p>The server trains neural networks with the well-known stochastic gradient descent (SGD) algorithm, iterating on minibatches of the training dataset to update model parameters. Algorithm 1 outlines Sphinx's training algorithm. During the process, the linear component W i for each layer is perturbed with DP via introducing additive noise on their gradients. At each step, the client takes a random sample batch with size B, encrypts it with the generated public key, and sends it to the server as the input. Based on the input batch, the server computes the encrypted gradients of all the model parameters following the forwarding and back-propagation formulas described above. For the bias components, the server aggregates and averages the gradients locally, and directly updates the model parameters <ref type="bibr">[b]</ref>. For the linear components, the server sends their gradients [? W L(x j )] back to the client for decryption. Upon receiving the encrypted gradients, the client decrypts them with the private key and clips the gradients to ensure that ? W L(x j ) 2 ? C, thus guaranteeing that the noise added later is sufficient to meet the target DP level. Finally, the client adds Gaussian noise on the averaged gradients and sends perturbed g W back to the server to update the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Inference Phase</head><p>In the inference phase, the client sends the encrypted samples to the service provider for prediction or classification requests. We design a specific protocol for inference based on the following insights: i) while the training data are prepared beforehand in mini-batches, inference samples often come in a real-time stream requiring immediate feedback, and ii) in Sphinx, the updated model contains plain noisy linear components. Thus, we can utilize the secret sharing based technique used in private inference protocols on linear operations <ref type="bibr" target="#b4">[5]</ref>, which assumes a public plaintext model on the server. Here we give an overview of our protocol.</p><p>On the server, Sphinx converts the linear components W into fixed-point representations and embeds them into a prime finite field F. Once the model is updated, Sphinx takes a preprocessing phase to generate secret shares between the client and the server for each layer i:</p><p>1) The client and the service provider respectively generate random masking vectors r i , s i in R n and R m , where n, m are the sizes of the input and output in i-th linear layer.</p><p>2) The client encrypts r i with HE scheme and sends [r i ] to the server. The server then computes</p><formula xml:id="formula_13">[W i ? r i + b i -s i ]</formula><p>and sends it back to the client.</p><p>3) The client decrypts it and obtains</p><formula xml:id="formula_14">(W i ? r i + b i -s i ).</formula><p>Thus the client and the server hold an additive secret sharing of W i r i + b i for each layer. In the online phase, once an input sample x comes, Sphinx executes the following steps starting with i = 1:</p><p>1) The client calculates xr i and sends it to the server, and the server calculates</p><formula xml:id="formula_15">W i (x -r i ) + s i . Since the client has (W i ? r i + b i -s i )</formula><p>, the client and the server now hold an additive secret sharing of W i x + b i .</p><p>2) The server sends W i (x -r i ) + s i to the client who then adds it with the local share to obtain W i x + b i , and performs non-linear operations on it.</p><p>3) The client repeats the same procedure for the next layer. In the inference algorithm, the encrypted bias components are handled in the preprocessing phase, and all the arithmetic operations in the online phase are performed over prime fields.</p><p>We note that private inference protocols generally perform non-linear layers using MPC techniques such as garbled circuits to protect the intermediate features against both parties <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>. In our online learning setting, the training data also comes from the client. Thus, Sphinx has no need to protect the non-linear function results against the client and avoids heavyweight cryptographic schemes.</p><p>To handle n inference samples, Sphinx needs n different additive secret shares, which can also be prepared in minibatches to accelerate the preprocessing phase, as homomorphic ciphertexts in CKKS are efficient for SIMD computations <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PRIVACY ANALYSIS</head><p>We now provide privacy analysis for Sphinx. Essentially, Sphinx has different levels of privacy preservations of user data for the training and inference phases.</p><p>For the training phase of Sphinx, we adopt DP to provide theoretical privacy guarantees. Our training protocol can be viewed as a homomorphic version of DPSGD <ref type="bibr" target="#b14">[15]</ref> without a trusted server. It adopts the Sampled Gaussian mechanism (SGM) that samples mini-batches from the dataset and adds Gaussian noise on the aggregated gradients w.r.t. the linear components. Sphinx enables centralized differentially private training without a trusted server for several reasons: i) The input data sent is protected by HE, thus not exposed to the server; ii) the gradients calculated on the server side are in encrypted form so that individual gradients are not exposed to the cloud server, and iii) the noise-additive mechanism is applied to the aggregated gradients on the client side after decryption. Thus, the random number generator is preserved against the server. During the training, only the perturbed aggregated gradients of the linear components are exposed to the server, which conforms to the threat model used in DP-based methods ([15]- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>) that the DP mechanism protects both the final model and the aggregated gradients in mini-batches.</p><p>The DP mechanism in Sphinx is an instance of adaptive composition, where we sequentially introduce noise on the gradients for each training step. In DP analysis, the privacy loss is defined as the random variable measuring the difference between the two probability distributions before and after adding noise. Specifically, for two adjacent inputs d and d , the privacy loss of a DP mechanism M at the outcome o is defined as:</p><formula xml:id="formula_16">c (o; M, d, d ) log Pr[M(d) = o] Pr [M (d ) = o] .<label>(5)</label></formula><p>With more and more aggregated gradients exposed to the server during the training process, the privacy loss accumulates. In our training protocol, we adopt the moments accountant method proposed in <ref type="bibr" target="#b14">[15]</ref> to estimate and trace the bound of the accumulated privacy loss. The moments accountant performs composition on the log moments bounds, which can lead to a tighter bound compared to the strong composition theorem in <ref type="bibr" target="#b57">[59]</ref>. Given two adjacent inputs d, d and the DP mechanism M , the ? th moment is defined as:</p><formula xml:id="formula_17">? M (?; d, d ) log E o?M(d) [exp (?c (o; M, d, d ))] .<label>(6)</label></formula><p>[15] has proven the composability of moments bound and how it is related to the ( , ?)-DP. Thus in Sphinx, the moments accountant estimates the bound of ? M (?) at each learning step and sums them all to estimate the overall bound, which can be converted to the accumulated privacy cost during the training procedure, i.e., the current ( , ?)-DP guarantee. The bound on the privacy loss of Sample Gaussian mechanism has been studied in several works ( <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b58">[60]</ref>, <ref type="bibr" target="#b59">[61]</ref>). Based on the asymptotic bound proof in DPSGD <ref type="bibr" target="#b14">[15]</ref>, we have the DP guarantee of our training protocol. Due to the post-processing immunity property of DP, calculations on the model in the inference phase will not incur any further privacy leaks on the training data. , Algorithm 1 is ( , ?)-DP.</p><p>For the inference phase, we adopt the private inference technique based on secret sharing. Here we prove that the inference protocol gives the privacy guarantee that the service provider cannot infer any useful information about the evaluated data. Specifically, for the inference protocol ? between the server holding neural network model parameters</p><formula xml:id="formula_18">M = (W 1 , W 2 , .., W K , [b 1 ], [b 2 ], ...[b K ]</formula><p>) and the client having the evaluated input data x, we prove that there exists an efficient simulator Sim such that the view of of the server executing ? is computationally indistinguishable from the output of the simulator, i.e., View ? S ? c Sim S (M). We give the simulator Sim that proceeds as follows: 1) Sim initializes the server with a uniform random tape. It chooses a public key P K for the HE scheme and sends P K to the server. 2) In the preprocessing phase, once the model is updated, Sim encrypts 0, sends [0] to the server, and receives the returned result for each layer. 3) In the online phase, once an input sample x comes, Sim chooses a uniformly chosen r, sends it to the server, and receives the returned result for each layer. In the preprocessing phase, instead of sending the random mask [r i ], the simulator sends [0] to the server for each layer. It follows from the semantic security of HE that they are computationally indistinguishable. In the online phase, instead of sending masked input xr i , the simulator sends uniformly chosen value from R. They are both uniformly distributed and thus computationally indistinguishable.</p><p>As a result, the real world distribution is computationally indistinguishable from the simulated distribution via a simple hybrid argument. As the simulator does not use any information about the evaluated input x, the server learns nothing in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. OPTIMIZATIONS</head><p>We present several system optimizations for Sphinx, which fully exploit the possible merits of the combination of cryptographic tools and DP techniques to further accelerate the encrypted model training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fast Homomorphic Multiplication</head><p>To enable further multiplication, multiplication operation between a ciphertext and a plaintext are followed by rescaling, and that between two ciphertexts are followed by both rescaling and relinearization. Inspired by previous HE-based methods seeking to avoid heavy rescaling and relinearization with lazy rescaling <ref type="bibr" target="#b25">[26]</ref> and lazy relinearization <ref type="bibr" target="#b60">[62]</ref> techniques, we deliberately design multiplication behaviors between model parameters, input features and gradients in neural network layers. We show that with the linear components as plaintexts, Sphinx can eliminate most of the rescaling and relinearization operations during the training procedure <ref type="foot" target="#foot_1">1</ref> , which further enables it to adopt smaller encryption parameters with a shallower multiplicative depth, leading to less computation time and communication cost. In particular, while the pure Fig. <ref type="figure">3</ref>: The arithmetic operation behaviors between two layers in Sphinx.</p><p>HE method requires at least three levels (L = 3, N = 2 13 ) for rescaling operations during the training procedure<ref type="foot" target="#foot_2">2</ref> , Sphinx allows the encryption parameters (L = 1, N = 2 12 ) with only one level.</p><p>For every ciphertext, we set two flags (Rx, Lx) to represent whether the ciphertext needs rescaling or relinearization for further multiplication. Fresh encrypted ciphertexts have the flags (R0, L0), which means that they are ready for multiplication. The resulting ciphertext of multiplication between two ciphertexts has the flags (R1, L1), requiring rescaling and relinearization operations to enable further multiplication. The resulting ciphertext of multiplication between a ciphertext and a plaintext has the flags (R1, L0), requiring only rescaling.</p><p>Based on the design in Section V, we illustrate the detailed arithmetic operation behaviors for each layer in Sphinx in Figure <ref type="figure">3</ref>. In the forward propagation, the activations or input features [a i ] fed into layer i are multiplied and aggregated with the plaintext linear component W i with no rescaling needed. The reason is that homomorphic addition can be performed among the ciphertexts with the same augmented scaling factor and ciphertext size. In particular, we can perform addition on two ciphertexts if they both have the flags (R0, L1), (R1, L0), or (R1, L1). We maintain our encrypted bias components with the flags (R1, L0), thus the addition between W T l [a i ] and [b i ] can be proceed without rescaling. The result is then fed into the non-linear activation function.</p><p>For backpropagation, given the gradients from the next layer i + 1, the procedure follows Equation <ref type="formula" target="#formula_12">4</ref>. The gradients of the bias components [? b i L] can be obtained directly from [? a i+1 L] f (W i a i + b i ) and have the flags (R0, L0). In order to update b i efficiently, instead of performing rescaling on [b i ], we perform an inverse-rescaling operation on the gradients [? b i L] to increase the scaling factor of ciphertexts by simply performing scalar multiplication with 1, which is much faster. The inverse-rescaled gradients have the flags (R1, L0) now and can be updated into [b i ] without further rescaling.</p><p>For the computation of gradients with respect to the linear components [? W i L] and the propagated gradients [? a i L],</p><p>the result goes back to the client for decryption and DP mechanism (DP-decryption in Figure <ref type="figure">3</ref>) to avoid further rescaling and relinearization operations. Directly performing decryption without rescaling and relinearization also results in more accurate plaintexts, because rescaling performs rounding on ciphertexts and relinearization introduces noise. We also note that without relinearization, the size of the gradients of linear components [? W i L] will increase, resulting in a higher communication cost. However, because the bottleneck of DP-decryption is the ciphertext decryption on the client side and Sphinx conducts the communication in an asynchronous fashion, the overall training runtime will not increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accelerating Client-Server Communication</head><p>Sphinx adopts an interactive protocol between the client and the cloud server mainly for two purposes: i) to perform the DP aggregation on the gradients of model linear components as shown in Algorithm 1, and ii) to evaluate the non-linear activation functions in plaintext on the client side for both training and inference phases, which are not supported by the current HE schemes. Also, the decryption and re-encryption on the client-side work as client-aided bootstrapping to refresh ciphertexts to allow more arithmetic operations <ref type="bibr" target="#b2">[3]</ref>. The interactive protocol is straightforward: the server sends the ciphertexts back to the client for evaluation. The client decrypts and evaluates them in plaintext and sends them back to the server after either re-encryption or DP mechanism as shown in Algorithm 1.</p><p>To reduce the communication cost between the two parties, we implement the following optimizations: Forward propagation cache. In the training procedure, to backpropagate the gradients across layers, i.e. calculating [? a i+1 L] f (W i a i + b i ), the gradients of the non-linear activation functions f (W i a i + b i ) are involved. Instead of sending these encrypted activation functions back to the client for decryption and gradient calculation in the backpropagation phase, we realize that it is more efficient to calculate them beforehand in the forward propagation to avoid duplicate computations and communications. Thus, we design the forward propagation cache in Sphinx: In the forward propagation phase, the client caches the gradients of each non-linear layer locally as plaintexts, which can later be used in the backpropagation phase. For max-pooling layers, the client caches the mapping between the input features and subsampled output features. Thus, both the maximum feature values and the subsampled indices are protected against the service provider.</p><p>Zero decryption and zero encryption. In the training protocol, decryption and re-encryption occur in every ciphertext sent to the client, for which we realize that no more arithmetic operations are needed. Thus, Sphinx adopts zero decryption to reduce the level of prepare-to-send ciphertexts to 0, resulting in a much smaller ciphertext size during the communication. Table <ref type="table" target="#tab_6">III</ref> shows the ciphertext sizes under different encryption parameters.  For the re-encryption part on the client size, we realize that homomorphic addition between ciphertext and plaintext is much faster than the encryption operation, as shown in Table <ref type="table" target="#tab_3">II</ref>. Thus, the client in Sphinx adopts an encryption method we called zero encryption. In the offline preprocessing phase, the client prepares a stream of zero ciphertexts by encrypting 0, which works as one-time pad ciphertexts. Once the client needs to encrypt a message, it encrypts the plaintext by directly adding it with a zero ciphertext and sends it to the server, thus shortening the encryption time in the online phase.</p><formula xml:id="formula_19">N = 2 12 N = 2 13 N = 2 14 L = 1 L = 0 L = 3 L = 0 L = 7 L = 0 Size (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. SYSTEM IMPLEMENTATION</head><p>We implement Sphinx, our privacy-preserving online learning framework in C++. We use SEAL 3.6 as the homomorphic encryption library <ref type="foot" target="#foot_4">3</ref> . Inspired by TenSEAL<ref type="foot" target="#foot_5">4</ref> , a library supporting homomorphic encryption operations on high-dimensional tensors, we implement the homomorphic versions of the common operations in neural networks layers, such as convolution, pooling, and dot-product. Following <ref type="bibr" target="#b25">[26]</ref> and <ref type="bibr" target="#b23">[24]</ref>, we pack our data and activation features in the batch axis (batchaxis packing) during the training. For example, for an input image, Sphinx stores a 3D ciphertext tensor of shape (C,H,W), removing the batch dimension, which is much efficient for training over mini-batches. Sphinx's deep learning framework implementation is based on KANN <ref type="foot" target="#foot_6">5</ref> , a lightweight library allowing efficient inference and training on neural networks including MLP, CNN, and RNN. We implement our inference protocol based on Delphi<ref type="foot" target="#foot_7">6</ref>  <ref type="bibr" target="#b4">[5]</ref>. Furthremore, we adopt the moment accountant implemented in <ref type="bibr" target="#b58">[60]</ref> to keep track of the privacy loss during the learning process <ref type="foot" target="#foot_8">7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. EVALUATION</head><p>In this section, we evaluate the performance of Sphinx with respect to both computation efficiency and privacy preservation. Our results reveal that:</p><p>? For training, Sphinx achieves 35? less training time and 5? lower communication cost for both neural networks on MNIST and CIFAR-10 datasets than pure HE methods (Section IX-B). ? For inference, Sphinx achieves real-time inference in the online phase (0.05s for MNIST and 0.08s for CIFAR-10), which is 5.8 * 10 4 ? and 4.2 * 10 5 ? faster compared to pure HE methods, respectively. Moreover, the communication cost of Sphinx is 1.9 * 10 5 ? and 3.6 * 10 4 ? lower for both tasks (Section IX-C).</p><p>? Given the same privacy budget, Sphinx achieves comparable model accuracy compared with the pure DP training algorithm DPSGD <ref type="bibr" target="#b14">[15]</ref>, without a trusted server (Section IX-D). ? We observe that by masking the bias components, Sphinx effectively defends the reconstruction attack from exposed gradients, compared to the pure DP algorithm (Section IX-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Setup</head><p>We evaluate Sphinx on two physical machines for the two parties, each with 40 Intel Xeon CPU E5-2683 v4 cores at 2.1GHz and 128GB memory. The two machines are connected in LAN networking using a 10Gbps link. All experiments are conducted under Ubuntu 18.04.5 LTS, and our library is compiled using GCC 7.5.0 with -O2 optimization setting.</p><p>In following experiments, we use two image datasets:</p><p>1) The MNIST dataset contains 70,000 28*28 grayscale images for handwritten digits from '0' to '9', 60,000 for training, and 10,000 for testing. The task is to classify the correct handwritten digit in the given image. 2) CIFAR-10 is another image dataset containing 60,000 32?32 color images (with 3 channels), 50,000 for training and 10,000 for testing. The images in CIFAR-10 are classified into 10 classes, including birds, cats, planes, etc. The task is to classify the correct label for the given color image. The images in CIFAR-10 are more complicated with more labels and channels, thus requiring deeper neural networks. We implement and train the two neural network architectures (Figure <ref type="figure" target="#fig_6">4</ref> and Figure <ref type="figure" target="#fig_7">5</ref>) for each dataset similar to <ref type="bibr" target="#b1">[2]</ref>. We add the bias components after the linear operation for each convolutional layer. The best accuracy we achieve for both non-private models on MNIST and CIFAR-10 is 98.5% and 78.8%, respectively. To avoid the influence of thread synchronization, we use a single thread for the server and the client for runtime measurements in Section IX-B and IX-C.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training with Sphinx</head><p>For training, we set the batch size B = 500 for the model in Figure <ref type="figure" target="#fig_6">4</ref> on MNIST, and B = 2000 for the model in Figure <ref type="figure" target="#fig_7">5</ref> 1) Input: R 3?32?32 2) Convolution: window size 5?5 with stride <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b0">1)</ref>  on CIFAR-10. The training data is shuffled, batched, encrypted on the client and then sent to the cloud. We adopt the stochastic gradient descent (SGD) method and set the learning rates to be 0.1 and 0.05 for both tasks. For CIFAR-10 task, we initialize the convolutional layers with the pre-trained weights trained on CIFAR-100 dataset.</p><p>We compare Sphinx with the pure homomorphic encryption (pure-HE) training method, where the whole model is encrypted, and all homomorphic arithmetic operations occur between ciphertexts. Our pure-HE version of the training protocol is similar to <ref type="bibr" target="#b9">[10]</ref>. For a fair comparison, we also adopt the client-aided method for non-linear layers in the pure-HE method to avoid expensive bootstrapping operations, but remove all the optimizations in Section VII. For both algorithms, we encrypt the input and bias weights with the lowest modulus levels available. We note that CryptoDL <ref type="bibr" target="#b21">[22]</ref> also offers a HEbased privacy-preserving training. However, it focuses on how to accelerate the non-linear activation functions by replacing them with polynomial functions, while Sphinx makes no assumption on the model architecture and has no restrictions on the form of non-linear activation functions. Thus it is orthogonal to this paper.</p><p>Figure <ref type="figure" target="#fig_8">6</ref> and Figure <ref type="figure" target="#fig_9">7</ref> show the improvement of Sphinx over the pure-HE method in terms of training time and communication cost, respectively. We observe that Sphinx achieves about 35? less training time and 5? lower communication cost for both neural networks on MNIST and CIFAR-10 than the pure-HE method. The main reason behind such improvement is that Sphinx avoids most of the heavy homomorphic operations, including ciphertext-ciphertext matrix multiplication, rescaling, and relinearization, by protecting the linear components as plaintexts. Furthermore, with the fast homomorphic multiplication optimization, Sphinx enables a shallower ciphertext level with lesser computation and communication costs.  Table <ref type="table" target="#tab_10">IV</ref> shows the breakdown performance in terms of the contribution by each optimization proposed in Section VII. We observe that while our na?ve Sphinx achieves a 7-8? runtime improvement compared to pure-HE method, fast homomorphic multiplication optimization (Section VII-A) further makes another 4-5? improvement on it. However, fast homomorphic encryption optimization may cause a higher communication cost due to the larger ciphertext size with lazy relinearization. The other two optimizations, forward propagation cache and zero encryption/decryption (Section VII-B), reduce the communication overhead of the na?ve method by about 20%, by reducing the required transferred ciphertexts and their sizes. Putting all optimizations together, Sphinx makes 5? and 1.3? improvements on the overall runtime and the communication cost, respectively, compared to the na?ve method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Inference with Sphinx</head><p>For the inference task, we conduct inference queries for a single sample and measure its inference latency. We compare our inference protocol with i) the pure-HE method mentioned above; ii) the forward propagation in the training protocol of Sphinx (Sphinx-FP), where noisy linear components, as well as all optimizations in Section VII, are used; iii) Gazelle <ref type="bibr" target="#b2">[3]</ref>, a hybrid private inference method based on HE and garbled circuits, and iv) Delphi <ref type="bibr" target="#b4">[5]</ref>, one of the state-of-the-art private inference protocols based on secret sharing and garbled circuits.   Sphinx-FP prepares zero ciphertexts in the preprocessing phase. Delphi requires a plain public model on the server, thus cannot be used for privacy-preserving online learning.</p><p>Table <ref type="table" target="#tab_11">V</ref> shows the latency and communication cost for inference with Sphinx in both preprocessing and online phases. Though we observe that the forward propagation of our training protocol in Sphinx already achieves almost 30? faster inference latency and 4? lower communication cost in both datasets compared to the pure-HE inference, they are still both expensive due to their homomorphic operations and batchaxis processing style<ref type="foot" target="#foot_9">8</ref> . However, our secret sharing based inference protocol in Sphinx achieves 5.8 * 10 4 ? and 4.2 * 10 5 ? faster inference latency and lower communication cost by about 1.9 * 10 5 ? and 3.6 * 10 4 ? in MNIST and CIFAR-10, respectively, compared to the pure-HE method, which is also comparable to state-of-the-art private inference methods such as Delphi. The reason behind the improvement is that Sphinx moves most of the heavy cryptographic operations to the preprocessing phase. Therefore, it executes the online inference involving only plain arithmetic operations over prime field, achieving significantly lower latency. Sphinx also achieves slightly better latency and communication overhead than Delphi because our setting avoids the overhead of heavy MPC techniques for non-linear layers, as mentioned in Section V-B. We note that these private inference protocols, including Gazelle and Delphi, require a public model and thus cannot be utilized in the online learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Privacy Cost of Sphinx</head><p>To evaluate the privacy-utility trade-off in Sphinx, we fix ? = 10 -5 and keep track of privacy cost of the DP mechanism and the model performance during the training on MNIST and CIFAR-10, as suggested in <ref type="bibr" target="#b14">[15]</ref>. To explore the influence of the noise mechanism on model performance, we conduct the experiments under different noise levels (? = 2, 4, 8). The gradient norm bound C is set to 3. We compare our Sphinx with the all-noisy algorithm, where all model parameters, including the bias components, are protected with the Sampled Gaussian mechanism, thus stored in the server as plaintexts, which is equivalent to the DPSGD algorithm in <ref type="bibr" target="#b14">[15]</ref>.</p><p>Figure <ref type="figure">8</ref> shows the evolution of accuracy and privacy cost of Sphinx and the all-noisy algorithm as the training proceeds. We have several observations: i) Sphinx achieves comparable model accuracy compared with the all-noisy algorithm across various privacy budgets and noise magnitudes without a trusted data aggregator; ii) With a larger magnitude of noise (?), the training process consumes the privacy budget more slowly ( ), yet leads to a more unstable learning process and harms the convergence rate. For example, when ? = 8 for MNIST and CIFAR-10, the model accuracies rise rapidly to 88% and 40% respectively in the first few epochs, but then struggle to climb slowly under strong fluctuations; iii) On MNIST, Sphinx achieves at most 92%, 94%, and 96% test accuracy for (0.5,10 -5 ), (1,10 -5 ), and (2,10 -5 )-DP under different noise levels. For CIFAR-10, Sphinx achieves 54%, 58%, and 72% test accuracy for (2,10 -5 ), (4,10 -5 ), and (12,10 -5 )-DP, which is slightly better across different noise levels compared to the all-noisy algorithm. It is because the gradients of the bias components are updated in encrypted form in Sphinx and thus exempted from the DP mechanism, resulting in overall lesser noise. Also, Sphinx only needs to clip the gradients over the norm of the linear components, which is smaller than that of the whole model: g W 2 &lt; g 2 , resulting in lesser gradient loss during clipping and faster convergence.</p><p>We note that the accuracy drop of the differentially private models on CIFAR-10 is larger than those on MNIST. The phenomenon is also observed in previous work <ref type="bibr" target="#b14">[15]</ref>. Besides the large data and task complexity of CIFAR-10, one reason is that deeper neural networks have more parameters, which leads to a larger g W 2 . Thus, with the same norm bound C, more information in the gradients will be clipped. We have also tried to choose a larger C, which, however, will increase the added noise N (0, ? 2 C 2 B 2 I) given the same noise level ? and degrades the training performance even more. Due to the lack of a solid theoretical foundation of deep learning and poor interpretability of neural networks, how to choose a proper model architecture, noise level ? and norm bound C based on the task/dataset to maximize the model performance is a challenging topic, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Sphinx Against Attacks</head><p>To evaluate how well Sphinx defends against known attacks in practical scenarios, we conduct the gradient-matching attack proposed in <ref type="bibr" target="#b20">[21]</ref> on the noisy gradients of Sphinx's linear components. The gradient-matching attack is a type of reconstruction attack method that can recover input images and labels from the gradients of the model. Initialized from random input, it utilizes the gradient descent method to recover the input images and labels by minimizing the difference between the derived gradients and real gradients. We conduct the attack on both Sphinx and the all-noisy model under the CIFAR-10 configuration in Section IX-B. With a large batch size, the attack effectiveness will decrease significantly due to the huge number of possible combinations of individual gradients in a batch. Hence, we relax the constraint and feed the attacker with the individual gradients of input images, on which the introduced noise on the aggregated gradients is distributed uniformly. Because the gradient-matching attack requires second-order derivatives of the loss function, we replace the ReLU functions in non-linear layers with Sigmoid functions, which have also been widely used in various deep neural networks. For Sphinx, since the bias components are encrypted, the attacker cannot derive the simulated input gradients. Thus, we also initialize the bias components with random values, and the attacker needs to learn the input as well as the bias weights simultaneously.</p><p>Figure <ref type="figure" target="#fig_11">9</ref> shows the results of the attack. We observe that although the defense against gradient-matching attack strengthens with the increase in noise level, we can recover most pixels of the input image from the complete set of noisy gradients with small noise. With the noise level ? = 2, we can still recognize the objects in some of the recovered images. On the contrary, by masking the bias components with the HE scheme, which is only a small part of the entire model, Sphinx effectively defends the reconstruction attack than the pure DP algorithm, even when their privacy costs are almost the same as shown in Section IX-D. It is because the masking technique requires the attacker to infer the hiding model parameters and the input data simultaneously, which is often harder under the same noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION AND FUTURE WORK</head><p>In conclusion, we presented Sphinx, a privacy-preserving online learning system. Sphinx divides online learning into the training and inference phases and combines different privacypreserving techniques reciprocally. Sphinx has significantly accelerated the privacy-preserving training and inference, enabling privacy-preserving online learning services on deep neural networks.</p><p>As Sphinx is compatible with any differentially private training algorithms that protect aggregated gradients in minibatches, we can also replace the Sampled Gaussian Mechanism with other DP mechanisms with various features according to the application requirements and data characteristics. For example, Kairouz et al. proposed an online learning DP mechanism that allows a more flexible data access pattern <ref type="bibr" target="#b18">[19]</ref>, which can be adopted in Sphinx when the training data is Non-IID. We will explore and evaluate the combinations between different DP mechanisms and HE schemes in the future.</p><p>In the inference phase of Sphinx, we adopted the secret sharing technique in the preprocessing phase to accelerate the inference. However, we need to rerun the preprocessing phase upon the updated model parameters, causing a waste of computation resources. For future work, we envision a privacypreserving online learning approach whose previous preprocessing results can be aligned with new model parameters with few extra calculations once the model is updated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: A privacy-preserving online deep learning service with both training and inference services. Both the data and model parameters are protected against the service provider.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 1 .</head><label>1</label><figDesc>A randomized mechanism M : D ? R with domain D and range R satisfies ( , ?)-differential privacy if and only if for any two adjacent inputs d, d ? D and for any subset of outputs S ? R, it holds that P r[M (d) ? S] ? e P r[M (d ) ? S] + ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: The architecture of Sphinx. For an DNN model, the linear components W are perturbed with DP, and the bias components Bias are encrypted with HE. F denotes the non-linear layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>By leaving the linear components unencrypted and handled by DP mechanisms, we can avoid most of the expen-sive homomorphic ciphertext-ciphertext matrix operations in the training procedure; ? With the noisy linear components as plaintexts, we can adopt state-of-the-art secure inference protocols based on secret sharing in the inference phase to achieve real-time prediction response; ? With data and gradients encrypted, we can perform DP mechanisms to add noise on the aggregated gradient with no need to trust the cloud; ? Masked neural network layers make it practically harder for attackers to spy on the input features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Sphinx Training Algorithm Input : Client: Training dataset X with size N , noise scale ?, gradient norm bound C; Server: Model M with K layers, loss function L(?) = 1 N j L(?, x j ), learning rate ? t , batch size B Output: Trained model parameters ? T = {W T , [b T ]} 1 The client creates an encryption key pair (P K, SK) and send the public key to the server; 2 The server initializes the plaintext linear components W 0 and the encrypted bias components [b 0 ] with P K; 3 for t ? 0 to T -1 do 4 Client: 5 Takes a random sample batch B = {x 1 , x 2 , ...x B } with sampling probability B/N , encrypt them with P K and send the encrypted batch to the server; 6 Server: 7 Computes the gradients for the linear components [? W L(x j )] and bias components [? b L(x j )] according to Equation 3,4 for each sample x j ; 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 1 . 2 N 2 T</head><label>122</label><figDesc>Given the training batch size m, the number of training steps T , and the size of the training dataset N , these exists constants c 1 , c 2 that, ? &lt; c 1 m , ?? &gt; 0, ?? ? c 2 m ? T log(1/?) N</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: The neural network architecture for the MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: The neural network architecture for the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: The training execution times on Sphinx and pure-HE method for one batch.</figDesc><graphic url="image-7.png" coords="11,71.68,218.87,205.28,131.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: The training communication costs on Sphinx and pure-HE method for one batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>8 Fig. 8 :</head><label>88</label><figDesc>Fig. 8: Results on the test accuracy and the privacy cost with Gaussian noise of different magnitudes on the MNIST and CIFAR-10 datasets. We fix the ? = 10 -5 for all the experiments. One epoch denotes one iteration over the training dataset.</figDesc><graphic url="image-11.png" coords="12,62.97,174.05,159.04,104.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Visualization of the images recovered by the gradient-matching attack in [21] with different magnitudes of Gaussian noise: (a) the original images, (b) the images recovered from the gradients of all noisy model, where both the linear and bias components are plaintexts and protected by DP mechanisms, and (c) the images recovered from the gradients of Sphinx.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>A qualitative comparison between existing privacy-preserving deep learning outsourcing schemes and Sphinx. A and P denote active and passive adversarial capabilities. MT denotes honest majority setting, where most of the servers are assumed to be trustworthy. HE, GC, SS and DP stand for homomorphic encryption, garbled-circuits, secret sharing and differential privacy, respectively. The relative latency and throughput are approximated from their evaluation results.</figDesc><table><row><cell>DP-based algorithms do not protect the training and inference</cell></row><row><cell>data against the service provider in the MLaaS setting. Further-</cell></row><row><cell>more, they assume a trusted data aggregator to apply noise and</cell></row><row><cell>perform data aggregation, which is not needed in our work.</cell></row><row><cell>A number of works for distributed training solutions are</cell></row><row><cell>based on MPC algorithms, where the computations on private</cell></row><row><cell>data are distributed across multiple servers ([11]-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc></figDesc><table /><note><p>: SEAL CKKS performance test for basic operations under ?=128-bit security. N : the cyclotomic polynomial degree. L: the 'level' of the ciphertexts. training algorithms without the requirement of a trusted aggregator. Experiments in Section IX-D have shown that Sphinx, without a trusted server, achieves a comparable accuracyprivacy trade-off to the pure DP methods.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III :</head><label>III</label><figDesc>The ciphertext sizes under different cyclotomic polynomial degrees and levels.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>, number of channels 64, pad (2,2), with bias. Outputs: R 64?32?32 3) ReLU: ReLU non-linear function. Outputs: R 64?32?32 4) Average Pooling: window size 1?2?2, stride (2,2). Outputs: ReLU non-linear function. Outputs: R 64?8?8 12) Convolution: window size 1?1 with stride (1,1), number of channels 16, bias is included for each channel. Outputs: R 16?8?8 13) ReLU: ReLU non-linear function. Outputs: R 16?8?8 14) Fully Connected: hidden neuron number 10. Outputs: R 10?1</figDesc><table><row><cell>R 64?16?16</cell></row><row><cell>5) Convolution: window size 5?5 with stride (1,1), number of</cell></row><row><cell>channels 64, pad (2,2), with bias. Outputs: R 64?16?16</cell></row><row><cell>6) ReLU: ReLU non-linear function. Outputs: R 64?16?16</cell></row><row><cell>7) Average Pooling: window size 1?2?2, stride (2,2). Outputs:</cell></row><row><cell>R 64?8?8</cell></row><row><cell>8) Convolution: window size 3?3 with stride (1,1), number of chan-</cell></row><row><cell>nels 64, bias is included for each channel. Outputs: R 16?8?8</cell></row><row><cell>9) ReLU: ReLU non-linear function. Outputs: R 64?8?8</cell></row><row><cell>10) Convolution: window size 1?1 with stride (1,1), number of</cell></row><row><cell>channels 64, pad (1,1), with bias. Outputs: R 64?8?8</cell></row><row><cell>11) ReLU:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IV :</head><label>IV</label><figDesc>The benchmarks of Sphinx variants for training one batch. Na?ve Sphinx only uses plain noisy linear components. FHM means the fast homomorphic multiplication design. FP-Cache denotes the forward propagation cache. ZE/ZD means the zero encryption/decryption techniques.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Time (s)</cell><cell cols="2">Communication (MB)</cell></row><row><cell></cell><cell>Framework</cell><cell cols="3">preproc. online preproc.</cell><cell>online</cell></row><row><cell></cell><cell>Pure-HE</cell><cell>-</cell><cell>2915</cell><cell>-</cell><cell>13134</cell></row><row><cell>MNIST</cell><cell>Sphinx-FP Gazelle</cell><cell>14.4 12.46</cell><cell>108 1.44</cell><cell>-566.2</cell><cell>3014 127.3</cell></row><row><cell></cell><cell>Delphi</cell><cell>6.34</cell><cell>0.09</cell><cell>91.8</cell><cell>0.42</cell></row><row><cell></cell><cell>Sphinx</cell><cell>6.01</cell><cell>0.05</cell><cell>87.6</cell><cell>0.07</cell></row><row><cell></cell><cell>Pure-HE</cell><cell>-</cell><cell>33909</cell><cell>-</cell><cell>53048</cell></row><row><cell>CIFAR-10</cell><cell>Sphinx-FP Gazelle</cell><cell>130 42.65</cell><cell>1065 4.82</cell><cell>-1907</cell><cell>13022 623</cell></row><row><cell></cell><cell>Delphi</cell><cell>48.9</cell><cell>0.18</cell><cell>145</cell><cell>5.45</cell></row><row><cell></cell><cell>Sphinx</cell><cell>48.3</cell><cell>0.08</cell><cell>128</cell><cell>1.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE V :</head><label>V</label><figDesc>The benchmarks of various inference protocols.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: SICHUAN UNIVERSITY. Downloaded on December 24,2022 at 11:52:58 UTC from IEEE Xplore. Restrictions apply.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Except for those used in the rotation-based gradient aggregation over the encrypted bias components in our algorithm, which consume at least one 'level' of ciphertext.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>The encrypted linear layers require at least</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>levels: two levels for the rescaling operations after matrix multiplication in both FP and BP phases, and one for rotation-based aggregation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>https://github.com/microsoft/SEAL</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>https://github.com/OpenMined/TenSEAL</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>https://github.com/attractivechaos/kann</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>https://github.com/mc2-project/delphi</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>https://github.com/tensorflow/privacy</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9"><p>Sphinx-FP supports the batch size up to 2048 with the encryption parameters (L = 1,N = 2<ref type="bibr" target="#b11">12</ref> ).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This work is supported in part by the <rs type="funder">Hong Kong RGC TRS</rs> <rs type="grantNumber">T41-603/20R</rs>, <rs type="grantNumber">GRF-16215119</rs>, <rs type="grantNumber">GRF-16213621</rs> and <rs type="funder">National Key R&amp;D Program of China</rs> under Grant No.<rs type="grantNumber">2018AAA0101100</rs>. We thank our shepherd and the anonymous reviewers for their constructive feedback and suggestions. <rs type="person">Kai Chen</rs> is the corresponding author of this paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eGW4qUN">
					<idno type="grant-number">T41-603/20R</idno>
				</org>
				<org type="funding" xml:id="_ZRqKQch">
					<idno type="grant-number">GRF-16215119</idno>
				</org>
				<org type="funding" xml:id="_Bd7QcS6">
					<idno type="grant-number">GRF-16213621</idno>
				</org>
				<org type="funding" xml:id="_ZjRF5aR">
					<idno type="grant-number">2018AAA0101100</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR09</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Oblivious neural network predictions via minionn transformations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Juuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Asokan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="619" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">{GAZELLE}: A low latency framework for secure neural network inference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Juvekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaikuntanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chandrakasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th {USENIX} Security Symposium</title>
		<title level="s">{USENIX} Security</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1651" to="1669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Slalom: Fast, verifiable and private execution of neural networks in trusted hardware</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03287</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Delphi: A cryptographic inference service for neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lehmkuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Popa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2505" to="2522" />
		</imprint>
	</monogr>
	<note>in 29th {USENIX} Security Symposium ({USENIX} Security 20</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Doing real work with fhe: the case of logistic regression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gentry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Halevi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shoup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Encrypted Computing &amp; Applied Homomorphic Cryptography</title>
		<meeting>the 6th Workshop on Encrypted Computing &amp; Applied Homomorphic Cryptography</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Secure logistic regression based on homomorphic encryption: Design and evaluation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMIR medical informatics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">e19</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Privacy-preserving ridge regression on hundreds of millions of records</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nikolaenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Weinsberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Taft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="334" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Healer: homomorphic computation of exact logistic regression for secure rare disease variants analysis in gwas</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards deep neural network training on encrypted data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nandakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ratha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pankanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Halevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Secureml: A system for scalable privacypreserving machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mohassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="19" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Aby3: A mixed protocol framework for machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mohassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2018 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="35" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Securenn: 3-party secure computation for neural network training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on Privacy Enhancing Technologies</title>
		<meeting>on Privacy Enhancing Technologies</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="26" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Quotient: two-party secure neural network training and prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Shamsabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><surname>Gasc?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1231" to="1247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 2016 ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Differential privacy preservation for deep auto-encoders: an application of human behavior prediction</title>
		<author>
			<persName><forename type="first">N</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive laplace mechanism: Differential privacy preservation in deep learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning with gaussian differential privacy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard data science review</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">23</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Practical and private (deep) learning without sampling or shuffling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v139/kairouz21b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Research</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ser. Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07-24">18-24 Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="5213" to="5225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Evaluating differentially private machine learning in practice</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th {USENIX} Security Symposium ({USENIX} Security 19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1895" to="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">idlg: Improved deep leakage from gradients</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02610</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Privacypreserving machine learning as a service</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hesamifard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghasemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Priv. Enhancing Technol</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="123" to="142" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Flash: Fast and robust framework for privacy-preserving machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Byali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Priv. Enhancing Technol</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="459" to="480" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dowlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wernsing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cryptodl: Deep neural networks over encrypted data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hesamifard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghasemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05189</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ngraphhe2: A high-throughput framework for neural network inference on encrypted data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Boemer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Costache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cammarota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wierzynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM Workshop on Encrypted Computing &amp; Applied Homomorphic Cryptography</title>
		<meeting>the 7th ACM Workshop on Encrypted Computing &amp; Applied Homomorphic Cryptography</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="45" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">{XONN}: Xnor-based oblivious deep neural network inference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Riazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samragh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koushanfar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1501" to="1518" />
		</imprint>
	</monogr>
	<note>in 28th {USENIX} Security Symposium ({USENIX} Security 19</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Blaze: blazing fast privacy-preserving machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Patra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09042</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Applying deep neural networks over homomorphic encrypted medical data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vizitiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Nita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Itu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational and mathematical methods in medicine</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Our data, ourselves: Privacy via distributed noise generation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Naor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual International Conference on the Theory and Applications of Cryptographic Techniques</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="486" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Functional mechanism: regression analysis under differential privacy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winslett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1208.0219</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised knowledge transfer for deep learning from private training data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05755</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep leakage from gradients</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Federated Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02753</idno>
		<title level="m">Privacy loss in apple&apos;s implementation of differential privacy on macos 10.12</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rappor: Randomized aggregatable privacy-preserving ordinal response</title>
		<author>
			<persName><forename type="first">?</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 2014 ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1054" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Realizing private and practical pharmacological collaboration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6412</biblScope>
			<biblScope unit="page" from="347" to="350" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Trident: Efficient 4pc framework for privacy preserving machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rachuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Suresh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Falcon: Honest-majority maliciously secure framework for private deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tople</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Benhamouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rabin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings on Privacy Enhancing Technologies</title>
		<meeting>on Privacy Enhancing Technologies</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aby-a framework for efficient mixed-protocol secure two-party computation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demmler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zohner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NDSS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGSAC conference on computer and communications security</title>
		<meeting>the 22nd ACM SIGSAC conference on computer and communications security</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1310" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Practical secure aggregation for federated learning on user-held data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcedone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04482</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning via additively homomorphic encryption</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moriai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1333" to="1345" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Protection against reconstruction and its applications in private federated learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freudiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rogers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00984</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed learning without distress: Privacy-preserving empirical risk minimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Hybridalpha: An efficient approach for privacy-preserving federated learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ludwig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 12th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A hybrid approach to privacy-preserving federated learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steinke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Workshop on Artificial Intelligence and Security</title>
		<meeting>the 12th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Poseidon: Privacy-preserving federated neural network learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pyrgelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Troncoso-Pastoriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Froelicher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Bossuat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Hubaux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Secure efficient federated knn for recommendation systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1808" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Secure federated matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE Intelligent Systems</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Federated recommendation systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Federated Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Federated singular vector decomposition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08925</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Homomorphic encryption for arithmetic of approximate numbers</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on the Theory and Application of Cryptology and Information Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="409" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient multi-key homomorphic encryption with packed ciphertexts with application to oblivious neural network inference</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2019 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="395" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Secure multiparty aggregation with differential privacy: A comparative study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goryczka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sunderam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint EDBT/ICDT 2013 Workshops</title>
		<meeting>the Joint EDBT/ICDT 2013 Workshops</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Inverting gradients-how easy is it to break privacy in federated learning?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bauermeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dr?ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14053</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Boosting and differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vadhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE 51st Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">R\&apos;enyi differential privacy of the sampled gaussian mechanism</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10530</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Composable and versatile privacy via truncated cdp</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Rothblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing</title>
		<meeting>the 50th Annual ACM SIGACT Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="74" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Optimized homomorphic encryption solution for secure genome-wide association studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Polyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rohloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vaikuntanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Genomics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
