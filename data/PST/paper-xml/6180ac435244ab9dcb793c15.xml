<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimizing Sparse Matrix Multiplications for Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-30">30 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shenghao</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">You</forename><surname>Liang</surname></persName>
							<email>youliang.yl@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
							<email>z.wang5@leeds.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Leeds</orgName>
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimizing Sparse Matrix Multiplications for Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-30">30 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.00352v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) are emerging as a powerful technique for modeling graph structures. Due to the sparsity of realworld graph data, GNN performance is limited by extensive sparse matrix multiplication (SpMM) operations involved in computation. While the right sparse matrix storage format varies across input data, existing deep learning frameworks employ a single, static storage format, leaving much room for improvement. This paper investigates how the choice of sparse matrix storage formats affect the GNN performance. We observe that choosing a suitable sparse matrix storage format can significantly improve the GNN training performance, but the right format depends on the input workloads and can change as the GNN iterates over the input graph. We then develop a predictive model to dynamically choose a sparse matrix storage format to be used by a GNN layer based on the input matrices. Our model is first trained offline using training matrix samples, and the trained model can be applied to any input matrix and GNN kernels with SpMM computation. We implement our approach on top of PyTorch and apply it to 5 representative GNN models running on a multi-core CPU using real-life and synthetic datasets. Experimental results show that our approach gives an average speedup of 1.17x (up to 3x) for GNN running time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, graph neural networks (GNNs) <ref type="bibr" target="#b45">[46]</ref> are shown to be effective in extracting information from graph structures like social networks with millions of nodes and billions of edges <ref type="bibr" target="#b7">[8]</ref>. Indeed, GNNs account for over 90% of the leading models in solving the open graph benchmark suite <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>A GNN is designed to propagate and aggregate information across graph nodes. This is achieved by applying a kernel function to a feature matrix of graph nodes, which captures the properties of nodes, as well as an adjacency matrix that encodes the connectivity of graph edges. The kernel function is typically implemented using matrix multiplications <ref type="bibr" target="#b45">[46]</ref> that often dominate the GNN execution time during training and inference. Because most of the nodes in a real-life graph only have a small number of direct neighbors, the graph adjacency matrix that a GNN kernel operates on is often sparse (i.e., many matrix elements are zeros). As a result, the matrix multiplication computation within a GNN is essentially sparse matrix multiplication (SpMM) operations.</p><p>There is an extensive body of work in optimizing SpMM for scientific workloads <ref type="bibr" target="#b12">[13]</ref>. Various sparse matrix storage formats have been proposed to reduce the memory and computation overhead of SpMM <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>. Studies have also shown that choosing the right storage format can have a significant impact on the SpMM performance <ref type="bibr" target="#b20">[21]</ref>. Although SpMM performance optimization is a well-studied field in traditional high-performance computing (HPC) domains, the benefit of sparse matrix storage format selection is unclear on the new GNN workloads. Existing deep learning frameworks like PyTorch <ref type="bibr" target="#b22">[23]</ref> and Tensorflow <ref type="bibr" target="#b0">[1]</ref> all use a single, static sparse matrix storage format across graph inputs. Since GNNs are becoming an important application class, it is essential to understand how GNN performance can benefit from sparse matrix format selection.</p><p>This paper presents the first study of sparse matrix storage selection on GNN performance. We consider five representative GNN architectures and six commonly used sparse matrix storage formats. We empirically demonstrate that choosing a suitable sparse matrix storage format can have a significant performance benefit, but the right format changes depending on the input matrix. We show that unlike traditional HPC workloads, the matrix sparsity can change over time as the GNN iterates over the input graph; and as a result, the suitable format can vary throughout GNN execution.</p><p>In light of this observation, we employ machine learning to automatically construct a predictive model based on XGBoost <ref type="bibr" target="#b6">[7]</ref> for sparse matrix format selection. Our predictor predicts, at runtime, the sparse matrix storage format and the associate SpMM computation kernel for each GNN kernel. Our predictor is first trained off-line using synthetic matrix data. Then, using a set of automatically tuned features of the matrix input, the predictor determines the optimal storage format to use before entering a kernel. We showcase that our approach is generally applicable and can adapt to various optimization goals to find different trade-offs between the memory overhead and execution time.</p><p>We evaluate our approach by applying it to five GNN architectures running on multi-core CPUs using both real-life and synthetic graph data. We compare our approach against two prior machine-learning methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref> for selecting sparse matrix storage formats. Experimental results show that our approach gives better performance over alternative optimization strategies by giving an average 1.17x speedup. The performance of our approach translates to average 89% of the oracle, a theoretically perfect predictor for storage form selection (Section 6.3). performance given by a theoretically perfect predictor.</p><p>This paper makes the following contributions:</p><p>-It is the first paper to study sparse matrix storage format selection on GNN performance; -It shows how machine learning techniques can be employed to develop a runtime predictor for optimizing GNN sparse matrix format selection;</p><p>-It provides quantified performance results of widely used sparse matrix storage formats on representative GNN architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>A GNN operates on a graph structure, where each graph node is associated with a d-dimensional feature vector of numerical values known as embeddings. Edges between nodes indicate their relationship, quantified with edge weights. For a graph with N nodes, the graph edges are encoded in an N ×N adjacency matrix, A, and the node embeddings are stored in an N × d feature matrix, X.</p><p>Like most neural networks, a GNN model can have multiple layers. Each layer is represented by two functions: i) an aggregation function and ii) an update function (i.e., a combination function). During training, a GNN takes as input the adjacency matrix, A, of the graph. It then uses a neighbourhood aggregation scheme to update the feature vector of each graph node based on the feature vector of its neighboring nodes. Feature aggregation is performed by first applying the aggregation function (e.g., reductions) to collect the features of the neighbours for a given node and then updating each node's feature vectors using the updating function. After repeating this process of updating node features for a fixed number of times, a readout function is applied to aggregate the feature matrix to a single numerical vector to be used as the graph representation.</p><p>The aggregation and update functions used by a GNN layer are implemented using matrix multiplications. Because the graph adjacency matrix, A, is sparse in many real-life graphs, the GNN matrix multiplications are often realized as SpMM to reduce the memory footprint and processing time <ref type="bibr" target="#b16">[17]</ref>. When profiling 5 representative GNN models (Section 5.1) on real-life datasets, we find that SpMM can account for 95% of the GNN processing time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sparse Matrix Storage Formats</head><p>Our work considers the following commonly used sparse matrix storage formats: COO. The coordinate list (COO) stores a list of (row, column, value) tuples of non-zero elements. This is the default storage format used by PyTorch-geometric <ref type="bibr" target="#b10">[11]</ref> for graph processing. CSR. The compressed sparse row (CSR) format uses three arrays to represent non-zero matrix elements, that respectively contain non-zero values, the beginning position of each row, and the column indices of non-zero elements. CSR is similar to COO, but compresses the row indices, hence the name. CSC. The compressed sparse column format (CSC) is similar to CSR, with one exception for using an array to store the target matrix's row indices of non-zero elements instead of column indices as in CSR. DIA. The diagonal format (DIA) stores non-zero elements along the diagonal direction of a matrix into a row of a 2-dimensional array. It is best suited for non-zero elements that appear along the diagonals of a matrix.   BSR. The block sparse row format (BSR) evenly divides the input matrix into blocks. It is CSR with dense sub-matrices of fixed shape instead of scalar items. DOK. The dictionary of keys format (DOK) stores key-value pairs &lt;(row,column), value&gt; in a dictionary (e.g., a hash table). Elements that are not presented in the dictionary are treated as zero elements.</p><formula xml:id="formula_0">C S R B S R C O O C S R B S R C o r a F u l l C o</formula><p>LIL. The linked list (LIL) format stores non-zero elements and their column indices in a linked list. This format uses a row-based linked list, where each row is a list of column indices of non-zero elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivation</head><p>As a motivating example, consider applying a two-layered graph convolution network (GCN) model <ref type="bibr" target="#b17">[18]</ref> to 5 real-life graph datasets (Table <ref type="table" target="#tab_0">1</ref>) using the 7 sparse matrix storage formats described in Section 2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>In this experiment, we consider five real-life graph datasets used in prior work <ref type="bibr" target="#b1">[2]</ref>. Table <ref type="table" target="#tab_0">1</ref> summarizes the size and sparsity of the graph adjacency matrix, and the dimension of the node feature vector (a dense vector). We run the GCN model on a 2.0 GHz 20-core Intel Xeon CPU. We note that it is common to run a GNN on the CPU due to the large memory footprint of graph processing <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the best-performing sparse matrix format for each dataset, when a format is used to encode the initial model input and used throughout the model training process. Here, we normalize the measured runtime against the time of the PyTorch-geometric default COO format. While COO gives the best performance on DBLPFull, it leaves much room for performance improvement on other datasets. Furthermore, we also observe that the best-performing storage format varies depending on the input dataset. If we now consider Figure <ref type="figure" target="#fig_1">2</ref>, we see that the density of the input matrix increases as we iterate over the GNN model on the CoraFull dataset. This is expected as a GNN tries to incorporate further neighbourhood information by iterating over the graph, which in turn increases the reach and information propagation of a graph node. As can be seen in figure <ref type="figure" target="#fig_2">3</ref>, CSR is the best format used to store the neural network input (i.e., the feature and the adjacency matrix) for both the CoraFull and PubmedFull datasets. Thus, for a model with a single layer GNN, CSR might be the best storage format. However, for a typical GNN model with multiple GNN layers, the sparsity of the matrices processed by the latter layers can change, calling for a different storage format to be used. Specifically, for CoraFull (figure <ref type="figure" target="#fig_2">3</ref>(a)) used in our setting, using CSC, LIL and DIA after the first GNN layer can also give a relatively good speedup over COO, but these format give no benefit on PubmedFull (Figure <ref type="figure" target="#fig_2">3</ref>(b)) because of the changing distribution of the non-zero elements, the details can be seen in figure <ref type="figure" target="#fig_2">3</ref>. Lesson learned. This example shows that choosing the right sparse matrix storage format can have a significant performance benefit, but the choice depends on the input data and the GNN layers. Therefore, the decision for storage format should be made on a per GNN layer basis during runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Our Approach</head><p>Our work aims to choose the most efficient sparse matrix storage format for accelerating GNN performance or finding a trade-off between the memory footprint and runtime. As the right choice depends on the characteristics of the input matrix processed by a GNN layer, and the optimal storage format can change over the duration of the training, we wish to develop an approach to automatically derive a storage format (and the SpMM kernel) on a per input basis.</p><p>To this end, we employ machine learning to build a classifier to predict the sparse matrix storage format to use from a pool of candidate formats. The predictive model takes as input a feature vector of numerical values, which describe the essential characteristics of the input matrix. It then produces a label, indicating which of the storage formats to be used by a GNN layer. We provide APIs (Section 4.6) to monitor the input matrix sparsity and dynamically adjust the storage format to use before entering a GNN layer at runtime. If the chosen format is different from the one used by the previous layer or a prior training  epoch, our library will convert the input matrix to the chosen format. Note that we include the overhead of format conversion and feature extraction in all our experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predictive Modeling</head><p>Our predictive model builds upon the XGBoost classifier <ref type="bibr" target="#b6">[7]</ref>. We have evaluated a number of alternative classification techniques, including multilayer perceptron (MLP) neural networks, K-Nearest neighbour (KNN), and support vector machines (SVM). We choose XGBoost because of its good generalization ability <ref type="bibr" target="#b6">[7]</ref>, its decision-tree-like structure is interpretable, and its better and more robust performance over alternatives on our problem (Section 6.4). In the remainder of this section, we describe our predictive model by following the classical 4-step process for supervised learning: i) problem modeling, ii) training data generation, iii) train a predictor and iv) implement the predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Problem Modeling</head><p>Figure <ref type="figure">4</ref> depicts the workflow of our approach. The deployed model extracts features from the adjacency and feature matrices and uses the feature values to predict the sparse matrix storage format to use. Our library automatically converts the input matrix to the selected storage format if needed. Note that a SpMM computation kernel can be chosen based on the object type of the input. Since we implemented our prototype in PyTorch, this computation kernel selection process is performed automatically by the Python library. As depicted in Figure <ref type="figure">5</ref>, our model is trained offline using training samples. The trained model can be applied to any previously unseen matrix. Training involves finding the best storage format, extracting feature values for each training matrix and learning a model from the training data, described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Data Generation</head><p>We use 300 synthetically generated square matrices to train the XGBoost model. The matrix size of our training samples ranges from 1, 000 to 15, 000, increased with a step of 200. We populate the matrix with random values of 0 and 1 with a sparsity ranging from 0.1% to 70%, to simulate the matrix sparsity seen at the initial model graph input and later message propagation stages. For each training matrix, we exhaustively execute the SpMM computation kernel with each sparse matrix storage format and record the best performing format for each matrix sample on each kernel. We then label each best-performing configuration with Fig. <ref type="figure">6</ref>. How often a storage format is considered to be optimal on our synthetic training data when varying the weight w in Eq 1. Noted that there might be multiple optimal formats for a single input if the final output O is very similar (±0.0001).</p><p>a unique number (i.e., class label). Note that we apply cross-validation in our evaluation to make sure we always test the trained model on unseen datasets. Optimization goal. Our approach allows the user to find a trade-off between the memory footprint and the GNN performance and train a predictive model for their optimization goal. Specifically, in this work, we consider the following optimization formulation, but other formulas can also be used:</p><formula xml:id="formula_1">min O O l∈L = w × R + (1.0 − w) × M<label>(1)</label></formula><p>where R and M are the normalized running time and memory footprint for a sparse matrix storage format from a collection of candidate formats (L), and w is a configurable weight parameter. Note that we scale the execution time and memory footprint to the (0, 1) range using the min-max values found from the profiled training data. Essentially, our goal is to minimize the weighted sum, O in Eq 1 to trade runtime for a lower memory footprint. For example, setting w to 0 and 1.0 means we only optimize for memory overhead and speeds respectively.</p><p>Our training data includes the raw measurements of the execution time and memory footprint for each storage format under each matrix. We then apply the Eq 1 to label the storage format that gives the smallest O for each training sample. Figure <ref type="figure">6</ref> lists the frequency of a storage format to be found to be optimal on our training dataset. Here, the x-axis shows different settings of w in Eq 1. As can be seen from the diagram, the optimal storage format can vary depending on the optimization criterion. Our approach can adapt to such changes by automatically learning from the training samples (see Section 4.5).</p><p>For each training data sample, we also extract the values of a selected set of features (described in Section 4.4). We note that training is a one-off cost, and the trained predictive model can be used by any GNN model to optimize the SpMM computation kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Feature Engineering</head><p>Feature selection. A key aspect in building a good machine learning predictor is finding the right representation, or features, to capture the essential characteristics of the input workload. We start by considering over 30 raw features chosen based on previous work of SPMV optimization <ref type="bibr" target="#b26">[27]</ref>. Most of the features  are used to capture the distribution of non-zero elements of the input matrix, which can be extracted in parallel to reduce the overhead of feature extraction.</p><formula xml:id="formula_2">c v E R _ D I A E R _ R D E R _ C D d e v _ C</formula><p>To learn effectively over a small training dataset, we use the feature score given as a by-product of the XGBoost training process to select a compact set of features. The feature score is computed summing up how many times each feature is split on the decision tree. We then keep features that contribute to 95% of the aggregated importance scores across all raw features. Using a fewer number of features also help us to reduce the overhead of runtime feature extraction. Table <ref type="table" target="#tab_2">2</ref> summarizes our chosen matrix features.</p><p>Feature normalization. In the final step, we scale each of the extracted feature values to a common range (between 0 and 1) to prevent the range of any single feature from being a factor in its importance. We record the minimum and maximum values of each feature in the training dataset in order to scale the feature values of an unseen matrix. We also clip a feature value to make sure it is within the expected range during deployment. Feature importance. Figure <ref type="figure" target="#fig_5">7</ref> shows the top 8 dominant features based on their impact on our predictive model accuracy. We calculate feature importance by first training a model using all 19 of our chosen features, and record the accuracy of our model. In turn, we then remove each of our features, retraining and evaluating our model on the other 18, noting the drop in prediction accuracy. We then normalize the values to produce a percentage of importance for each of our features. Features for measuring the non-zero element distribution, like ER DIA and cv in Table <ref type="table" target="#tab_2">2</ref>, are important for choosing the storage format. The similar distribution of feature importance is an indication that each of our features is able to represent distinct information about the matrix workload, all of which is important for the prediction task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training The Model</head><p>The collected feature values, together with the desired label for each training matrix, are passed to a supervised learning algorithm to learn the XGBoost model. The time for training the predictor is dominated by generating the training data. In this work, it takes less than a week to label all the training samples using a single multi-core server. In comparison, processing the raw data and building the models took a negligible amount of time, less than an hour run in a RTX 2060 GPU. Since training is only performed once, it is a one-off cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Using The Model</head><p>The trained predictor can be applied to a new, unseen matrix used by a SpMM kernel. We implement our predictive model using the Python Scikit-learn <ref type="bibr" target="#b3">[4]</ref> package, which can be easily integrated with mainstream deep learning frameworks. We have encapsulated all of the inner workings, such as feature extraction, prediction and storage format conversion and kernel selection, into a single package. Prediction is done by calling a dedicated SpMMPredict function (provided by our library) before each GNN layer. The function takes as input a matrix object and outputs a matrix object stored using the predicted storage format. Depending on the matrix object type, the corresponding SpMM kernel will be automatically chosen. Our current implementation supports PyTorch, but it can be easily ported to other deep learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>5.1 Software and Hardware Evaluation platform. Our hardware platform is a dual-socket multi-core server with two 20-core Intel Sky Lake Xeon Gold 6138 CPUs running at 2.0 Ghz with 192GB of RAM. Our evaluation platform runs Centos 7 with Linux kernel version 3.10. We test our approach on PyTorch v1.4.0, running on the CPU. GNN models. We apply our approach to 5 representative GNN architectures, including GCN, graph attention network (GAT) <ref type="bibr" target="#b29">[30]</ref>, relational graph convolutional neural network (RGCN) <ref type="bibr" target="#b25">[26]</ref>, GNN with feature-wise linear modulation (FiLM) <ref type="bibr" target="#b2">[3]</ref> and efficient graph convolutions (EGN) <ref type="bibr" target="#b27">[28]</ref>. We use the opensource implementation provided by PyTorch-geometric library <ref type="bibr" target="#b10">[11]</ref> by stacking two GNN layers to form a standard graph model. Datasets. In our evaluation, we use two graph data suites, CoraFull <ref type="bibr" target="#b39">[40]</ref> and Entities <ref type="bibr" target="#b25">[26]</ref>, containing a total of 5 graph datasets with matrix sizes ranging from 19,793 to 58,086. To evaluate the generalization ability of our approach, we also apply our approach to 100 synthetic matrices of different sizes and sparsity. For the synthetic data, we initialize weights in the adjacency matrices by populating them with random single floating numbers between 0 and 1.0. . Speedup given by our approach over COO. GeoMean represents the geometric mean given by the previous performance.</p><formula xml:id="formula_3">R G C N E G N G A T F I L M G C N G e o M e</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation Methodology</head><p>Competitive methods. We compare our approach against two closely related predictive methods for using machine learning to choose the sparse matrix storage format. The first approach employs a convolutional neural network (CNN) <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b23">24]</ref>, and the second uses a decision tree model for format selection <ref type="bibr" target="#b26">[27]</ref>.</p><p>We use an open-source implementation of ResNet <ref type="bibr" target="#b22">[23]</ref> as the CNN model. To provide a fair comparison, we train all machine learning models on the same training dataset using the methodology described in the source publications.</p><p>Performance report. We consider the end-to-end execution time, including the overhead of our predictive model (i.e., the time spending on feature extraction, storage format transformation and model prediction). Our feature extraction process runs in parallel using all CPU cores. We measure the end-to-end training time by training each model on each dataset for 10 epochs. We run each matrix input 5 times and report the geometric mean of the end-to-end training time and show the variations across different runs as a min-max bar. Note that we only need to decide the matrix storage format once for each GNN layer across training epochs. Given that in our evaluation, the sparse matrix distribution is similar across training epochs, and hence the overhead of our approach can be further amortised across multiple training epochs.</p><p>6 Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overall Results</head><p>Figure <ref type="figure" target="#fig_6">8</ref>(a) shows the speedup over the PyTorch COO sparse matrix storage format for each GNN model across our evaluation datasets. Here, the min-max bar show the variance across the evaluated datasets. In this experiment, we aim to optimize for speedups by setting w of Eq. 1. Moreover, in Section 6.4 we show our approach can generalize to other settings of w.</p><p>As can be seen from the diagrams, choosing the right sparse matrix storage format can improve the GNN performance. Our approach delivers an average speedup of 1.3x (up to 3x) on GCN, which involves many SpMM computations when performing the graph convolution operations. Our approach gives less performance improvement on RGCN because the dataset that RGCN operates is a dense edge-based dataset that does not benefit from sparse matrix format  selection. Furthermore, on a small number of datasets, where the COO is the best format, our approach shows a minor slowdown, less than 7%, due to the overhead of feature extraction. But for the majority of the evaluated datasets, our approach gives a noticeable improvement over COO. Overall, our techniques give an average speedup of 1.17x across GNN models and evaluation datasets. Figure <ref type="figure" target="#fig_6">8</ref>(b) shows the achieved performance per real-world graph dataset across models. For most of the datasets, our approach gives noticeable speedups across GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Compare to Prior Methods</head><p>Table <ref type="table" target="#tab_3">3</ref> compares our approach against a CNN and a decision tree model for choosing the matrix storage format, where our approach gives a better overall prediction accuracy. The CNN model gives a poor prediction accuracy when the model is trained on 300 synthetic matrices. While the performance of the CNN model can be improved by using more training data, doing so would incur a higher overhead. Table <ref type="table" target="#tab_3">3</ref> confirms that a higher prediction accuracy does translate into better speedup performance, where our approach improves the CNN and the decision tree model by 27% and 3%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Compare to Oracle Performance</head><p>Figure <ref type="figure">9</ref> compares our approach against a theoretically perfect predictor for storage form selection, for which we call oracle. We obtain the oracle performance by exhaustively profiling all candidate storage formats for each GNN layer to find out the best-performing format. The results show how close our predictive modeling approach is to the theoretical upper bound. Our approach achieves, on average, 89% of the oracle performance. Our model can be further improved by using more training samples together with more representative features to characterise some of the input matrices better to improve the prediction accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Model Analysis</head><p>Impact of optimization goal. Our evaluation so far set w to 1 of our optimization function (Eq. 1) by solely optimizing for speeds. Figure <ref type="figure" target="#fig_0">10</ref> shows prediction accuracy when we vary the parameter settings. Our approach has a good generalization by giving the average accuracy of 90%. This experiment shows that our approach is flexible and can adapt to different optimization trade-offs.</p><p>Alternative modeling techniques. Figure <ref type="figure" target="#fig_8">11</ref> compares our XGBoost-based predictor against three other classification methods used in prior works for code optimization <ref type="bibr" target="#b33">[34]</ref>: MLP neural network <ref type="bibr" target="#b11">[12]</ref>, KNN (with k = 1) <ref type="bibr" target="#b41">[42]</ref>, and SVM <ref type="bibr" target="#b21">[22]</ref>. All the alternative techniques were trained and evaluated using the same method and training data as our model. In this experiment, we consider the model prediction accuracy and the time for making a prediction. As can be seen from the diagram, our approach has the lowest runtime overhead while giving the highest accuracy when compared to alternative modeling techniques. Since XGBoost is a decision-tree-based model, it also has the advantage of being interpretable because its decision process can be followed by traversing the tree.</p><p>Training and deployment overhead. Training of our predictive model only needs to be performed once, after which the trained model can be applied to any matrices. Training is dominated by the generation of training data which takes in total less than a week's machine time (Section 4.3). We can speed this up by using multiple machines. The overhead for learning the XGBoost model is negligible, less than 5 minutes. Our approach has a negligible runtime overhead compared to the GNN kernel execution time, the overhead of feature extraction and prediction is less than 3% to the end-to-end kernel execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Discussion</head><p>Supporting other storage formats. Our approach can be easily extended to support other sparse matrix storage formats. As we formulate the storage format prediction as a classification problem, this can be achieved by adding a new class label (for the newly supported format) into our training dataset. Doing so would also require providing the relevant SpMM kernel implementation. Other than these, a large part of the training process and deployment can remain unchanged.</p><p>Supporting GPU computation. This work focuses on the CPU execution of GNN models due to the large graph datasets that a GNN model typically processes. There are methods to support large-scale graph processing on GPUs such as GraphSAGE <ref type="bibr" target="#b14">[15]</ref>. Our approach can be ported to support GPU processing. This will require using training data collected from the targeting GPU to train our predictive model. Optimize SpMM algorithms. Optimizing SpMM computation is an active research field <ref type="bibr" target="#b9">[10]</ref>. It is interesting to investigate how the SpMM computation kernel can be tailored for GNN computation and what parameters can be opened to a tuning framework. As the best algorithm parameters are likely to change depending on the matrix input and the underlying hardware, an automatic machine learning-based approach similar to our approach is highly attractive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>Several approaches have been proposed to optimize graph processing <ref type="bibr" target="#b38">[39]</ref>. Some provide new programming abstractions to optimize vertex/node-centric or edgecentric processing <ref type="bibr" target="#b45">[46]</ref>. For example, Pytorch-Geometric (PyG) <ref type="bibr" target="#b10">[11]</ref> and Deep Graph Library (DGL) <ref type="bibr" target="#b32">[33]</ref> are two major frameworks for GNN computation. Both libraries rely on a low-level, hand-optimized SpMM library, but they use a single sparse matrix storage format throughout the execution. Our work complements these prior efforts by dynamically adapting the sparse matrix storage format and the associated computation kernel for each GNN layer, which can be easily integrated with existing graph programming models.</p><p>Various sparse matrix storage formats have been proposed in the past <ref type="bibr" target="#b18">[19]</ref>. Studies have shown that there is no "one-fit-for-all" storage format, and the right format can change from one matrix to the other <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>. Methods have been proposed to dynamically choose sparse matrix storage format based on the input workloads <ref type="bibr" target="#b26">[27]</ref>. These include approaches build around analytical methods <ref type="bibr" target="#b30">[31]</ref> or machine-learning-based predictive models <ref type="bibr" target="#b4">[5]</ref>. The latter has the benefit of can be easily ported to different architectures as machine learning learns from empirical observations rather than simplified assumptions used by an analytical model. However, prior machine-learning-based solutions have been concentrated on optimizing sparse matrix-vector multiplication (SpMV) of scientific workloads <ref type="bibr" target="#b44">[45]</ref>. They choose a storage format at the beginning of the program execution but do not adjust the format during application execution. No work so far has concerned choosing the sparse matrix storage format for GNN SpMM throughout program execution. Our work is the first to do so.</p><p>Machine learning is a proven design methodology for systems modeling and optimization <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b43">44]</ref>. Studies have demonstrated the success of applying machine learning for a wide range of code optimization tasks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b31">32]</ref> In this work, we employ machine learning techniques to develop an automatic approach to optimize GNN SpMM. We remark that our work does not seek to advance machine learning algorithms; instead, it explores and applies a wellestablished modeling method to tackle the GNN SpMM optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>This paper has presented a machine-learning based predictive model to dynamically choose the sparse matrix storage format and the associate computation kernel during GNN execution. Our model uses numerical features to characterize the input matrix to predict the storage format to use for the next GNN layer. We evaluate our approach by applying it to five representative GNN models running on a multi-core CPU using both real-world and synthetic datasets. Experimental results show that our approach gives an average speedup of 1.17x (up to 3x) over the Pytorch default strategy and exhibits a good generalization ability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The best-performing storage format per dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Changes of the adjacency matrix density over GNN training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance improvement over the PyTorch-geometric default COO format on the CoraFull (a) and PubmedFull dataset (b) when using different sparse matrix format to store the output of the first GNN layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Overview of our predictive model for choosing sparse matrix storage format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>D r o w _ b o u n c e m aFig. 7 .</head><label>7</label><figDesc>Top-8 features which can lead to a high loss in accuracy if they are not used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>GFig. 8</head><label>8</label><figDesc>Fig.8. Speedup given by our approach over COO. GeoMean represents the geometric mean given by the previous performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig.9. Performance of our approach related to the Oracle performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>11 .</head><label>11</label><figDesc>e r e n c e T i m e ( m s ) C l a s s i f i e r (b) Inference Time Fig. Comparing our XGBoost model against alternative modeling techniques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Input matrix sparsity from graph datasets</figDesc><table><row><cell>Name</cell><cell cols="2">Adj. Matrix Density Adj. Matrix Size</cell><cell>Node Feature Vector</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dimension</cell></row><row><cell>CoraFull</cell><cell>0.6%</cell><cell>19, 793 × 8, 710</cell><cell>19,793</cell></row><row><cell>Cora</cell><cell>1.27%</cell><cell>2, 708 × 1, 433</cell><cell>2,708</cell></row><row><cell>DblpFull</cell><cell>0.31%</cell><cell>17, 716 × 1, 639</cell><cell>17,716</cell></row><row><cell>PubmedFull</cell><cell>10.02%</cell><cell>19, 717 × 500</cell><cell>19,717</cell></row><row><cell>KarateClub</cell><cell>2.94%</cell><cell>34 × 34</cell><cell>34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Matrix feature used by in our predictive model</figDesc><table><row><cell>No.</cell><cell>Featur.</cell><cell>Description</cell><cell>No.</cell><cell>Featur.</cell><cell>Description</cell></row><row><cell>F1</cell><cell>numRow</cell><cell># rows</cell><cell>F2</cell><cell>numCol</cell><cell># columns</cell></row><row><cell>F3</cell><cell>NNZ</cell><cell># Non-zeros</cell><cell>F4</cell><cell>N diags</cell><cell># diagonals</cell></row><row><cell>F5</cell><cell>aver RD</cell><cell>Avg. # non-zero ele-</cell><cell>F6</cell><cell>max RD</cell><cell>Max. # non-zeros per</cell></row><row><cell></cell><cell></cell><cell>ments per row</cell><cell></cell><cell></cell><cell>row</cell></row><row><cell>F7</cell><cell>min RD</cell><cell>Min. # non-zeros per</cell><cell>F8</cell><cell>dev RD</cell><cell>Standard deviation of</cell></row><row><cell></cell><cell></cell><cell>row</cell><cell></cell><cell></cell><cell>non-zero numbers per</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>row</cell></row><row><cell>F9</cell><cell>aver CD</cell><cell>Avg. # non-zeros per</cell><cell>F10</cell><cell>max CD</cell><cell>Max. # non-zero values</cell></row><row><cell></cell><cell></cell><cell>column</cell><cell></cell><cell></cell><cell>per column</cell></row><row><cell>F11</cell><cell>min CD</cell><cell>Min. # non-zero values</cell><cell>F12</cell><cell>dev CD</cell><cell>The deviation number of</cell></row><row><cell></cell><cell></cell><cell>per column</cell><cell></cell><cell></cell><cell>non-zeros per column</cell></row><row><cell>F13</cell><cell>ER DIA</cell><cell>Ratio of non-zeros in di-</cell><cell>F14</cell><cell>ER CD</cell><cell>Ratio of non-zeros in</cell></row><row><cell></cell><cell></cell><cell>agonals</cell><cell></cell><cell></cell><cell>column-packed structure</cell></row><row><cell>F15</cell><cell cols="2">row bounce Avg. differences between</cell><cell>F16</cell><cell cols="2">col bounce Avg. difference between</cell></row><row><cell></cell><cell></cell><cell>non-zeros of adjacent</cell><cell></cell><cell></cell><cell>non-zeros of adjacent</cell></row><row><cell></cell><cell></cell><cell>rows</cell><cell></cell><cell></cell><cell>columns</cell></row><row><cell>F17</cell><cell>density</cell><cell>Density of non-zeros</cell><cell>F18</cell><cell>cv</cell><cell>Normalized variation of</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>non-zeros per row</cell></row><row><cell>F19</cell><cell>max mu</cell><cell>max. RD -avg. RD</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparing our XGBoost approach with prior work</figDesc><table><row><cell>Model</cell><cell cols="3">Inference Time (s) Prediction Accuracy (%) Realized Speedup</cell></row><row><cell>XGboost (ours)</cell><cell>0.0008</cell><cell>89.1</cell><cell>1.17</cell></row><row><cell>CNN [45,24]</cell><cell>0.002</cell><cell>66.8</cell><cell>0.86</cell></row><row><cell>Decision-Tree [27]</cell><cell>0.0002</cell><cell>83.8</cell><cell>1.14</cell></row><row><cell cols="2">7 5 % 8 6 % 8 9 % 8 9 % 9 4 % 9 7 % 9 8 % 8 9 %</cell><cell></cell><cell></cell></row><row><cell>S y n t h e t i c D a t a s e t s</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This project was supported in part by an Alibaba Innovative Research Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>OSDI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<title level="m">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gnn-film: Graph neural networks with feature-wise linear modulation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
				<imprint>
			<date type="published" when="2020-07-18">18 July 2020. 2020</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">API design for machine learning software: experiences from the scikit-learn project</title>
		<author>
			<persName><forename type="first">L</forename><surname>Buitinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD Workshop</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimizing sparse matrix-vector multiplications on an armv8-based many-core architecture</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Characterizing scalability of sparse matrix-vector multiplications on phytium ft-2000+</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Xgboost: extreme gradient boosting</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>R package</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on network embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">End-to-end deep learning of optimization heuristics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">PACT</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing sparse matrix-matrix multiplication for the gpu</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dalton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOMS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artificial neural networks (the multilayer perceptron)-a review of applications in the atmospheric sciences</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dorling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmospheric environment</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified framework for numerical and combinatorial computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science &amp; Engineering</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Efficient sparse matrix-vector multiplication on gpus using the csr storage format</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Greathouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>SC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding and bridging the gaps in current gnn performance optimizations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPoPP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Evaluation criteria for sparse matrix storage formats</title>
		<author>
			<persName><forename type="first">D</forename><surname>Langr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tvrdik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Smat: an input adaptive auto-tuner for sparse matrix-vector multiplication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>PLDI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning sparse matrix row permutations for efficient spmm on gpu architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPASS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">What is a support vector machine?</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse matrix classification on imbalanced datasets using convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Pichel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pateiro-López</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Optimise web browsing on heterogeneous mobile platforms: a machine learning based approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INFOCOM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic selection of sparse matrix representation on gpus</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sedaghati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adaptive filters and aggregator fusion for efficient graph convolutions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Opolka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Towards a holistic approach to auto-parallelization: integrating profile-driven parallelism detection and machine-learning based mapping</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tournavitis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>PLDI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<title level="m">Graph attention networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Loop and data transformations for sparse matrix code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venkat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining graph-based learning with automated data collection for code vulnerability detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep graph library: Towards efficient and scalable deep learning on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Machine learning in compiler optimization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mapping parallelism to multi-cores: a machine learning based approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>O'boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPoPP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Partitioning streaming parallelism for multi-cores: a machine learning based approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>O'boyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">PACT</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic and portable mapping of data parallel programs to opencl for gpu-based heterogeneous systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TACO</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Integrating profile-driven parallelism detection and machinelearning-based mapping</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM TACO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">When do gnns work: Understanding and improving neighborhood aggregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cross-lingual knowledge graph alignment via graph matching neural network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep program structure modeling through multi-relational graphbased learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">PACT</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ml-knn: A lazy learning approach to multi-label learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Auto-tuning streamed applications on intel xeon phi</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPDPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimizing streaming parallelism on heterogeneous many-core architectures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Bridging the gap between deep learning and sparse matrix format selection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PPoPP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<title level="m">Graph neural networks: A review of methods and applications. AI Open</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
