<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Multi-hop Question Answering as Single Sequence Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-18">18 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
							<email>syavuz@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
							<email>k.hashimoto@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
							<email>yingbo.zhou@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Nitish</forename><forename type="middle">Shirish</forename><surname>Keskar</surname></persName>
							<email>nkeskar@salesforce.com</email>
						</author>
						<author>
							<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
							<email>cxiong@salesforce.com</email>
						</author>
						<title level="a" type="main">Modeling Multi-hop Question Answering as Single Sequence Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-18">18 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.09226v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fusion-in-decoder (FID) (Izacard and Grave, 2021) is a generative question answering (QA) model that leverages passage retrieval with a pre-trained transformer and pushed the state of the art on single-hop QA. However, the complexity of multi-hop QA hinders the effectiveness of the generative QA approach. In this work, we propose a simple generative approach (PATHFID) that extends the task beyond just answer generation by explicitly modeling the reasoning process to resolve the answer for multi-hop questions. By linearizing the hierarchical reasoning path of supporting passages, their key sentences, and finally the factoid answer, we cast the problem as a single sequence prediction task. To facilitate complex reasoning with multiple clues, we further extend the unified flat representation of multiple input documents by encoding cross-passage interactions. Our extensive experiments demonstrate that PATHFID leads to strong performance gains on two multi-hop QA datasets: HotpotQA and IIRC. Besides the performance gains, PATHFID is more interpretable, which in turn yields answers that are more faithfully grounded to the supporting passages and facts compared to the baseline FID model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Leveraging knowledge to make complex reasoning has been a fundamental problem of artificial intelligence. Open-domain question answering (QA) <ref type="bibr" target="#b37">(Voorhees, 1999)</ref> is an integral part of such a line of research with impactful applications <ref type="bibr" target="#b7">(Esteva et al., 2020;</ref><ref type="bibr" target="#b43">Zhang et al., 2020)</ref>, where the task is to answer general domain questions by gathering evidence from a large collection of documents. While super-human level performance has been achieved on single-passage reading comprehension dataset like SQuAD <ref type="bibr" target="#b32">(Rajpurkar et al., 2016)</ref>, open-domain QA still has a long way to go, especially for questions requiring more complex reasoning. The main challenge in the task of complex QA, namely multihop QA, is that it requires a QA system to combine multiple pieces of evidence from multiple documents <ref type="bibr" target="#b38">(Welbl et al., 2018;</ref><ref type="bibr" target="#b35">Talmor and Berant, 2018;</ref><ref type="bibr" target="#b42">Yang et al., 2018)</ref>. Even for single-hop QA, it has been shown challenging for extractive QA models to effectively aggregate evidence from the combined pool of multiple passages, which has been the focus of recent work <ref type="bibr" target="#b3">(Clark and Gardner, 2018;</ref><ref type="bibr" target="#b22">Min et al., 2019;</ref><ref type="bibr" target="#b10">Guu et al., 2020)</ref>.</p><p>Recent work <ref type="bibr">(Lewis et al., 2020b;</ref><ref type="bibr" target="#b23">Min et al., 2020)</ref> has demonstrated the promise of a generative approach at combining evidences from multiple passages for answer generation. Thanks to large pre-trained transformers like T5 <ref type="bibr" target="#b31">(Raffel et al., 2020)</ref>, <ref type="bibr" target="#b14">Izacard and Grave (2021)</ref> introduced fusion-in-decoder (FID) that leverages passage retrieval with generative models for open-domain QA, achieving state-of-the-art scores across several single-hop QA benchmarks. However, we observe that the success of the FID model does not extend to multi-hop QA, which is corroborated by the findings in <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref>. Further, the FID model is a rather opaque model in terms of interpretation of the answer generation process. This capability becomes especially important for multi-hop QA, which requires sequential reasoning across multiple evidences from the pool of retrieved passages.</p><p>In this work, we propose PATHFID, a generative QA model that learns to generate an answer along with a reasoning path to improve its capability of multi-hop reasoning. PATHFID extends multi-hop QA beyond just answer generation by explicitly modeling the full reasoning path to resolve the answer with a generative sequence-tosequence model. To this end, we cast the problem as a single sequence prediction task that simultaneously models reasoning path consisting of supporting passages and facts, and eventually the factoid answer. Furthermore, we extend PATHFID to allow for cross-passage interactions between the Figure <ref type="figure">1</ref>: An example of multi-hop question from HotpotQA dataset. It requires fusing multiple evidences (supporting facts) from multiple passages in a certain order to arrive at the correct answer. We formulate the entire problem as a single sequence prediction of the linearized hierarchical path ending with the answer.</p><p>retrieved passages to obtain more expressive representations from the encoder to facilitate modeling a complex reasoning chain by the decoder. Figure <ref type="figure">1</ref> shows an example of our task formulation, and Figure <ref type="figure" target="#fig_1">2</ref> shows an overview of our approach. We evaluate our proposed approach on two multihop QA datasets: HotpotQA <ref type="bibr" target="#b42">(Yang et al., 2018)</ref> and IIRC <ref type="bibr" target="#b9">(Ferguson et al., 2020)</ref>. Our extensive experiments demonstrate that (i) PATHFID leads to significant performance gains over FID on answer generation, (ii) PATHFID is the first generative model unlocking the possibility of generating the reasoning path jointly with the answer while achieving competitive performance on supporting fact extraction metric as well. Besides the performance gains, PATHFID is able to expose the underlying reasoning process behind the answer generation, which allows us to conduct a much finer-grained qualitative and quantitative analysis on the model's behavior, providing insights into further improving and better understanding generative models for multi-hop QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Setup and Background</head><p>In this section, we formally introduce the problem setup and establish the necessary background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-hop Question Answering</head><p>We first describe the multi-hop QA task in a general way. We assume that a collection of K passages are given for a question q: D q = {p 1 , p 2 , . . . , p K }, where D q can be a pre-defined set, or it can also be an output from a text retrieval system (e.g., DPR <ref type="bibr" target="#b15">(Karpukhin et al., 2020)</ref> and MDR <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref>) in an open-domain QA setting. That is, in the case of the open-domain setting, D q is a subset of a large collection of passages, such as Wikipedia. The task is to generate an answer string a given q and D q . In addition, we aim at identifying which passages provide evidence, and which sentences in them are describing the evidence. Figure <ref type="figure">1</ref> shows a comprehensive example of the task definition, where we can see that some sentences (called supporting facts) in the two paragraphs are crucial to answer the question. Moreover, there is a reasoning flow: the question ? the first paragraph ? the second paragraph, which is called a reasoning path in previous work <ref type="bibr" target="#b0">(Asai et al., 2020)</ref>. The overall task is then to predict the reasoning path along with the supporting facts, and the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Fusion-in-Decoder Model (FID)</head><p>Fusion-in-Decoder (FID) is a generative reader based on a sequence-to-sequence architecture, initialized from pre-trained models such as T5 <ref type="bibr" target="#b31">(Raffel et al., 2020)</ref> or BART <ref type="bibr">(Lewis et al., 2020a)</ref>. It consists of an encoder (Enc) and a decoder (Dec). First, it constructs a single block of text b n := question: q title: t n context: p n of concatenated evidence from each passage-title pair (p n , t n ) together with the question (q). Then, each of the resulting evidence block Note that, the motivation behind this strategy is to avoid the expensive quadratic self-attention computation on the encoder-side, effectively reducing the complexity from O((</p><formula xml:id="formula_0">|b n |) 2 ) to O( |b n | 2 ).</formula><p>Then, the overall answer generation is modeled as a conditional generation p ? (a|X) given X consuming the unified input representation X, where ? represents the set of all model parameters. The model is trained to minimize the cross-entropy loss for generating answer tokens on the decoder side. At inference time, FID first computes X based on the retrieved passages, and then decodes the answer token by token following p ? (a i |a &lt;i , X) with the learned model parameters ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PATHFID Reader for Multi-hop QA</head><p>In this section, we introduce a generative reader (PATHFID) for K-hop QA that jointly generates an alternating sequence of passage-level and factlevel clues on the reasoning path by more explicit fusion of evidence from the pool of input passages to arrive at the correct answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of PATHFID</head><p>As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, PATHFID employs a single sequence-to-sequence architecture that independently encodes the input passages after inserting special fact markers (&lt;f i &gt;) before the i-th sentence of each passage. Conditioning on the concatenation of token-level input representations per passage, its decoder then generates the linearized hierarchical reasoning path obtained by concatenating the sequence of passage titles and their corresponding supporting fact pointers followed by the answer. Each segment on the reasoning path is separated by special markers in a way that makes it possible to uniquely recover the individual segment predictions after decoding in the inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extending Multi-hop QA beyond Answer Generation</head><p>The opaqueness of the FID model, which makes understanding of the reasoning process more difficult, motivated our approach and its emphasis on exposing the reasoning path. Instead of only modeling answer generation, we propose to jointly model it with the full reasoning path in an hierarchical fashion to derive the answer in a unified way using multi-task maximum likelihood training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Global Input Representation</head><p>We utilize the core input encoding architecture from FID approach (Section 2.2) by introducing a new passage representation that will facilitate supporting fact generation on the reasoning path as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. To this end, we independently encode each input passage-title pair (p n , t n ) along with the question q as a separate block b</p><p>path n := question: q title: t n context: p path n where we redefine the context representation by inserting special tokens (&lt;f i &gt;) before each sentence of the passage as</p><formula xml:id="formula_1">p path n := &lt;f 1 &gt; s (1) n &lt;f 2 &gt; s (2) n ? ? ? &lt;f ln &gt; s (ln) n (2)</formula><p>where s</p><formula xml:id="formula_2">(i)</formula><p>n denotes the i-th sentence of passage p n , and l n is the number sentences it contains. Having redefined the input blocks (b path n ) per passage, we then compute the global input representation similar to Eq. 1 by</p><formula xml:id="formula_3">X path q = [Enc(b path 1 ); Enc(b path 2 ); . . . ; Enc(b path N )]<label>(3)</label></formula><p>Note that sentence indicators (&lt;f i &gt;) are shared across all passages, encouraging a more hierarchical passage representation by explicitly breaking them down into sentence-level sub-blocks using the same indicator tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Hierarchical Reasoning Path as a Sequence</head><p>The hierarchical design of reasoning path is inspired by the human reasoning process for multihop QA task. More precisely, if a question q requires K-hop reasoning, then we process these K passages in a sequential order alternating between their passage-level and sentence-level evidence until we reach the answer. To this end, let R q = {p r 1 , p r 2 , . . . , p r K } with r i ? [1, N ] denote the sequence of passages from the larger pool D q reflecting this reasoning process for locating the answer a for question q. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we define the hierarchical reasoning path as a linearized sequence of blocks of passage titles and supporting facts followed by the answer block</p><formula xml:id="formula_4">Y path q := [T r 1 ; E r 1 ; T r 2 ; E r 2 ; ? ? ? ; T K ; E r K ; A] (4)</formula><p>where T r i represents the i-th title block obtained by inserting a special token (&lt;title-i&gt;) before the title t r j and A denotes the answer block derived by prepending a special token (&lt;answer&gt;) to the answer a as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. On the other hand, i-th supporting fact block is defined as the sequence of fact indicators following &lt;facts-i&gt; token by where {j 1 , j 2 , . . . , j m i } denote the indices of key sentences to leverage from passage p r i to transition to the next evidence on the reasoning process R q for question q, and 1 ? m i ? l r i denotes the number of supporting facts. Note that fact indicators &lt;f i &gt; are shared between the contexts p path n of input blocks (Eq. 2) and supporting fact blocks (Eq. 5) on the target reasoning path to allow the decoder to follow along the sequential reasoning R q by pointing to the facts E r i of passage p r i .</p><formula xml:id="formula_5">E r i := &lt;facts-i&gt; &lt;f j 1 &gt; &lt;f j 2 &gt; ? ? ? &lt;f jm i &gt; (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Encoding Cross-Passage Interactions (PATHFID+)</head><p>PATHFID enables more explicit evidence fusion through the reasoning path to guide the model to towards correct answer in a structured way. However, it still relies on the decoder to combine all the clues together, which might still struggle due to lack of cross-passage interactions as input blocks are encoded independently. To address this potential limitation, we propose PATHFID+, where we further extend PATHFID in a way that enables crosspassage interaction by redefining the input block consisting of a pair of passages (p n 1 , p n 2 ) as</p><formula xml:id="formula_6">b path+ n 1 ,n 2 := question: q &lt;title-1&gt; t n 1 &lt;context-1&gt; p path n 1 &lt;title-2&gt; t n 2 &lt;context-2&gt; p path n 2</formula><p>assuming that a set of passage pairs (p n 1 , p n 2 ) are available for model to consume. In particular, we derive a set of pairs of passages from the initial set D q by D + q = {(p * , p 1 ), (p * , p 2 ), . . . , (p * , p N )} where p * corresponds to the first passage that is possible to immediately hop to from question q, which may be determined by another model, or by executing the original PATHFID on D q in our case. Global input representation X path+ q is obtained similarly (Eq. 3) by except encoding the new blocks b path+ n 1 ,n 2 allowing for cross-passage interactions, while the target reasoning path Y path+ q remains the same as Y path q . Note that &lt;title-i&gt; special markers are shared between new input block b path+ n 1 ,n 2 and target reasoning path Y path+ q to provide the model with additional clue regarding the first passage on the reasoning path while still relaying the complete evidence fusion to the decoder via information redundancy encoded in X path+ q .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training and Inference</head><p>Having defined global input representation X path q , the decoder autoregressively generates the reasoning path Y path q per token at each step by following self-attention, cross-attention on the entire X path q , and feed-forward modules. So, the overall reasoning path generation is modeled as conditional generation p ? path (Y</p><formula xml:id="formula_7">path q |X path q ). The model then is trained to minimize J(? path ) = - |Y path q |</formula><p>i=1 log p ? (y i |y &lt;i , X path q ) with teacher forcing over a training set of {(q, a, D q )}.</p><p>In the inference, the decoder consumes the input representation X path q computed by encoder, and generates the full reasoning path token by token. We then post-process the decoded sequence using the answer indicator (&lt;answer&gt;) to first obtain the answer, followed by recursively parsing the remaining sequence using the special separator tokens (&lt;title-k&gt;, &lt;facts-k&gt;) to reconstruct the title and retrieve its relevant sentences at each hop k. As illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, the final result of the inference can be summarized into a dictionary which maps each generated passage title to the list of sentence pointers as well as the final answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and General Setup</head><p>We conduct experiments on two multi-hop question answering datasets: HotpotQA and IIRC. HotpotQA <ref type="bibr" target="#b42">(Yang et al., 2018</ref>) is a large-scale human-annotated dataset including 113K multihop questions. It focuses on using documents from Wikipedia as the source of information for answering questions rather than knowledge bases as in other multi-hop QA datasets <ref type="bibr" target="#b38">(Welbl et al., 2018;</ref><ref type="bibr" target="#b35">Talmor and Berant, 2018)</ref>. The questions in Hot-potQA are not constrained by the fixed knowledgebase schema, hence they can cover more diverse topics. The answer for each question in HotpotQA is extracted from 10 paragraphs in the distractor setting, while it is allowed to use the entire Wikipedia for the full wiki setting. There are two main question types bridge (80%) and comparison (20%) in the corpus, where each question is designed in a way that extracting the correct answer requires reasoning over multiple evidence distributed across two passages. While comparison questions do not require the these passages to be processed in a particular order, bridge questions often require identifying the bridge entity in the first passage to correctly hop to the second one that contains the answer. Each question is also provided with the annotation of 2 supporting passages and up to 5 corresponding relevant sentences as their supporting facts. Since our proposed approach is a reader model that reasons over a given set of evidence documents, we primarily focus our experiments on the distractor setting<ref type="foot" target="#foot_0">1</ref> . IIRC <ref type="bibr" target="#b9">(Ferguson et al., 2020)</ref> is a dataset of more than 13K human-written questions over paragraphs from English Wikipedia, where crowdworkers had access only to initial paragraph and list of hyperlinks to other relevant Wikipedia articles, with the missing information occurring in one or more linked documents. This annotation design encouraged less lexical overlap between the questions and the contexts that actually contain the answer. This dataset presents unique challenges compared to HotpotQA because (1) it additionally requires discrete/numerical reasoning and identification of unanswerable questions, which adds up to 4 different possible answer types (span, binary, numerical, unanswerable), and (2) about 30% of questions require reasoning over more than 2 passages including the main passage. Evaluation Metrics. We use standard metrics exact-match (EM) and F 1 scores for measuring the quality of predicted answers. For HotpotQA experiments, we are also able to evaluate PATHFID on supporting fact predictions using the official metrics (Support-EM, Support-F 1 ), which measures the performance of the reader model in correctly identifying the supporting facts from the relevant passages. Note that this metric implicitly requires correctly identifying relevant passages among the distractors as well. For our experiments on IIRC dataset, similar to the baseline model constructed in the original work <ref type="bibr" target="#b9">(Ferguson et al., 2020)</ref>, we follow the evaluation methods used by DROP <ref type="bibr" target="#b6">(Dua et al., 2019)</ref>. Implementation Details. We use pre-trained T5large encoder-decoder <ref type="bibr" target="#b31">(Raffel et al., 2020)</ref> to initialize the models in our experiments. We train the model with batch size of 64 with constant learning rate of 1e-4 for 10 epochs. We use maximum length of 256 (resp. 512) tokens for input blocks of PATHFID (resp. PATHFID+), while the maximum target sequence length is set to be 64. However, the sequence truncation is performed on the reasoning path excluding answer part for sequences of length longer than 64 tokens. All the experiments are conducted on a machine with 4 or 8 many 40GB A100 GPUs. Our code is based on Huggingface Transformers <ref type="bibr" target="#b39">(Wolf et al., 2019)</ref>. Please see Appendix for further details on the hyperparameter settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Experiments: HotpotQA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Overall Results</head><p>We present our main results on the HotpotQA distractor setting in Table <ref type="table" target="#tab_0">1</ref>. We report results on the HotpotQA development set in comparison with the Answer Support Methods EM F1 EM F1</p><p>Baseline <ref type="bibr" target="#b42">(Yang et al., 2018)</ref> 44.4 58.3 22.0 66.7 DFGN <ref type="bibr" target="#b30">(Qiu et al., 2019)</ref> 55.4 69.2 --QFE <ref type="bibr" target="#b26">(Nishida et al., 2019)</ref> 53.7 68.7 58.8 84.7 SAE <ref type="bibr" target="#b36">(Tu et al., 2020)</ref> 61.3 74.8 58.1 85.3 SAE-large <ref type="bibr" target="#b36">(Tu et al., 2020)</ref> 67.7 80.8 63.3 87.4 Graph Recurrent Retriever <ref type="bibr" target="#b0">(Asai et al., 2020)</ref>   <ref type="bibr" target="#b0">(Asai et al., 2020)</ref>. In summary, PATH-FID establishes the usefulness of modeling the full reasoning path along with answer generation for multi-hop QA. More notably, PATHFID+ achieves a quite significant performance gain across all the central evaluation metrics, demonstrating the importance of cross-passage interactions. Overall results validate the effectiveness of the two central modeling contributions of our proposed method. Next, we present further analysis and discussion on the unique advantages of PATHFID approach under a few central questions which motivated our research at the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Analysis</head><p>How faithfully grounded are the generated answers on supporting facts?</p><p>In Table <ref type="table">2</ref>, we present a detailed analysis comparing different models in terms of the faithfulness of their gen-erated answers on both gold and predicted supporting facts. The first row focuses on the passagelevel answer grounding computed by the percentage of the answers found in one of the gold supporting passages, while the second row reports the same analysis on sentence-level. We can observe that PATHFID models significantly improves on how faithfully the generated answers are grounded on the supporting facts both at passage-level and sentence-level granularities. The next two rows provide further insight into the quality of the generated supporting facts by PATHFID models by measuring how often the gold answer can be found in them. This analysis shows that the generated supporting facts are of quite high-quality including the gold answer for more than 95.3% and 96.2% at sentence-level and passage-level, respectively. The last two rows measure the faithfulness of the generated answers on the model generated supporting facts, which is not applicable to FID model as it does not perform supporting fact prediction. We observe that the generated answers are quite faithfully grounded on the predicted supporting facts, showing the path generation not only improves the answer EM performance but also successfully grounds them on the evidence it generates as part of the full reasoning path.</p><p>It is important emphasize here that extractive reader models can be guaranteed to output perfectly grounded answers simply by locating the answer in their predicted supporting facts. On the other hand, it is difficult for generative models to ensure 100% answer grounding simply due to its generative na- ture. However, we are able to provide additional evidence validating the answers generated by PATH-FID are significantly grounded in the supporting facts it generates, which might implicitly indicate that the generated reasoning path tightly aligns with the model's underlying process for answer generation. Although this is a strong evidence, it is still quite implicit in exposing the model's prediction process, so we see our approach as a step in the right direction rather than a complete solution.</p><p>Performance breakdown by the number of supporting facts and question types. In Table <ref type="table" target="#tab_1">3</ref>, we compare the performance of models by breaking them down based on the number of gold supporting sentences and the question type (e.g., bridge and comparison). Our first observation is that PATH-FID provides consistent improvement on answer-EM score over FID across both the question types and different number of supporting facts required to answer the question. The high variance in the answer-EM score on comparison questions can be attributed to the strictness of exact-match metric as well as the imbalanced nature of the dataset where only 5% of the comparison questions have more than 3 supporting facts. Surprisingly, both FID and PATHFID models perform considerably well on the comparison questions even when it requires at least 5 supporting facts.</p><p>A more important motivation behind the performance breakdown analysis was to understand how the supporting fact prediction of PATHFID would change as the number of gold supporting facts grows. Although it starts degrading on examples with more than 2 supporting facts, it still achieves more than 25% Support-EM for bridge questions with up to 4 supporting facts. Recalling the average performance on the whole dataset is less than 60%, we conclude this result might be satisfactory enough, especially for a fully generative Analyzing the evolution of sub-tasks during joint training with PATHFID. In Figure <ref type="figure" target="#fig_2">3</ref>, we present the evolution of PATHFID model on the Hot-potQA development set at every 500 training steps. We observe that while the model more quickly picks up the patterns for title generation, it takes much longer for it to reach to a reasonable level of fact prediction. As one would expect, the general trend in the evolution of different segments (title-1, facts-1, title-2, facts-2, answer) of the reasoning path mostly follows the difficulty of the corresponding sub-task although all the sub-tasks are jointly formulated and trained in an end-to-end fashion. On the other hand, it seems counter-intuitive for model to reach to a better accuracy on predicting the facts of the second passage (F2-EM) on the reasoning path earlier despite having a better accuracy on (T1-EM). However, one can also interpret it as a result of stronger feedback provided by the answer segment of the reasoning path as most of the ground-truth answers are contained in the facts of the second passage.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments: IIRC</head><p>In addition to our main experiments presented in greater detail, we also conduct experiments on IIRC dataset to verify the generalization of the proposed approach. To this end, we closely follow the authors' model-free retrieval setting (referred to as Oracle L+C in Table -3) because the model checkpoints for the baseline retrieval model are not available in the public release. We use a python script 2 provided in the open-sourced repository to replicate the same setting for a fair comparison.</p><p>In Table <ref type="table" target="#tab_3">5</ref>, we present the results on the development set for our proposed PATHFID and PATH-FID+ in comparison with the baseline reported in the original paper <ref type="bibr" target="#b9">(Ferguson et al., 2020)</ref> and our implementation of the FiD (Izacard and Grave, 2021) baseline. FID model obtains a comparable F1 with IIRC baseline with a slightly worse exact-match performance. However, the proposed PATHFID approach is able to provide 1.3% and 1.4% improvement in F1 score over the two baselines. Furthermore, PATHFID+ extension leads to the best performance achieving 4.7% and 4.2% EM score improvement in absolute value over the FID 2 https://github.com/jferguson144/ IIRC-baseline/blob/main/make_drop_style. py baseline and IIRC baseline, respectively. Our experimental results validate the benefit of the proposed approach on the IIRC dataset, suggesting strong evidence for the generalizability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analyzing the Benefit of Joint Training</head><p>In Table <ref type="table" target="#tab_2">4</ref>, we present the results of a case study where we analyze the benefit of multi-task training on the passage chain prediction. The first row of the table shows the results for training PATHFID only to predict the sequence of titles for the gold passages (i.e., [t1-t2]), which is just a subsequence of the full reasoning path obtained by discarding facts and the answer. The second row is another variant, where we add the answer back to the linearized target sequence while still excluding the segments corresponding to the facts. The last row correspond to the full reasoning path generation, which is corresponding to the original formulation of PATHFID as described in Section 3 and illustrated in Figure <ref type="figure" target="#fig_1">2</ref>. Comparing first two rows in Table <ref type="table" target="#tab_2">4</ref>, we can immediately observe that including answer segment in the target reasoning path (i.e., [t1-t2-answer]) boosts the performance across the board although in principle it makes the task more complicated while utilizing the same underlying model capacity. Further including segments corresponding to FACTS (sentences within supporting passages) in addition to answer segment (i.e., [t1-f1-t2-f2-answer] -full reasoning path) boosts the title-EM even further, especially before applying title reconstruction post-processing step. Although the objective of the first task <ref type="bibr">(i.e., [t1-t2]</ref>) is perfectly aligned with the evaluation metric used in Table <ref type="table" target="#tab_2">4</ref>, the performance of the resulting model remains inferior compared to jointly modeling the same task with the answer (and/or supporting facts) prediction. These two observations elicit a compelling evidence regarding the benefit of jointly modeling the sub-tasks of multi-hop QA as single sequence capturing the full reasoning path.</p><p>Multi-hop question answering. Research on multi-hop QA aims to tackle complex questions that require reasoning across multiple pieces of evidence in multiple documents <ref type="bibr" target="#b38">(Welbl et al., 2018;</ref><ref type="bibr" target="#b42">Yang et al., 2018;</ref><ref type="bibr" target="#b9">Ferguson et al., 2020)</ref>. In particular, the HotpotQA dataset <ref type="bibr" target="#b42">(Yang et al., 2018)</ref> provides both the closed and open-domain settings to evaluate multi-hop reading comprehension models. Compared to single-hop QA, such complex questions pose additional challenges for both reader and retriever models since they are required to capture relationships between documents, instead of independently processing each document. This is challenging because the number of document combinations exponentially grows due to the sequential nature of the process. Two recent works <ref type="bibr" target="#b25">(Nie et al., 2019;</ref><ref type="bibr" target="#b0">Asai et al., 2020)</ref> have tackled this challenge by leveraging hyperlink structure in the underlying Wikipedia corpus, while <ref type="bibr" target="#b40">Xiong et al. (2021)</ref> has taken a recursive approach to extend the dense retrieval process to handle sequential search. Most of the reading comprehension (RC) models in existing work <ref type="bibr" target="#b41">(Xiong et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2019;</ref><ref type="bibr" target="#b26">Nishida et al., 2019;</ref><ref type="bibr" target="#b28">Qi et al., 2021;</ref><ref type="bibr" target="#b20">Li et al., 2020;</ref><ref type="bibr" target="#b40">Xiong et al., 2021)</ref> follow an extractive architecture <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> for selection of the answer spans and their corresponding supporting evidence with minor modifications such as initializing the backbone model from a stronger or larger pre-trained models <ref type="bibr" target="#b4">(Clark et al., 2020)</ref>. On the other hand, some recent works <ref type="bibr">(Inoue et al., 2021</ref>) take a more abstractive approach and generate question-focused summaries of input paragraphs as concise explanations to be fed to the RC module. Generative question answering. Especially after the emergence of the SQuAD dataset <ref type="bibr" target="#b32">(Rajpurkar et al., 2016)</ref>, neural extractive QA models have been widely studied. An underlying assumption is that we can extract a short text span (or a phrase) as an answer, but it is not always the case in reality. Motivated by this, the generative QA approach has also been investigated <ref type="bibr" target="#b11">(Hewlett et al., 2017;</ref><ref type="bibr" target="#b8">Fan et al., 2019)</ref>. Recent advances on pre-trained transformers have pushed this direction; for example, <ref type="bibr">Lewis et al. (2020a)</ref> jointly trained a generative QA model along with a text retrieval model, and <ref type="bibr" target="#b33">Roberts et al. (2020)</ref> explored an ambitious approach to directly generate an answer without any evidence documents. We focused on the fusionin-decoder model <ref type="bibr" target="#b14">(Izacard and Grave, 2021)</ref>; they claimed that the decoder might be good at aggregating information across multiple documents. However, we have shown that it is not trivial in the multihop reasoning task, and pushed the model's ability to jointly learn to predict reasoning paths. Besides question answering, jointly learning multiple intrinsic capabilities required by the final objective with a generative approach has been shown useful in modeling other NLP tasks such as task-oriented dialogues <ref type="bibr" target="#b24">(Neelakantan et al., 2019;</ref><ref type="bibr" target="#b12">Hosseini-Asl et al., 2020;</ref><ref type="bibr" target="#b28">Peng et al., 2021)</ref>. Open-domain question answering.</p><p>Opendomain QA <ref type="bibr" target="#b37">(Voorhees, 1999)</ref> is practically important, which requires a system to retrieve relevant documents to answer a given question. The task is recently gaining much attention, thanks to the development of large-scale datasets like HotpotQA, SQuAD Open <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>, Natural Questions Open <ref type="bibr" target="#b16">(Kwiatkowski et al., 2019;</ref><ref type="bibr" target="#b17">Lee et al., 2019)</ref>, etc. Pre-trained transformer models like BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> have accelerated the development of neural text retrievers <ref type="bibr" target="#b17">(Lee et al., 2019;</ref><ref type="bibr" target="#b15">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b0">Asai et al., 2020;</ref><ref type="bibr" target="#b40">Xiong et al., 2021;</ref><ref type="bibr" target="#b21">Liu et al., 2021)</ref> in the retrieverreader framework <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>. We have investigated the effectiveness of our method in the multi-hop open-domain QA task (see Appendix B) using an existing external retriever component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we propose a generative question answering (QA) approach that models multi-hop QA as a single sequence prediction task. It learns to generate an answer along with a reasoning path to improve its capability of multi-hop reasoning. Our experiments on prominent multi-hop QA benchmarks, HotpotQA and IIRC, validate the promise and effectiveness of our proposed method PATH-FID and its extension PATHFID+. Future work will explore (1) our PATHFID approach more closely with text retrieval models in open-domain QA scenarios and (2) more explicit grounding on the input information to make our approach even more interpretable and controllable.  In Figure <ref type="figure" target="#fig_4">4</ref> and 5, we visualize the correlation between supporting evidence and answer prediction performances for comparison and bridge question types, respectively. To obtain these plots, we first split the examples into 10 buckets where n-th bucket contains the examples with support-F1 score in (10 * (n -1), 10 * n] percentile for n = {1, 2, . . . , 10}. Then, we take the average answer prediction accuracy (both EM and F1) over these examples for each bucket, and report this number on the y-axis of the plot at the corresponding support-F1 bucket on the x-axis, while dropping the empty buckets. Note that x = 0 corresponds to examples with support-F1 score of 0. Also note that the size of a data point on the figure reflects the number of examples in the corresponding bucket as also indicated by the legend. From Figures <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_5">5</ref>, we can observe that the accuracy of the generated answers is significantly lower, 30% for bridge and 10% for comparison, for the first bucket with zero support-F1 compared to buckets with positive support-F1 score. This suggests that the model has a difficult time figuring out the an-swer when the supporting evidence prediction is poor. Another observation that holds for both categories is the general trend of increased answer quality as the supporting fact prediction improves. Combining these two points provide additional evidence (in addition to Table <ref type="table">2</ref> in the main paper) implicitly supporting the answer generation process of PATHFID being grounded on the generated supporting facts, which is generated as the prefix of the answer segment in the full decoded reasoning path sequence during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Case Study: Full-Wiki Setting with</head><p>Multi-hop Dense Retriever</p><p>In this subsection, we evaluate PATHFID in open domain setting of HotpotQA leveraging a recently proposed multi-hop dense retriever (MDR) <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref> for passage retrieval. Unlike distractor setting, MDR returns a set of passage pairs</p><formula xml:id="formula_8">D MDR q = {(p (1) 1 , p (2) 1 ), (p (1) 2 , p (2) 2 ), . . . , (p (1) N , p<label>(2)</label></formula><p>N )} for question q, where each passage p (i) n comes with a title t (i) n , being retrieved from Wikipedia corpus. This setting naturally fits into how we formulate PATHFID+, which operates on the pairs of input passages as introduced in Section 3.3, where we simply set D + q = D MDR q . For experiments with FID and PATHFID, which operate on set of single input passages, we simply split the pairs into single passages, ending up with 2K passages when using top-K retrieved paths from MDR. We present our results for this setting in Table <ref type="table" target="#tab_4">6</ref>. Similar to our observation in distractor setting, PATHFID provides a significant (%1.8) answer EM score improvement over FID, while also achieving a quite competitive performance on the supporting fact prediction compared to strong discriminative models <ref type="bibr" target="#b0">(Asai et al., 2020;</ref><ref type="bibr" target="#b20">Li et al., 2020)</ref> optimized for better retrieval performance. Most notably, PATHFID+ provides significant gains over PATHFID, achieving 59.8% answer-EM and 52.8% supporting fact EM score, showing the importance of encoding cross-passage interactions. It is important to note here that our results with PATHFID+ is not directly comparable to the reader results from MDR <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref> because we are able to only use top-25 retrieved paths due to hardware limitations. Finally, we also evaluate the same PATHFID+ model on Dev * obtained by adding the pair of gold passages in D MDR q , where we aim to isolate the error propagation from the Answer Support Methods EM F1 EM F1</p><p>GoldEn Retriever <ref type="bibr" target="#b29">(Qi et al., 2019</ref>) -49.8 -64.6 Semantic Retrieval <ref type="bibr" target="#b25">(Nie et al., 2019)</ref> 46.5 58.8 39.9 71.5 Transformer-XH <ref type="bibr" target="#b44">(Zhao et al., 2020)</ref> 50.2 62.4 42.2 71.6 Graph Recurrent Retriever <ref type="bibr" target="#b0">(Asai et al., 2020</ref><ref type="bibr">) (wwm) 60.5 73.3 49.3 76.1 Graph Recurrent Retriever (Asai et al., 2020) (base)</ref> 52.7 65.8 47.9 75.0 HopRetriever <ref type="bibr" target="#b20">(Li et al., 2020)</ref> 62.1 75.2 52.5 78.9 HopRetriever-plus <ref type="bibr" target="#b20">(Li et al., 2020)</ref> 66.6 79.2 56.0 81.8 MDR-Electra (Top-50 paths) <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref> 61.7 74.3 --MDR-FiD (Top-50 paths) <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref> 61 underlying retriever. Table <ref type="table" target="#tab_4">6</ref> shows that both the answer and supporting fact prediction performance improves quite significantly, showing the potential impact that developments on retriever side of the problem can also make.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The Effect of Model Size for Future Reference</head><p>As discussed in Section D, fine-tuning PATHFID+ with T5-large initialization might require significant resources and non-trivial memory efficient optimization (e.g., gradient checkpointing). To provide a baseline with a smaller model for future research, here we include the results of PATHFID+ with T5-base initialization using the same setting reported in Table <ref type="table" target="#tab_4">6</ref> in the main paper. As presented in Table <ref type="table">7</ref>, although the performance difference on the supporting fact prediction is relatively small ( 1%), answer prediction performance drops significantly (by 3.2%) when we switch from T5-large to T5-base. However, working with T5-base is much more efficient in terms of resources and iteration time for building baselines, trying out new ideas and thought experiments. So, we hope this baseline will be helpful for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More on Training and Implementation Details</head><p>Hop ordering. HotpotQA benchmark provides annotation only for unordered gold passages, without explicitly specifying which passage corresponds to the k-th hop (e.g., first-hop, second-hop, etc.) on the reasoning path. In our implementation, we combine the heuristic strategies applied by GRR <ref type="bibr" target="#b0">(Asai et al., 2020)</ref> and MDR <ref type="bibr" target="#b40">(Xiong et al., 2021)</ref>. More precisely, if only one of the gold passages contains the answer, then we take the passage that includes the answer span as the final passage. If the answer span is included in both passages, we break the tie by falling back to the hyperlink-based ordering strategy proposed by GRR <ref type="bibr" target="#b0">(Asai et al., 2020)</ref>.</p><p>Post-processing for passage title reconstruction.</p><p>Note that PATHFID generates the titles of the passages on the reasoning path token by token including the separator tokens. However, the decoder might fall into some minor errors during the generation process, which may cause the resulting titles to end up slightly different from the original ones.</p><p>To account for such minor errors, we leverage the set of titles coming from the input passages and find the most similar among them to our generated passage titles based on token-level F1-score. We call this process title reconstruction and apply it while reporting the performance for supporting fact predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hyperparameter Settings</head><p>In Tables 9, 8 and 10, we provide the full set of important hyperparameters used for the models reported both in the main paper (HotpotQA-distractor and IIRC) and in the Appendix B (HotpotQAfullwiki), respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Qualitative Analysis</head><p>In this section, we provide examples comparing the predictions of FID and PATHFID over bridge and comparison question types. Each of the example Table <ref type="table" target="#tab_7">11</ref>, 12, 13 in the next pages follows a similar structure, where we include gold answer, FID answer prediction, PATHFID answer (and full path) prediction, and 5 supporting passages (out of 10) for the brevity of presentation. Among the input passages, the first two correspond to gold passages, for which we include the full content as well as highlighting the key supporting facts/sentences with orange color. The following three passages are presented as a subset of the distractors, for each of which we include a one-line content unless it plays a crucial role in distracting at least one of the models in making a wrong prediction. In this case, we also add the content of this particular passage as well as highlighting the specific distractor span/sentence causing the failure of either FID or PATHFID.  <ref type="table" target="#tab_0">12</ref>: BRIDGE-type question example, where both PATHFID and FID fail to predict the exact gold answer. Although the generated answers are wrong, they can both be acceptable by humans. On the other hand, both answers fail in EM accuracy, but PATHFID manages to perfectly generate the reasoning path starting from the right sentence of the correct first passage, then jumping to correct second-hop passage, followed by identifying its key sentence (&lt;f2&gt;), then finally locating answer in the right part of this evidence, but only failing in getting the span perfectly, which still rewards it with a reasonable F1 score. However, this example is also important in showing the possible ambiguities in questions and strictness of the exact-match accuracy metric. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>b n is independently encoded into |b n | ? d-dimensional output representations, which are then concatenated to form a unified input representation X = [Enc(b 1 ); Enc(b 2 ); . . . , Enc(b N )] (1) of dimension ( n |b n |) ? d where |b n | denotes the length of the n-th block b n in number of tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PATHFID model overview. Each question+passage block is encoded in parallel, which are then concatenated in to a long flat sequence of vector representations. The decoder then consumes this long sequence and generates the full reasoning path, which is then uniquely parsed into the final answer along with the supporting facts exposing the underlying reasoning.</figDesc><graphic url="image-2.png" coords="4,70.87,70.87,453.53,229.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PATHFID model evolution on the HotpotQA Dev set during training. T1-EM, T2-EM, indicate the model's accuracy on predicting the title-1 and title-2 on the reasoning path. Similarly F1-EM, and F2-EM denote the model's accuracy on predicting set of supporting facts in passage-1 and passage-2.model on a very strict evaluation metric.</figDesc><graphic url="image-3.png" coords="7,306.43,233.89,217.70,149.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualizing the correlation between evidence and answer prediction for COMPARISON questions.</figDesc><graphic url="image-4.png" coords="13,70.87,120.86,226.77,122.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visualizing the correlation between evidence and answer prediction for BRIDGE questions.</figDesc><graphic url="image-5.png" coords="13,70.87,298.78,226.77,122.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on the development set of HotpotQA distractor setting in comparison with previous work. FID* indicates that the reported results are obtained by our implementation following the training details in the paper.</figDesc><table><row><cell>(base)</cell><cell>52.7 65.8 57.4 84.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Performance breakdown on Answer-EM and Support-EM by question type and the number of gold supporting facts (rows). Since FID does not generate supporting facts, corresponding columns are left empty.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Answer-EM</cell><cell></cell><cell></cell><cell cols="2">Support-EM</cell><cell></cell></row><row><cell></cell><cell cols="2">Comparison</cell><cell cols="2">Bridge</cell><cell cols="2">Comparison</cell><cell cols="2">Bridge</cell></row><row><cell cols="9"># Supp Facts FID PATHFID FID PATHFID FID PATHFID FID PATHFID</cell></row><row><cell>2</cell><cell>70.4</cell><cell>71.8</cell><cell>63.3</cell><cell>64.6</cell><cell>-</cell><cell>86.7</cell><cell>-</cell><cell>70.0</cell></row><row><cell>3</cell><cell>66.1</cell><cell>68.2</cell><cell>62.7</cell><cell>63.1</cell><cell>-</cell><cell>43.4</cell><cell>-</cell><cell>30.7</cell></row><row><cell>4</cell><cell>62.2</cell><cell>63.8</cell><cell>64.3</cell><cell>66.5</cell><cell>-</cell><cell>5.4</cell><cell>-</cell><cell>26.2</cell></row><row><cell>&gt;=5</cell><cell>83.3</cell><cell>87.5</cell><cell>60.0</cell><cell>65.0</cell><cell>-</cell><cell>0.0</cell><cell>-</cell><cell>3.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>The effect of joint training as a case study on title prediction performance of PATHFID variants trained with different target reasoning paths. Generated-Title column corresponds to ordered passage chain prediction performance in exact-match (EM), while Reconstructed-Title version is computed after applying title reconstruction post-processing described in Section D.</figDesc><table><row><cell>[t1-t2]</cell><cell>74.3</cell><cell>74.8</cell><cell>71.6</cell><cell>75.4</cell><cell>75.4</cell><cell>72.9</cell></row><row><cell>[t1-t2-answer]</cell><cell>74.8</cell><cell>75.0</cell><cell>71.8</cell><cell>75.8</cell><cell>75.8</cell><cell>73.3</cell></row><row><cell>[t1-f1-t2-f2-answer]</cell><cell>75.0</cell><cell>75.1</cell><cell>71.9</cell><cell>76.0</cell><cell>75.6</cell><cell>73.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Answer</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell>EM</cell><cell>F1</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IIRC* (Ferguson et al., 2020)</cell><cell cols="2">63.9 69.2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">FID** (Izacard and Grave, 2021) 63.4 69.1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>This Work</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PATHFID</cell><cell></cell><cell cols="2">65.2 70.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell>PATHFID+</cell><cell></cell><cell cols="2">68.1 72.9</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on IIRC dataset in model-free retrieval setting comparing the proposed method against two baselines.</figDesc><table /><note><p><p><p>* indicates that the result is taken directly from the original paper</p><ref type="bibr" target="#b9">(Ferguson et al., 2020)</ref> </p>(see their Table-3), while ** indicates that we obtain the result of FID with our implementation.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Results for open-domain setting using MDR<ref type="bibr" target="#b40">(Xiong et al., 2021)</ref> as the retriever. Dev * refers to the development set where the retrieved passages are expanded with the gold passage (as an oracle setting) to account for the cases where the retriever fails to retrieve the gold passages. FID* indicates our implementation.</figDesc><table><row><cell>.7 73.1</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows the benefit of title re-</cell></row><row><cell>construction for mitigating such minor generation</cell></row><row><cell>errors. On the other hand, the small performance</cell></row><row><cell>boost suggests that titles PATHFID already gener-</cell></row><row><cell>ates quite faithful title predictions.</cell></row><row><cell>Model selection. For all the models reported in</cell></row><row><cell>this work, we perform evaluation at every 500 steps</cell></row><row><cell>during training by decoding the whole development</cell></row><row><cell>set on a separate machine in a non-blocking fash-</cell></row><row><cell>ion. We then select the best model based on the</cell></row><row><cell>answer exact-match score performance. However,</cell></row><row><cell>since PATHFID variants generate more than just</cell></row><row><cell>the answer, it can be leveraged to optimize for a</cell></row><row><cell>more holistic metric including the supporting fact</cell></row><row><cell>prediction performance, offering further control on</cell></row><row><cell>model selection. We leave further exploration of</cell></row><row><cell>this phenomenon to future work.</cell></row><row><cell>Scaling to larger evidence pools for full-wiki set-</cell></row><row><cell>ting. As briefly noted in Appendix B, we report</cell></row><row><cell>results in full-wiki setting using only top-25 paths</cell></row><row><cell>returned by MDR (Xiong et al., 2021) due to hard-</cell></row><row><cell>ware constraints. More precisely, a single training</cell></row><row><cell>example becomes impossible to fit into GPU mem-</cell></row><row><cell>ory (40GB) even for top-25 paths for PATHFID+</cell></row><row><cell>model with T5-large initialization. To make the</cell></row><row><cell>training feasible, we resort to gradient checkpoint-</cell></row><row><cell>ing 3 which trades off GPU memory with speed.</cell></row><row><cell>However, in this case, even with 25 retrieved paths,</cell></row><row><cell>training PATHFID+ for 10K steps with batch size of</cell></row><row><cell>64 using gradient accumulation takes 19 hours on</cell></row><row><cell>8 A100 GPUs with 40GB memory each, which is</cell></row><row><cell>one of the most prominent limitations hurdling the</cell></row><row><cell>progress for this line of research. Further research</cell></row><row><cell>on making generative approaches with large pre-</cell></row><row><cell>trained models more efficient without losing on the</cell></row><row><cell>performance side holds a great potential impact to</cell></row><row><cell>accelerate the progress of fully generative models</cell></row><row><cell>for question answering.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters for experiments on HotpotQA Full-wiki setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 11 :</head><label>11</label><figDesc>Memphis Hustle: &lt;f1&gt; The Memphis Hustle are an American professional basketball team of the NBA G League announced to begin play for the 2017-18 season as an affiliate of the Memphis Grizzlies of the National Basketball Association (NBA). &lt;f2&gt; Based in the Memphis suburb of Southaven, Mississippi, the team will play their home games at the Landers Center. 2. Southaven, Mississippi: &lt;f1&gt; Southaven is a city in DeSoto County, Mississippi, United States. &lt;f2&gt; It is a suburb of Memphis, Tennessee, and a principal city in the Memphis metropolitan area. &lt;f3&gt; The 2010 census reported a population of 48,982, making Southaven the third largest city in Mississippi. &lt;f4&gt; Southaven is traversed from north to south by the I-55/I-69 freeway. &lt;f5&gt; The city's name derives from the fact that Southaven is located south of Whitehaven, a neighborhood in Memphis. 3. Lakeland, Tennessee: Lakeland is a city in Shelby County, Tennessee, and a suburb of Memphis. The population was 12,430 at the 2010 census. BRIDGE-type question example, where PATHFID predicts the correct answer while FID fails to do so. The third passage is the distractor causing FID to make a wrong prediction due to the highlighted sentence in red.QuestionWhat government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?Input Passages1. Kiss and Tell (1945 film): &lt;f1&gt; Kiss and Tell is a 1945 American comedy film starring then 17-year-old Shirley Temple as Corliss Archer. &lt;f2&gt; In the film, two teenage girls cause their respective parents much concern when they start to become interested in boys. &lt;f3&gt; The parents' bickering about which girl is the worse influence causes more problems than it solves. 2. Shirley Temple: &lt;f1&gt; Shirley Temple Black(April 23, 1928 -February 10,  2014)  was an American actress, singer, dancer, businesswoman, and diplomat who was Hollywood's number one box-office draw as a child actress from 1935 to 1938. &lt;f2&gt; As an adult, she was named United States ambassador to Ghana and to Czechoslovakia and also served as Chief of Protocol of the United States. 3. Meet Corliss Archer (TV series): Meet Corliss Archer is an American television sitcom that ... 4. Meet Corliss Archer: Meet Corliss Archer, a program from radio's Golden Age, ran from ... 5. Charles Craft: Charles Craft (May 9, 1902 -September 19, 1968) was an English-born ... ... Chief of Protocol of the United States PATHFID Output &lt;title-1&gt; Kiss and Tell (1945 film) &lt;facts-1&gt; &lt;f1&gt; &lt;title-2&gt; Shirley Temple &lt;facts-2&gt; &lt;f2&gt; &lt;answer&gt; Chief of Protocol of the United States Table</figDesc><table><row><cell>Question</cell><cell>The Memphis Hustle are based in a suburb of a city with a population of what in</cell></row><row><cell></cell><cell>2010?</cell></row><row><cell>Input Passages</cell><cell>1. 4. Marion, Arkansas: Marion is a city in and the county seat of Crittenden</cell></row><row><cell></cell><cell>County, Arkansas ...</cell></row><row><cell></cell><cell>5. West Memphis, Arkansas: West Memphis is the largest city in Crittenden</cell></row><row><cell></cell><cell>County, Arkansas ...</cell></row><row><cell></cell><cell>...</cell></row><row><cell>Gold Answer</cell><cell>48,982</cell></row><row><cell>FID Answer</cell><cell>12,430</cell></row><row><cell cols="2">PATHFID Answer 48,982</cell></row><row><cell>PATHFID Output</cell><cell>&lt;title-1&gt; Memphis Hustle &lt;facts-1&gt; &lt;f1&gt; &lt;f2&gt; &lt;title-2&gt; Southaven, Mississippi &lt;facts-2&gt; &lt;f1&gt; &lt;f2&gt; &lt;f3&gt; &lt;answer&gt; 48,982</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 13 :</head><label>13</label><figDesc>QuestionWhich band, Letters to Cleo or Screaming Trees, had more members? Screaming Trees: &lt;f1&gt; Screaming Trees was an American rock band formed in Ellensburg, Washington in 1985 by vocalist Mark Lanegan, guitarist Gary Lee Conner, bass player Van Conner and drummer Mark Pickerel. &lt;f2&gt; Pickerel had been replaced by Barrett Martin by the time the band reached its most successful period. &lt;f3&gt; Although widely associated with grunge, the band's sound incorporated hard rock and psychedelic elements. &lt;f4&gt; During Screaming Trees' existence the band released seven studio albums, five EPs, and three compilations. 2. Letters to Cleo: &lt;f1&gt; Letters to Cleo are an alternative rock band from Boston, Massachusetts, best known for the 1994 single, "Here &amp; Now", from their fulllength debut album, "Aurora Gory Alice". &lt;f2&gt; The band's members are Kay Hanley, Greg McKenna, Michael Eisenstein, Stacy Jones, Scott Riebling, and later, Tom Polce. 3. Change Has Come: Change Has Come was the only recording the Screaming Trees released ... 4. Jamboree (Beat Happening album): Jamboree is the second album by Beat Happening, released ... 5. Gary Lee Conner: Gary Lee Conner (born Lee Gary Conner on August 22, 1962 in Fort Irwin ... ... COMPARISON-type question example, where PATHFID predicts the correct answer while FID fails to make a correct prediction.</figDesc><table><row><cell cols="2">Input Passages 1. Gold Answer Letters to Cleo</cell></row><row><cell>FID Answer</cell><cell>Screaming Trees</cell></row><row><cell cols="2">PATHFID Answer Letters to Cleo</cell></row><row><cell>PATHFID Output</cell><cell>&lt;title-1&gt; Screaming Trees &lt;facts-1&gt; &lt;f1&gt; &lt;title-2&gt; Letters to Cleo &lt;facts-2&gt; &lt;f1&gt; &lt;f2&gt; &lt;answer&gt; Letters to Cleo</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>See Appendix B for PATHFID results in open-domain setting using MDR(Xiong et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2021) as the retriever.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://pytorch.org/docs/stable/checkpoint.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank the members of <rs type="institution">Salesforce AI Research team</rs> for fruitful discussions, as well as the anonymous reviewers for their helpful feedback.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-hop question answering via reasoning chains</title>
		<author>
			<persName><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Ting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno>CoRR, abs/1910.02610</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<title level="m">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Co-search: COVID-19 information retrieval with semantic search, question answering, and abstractive summarization</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuprit</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romain</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/2006.09595</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">ELI5: long form question answering</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>CoRR, abs/1907.09190</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">IIRC: A dataset of incomplete information reading comprehension questions</title>
		<author>
			<persName><forename type="first">James</forename><surname>Ferguson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrieval augmented language model pre-training</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3929" to="3938" />
		</imprint>
	</monogr>
	<note>Panupong Pasupat, and Mingwei Chang</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Accurate supervised and semisupervised machine reading for long documents</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hewlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple language model for task-oriented dialogue</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Hosseini-Asl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Niranjan Balasubramanian, and Kentaro Inui. 2021. Summarize-then-answer: Generating concise explanations for multi-hop reading comprehension</title>
		<author>
			<persName><forename type="first">Naoya</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Sinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Natural questions: A benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the</title>
		<meeting>the 58th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tim Rockt?schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledgeintensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Hopretriever: Retrieve hops over wikipedia to answer complex questions</title>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhou</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingquan</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2012.15534</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dense hierarchical retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Ye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A discrete hard EM approach for weakly supervised question answering</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AmbigQA: Answering ambiguous open-domain questions</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Neural assistant: Joint action prediction, response generation, and latent knowledge reasoning</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Semih</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishaal</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinnadhurai</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<idno>CoRR, abs/1910.14613</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP). Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Answering while summarizing: Multi-task learning for multi-hop QA with evidence extraction</title>
		<author>
			<persName><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the</title>
		<meeting>the 57th Annual Meeting of the</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics. Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Soloist: Building task bots at scale with transfer learning and machine teaching</title>
		<author>
			<persName><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahin</forename><surname>Shayandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Liden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Retrieve, read, rerank, then iterate: Answering open-domain questions of varying reasoning steps from text</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haejun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">"</forename><surname>Oghenetegiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Tg" Sido</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Answering complex open-domain questions through iterative query generation</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP). Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Memory augmented sequential paragraph retrieval for multi-hop question answering</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno>CoRR, abs/2102.03741</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers). Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The trec-8 question answering track report</title>
		<author>
			<persName><forename type="first">Ellen</forename><forename type="middle">M</forename><surname>Voorhees</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of TREC-8</title>
		<meeting>TREC-8</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="77" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>CoRR, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Answering complex open-domain questions with multi-hop dense retrieval</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srini</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashar</forename><surname>Mehdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple yet effective bridge reasoning for open-domain multi-hop question answering</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Machine Reading for Question Answering</title>
		<meeting>the 2nd Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Covidex: Neural ranking models and keyword search infrastructure for the COVID-19 open research dataset</title>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Scholarly Document Processing</title>
		<meeting>the First Workshop on Scholarly Document Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
