<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Entailment Graph Learning with Textual Entailment and Soft Transitivity</title>
				<funder ref="#_UuynzZk">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_B9VmWhD">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-04-07">7 Apr 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhibin</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
							<email>fengyansong@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
							<email>zhaody@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Entailment Graph Learning with Textual Entailment and Soft Transitivity</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-07">7 Apr 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2204.03286v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Typed entailment graphs try to learn the entailment relations between predicates from text and model them as edges between predicate nodes. The construction of entailment graphs usually suffers from severe sparsity and unreliability of distributional similarity. We propose a two-stage method, Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 learns local entailment relations by recognizing possible textual entailment between template sentences formed by typed CCGparsed predicates. Based on the generated local graph, EGT2 then uses three novel soft transitivity constraints to consider the logical transitivity in entailment structures. Experiments on benchmark datasets show that EGT2 can well model the transitivity in entailment graph to alleviate the sparsity issue, and lead to significant improvement over current stateof-the-art methods 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Entailment, as an important relation in natural language processing (NLP), is critical to semantic understanding and natural language inference (NLI). Entailment relation has been widely applied in different NLP tasks such as Question Answering <ref type="bibr" target="#b24">(Pathak et al., 2021;</ref><ref type="bibr" target="#b14">Khot et al., 2018)</ref>, Machine Translation <ref type="bibr" target="#b22">(Pad? et al., 2009)</ref> and Knowledge Graph Completion <ref type="bibr" target="#b34">(Yoshikawa et al., 2019)</ref>. When coming across a question that "Which medicine cures the infection?", one can recognize the information "Griseofulvin is preferred for the infection," in the corpus and appropriately write down the answer with the knowledge that "is preferred for" entails "cures" when their arguments are medicines and diseases, although the surface form of predicate "cures" does not exactly appear in the corpus. There are many ways to present one question, and it</p><p>[medicine] is preferred for <ref type="bibr">[disease]</ref> [medicine] cures <ref type="bibr">[disease]</ref> [medicine] is effective for <ref type="bibr">[disease]</ref> [medicine] is related to <ref type="bibr">[disease]</ref> [medicine] causes <ref type="bibr">[disease]</ref> t 1 =medicine t 2 =disease</p><p>[medicine] is preferred for <ref type="bibr">[disease]</ref> [medicine] cures <ref type="bibr">[disease]</ref> [medicine] is effective for <ref type="bibr">[disease]</ref> [medicine] is related to <ref type="bibr">[disease]</ref> [medicine] causes <ref type="bibr">[disease]</ref> t 1 =medicine t 2 =disease is impossible to handle them without understanding the entailment relations behind the predicates. Previous works on analyzing entailment mainly focus on Recognizing Textual Entailment (RTE) between pairs of sentences, and many recent attempts have achieved quite promising performance in detecting entailment relations using transformer-based language models <ref type="bibr" target="#b9">(He et al., 2020;</ref><ref type="bibr" target="#b26">Raffel et al., 2020;</ref><ref type="bibr">Schmitt and Sch?tze, 2021b)</ref>. By modeling typed predicates as nodes and entailment relations as directed edges, the Entailment Graph (EG) is a powerful and well-established form to represent the contextindependent entailment relations between predicates and reflect the global features of entailment inference, such as paraphrasing and transitivity. As EGs are able to help reasoning without additional context or resource, they can be seen as a special type of structural knowledge in natural language. Figure <ref type="figure" target="#fig_0">1</ref> shows an excerpt entailment graph about two types of arguments, Medicine and Disease. Generally speaking, an entailment graphs can be built based on a three-step process: extracting predicate pairs from a corpus, building local graphs with locally computed entailment scores, and modifying the graphs with global methods.</p><p>However, existing EG construction methods still face challenges in both local and global stages. The Distributional Inclusion Hypothesis (DIH) about entailment assumes that given a predicate (relation) p, it can be replaced in any context by another predicate (relation) q if and only if p entails q <ref type="bibr" target="#b6">(Geffet and Dagan, 2005)</ref>. Most local methods in previous works are guided by DIH, thus rely on the distributional co-occurrences from corpora, including named entities, entity pairs and context, as features to compute the local entailment scores. Since different predicate pairs are processed independently, the locally built graphs suffer from severe data sparsity. That is, there are many entailment relations missing (as edges) in the graphs if the predicate pairs do not co-occur in the corpus. Furthermore, predictions from local models may not be coherent with each other, for example, a local model may output three predictions like, a entails b, b entails c and c entails a at the same time, which actually indicate possible errors among the local predictions.</p><p>To overcome the challenges faced by local models, different global approaches are used to take the interactions and dependencies between entailment relations into consideration. The first discussed global dependency is the logical transitivity among different predicates, that is, predicate a entails predicate c if there is another predicate b making both "a entails b" and "b entails c" hold simultaneously. <ref type="bibr" target="#b2">Berant et al. (2011)</ref> uses the Integer Linear Programming (ILP) to ensure the transitivity constraints on the entailment graphs, which is , unfortunately, not scalable on large graphs with thousands of nodes. <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> models the structural similarity across graphs and paraphrasing relations within graphs to learn the global consistency, but does not gain further improvement due to the lack of high-quality local graphs and proper transitivity modeling.</p><p>In order to deal with the problems in the local and global stages, we propose a novel entailment graph learning approach, Entailment Graph with Textual Entailment and Transitivity (EGT2). EGT2 builds high-quality local entailment graphs by inputting predicates as sentences into a transformer-based language model fine-tuned on an RTE task to avoid the unreliability of distributional scores, and models the global transitivity on these scores through carefully designed soft constraint losses, which alleviate the data sparsity and are feasible on large-scale local graphs. Our key insight is that the entailment relation a ? c correctly implied by the transitivity constraint is based on two conditions: (1) the appropriate constraint scalable on large graphs containing rich information, and (2) the reliability of local graphs offering the premise a ? b and b ? c, which is impractical for previous distributional approaches, but may be available for the models well-behaved on RTE tasks. Specifically, the input sentences fed to transformer-based language models are formed without context, which makes our method accessible to those predicates not appearing in the corpus. The transitivity implication is confined to entailment relations with high confidence, which improves the quality of implied edges and cuts down the computational overheads. In a word, this paper makes the following contributions:</p><p>? we present a novel approach based on textual entailment to scoring predicate pairs on local entailment graphs, which is reliable without distributional features and valid for arbitrary predicate pairs.</p><p>? we present three carefully designed global soft constraint loss functions to model the transitivity among entailment relations on large entailment graphs, thus alleviate the data sparsity issue of previous local approaches.</p><p>? we evaluate our method on benchmark datasets, and show that our EGT2 significantly outperforms previous entailment graphs construction approaches. The further analysis proves that our local and global approaches are both useful for learning entailment graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Based on DIH, previous works extract feature vectors for typed predicates to compute the local distributional similarity. The set of entity argument pair strings, like "Griseofulvin-infection" in the example of Section 1, are used as the features weighted by Pointwise Mutual Information <ref type="bibr" target="#b0">(Berant et al., 2015;</ref><ref type="bibr" target="#b11">Hosseini et al., 2018)</ref>. Given the feature vectors for a predicate pair, different similarity scores, like cosine similarity, Lin <ref type="bibr" target="#b18">(Lin, 1998)</ref>, DIRT <ref type="bibr" target="#b19">(Lin and Pantel, 2001)</ref>, Weeds <ref type="bibr" target="#b31">(Weeds and Weir, 2003)</ref> and Balanced Inclusion <ref type="bibr" target="#b30">(Szpektor and Dagan, 2008)</ref>, are calculated as the local similarities.  <ref type="bibr" target="#b2">Berant et al. (2011)</ref>, which selects a transitive sub-graph of a local weighted graph to maximize the summation over the weights of its edges. Their work is limited to a few hundreds of predicates due to the computational complexity of ILP. For better scalability, <ref type="bibr" target="#b1">Berant et al. (2012)</ref> and <ref type="bibr" target="#b0">Berant et al. (2015)</ref> make a strong FRG-assumption that if predicate a entails predicates b and c, b and c entail each other, and an approximation method, called Tree-Node-Fix (TNF). Obviously, the assumption is too strong to be satisfied by real cases.</p><p>Since the hard constraints are difficult to work well on large-scale entailment graphs, <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> propose two global soft constraints that maintain the similarity between paraphrasing predicates within typed graphs and between predicates with the same names in graphs with different argument types. Their soft constraints are also used in <ref type="bibr" target="#b12">Hosseini et al. (2019)</ref> and <ref type="bibr" target="#b13">Hosseini et al. (2021)</ref>. The similarity between paraphrasing predicates, which ensures (a ? c) (b ? c) and (c ? a) (c ? b) when a ? b, implicitly takes the transitivity between paraphrasing predicates and third predicate into consideration. But it ignores the transitivity in more common cases, and leads to a limited improvement on performance.</p><p>Meanwhile, the transformer-based Language Model (LM), although proved to be effective in RTE tasks <ref type="bibr" target="#b9">(He et al., 2020;</ref><ref type="bibr" target="#b26">Raffel et al., 2020;</ref><ref type="bibr">Schmitt and Sch?tze, 2021b)</ref>, has received less attention in entailment graph learning. <ref type="bibr">Schmitt and Sch?tze (2021a)</ref> uses pretrained LM on the Lexical Inference in Context (LIiC) task, which is closely related to entailment graph learning. <ref type="bibr" target="#b13">Hosseini et al. (2021)</ref> uses pretrained BERT to initialize the con-textualized embeddings in their contextualized link prediction and entailment score calculation. Higher scores are assigned to the entailed predicates in the context of their premises, which is one implicit expression form of DIH and different from our direct utilization of LM on textual entailment.</p><p>3 Our Method: EGT2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Definition and Notations</head><p>The goal of entailment graph learning is to extract predicates, learn the entailment relations and build entailment graphs from raw text corpora. Following previous works <ref type="bibr" target="#b11">(Hosseini et al., 2018</ref><ref type="bibr" target="#b12">(Hosseini et al., , 2019))</ref>, we use the binary relations from neo-Davisonian semantics as predicates, which is a type of first-order logic with event identifiers. For instance, with the semantic parser (here, GraphParser <ref type="bibr" target="#b27">(Reddy et al., 2014)</ref>), the sentence:</p><p>"Griseofulvin is preferred for the infection." can be transformed into the logical form ?e.pref er 2 (e, Griseof ulvin) ?pref er f or (e, inf ection) where e denotes an event. By considering a relation for each pair of extracted arguments, this sentence refers to one predicate, p = (prefer.2,prefer.for.2,medicine,disease)<ref type="foot" target="#foot_0">2</ref> . Likely, the sentence "Griseofulvin cures the infection." contains q = (cure.1,cure.2,medicine,disease). Formally, a predicate with argument types t 1 and t 2 is represented as p = (w p,1 .i p,1 , w p,2 .i p,2 , t 1 , t 2 ). The event-based predicate form is strong enough to describe most of the relations in real cases <ref type="bibr" target="#b23">(Parsons, 1990)</ref>.</p><p>With T as the set of types and P as the set of all typed predicates, V (t 1 , t 2 ) contains typed predicates p with unordered argument types t 1 and t 2 , where p ? P and</p><formula xml:id="formula_0">t 1 , t 2 ? T . For predicate p = (w p,1 .i p,1 , w p,2 .i p,2 , t 1 , t 2 ), we denote that ? 1 (p) = t 1 , ? 2 (p) = t 2 and ?(p) = (w p,1 .i p,1 , w p,2 .i p,2 ). In other words, V (t 1 , t 2 ) = {p|(? 1 (p) = t 1 ?? 2 (p) = t 2 ) ? (? 1 (p) = t 2 ? ? 2 (p) = t 1 )}. A typed entailment graph G(t 1 , t 2 ) =&lt; V (t 1 , t 2 ), E(t 1 , t 2 ) &gt; is composed of the nodes of typed predicates V (t 1 , t 2</formula><p>) and the weighted edges E(t 1 , t 2 ). The edges can be also represented as sparse score matrix scores between predicates with type t 1 and t 2 . As the different argument types can naturally determine whether two predicates have the same order of arguments, the order of argument type is not important while t 1 = t 2 , and therefore we can ensure that G(t 1 , t 2 ) = G(t 2 , t 1 ). For those predicates p with ? 1 (p) = ? 2 (p), the two argument types are labeled with orders, which allows the graph to contain the entailment relations with different argument orders, like (be.1,be.capital.of.2,location 1 ,location 2 ) ? (contain.1,contain.2,location 2 ,location 1 ).</p><formula xml:id="formula_1">W (t 1 , t 2 ) ? [0, 1] |V (t 1 ,t 2 )|?|V (t 1 ,t 2 )| , containing the entailment</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local Entailment based on Textual Entailment</head><p>Inspired by the outstanding performance of pretrained and fine-tuned LMs on RTE task, which is closely related to the entailment graphs, EGT2 uses fine-tuned transformer-based LM to calculate the local entailment scores of typed predicated pairs. In order to utilize the knowledge about entailment relations in pretrained and fine-tuned LM, EGT2 firstly transfers the predicate pair (p, q) into corresponding sentence pair (S(p), S(q)) by sentence generator S, as the complicated predicates cannot be directly input into the LM. For typed predicate p = (w p,1 .i p,1 , w p,2 .i p,2 , t 1 , t 2 ), the generator deduces the positions of arguments about the predicate based on i p,1 and i p,2 , generates the surface form of p based on w p,1 and w p,2 , and finally concatenates the surface form with capitalized types as its arguments. Some generated examples are shown in Table <ref type="table" target="#tab_1">1</ref>, and the detailed algorithm of S is described in Appendix A.</p><p>After generating sentence pair (S(p), S(q)) for predicate pair (p, q), EGT2 inputs (S(p), S(q)) into a transformer-based LM to calculate the probability of the entailment relation p ? q as the local entailment score in G(t 1 , t 2 ). In our experiments, the LM is implemented as DeBERTa <ref type="bibr" target="#b9">(He et al., 2020)</ref>. Generally, an entailment-oriented LM will output three scores for a sentence pair, representing the probability of relationship entail, contra-dict and neutral respectively. Formally, we denote the weighted matrix of local entailment graph with type t 1 and t 2 as W local , and the weight of the edge between p and q in W local is calculated as: ,q)   r?{entail,contradict,neutral} e LM (r|p,q) , (1) where LM (r|p, q) is the output score of corresponding relationship by the LM. As the local entailment is based on the LM fine-tuned to perform textual entailment, the local graph can be built for any predicates in the parsed semantic form, or in any other forms by changing sentence generator S.</p><formula xml:id="formula_2">W local p,q = P (p ? q) ? [0, 1], P (p ? q) = e LM (entail|p</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Global Entailment with Soft Transitivity Constraint</head><p>Existing approaches use global learning to find correct entailment relations which are missing or underestimated in local entailment graphs to overcome the data sparsity. Following <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref>, the evidence from existing local edges with high confidence is used by EGT2 to predict missing edges in the entailment graphs.</p><p>The transitivity in entailment relation inference implies a ? c while both a ? b and b ? c hold. For instance, in the example of Figure <ref type="figure" target="#fig_0">1</ref>, the entailment "is preferred for" ? "is effective for" is discovered because "is preferred for" ? "cures" and "cures" ? "is effective for" have been learned.</p><p>The key challenge to incorporate the transitivity constraint into weighted graphs is discreteness of logical rules. Discreteness makes the rules impossible to be directly used in gradient-based learning methods without NP-hard complexity, as different predicate pairs are jointly involved in the calculation. To unify the discrete logical rules with gradient-based learning, inspired by <ref type="bibr" target="#b17">Li et al. (2019)</ref>, EGT2 uses the logical constraints in the form of differentiable triangular norms <ref type="bibr" target="#b8">(Gupta and Qi, 1991;</ref><ref type="bibr" target="#b15">Klement et al., 2013)</ref>, or called t-norms, as the</p><formula xml:id="formula_3">L 1 = -log a,b,c?V (t 1 ,t 2 ), W a,b ,W b,c &gt;1- min(1, W a,c W a,b W b,c ) = a,b,c?V (t 1 ,t 2 ) I 1-(W a,b )I 1-(W b,c )ReLU (logW a,b + logW b,c -logW a,c ) L 2 = a,b,c?V (t 1 ,t 2 ) -I 1-(W a,b )I 1-(W b,c )I 0 (W a,b W b,c -W a,c )logW a,c L 3 = a,b,c?V (t 1 ,t 2 ) -I 1-(W a,b )I 1-(W b,c )I 0 (W a,b W b,c -W a,c )W a,b W b,c logW a,c<label>(2)</label></formula><p>soft constraints so that the gradient-based learning methods can be applied. Different t-norm methods transfer the discrete rules into different continuous loss functions. Traditional product t-norm maps P (A ? B) into P (A)P (B), P (A ? B) into P (A) + P (B) -P (A)P (B), and P (A ? B) into min(1, P (B) P (A) ). For the entailment relations, the probability of transitivity to be satisfied is:</p><formula xml:id="formula_4">P [(a ? b ? b ? c) ? (a ? c)] = min(1, W a,c W a,b W b,c ),<label>(3)</label></formula><p>where the probability of the entailment relation a ? b is represented by the local entailment scores W a,b . To alleviate the noise from those edges assigned low confidence by local LM, EGT2 only takes the local edges whose scores are higher than 1into account (as a ? b and b ? c), where is a small hyper-parameter because the local probability scores tend to be close to 0 or 1 in practice. Therefore, to maximize the probability of transitivity constraint satisfied over all predicates in the entailment graph G(t 1 , t 2 ), EGT2 tries to minimize the following minus-log-likelihood loss function L 1 in Eq. 2, where I y (x) = 1 if x &gt; y, or 0 otherwise. Another important t-norm, called the G?del tnorm, maps P (A ? B) into 1 if P (B) ? P (A) or P (B) otherwise. Therefore, the G?del probability of transitivity to be satisfied is:</p><formula xml:id="formula_5">P [(a ? b ? b ? c) ? (a ? c)] = W a,c W a,b W b,c &gt; W a,c 1 otherwise ,<label>(4)</label></formula><p>and EGT2 similarly tries to minimize the loss function L 2 in Eq. 2. It should be noted that transitivity constraints will be disobeyed not only by the missing edges, but also by the spurious edges in the local graphs. Therefore, we expect the soft constraints to take reducing the weights of premise edges into consideration. L 1 achieves this by the loss item W a,b and W b,c , and we modify L 2 to L 3 in Eq. 2 so that the low confidence of W a,c will help to detect whether W a,b and W b,c are spurious. Our t-norm soft constraints, although do not guarantee the obedience of transitivity, are effective approximations for the transitivity property.</p><p>Given the local entailment graph G(t 1 , t 2 ) with weighted edges W local , in order to ensure that the global entailment graph W is not too far from W local , EGT2 finally minimizes the following loss function L to trade off the distance from local graphs and the soft transitivity constraint:</p><formula xml:id="formula_6">L = a,b?V (W a,b -W local a,b ) 2 + ?L i , i = 1, 2, 3</formula><p>(5) where L i is the specified implementation of soft transitivity constraint in Eq. 2, and ? is a nonnegative hyper-parameter that controls the influence of two loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Predicate Extraction</head><p>Following <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> and <ref type="bibr" target="#b12">Hosseini et al. (2019)</ref>, we use the multiple-source NewsSpike corpus <ref type="bibr" target="#b35">(Zhang and Weld, 2013)</ref>, which contains 550K news articles, to extract binary relations as generated predicates in EGT2. We make use of the triples released and filtered in <ref type="bibr" target="#b12">Hosseini et al. (2019)</ref>, which applies GraphParser <ref type="bibr" target="#b27">(Reddy et al., 2014)</ref> based on Combinatorial Categorial Grammar (CCG) syntactic derivations to extracting binary relations between predicates and arguments. The argument entities are linked to Freebase <ref type="bibr" target="#b3">(Bollacker et al., 2008)</ref> and mapped to the first level of FIGER types <ref type="bibr" target="#b20">(Ling and Weld, 2012)</ref> hierarchy. The type of a predicate is determined by its two corresponding argument entities. The triples are filtered by two rules to remove the noisy binary relations and arguments: (1) we only keep those argument-pairs appearing in at least 3 relations; (2) we only keep those relations with at least 3 different argumentpairs. The number of relations in the corpus is reduced from 26M to 3.9M, covering 304K typed predicates in 355 typed entailment graphs. Only those predicate pairs co-occurring with at least one same entity-pair (e.g., Griseofulvin-infection) will be linked to calculate the local scores, and as a result, our local predicate pairs are identical with <ref type="bibr" target="#b12">Hosseini et al. (2019)</ref>. As we focus on using global models to alleviate the sparsity of local edges, more potential methods to extracting denser local edges will be studied in our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Datasets and Metrics</head><p>We use Levy/Holt Dataset <ref type="bibr" target="#b16">(Levy and Dagan, 2016;</ref><ref type="bibr" target="#b10">Holt, 2018)</ref> and Berant Dataset <ref type="bibr" target="#b2">(Berant et al., 2011)</ref> to evaluate the performance of entailment graph models.</p><p>In Levy's dataset, each example contains a pair of triples with the same entities but different predicates. Some questions with one predicate were shown to the annotating workers, like "Which medicine cures the infection?". The label for each example are either True or False, indicating whether the first typed predicate entails the second one, by asking the workers whether the first predicates can answer the question with the second one. For example, if "Griseofulvin is preferred for the infection" is a correct answer of the above question, the dataset labels "is preferred for" ? "cures". Holt (2018) re-annotates Levy's dataset and forms a new dataset with 18,407 examples (3,916 positive and 14,491 negative), referred as Levy/Holt Dataset. The dataset is split into validation set (30%) and test set (70%) as <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> in our experiments. <ref type="bibr" target="#b2">Berant et al. (2011)</ref> annotates all the entailment relations in their corpus, which generates 3,427 positive and 35,585 negative examples, referred as Berant Dataset. Their entity types do not exactly match with the first level of FIGER types hierarchy, and therefore a simple hand-mapping by <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> is used to unify the predicate types.</p><p>To be comparable with previous works, we evaluate our methods on the test set of Levy/Holt Dataset and the whole Berant Dataset by calcu-lating the area under the curves (AUC) with changing the classification threshold of global entailment scores. <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> argues that the AUC of Precision-Recall Curve (PRC) for precisions in the range [0.5, 1], as predictions with higher precision than random are more important for the downstream applications. Therefore, we report both the AUC of PRC for precisions in the range [0.5, 1] and the traditional AUC of ROC, which is more widely used in evaluation of other tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison Methods</head><p>We compare our model with existing entailment graph construction methods <ref type="bibr" target="#b2">(Berant et al., 2011;</ref><ref type="bibr" target="#b11">Hosseini et al., 2018</ref><ref type="bibr" target="#b12">Hosseini et al., , 2019</ref><ref type="bibr" target="#b13">Hosseini et al., , 2021) )</ref> and the best local distributional method, Balanced Inclusion <ref type="bibr" target="#b30">(Szpektor and Dagan, 2008)</ref>, referred as BInc. We also include ablation variants of our EGT2, including local models with or without fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>For local transformer-based LM, EGT2 uses De-BERTa <ref type="bibr" target="#b9">(He et al., 2020)</ref> implemented by the Hugging Face transformers library <ref type="bibr" target="#b33">(Wolf et al., 2019)</ref> <ref type="foot" target="#foot_1">3</ref> , which has been fine-tuned on MNLI <ref type="bibr" target="#b32">(Williams et al., 2018)</ref> dataset. In order to adapt it to the special type-oriented sentence pattern generated by S, we expand the validation set by extracting all of the predicates, generating sentence pairs by generator S for every two predicates, and checking whether they are labeled as paraphrase or entailment in the Paraphrase Database collection (PPDB) <ref type="bibr" target="#b25">(Pavlick et al., 2015)</ref>. We split 80% of the generated corpus to fine-tune the DeBERTa with Cross-Entropy Loss, and the rest as the validation set of fine-tuning process. The fine-tuning learning rate ? f = 10 -5 , and the process is terminated while the F 1 score of entail on validation set does not increase in 10 epochs or training after 100 epochs.</p><p>For global soft transitivity constrains, we use SGD <ref type="bibr" target="#b5">(Cun et al., 1998)</ref> to optimize the scores W in entailment graphs with loss function L in Eq. 5 for e = 5 epochs. The SGD learning rate ? = 0.05, the coefficient ? = 1, and the confidence threshold = 0.02. The hyper-parameters are selected based on Levy/Holt validation dataset. More implementation details are given in Appendix B.</p><p>For testing, if one or both predicates of the example do not appear in the corresponding typed entailment graph, we handle the example as un- 5 Experiment Results and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>We summarize the model performances on both Levy/Holt and Berant datasets in Table <ref type="table" target="#tab_2">2</ref>. All global methods, including <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref>, <ref type="bibr" target="#b12">Hosseini et al. (2019)</ref> and EGT2, perform better than their corresponding local methods, which demonstrates the effect of global constraints in alleviating the data sparsity. Although using the same extracted entailment relations with <ref type="bibr" target="#b12">Hosseini et al. (2019)</ref>, our EGT2-Local significantly outperforms previous local methods because of the highquality entailment scores generated by reliable finetuned textual entailment LM. On the whole, EGT2 with transitivity constraint L 3 outperforms all the other models on both Levy/Holt Dataset and Berant Dataset with AUC of PRC, while EGT2-L 1 performs best with AUC of ROC. All of three soft transitivity constraints boost the performance of local model on all evaluation metrics, which shows that making use of transitivity rule between entailment relations improves the local entailment graph. EGT2-L 1 or EGT2-L 3 performs better than EGT2-L 2 , which indicates that involving the premises a ? b and b ? c into loss function is also important for using transitivity constraints. The Precision-Recall Curves of different meth-ods and the Precision-Recall Point of <ref type="bibr" target="#b2">Berant et al. (2011)</ref> on the two evaluation datasets are shown in Figure <ref type="figure" target="#fig_1">2</ref>(a) and 2(b) respectively. The local and global models of EGT2 consistently outperform previous state-of-the-art methods on all levels of precision and recall, which indicates the effect of our local model based on textual entailment and global soft constraints based on transitivity. The EGT2-Local achieves slightly higher precision than global models in the range recall &lt; 0.5, but its precision drops quickly if we require higher recall and therefore leads to worse performance than global models. The result indicates that global models with transitivity constraints gain significant improvement on recall with far less expense on precision than EGT2-Local.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How the local model fine-tuning works?</head><p>As described in Section 4.4, a new corpus is generated for fine-tuning the local model. We claim that the fine-tuning corpus helps to improve the performance of EGT2-Local by adapting it to the special sentence pattern by S, rather than offering additional data to fit the distribution of target datasets as traditional training datasets do. To prove this, we also test a simple supervised method, labelled as Local-Sup, which fits a 2-layers feedforward neural network on the fine-tuning corpus with cosine similarity, Weed, Lin and BInc scores as features. If the corpus acts as training dataset, the performance of Local-Sup should be obviously better than its unsupervised features.</p><p>As shown in Table <ref type="table" target="#tab_2">2</ref>, Local-Sup does not perform significantly better on Levy/Holt Dataset, and even worse on Berant Dataset than BInc, which is one of the inputting features of Local-Sup. The result illustrates the difference between the finetuning corpus and the evaluation datasets, and shows that the corpus plays a role as pattern adapting corpus rather than training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Why are global constraints helpful?</head><p>In Section 1, we expect that the improvement of soft transitivity constraints is attributed to the alleviation of data sparsity in corpus. To examine the sparsity before and after the applying of transitivity constraints, we count how many the positive and negative entailment relations in the Levy/Holt test set exactly appear in the local and global entailment graph respectively, and show the counting results in Table <ref type="table" target="#tab_3">3</ref>. All three soft transitivity constraints help to find more entailment relations than   We report the model performance on the directional section of Levy/Hold Dataset in Table <ref type="table" target="#tab_4">4</ref>. We can see that previous baselines do not perform well on AUC of PRC, which indicate that it is difficult for them to reach precision &gt; 0.5. Meanwhile, EGT2-Local and EGT2-L 3 outperform all baselines on the directional section of Levy/Holt Dataset. Unsurprisingly, all models' AUC scores on the directional section become lower compared on the original Levy/Holt Dataset, showing the challenges of directional entailment inference. Two EGT2 variants maintain high performance, which proves that our local model can learn to capture directional predicate entailment better than distributional baselines, and the global soft constraint also helps to make directional entailment inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Error Analysis</head><p>We randomly sample and analyze 100 false positive (FP) examples and 100 false negative (FN) examples from Levy/Holt test set according to predictions by EGT2-L 3 . We manually setup the decision threshold as 0.574 to make the precision level close to 0.76, which is the same as <ref type="bibr" target="#b2">Berant et al. (2011)</ref>. The major error types are shown in Table <ref type="table" target="#tab_5">5</ref>. Although the global constraint is used, about half of FN errors are due to the data sparsity where the entailment relations are not found in the entailment graph. When compared with the results in <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref>, EGT2-L 3 reduces the ratio of Sparsity in FN errors from 93% to 46% with stronger alleviation ability of data sparsity. About a quarter of FN are caused by the Under-weighted Relations in the graph, where EGT2 finds the entailment relations but gives them scores lower than the threshold. The rest of FN are related to Dataset Wrong Labels which happens when the predicates are indeed entailed by others but labelled as negative, or the predicate pairs are incomplete.</p><p>Most of FP errors are caused by the Spurious Correlation as these relations are too fraudulent for EGT2 to see through their spurious relationships and consequently given high scores. A few FP errors are caused by Lemma-based Processing in LM inevitably, but the ratio still reduces from 12% in <ref type="bibr" target="#b11">Hosseini et al. (2018)</ref> to 5%. The result indicates that our fine-tuned LM can handle the predicates even with similar surface forms and contexts better than parsing-based distributional local features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a novel typed entailment graph learning framework, EGT2, which uses language models fine-tuned on textual entailment tasks to calculate local entailment scores and applies soft transitivity constraints to learn global entailment graphs in gradient-based method. The transitivity constraints are achieved by carefully designed loss functions, and effectively boost the quality of local entailment graphs. By using the fine-tuned local LM and global soft constraints, EGT2 does not rely on distributional features, and can be easily applied to large-scale graphs. Experiments on standard benchmark datasets show that EGT2 achieves significantly better performance than existing state-of-the-art entailment graph methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A simple example of entailment graph with types medicine and disease. The dashed line represents a missing entailment recovered by considering the transitivity constraint (red) based on the two premise entailment between three boldfaced predicates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The Precision-Recall Curves of different methods on (a) Levy/Holt Dataset and (b) Berant Dataset. The result of Berant et al. (2011) is shown as a point, as they generate entailment graphs without weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>on a fur. ? The cat has a fur. (0.683) made of metal. ? The lamps are made of metal. (1.0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Examples of sentence generator S.</figDesc><table><row><cell>Predicates</cell><cell>Sentences</cell></row><row><cell>(be.1,be.capital.of.2,location 1 ,location 2 )</cell><cell>Location A is capital of Location B.</cell></row><row><cell>(contain.1,contain.2,location 2 ,location 1 )</cell><cell>Location B contains Location A.</cell></row><row><cell>(prefer.2,prefer.for.2,medicine,disease)</cell><cell>Medicine A is preferred for Disease B.</cell></row><row><cell>(give.2,give.3,person,thing)</cell><cell>Person A is given Thing B.</cell></row><row><cell cols="2">(aggrieved.by.2,aggrieved.felt.1,thing,person) Person B feels aggrieved by Thing A.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model performance on Levy/Holt Dataset and Berant Dataset. The best performances on every metric are boldfaced. Results with * are from the original papers.</figDesc><table><row><cell>Methods</cell><cell cols="2">Levy/Holt</cell><cell>Berant</cell><cell></cell></row><row><cell>Metrics</cell><cell cols="4">PRC ROC PRC ROC</cell></row><row><cell>BInc</cell><cell cols="4">.155 .632 .147 .677</cell></row><row><cell>Local-Sup</cell><cell cols="4">.161 .632 .129 .651</cell></row><row><cell>Hosseini18</cell><cell cols="4">.163 .637 .174 .682</cell></row><row><cell>Hosseini19  *</cell><cell>.187</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>-Local</cell><cell cols="4">.167 .639 .118 .378</cell></row><row><cell>Hosseini21  *</cell><cell>.195</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EGT2-Local</cell><cell cols="4">.313 .712 .360 .857</cell></row><row><cell cols="5">-w/o Fine-tuning .234 .673 .147 .732</cell></row><row><cell>EGT2-L 1</cell><cell cols="4">.345 .761 .437 .880</cell></row><row><cell>EGT2-L 2</cell><cell cols="4">.319 .755 .361 .879</cell></row><row><cell>EGT2-L 3</cell><cell cols="4">.356 .755 .443 .871</cell></row><row><cell cols="5">typed one by resorting to its average score among</cell></row><row><cell cols="5">all typed entailment graphs. This setting is also</cell></row><row><cell cols="5">used for all local and global methods in the experi-</cell></row><row><cell cols="2">ments for fair comparison.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The number of testing examples appearing in entailment graphs learnt by corresponding models. and L 3 as shown in Table2. On the other hand, EGT2-L 1 and EGT2-L 3 obtain more proportions of positive examples by considering premise relations during the gradient calculation. The low confidence of hypothesis relationship W a,c should be helpful to detect spurious premises W a,b and W b,c . Therefore, EGT2-L 3 slightly outperforms EGT2-L 1 as the gradients of W a,b and W b,c in L 3 are related to the hypothesis relationship W a,c .We have also applied the soft transitivity constraints on the local graph with BInc and Hosseini et al. (2019), but observed only slightly improvement of performance, as .155 ? .157 and .167 ? .170 for EGT2-L 3 on PRC of Levy/Holt Dataset respectively. Comparing it with the significant improvement based on EGT2-Local, we claim that the high-quality local entailment graphs are the basis of effective soft transitivity constraints.The previous cross-graph soft constraint and paraphrase resolution soft constraint proposed in</figDesc><table><row><cell>Methods</cell><cell cols="2">Positive # Negative #</cell></row><row><cell>EGT2-Local</cell><cell>378</cell><cell>75</cell></row><row><cell>EGT2-L 1</cell><cell>642</cell><cell>174</cell></row><row><cell>EGT2-L 2</cell><cell>783</cell><cell>277</cell></row><row><cell>EGT2-L 3</cell><cell>685</cell><cell>190</cell></row><row><cell cols="3">local entailment graph and therefore achieve better</cell></row><row><cell cols="3">performance on the evaluation datasets. Although</cell></row><row><cell cols="3">EGT2-L 2 finds the most entailment relations in</cell></row><row><cell cols="3">the dataset in global stage, it finds more negative</cell></row><row><cell cols="3">examples concurrently and thus performs worse</cell></row><row><cell>than L 1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Model performance on the directional section of Levy/Holt Dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">PRC ROC</cell></row><row><cell>BInc</cell><cell>.038</cell><cell>.567</cell></row><row><cell>Hosseini18</cell><cell>.038</cell><cell>.564</cell></row><row><cell cols="2">Hosseini19-Local .040</cell><cell>.579</cell></row><row><cell>EGT2-Local</cell><cell>.176</cell><cell>.654</cell></row><row><cell>EGT2-L 3</cell><cell>.171</cell><cell>.696</cell></row><row><cell cols="3">Hosseini et al. (2018) have shown improvement</cell></row><row><cell cols="3">of performance based on their local graphs. How-</cell></row><row><cell cols="3">ever, due to the distinct distribution and scales of</cell></row><row><cell cols="3">local scores, their constraints are computationally</cell></row><row><cell cols="3">unavailable on our local graphs, partially due to the</cell></row><row><cell cols="3">high overhead for cross-graph calculation.</cell></row><row><cell cols="3">5.4 Does EGT2 learn directional entailment?</cell></row><row><cell cols="3">Generally, the logical entailment should be direc-</cell></row><row><cell cols="3">tional which makes it different from paraphrase.</cell></row><row><cell cols="3">Although EGT2 significantly improves the per-</cell></row><row><cell cols="3">formance on two datasets, it is unclear whether</cell></row><row><cell cols="3">the improvement comes from the directional en-</cell></row><row><cell cols="3">tailment cases, or only paraphrasing ones, as the</cell></row><row><cell cols="3">local LM might be strong in recognizing para-</cell></row><row><cell cols="3">phrases but weak in recognizing directional entail-</cell></row><row><cell cols="3">ment (Cabezudo et al., 2020). To examine how</cell></row><row><cell cols="3">EGT2 works under directional cases, we eliminate</cell></row><row><cell cols="3">those paraphrase predicate pairs a ? b with label</cell></row></table><note><p>l ? {T rue, F alse} from Levy/Holt test dataset, if the corresponding b ? a is also appearing and labelled as l in the test dataset. The rest directional section of Levy/Holt Dataset contains 8,140 examples (753 positive and 7,387 negative). We expect that this section should be more challenging as undirectional paraphrase becomes unavailable.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>The major error types of false positive and false negative predictions by EGT2-L 3 in Levy/Holt test set, with predicted scores in the parentheses.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>The numbers after the predicate words are corresponding argument positions of entity "Griseofulvin" (second argument of prefer) and "infection" (second argument of the preposition for), and the later two items are the types of arguments.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/huggingface/transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported in part by <rs type="funder">National Key R&amp;D Program of China</rs> (No. <rs type="grantNumber">2020AAA0106600</rs>) and <rs type="funder">NSFC</rs> (<rs type="grantNumber">62161160339</rs>). We would like to thank the anonymous reviewers and action editors for their helpful comments and suggestions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_B9VmWhD">
					<idno type="grant-number">2020AAA0106600</idno>
				</org>
				<org type="funding" xml:id="_UuynzZk">
					<idno type="grant-number">62161160339</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Implementation Details</head><p>We select the SGD learning rate ? from {0.02, 0.05, 0.1}, the number of training epochs from {2, 3, 5, 7}, the coefficient ? from {0.5, 1, 2}, and the confidence threshold from {0.005, 0.01, 0.02}. We manually tune the hyper-parameters based on the AUC of PRC on Levy/Holt validation dataset, which is .327 corresponding to our settings.</p><p>Under our experiment settings, one training epoch costs about 4 hours on an NVIDIA A40 GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient global learning of entailment graphs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noga</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00220</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="263" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient tree-based approximation for entailment graph learning</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meni</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Korea. Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Global learning of typed entailment rules</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="610" to="619" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2008 ACM SIGMOD international conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural language inference for portuguese using bert and multilingual information</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sobrevilla</forename><surname>Cabezudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcio</forename><surname>In?cio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Processing of the Portuguese Language</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="346" to="356" />
		</imprint>
	</monogr>
	<note>Ana Carolina Rodrigues, Edresson Casanova, and Rog?rio Figueredo de Sousa</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient backprop, neural networks: Tricks of the trade. Lecture notes in computer sciences</title>
		<author>
			<persName><surname>Yl Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><surname>Muller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1524</biblScope>
			<biblScope unit="page" from="5" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName><forename type="first">Maayan</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.3115/1219840.1219854</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)<address><addrLine>Ann Arbor, Michigan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating temporal information in entailment graph mining</title>
		<author>
			<persName><forename type="first">Liane</forename><surname>Guillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Bijl De Vroe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.textgraphs-1.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Graphbased Methods for Natural Language Processing (TextGraphs)</title>
		<meeting>the Graphbased Methods for Natural Language Processing (TextGraphs)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Theory of t-norms and fuzzy inference methods. Fuzzy sets and systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Madan</surname></persName>
		</author>
		<author>
			<persName><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="431" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654</idno>
		<title level="m">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Holt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12048</idno>
		<title level="m">Probabilistic models of relational implication</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning typed entailment graphs with global soft constraints</title>
		<author>
			<persName><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><forename type="middle">R</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00250</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="703" to="717" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Duality of link prediction and entailment graph induction</title>
		<author>
			<persName><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1468</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4736" to="4746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open-domain contextual link prediction and its complementarity with entailment graphs</title>
		<author>
			<persName><forename type="first">Mohammad Javad</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2790" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Triangular norms</title>
		<author>
			<persName><forename type="first">Erich</forename><surname>Peter Klement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radko</forename><surname>Mesiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Endre</forename><surname>Pap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Annotating relation inference in context via question answering</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-2041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="249" to="255" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A logic-driven framework for consistency of neural models</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maitrey</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><surname>Vivek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">Srikumar. 2019</date>
			<biblScope unit="page" from="3924" to="3935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic retrieval and clustering of similar words</title>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.3115/980691.980696</idno>
	</analytic>
	<monogr>
		<title level="m">36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="768" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovery of inference rules for question-answering</title>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="343" to="360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Mohammad Javad Hosseini, Sander Bijl de Vroe, Mark Johnson, and Mark Steedman. 2021. Multivalent entailment graphs for question answering</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Guillou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07846</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Measuring machine translation quality as semantic equivalence: A metric based on entailment features</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Translation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="193" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Events in the semantics of english: A study in subatomic semantics</title>
		<author>
			<persName><forename type="first">Terence</forename><surname>Parsons</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scientific text entailment and a textual-entailment-based framework for cooking domain question answering</title>
		<author>
			<persName><forename type="first">Amarnath</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyanka</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">S?dhan?</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PPDB 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification</title>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpendre</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-2070</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale semantic parsing without questionanswer pairs</title>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00190</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="377" to="392" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03695</idno>
		<title level="m">Continuous entailment patterns for lexical inference in context</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models for lexical inference in context</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.108</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1267" to="1280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning entailment rules for unary templates</title>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)</title>
		<meeting>the 22nd International Conference on Computational Linguistics (Coling 2008)<address><addrLine>Manchester, UK. Coling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="849" to="856" />
		</imprint>
	</monogr>
	<note>Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A general framework for distributional similarity</title>
		<author>
			<persName><forename type="first">Julie</forename><surname>Weeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Weir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining axiom injection and knowledge base completion for efficient natural language inference</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koji</forename><surname>Mineshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Noji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisuke</forename><surname>Bekki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7410" to="7417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Harvesting parallel news streams to generate paraphrases of event relations</title>
		<author>
			<persName><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1776" to="1786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A Algorithm for Sentence Generator Algorithm 1 The sentence generator S</title>
		<imprint/>
	</monogr>
	<note>Require: p = (wp,1.ip,1, wp,2.ip,2, t1, t2): a typed predicate</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">if Order of (t1,t2) is equal to graph types then 2: Actor1 = concat(t1</title>
		<imprint>
			<publisher>Ensure: Sentence S</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>3: Actor2 = concat(t2,&quot;B&quot;) 4: else 5: Actor1 = concat(t1,&quot;B&quot;) 6: Actor2 = concat(t2,&quot;A&quot;) 7: end if 8: if The first word of wp,1 or wp,2 is not a verb then 9: wp,1 = concat(&quot;is&quot;,wp,1) 10: wp,2 = concat(&quot;is&quot;,wp,2) 11: end if 12: Active1=Boolean(ip,1 = 1) 13: Active2=Boolean(ip,2 = 1) 14: MinLen=min(Length(wp,1),Length(wp,2)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Actor2,wp,1[1: MinLen]) 20: end if 21: return concat(Actor1,&quot;and&quot;,Actor2,wp,1[1]) 22: end if 23: if Active1 and not Active2 then 24: if Pathway then 25: Act=wp,1 26: if Length(wp,1)&lt;MinLen then 27: Act=wp,2 28: end if 29: return concat(Actor1,Act,Actor2) 30: end if 31: return concat(Actor1,wp,1</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Mml=max I</surname></persName>
		</author>
		<author>
			<persName><surname>Wp</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Reverse(wp,2</pubPlace>
		</imprint>
	</monogr>
	<note>32: end if 33: if Active2 and not Active1 then 34: if The first words of wp,1 is verb then 35: return concat. Reverse( wp,2[MML:]),&quot;to&quot;,wp,1,Actor2) 36: end if 37: return concat. wp,1[MML:],Actor2) 38: end if 39: if Pathway then 40: return concat. Passive(wp,1), wp,2[MML:],Actor2) 41: end if 42: return concat(&quot;Something&quot;,wp,1,Actor1, wp,2[MML:],Actor2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
