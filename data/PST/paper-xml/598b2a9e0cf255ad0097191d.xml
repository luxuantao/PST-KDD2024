<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tingfang</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andrei</forename><surname>Pȃun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Linqiang</forename><surname>Pan</surname></persName>
							<email>lqpan@mail.hust.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Spiking Neural P Systems With Polarizations</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Automation</orgName>
								<orgName type="laboratory">Key Laboratory of Image Informa-tion Processing and Intelligent Control of Education Ministry of China</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">ICUB/Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<postCode>010014</postCode>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Automa-tion</orgName>
								<orgName type="laboratory">Key Laboratory of Image Information Processing and Intelligent Control of Education Ministry of China</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
								<address>
									<postCode>430074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Zhengzhou University of Light Industry</orgName>
								<address>
									<postCode>450002</postCode>
									<settlement>Zhengzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Tech-nology</orgName>
								<orgName type="institution">Huazhong University of Science</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Rovira</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Virgili University</orgName>
								<address>
									<settlement>Tarragona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">Vienna University of Technology</orgName>
								<address>
									<postCode>2003</postCode>
									<settlement>Vienna</settlement>
									<region>in</region>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Louisiana Tech University</orgName>
								<address>
									<settlement>Ruston</settlement>
									<region>LA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution" key="instit1">University of Bucharest</orgName>
								<orgName type="institution" key="instit2">Universi-tad Politécnica de Madrid</orgName>
								<address>
									<settlement>Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E660C349F7291F565AD9D9C96DD62FA9</idno>
					<idno type="DOI">10.1109/TNNLS.2017.2726119</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Bio-inspired computing</term>
					<term>membrane computing</term>
					<term>polarization</term>
					<term>spiking neural P (SN P) system</term>
					<term>Turing completeness</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spiking neural P (SN P) systems are a class of parallel computation models inspired by neurons, where the firing condition of a neuron is described by a regular expression associated with spiking rules. However, it is NP-complete to decide whether the number of spikes is in the length set of the language associated with the regular expression. In this paper, in order to avoid using regular expressions, two major and rather natural modifications in their form and functioning are proposed: the spiking rules no longer check the number of spikes in a neuron, but, in exchange, a polarization is associated with neurons and rules, one of the three electrical charges -, 0, +. Surprisingly enough, the computing devices obtained are still computationally complete, which are able to compute all Turing computable sets of natural numbers. On this basis, the number of neurons in a universal SN P system with polarizations is estimated. Several research directions are mentioned at the end of this paper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>in <ref type="bibr" target="#b12">[13]</ref>, which abstracts computing ideas from the structure and the functioning of living cells, e.g., data structures, operations with data, ways to control operations, and computing models.</p><p>The distributed and parallel computing models in membrane computing are usually called P systems. By the structure of P systems, the systems have three main categories: cell-like P systems <ref type="bibr" target="#b12">[13]</ref> (membranes are hierarchically arranged), tissue-like P systems <ref type="bibr" target="#b13">[14]</ref> (cells are arranged in a undirected graph), and neural-like P systems <ref type="bibr" target="#b14">[15]</ref> (cells are arranged in a directed graph). P systems have been extensively investigated with both mathematical and theoretical computer science motivations <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>. P systems have also wide applications, ranging from biology oriented <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> to technology oriented applications <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. For a comprehensive survey (at the level of the end of the year 2009), the reader is referred to <ref type="bibr" target="#b23">[24]</ref>. The up-to-date information of membrane computing is available at the website http://ppage.psystems.eu.</p><p>Spiking neural P (SN P) systems, as a special class of neural-like P systems, were introduced in <ref type="bibr" target="#b24">[25]</ref>, which are inspired from neural cells, more precisely, from the fact that neurons communicate with each other by means of spikes. In such systems, both spatial and temporal information of incoming spikes are used to encode their message, where the number and timing of individual spikes matter and neurons process the information by rules, which are associated with a specific application context (i.e., regular expression) and these rules can be considered as special functions. In this sense, SN P systems can be considered as a member of the third generation of neural network models <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>.</p><p>The basic structure of an SN P system is a directed graph, whose arcs represent synapses and nodes represent neurons (i.e., computing units) containing a unique object, denoted by a, the spike, in a certain number of copies. Neurons fire by spiking rules. Standard spiking rules are of the form E/a c → a; d, where E is a regular expression over the symbol a and c ≥ 1 and d ≥ 0 are the natural numbers. The application of such a rule means that c spikes are "consumed," one spike is "produced" after a delay of d steps, and this spike is sent to all neighboring neurons linked by synapses with the neuron where the rule resides. Standard forgetting rules are also considered, which are of the form a s → λ. The application of a forgetting rule means that all spikes are removed if the neuron contains exactly s spikes. SN P systems with extended rules were introduced in <ref type="bibr" target="#b28">[29]</ref>. Extended rules are of the general form E/a c → a p ; d, which are a generalization of both standard spiking and forgetting rules since p can be 0 or greater than 0. An important point: a rule is applied in a neuron only if the following condition is satisfied: the number of spikes contained in the neuron is in the length set of languages described by the regular expression E (formally, the neuron contains k spikes, such that a k ∈ L(E), where L(E) is the regular language identified by E).</p><p>Inspired by biological features or computer science and mathematical motivations, some variants of SN P systems and different working modes are proposed. There are variants of SN P systems such as SN P systems with astrocytes <ref type="bibr" target="#b29">[30]</ref>- <ref type="bibr" target="#b31">[32]</ref>, SN P systems with anti-spikes <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, SN P systems with thresholds <ref type="bibr" target="#b34">[35]</ref>, SN P systems with weights <ref type="bibr" target="#b35">[36]</ref>, SN P systems with rules on synapses <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and cell-like SN P systems <ref type="bibr" target="#b38">[39]</ref>. The working modes include asynchronous mode <ref type="bibr" target="#b39">[40]</ref>, asynchronous mode with local synchronization <ref type="bibr" target="#b40">[41]</ref>, sequential mode <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, exhaustive mode <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, and so on. SN P systems can be used as number accepting or generating devices <ref type="bibr" target="#b24">[25]</ref>, language generators <ref type="bibr" target="#b45">[46]</ref>, and function computing devices <ref type="bibr" target="#b46">[47]</ref>. Most variants of SN P systems are proved to be computationally complete (equivalent to Turing machines), working in different modes and used as different computing devices. SN P systems were used to solve computationally hard problems such as SAT problem <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>.</p><p>In the classical SN P systems <ref type="bibr" target="#b24">[25]</ref> and the variants mentioned above, a powerful ingredient is a regular expression associated with each spiking rule, as it provides information about the contents of the neuron, irrespective of how large is the number of spikes present in the neuron; moreover, this information is obtained "in no time," which is not completely "fair" from the complexity point of view. Actually, it was shown that it is NP-complete to decide whether a rule can be applied <ref type="bibr" target="#b49">[50]</ref>. Thus, it is a rather natural idea to avoid using regular expressions. The study of SN P systems with spiking rules without the regular expressions (we can call them, more suggestively, SN P systems with free rules) remains to be carried out.</p><p>In the nervous system, every neuron maintains a resting membrane potential (an electrical voltage difference across the membrane), whose typical value is -70 mV. So, a neuron has a negative charge. A neuron has a threshold potential whose typical value is -55 mV. If a neuron has the membrane potential reaching or exceeding its threshold potential, then the neuron fires and sends out an action potential (electrical signals), its typical value is +40 mV and the neuron has a positive charge, which travels rapidly along the neuron's axon and is transferred across a synapse to a neighboring neuron. For more details about spiking neuron models, the reader is referred to <ref type="bibr" target="#b27">[28]</ref>. Motivated by the biological phenomena that each neuron has a charge either positive or negative, in this paper, we consider a new mechanism for controlling the application of rules using three kinds of charges (i.e., positive, neutral, and negative charges), instead of using regular expressions as in the original definition of SN P systems.</p><p>In this paper, the new mechanism is first introduced to avoid using the regular expressions. A rule of the form a c → a p is used in a neuron as a usual multiset rewriting rule in general P systems: it is only necessary that the neuron contains at least c spikes, without any other information about the number of spikes. To this aim, polarizations are associated with neurons: initially, each neuron has one of the three electrical charges -, 0, +, while each rule, spiking or forgetting rule, is of the form α/a c → a p ; β, where α and β are the electrical charges. The rule α/a c → a p is applied only if the neuron has the charge α and at least c spikes inside, then not only p spikes but also the charge β is transmitted to the neighboring neurons. Then, the charges received by a neuron, together with the current charge of the neuron, compute in a natural way, as the next charge of the neuron. Note that in the proofs of the results about the universality obtained in this paper, the feature of delay is not used, as well as the important detail that only standard spiking rules with p = 1 and forgetting rules with p = 0 are used. In this way, we add an apparently weak control on the application of rules using the three electrical charges, which also resembles the biological idea of spikes as electrical signals sent from a neuron to another one. The computing devices obtained are called SN P systems with polarizations (in short, PSN P systems).</p><p>The computation power of PSN P systems is first investigated. Although PSN P systems seem rather "context-free" at the first sight, they are proved to be computationally complete. Besides this computational completeness result, we also consider the issue of constructing a small universal PSN P system. Based on one of the universal register machines from <ref type="bibr" target="#b50">[51]</ref>, a universal PSN P system with 164 neurons is obtained.</p><p>The main contributions of this paper are summarized as follows.</p><p>1) A new mechanism is introduced for controlling the applicability of rules using electrical charges, -, 0, + associated with neurons, instead of using the regular expressions, in order to avoid the NP-complete problem of deciding the applicability of rules by checking against the regular expressions. 2) We demonstrate that PSN P systems are Turing universal, where extended rules and the feature of delay are not used. Furthermore, the number of charges used to control the application of rules is three, which is finite; however, in standard SN P systems, if the languages defined by the regular expressions are finite, the systems are proved to compute only semilinear sets of numbers <ref type="bibr" target="#b24">[25]</ref>. So, the feature of charges provides a powerful "programming capability" for achieving a desired computation power. 3) We compared PSN P systems and the two existing computation models, recurrent neural networks and SN P systems, in terms of "activation functions" contained in the computing units and the lower bound on the number of computing units to construct Turing universal computing devices for computing functions. The comparison results illustrate that "activation functions" contained in the computing units of PSN P systems are more natural and much simpler than those of SN P systems and recurrent neural networks; moreover, the number of computing units obtained for achieving universality of PSN P systems is much smaller than that of recurrent neural networks, yet is larger than that of SN P systems, but this is at the expense of using the regular expressions in SN P systems. These results of comparison illustrate that the rules by which PSN P systems evolve are more suitable for achieving Turing universality, which suggests that PSN P systems are promising in possible applications. As this is the first step in investigating SN P systems with polarizations, there are many research directions deserved to be investigated. Some open problems are formulated in the last section of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SPIKING NEURAL P SYSTEMS WITH POLARIZATIONS</head><p>The formal definition of PSN P systems is given in <ref type="bibr">[52, pp. 297-298</ref>], here we will not recall this definition. In this paper, the result of a computation of a PSN P system is defined as the first two time instances t 1 and t 2 that the output neuron out spikes, that is, the number t 2t 1 .</p><p>The set of all numbers computed in this way by a PSN P system is denoted by N 2 ( ). The family of all the sets of numbers generated by PSN P systems with at most p charges is denoted by N 2 P S N P(ch p ).</p><p>Some basic notions of formal language theory are used in this paper, including the regular expressions, the reader is advised to consult the bibliography (see <ref type="bibr" target="#b52">[53]</ref>).</p><p>In this section, we provide an example to illustrate the definition of PSN P systems, also clarifying the functioning of the system. In what follows, for an easier understanding, PSN P systems are represented graphically as usual in SN P systems area; each neuron is represented by an ellipsis and identified by a couple (l, α), where l is a label and α is the current electrical charge. Furthermore, in the context, if there is no confusion caused, we write "neuron l" instead of "neuron σ l ."</p><p>Consider the PSN P system shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which consists of seven neurons, with the number of initial spikes, the initial charges, and the rules indicated for each of them.</p><p>Initially, neuron 1 has the neutral charge and one spike, and neuron out has the positive charge and one spike. With the initial positive charge and one spike, neuron out fires using the rule +/a → a; 0, sending a spike to the environment, as well as to neurons 1 and 6. In the next step, neuron 6 fires by the rule 0/a → λ; -, sending the negative charge to neuron out, thus neuron out becomes neutral. Simultaneously, because neuron 1 accumulates two spikes, the rule 0/a 2 → a; 0 is activated, sending one spike to each of neurons 2 and 3 together with a neutral charge.</p><p>For neuron 2, with the neutral charge and one spike inside, both rules 0/a → a; 0 and 0/a → a;are enabled one of them will be nondeterministically chosen to be applied. If the rule 0/a → a; 0 is applied, then neurons 2 and 3 fire, each one sending one spike with a neutral charge to neurons out, 1, 4, 5, respectively. In this way, neuron out accumulates two spikes and has the neutral charge, thus it fires by the rule 0/a 2 → λ; 0, removing the two spikes; at the same time, neuron 1 fires again by the rule 0/a 2 → a; 0. Meanwhile, neurons 4 and 5 fire using the rule 0/a 2 → a; 0, each one sending one spike with a neutral charge to neuron out. Neuron out also fires by the rule 0/a 2 → λ; 0 to remove the two spikes. The process can be repeated for an even number of steps.</p><p>If the rule 0/a → a;in neuron 2 is applied, then neuron out receives not only two spikes each one from neurons 2 and 3, but also a negative charge from neuron 2, in this way, it becomes negative and fires by the rule -/a 2 → a; 0, sending a spike to the environment, which is the second spike that the system sends to the environment; besides the spike sent out, it also sends a spike to neurons 1 and 6, respectively. In turn, neuron 1 receives two spikes as well as a negative charge, then it forgets these two spikes by the rule -/a 2 → λ; 0, not sending spikes to neurons 2 and 3 any more. At the same step, after receiving two spikes and the negative charge, neurons 4 and 5 can use the rule -/a 2 → λ; 0, removing these two spikes, also not sending spikes to neuron out again. At the next step, neuron 6 fires again by the rule 0/a → λ; -, sending a negative charge to neuron out; with the negative charge and one spike, neuron 1 cannot use any rule. After that, the system halts.</p><p>We can check that the time interval between the first two consecutive spikes sent out by neuron out is exactly 2n + 1 steps, for some n ≥ 1. Therefore, we conclude that</p><formula xml:id="formula_0">N 2 ( ) = {2n + 1 | n ≥ 1}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMPUTATION POWER OF PSN P SYSTEMS</head><p>In this section, we investigate the computation power of PSN P systems. Specifically, we prove that PSN P systems can generate all recursively enumerable sets of numbers, i.e., they characterize N RE (the family of length sets of recursively enumerable languages, that is, those recognized by Turing machines).</p><p>Theorem 1: N 2 P S N P(ch 3 ) = N RE. Proof: It is sufficient to prove the inclusion N RE ⊆ N 2 P S N P(ch 3 ); for the converse inclusion, we can invoke the Turing-Church thesis.</p><p>In order to prove the inclusion N RE ⊆ N 2 P S N P(ch 3 ), we use the characterization of N RE by means of register machines working in the generating mode. For the detail about register machines, the reader is referred to <ref type="bibr" target="#b53">[54]</ref>.</p><p>Let us consider a register machine M = (m, H, l 0 , l h , I ), with the assumption that the output register 1 is never decremented during any computation, the initial instruction of M is an ADD instruction, and in the halting configuration all registers different from register 1 are empty. In what follows, a specific PSN P will be constructed to simulate the register machine M.</p><p>The PSN P system consists of modules of the following three types: ADD module, SUB module, and the FIN module as shown in Figs. <ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_2">3</ref><ref type="figure" target="#fig_3">4</ref>, respectively. The ADD and SUB modules simulate the ADD and SUB instructions of M, respectively; the FIN module is used to output the computation result.</p><p>In general, for register machine M, register 1 is associated with a neuron σ 1 ; for r ≥ 2, each register r is associated with a neuron σ r and ten auxiliary neurons σ r i (i = 1, 2, . . . , 10). If register r stores the value n, then neuron σ r contains n spikes. With each label l i ∈ H of an instruction in M, a neuron σ l i is associated, and some auxiliary neurons σ i j ( j = 1, 2, 3, . . . ) will also be considered (each label l i ∈ H is associated with a unique instruction of M, hence all neurons σ l i , σ i j are precisely associated with a unique instruction of M).</p><p>In the initial configuration, each neuron of has an initial polarization but no spike, except that the neuron associated with the label l 0 contains exactly one spike, indicating that the instruction l 0 in M is to be simulated. During a computation, simulating an instruction l i : (OP(r ), l j , l k ) in M means that neuron σ l i having a neutral charge receives one spike and it is activated; depending on the operation OP ∈ {ADD, SUB}, an operation is performed on σ r . When the simulation ends, either neuron σ l j or σ l k receives a spike and a neutral charge and becomes activated. When neuron σ l h is activated, associated with the halting label l h of M, it means that the computation in M is completely simulated in ; then the output neuron σ out sends two spikes to the environment at an interval of time equal to the value stored in register 1 of M.</p><p>In what follows, we explain how these modules work.</p><p>Module ADD (shown in Fig. <ref type="figure" target="#fig_1">2</ref>): Simulating an ADD instruction l i : (ADD(r ), l j , l k ).</p><p>We assume that the initial instruction with label l 0 is an ADD instruction, and at step t, an ADD instruction l i : (ADD(r ), l j , l k ) has to be simulated, which means that neuron l i has a neutral charge and one spike.</p><p>At step t, neuron l i fires, sending a spike and a positive charge to neuron i 1 . As neuron i 1 was already positive, thus the positive charge of the neuron is not changed, but the spike activates the nondeterministic choice between the two spiking rules +/a → a;and +/a → a; +.</p><p>If neuron i 1 fires using the rule +/a → a;at step t + 1, then neuron i 2 is changed to neutral, while neuron i 3 is still negative. At step t + 2, with negative charge, neuron i 3 forgets the spike received from i 1 using the rule -/a → λ, and this branch will stop executing. Simultaneously, neuron i 2 fires using the rule 0/a → a; 0, sending a spike and a neutral charge to neuron r , simulating the increase in the number stored in register r by one, as well as to neurons i 4 and l j . At step t+3, neuron l j starts the simulation of the instruction l j of the register machine M, while neuron i 4 sends a positive charge back to neuron i 2 , in order to bring it from neutral charge back to the initial positive charge.</p><p>If neuron i 1 fires using the rule +/a → a; + at step t + 1, then at step t + 2, neuron i 2 keeps its positive charge and uses the forgetting rule, finishing the execution on that branch; at the same step, with neutral charge, neuron i 3 fires using the rule 0/a → a; 0, which simulates that the register r is incremented by one, and at the next step, neurons i 5 and l k are activated. At step t + 3, neuron i 5 brings the charge of i 3 to negative charge through its forgetting rule, while neuron l k continues the execution.</p><p>After these steps, all neurons are in their initial configuration with respect to their charges and the number of spikes, the only exception being the neuron r whose spikes are incremented by one.</p><p>Therefore, the ADD module first fires neuron l i and it finally nondeterministically fires one of the neurons l j and l k , which means that the ADD instruction l i : (ADD(r ), l j , l k ) is correctly simulated by the ADD module.</p><p>Module SUB (shown in Fig. <ref type="figure" target="#fig_2">3</ref>): Simulating a SUB instruction l i : (SUB(r ), l j , l k ).</p><p>In the following, we will give the description of the SUB module. First, we would like to point out that there are two types of neurons in Fig. <ref type="figure" target="#fig_2">3</ref>: the neurons that depend on the label l i of the SUB instruction, and they are labeled as i 1 , i 2 , i 3 , i 4 , i 5 , and the neurons that do not depend on the label l i and are in a single copy for each register r that is the subject of subtracting instructions. These latter neurons are labeled with one of the variants r j where j ∈ {1, . . . , 10}. To emphasize the difference, we draw the neurons in different ways: as squares, the neurons associated with each individual SUB instruction of M, and as ellipses the ones associated with the register r . For example, if we would have four SUB instructions acting on register r , we will have four times the neurons i 1 through i 5 , while we will have a single copy of each neuron r 1 through r 10 .</p><p>What is differentiating our construction from the previous constructions in the area of standard SN P systems is the fact that the SUB module of this paper has parts that are shared by various SUB instructions working on the same register. This is necessary due to the fact that various SUB modules working on the same register r could interfere with each other, and this interference should not lead to wrong results of the simulation. The SUB module shown in Fig. <ref type="figure" target="#fig_2">3</ref> simulates a SUB instruction l i : (SUB(r ), l j , l k ). A SUB instruction l i is simulated in system in the following way. Suppose that at step t neuron l i fires and sends a spike and a positive charge to the five neurons: neuron r associated with register r , and the auxiliary neurons associated with the SUB instruction, i 1 , i 2 , i 3 , and i 4 .</p><p>Neurons i 1 and i 2 are used for checking whether the number of spikes in neuron r is 0 (corresponding to the emptiness of register r ), and i 3 and i 4 are finalizing the simulation and resetting the module to the initial state. At step t + 1, neurons i 3 and i 4 have the neutral charge and a spike inside, then no rule can be applied in neurons i 3 and i 4 , and the computation in neurons i 3 and i 4 will continue depending on the result of the computation in neuron r . For neuron r , there are the following two cases.</p><p>Case I: At step t, neuron r has at least one spike (corresponding to the fact that the number stored in register r is nonzero). Then, at step t + 1, neuron r is positively charged and stores at least two spikes (one already inside plus the spike received from neuron l i ), and it fires using the rule +/a 2 → a; +, sending a positive charge as well as a spike to each of neurons r 1 , r 3 , and r 4 . The number of spikes in neuron r is decreased by 2 (one received initially from neuron l i ), which simulates that the value stored in register r is decreased by one, and at the same step, neuron i 1 fires using the rule +/a → λ; -, sending a negative charge to neuron r , which resets neuron r to the neutral charge. The functioning of neuron i 1 is to bring the charge of the neuron r back to neutral charge. Simultaneously, neuron i 2 fires using the rule +/a → a; -, sending out a negative charge and a spike to r 1 . In this way, neuron r 1 is neutral and contains two spikes, at step t + 2, these two spikes are removed by the forgetting rule 0/a 2 → λ; 0, not interfering anymore with the execution of the other rules. Neuron i 2 provides a timer for checking the value of the register r -this will be better understood when we deal with the case of the register r being empty.</p><p>At step t +2, neuron r 3 , which was positive, after receiving a positive charge and one spike from neuron r , is still positive, and it fires using the rule +/a → a; +; at the same time, neuron r 4 , which was negative, after receiving a positive charge, becomes neutral and has a single spike, and it fires using the rule 0/a → a; +.</p><p>At step t + 3, neurons r 7 and r 8 are activated by rules ch/a → λ; + and ch/a → λ; -, respectively,  where ch ∈ {+, 0, -}: their only purpose is to reset the charges of neurons r 3 , r 4 , r 5 , and r 6 to their initial state (in this case neurons r 3 and r 5 do not need to be reset, but in the case when the register is empty they will be necessary). At the same step, neurons r 5 and r 6 are activated, and in neuron r 5 , the rule +/a → a; 0 is executed, while in neuron r 6 , the forgetting rule 0/a → λ;is applied. This signifies that neuron i 3 does not change its charge but a new spike is added to the one received at step t, and i 4 receives only a negative charge from neuron r 6 , resetting it to the original negative charge. At step t + 4, neuron i 4 is negative and the forgetting rule -/a → λ; 0 is applied, removing the spike received from neuron l i , while neuron i 3 is neutral and has two spikes, thus uses the rule 0/a 2 → a; 0, which activates neurons l j and i 5 .</p><p>At step t + 5, the simulation of the instruction l j happens in parallel with the resetting of the charge of i 3 by the forgetting rule 0/a → λ;in neuron i 5 ; neuron i 4 also receives a negative charge from neuron i 5 at this step, but the charge of neuron i 4 is still negative and remains unchanged.</p><p>In order to make the work of the SUB module clearer, in Table <ref type="table" target="#tab_0">I</ref>, we provide a step-by-step description of the simulation for the case of register r having a value greater than 0. (For each neuron, we indicate the label, the charge, and the spikes present inside it.)</p><p>Case II: At step t, neuron r has no spike (corresponding to the fact that the number stored in register r is 0). Then, at step t + 1, neurons i 1 and i 2 can fire again, but neuron r is not able to fire as it does not have enough spikes to use the rule +/a 2 → a; +. Neuron i 1 fires by the forgetting rule sending out a negative charge to neuron r , while neuron i 2 fires sending out a negative charge and a spike to neuron r 1 . In this way, neuron r is set from positive charge to neutral and is not able to spike at the next step, neuron r 1 becomes negative and receives a single spike (for comparison, in the nonzero case it is not changing its charge and receives two spikes), thus making r 1 to fire using the rule -/a → a;at step t + 2, and send a spike and a negative charge to neurons r 2 and r 10 .</p><p>At step t + 3, neuron r 2 fires using the rule -/a → a; +, sending a spike and a positive charge to neurons r 9 and r 1 , thus, neuron r 1 is set to neutral and receives a spike; meanwhile neuron r 10 sends a negative charge and a spike to neuron r , in this way, neuron r becomes negative (it was positive after step t and neutral after step t + 1) and accumulates two spikes.</p><p>At step t + 4, neuron r fires using the rule -/a 2 → a; -, and sends a spike as well as a negative charge to neurons r 1 , r 3 , and r 4 , respectively. Simultaneously, neuron r 9 fires using the rule +/a → λ; +, sending a positive charge to neurons r 1 and r , thus both of neurons r 1 and r become neutral.</p><p>At step t + 5, with the neutral charge and two spikes inside (one from neuron r 2 at step t + 3 and one from neuron r at step t + 4), neuron r 1 fires using the rule 0/a 2 → λ; 0, these two spikes are forgotten, returning neuron r 1 to its initial configuration. Neurons r 3 and r 4 receive a spike as well as the negative charge from neuron r in the previous step, then neuron r 3 becomes neutral while neuron r 4 is still negative, so at step t + 5, the rules 0/a → a;and -/a → a;in them are applied, respectively.</p><p>At step t + 6, neuron r 5 fires by the rule 0/a → λ; -, sending a negative charge to neuron i 3 ; and neuron r 6 fires by the rule -/a → a; 0, sending out a spike and the neutral charge to neuron i 4 . At the same time, neuron r 7 resets the charges of neurons r 3 and r 5 to their initial state (in this case neurons r 4 and r 6 do not need to be reset).</p><p>At step t + 7, neuron i 3 executes the rule -/a → λ; 0, stopping that branch of execution and returning the whole branch to its original state. Simultaneously, neuron i 4 fires by the rule 0/a 2 → a; 0, sending a spike and the neutral charge to neurons l k and i 5 , respectively.</p><p>At step t + 8, neuron i 5 resets neuron i 4 to negative charge using the forgetting rule 0/a → λ; -, and at the same time neuron l k is activated meaning that the instruction l k is simulated.</p><p>As for Case I, we here also provide a step-by-step description of the simulation for the case of register r holding the value zero as shown in Table <ref type="table" target="#tab_1">II</ref>.</p><p>One can note that at the end of the simulation the neurons are reset to their initial charge state as well as their initial number of spikes (all are empty but the neuron modeling register r is also empty if it was originally empty or the number of spikes inside it is decreased by one if it was originally nonempty). It should be clear that the module can be reused as many times as necessary to simulate the work of the register machine.</p><p>Consequently, the simulation of the SUB instruction l i : (SUB(r ), l j , l k ) is correct: system starts from neuron l i having one spike inside and ends with sending one spike and the neutral charge to neuron l j (if the number stored in register r is greater than 0 and it was decreased by one) or sending one spike and the neutral charge to neuron l k (if the number stored in register r is 0).</p><p>It is clear that no interference will happen between the ADD modules and the SUB modules. However, interferences between two SUB modules can possibly happen. Specifically, if there are several SUB instructions l s acting on register r , then neurons r 5 and r 6 associated with register r have synapses to all neurons s 3 and s 4 , respectively, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>. When a SUB instruction l i : (SUB(r ), l j , l k ) is simulated, in the SUB module associated with l s , for l s = l i , all neurons receive no spike and charge except for neurons s 3 and s 4 . If neuron r has n &gt; 0 spikes, then neuron s 3 receives one spike and the neutral charge from neuron r 5 and neuron s 4 receives a negative charge from neuron r 6 . With one spike inside and the negative charge, neuron s 3 uses the rule -/a → λ; 0, forgetting the spike; neuron s 4 is negative, after receiving the negative charge, it is still negative. Thus, neurons s 3 and s 4 return to the initial state. If neuron r contains no spike, then neuron s 4 receives one spike and the neutral charge from neuron r 6 , while neuron s 3 receives a negative charge from neuron r 5 ; like in the previous case, neurons s 3 and s 4 are brought to their initial state with respect to the charge and the initial number of spikes. Consequently, no undesired steps in (i.e., steps that do not correspond to correct simulations of instructions of M) will be caused by the interference among SUB modules.</p><p>Module FIN (shown in Fig. <ref type="figure" target="#fig_3">4</ref>): Outputting the result of computation.</p><p>Assume now that the computation in M halts, which means that the halt instruction l h : HALT is reached and the result of the computation is placed in register 1, which is never decremented during the computation. This means that the neuron l h receives one spike and the neutral charge. At that moment, neuron 1 contains n spikes, for the number n stored in register 1 of M. Let t be the moment when neuron l h fires by the rule 0/a → a; +, sending one spike as well as a positive charge to each of neurons h 1 , h 2 , h 3 , and 1. The first spike arrives in neuron out by passing along the path of neurons h 1 , h 4 , h 6 , then neuron out fires by the rule 0/a → a; 0, sending the first spike to the environment at step t + 5.</p><p>Let us now follow the other spikes sent to neurons 1, h 2 , h 3 from neuron l h . At step t + 2, after receiving the positive charge and the spike, neurons h 2 and h 3 become activated and form a pair of self-sustaining neurons, spiking to each other in each step; meanwhile, neurons h 2 and h 3 send a spike to neuron h 5 and neuron h 2 also sends a positive charge to neuron h 5 . At the same step, neuron 1 also fires by the rule +/a → a; -, sending a negative charge as well as a spike to neuron h 5 . In this way, neuron h 5 is still neutral and accumulates three spikes, then it fires by the rule 0/a 3 → λ; 0, removing the three spikes. At each step, neuron 1 consumes one spike, which corresponds to decreasing by one the value stored in register 1. These operations continue until exhausting the spikes from neuron 1, and this means that neuron 1 can fire for n + 1 times (one more spike from neuron l h ). Therefore, the last time is at step t + n + 2 when neuron 1 fires (one step was necessary initially, for firing neuron l h ).</p><p>At step t + n + 3, the first step when neuron 1 does not fire, while neurons h 2 and h 3 fire again. At step t + n + 5, neuron out fires, sending the second spike to the environment, hence the distance between the two spikes of neuron out is (t + n + 5) -(t + 5) = n, which is exactly the number stored in register 1 when the computation of M halts.</p><p>At step t + n + 4, neurons h 2 and h 3 fire again, hence neuron h 5 receives two spikes and a positive charge, then it fires at step t + n + 5. At step t + n + 5, neuron h 7 fires by the rule 0/a → λ; -, sending a negative charge to neurons out and h 2 , respectively. In this way, neuron out receives a spike and a neutral charge from neuron h 5 and a negative charge from neuron h 7 , thus, at step t + n + 6, it fires by the rule -/a → λ; 0. The negative charge of neuron h 7 also reaches neuron h 2 , neuron h 2 becomes neutral and holds one spike, hence it will never fire again. Neuron h 3 fires once more at step t + n + 6, and this is the last step of the computation: sending one spike and a neutral charge to neurons h 2 and h 5 , respectively. Then neuron h 2 contains already two spikes; neuron h 5 is positive and stores one spike inside, and it has no rule to be used. The computation halts.</p><p>In conclusion, ADD modules, SUB modules, and the FIN module correctly simulate ADD instructions, SUB instructions, and output the computation result, respectively. Therefore, the system correctly simulates the register machine M and N(M) = N 2 ( ), and this completes the proof.</p><p>A similar result can be obtained for PSN P systems working in the accepting mode, but we do not enter into details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SMALL UNIVERSAL PSN P SYSTEM</head><p>A natural question in this framework is to look for small universal PSN P systems. We will construct a universal PSN P system for computing functions, based on simulating the universal register machine M u given in [47, Fig. <ref type="figure" target="#fig_0">1</ref>]. The goal is to obtain a universal PSN P system with as small number of neurons as possible.</p><p>Before passing to the construction, a modification should be made in M u , because the construction from Theorem 1 in the previous section does not allow subtraction operations acting on the register where the computation result is placed, but register 0 of M u is subject to such operations. Thus, a further register labeled by 8 is added, and the halt instruction l h of M u is replaced by the following instructions: These three instructions move the contents of register 0 to register 8 that is never decremented during a computation. Let us Proof: As in the proof of Theorem 1, a PSN P system u is constructed to simulate the universal register machine M u , where 200 neurons are used. By some "code optimization," the number of neurons can be decremented from 200 to 164. See the Supplementary Material for the detail of the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. COMPARISONS WITH OTHER COMPUTING DEVICES</head><p>In the previous sections, we proved that PSN P systems are powerful and do what a Turing machine can do; then we constructed a small universal PSN P system. Specifically, in the case using standard spiking rules, we constructed a universal PSN P system having 164 computing units (i.e., neurons) as a function computing device. In order to validate the computation power of PSN P systems, in terms of activation functions and lower bound on the number of computing units, we compare PSN P systems with recurrent neural networks and SN P systems.</p><p>Recurrent neural networks are a well-known model of artificial neural networks and intensively investigated, which consist of interconnections of computing units (i.e., neurons). In a recurrent neural network, all neurons synchronously evolve, and each neuron updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all neurons. It is proven that recurrent neural networks can simulate any Turing machine, and there is a network made up of 886 neurons that can achieve universality for computing functions <ref type="bibr" target="#b54">[55]</ref>. In the recurrent neural network constructed in <ref type="bibr" target="#b54">[55]</ref>, the network has four layers: the input layer, the main layer, the hidden layer, and the output layer, which consist of 5, 108, 762, and 11 neurons, respectively. The neurons in the four layers contain the sigmoidal function σ shown in Table <ref type="table" target="#tab_2">III</ref>, namely, the saturated-linear function. The weights of synapses in the network are rational numbers, which are simple and small. The output values of the network are binary values. In the proofs of achieving universality for computing functions, a specific processor net is constructed to simulate a given p-stack machine that computes a recursively computable partial function φ : {0, 1} + → {0, 1} + . In the main layer of the constructed network, twelve neurons are needed per stack, i.e., four neurons hold the stack encoding, four neurons hold the value of the top element of the stack, and four neurons for testing whether the stack is empty. In addition, the main layer contains a set of neurons for representing the state of the p-stack machine. In the hidden layer of the network, a set of neurons is included to compute the next state by combining the current state and stack readings. The output layer of the network is used to decode the output result. Specifically, if the recursively computable partial function is not defined on the input, then the ninth and tenth processors of the output layer always have activation value equal to zero, and if it is defined, once the ninth processor ever becomes equal to one, the tenth processor begins to output the decoding of the result. In total, 886 neurons are used in the universal recurrent neural network constructed here <ref type="bibr" target="#b54">[55]</ref>.</p><p>Instead of using a sigmoidal function to imitate the behavior of biological neurons in artificial neural networks, in SN P systems, the computing units (i.e., neurons) evolve by spiking and forgetting rules. It was proved that the SN P system with 84 neurons can achieve Turing universality when using standard spiking rules <ref type="bibr" target="#b46">[47]</ref>. However, as we already see in Section I, SN P systems use the regular expressions, and considerable computation power is hidden in the implicit mechanism that SN P systems use to decide whether a given rule can be applied.</p><p>The activation functions and the number of computing units for achieving Turing universality of PSN P systems, recurrent neural networks, and SN P systems are shown in Table <ref type="table" target="#tab_2">III</ref>. From this table, we can find that the activation functions contained in the computing units of PSN P systems are more natural and simpler than that of the two compared computation models; in terms of the number of computing units for achieving universality, PSN P systems are much smaller than that of recurrent neural network but larger than that of SN P systems. These results reveal that the evolving way of PSN P systems is more suitable for achieving Turing universality, which suggests that PSN P systems are appropriate in possible applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In traditional SN P systems, the application of a spiking rule depends on the contents of a neuron that is checked by means of a regular expression associated with the rule in the neuron; however, it is at least NP-complete to decide whether the number of spikes contained in a neuron matches a regular expression. In this paper, we have introduced a rather attractive variant of SN P systems, on the one hand, removing the regular expressions from spiking and forgetting rules, on the other hand, considering three kinds of electrical charges, -, 0, +, associated with the neurons to control the applicability of rules. The computational completeness was proved for PSN P systems, and a universal PSN P system with 164 neurons was constructed.</p><p>As a new variant of SN P systems, many research directions can be considered. In general, the research topics that have been explored for classical SN P systems also deserve to be investigated for PSN P systems. For example, the normal forms of PSN P systems are worthy to be investigated, e.g., it remains open whether results about the universality of PSN P systems still hold without using forgetting rules. In the PSN P systems constructed in Theorem III, standard spiking rules are used. It is worth considering the use of rules in order to simplify the proofs. Besides the synchronized mode considered in PSN P systems, it deserves to consider PSN P systems working the nonsynchronized mode, the exhaustive mode, or the sequential mode. Recently, cell-like SN P systems are introduced in <ref type="bibr" target="#b38">[39]</ref>. The investigation of the computation power of polarized cell-like SN P systems seems to be challenging. We conjecture that such systems may not be universal.</p><p>Some specific open problems about PSN P systems can also be formulated. In the proofs of the universality of PSN P systems, three charges are used. Is it possible to decrease the number of charges without the loss of the computation power, or characterizing the corresponding classes of subuniversal computing devices? A universal PSN P system with 164 neurons was constructed in this paper. Using the extended rules and adding the feature of delay, the improvement about the number of neurons can be obtained. It is meaningful and challenging to find the optimal number of the universal PSN P system.</p><p>It is of interest to consider whether the proposed threestate polarization method can reduce back to the usage of the regular expression. In other words, is it possible to design a regular expression to express the behavior of polarization? If so, it is expected to find restricted regular expressions without decreasing the computation power of SN P systems, such that it is a P problem to decide whether a number is in the languages defined by the restricted regular expressions.</p><p>The applications of SN P systems have been investigated in both theoretical and practical engineering fields. For example, SN P systems have been used to design optimization algorithm to solve combinatorial optimization problems <ref type="bibr" target="#b55">[56]</ref>, perform fuzzy inference and knowledge representation <ref type="bibr" target="#b56">[57]</ref>, and diagnose the fault of electric power systems <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. As a variant of SN P systems, we believe that PSN P systems are useful in solving real-life problems, e.g., the recognition of handwritten digit letters <ref type="bibr" target="#b59">[60]</ref>.</p><p>The learning strategies and feedback mechanisms on spiking neural networks have been intensively investigated <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>,</p><p>and have a lot of applications, such as complicated time-dependent pattern recognition <ref type="bibr" target="#b2">[3]</ref>, autonomous robot control <ref type="bibr" target="#b62">[63]</ref>, information processing and learning <ref type="bibr" target="#b63">[64]</ref>, machine learning <ref type="bibr" target="#b61">[62]</ref>, and natural language processing <ref type="bibr" target="#b64">[65]</ref>. Inspired by the learning strategies in spiking neural networks, it is a promising direction to look into the strategies (e.g., Hebbian learning and backpropagation algorithm) and transplant these learning strategies into PSN P systems. Furthermore, PSN P systems with learning strategies may have potential applications in solving practical engineering problems like spiking neural networks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of a PSN P system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. ADD module of .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. SUB module of .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. FIN module of .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>l 22 :</head><label>22</label><figDesc>(SUB(0), l 23 , l h ), l 23 : (ADD(8), l 22 ), l h : HALT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Zhiqiang</head><label></label><figDesc>Zhang received the Ph.D. degree from the Huazhong University Science and Technology, Wuhan, China, in 2016. He is currently a Post-Doctoral Fellow with Peking University, Beijing, China. His current research interest includes membrane computing. Linqiang Pan (M'09) received the Ph.D. degree from Nanjing University, Nanjing, China, in 2000. He has been a Professor with the Huazhong University of Science and Technology, Wuhan, China, since 2004. His current research interests include membrane computing, DNA nanotechnology, complex network, and systems biology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SIMULATION</head><label>I</label><figDesc>FOR THE CASE OF REGISTER r STORING A VALUE GREATER THAN 0</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II SIMULATION</head><label>II</label><figDesc>FOR THE CASE OF REGISTER r STORING THE VALUE 0</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ACTIVATION</head><label>III</label><figDesc>FUNCTIONS AND THE NUMBER OF COMPUTING UNITS FOR ACHIEVING UNIVERSALITY OF THREE COMPUTATION MODELSdenote by M u the modified universal register machine. In this way, M u has 9 registers (numbered from 0 to 8), 24 ADD and SUB instructions (10 ADD and 14 SUB instructions, respectively), and 25 labels. The result of a computation of M u is placed in register 8.Theorem 2: There exists a universal PSN P system having 164 neurons for computing functions.</figDesc><table /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation of China under Grant 61320106005 and Grant 61033003, in part by the Ph.D. Programs Foundation of Ministry of Education of China under Grant 20120142130008, in part by the Innovation Scientists and Technicians Troop Construction Projects of Henan Province under Grant 154200510012, and in part by the UEFSCDI Project Remote Forest under Project PN-II-PTPCCA-2011-3.2-1710.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Rozenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bäck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Kok</surname></persName>
		</author>
		<title level="m">Handbook of Natural Computing</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artificial neural networks: A tutorial</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Mohiuddin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="31" to="44" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spiking neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh-Dastidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="295" to="308" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Unbehauen</surname></persName>
		</author>
		<title level="m">Neural Networks for Optimization and Signal Processing</title>
		<meeting><address><addrLine>Chichester, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>Comput. Vis. Pattern Recognit. (CVPR)<address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<meeting><address><addrLine>Oxford, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic evolving spiking neural networks for on-line spatio-and spectro-temporal pattern recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kasabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dhoble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nuntalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Indiveri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="188" to="201" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dynamical analysis of continuous higher-order Hopfield networks for combinatorial optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sandoval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1802" to="1819" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent neural network for non-smooth convex optimization problems with application to the identification of genetic regulatory networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="714" to="726" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural nets as dynamical Boolean systems with application to associative memory</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Watta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hassoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1268" to="1280" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A behavior controller based on spiking neural networks for mobile robots</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="655" to="666" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computing with membranes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="143" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tissue P systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Martin-Vide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pazos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodríguez-Patón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">296</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="326" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Membrane Computing: An Introduction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">P systems with active membranes: Attacking NP complete problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Autom. Lang. Combinat</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="75" to="90" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The power of communication: P systems with symport/antiport</title>
		<author>
			<persName><forename type="first">P</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generat. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="305" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tissue P systems with channel states</title>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="116" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Frisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gheorghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
		<title level="m">Applications of Membrane Computing in Systems and Synthetic Biology</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Biological networks in metabolic P systems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Manca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bianco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioSystems</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="489" to="498" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<title level="m">Membrane Computing: Theory and Applications</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Science Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using enzymatic numerical P systems for modeling mobile robot controllers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Pavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Buiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="387" to="393" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A quantum-inspired evolutionary algorithm based on P systems for knapsack problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gheorghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Informat</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="116" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The Oxford Handbook of Membrane Computing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rozenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salomaa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Oxford Univ. Press</publisher>
			<pubPlace>Oxford, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spiking neural P systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yokomori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Informat</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="308" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Networks of spiking neurons: The third generation of neural network models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Maass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1659" to="1671" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Pulsed Neural Networks</title>
		<editor>W. Maass and C. M. Bishop</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Gerstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Kistler</surname></persName>
		</author>
		<title level="m">Spiking Neuron Models: Single Neurons, Populations, Plasticity</title>
		<meeting><address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with extended rules: Universality and languages</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ishdorj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><surname>Pérez-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Comput</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="166" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with astrocyte-like control</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Universal Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1707" to="1721" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Extended spiking neural P systems with excitatory and inhibitory astrocytes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Oswald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fifth Brainstorming Week on Membrane Computing</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gutiérrez-Naranjo</surname></persName>
		</editor>
		<meeting>Fifth Brainstorming Week on Membrane Computing</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="63" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with astrocytes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Hoogeboom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="805" to="825" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with anti-spikes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput., Commun., Control</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="282" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Normal forms of spiking neural P systems with antispikes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Abdullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nanobiosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="352" to="359" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with thresholds</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1340" to="1361" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with weights</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérenz-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2615" to="2646" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with rules on synapses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="82" to="95" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with rules on synapses working in maximum spiking strategy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Nanobiosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="465" to="477" />
			<date type="published" when="2015-06">Jun. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cell-like spiking neural P systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">623</biblScope>
			<biblScope unit="page" from="180" to="189" />
			<date type="published" when="2016-04">Apr. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Asynchronous spiking neural P systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cavaliere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">H</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ecegioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Woodworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="2352" to="2364" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Asynchronous spiking neural P systems with local synchronization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">219</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequential SNP systems based on min/max spike number</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">H</forename><surname>Ibarra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rodríguez-Patón</surname></persName>
		</author>
		<idno>nos. 30-32</idno>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">410</biblScope>
			<biblScope unit="page" from="2982" to="2991" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Universality of sequential spiking neural P systems based on minimum spike number</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">499</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with an exhaustive use of rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yokomori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Unconvent. Comput</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="156" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with a generalized use of rules</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2925" to="2943" />
			<date type="published" when="2014-12">Dec. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On string languages generated by spiking neural P systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundam. Informat</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="162" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Small universal spiking neural P systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioSystems</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="60" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spiking neural P systems with neuron division and budding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. China Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1596" to="1607" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deterministic solutions to QSAT and Q3SAT by spiking neural P systems with pre-computed resources</title>
		<author>
			<persName><forename type="first">T.-O</forename><surname>Ishdorj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leporati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">411</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="2345" to="2358" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the computational power of spiking neural P systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Leporati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zandron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ferretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mauri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Unconvent. Comput</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="459" to="473" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Small universal register machines</title>
		<author>
			<persName><forename type="first">I</forename><surname>Korec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="301" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Open problems, research topics, recent results on numerical and spiking neural p systems (The &apos;curtea de argeş 2015 series&apos;)</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pȃun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://www.gcn.us.es/?q=14bwmc_proceedings" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Fourteenth Brainstorming Week on Membrane Computing</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Graciani</surname></persName>
		</editor>
		<meeting>Fourteenth Brainstorming Week on Membrane Computing</meeting>
		<imprint>
			<publisher>Fenix Editora</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Handbook of Formal Languages</title>
		<editor>G. Rozenberg and A. Salomaa</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Computation: Finite and Infinite Machines</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Minsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1967">1967</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On the computational power of neural nets</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="132" to="150" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">An optimization spiking neural P system for approximately solving combinatorial optimization problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Neri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Weighted fuzzy spiking neural P systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Fuzzy Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Fuzzy reasoning spiking neural P system for fault diagnosis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">235</biblScope>
			<biblScope unit="page" from="106" to="116" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Fault diagnosis of electric power systems based on fuzzy reasoning spiking neural P systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pérez-Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Power Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1182" to="1194" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A new supervised learning algorithm for multiple spiking neural networks with application in epilepsy and seizure detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh-Dastidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1419" to="1431" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Introduction to spiking neural networks: Information processing, learning and applications</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ponulak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kasiński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Neurobiol. Experim</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="409" to="433" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">The application of spiking neural networks in autonomous robot control</title>
		<author>
			<persName><forename type="first">P</forename><surname>Trhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Informat</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="823" to="847" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Practical applications of spiking neural network in information processing and learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Khademian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Khanbabaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Babol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Sci., Int. J</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="133" to="137" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A spiking neural network model of multi-modal language processing of robot instructions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Panchev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Biomimetic Neural Learning for Intelligent Robots</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Wermter</surname></persName>
		</editor>
		<editor>
			<persName><surname>Palm</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">3575</biblScope>
			<biblScope unit="page" from="182" to="210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
