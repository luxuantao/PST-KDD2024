<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the probability-quality paradox in language generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-31">31 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
							<email>clara.meister@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gian</forename><surname>Wiher</surname></persName>
							<email>gian.wiher@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
							<email>ryan.cotterell@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Z?rich University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the probability-quality paradox in language generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-31">31 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.17217v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable.</p><p>We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text-covering multiple tasks and common decoding strategiessuggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Today's probabilistic neural language models are often trained on millions-if not billions-of lines of human text; thus, at least at an intuitive level, we would expect high-probability generations to be human-like. Yet the high-quality<ref type="foot" target="#foot_0">1</ref> texts these models have become famous for producing <ref type="bibr" target="#b1">(Brown et al., 2020;</ref><ref type="bibr" target="#b2">Clark et al., 2021)</ref> are usually not those assigned the highest probability by the model <ref type="bibr" target="#b6">(Fan et al., 2018;</ref><ref type="bibr" target="#b11">Holtzman et al., 2020;</ref><ref type="bibr" target="#b0">Basu et al., 2021;</ref><ref type="bibr" target="#b4">DeLucia et al., 2021)</ref>. Rather, the relationship between probability and quality appears to have an inflection point,<ref type="foot" target="#foot_1">2</ref> i.e., quality and probability are positively correlated only until a certain threshold, after which the correlation becomes negative. While the existence of such a trend has received informal explanations (see, e.g., <ref type="bibr" target="#b12">Ippolito et al. (2019)</ref> and <ref type="bibr" target="#b31">Zhang et al. (2021)</ref> for a qualitative discussion about the trade-off between diversity and quality), it lacks a more fundamental understanding. Why does the lower probability text produced by stochastic decoding methods-such as nucleus or top-k sampling-outperform text generated using probability-maximizing approaches? In this note, we take an information-theoretic approach in an attempt to answer this question.</p><p>In information theory, probability has another interpretation: its negative log quantifies information content. In the context of natural language, the notion of information content is intuitive; humans use strings as a means to convey information. Further, less predictable text, i.e., text which would be harder for us to anticipate, conveys more information. If we assume that the goal of human communication is to transmit messages efficiently and reliably <ref type="bibr" target="#b7">(Gibson et al., 2019)</ref>, we may predict that these strings' information content should concentrate inside a specific interval. At one extreme, strings with more-than-expected information may be hard to process, and thus ought to be disfavored when producing language.<ref type="foot" target="#foot_2">3</ref> At the other extreme, low-information strings may be seen as boring and uninformative.</p><p>Collectively, these concepts lead us to propose the expected information hypothesis: Text perceived as human-like should have an information content within a small interval around the expected information-i.e., the entropy-of natural language strings. Such a hypothesis offers an intuitive explanation for the trends observed in natural language generation (NLG), i.e., why desirable text seems to exist not always at the high end of the probability spectrum but around a certain inflection point. 4 Moreover, it also gives us a testable hypothesis: given a language generation model q whose entropy we can empirically estimate, we can evaluate whether high-quality text indeed has an information content that falls within an interval around this quantity.</p><p>To test our hypothesis, we perform an analysis comparing human and model-generated text, investigating multiple common decoding strategies and NLG tasks. Specifically, our analysis focuses exclusively on English text. We indeed observe that the information content of highly ranked text (as judged by humans) often falls within a standard deviation of model entropy; there is statistically significant evidence that this is not due to chance. Further, the best-performing decoding methods appear to select strings with an information content within this interval. We take these observations as empirical support for our hypothesis, helping to explain the probability-quality paradox observed in language generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Language Generators</head><p>In this work, we focus on probabilistic models for language generation tasks. Formally, these models are probability distributions q over natural language strings y ? Y, where Y is the (countably infinite) set consisting of all possible strings that can be constructed from a set vocabulary V:</p><formula xml:id="formula_0">Y def = {BOS ? v ? EOS | v ? V * } (1)</formula><p>Here, BOS and EOS stand for special reserved beginning-and end-of-string tokens, respectively, and V * denotes the Kleene closure of V. In practice, we limit the set of strings we consider to Y N ? Y for some maximum sequence length N . Note that q may be a conditional model. For instance, we may model q(? | x) where x is an input text, as in the case of machine translation, or an input image, as in the case of image captioning. However, for notational brevity, we omit this explicit dependence in most of our subsequent analyses. In order to estimate q, it is standard practice to maximize the log-probability of a 4 Similar ideas have been used to improve language models and language generation before <ref type="bibr" target="#b14">(Meister et al., 2020;</ref><ref type="bibr" target="#b26">Wei et al., 2021)</ref>.</p><p>training corpus C under the model with respect to the model's parameters ?. This is equivalent to minimizing its negative log-probability:</p><formula xml:id="formula_1">L(?; C) = - y?C log q(y)<label>(2)</label></formula><p>There are many different decision rules one can employ for generating natural language strings from a model q; such sets of rules are generally referred to as decoding strategies; see <ref type="bibr" target="#b28">Wiher et al. (2022)</ref> for an in-depth review. Given the probabilistic nature of the models we consider, an intuitive strategy for decoding would be to choose the string with the highest probability under q, an approach referred to as maximum-a-posteriori (MAP) decoding. <ref type="foot" target="#foot_3">5</ref> Yet recent research has shown that solutions to MAP decoding-or, even more generally, to heuristic mode-seeking methods such as beam search-are often not high-quality, even in state-of-the-art NLG models. For example, in the domain of machine translation, the most probable string under the model is often the empty string <ref type="bibr" target="#b23">(Stahlberg and Byrne, 2019)</ref>. Similarly, in the domain of openended generation, mode-seeking methods produce dull and generic text <ref type="bibr" target="#b11">(Holtzman et al., 2020)</ref>.</p><p>Where maximization has failed, authors have turned to stochastic methods, taking random samples from q. While the resulting text is often assigned much lower probability than the mode, it can be qualitatively much better. This peculiarity has puzzled the language generation community for the last few years, with only qualitative intuitions being offered as explanation. This paper in turn offers a quantitative explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Language as Communication</head><p>While many aspects of natural language may not perfectly adhere to Shannon's mathematical theory of communication, there are several characteristics of human language that can fruitfully be described using an information-theoretic framework. <ref type="foot" target="#foot_4">6</ref> Here we employ this framework for explaining recent phenomena observed in probabilistic NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Measuring Information</head><p>We can precisely compute the information content of a string given the true (perhaps conditional) probability distribution p over natural language strings. Fortunately, this is the exact distribution our language generation models in ?2 are trained to approximate.<ref type="foot" target="#foot_5">7</ref> Assuming q approximates p well (as quantified by metrics such as perplexity), we may thus use it to estimate such attributes of natural language strings. In this work, we will measure the amount of information a specific realization y contains, which we denote I(y) def = -log q(y), as well as the expected amount of information a random y ? Y N drawn from q contains, also termed the entropy of q: <ref type="bibr">Pimentel et al. (2021b, Theorem 2)</ref> prove that, as long as the probability of EOS under q is bounded below by some &gt; 0, then the entropy of q is finite. In our case we restrict q to a finite subset Y N of Y, which also implies that Eq. ( <ref type="formula">3</ref>) is finite.</p><formula xml:id="formula_2">E q [I(y)] = H(q) = - y?Y N q(y) log q(y) (3) Note that</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Expected Information Hypothesis</head><p>Language is used as a means for transferring information. This property of language has in fact motivated several theories of language evolution; many have posited, for instance, that natural language has developed to optimize for reliable and efficient data communication, subject to cognitive resources <ref type="bibr" target="#b32">(Zipf, 1949;</ref><ref type="bibr" target="#b10">Hockett, 1960;</ref><ref type="bibr" target="#b9">Hawkins, 2004;</ref><ref type="bibr" target="#b17">Piantadosi et al., 2011)</ref>. The above theories arguably imply that humans tend to produce natural language strings with a certain amount of information; they also imply that, on the receiving end of communication, humans would expect similar strings. We argue that this amount is intuitively close to the language's entropy, i.e., close to the average string's information content.</p><p>Expected Information Hypothesis. Text perceived as human-like typically encodes an amount of information close to the expected information content of natural language strings, i.e., in the interval [H(p) -?, H(p) + ?] for a natural language string distribution p and some ?.<ref type="foot" target="#foot_6">8</ref> Text that falls outside of this region is likely perceived as unnatural.</p><p>This viewpoint can be applied to the problem of decoding neural text generators. In the context of a model q of the distribution p, this implies thatwhen q is a good approximation-human-like text should typically have a negative log-probability close to the entropy of q. In ?4, we provide empirical evidence for this hypothesis.</p><p>Relationship to the typical set. The set of strings that we discuss has an intuitive relationship to the typical set <ref type="bibr" target="#b21">(Shannon, 1948)</ref>, an informationtheoretic concept defined for stationary ergodic stochastic processes. However, generation from standard neural probabilistic language models cannot be framed as such a process.<ref type="foot" target="#foot_7">9</ref> While we cannot utilize the formal mathematical underpinnings of typicality, the connection can still be useful for understanding why strings with a given information content exhibit certain characteristics. An overview of the concept is in App. A for the interested reader; also see <ref type="bibr" target="#b5">Dieleman (2020)</ref> for further insights on typicality in the context of generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experiments present an analysis of the distribution of information content in text generated by both humans and probabilistic models. Specifically, we look at the relationship between information content and quality-as measured by human judgments. We perform experiments on two natural language generation tasks: abstractive summarization and story generation. We present the results for story generation here, while the results for summarization can be found in App. B due to space constraints. A recreation of the probability versus quality plots of <ref type="bibr" target="#b31">Zhang et al. (2021)</ref> can also be found in App. B.</p><p>We use the following Monte Carlo estimator for the entropy, i.e., expected information content, of our model q:</p><formula xml:id="formula_3">H(q) = 1 M M m=1 -log q(y (m) )<label>(4)</label></formula><p>where we sample y (m) i.i.d. ? q. Algorithmically, taking these samples may be done in linear time using ancestral sampling. All computations are performed with the test sets of respective datasets. Note that for both abstractive summarization and story generation, where we condition on some input x, we must compute the conditional entropy for each input, i.e., using q(? | x) instead of q(?). For each x, we take M = 100 to estimate H(q(? | x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Models and Data. We only conduct experiments on the English language. For story generation, we fine-tune GPT-2 (medium) <ref type="bibr" target="#b20">(Radford et al., 2019)</ref> (checkpoint made available by OpenAI) on the WRITINGPROMPTS dataset <ref type="bibr" target="#b6">(Fan et al., 2018)</ref>. For abstractive summarization, we use BART <ref type="bibr" target="#b13">(Lewis et al., 2020)</ref>, fine-tuned on the CNN/DAILYMAIL dataset <ref type="bibr" target="#b16">(Nallapati et al., 2016)</ref>. We rely on the open-sourced code-base from the HuggingFace framework <ref type="bibr" target="#b30">(Wolf et al., 2020)</ref> for reproducibility.</p><p>Decoding Strategies. We explore text generated according to a number of different decoding strategies. Unless otherwise stated, we use the implementation provided by Hugging Face for each of the decoding algorithms. Along with standard ancestral sampling, we experiment with the following six decoding strategies:</p><p>? greedy search; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>In Fig. <ref type="figure" target="#fig_0">1</ref>, we plot the distribution of information content assigned by q to four different sets of strings: our reference (human-generated) text, the top and bottom ranked (according to human annotators) strings generated from q via our different decoding strategies,<ref type="foot" target="#foot_12">14</ref> and strings sampled i.i.d. from q. Note that the latter should represent the distribution of negative log-probabilities assigned to strings by the model. We see that both the references and the top-ranked model-generated stringsboth of which we assume are of relatively high quality-contain an amount of information clustered around the (estimated) model entropy. On the other hand, the distribution of the information content of poorly rated strings is skewed towards much lower values. The same trends hold when looking at information normalized by string length, i.e., I(y)/|y| (see App. B), demonstrating these trends are not purely an artifact of string length. We note that in our human evaluations, the reference string was ranked first in 47% of cases and it was tied for first in an additional 16% of the cases. This suggests that the quality of the reference strings is on par with-if not higher than-the set of "top 1" model-generated strings. Fig. <ref type="figure" target="#fig_1">2</ref> shows the distribution of deviations of strings' information content from the model entropy;<ref type="foot" target="#foot_13">15</ref> results are shown for both reference strings and top-ranked model-generated strings. Because these values are distributed quite evenly around 0, we take this as additional evidence that high-quality text usually has information content close to H(q). Further, the shapes of these curves motivate us to perform our next set of tests using ? = ?, the standard deviation of information values under q. <ref type="foot" target="#foot_14">16</ref>We employ statistical hypothesis testing to see if the percentage of high-quality strings whose information content falls in the interval [H(q) -?, H(q) + ?] is greater than chance. For each input x (i.e., either a story prompt or article), we compute the information content of the reference and top-3 human-ranked strings.</p><p>We then compute the percentage of items (among these four) that fall within [H(q(? | x)) -?, H(q(? | x)) + ?]. We compare this percentage to the percentage of strings sampled directly from q(? | x) that falls within this interval. The former should (in expectation) be greater than the latter if the probability of high-quality strings having information content within this interval is greater than chance. Specifically, we test this using a paired, unequal-variance t-test, where samples with the same input are paired. At significance level ? = 0.01, we reject our null hypothesis-i.e., we reject that the percentage of highly rated strings (reference plus top-3 human-ranked strings) that fall within this interval is equal to (or less than) what we should expect by chance. Further, using a simple unpaired t-test, we find that the mean human score of strings (across all decoding strategies) within this region is significantly higher than those outside of this region. This characteristic is visualized in Fig. <ref type="figure" target="#fig_2">3</ref>, where we plot the distributions of human quality ratings for strings inside and outside of this interval. We include a version of Fig. <ref type="figure" target="#fig_2">3</ref> further broken down by whether strings fall above or below this interval in App. B.</p><p>Additional plots reinforcing these observations can be found in App. B. Also see <ref type="bibr" target="#b15">Meister et al. (2022)</ref> for follow-up experiments to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we present the expected information hypothesis, which states that human-like strings typically have negative log-probability close to the expected information content of the probabilistic model from which they were generated. We use this hypothesis to explain why high-quality text seems to exist not necessarily at the high end of the probability spectrum but, rather, close to the entropy of the model. We provide empirical evidence in support of our hypothesis in an analysis of both human and machine-generated text, demonstrating that, overwhelmingly, high-quality text indeed has information content in the proposed region. ) within 1 std of model entropy and above/below this interval. Note that "above" corresponds to text that has lower probability than the specified interval; due to the nature of the decoding strategies explored in this work, which all to some extent (except for ancestral sampling) disproportionately favor higher probability strings, only &lt; 5% of all strings evaluated fall into the "above" category. Thus, we do not have a representative evaluation of this region of the probability space. However, it is often observed that extremely low-probability strings are usually incoherent or nonsensical.      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The distribution over information I(y) values of: MODEL, the model, as estimated using samples from q; REFERENCE, the reference strings; TOP 1 and BOTTOM 1, model-generated strings ranked first and last (respectively) among all decoding strategies by human annotators. The latter 3 are all w.r.t. a held-out test set. Same graph is reproduced for individual decoding strategies in App. B.</figDesc><graphic url="image-7.png" coords="4,70.87,70.87,218.27,110.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The distribution of the difference in total information content for (1) test-set references and (2) topranked model-generated strings from the (conditional) entropy of the model from which they were generated.</figDesc><graphic url="image-8.png" coords="4,306.14,70.87,218.27,115.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Human scores for strings (including both reference text and model-generated text) within 1 std of model entropy and outside of this interval. There is a statistically significant difference in means (p &lt; 0.001).</figDesc><graphic url="image-9.png" coords="5,70.87,70.87,218.26,52.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Human scores for strings (including both reference text and model-generated text) within 1 std of model entropy and above/below this interval. Note that "above" corresponds to text that has lower probability than the specified interval; due to the nature of the decoding strategies explored in this work, which all to some extent (except for ancestral sampling) disproportionately favor higher probability strings, only &lt; 5% of all strings evaluated fall into the "above" category. Thus, we do not have a representative evaluation of this region of the probability space. However, it is often observed that extremely low-probability strings are usually incoherent or nonsensical.</figDesc><graphic url="image-10.png" coords="9,70.87,80.42,218.28,51.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: For story generation, median human scores (averaged across the two criterion) versus information, grouped by intervals; bars represent std. We normalize I(y) by length to mimic setup of Zhang et al. (2021), which controls for length during generation. As with Zhang et al. (2021), we see an inflection point in the relationship along the information (equivalently, negative log-probability) axis.</figDesc><graphic url="image-11.png" coords="9,103.61,314.16,152.78,122.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: For abstractive summarization, the distribution over information I(y) values of: (model) the model, as estimated using samples from q; (reference) the reference strings; model-generated strings ranked (top 1) first and (bottom 1) last among all decoding strategies by human annotators. The latter 3 are all w.r.t. a held-out test set.</figDesc><graphic url="image-12.png" coords="9,70.87,559.55,218.26,118.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: For abstractive summarization, the distribution of the difference in total information content for (1) test-set references and (2) top-ranked model-generated strings from the entropy of the model from which they were generated.</figDesc><graphic url="image-13.png" coords="9,306.14,133.86,218.27,142.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: For story generation, the distribution over information (I(y)) values normalized by length of: (model) the model, as estimated using samples from q; (reference) the reference strings; model-generated strings ranked (top 1) first and (bottom 1) last among all decoding strategies by human annotators. The latter 3 are all w.r.t. a held-out test set.</figDesc><graphic url="image-14.png" coords="9,306.14,470.28,218.27,154.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: The distribution over information (I(y)) values for strings generated under different decoding strategies for story generation (top) and abstractive summarization (bottom). Inputs are taken from a held-out test set.</figDesc><graphic url="image-16.png" coords="10,112.52,402.68,370.23,172.35" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We assume that "human-like" is a (necessary but not sufficient) prerequisite for "high-quality" in the context of natural language strings.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The inflection point is empirically demonstrated in our App. B or in Fig.1of<ref type="bibr" target="#b31">Zhang et al. (2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Many works in psycholinguistics have shown a direct relationship between information content and processing effort<ref type="bibr" target="#b22">(Smith and Levy, 2013;</ref> Wilcox et al., 2020, inter alia).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Note that MAP decoding is somewhat of a misnomer since we are not maximizing over a Bayesian posterior. Nonetheless, the term has become commonplace in the language generation literature.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>  6  A large body of work has explored the extent to which attributes of human languages-such as word lengths or phoneme distributions-can be explained as informationtheoretic design features<ref type="bibr" target="#b7">(Gibson et al., 2019)</ref>. Surprisal theory, for instance, directly relates human language processing difficulty to information content<ref type="bibr" target="#b8">(Hale, 2001)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>To see this, recall that minimizing the objective in Eq. (2) is (up to an additive constant) equivalent to minimizing the Kullback-Leibler divergence-an information-theoretic quantity that measures the amount of information lost when approximating one probability distribution with anotherbetween the empirical distribution p and our model q.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>While we do not offer a concrete explanation of why distributions over natural language strings have a particular entropy, we posit that it is determined by cognitive constraints, as observed with other phenomena in natural language<ref type="bibr" target="#b3">(Coup? et al., 2019;</ref> Pimentel et al., 2021a).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Specifically, most neural language models are neither stationary (due to their ability to encode arbitrarily long sequences;<ref type="bibr" target="#b27">Welleck et al. 2020)</ref> nor ergodic (because of the absorbing nature of the EOS state). This implies that we cannot guarantee the existence of an entropy rate, which is necessary to define the typical set.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>The choice of dissimilarity function and hyperparameters (?, G, k) is based on the recommendations from the original work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>This choice is based on experiments in (DeLucia et al., 2021) that suggest a parameter range p ? [0.7, 0.9].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_10"><p>We use the github.com/Roxot/mbr-nmt framework.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_11"><p>The number of Monte Carlo samples was chosen based on the batch size constraint.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_12"><p>Specifically, for each input, we generate a single string according to each decoding strategy. We then rank these strings according to scores from human annotators.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_13"><p>Note that this is not simply Fig.1shifted by a constant, as deviations are computed w.r.t. input-dependent conditional entropy estimates, i.e., H(q(? | x)).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_14"><p>Similarly to our estimation of H(q) in Eq. (3), ? can be estimated from the distribution of values of I(y) sampled from the model.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank the anonymous reviewers for their helpful recommendations, as well as <rs type="person">Sander Dieleman</rs> for feedback on a preliminary draft of this paper.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>In order to complete our human evaluation, we used a crowdsourcing platform. For each task, we estimated the amount of time we expected the task to take and made sure that the crowdworkers would be paid (at minimum) a wage of $15 per hour. A further ethical consideration of this work is in the context of the use of language models for text generation. Language models have been used for the generation of malicious text, e.g., fake news and triggering content. The results in this work may provide insights for those using language models for such purposes as to how generations can be chosen to seem more "human-like."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Typical Set</head><p>Let us imagine flipping N biased coins; specifically, let X ? p be an indicator random variable that takes values H and T. Take p(X = H) = 0.6 and p(X = T) = 0.4. Flipping N biased coins is then equivalent to taking N i.i.d. samples x n ? p. For reasonably large N , what might you expect the sequence x 1 , . . . , x N to look like? Few people would answer "all heads," even though this is technically the highest probability sequence. Rather, intuition tells you: an expected sequence would be one comprised of approximately 60% heads and 40% tails.</p><p>The samples that fall into the latter category have a distinctive characteristic: they contain a near-average amount of information w.r.t the support of the distribution over X 1 , . . . , X N , where the information content of a realization x 1 , . . . , x N is defined as its negative log-probability. More formally, the (weakly)</p><p>where H(p) def =x p(x) log p(x) is the entropy-or equivalently, the expected value of the information content-of the random variable X. Under this definition we can prove that, for every ? &gt; 0, there exists an N 0 such that for all N &gt; N 0 , we have that the (?, N )-typical set contains at least (1 -?) of the probability mass of the joint distribution over -? X . The concept of the typical set also generalizes to stochastic processes when we can actually compute their average information rate-or equivalently, their entropy rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Design</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Human Evaluations</head><p>For story generation and abstractive summarization, the raters are first presented with a news article/prompt. Next, they are presented, in random order, with the corresponding reference and the summaries/stories generated by different decoders. For each of two rating criteria, a score from 0 to 7 is assigned. For story generation the criteria are FLUENCY and NATURALNESS while for abstractive summarization QUALITY and ACCURACY are used. We provide the following short descriptions of the criteria to the raters: FLUENCY: How fluent is the English text?</p><p>NATURALNESS: Does the text seem to be natural English text?</p><p>QUALITY: How high is the overall quality of the text?</p><p>ACCURACY: How well does the summary summarize the article?</p><p>After we obtain the ratings, we reject ratings that have not been filled out with care. Specifically, a rater is rejected if he assigns high scores to multiple examples that do not fulfill the specified criteria at all. If a rater has been rejected, we obtain a fresh set of ratings from a new rater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Figures</head><p>We provide several additional results, looking further into the relationship between text information content and perceived quality. We see that in general, the distribution of information content of reference strings is quite close to that of the model. While the distribution of information content of top 1 ranked strings is also closer to the model distribution than many of the individual decoding strategies, the overlap is not as high as for reference strings.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mirostat: A perplexity-controlled neural text decoding algorithm</title>
		<author>
			<persName><forename type="first">Sourya</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Govardana</forename><surname>Sachitanandam Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All that&apos;s &apos;human&apos; is not gold: Evaluating human evaluation of generated text</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Haduong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.565</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="7282" to="7296" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Different languages, similar encoding efficiency: Comparable information rates across the human communicative niche</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Coup?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franc ?ois</forename><surname>Dediu</surname></persName>
		</author>
		<author>
			<persName><surname>Pellegrino</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.aaw2594</idno>
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">2594</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decoding methods for neural narrative generation</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Delucia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?o</forename><surname>Sedoc</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.gem-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics</title>
		<meeting>the 1st Workshop on Natural Language Generation, Evaluation, and Metrics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="166" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Is MAP decoding all you need? The inadequacy of the mode in neural machine translation</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4506" to="4520" />
		</imprint>
	</monogr>
	<note>Musings on typicality. Bryan Eikema and Wilker Aziz. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hierarchical Neural Story Generation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">How efficiency shapes human language</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Dautriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Bergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tics.2019.02.003</idno>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A probabilistic Earley parser as a psycholinguistic model</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">T</forename><surname>Hale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Meeting of the North American Chapter</title>
		<meeting>the Second Meeting of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics and Language Technologies</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficiency and Complexity in Grammars</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">A</forename><surname>Hawkins</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:oso/9780199252695.001.0001</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The origin of speech</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">F</forename><surname>Hockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Comparison of Diverse Decoding Methods from Conditional Language Models</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reno</forename><surname>Kriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo?o</forename><surname>Sedoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Kustikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3752" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">Marjan Ghazvininejad,. 2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">If beam search is the answer, what was the question?</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Vieira</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2173" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Typical decoding for natural language generation</title>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gian</forename><surname>Wiher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno>CoRR, abs/2202.00666</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Abstractive text summarization using sequence-to-sequence RNNs and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>?aglar Gulc ?ehre</surname></persName>
		</author>
		<author>
			<persName><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1028</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="280" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Word lengths are optimized for efficient communication</title>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">T</forename><surname>Piantadosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Tily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3526" to="3529" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">2021a. A surprisal-duration trade-off across and within the world&apos;s languages</title>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dami?n</forename><surname>Blasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.73</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="949" to="962" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">2021b. How (non-)optimal is the lexicon?</title>
		<author>
			<persName><forename type="first">Tiago</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Nikkarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dami?n</forename><surname>Blasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.350</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4426" to="4438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<idno type="DOI">10.1002/j.1538-7305.1948.tb01338.x</idno>
	</analytic>
	<monogr>
		<title level="j">Bell System Technical Journal</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="623" to="656" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The effect of word predictability on reading time is logarithmic</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.02.013</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="319" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On NMT search errors and model errors: Cat got your tongue?</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Byrne</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1331</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3356" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Chris van der Lee, Albert Gatt, Emiel van Miltenburg, and Emiel Krahmer. 2021. Human evaluation of automatically generated text: Current trends and best practice guidelines</title>
		<author>
			<persName><forename type="first">Milo?</forename><surname>Stanojevi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalil</forename><surname>Sima'an</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2020.101151</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">101151</biblScope>
		</imprint>
	</monogr>
	<note>Fitting sentence level translation evaluation with many dense features</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Diverse beam search: Decoding diverse solutions from neural sequence models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ashwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vijayakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
		<idno>CoRR, abs/1610.02424</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A cognitive regularizer for language modeling</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.404</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5191" to="5202" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Consistency of a recurrent language model with respect to incomplete decoding</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaedeok</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.448</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5553" to="5568" />
		</imprint>
	</monogr>
	<note>Richard Yuanzhe Pang, and Kyunghyun Cho</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">On decoding strategies for neural text generators</title>
		<author>
			<persName><forename type="first">Gian</forename><surname>Wiher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Cotterell</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2203.15721</idno>
		<idno>CoRR, abs/2203.15721</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the predictive power of neural language models for human realtime comprehension behavior</title>
		<author>
			<persName><forename type="first">Ethan Gotlieb</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Gauthier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Cognitive Science Society</title>
		<meeting>the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Trading off diversity and quality in natural language generation</title>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Duckworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Human Evaluation of NLP Systems</title>
		<meeting>the Workshop on Human Evaluation of NLP Systems</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Human Behavior and the Principle of Least Effort</title>
		<author>
			<persName><forename type="first">George</forename><surname>Kingsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zipf</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1949">1949</date>
			<publisher>Addison-Wesley Press</publisher>
			<pubPlace>Oxford, UK.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
