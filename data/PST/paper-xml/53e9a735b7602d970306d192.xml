<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The many facets of linear programming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2001-10-02">October 2, 2001</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
							<email>miketodd@orie.cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of OR &amp; IE</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14853-3801</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of OR &amp; IE</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<postCode>14853-3801</postCode>
									<settlement>Ithaca</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The many facets of linear programming</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2001-10-02">October 2, 2001</date>
						</imprint>
					</monogr>
					<idno type="MD5">0F8368D14505C78F21C794606E138ED6</idno>
					<idno type="DOI">10.1007/s101070100261</idno>
					<note type="submission">Received: June 22, 2000 / Accepted: April 4, 2001</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We examine the history of linear programming from computational, geometric, and complexity points of view, looking at simplex, ellipsoid, interior-point, and other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>At the last Mathematical Programming Symposium in Lausanne, we celebrated the 50th anniversary of the simplex method. Here, we are at or close to several other anniversaries relating to linear programming: the sixtieth of Kantorovich's 1939 paper on "Mathematical Methods in the Organization and Planning of Production" (and the fortieth of its appearance in the Western literature) <ref type="bibr" target="#b53">[55]</ref>; the fiftieth of the historic 0th Mathematical Programming Symposium which took place in Chicago in 1949 on Activity Analysis of Production and Allocation <ref type="bibr" target="#b62">[64]</ref>; the forty-fifth of Frisch's suggestion of the logarithmic barrier function for linear programming <ref type="bibr" target="#b36">[37]</ref>; the twenty-fifth of the awarding of the 1975 Nobel Prize in Economics to Kantorovich and to Koopmans (and most disappointingly not to Dantzig -see the article <ref type="bibr" target="#b7">[8]</ref> by Balinski for some related history) for their contributions to the theory of optimum allocation of resources; the twentieth anniversaries of <ref type="bibr" target="#b55">Khachiyan's 1979 and</ref><ref type="bibr">1980</ref> papers <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref> using the ellipsoid method to prove polynomiality of the linear programming problem; and the fifteenth of Karmarkar's paper introducing the projective method to again establish polynomiality and reinvigorating the study of interior-point methods to such a remarkable extent.</p><p>Let me start by giving two quotes from the Nobel prizewinners (two of the notable list of individuals with the initial K who have made significant contributions to linear programming, including also Klee, Khachiyan, Karmarkar, and Kalai, who will figure later in this paper) and one from Dantzig:</p><p>Kantorovich writes in the introduction of <ref type="bibr" target="#b53">[55]</ref>: "I want to emphasize again that the greater part of the problems of which I shall speak, relating to the organization and planning of production, are connected specifically with the Soviet system of economy and in the majority of cases do not arise in the economy of a capitalist society." (This was undoubtedly added to make the paper, with its disguised decadent ideas of dual prices, more palatable to his communist censors -and we shall see more instances where the science of linear programming came up against the realities of the Cold War. In this regard, see the comments <ref type="bibr" target="#b23">[24]</ref> on the translated version by Charnes and Cooper and the following rebuttal by Koopmans. See also the article by B.T. Polyak in this volume.)</p><p>A prescient quote from Tjalling Koopmans in the introduction to <ref type="bibr" target="#b62">[64]</ref> reads: "It has been found so far that, for any computation method which seems useful in relation to some set of data, another set of data can be constructed for which that method is obviously unsatisfactory." (This compares strikingly with the quote from Bixby et al. <ref type="bibr" target="#b12">[13]</ref> at the end of this section.)</p><p>In <ref type="bibr" target="#b29">[30]</ref>, Dantzig writes: "Luckily the particular geometry used in my thesis was the one associated with the columns of the matrix instead of its rows. This column geometry gave me the insight which led me to believe that the simplex method would be an efficient solution technique. I earlier had rejected the method when I viewed it in the row geometry because running around the outside edges seemed so unpromising."</p><p>Since much has been written about the early history (and pre-history) of linear programming, for example in <ref type="bibr" target="#b28">[29]</ref>, Chap. 2, <ref type="bibr" target="#b29">[30]</ref>, and <ref type="bibr" target="#b81">[83]</ref>, pp. 209-225, this paper will concentrate more on developments since the seventies. I hope to intrigue the reader enough to investigate some of the byways and alleys associated with linear programming as well as the more well-travelled highways. We will look at simplex, ellipsoid, and interior-point methods, and also at least mention some other approaches. Of course, I hope the reader will forgive my personal bias in the topics selected. (Let me mention here Megiddo's article <ref type="bibr" target="#b73">[75]</ref>, which also surveys some recent developments from a different viewpoint.)</p><p>Following the development of the simplex method in 1947 <ref type="bibr" target="#b26">[27]</ref>, the '50s had been the decade of developing the theoretical underpinnings of linear programming, of extending its applicability in industrial settings and to certain combinatorial problems, and of the first general-purpose codes. The '60s saw the emergence of large-scale linear programming, of exploitation of special structure (again pioneered by Dantzig and Dantzig-Wolfe in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref>), and of extensions to quadratic programming and linear complementarity. If the '50s and the '60s were the decades of unbridled enthusiasm, the '70s were the decade of doubt, as the theory of computational complexity was developed and Klee and Minty <ref type="bibr" target="#b58">[60]</ref> showed that the simplex method with a common pivot rule was of exponential complexity. We will concentrate on the developments since that time; hope has been restored by new polynomial-time algorithms, by bounds on the expected number of pivot steps, and by amazing computational studies on problems with numbers of variables ranging up to the millions.</p><p>Linear programming studies the optimization of a linear function over a feasible set defined by linear inequalities, hence a polyhedron. The problem is in some sense trivial, since it is only necessary to examine a finite number of vertices (and possibly edges), but if one is interested in efficient computation, the topic is wonderfully rich and has been the subject of numerous surprising new insights.</p><p>A geometric view can be at times helpful (but also profoundly misleading, as mentioned by Dantzig on several occasions). Let us consider two paradigms: optimization of a linear function over a simplex or over an ellipsoid. In the first case the feasible region is "spiky"; there are few vertices, and any one can be reached from any other in one step: here the simplex method is a natural choice. In the second case, an optimal solution can easily be found by calculus, but there are an infinite number of extreme points. Are the feasible regions of large-scale instances arising in important problem domains more like "quartz crystals" [51], with long edges from one side to the other, so that combinatorial algorithms will be efficient, or more like "disco balls" <ref type="bibr" target="#b49">[50]</ref>, where simplex-like methods seem doomed to be slow (as in Dantzig's earlier intuition mentioned above) but approximations by ellipsoids as in the ellipsoid method or interior-point methods look promising? The remarkable success of algorithms from both classes suggests that real problems have both features.</p><p>After a short section setting our notation, the subsequent sections consider the simplex method, the ellipsoid method, interior-point methods, and other approaches. In each section we start with an overview and historical comments, and then provide some more technical material relating to understanding the methods better or giving relations between them. I have of course omitted much that is highly important (everything concerned with efficient computation and exploitation of special structure, for example!), but I hope to have mentioned some avenues that are new to readers. The paper concludes with some very brief remarks on the future.</p><p>Let me close this section with an anecdote: One day in April, I experienced an interesting time warp. I read a paper by <ref type="bibr">Hoffman et al. from 1953 [49]</ref>, discussing computational experiments comparing three methods for linear programming problems and computing optimal strategies for zero-sum games (the simplex method, fictitious play, and the relaxation method); the authors concluded that the simplex method was most effective, and could even solve large-scale problems with dimensions of the order of 50 by 100, using the SEAC (Standards Eastern Automatic Computer)! (Also see Hoffman's recollections of this <ref type="bibr" target="#b47">[48]</ref>.) I later attended a seminar by Bob Bixby, also recounting computational experience in solving linear programming problems, and giving results for a particular problem of size 49,944 by 177,628. The contrast was quite striking. Incidentally, Bixby was also comparing three methods (primal and dual simplex methods and a primal-dual interior-point method), and his conclusion was that, for a suite of large problems, the dual simplex method using the steepest-edge pivot rule was the fastest (see <ref type="bibr" target="#b12">[13]</ref>). Bixby and his co-authors indicate that the figures given can be viewed as biased against the interior-point (barrier) code, but state: "What can one say in general about the best way to solve large models? Which algorithm is best? If this question had been asked in 1998, our response would have been that barrier was clearly best for large models. If that question were asked now, our response would be that there is no clear, best algorithm. Each of primal, dual, and barrier is superior in a significant number of important instances."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We consider a primal-dual pair of linear programming problems:</p><formula xml:id="formula_0">(P) minimize c T x subject to Ax = b, x ≥ 0, (D) maximize b T y subject to A T y ≤ c,</formula><p>where c ∈ R n , A ∈ R m×n and b ∈ R m are given data, and x ∈ R n and y ∈ R m are the decision vectors. For simplicity of argument, we assume that the matrix A has full row rank m and that both problems have feasible solutions. Then the feasible region of (P) is the intersection of a d := n-m dimensional affine subspace with the nonnegative orthant in R n , while the feasible region of (D) is the intersection of n halfspaces in R m . Both are polyhedra with vertices, by virtue of our assumption on A. Some of our arguments are most easily expressed in one formulation or the other, but it is easy to switch between them: the d-dimensional affine subspace can be parametrized by d variables, and then the feasible region of (P) is the intersection of n halfspaces in R d ; and the dual slack vector s := c -A T y lies in an m-dimensional affine subspace and is also required to be nonnegative.</p><p>Both of these viewpoints consider the geometry of the set of feasible solutions, but another perspective, the column geometry of Dantzig <ref type="bibr" target="#b28">[29]</ref>, pp. 160 ff., is also very valuable: indeed, without this perspective, the simplex method might have been stillborn (see <ref type="bibr" target="#b29">[30]</ref>). Here we consider the convex cone C spanned by the vectors (a j ; c j ) in R m+1 , where a j is the jth column of A and c j the jth component of c. If the objective function is not constant on the feasible region, this cone will have dimension m + 1. We seek the "lowest" point (in terms of its last component) that lies in the intersection of this cone and the "vertical" line</p><formula xml:id="formula_1">{(b; ζ) : ζ ∈ R}.</formula><p>Different solution strategies for linear programming arise from different views of these geometries. Since we are optimizing a linear function over a polyhedron with vertices, an optimal solution (if it exists) will occur at a vertex; it is therefore natural to consider only the vertices and the adjacency between them given by the edges of the polyhedron. This is the viewpoint of the simplex method, and it relies on considering the combinatorial structure of the faces of polyhedra. In terms of the column geometry, we consider the set of simplicial subcones of dimension (m + 1) of the cone C described above that intersect the vertical line, with two cones adjacent if they share an m-face. This simplex interpretation is described in <ref type="bibr" target="#b26">[27]</ref>, in <ref type="bibr" target="#b48">[49]</ref>, and in more detail in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Other methods instead view the feasible region as a convex set, which can be approximated by simpler convex sets with smooth boundaries over which linear optimization is trivial. Then the focus is on obtaining "good" approximations to the feasible polyhedron by suitable simpler sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The simplex method</head><p>For definiteness we consider the primal simplex method, which moves from vertex to vertex of the primal feasible region until it reaches an optimal solution (or gets an indication of unboundedness). Each vertex corresponds to a basic feasible solution and hence to a choice of m basic variables from the n variables in total.</p><p>It is clear that, with hindsight, we can move from the initial vertex to the final vertex in at most m basic exchanges, at each step replacing one of the initial basic variables by one of the final basic variables. However, there is no reason why the intermediate basic solutions should be feasible. Moreover, the simplex method cannot use hindsight, and indeed uses only local information at the current vertex. It is therefore worth emphasizing the Remarkable fact: the (primal) simplex method typically requires at most 2m to 3m pivots to attain optimality. This serendipitous property was recognized quite early (something similar is mentioned by Hoffman et al. in <ref type="bibr" target="#b48">[49]</ref> in 1953), and was stated in Dantzig <ref type="bibr" target="#b28">[29]</ref>, page 160, as based on empirical experience with thousands of practical problems. Numerical evidence can be found in Wolfe and Cutler <ref type="bibr" target="#b94">[96]</ref> and <ref type="bibr">Kuhn and Quandt [66]</ref>. This experience was perhaps a factor in discouraging the development of other algorithms from the late '50s and '60s, although some alternatives continued to be advanced. Of course, the dual simplex method of Lemke <ref type="bibr" target="#b67">[69]</ref> was of great importance, both for reoptimization following a change in the right-hand sides and to solve problems from scratch (indeed the dual steepest-edge simplex method studied and made practical by Forrest and Goldfarb <ref type="bibr" target="#b35">[36]</ref> seems to be the simplex method of choice today), but for our purposes here it can be viewed as the primal simplex method applied to the dual problem. More recent evidence for the claim comes from computational experiments carried out to demonstrate the competitive behavior of the simplex method following the great excitement and provocative computational results of interior-point methods. For example, Bixby <ref type="bibr" target="#b10">[11]</ref> gives results for an early version of CPLEX (1.0) on 90 Netlib <ref type="bibr" target="#b38">[39]</ref> problems. For 72, the number of total iterations was at most 3 times the row size; for 16, the ratio was between 3 and 7; and for the remaining three, it was 10.7, 39.5 and 469.1. (The last three had "unbalanced" m and n: their m × n sizes were 1000 x 8806, 24 x 1026, and 25 x 10,500 respectively; here m denotes the number of general linear constraints, while the last two problems had both upper and lower bounds on all the variables, so that perhaps m should be taken as 1050 and 10525, and then the ratios are under 1.2.) On a set of 8 larger problems considered in <ref type="bibr" target="#b11">[12]</ref>, the ratios for CPLEX 2.2 (primal) were from .46 to 1.99 on three, from 4.52 to 9.02 on four, and 17.58 on the last. Note that these problems (especially the largest) were chosen to present difficulties for linear programming algorithms; their general favorable performance is then even more surprising.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Diameter</head><p>One way to try to explain the success of the simplex method is to study the diameter of a polyhedron, the largest number of edges in a shortest path joining two of its vertices. Let (d, n) denote the largest diameter of a d-dimensional polyhedron with n facets; this represents the best-possible number of iterations for the worst linear programming problem of these dimensions, initiated at the worst vertex. W.M. Hirsch conjectured in 1957 that (d, n) ≤ nd (so that m steps would be sufficient in the remarkable fact above), see Dantzig <ref type="bibr" target="#b28">[29]</ref>, pp. 160. It is known that this bound fails for unbounded polyhedra (Klee and Walkup <ref type="bibr" target="#b59">[61]</ref>), and it also fails for bounded polyhedra (polytopes) if the path is required to be monotonic with respect to the objective function <ref type="bibr" target="#b85">[87]</ref>. The general case for polytopes is still open.</p><p>The best bounds available are (d, n) ≤ 2 d-3 n due to Larman <ref type="bibr" target="#b65">[67]</ref>, and (d, n) ≤ n 1+log d due to Kalai and Kleitman <ref type="bibr" target="#b52">[54]</ref>. The Hirsch conjecture holds for 0-1 polytopes (Naddef <ref type="bibr" target="#b75">[77]</ref>) and for dual transportation polyhedra (Balinski <ref type="bibr" target="#b6">[7]</ref>); and the diameters of certain combinatorial polytopes (e.g., the assignment polytope and the asymmetric traveling salesman polytope) are just 2 (Balas and Padberg <ref type="bibr" target="#b5">[6]</ref>, Padberg and Rao <ref type="bibr" target="#b77">[79]</ref>). Klee and Kleinschmidt <ref type="bibr" target="#b57">[59]</ref> give an excellent survey on these matters, and Kleinschmidt <ref type="bibr" target="#b60">[62]</ref> provides an update and some algorithmic consequences. (Note added in proof: see also Amenta and Ziegler [102].)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Exponential and subexponential pivot rules</head><p>Klee and Minty <ref type="bibr" target="#b58">[60]</ref> were the first to give an example of a class of linear programming problems for which Dantzig's classic most-negative-reduced-cost pivot rule required an exponential number of pivots. Since then exponential examples have been found for several other pivot rules, including the best-neighbor rule which solves linear optimization over the simplex (and the original Klee-Minty examples) in just one pivot, and the steepest-edge rule which is the basis for the fast simplex codes of today.</p><p>It is therefore surprising that subexponential (but superpolynomial) pivot rules have been found, by Kalai <ref type="bibr" target="#b50">[52]</ref> and Matousek, Sharir, and Welzl <ref type="bibr" target="#b71">[73]</ref>. These are all (thus far) randomized, with a best bound on the expected number of pivots of exp(K √ d log n) for some constant K . One version is particularly easy to describe (roughly):</p><p>• Given a vertex v, choose a facet F containing v at random.</p><p>• Apply the algorithm recursively to find the optimizing vertex w in F.</p><p>• Repeat the algorithm from w.</p><p>The analysis, and much more fascinating material, is in Kalai <ref type="bibr" target="#b51">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Probabilistic analysis</head><p>Instead of randomizing the pivot rule, we can ask for the expected behavior of a deterministic rule on a random linear programming problem drawn from some distribution. This was a topic under intense study in the late '70s and early '80s. Major contributions were made by Borgwardt <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, Smale <ref type="bibr" target="#b83">[85]</ref>, Adler <ref type="bibr" target="#b0">[1]</ref>, Haimovich <ref type="bibr" target="#b46">[47]</ref>, Adler, Karp, and Shamir <ref type="bibr" target="#b1">[2]</ref>, Adler and Megiddo <ref type="bibr" target="#b2">[3]</ref>, and Todd <ref type="bibr" target="#b86">[88]</ref>.</p><p>Borgwardt was the first to obtain a polynomial bound. His final estimate, from <ref type="bibr" target="#b17">[18]</ref>, gives a bound 0(m 3 n 1/(m-1) ) for the expected total number of iterations of a dimensionby-dimension simplex method for the dual problem (D). Here, the data b, a 1 , ..., a n , where a j denotes the jth column of A, are required to come from a rotationally symmetric distribution, and c is the vector of ones. Hence the origin is always feasible, and is used by the algorithm as a starting point. The generated problems are always feasible and have optimal solutions with high probability if n m. Some results have also been obtained for a related parametric method where the c j 's are also random and can be negative <ref type="bibr">(Borgwardt [17]</ref>).</p><p>In contrast, Adler and Megiddo <ref type="bibr" target="#b2">[3]</ref>, Adler, Karp, and Shamir <ref type="bibr" target="#b1">[2]</ref>, and Todd <ref type="bibr" target="#b86">[88]</ref> deal with the so-called sign-invariant model, and obtain a bound of 0(min{d 2 , m 2 }) on the expected total number of iterations to show infeasibility, show unboundedness, or attain optimality for a lexicographic parametric method. One (severe) disadvantage of this probabilistic model is that problems are either infeasible or unbounded with probability approaching 1 as n tends to ∞; however, in the case that n = 2m it is possible to derive a bound of 0(m 5 2 ) expected total iterations with the expectation conditioned on the problem's having an optimal solution.</p><p>For a survey of these and related results, see the 0th chapter of Borgwardt's book <ref type="bibr" target="#b15">[16]</ref> or the last section of <ref type="bibr" target="#b17">[18]</ref>. There is also a discussion of these issues in <ref type="bibr" target="#b88">[90]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Big faces, little faces, long edges</head><p>To conclude this section we describe some results that try to give some intuition as to why the simplex method works so well.</p><p>First, there may be a few "big" faces on which most of the action takes place. Kuhn <ref type="bibr" target="#b63">[65]</ref> did some experiments in 1953 in which he "shot" ten random bullets from the centroid of the asymmetric travelling salesman polytope on 5 cities, finding which facet each penetrated. Each shot led to a 21 x 25 LP problem which was (just) within the capability of the SEAC, and each went through a trivial (nonnegativity) facet. In 1991, he repeated the experiment on a larger scale. Of over 150,000 shots, 80% went through these trivial facets, which numbered 20 out of the 390 facets in all.</p><p>Of course, polytopes with a relatively small number of vertices can have a large number of facets. The symmetric travelling salesman polytope on 7 cities has 360 vertices and 3,437 facets: for 8 cities, the figures are 2,520 vertices and 194,187 facets (Christof et al. <ref type="bibr" target="#b24">[25]</ref>). How many of these are "significant"? Goemans <ref type="bibr" target="#b40">[41]</ref> showed that several classes of facets for the graphical travelling salesman problem had a very small effect on the optimal value when their inequalities were added to an LP relaxation. We can infer that perhaps many of these facets are "small." By polarity, these results have implications for the vertices of polytopes with a relatively small number of facets (i.e., the feasible region of LP problems). There may be many vertices: the d-cube has 2d facets and 2 d vertices, and a polytope with n = 2d can have as many as (d d 2 ) vertices. But many of these vertices may be optimal for a very small set of objective functions, and may be relevant for a very small set of simplex algorithms.</p><p>Lastly, let us briefly consider long edges. It has been known for a long time that in dimension d ≥ 4 there are neighborly polytopes (every pair of vertices is joined by an edge) with arbitrarily many vertices <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b98">100]</ref>. Clearly such polytopes are not simple (each vertex has degree much larger than the dimension) unless they are simplices, but at least there is some basis representation of the initial vertex such that a single pivot reaches the optimal vertex. As mentioned above, there are classes of polyhedra arising in practice whose diameters are two. To complement these results I ran a very simple experiment to suggest that many polyhedra had "long edges" going from one side of the polyhedron to the other. For d = 3, 4, ..., 11, and n = 2 d and 2 d+1 , I generated 100 polytopes as the convex hull of n d-vectors. For the first type, each component of each vector was independently drawn from the standard normal distribution. For the second type, the resulting vectors were each scaled independently to have length the reciprocal of a number drawn uniformly at random from [0,1]. For the first type of polytope, at least half of the vectors were in fact vertices. I took the two maximally distant vertices and checked whether they were joined by an edge: this was true in between 3% and 49% of the cases, depending on the dimension d. For the second type, only a few of the vectors were vertices (only an average of 46 to 48 for the problems of dimension 10), but from 67% to 100% of the polytopes (and from 99% to 100% for those with d ≥ 7) had their maximally-distant vertices joined by an edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The ellipsoid method</head><p>The ellipsoid method was originally developed by Yudin and Nemirovski <ref type="bibr" target="#b97">[99]</ref> and Shor <ref type="bibr" target="#b82">[84]</ref> for convex nonlinear programming, but it become famous when Khachiyan <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b56">58]</ref> used it to obtain a polynomial-time algorithm for linear programming. This was a major theoretical advance, but the popular press misinterpreted it in a rather ludicrous way (with headlines like "Soviet Answer to 'Travelling Salesmen' "; see Lawler's article <ref type="bibr" target="#b66">[68]</ref>); later headlines retreated from their earlier excesses by invoking cold-war suspicions (as in the New York Times's "A Russian's Solution in Math Questioned," 21 March 1980, p. A13).</p><p>For a problem with n inequalities and integral data with total bit size L, the method generates an approximate solution from which an exact solution can easily be obtained in 0(n 2 L) iterations, requiring 0(n 4 L) arithmetic operations on numbers with 0(L) digits. (These bounds appear in Khachiyan <ref type="bibr" target="#b56">[58]</ref>; his earlier extended abstract <ref type="bibr" target="#b55">[57]</ref> gave a bound of 0(n 5 L) arithmetic operations on numbers with 0(nL) digits.) This gives a polynomial bound, and thus Khachiyan was the first to show that linear programming was in the class P of polynomial-time solvable problems. Nevertheless, the performance in practice was typically very close to its worst-case bound, so that despite a number of refinements the method is not competitive for linear programming; for example, see the discussion in Bland et al. <ref type="bibr" target="#b13">[14]</ref>.</p><p>The basic idea of the ellipsoid method is well-known. At each iteration an ellipsoid is given which contains all optimal solutions. By considering the center of the ellipsoid, a hyperplane is constructed so that all optimal solutions lie on one side of the hyperplane (and the center either lies strictly on the other side (a deep cut) or on the hyperplane itself (a central cut)). Then a new ellipsoid is found which contains all points in the old ellipsoid and on the correct side of the hyperplane.</p><p>We will give some formulae below (as well as a new interpretation of the process). But first, let us mention that from this geometric view it appears that the method is quite insensitive to the number of inequalities, and seems likely to work well when the feasible region of the linear programming problem is close to a "disco ball" (as opposed to a "quartz crystal") -in this way it seems to complement the simplex method. Also, as long as a suitable hyperplane can be found, there is no need for the problem to be of linear programming type. Hence it can be used for convex programming, and is also highly useful for theoretically analyzing combinatorial optimization problems via the polyhedral combinatorics paradigm. This leads to the famous "separation = optimization" meta-theorem: if you can separate efficiently, you can optimize efficiently (over a suitable collection of convex bodies). We will not pursue this, as it falls outside the scope of this article, but instead refer the reader to the excellent monograph by Grötschel, Lovász, and Schrijver <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Update formulae</head><p>Now we briefly discuss the mechanics of an iteration, in order to describe the new interpretation. For simplicity, we assume that we are just trying to find a feasible point for (D), i.e., a point in Y := {y : A T y ≤ c}. We write the individual constraints as a T j y ≤ c j , j = 1, 2, ..., n, and assume that Y ⊆ E 0 , where</p><formula xml:id="formula_2">E 0 := {y : y 2 ≤ R}. (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>Any ellipsoid in m can be written in the form</p><formula xml:id="formula_4">E = E(ȳ, B) = y : (y -ȳ) T B -1 (y -ȳ) ≤ 1 , (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where ȳ is its center and B is a symmetric positive definite matrix of order m. Indeed, E 0 = E(y 0 , B 0 ), with y 0 = 0 and</p><formula xml:id="formula_6">B 0 = R 2 I. Given E k = E(y k , B k ), E k+1 = E(y k+1 , B k+1</formula><p>) can be constructed as follows:</p><p>Find j so that a T j y k &gt; c j (if none, STOP:</p><formula xml:id="formula_7">y k ∈ Y ). Set y k+1 := y k - τB k a j a T j B k a j 1 2 , (<label>3</label></formula><formula xml:id="formula_8">)</formula><formula xml:id="formula_9">B k+1 = δ B k -σ B k a j a T j B k a T j B k a j ,<label>(4)</label></formula><p>where τ = 1/(m + 1), δ = m 2 /(m 2 -1), and σ = 2/(m + 1). This gives E k+1 as the ellipsoid of minimum volume that contains the semi-ellipsoid {y ∈ E k : a T j y ≤ a T j y k }; if the minimum-volume ellipsoid containing {y ∈ E k : a T j y ≤ c j } is desired, the formulae are similar with different choices for τ, δ, and σ (deep cuts). It can be shown that vol</p><formula xml:id="formula_10">(E k+1 )/vol(E k ) ≤ exp(-1/[2m + 2]</formula><p>), and this systematic volume reduction leads to the complexity bound: see, e.g., Bland et al. <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">An alternate representation</head><p>There is another way to represent the ellipsoid that leads to a surprising parallel with interior-point methods (see also <ref type="bibr" target="#b87">[89]</ref>). Since we assume that Y ⊆ E 0 , we can find lower bounds on a T j y for y ∈ Y for each j. So suppose Y = {y : ≤ A T y ≤ c}. Now let D be a nonnegative diagonal matrix. Then since A T y -≥ 0 and</p><formula xml:id="formula_11">A T y -c ≤ 0 for all y ∈ Y, Y ⊆ Ē(D, ) := y : A T y - T D A T y -c ≤ 0 , (<label>5</label></formula><formula xml:id="formula_12">)</formula><p>and the set on the right-hand side is an ellipsoid as long as ADA T is positive definite. The advantage of this representation is that it gives a short certificate that E = Ē(D, ) contains Y . (The disadvantage is that it can only be used with linear programming and not with convex or combinatorial optimization where the constraints are not pre-specified.)</p><p>From this viewpoint, the ellipsoid method generates a sequence E k = Ē(D k , k ) of ellipsoids containing Y . The center of E k is y k , the solution of</p><formula xml:id="formula_13">AD k A T y = AD k ( k + c)/2, (<label>6</label></formula><formula xml:id="formula_14">)</formula><p>or equivalently of the weighted least-squares problem</p><formula xml:id="formula_15">D 1 2 k A T y ≈ D 1 2 k ( k + c)/2. (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>At the kth iteration, the index j of a violated constraint is found, the jth component of the vector k is possibly updated, and the jth diagonal entry of the matrix D k is increased. Since only one entry of D k is changed, y k can be updated cheaply (the update is exactly that given in (3)), as can B k = (AD k A T ) -1 . Details can be found in Burrell and Todd <ref type="bibr" target="#b21">[22]</ref>. That paper also shows how k is updated (each component is guaranteed to be a suitable lower bound by an application of the Farkas lemma).</p><p>Besides showing that the quadratic inequality defining each ellipsoid can be viewed as a weighted sum of quadratic constraints that ensure that each a T j y lies in an appropriate range, this representation gives a clue to the slow convergence of the ellipsoid method. Suppose the lower-bound vector remains unchanged. Then the volume of Ē(D, ) is a function of just the diagonal entries of D, and the ellipsoid method can be thought of as a coordinate descent method to minimize this nonlinear function. It is known that coordinate descent methods can be very slow, and the ellipsoid method is no exception, although dramatic volume reductions are possible at some iterations. (See also Liao and Todd <ref type="bibr" target="#b69">[71]</ref>.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Interior-point methods</head><p>The idea of moving through the interior of the feasible region goes back at least to Frisch in 1955 <ref type="bibr" target="#b36">[37]</ref>, who proposed using a logarithmic barrier function. Barrier and penalty methods for nonlinear programming were studied in depth in the 1960s, leading to the classic text of Fiacco and McCormick <ref type="bibr" target="#b34">[35]</ref>, but came into disfavor as their numerical drawbacks became more recognized. The modern reincarnation of these methods is due to Karmarkar <ref type="bibr" target="#b54">[56]</ref> in 1984, who established the polynomial-time boundedness of the projective method and also obtained some very competitive times (comparing with Fortran implementations of the simplex method) on a widely-used set of test problems. It turns out that the interior-point method implemented by Karmarkar (the affine-scaling method), besides being discovered simultaneously by a number of researchers in the mid 1980s, had in fact been proposed in 1967 and analyzed in 1974 by I.I. Dikin, a student of Kantorovich <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Karmarkar's results and claims led to a furor of activity, though the popular press was much more accurate this time (viz. the front-page New York Times article "Breakthrough in Problem Solving" of Monday, <ref type="bibr">November 19, 1984</ref>, written by James Gleick of chaos fame). The complexity bound was only slightly better than that of the ellipsoid method (0(n 3.5 L) arithmetic operations on a problem with n inequalities and integer data of total bit length L, although in practice the behavior is much better). However, the new ideas employed were very intriguing: at each iteration a projective transformation was used to bring the current iterate into the center of the feasible region, and a nonlinear potential function, invariant under such transformations, was used to measure progress. This potential function is close to Frisch's logarithmic barrier, and Gill, Murray, Saunders and Wright <ref type="bibr" target="#b39">[40]</ref> showed that Karmarkar's search direction in fact is equivalent to that arising from a barrier function method with a suitable choice of barrier parameter (sometimes negative!) at each iteration.</p><p>The idea of making a projective transformation is to bring the current iterate to a point far from the constraints so that a steepest descent step (on the transformed objective function or the potential function) will give good decrease. However, projective transformations are not used much in interior-point methods nowadays. The key concept of making a transformation or changing the metric so the current iterate is in some sense far from the boundary remains highly valuable. I will discuss briefly dual and primal-dual path-following methods, and then make some remarks about potentialreduction algorithms. For more details, I recommend Gonzaga <ref type="bibr" target="#b43">[44]</ref> and Wright <ref type="bibr" target="#b95">[97]</ref> for path-following methods and Anstreicher <ref type="bibr" target="#b4">[5]</ref> and Todd <ref type="bibr" target="#b89">[91]</ref> for potential-reduction algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dual path-following</head><p>Suppose we have a current strictly feasible solution ȳ to the dual problem max{b T y : A T y ≤ c}; strictly feasible means that the dual slack vector s := c -A T ȳ is positive in each component (written s &gt; 0). The largest ellipsoid around s that is contained in the nonnegative orthant is E s := {s ∈ n : (s -s) T S-2 (s -s) ≤ 1}, where S is the diagonal matrix with the components of s on its diagonal. Thus the set of feasible dual solutions whose slack vectors lie in E s is</p><formula xml:id="formula_17">E := {y ∈ m : (y -ȳ) T A S-2 A T (y -ȳ) ≤ 1},</formula><p>and E ⊆ Y := {y ∈ m : A T y ≤ c}. Note that here we have a point ȳ that lies in Y , and E is inscribed in Y rather than circumscribing it, but that otherwise the ellipsoid E is remarkably similar to Ē(D, ) in ( <ref type="formula" target="#formula_11">5</ref>).</p><p>In addition to the fact that E ⊆ Y , the matrix A S-2 A T appearing therein defines a highly useful local metric at ȳ. Note that this matrix arises as the Hessian of the logarithmic barrier function f(y) :=j ln c -A T y j <ref type="bibr" target="#b7">(8)</ref> evaluated at ȳ. This is a special case of a very general theory of self-concordant barrier functions developed by Nesterov and Nemirovski: see their monograph <ref type="bibr" target="#b76">[78]</ref>. The knowledge of such a barrier function and its derivatives for a convex set is a sufficient condition for devising theoretically efficient algorithms for optimizing a linear function over the set. For any such barrier, the ball of radius 1 at any point defined by the local norm given by the Hessian of the barrier function is always contained in the set. At the analytic center, the point that minimizes the barrier (assuming it exists), a corresponding ball of radius a constant times the so-called complexity value of the barrier (n for the function f above) contains the convex set. Hence the metric defined by the local norm gives a very useful approximation to the local geometry of the set. For our interests, this gives a differential geometry view of a polyhedron as compared to the combinatorial geometry viewpoint of the simplex method.</p><p>Let us now return to our strictly feasible point ȳ and the metric defined by the matrix A S-2 A T . With respect to this metric, the steepest ascent direction for the objective function is</p><formula xml:id="formula_18">d AFF := A S-2 A T -1 b<label>(9)</label></formula><p>(the affine-scaling direction), while the steepest descent direction for the logarithmic barrier function is</p><formula xml:id="formula_19">d CEN := -A S-2 A T -1 As -1<label>(10)</label></formula><p>(the centering direction), where s-1 denotes the vector whose components are the reciprocals of those of s. Dual interior-point methods generally choose as search direction a linear combination of these two directions. Strictly feasible points ȳ where these two directions are diametrically opposed (so that µAs -1 = b for some positive µ) lie on the so-called dual central path. Such points maximize the dual penalized function b T y -µ f(y) over strictly feasible y. Note that then x := µs -1 is a strictly feasible point for (P), and it is not hard to see that x minimizes the primal penalized function c T x -µ j ln x j over strictly feasible x; x is then a point on the primal central path. Together x, ȳ, and s solve</p><formula xml:id="formula_20">A T y + s = c, s &gt; 0, Ax = b, x &gt; 0, x • s = µe,<label>(11)</label></formula><p>where x • s is the vector of component-wise products of x and s and e is the n-vector of ones.</p><p>Note that points satisfying <ref type="bibr" target="#b10">(11)</ref> are strictly feasible, with duality gap</p><formula xml:id="formula_21">c T x -b T y = x T s = nµ.</formula><p>So as µ tends to zero from above, points on the central paths converge to optimal solutions (this requires additional work to show that limits exist). It thus makes sense to approximately follow the central path(s) as µ decreases.</p><p>We say that ȳ is close to the dual central path if for some µ &gt; 0, the Newton step for the penalized function,</p><formula xml:id="formula_22">µ -1 d AFF + d CEN , is below some tolerance δ in the local norm, i.e., b -µAs -1 T A S-2 A T -1 b -µAs -1 1 2 ≤ δµ.</formula><p>We then try to find such a point for a smaller value of µ, say σµ for 0 &lt; σ &lt; 1, by taking a Newton step. It turns out that we can choose σ as 1 -O(1/ √ n) and guarantee that we stay close to the central path. Iterating this procedure gives an O( √ n ln(1/ε))iteration algorithm to obtain an ε-optimal solution. Such methods were first developed by Renegar <ref type="bibr" target="#b78">[80]</ref>.</p><p>Note that every iteration requires the solution of a linear system with coefficient matrix A S-2 A T . This compares nicely with the ellipsoid method (see ( <ref type="formula" target="#formula_13">6</ref>)) except that now every diagonal entry of S-2 changes, whereas only one entry of D k changed before. To compensate, far fewer iterations are typically necessary in practical interior-point methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Primal-dual path-following</head><p>We have gone into some detail concerning dual path-following methods, in order to highlight their motivation from a special local metric defined on the interior of the dual feasible region with very attractive properties. We shall be much more brief here.</p><p>Of course, we can define primal path-following methods by following the arguments of the previous subsection: these require the solution of a linear system with coefficient matrix A X2 A T at each iteration, where X is the diagonal matrix containing the components of the current iterate x.</p><p>Primal-dual methods are usually motivated by considering the system of nonlinear equalities <ref type="bibr" target="#b10">(11)</ref>. Given a strictly feasible triple (x, ȳ, s) that is close to the central path (i.e., the Euclidean norm of µ -1 x • se is small), we move in the direction of the Newton step for <ref type="bibr" target="#b10">(11)</ref> with µ replaced by σµ. These methods are admirably discussed and analyzed in Gonzaga <ref type="bibr" target="#b43">[44]</ref> and Wright <ref type="bibr" target="#b95">[97]</ref>.</p><p>There is a another way we can view these methods. If we wanted to simultaneously increase the dual penalized function and decrease the primal penalized function, we could solve two linear systems with coefficient matrices A S-2 A T and A X2 A T . These can be viewed as Newton steps or alternatively as steepest descent steps with respect to the local norms. To avoid the work of solving two linear systems, we can use steepest descent with respect to the local norm at a point intermediate between x and s-1 , their geometric mean x 1 2 s-1 2 (interpreted component-wise). This leads to requiring the solution of just a single linear system to obtain the steepest descent directions for both penalized functions, and the resulting search directions are exactly those found by the primal-dual Newton approach of the previous paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Potential-reduction methods</head><p>Karmarkar used the primal function</p><formula xml:id="formula_23">φ P (x) := n ln c T x -z * - j ln x j ,</formula><p>where z * is the known optimal value of (P), to monitor progress of his projective algorithm <ref type="bibr" target="#b54">[56]</ref>, and showed that this function could be decreased by a constant at each iteration. Later research removed the assumption that the optimal value is known, but it seems better to use instead the primal-dual potential function PD (x, y) := (n + ρ) ln c T xb T yj ln x jj ln c -A T y j , defined for strictly feasible pairs, where ρ is a nonnegative parameter. The latter function was introduced independently by Tanabe <ref type="bibr" target="#b84">[86]</ref> and by Todd and Ye <ref type="bibr" target="#b90">[92]</ref>; <ref type="bibr" target="#b90">[92]</ref> (in a restricted setting) and then Ye <ref type="bibr" target="#b96">[98]</ref> and Kojima, Mizuno, and Yoshise <ref type="bibr" target="#b61">[63]</ref> proved that, as long as ρ ≥ √ n, it can be reduced by a constant at each iteration. In both primal and primal-dual guises, this constant reduction suffices to obtain polynomial-time bounds on the number of iterations required to obtain ε-optimal solutions, of O(n ln( 1 ε )) and O(ρ ln( 1 ε )) respectively. Hence the primal-dual method can attain the same bound as path-following methods by choosing ρ = θ( √ n). More significantly, these bounds can be achieved without any restrictions on the iterates remaining close to the central path: as long as a sufficient reduction in the potential function is maintained, the iteration bound stays valid.</p><p>The symmetric primal-dual search directions used by Kojima et al., while motivated by scaled steepest descent for the potential function, turn out to coincide with those arising from the path-following approach with σ = n/(n+ρ).</p><formula xml:id="formula_24">Thus choosing ρ = θ( √ n) corresponds to choosing σ = 1 -θ(1/ √ n).</formula><p>Modern implementation of interior-point methods are usually called path-following variants, although there is rarely an attempt to maintain the iterates in any neighborhood of the central path. The parameter σ is typically chosen adaptively, often very close to zero. Apart from the adaptive choice of σ, these methods could just as easily be viewed as potential-reduction methods which do not check that the potential function is actually reduced.</p><p>As a final remark on potential-reduction methods, note that, for the more general area of semidefinite programming, Benson et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> have shown that dual potential-reduction methods can exploit the structure of certain classes of problems more effectively than path-following methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Exponential gaps</head><p>To conclude this section, I want to point out an interesting parallel between worst-case and typical behaviors of the simplex method and interior-point methods. As I have indicated, the best bound we have on the number of steps of the (deterministic) simplex method grows exponentially with the dimension, while its usual behavior is linear. Moreover, there are examples showing that the gap is real: for many simplex pivot rules, there are examples where the number of pivots required is exponential.</p><p>For interior-point methods, the best bounds are polynomial in the dimension, growing with n or √ n. Due to the cost of each iteration, such growth would be disastrous in practical cases, and thus it is fortunate that the observed growth is much slowerperhaps logarithmic in the dimension (see Lustig et al. <ref type="bibr" target="#b70">[72]</ref>). This is again an exponential gap! The question arises as to whether it is "real," or just an artefact of our inability to prove a better bound. The answer is that the gap is indeed real: for many practical interior-point methods, there are examples showing that the number of iterations grows at least as fast as (n 1 3 ); see Todd and Ye <ref type="bibr" target="#b91">[93]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Other methods</head><p>Here we collect some miscellaneous methods that have been proposed for linear programming, and provide some references. Despite some encouraging results quoted in some of these papers (often on small random instances), none is regarded as competitive with simplex or interior-point methods (but we should note the excellent results of De Leone and Mangasarian <ref type="bibr" target="#b31">[32]</ref> using an SOR (successive over-relaxation) method on very large sparse random problems in 1987).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Gradient-like methods</head><p>These try to follow projected gradient directions from one feasible point to another. Since this seems like such a natural strategy, and apparently so obviously superior to edge-following methods, such methods have been proposed numerous times in slightly varying guises. Indeed, Brown and Koopmans discussed such a method <ref type="bibr" target="#b19">[20]</ref> in the same volume that contained Dantzig's initial publication <ref type="bibr" target="#b26">[27]</ref>. Later references include Zoutendijk's method of feasible directions ([101], Chap. 9), Rosen's projected gradient method <ref type="bibr" target="#b80">[82]</ref>, the constrained gradient method of J.K. Thurber and Lemke <ref type="bibr" target="#b68">[70]</ref>, and Chang and Murty's steepest descent gravitational method <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Fictitious play</head><p>An iterative method to find optimal strategies and the value of 2-person zero-sum games was devised by Brown <ref type="bibr" target="#b18">[19]</ref> and shown to be convergent by <ref type="bibr">Robinson [81]</ref>. This method is iterative, and at each stage selects a strategy for each player that is a best response to the empirical mixed strategy exhibited by the previous choices of his or her opponent. Convergence is slow, but the method is very simple to implement. Since linear programming problems can be reduced to the solution of (symmetric) games, this provides a method for general LP problems also.</p><p>The idea behind the method can also be based on differential equations, and Brown and von Neumann prove the existence of the value and of optimal strategies based on a system of differential equations in <ref type="bibr" target="#b20">[21]</ref>. The proof is semi-constructive, in that no particular technique for solving the resulting initial value problem is mentioned. Apparently the paper is based on results obtained independently by the two authors. This similarity in the approaches is perhaps the reason that Dantzig states in <ref type="bibr" target="#b29">[30]</ref> that Hoffman et al. <ref type="bibr" target="#b48">[49]</ref> compared a scheme of von Neumann to the simplex method and relaxation methods, whereas their comparison is of Brown's fictitious play method.</p><p>A much-cited note of von Neumann, in which he proposed another method for linear programming soon after Dantzig visited him in 1947, eventually appeared in his collected works <ref type="bibr" target="#b93">[95]</ref>. Again, a system of differential equations is proposed, but the details of a specific algorithm are not given. The approaches of Brown and of von Neumann are very similar, but while Brown adjusts the weight on just one strategy (and changes the others proportionately), von Neumann adjusts the weights on many of the strategies based on a residual vector.</p><p>Finally, a more refined analysis of a related method appears in von Neumann <ref type="bibr" target="#b92">[94]</ref>. Again the emphasis is on solving a zero-sum game, but here an explicit bound on the computational complexity (the first in mathematical programming?) is given: to approximate the value of an m × n matrix game to within a multiple ε of the range of payoffs requires at most m+n ε 2 iterations, each requiring about 4mn flops. If only the 1 ε 2 factor could be replaced by ln( 1 ε ), this would yield a polynomial algorithm!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Relaxation methods</head><p>The classical relaxation method of Agmon <ref type="bibr" target="#b3">[4]</ref> and Motzkin and Schoenberg <ref type="bibr" target="#b74">[76]</ref>, closely related to iterative methods for linear equations, was of great interest in the 1950s, partly because each iteration requires minimal computation, merely some matrixvector multiplications. To find a point in Y := {y : A T y ≤ c}, at every iteration the current point is projected onto the hyperplane defined by a violated constraint (actually, a stepsize λ ∈ (0, 2] is usually included, where λ = 1 (respectively, 2) corresponds to projection onto (respectively, reflection in) the hyperplane). This method is closely related to the classical Gauss-Seidel and Jacobi iterative methods for "solving" the system A T Av = c of linear equations. (Here, at each iteration, one component of v is changed to satisfy the corresponding equation, again usually with a relaxation parameter to speed convergence.) However, it did not perform well in the computational experiments of Hoffman et al. <ref type="bibr" target="#b48">[49]</ref> due to its very slow convergence. The appearance of the ellipsoid method (which can be viewed as a variable-metric form of the relaxation method) prompted some further study: see Goffin <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. Much more successful were variants of the successive over-relaxation (SOR) technique applied to either a linear complementarity form of the problem or to an augmented Lagrangian dual problem. Indeed, De Leone and Mangasarian <ref type="bibr" target="#b31">[32]</ref> report very encouraging results for this method applied to random sparse linear programming problems of dimensions from 25, 000 × 100, 000 up to 125, 000 × 500, 000. Once again, each iteration requires only some matrix-vector multiplications with the original sparse coefficient matrix. However, choosing the augmented Lagrangian penalty parameter presents some problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Methods based on ideas of computational geometry</head><p>Here we collect a number of methods motivated by two concerns: the desire for a strongly polynomial linear programming algorithm (both the ellipsoid and interior-point methods have iteration bounds depending on the bit length of the data, not just its dimension), and the interest in solving low-dimensional problems with a large number of constraints.</p><p>Megiddo was the first to obtain a method that is linear in n for fixed dimension d <ref type="bibr" target="#b72">[74]</ref>. However, the dependence on d was doubly exponential. Clarkson <ref type="bibr" target="#b25">[26]</ref>   (1) ln n) arithmetic operations. One key idea of Clarkson was to take a sample of the constraints, solve the resulting smaller problem (by another algorithm), and find the set of constraints violated by its solution. These constraints are then either forced into the next sample, or given increased probability, and the process is continued.</p><p>Gärtner and Welzl <ref type="bibr" target="#b37">[38]</ref> describe an O(d 2 n + exp(K √ d ln d))-operation algorithm that combines two methods of Clarkson with the subexponential pivoting methods of Kalai <ref type="bibr" target="#b50">[52]</ref> and Matousek et al. <ref type="bibr" target="#b71">[73]</ref>. This paper also gives a nice historical review of this class of methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">The future</head><p>What will we see in the next fifty, or even five, years? Linear programming has a history of reinventing itself. At present there is a rough computational parity between simplex and interior-point methods: variants of the simplex method are usually better for small problems, hold a significant edge in post-optimal analysis, and are more effective for some large-scale problems, while interior-point methods hold sway for other large problems. Will our complacency in the status quo be shattered by another computationally effective class of methods? I wouldn't bet on it in the next five years, but over the next ten, I'd take even odds.</p><p>On the theoretical side we still have the big questions: does the bounded Hirsch conjecture hold? Is there a polynomial pivot rule for the simplex method? For interiorpoint methods, can we give a theoretical explanation for the difference between worstcase bounds and observed practical performance? Can we devise an algorithm whose iteration complexity is better than O( √ n ln(1/ε)) to attain ε-optimality? Can we find a theoretically or practically efficient way to reoptimize?</p><p>Let us hope that the next fifty years brings as much excitement as the last!</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>devised a randomized algorithm needing only O(d 2 n + d 4 √ n ln n + [O(d)] d/2+O</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research supported in part by NSF through grant DMS-9805602 and ONR through grant N00014-96-1-0050.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note added in proof</head><p>102. Amenta, N., Ziegler, G. (1999): Deformed products and maximal shadows of polytopes. In: Chazelle, B., Goodman, J.E., Pollack, R., eds., Advances in Discrete and Computational Geometry, pp. 57-90. American Mathematical Society</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The expected number of pivots needed to solve parametric linear programs and the efficiency of the self-dual simplex method</title>
		<author>
			<persName><forename type="first">I</forename><surname>Adler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<pubPlace>Berkeley, CA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Industrial Engineering and Operations research, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A simplex variant solving an m × d linear program in O(min(m 2 , d 2 )) expected number of steps</title>
		<author>
			<persName><forename type="first">I</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complexity</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="372" to="387" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simplex algorithm where the average number of steps is bounded between two quadratic functions of the smaller dimension</title>
		<author>
			<persName><forename type="first">I</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="871" to="895" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The relaxation method for linear inequalities</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="382" to="392" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Potential reduction algorithms</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Anstreicher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interior Point Methods in Mathematical Programming</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Terlaky</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the set covering problem</title>
		<author>
			<persName><forename type="first">E</forename><surname>Balas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Padberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1152" to="1161" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Hirsch conjecture for dual transportation polyhedra</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Balinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="629" to="633" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mathematical programming: Journal, society, recollections</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Balinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">History of Mathematical Programming</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science Publishers</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="5" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mixed linear and semidefinite programming for combinatorial and quadratic optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optimization Methods and Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="515" to="544" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Solving large-scale sparse semidefinite programs for combinatorial optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="461" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Implementing the simplex method: The initial basis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bixby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ORSA Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="267" to="284" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progress in linear programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bixby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ORSA Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MIP: Theory and practiceclosing the gap</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bixby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fenelon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rothberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wunderling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">System Modelling and Optimization: Methods, Theory, and Applications</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J D</forename><surname>Powell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Scholtes</surname></persName>
		</editor>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="19" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The ellipsoid method: A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1039" to="1091" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The average number of pivot steps required by the simplex method is polynomial</title>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Operations Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="155" to="177" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Borgwardt</surname></persName>
		</author>
		<title level="m">The Simplex Method: A Probabilistic Analysis</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic analysis of the simplex method</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Developments Arising from Linear Programming</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lagarias</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</editor>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A sharp upper bound for the expected number of shadow vertices in LP-polyhedra under orthogonal projection on two-dimensional planes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="925" to="984" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Iterative solution of games by fictitious play</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Activity Analysis of Production and Allocation</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="374" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Computational suggestions for maximizing a linear function subject to linear inequalities</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Activity Analysis of Production and Allocation</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="377" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Solutions of games by differential equations</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</editor>
		<meeting><address><addrLine>Princeton</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1950">1950</date>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The ellipsoid method generates dual variables</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Burrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="688" to="700" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The steepest descent gravitational method for linear programming</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Murty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="211" to="239" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On some works of Kantorovich, Koopmans and others</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="246" to="263" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A complete description of the traveling salesman polytope on 8 nodes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Christof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jünger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Reinelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="497" to="500" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Las Vegas algorithms for linear and integer programming when the dimension is small</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="488" to="499" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Maximization of a linear function of variables subject to linear inequalities</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Activity Analysis of Production and Allocation</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1951">1951</date>
			<biblScope unit="page" from="339" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Upper bounds, secondary constraints, and block triangularity in linear programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="174" to="183" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Linear Programming and Extensions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963</date>
			<publisher>Princeton University Press</publisher>
			<pubPlace>Princeton, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linear programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">History of Mathematical Programming</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science Publishers</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="19" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The decomposition algorithm for linear programming</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="767" to="778" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Serial and parallel solution of large scale linear programs by augmented Lagrangian successive overrelaxation</title>
		<author>
			<persName><forename type="first">R</forename><surname>De Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename><surname>Mangasarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization, Parallel Processing and Applications</title>
		<title level="s">Lecture Notes in Economics and Mathematical Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Kurzhanski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Neumann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Pallaschke</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page" from="103" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Iterative solution of problems of linear and quadratic programming (in Russian)</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Dikin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">English Translation: Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="674" to="675" />
			<date type="published" when="1967">1967. 1967</date>
		</imprint>
	</monogr>
	<note>Doklady Akademiia Nauk SSSR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the speed of an iterative process (in Russian)</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Dikin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Upravlaemye Sistemy</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="54" to="60" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nonlinear Programming: Sequential Unconstrained Minimization Techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Fiacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIAM Classics in Applied Mathematics Series</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<date type="published" when="1968">1968. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Steepest-edge simplex algorithms for linear programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="341" to="374" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The logarithmic potential method of convex programming</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A K</forename><surname>Frisch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955">1955</date>
			<pubPlace>Oslo, Norway</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University Institute of Economics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Linear programming -randomization and abstract frameworks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Welzl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Symposium on Theoretical Aspects of Computer Science</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the 13th Annual Symposium on Theoretical Aspects of Computer Science<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1064</biblScope>
			<biblScope unit="page" from="669" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Electronic mail distribution of linear programming test problems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gay</surname></persName>
		</author>
		<ptr target="ftp://netlib2.cs.utk.edu/lp/data/" />
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Society Committee on Algorithms Newsletter</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="10" to="12" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On projected Newton barrier methods for linear programming and an equivalence to Karmarkar&apos;s projective method</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Gill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="183" to="209" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Worst-case comparison of valid inequalities for the TSP</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Goemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="335" to="349" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The relaxation method for solving systems of linear inequalities</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Goffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="388" to="414" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Variable metric relaxation methods, Part II: The ellipsoid method</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Goffin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="147" to="162" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Path following methods for linear programming</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Gonzaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="224" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Geometric Algorithms and Combinatorial Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grötschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lovasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>Berlin, Germany</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Grunbaum</surname></persName>
		</author>
		<title level="m">Convex Polytopes</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">The simplex method is very good! On the expected number of pivot steps and related properties of random linear programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haimovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Linear programming at the National Bureau of Standards</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">History of Mathematical Programming</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science Publishers</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="62" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Computational experience in solving linear programs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sokolowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wiegmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of SIAM</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="33" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">How to make a CD disco ball</title>
		<ptr target="http://www.ehow.com/eHow/eHow/0,1053,4570,00.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A subexponential randomized simplex algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM Symposium on Theory of Computing</title>
		<meeting>the 24th ACM Symposium on Theory of Computing</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="475" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Linear programming, the simplex algorithm and simple polytopes</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="217" to="233" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A quasi-polynomial bound for diameter of graphs of polyhedra</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kleitman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="315" to="316" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mathematical methods in the organization and planning of production</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Kantorovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Publication House of the Leningrad State University 68 pp. English Translation: Management Science</title>
		<imprint>
			<date type="published" when="1939">1939. 1960</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="363" to="422" />
		</imprint>
	</monogr>
	<note>in Russian</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A new polynomial-time algorithm for linear programming</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Karmarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="373" to="395" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A polynomial algorithm in linear programming (in Russian)</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Khachiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">English Translation: Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">224</biblScope>
			<biblScope unit="page" from="191" to="194" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
	<note>Doklady Akademiia Nauk SSSR</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Polynomial algorithms in linear programming (in Russian). Zhurnal Vychisitel&apos;noi Matematiki i Matematicheskoi Fiziki</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Khachiyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">English Translation: U.S.S.R. Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<author>
			<persName><forename type="first">V</forename><surname>Klee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kleinschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The d-step conjecture and its relatives</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="718" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">How good is the simplex algorithm?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Klee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Minty</surname></persName>
		</author>
		<editor>Shisha, O., ed., Inequalities, III</editor>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="159" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The d-step conjecture for polyhedra of dimension d &lt; 6</title>
		<author>
			<persName><forename type="first">V</forename><surname>Klee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Walkup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page" from="53" to="78" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The diameter of polytopes and related applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kleinschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Polytopes: Abstract, Convex, and Computational</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Bisztriczky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Mcmullen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Schneider</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</editor>
		<meeting><address><addrLine>Dordrecht, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="467" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An O( √ n L)-iteration potential reduction algorithm for linear complementarity problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mizuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoshise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="331" to="342" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Activity Analysis of Production and Allocation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951">1951</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On the origin of the Hungarian method</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">History of Mathematical Programming</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lenstra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H G</forename><surname>Rinnooy Kan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</editor>
		<imprint>
			<publisher>Elsevier Science Publishers</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="77" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An experimental study of the simplex method</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Quandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Arithmetic, High-Speed Computing and Mathematics</title>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Metropolis</surname></persName>
		</editor>
		<imprint>
			<publisher>American Mathematical Society</publisher>
			<date type="published" when="1963">1963</date>
			<biblScope unit="page" from="107" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Larman</surname></persName>
		</author>
		<title level="m">Paths on polytopes. Proceedings of the London Mathematical Society</title>
		<imprint>
			<date type="published" when="1970">1970</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="161" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The great mathematical Sputnik of 1979</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Mathematical Intelligencer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The dual method of solving the linear programming problem</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Lemke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="36" to="47" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The constrained gradient method of linear programming</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Lemke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of SIAM</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Solving LP problems via weighted centers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="933" to="960" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">The primal-dual interior point method on the Cray supercomputer</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Marsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Shanno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Large-Scale Numerical Optimization</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Coleman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</editor>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A subexponential bound for linear programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matousek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Welzl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithmica</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="498" to="516" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Linear programming in linear time when the dimension is fixed</title>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Computing Machinery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="127" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On the complexity of linear programming</title>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Economic Theory</title>
		<editor>
			<persName><forename type="first">T</forename><surname>Bewley</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="225" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The relaxation method for linear inequalities</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Motzkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Schoenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="393" to="404" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">The Hirsch conjecture is true for (0,1)-polytopes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Naddef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="109" to="110" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Interior Point Polynomial Methods in Convex Programming: Theory and Algorithms</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>SIAM Publications. SIAM</publisher>
			<pubPlace>Philadelphia, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The traveling salesman problem and a class of polyhedra of diameter two</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Padberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="32" to="45" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A polynomial-time algorithm based on Newton&apos;s method for linear programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Renegar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="59" to="93" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">An iterative method of solving a game</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="296" to="301" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">The gradient projection method for nonlinear programming, Part I: Linear constraints</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of SIAM</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="181" to="217" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Schrijver</surname></persName>
		</author>
		<title level="m">Theory of Linear and Integer Programming</title>
		<meeting><address><addrLine>Chichester</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Cut-off method with space extension in convex programming problems</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Shor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">English translation: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="96" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
	<note>Kibernetika</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">On the average speed of the simplex method in linear programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="241" to="262" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Centered Newton method for mathematical programming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">System Modelling and Optimization: Proceedings of the 13th IFIP Conference</title>
		<title level="s">Lecture Notes in Control and Information Sciences</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Iri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Yajima</surname></persName>
		</editor>
		<meeting><address><addrLine>Tokyo, Japan; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1987">1988. Aug./Sept. 1987</date>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The monotonic bounded Hirsch conjecture is false for dimension at least four</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="599" to="601" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Polynomial expected behavior of a pivoting algorithm for linear complementarity and linear programming problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="173" to="192" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Polynomial algorithms for linear programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Optimization and Control</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Eiselt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Pederzoli</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="49" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Probabilistic models for linear programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="767" to="768" />
			<date type="published" when="1991">1991. 1998</date>
		</imprint>
	</monogr>
	<note>Operations Research</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Potential-reduction methods in mathematical programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="3" to="45" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">A centered projective algorithm for linear programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="508" to="529" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A lower bound on the number of iterations of long-step and polynomial interior-point methods for linear programming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="233" to="252" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">A numerical method to determine optimal strategy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="109" to="115" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Discussion of a maximum problem</title>
		<author>
			<persName><forename type="first">.</forename><forename type="middle">J</forename><surname>Von Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">John von Neumann: Collected Works</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Taub</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Pergamon Press</publisher>
			<date type="published" when="1963">1963</date>
			<biblScope unit="volume">VI</biblScope>
			<biblScope unit="page" from="89" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Experiments in linear programming</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Mathematical Programming</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Graves</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1963">1963</date>
			<biblScope unit="page" from="177" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">Primal-dual Interior Point Methods</title>
		<meeting><address><addrLine>SIAM, Philadelphia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">An O(n 3 L) potential reduction algorithm for linear programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="258" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Informational complexity and efficient methods for the solution of convex extremal problems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Yudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ékonomika i Matematicheskie metody</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="3" to="25" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
	<note>Matekon</note>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Ziegler</surname></persName>
		</author>
		<title level="m">Lectures on Polytopes</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Zoutendijk</surname></persName>
		</author>
		<title level="m">Methods of Feasible Directions</title>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
