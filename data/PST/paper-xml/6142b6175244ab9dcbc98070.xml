<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-15">15 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Xinnian</forename><surname>Liang</surname></persName>
							<email>xnliang@buaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Contribution during internship at Tencent Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Cloud Xiaowei</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tencent Cloud Xiaowei</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Lab of Software Development Environment</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-15">15 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.07293v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010)  and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Keyphrase extraction (KE) task aims to extract a set of words or phrases from a document that can represent the salient information of the document <ref type="bibr" target="#b8">(Hasan and Ng, 2014)</ref>. KE models can be divided into supervised and unsupervised. Supervised methods need large-scale annotated training data and always perform poorly when transferred to different domain or type datasets. Compared with the supervised method, the unsupervised method is more universal and adaptive via extracting phrases based on information from input document itself. In this paper, we focus on the unsupervised keyphrase extraction (UKE) model.</p><p>UKE has been widely studied <ref type="bibr" target="#b18">(Mihalcea, 2004;</ref><ref type="bibr" target="#b29">Wan and Xiao, 2008a;</ref><ref type="bibr" target="#b2">Bougouin et al., 2013;</ref><ref type="bibr" target="#b1">Boudin, 2018;</ref><ref type="bibr" target="#b0">Bennani-Smires et al., 2018;</ref><ref type="bibr" target="#b27">Sun et al., 2020)</ref> in the keyphrase extraction field. Recently, with the development of text representation, embedding-based models <ref type="bibr" target="#b0">(Bennani-Smires et al., 2018;</ref><ref type="bibr" target="#b27">Sun et al., 2020)</ref> have achieved promising results and become the new state-of-the-art models. Usually, these methods compute phrase embeddings and document embedding with static word2vec models (e.g. GloVe <ref type="bibr" target="#b24">(Pennington et al., 2014;</ref><ref type="bibr" target="#b13">Le and Mikolov, 2014;</ref><ref type="bibr" target="#b22">Pagliardini et al., 2018)</ref>) or dynamic pre-trained language models (e.g. BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>). Then, they rank candidate phrases by computing the similarity between phrases and the whole document in the vector space. Though, these methods performed better than traditional methods <ref type="bibr" target="#b18">(Mihalcea, 2004;</ref><ref type="bibr" target="#b29">Wan and Xiao, 2008a;</ref><ref type="bibr" target="#b2">Bougouin et al., 2013)</ref>, the simple similarity between phrase and document is insufficient to capture different kinds of context and limits in performance.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows an intuitive explanation for the importance of context modeling. The nodes are candidate phrase embeddings, the star is the document embedding. Each black circle represents one local context. Nodes in the same black circle mean that these candidate phrases are all related to one vital local information (e.g. one topic/aspect of the document). Nodes in the red circle mean that these candidate phrases are similar with the document semantics. If only model the global context via computing similarity between candidate phrases and the document, the model will tend to select red nodes, which will ignore local salient information in three clusters. In order to get the keywords accurately, we should take the local con- To obtain information from context adequately, in this paper, we proposed a novel method which jointly models the local and global context of the input document. Specifically, we calculate the similarity between candidate phrases and the whole document for modeling global context. For local context modeling, we first build a graph structure, which represents each phrase as nodes and the edges are similarity between nodes. Then, we proposed a new centrality computation method, which is based on the insight that the most important information typically occurs at the start or end of documents (document boundary) <ref type="bibr" target="#b15">(Lin and Hovy, 1997;</ref><ref type="bibr" target="#b28">Teufel, 1997;</ref><ref type="bibr" target="#b5">Dong et al., 2021)</ref>, to measure salience of local context based on the graph structure. Finally, we further combine the measure of global similarity and local salience for ranking. To evaluate the effectiveness of our method, we compare our method with recent state-of-the-art models on three public benchmarks <ref type="bibr">(Inspec, DUC 2001</ref><ref type="bibr">, SemEval 2010)</ref>. The results show that our model can outperform most models while generalizing better on input documents with different domains and length. It is deservedly mentioned that our models have a huge improvement on long scientific documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>The overall framework of our model is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. We follow the general process of unsuper-vised keyphrase extraction. The main steps are as follows: (1) We tokenize the document and tag the document with part-of-speech (POS) tags. (2) We extract candidate phrases based on part-of-speech tags. We only keep noun phrases (NP) that consist of zero or more adjectives followed by one or multiple nouns <ref type="bibr" target="#b30">(Wan and Xiao, 2008b)</ref>. (3) We use a pre-trained language model to map the document text to low-dimension vector space and extract vector representation of candidate phrases and the whole document. (4) We score each candidate phrase with a rank algorithm which jointly models the global and local context. (5) We extract phrases with scores from the rank algorithm.</p><p>The main contribution of the whole process is the rank algorithm we proposed in step (4), which can be divided into three components: 1) phrasedocument similarity for modeling global context; 2) boundary-aware centrality for modeling local context; 3) the combination of global and local information. We will introduce the details of these components in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Document and Phrases Representations</head><p>Before introducing the rank algorithm, we first make clear step (1) -(3). We follow the common practice and use StanfordCoreNLP Tools * to accomplish step (1) and (2). After previous universal steps, the document D was tokenized into tokens {t 1 , t 2 , ..., t N } and candidate phrases {KP 0 , KP 1 , ..., KP n } were extracted from document D. Different from previous works <ref type="bibr" target="#b0">(Bennani-Smires et al., 2018)</ref> which use static vector to represent tokens in document, we employ BERT, which is a strong pre-trained language model, to obtain contextualized dynamic vector representations by Equ. (1).</p><formula xml:id="formula_0">{H 1 , H 2 , ..., H N } = BERT({t 1 , t 2 , ..., t N }) (1)</formula><p>Where H i is the vector representation of token t i . Then, we obtain the vector representation H KP i of candidate phrases by computing the average of the phrase's token vectors. The document vector representation is computed with max-pooling operation by Equ. (2).</p><formula xml:id="formula_1">H D = Maxpooling({H 1 , H 2 , ..., H N }) (2)</formula><p>Then, we can obtain document vector representation H D which contains the global semantic information of the full document and a set of candidate phrase representations V = {H KP i } i=1,...,n . Based on these representations, we will introduce the core rank algorithm of our model in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Proposed Rank Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Phrase-Document Similarity</head><p>We first introduce the computation of the phrasedocument similarity for modeling global context. Specifically, we empirically employ Manhattan Distant (i.e. L1-distance) to compute similarity by Equ. (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R(H KP</head><formula xml:id="formula_2">i ) = 1 H D − H KP i 1<label>(3)</label></formula><p>Where</p><p>• 1 means Manhattan Distant and R(H KP i ) represent the relevance between candidate phrase i and the whole document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Traditional Degree Centrality</head><p>Graph-based ranking algorithms for keyphrase extraction represent a document as a graph G = (V, E), where V = {H KP i } i=1,...,n is the set of vector that represent nodes in graph (i.e. candidate phrases in document), and E = {e ij } is the set of edges that represent interactions between candidate phrases. In this paper, we simply employ the degree of nodes as centrality to measure the importance of nodes. The degree centrality for candidate phrase i can be computed with Equ. (4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(H KP</head><formula xml:id="formula_3">i ) = n j=1 e ij<label>(4)</label></formula><p>Where e ij = H T KP i • H KP j is the dot-product similarity score for each pair (H KP i , H KP j ). We could also use other similarity measure methods (e.g. cosine similarity), but we empirically find that the simple dot-product performs better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Boundary-Aware Centrality</head><p>Traditional centrality computation is based on the assumption that the contribution of the candidate phrase's importance in the document is not affected by the relative position of them, and the similarities of two graph nodes are symmetric. From human intuition, phrases that exist at the start or the end of a document should be more important than others. To implement this insight, we propose a new centrality computation method called boundary-aware centrality based on the assumption that important information typically occurs near boundaries (the start and end of documents) <ref type="bibr" target="#b15">(Lin and Hovy, 1997;</ref><ref type="bibr" target="#b28">Teufel, 1997)</ref>.</p><p>We reflect this assumption by employing a boundary function d b (i) over position of candidate phrases. This function d b is formulated as Equ (5).</p><formula xml:id="formula_4">d b (i) = min(i, α(n − i)) (5)</formula><p>Where n is the number of candidate phrases, and α is a hyper-parameter that controls relative importance of the start and end of a document. For node i and j, if d b (i) &lt; d b (j), then node i is closer to the boundary than node j. When calculating the centrality of node i, we need to reduce the contribution of node j to the centrality of node i. Based on this assumption and the boundary function d b (i), we can reconstruct the centrality computation of node i in the graph as Equ. ( <ref type="formula">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(H KP</head><formula xml:id="formula_5">i ) = d b (i)&lt;d b (j) e ij + λ d b (i)≥d b (j) e ij (6)</formula><p>Where λ is used to reduce the influence of phrases which do not appear near the boundary to the centrality of node i.</p><p>Besides, we employ a threshold θ = β(max(e ij − min(e ij )) to filter the noise from nodes, which is far different from node i. We remove the influence of them to centrality by setting all e ij &lt; θ to zero. β is a hyper-parameter that controls the filter boundary. With the introduction of the noise filter strategy, we rewrite the Equ. (6) as Equ. (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C(H KP</head><formula xml:id="formula_6">i ) = d b (i)&lt;d b (j) max(e ij − θ, 0) +λ d b (i)≥d b (j) max(e ij − θ, 0) (7)</formula><p>Where C(H KP i ) represents the local salience of candidate phrase i.</p><p>For most long documents or news articles, the author tends to write the key information at the beginning of the document. <ref type="bibr" target="#b6">Florescu and Caragea (2017a)</ref> point out that the position-biased weight can greatly improve the performance for keyphrase extraction and they employ the sum of the position's inverse of words in the document as the weight. For example, the word appearing at 2th, 5th and 10th, has a weight p(w i ) = 1/2 + 1/5 + 1/10 = 0.8. Our boundary-aware centrality has considered relative position information with boundary function. To prevent double counting, we follow a simpler position bias weight from <ref type="bibr" target="#b27">(Sun et al., 2020)</ref>, which only considers where the candidate phrase first appears. The position bias weight is computed by p(KP i ) = 1 p 1 , where p 1 is the position of the candidate keyphrase's first appearance. After that the softmax function is used to normalize the position bias weight as follow:</p><formula xml:id="formula_7">p(KP i ) = exp(p(KP i )) n k=1 exp(p(KP k ))<label>(8)</label></formula><p>Then, boundary-aware centrality can be rewritten as Equ. (9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ĉ(H KP</head><formula xml:id="formula_8">i ) = p(KP i ) • C(H KP i )<label>(9)</label></formula><p>We finally employ Ĉ(H KP i ) to measure the local salience of candidate phrase i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4">Rank with Global and Local Information</head><p>To consider global and local level information at the same time, we simply combine the measure of global relevance R(H KP i ) and local salience Ĉ(H KP i ) of candidate phrase together with multiplication to obtain the final score by Equ. (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S(H KP</head><formula xml:id="formula_9">i ) = R(H KP i ) • Ĉ(H KP i )<label>(10)</label></formula><p>Finally, we rank candidate phrases with their final score S(H KP i ) and extract top-ranked k phrases as keyphrases of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Evaluation Metrics</head><p>We evaluate our model on three public datasets: Inspec, DUC2001 and SemEval2010. The Inspec dataset <ref type="bibr" target="#b9">(Hulth, 2003)</ref>  We follow the common practice and evaluate the performance of our models in terms of f-measure at the top N keyphrases (F1@N), and apply stemming to both extracted keyphrases and gold truth. Specifically, we report F1@5, F1@10 and F1@15 of each model on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC2001</head><p>Inspec SemEval2010 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DUC2001</head><p>Inspec SemEval2010 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Our Model 28. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Models and Implementation Details</head><p>We compare our methods with three types of models to comprehensively prove the effectiveness of our models. Firstly, we compare with traditional statistical methods TF-IDF and YAKE <ref type="bibr" target="#b3">(Campos et al., 2018)</ref>. Secondly, We compare five strong graph-based ranking methods. TextRank (Mihalcea and Tarau, 2004) is the first attempt to convert text to graph with the co-occurrence of words and employ PageRank to rank phrases. SingleRank <ref type="bibr" target="#b29">(Wan and Xiao, 2008a)</ref> improves the graph construction with a slide window. TopicRank <ref type="bibr" target="#b2">(Bougouin et al., 2013)</ref> considers keyphrase extraction with topic distribution. PositionRank (Florescu and Caragea, 2017b) employs position information to weight the importance of phrases. MultipartiteRank <ref type="bibr" target="#b1">(Boudin, 2018)</ref> splits the whole graph into sub-graph and ranks them with some graph theory. Finally, We compare three stateof-the-art embedding-based models. EmbedRank (Bennani-Smires et al., 2018) first employs embedding of texts with Doc2Vec/Sent2Vec and measures the relevance of phrases and documents to select keyphrases. SIFRank <ref type="bibr" target="#b27">(Sun et al., 2020)</ref> improves EmbedRank with contextualized embedding from a pre-trained language model. KeyGames <ref type="bibr" target="#b26">(Saxena et al., 2020)</ref> creatively introduces game theoretic approach into automatic keyphrase extraction.</p><p>All the models use Stanford CoreNLP Tools † for tokenizing, part-of-speech tagging and noun phrase chunking. And regular expression { N N. * |JJ * N N. * } is used to extract noun phrases as the candidate keyphrases. Our model's hyperparameters for testing are chosen based on our results with the sampled 200 validation sets. The test results are chosen from the following hyperparameter settings: α ∈ {0.5, 0.8, 1, 1.2, 1.5}, β ∈ {0.0, 0.1, 0.2, 0.3} and λ ∈ {0.8, 0.9, 1.0}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We report the results of our model in Tab. 1. We can observe that our model consistently outperforms most of the existing systems across the three datasets, each with different document length, covering two different domains. SIFRank and SIFRank+ have a remarkable performance on datasets with short input length due to document embedding of short documents can better represent the semantic information of full document and short document has fewer local information (e.g. aspects), which make embedding-based models perform well. We can further see that models with global similarity (i.e. EmbedRank and SIFRank) all outperform graph-based models on short length documents (i.e. DUC2001 and Inspec).</p><p>Compared with other works, our model and KeyGames, which is based on game theory, are more generalized and can tackle short and long input documents well. The advantages of our models are very obvious on the long scientific document dataset SemEval2010. This mainly benefits from the boundary-aware centrality for modeling local context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Ablation Study</head><p>We evaluate the contribution of the global and local component of our model with ablation study and the results can be seen in Tab. 2. From the results, we can find that the modeling of local context is more important than the modeling of global context. When we remove local information from our model, our model goes back to an embedding-based model. The performance on SemEval2010 is not sensitive to the removal of relevance-aware weighting. We guess that embedding of long documents may contain multi-aspects information which influences the measure of similarity between the phrase and the whole document, which leads to the influence of global information being limited. Overall, we can prove that jointly modeling global and local context is crucial for unsupervised keyphrase extraction and the revisit of degree centrality is effective for modeling local context and meaningful for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Impact of Hyper-Parameters</head><p>In this section, we first analyze the impact of hyperparameter β and then report the best setting on each dataset. We employ three hyper-parameters in our models, β is used to filter noise and we can see the impact of β from Fig. <ref type="figure" target="#fig_2">3</ref>. β = 0.2 is a proper configure for three datasets. α is used to control the importance of the start or end of a document. α &lt; 1 means the start of the document is more vital and α &gt; 1 means the end of the document is more vital.</p><p>The best settings are α = 0.8, β = 0.2, λ = 0.9 on DUC2001, α = 0.5, β = 0.2, λ = 0.9 on Inspec and α = 1.5, β = 0.2, λ = 0.8 on Se-mEval2010. From these settings, we can get the following three conclusions, which is conforming to the characteristics of these datasets. For DUC2001 and Inspec, most vital information occurs at the start of the document due to the fact that DUC2001 is from news articles, and Inspec is from the abstract. For SemEval2010, the setting of α is contrary to previous datasets due to SemEval2010 is long scientific documents and much key information occurring at the end of the document (section conclusion). The settings of λ on three datasets show that long documents need to reduce more influence from contexts not near the boundary, which is intuitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Impact of Different Similarity Measure Methods</head><p>Our model employs Manhattan Distance to measure the similarity between phrases and the whole document. We also attempt to employ different measure methods. The results of different similarity measure methods are shown in Tab. 3, and we can see that the advantage of Manhattan Distance is obvious. We also can see that cosine similarity performs badly and is not suitable for our models.</p><p>Similarity Measure DUC2001 Inspec SemEval2010 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 F1@5 F1@10 F1@15 Euclidean <ref type="bibr">Distance 23.31 28.04 30.39 28.5 37.01 29.25 10.99 16.37 18.41 Cosine similarity 15.01 17.96 19.44 23.67 30.26 33.35 9.70 12.22 13.29 Manhattan Distance 28.62 35.52 36.29 32.49 40.04 41.05 12.26 19.22 21.42</ref> Table <ref type="table">3</ref>: The results of different measure methods for similarity between candidate phrase and the whole document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4">Case Study</head><p>In this section, we show an example from DUC2001 in Fig. <ref type="figure" target="#fig_3">4</ref>. DUC2001 is a dataset from news articles. The correct keyphrases are underlined. Red text means the extracted gold truth and blue text means extracted phrases by our model.</p><p>We can see that all keyphrases occur at the start of the document. Our model extracted many correct phrases which are the same as gold truth and extracted the phrase "existing word record" which is semantically same with "word record" in gold truth. It is worth mentioning that our model focuses on the boundary of the document and most extracted phrases were located at the start of the document, which is controlled by our setting of α. This proves the effectiveness of our boundary-aware centrality. From the figure, we also can find that wrong phrases are highly relevant to topics of this document, which is influenced by our phrase-document relevance weighting. This example shows that the joint modeling of global and local context can improve the performance of keyphrase extraction and our model really captures local and global informa-tion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-trained Language Model</head><p>Pre-trained language model is the kind of model that is trained on large-scale unlabeled corpus to learn prior knowledge and then fine-tuned on downstream tasks. The pre-trained language model without fine-tuning also can provide high quality embedding of natural texts for unsupervised tasks. Different from static word embedding, such as Word2Vec <ref type="bibr" target="#b20">(Mikolov et al., 2013)</ref>, GloVe <ref type="bibr" target="#b24">(Pennington et al., 2014), and</ref><ref type="bibr">FastText (Joulin et al., 2017)</ref>. Pre-trained language models can encode words or sentences with context dynamically and solve the OOV problem. In addition, pre-trained language models can provide document-level or sentencelevel embedding which contains more semantic information than Sen2Vec <ref type="bibr" target="#b22">(Pagliardini et al., 2018)</ref> or Doc2Vec <ref type="bibr" target="#b13">(Le and Mikolov, 2014)</ref>.</p><p>ELMo <ref type="bibr" target="#b25">(Peters et al., 2018)</ref> employs Bi-LSTM structure and concatenate forward and backward information to capture bidirectional information.</p><p>BERT <ref type="bibr" target="#b4">(Devlin et al., 2019</ref>) is a bidirectional transformer structure pre-trained language model. Compared with the concatenation of bidirectional information, BERT can capture better context information. There are also a lot of other pre-trained language models such as RoBERTa <ref type="bibr" target="#b16">(Liu et al., 2019)</ref>, XLNET <ref type="bibr" target="#b31">(Yang et al., 2020)</ref>, etc. In this paper, we choose BERT, the most used, to obtain vector representation of documents and phrases by merging the embedding of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Unsupervised Keyphrase Extraction</head><p>Unsupervised keyphrase extraction can be divided into four main types: statistics-based models, graph-based models, topic-based models, and embedding-based models. Statistics-based models <ref type="bibr" target="#b3">(Campos et al., 2018)</ref> mainly analyze an article's probability features such as word frequency feature, position feature, linguistic features, etc. Topicbased models <ref type="bibr" target="#b10">(Jardine and Teufel, 2014;</ref><ref type="bibr" target="#b17">Liu et al., 2009)</ref> focus on how to mine keyphrases by making use of the probability distribution of articles.</p><p>Graph-based models are the most proposed and popular used in early works which convert the document into a graph. Inspired by <ref type="bibr" target="#b21">(Page et al., 1999)</ref>, <ref type="bibr" target="#b18">(Mihalcea, 2004)</ref> proposed TextRank to convert keyphrase extraction task into the rank of nodes in graph. After this, various works focused on the expansion of TextRank. <ref type="bibr" target="#b29">(Wan and Xiao, 2008a</ref>) proposed SingleRank, which employs co-occurrences of tokens as edge weights. <ref type="bibr" target="#b2">(Bougouin et al., 2013)</ref> proposed TopicRank, which assigns a significance score to each topic by candidate keyphrase clustering. MultipartiteRank <ref type="bibr" target="#b1">(Boudin, 2018)</ref> encodes topical information within a multipartite graph structure. Recently, <ref type="bibr">(Wang, 2015)</ref> proposed WordAt-tractionRank, which added distance between word embeddings into SingleRank, and <ref type="bibr" target="#b7">(Florescu and Caragea, 2017b</ref>) use node position weights, favoring words appearing earlier in the text. This position bias weighting strategy is very useful in news articles and long documents.</p><p>Embedding-based models benefit from the development of representation learning, which maps natural language into low-dimension vector representation. Therefore, in recent years, embeddingbased keyphrase extraction <ref type="bibr">(Wang et al., 2016;</ref><ref type="bibr" target="#b0">Bennani-Smires et al., 2018;</ref><ref type="bibr" target="#b23">Papagiannopoulou and Tsoumakas, 2018;</ref><ref type="bibr" target="#b27">Sun et al., 2020)</ref> has achieved good performance . <ref type="bibr" target="#b0">(Bennani-Smires et al., 2018)</ref> proposed EmbedRank, which ranks phrases by measuring the similarity between phrase embedding and document embedding. <ref type="bibr" target="#b27">(Sun et al., 2020)</ref> proposed SIFRank, which improves the static embedding from EmbedRank with a pretrained language model.</p><p>Embedding-based models just measured the similarity between document and candidate phrases and ignored the local information. To jointly model global and local context <ref type="bibr" target="#b32">(Zheng and Lapata, 2019;</ref><ref type="bibr" target="#b14">Liang et al., 2021)</ref>, in this paper, we revisit degree centrality, which can model local context, and convert it into boundary-aware centrality. Then, we combine global similarity and boundary-aware centrality for local salient information to rank and extract phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we point out that embedding-based models ignore the local information and propose a novel model which jointly models global and local context. Our model revisited degree centrality and modified it with boundary function for modeling local context. We combine global similarity with our proposed boundary-aware centrality to extract keyphrases. Experiments on 3 public benchmarks demonstrate that our model can effectively capture global and local information and achieve remarkable results. In the future work, we will focus on how to introduce our boundary-aware mechanism into supervised end2end keyphrase extraction/generation models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Visualization of embedding space. Nodes refer to candidate phrase representation and star is document representation. Black circles mean clusters which contain local salient information. Red circle means global similarity phrases.</figDesc><graphic url="image-1.png" coords="2,80.21,70.87,199.58,168.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The framework of our unsupervised keyphrase extraction ranking model. (1) Get tokenized document and POS tags. (2) Extract noun phrases that consist of zero or more adjectives followed by one or multiple nouns. (3) Obtain embeddings of tokens in document with BERT. (4) Compute boundary-aware centrality and global relevance of each candidate phrases with global and local similarities. (5) Rank and extract keyphrases from candidate phrases with scores from the previous step.</figDesc><graphic url="image-2.png" coords="3,79.79,70.86,435.71,254.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance with different β.</figDesc><graphic url="image-3.png" coords="6,311.60,70.87,207.36,142.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example from DUC2001. The correct keyphrases are underlined. Red text means the extracted gold truth and blue text means extracted phrases by our model.</figDesc><graphic url="image-4.png" coords="7,76.10,173.68,443.07,225.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our models with other baselines.</figDesc><table><row><cell></cell><cell>Statistical Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TF-IDF</cell><cell cols="2">9.21 10.63 11.06 11.28 13.88 13.83 2.81</cell><cell>3.48</cell><cell>3.91</cell></row><row><cell>YAKE</cell><cell cols="3">12.27 14.37 14.76 18.08 19.62 20.11 11.76 14.4</cell><cell>15.19</cell></row><row><cell></cell><cell>Graph-based Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TextRank</cell><cell cols="2">11.80 18.28 20.22 27.04 25.08 36.65 3.80</cell><cell>5.38</cell><cell>7.65</cell></row><row><cell>SingleRank</cell><cell cols="2">20.43 25.59 25.70 27.79 34.46 36.05 5.90</cell><cell>9.02</cell><cell>10.58</cell></row><row><cell>TopicRank</cell><cell cols="4">21.56 23.12 20.87 25.38 28.46 29.49 12.12 12.90 13.54</cell></row><row><cell>PositionRank</cell><cell cols="4">23.35 28.57 28.60 28.12 32.87 33.32 9.84 13.34 14.33</cell></row><row><cell cols="5">MultipartiteRank 23.20 25.00 25.24 25.96 29.57 30.85 12.13 13.79 14.92</cell></row><row><cell></cell><cell>Embedding-based Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">EmbedRank d2v 24.02 28.12 28.82 31.51 37.94 37.96 3.02</cell><cell>5.08</cell><cell>7.23</cell></row><row><cell cols="3">EmbedRank s2v 27.16 31.85 31.52 29.88 37.09 38.40 5.40</cell><cell>8.91</cell><cell>10.06</cell></row><row><cell>SIFRank</cell><cell>24.27 27.43 27.86 29.11 38.80 39.59</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SIFRank+</cell><cell>30.88 33.37 32.24 28.49 36.77 38.82</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KeyGames</cell><cell cols="4">24.42 28.28 29.77 32.12 40.48 40.94 11.93 14.35 14.62</cell></row><row><cell></cell><cell>Proposed Model</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Our Model</cell><cell cols="4">28.62 35.52 36.29 32.61 40.17 41.09 13.02 19.35 21.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The results of ablation experiments on three datasets.</figDesc><table><row><cell cols="3">62 35.52 36.29 32.49 40.04 41.05 12.26 19.22 21.42</cell></row><row><cell cols="3">-Global Similarity 26.22 33.85 34.61 30.75 38.49 40.52 12.00 18.93 21.29</cell></row><row><cell>-Local Similarity 17.45 18.64 19.03 22.51 27.58 30.36 8.81</cell><cell>10.7</cell><cell>11.45</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">* https://stanfordnlp.github.io/CoreNLP/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">† https://stanfordnlp.github.io/CoreNLP/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the three anonymous reviewers for their careful reading of our paper and their many insightful comments and suggestions. This work was supported in part by the National Natural Science Foundation of China (Grant Nos.U1636211, 61672081,61370126), the 2020 Tencent Wechat Rhino-Bird Focused Research Program, and the Fund of the State Key Laboratory of Software Development Environment (Grant No. SKLSDE-2021ZX-18).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Simple unsupervised keyphrase extraction using sentence embeddings</title>
		<author>
			<persName><forename type="first">Kamil</forename><surname>Bennani-Smires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Hossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Baeriswyl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K18-1022</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
				<meeting>the 22nd Conference on Computational Natural Language Learning<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="221" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised keyphrase extraction with multipartite graphs</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-2105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Short Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="667" to="672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">TopicRank: Graph-based topic ranking for keyphrase extraction</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bougouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Boudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Béatrice</forename><surname>Daille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
				<meeting>the Sixth International Joint Conference on Natural Language Processing<address><addrLine>Nagoya, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="543" to="551" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Yake! collection-independent automatic keyword extractor</title>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vítor</forename><surname>Mangaravite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arian</forename><surname>Pasquali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alípio</forename><surname>Mário Jorge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Célia</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval</title>
				<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="806" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discourse-aware unsupervised summarization of long scientific documents</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Mircea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><forename type="middle">C K</forename><surname>Cheung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A position-biased pagerank algorithm for keyphrase extraction</title>
		<author>
			<persName><forename type="first">Corina</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Posi-tionRank: An unsupervised approach to keyphrase extraction from scholarly documents</title>
		<author>
			<persName><forename type="first">Corina</forename><surname>Florescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cornelia</forename><surname>Caragea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1115" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic keyphrase extraction: A survey of the state of the art</title>
		<author>
			<persName><forename type="first">Saidul</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1119</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1262" to="1273" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Improved automatic keyword extraction given more linguistic knowledge</title>
		<author>
			<persName><forename type="first">Anette</forename><surname>Hulth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2003 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Topical PageRank: A model of scientific expertise for bibliographic search</title>
		<author>
			<persName><forename type="first">James</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/E14-1053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
				<meeting>the 14th Conference of the European Chapter<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers; Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SemEval-2010 task 5 : Automatic keyphrase extraction from scientific articles</title>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olena</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Medelyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Workshop on Semantic Evaluation</title>
				<meeting>the 5th International Workshop on Semantic Evaluation<address><addrLine>Uppsala</addrLine></address></meeting>
		<imprint>
			<publisher>Sweden. Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>II-1188-II-1196. JMLR.org</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving unsupervised extractive summarization with facet-aware modeling</title>
		<author>
			<persName><forename type="first">Xinnian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.147</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1685" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identifying topics by position</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.3115/974557.974599</idno>
	</analytic>
	<monogr>
		<title level="m">Fifth Conference on Applied Natural Language Processing</title>
				<imprint>
			<publisher>DC, USA. Association for Computational Linguistics</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering to find exemplar terms for keyphrase extraction</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph-based ranking algorithms for sentence extraction, applied to text summarization</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Interactive Poster and Demonstration Sessions</title>
				<meeting>the ACL Interactive Poster and Demonstration Sessions<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="170" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
				<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02">2013. May 2-4, 2013</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The pagerank citation ranking: Bringing order to the web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Stanford InfoLab</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Pagliardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL 2018 -Conference of the North American Chapter</title>
				<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Local word vectors guiding keyphrase extraction</title>
		<author>
			<persName><forename type="first">Eirini</forename><surname>Papagiannopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">KeyGames: A game theoretic approach to automatic keyphrase extraction</title>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mudit</forename><surname>Mangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goonjan</forename><surname>Jain</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.184</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2037" to="2048" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sifrank: A new baseline for unsupervised keyphrase extraction based on pre-trained language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2965087</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="10896" to="10906" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sentence extraction as a classification task</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Scalable Text Summarization</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single document keyphrase extraction using neighborhood knowledge</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence</title>
				<meeting>the 23rd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2008">2008a</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="855" to="860" />
		</imprint>
	</monogr>
	<note>AAAI&apos;08</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Corpus-independent generic keyphrase extraction using word embedding vectors</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
				<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press. R. Wang</publisher>
			<date type="published" when="2008">2008b. 2015. 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="932" to="942" />
		</imprint>
	</monogr>
	<note>The COLING 2016 Organizing Committee</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentence centrality revisited for unsupervised summarization</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1628</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6236" to="6247" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
