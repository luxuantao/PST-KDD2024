<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Localness of Software</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
							<email>zptu@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California at Davis</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
							<email>su@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California at Davis</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
							<email>ptdevanbu@ucdavis.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California at Davis</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Localness of Software</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3B11452BCD4BCDDB3E5CC85FC05E8579</idno>
					<idno type="DOI">10.1145/2635868.2635875</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.2.7 [Software Engineering]: Distribution</term>
					<term>Maintenance</term>
					<term>and Enhancement Algorithms</term>
					<term>Documentation</term>
					<term>Experimentation</term>
					<term>Measurement Localness</term>
					<term>Cache Language Model</term>
					<term>Code Suggestion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The n-gram language model, which has its roots in statistical natural language processing, has been shown to successfully capture the repetitive and predictable regularities ("naturalness") of source code, and help with tasks such as code suggestion, porting, and designing assistive coding devices. However, we show in this paper that this natural-language-based model fails to exploit a special property of source code: localness. We find that human-written programs are localized: they have useful local regularities that can be captured and exploited. We introduce a novel cache language model that consists of both an n-gram and an added "cache" component to exploit localness. We show empirically that the additional cache component greatly improves the n-gram approach by capturing the localness of software, as measured by both cross-entropy and suggestion accuracy. Our model's suggestion accuracy is actually comparable to a state-of-the-art, semantically augmented language model; but it is simpler and easier to implement. Our cache language model requires nothing beyond lexicalization, and thus is applicable to all programming languages.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The spectacular advances in natural language (NL) processing in recent years, in terms of speech recognition, translation, etc., have in great measure been due to the ability of language models to capture the repetition and regularity in commonplace speech and writing. Language models assign a probability to a word sequence using an estimated probability distribution. High-quality language models lie at the heart of most NL applications, such as speech recognition <ref type="bibr" target="#b22">[22]</ref>, machine translation <ref type="bibr" target="#b7">[7]</ref>, spelling correction <ref type="bibr" target="#b24">[24]</ref> and handwriting recognition <ref type="bibr" target="#b46">[46]</ref>. The most successful class of language models are n-gram models, introduced three decades ago <ref type="bibr" target="#b6">[6]</ref>.</p><p>Recently, Hindle et al. <ref type="bibr" target="#b18">[18]</ref>-building on the uniqueness property of source code reported by Gabel and Su <ref type="bibr" target="#b16">[16]</ref>-have shown that software corpora, like NL corpora, are highly repetitive and thus predictable using language models. Good quality language models show great promise in software engineering applications. Hindle et al. show that n-gram language models perform quite well, and leverage this fact for code suggestion. In addition, Nguyen et al. <ref type="bibr" target="#b35">[35]</ref> have adapted the n-gram models for cross-language porting, and Allamanis et al. <ref type="bibr" target="#b3">[3]</ref> have applied them to the automatic recognition and checking of coding styles. In this paper, we follow this work, to further improve language models. We begin our work by remarking on a special property of software: in addition to being even more repetitive and predictable than NL, source code is also very localized. Due to module specialization and focus, code tends to take special repetitive forms in local contexts. The n-gram approach, rooted as it is in NL, focuses on capturing the global regularities over the whole corpus, and neglects local regularities, thus ignoring the localness of software. The localness of source code refers to the skewed distribution of repetitive n-gram patterns in the locality, namely the endemism and specificity of n-gram patterns in the locality. Endemic n-grams occur only in one locality (such as a single file). For example, new programming units (files, modules) usually introduce new identifiers (variable names, method names, and such), along with endemic n-gram patterns of identifier use (e.g. myinvitees.next()). Specificity is a less degenerate version of endemism, signifying the tendency of some non-endemic n-grams to favor a specific locality, while not being endemic to it. For instance, different programmers have idiosyncratic n-gram frequencies (e.g. some programmers favor "for ( int i" while others prefer "for ( int size"), resulting in the frequency-biased n-gram patterns in the specific files that each programmer is involved in.</p><p>Since n-gram models can only provide suggestions occurring in the training data, n-gram patterns specific or endemic to the locality can be overlooked. For example, let us predict the next token after a sequence "for ( int". Suppose the token "i" followed these three tokens 30% of the time in the training data, while the token "size" followed them 5% of the time. The n-gram model will assign "i" a probability of 0.3 and "size" a probability of 0.05, and thus "i" is chosen. For an isolated line, this would be the reasonable choice to make. But now suppose that several previous lines in the same file contained the n-gram "for ( int size", while none contained "for ( int i". Arguably, the token "size" should then be assigned a much higher probability. A token used in the immediate past is much more likely to be used again soon than its overall probability that n-gram models would predict.</p><p>The key insight underlying our work concerns this critical limitation of traditional n-gram models. Our central hypothesis is: Source code is locally repetitive, viz. it has useful local regularities that can be captured in a locally estimated cache and leveraged for software engineering tasks.</p><p>We believe that this "localness" of software has it roots in the imperative to create modular designs, where modules secrete localized knowledge <ref type="bibr" target="#b37">[37]</ref>. To leverage this property, we deploy an additional cache component to capture the localized regularities of both endemic and specific n-gram patterns in the locality. However, we need to overcome several challenges:</p><p>• How to combine the global (n-gram) model with the local (cache) model? How to automatically pick the interpolation weights for different n-grams?</p><p>• What is the locality to use? For example, how much local data do we need? Which length of n-grams should we use?</p><p>• Does the additional cache component impose high computational resource cost?</p><p>In Section 4, we solve these problems by introducing a mechanism that does not compromise the robust simplicity of n-gram models.</p><p>Our empirical evaluation on projects from five programming languages demonstrates that a cache language model consisting of both n-gram and cache components yields relative improvements of 16%∼45% in suggestion accuracy over the state-of-the-art n-gram approach <ref type="bibr" target="#b18">[18]</ref>. Surprisingly, using just the cache component built on 5K tokens, by itself, outperforms the n-gram model trained on nearly 2M tokens in suggestion accuracy. It is worth emphasizing that the cache model is quite simple and only requires lexicalization; no parsing, type checking, etc. are required. Because of its simplicity and unique focus on localness, it is complementary to most state-ofthe-art research on statistical modeling of source code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>Our key contributions are:</p><p>1. We demonstrate (Section 3) empirically over large software corpora, that source code is localized in the sense that code regularities (n-gram patterns) are endemic, specific, and proximally repetitive in the locality .</p><p>2. We introduce (Section 4) a novel cache language model to capture the localness of software that is simple (requiring no additional information other than the tokens) and easy-touse (automatic selection of dynamic interpolation weights for n-gram and cache components).</p><p>3. We show (Section 5) that the cache model indeed captures the localness of source code and the strength of the cache model can be exploited for code suggestion, which substantially improves the state-of-the-art n-gram approach <ref type="bibr" target="#b18">[18]</ref>. <ref type="bibr" target="#b4">4</ref> We provide (Section 5.2.1) an option for code suggestion when there is not enough corpus to train a language model: using only the cache component built on thousands of tokens, which achieves comparable suggestion accuracy with the ngram model trained on millions of tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Statistical Language Models</head><p>Language models are statistical models that assign a probability to every sequence of words, which reflects the probability that this sequence is written by a human. Considering a code sequence S = t1t2 . . . tN , it estimates the probability as</p><formula xml:id="formula_0">P (S) = P (t1) • N i=2 P (ti|t1, . . . , ti-1)<label>(1)</label></formula><p>That is, the probability of a code sequence is a product of a series of conditional probabilities. Each probability P (ti|t1, . . . , ti-1) denotes the chance that the token ti follows the previous words, the prefix, h = t1, . . . , ti-1. However, the probabilities P (ti|t1, . . . , ti-1) are in practice impossible to estimate, since there are astronomically large numbers of possible prefixes. For a vocabulary of size 20, 000 and sentences with maximal length of 10, there are approximately 10 43 different probabilities to be estimated, which is impractical. Therefore, we need a method of grouping prefixes into a more reasonable number of equivalence classes. One possible approach to group them is by making a Markov assumption that the conditional probability of a token is dependent only on the n -1 most recent tokens. The n-gram model is such a model that places all prefixes that have the same n -1 tokens in the same equivalence class:</p><formula xml:id="formula_1">P (ti|h) = P (ti|t1, . . . , ti-1) = P (ti|ti-n+1, . . . , ti-1) (2)</formula><p>The latter is estimated from the training corpus as the ratio of the times that the token ti follows the prefix sequence ti-n+1, . . . , ti-1:</p><formula xml:id="formula_2">P (ti|h) = count(ti-n+1, . . . , ti-1, ti) count(ti-n+1, . . . , ti-1)<label>(3)</label></formula><p>Thus, if the fragment "for ( int" occurs 1000 times, and "for ( int i" occurs 300 times, then p(i | for int () = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Code Suggestion</head><p>The code suggestion task refers to recommending the next token based on the current context. There has been recent work on code suggestion. For instance, one thread of research <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b39">39]</ref> concerns on the suggestion of method calls and class names. Nguyen et al. <ref type="bibr" target="#b34">[34]</ref> and Zhong et al. <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51]</ref> aim to predict API calls, and Zhang et al. <ref type="bibr" target="#b48">[48]</ref> focus on completing the parameter list of API calls. Eclipse (and other development environments like IntelliJ IDEA) makes heavy use of compile-time type information to predict which tokens may apply in the current context.</p><p>N -gram models have been successfully applied to code suggestion <ref type="bibr" target="#b18">[18]</ref>. Hindle et al. <ref type="bibr" target="#b18">[18]</ref> exploit a trigram model built from the lexed training corpus. At every point in the code, the model gives a list of possible tokens along with their probabilities that are estimated from the training corpus. The probabilities can be used to rank the candidate tokens. Then, the top-k rank-ordered suggestions are presented to the user. The advantages of the n-gram approach are the possibility to suggest all types of tokens (not just method calls), permitting the model to incorporate valuable information (indicating what most often does apply) that is not described by the IDE-based approaches (guess what may apply). The n-gram model is also very simple, and unlike more recent work <ref type="bibr" target="#b36">[36]</ref>, does not require anything beyond tokenizing the code. On the other hand, n-gram models will not help much when dealing with tokens and n-grams locally specific to a particular context, and not present in the code used to train the n-gram model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">LOCALNESS OF SOURCE CODE</head><p>Is source code localized? To answer this question, we analyzed both natural language and code corpora. We found evidence supporting three aspects relating to our hypothesis: the endemism, specificity and repetitiveness of code regularities in the locality.</p><p>Data. We collected the same data set of Java projects and English corpora used by Hindle et al. <ref type="bibr" target="#b18">[18]</ref>. The data set contains nine Java projects with a total of more than 2M LOCs. To investigate whether the commonality holds across different programming languages, we also carried out experiments on another very different language, Python. Table <ref type="table" target="#tab_0">1</ref> shows the summary statistics on our data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Are N-grams Endemic to Localities?</head><p>In this experiment, we investigated whether some n-grams are only found in a local context (endemic to a locality). Table <ref type="table" target="#tab_1">2</ref> shows the percentage of the total n-grams in the corpus that were only found in a single file. 1 For example, suppose there are 1M 3-grams extracted from one corpus, 200K of which are only found in a single file, then the percentage is 20%. We can see that 25.63% of 3-1 In English corpus, each file is an article or a chapter of a book. Figure <ref type="figure">1</ref>: Entropies of the file distributions for non-endemic ngrams (grouped by the number of files) for Java. "Uniform" denotes that the n-grams are distributed uniformly in the files. grams from the Java corpus (41.82% from the Python corpus) are found only in one file. The large proportion of n-grams only seen in a local context denotes that source code is endemic to file localities.</p><p>Among the endemic n-grams in source code, there are over 10% of the total 3-grams that occur more than one time in a single file; and as the order (i.e. n) of the n-grams increases, a greater proportion repeat in single files. These endemic, but locally repeating n-grams represent an opportunity to improve upon language models that do not take locality into account. In contrast, the percentage of the endemic, but locally repeating n-grams in English is much lower than Java and Python, especially for long n-grams (e.g. 0.20% for 10-grams). This validates our hypothesis that the localness is a special feature of source code, which cannot be found in NL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Is Source Code Locally Specific?</head><p>In this experiment, we investigated whether some non-endemic n-grams favor a specific locality (locally specific). For each nonendemic n-gram that occurs in multiple files, there would be a discrete probability distribution p for the set of files F . Here we use locality entropy HL to measure the skewness of the distribution of locality of an n-gram sequence σ</p><formula xml:id="formula_3">HL(σ) = - f ∈F p(fσ) log 2 p(fσ)<label>(4)</label></formula><p>where</p><formula xml:id="formula_4">p(fσ) = count(n-gram σ in f) count(n-gram σ in project)<label>(5)</label></formula><p>Generally speaking, as | F | increases, HL increases. However, the more skewed the distribution, the lower the entropy (and the greater the opportunity for language models to exploit localization). One extreme case is the endemic n-grams that occur in only one file, then the entropy will be the lowest, viz., 0. Another example is that n-grams are distributed uniformly in k files, then the entropy will be the highest, viz., log 2 (k). The lower the entropy, the more the n-gram σ tends to favor a smaller subset of files in F .</p><p>We place all non-endemic n-grams that occur in the same number of files in the same group, and report their mean entropy, as plotted in Figure <ref type="figure">1</ref>. We can see that n-grams with varying orders share the same trend: the entropies of their file distributions are lower than that of a uniform distribution, indicating that the non-endemic n-grams indeed favor a specific locality. For example, the locality entropy of the 3-grams that occur in 326 files is 5.9, which is 2.5 bits lower than that of uniform distribution, viz, 8.4. The skewed file distribution of non-endemic n-grams reconfirms our hypothesis that n-gram models can benefit from capturing the locality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Is Source Code Proximally Repetitive?</head><p>The degree of proximity of repeated n-grams to each other is relevant to the design of locality-sensitive language models: exactly  what range of proximity should language models consider when estimating a local distribution? We employ n-gram distance to measure how far separated the repetitive n-grams in the project are. Distance for a specific n-gram is defined as the average span (in term of the number of tokens) between the proximate occurrences of this n-gram. <ref type="foot" target="#foot_0">2</ref> For example, if "for ( int size" occurs 50 times in the project and there are 49,000 tokens between the first and last positions where it occurs, we say the distance of "for ( int size" is 1000 tokens. In other words, we may expect the n-gram to appear again on average 1000 tokens after the last location where it is previously found.</p><p>We plot in Figure <ref type="figure" target="#fig_2">2</ref> the variation of n-gram distance (log10) with different n-gram order n. Generally, the n-gram distance goes up with the increase of the order n. As seen, the n-gram distance of 10-grams for Java is 1754 (10 3.24 ) and for Python is 1514 (10 3.18 ), suggesting that programming languages are locally repetitive. The highly proximate repetitive property of the n-grams suggests that locally-sensitive language models have modest memory requirements-we do not have to track too much local information.</p><p>Source code is localized in the sense that code regularities in the locality are endemic, specific, and proximally repetitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CACHE LANGUAGE MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Illustration</head><p>Our primary goal is to address a critical limitation of the standard traditional n-gram models: their inability to capture local regularities. We accomplish this by using a locally estimated cache component to capture the endemic and specific n-gram patterns.</p><p>Capturing the structure of the surrounding locality, in an evolving model that changes with the locality, is a good approach to capture the local regularities. First, if we memorize the endemic n-grams in the locality, we can offer the correct suggestion when they occur again, which can never be provided by the n-gram model estimated from the training data. This would be a good supplement to the global n-gram models, which do not perform well, e.g., with ngrams that are endemic and perhaps not seen before.</p><p>Second, a language model that captures short-term shifts in ngram frequencies might perform significantly better than the pure n-gram models described above. If we memorize the n-grams found in the locality, or cache, we can update the probability of the n-grams based on their frequency in the cache. Let us revisit the last example. From the n-gram model, we have p(i|for int () = 0.3 and p(size | for int () = 0.05. Suppose on the existing lines in the file, "for int ( size" occurs 3 times and "for int ( i" never appears. Then from the cache we have p(i | for int () = 0 and p(size | for int () = 1.0. If we use the average of the probabilities from the two components as the final probabilities, we will assign "i" a probability of 0.15 and "size" a probability of 0.525, and thus "size" is chosen.</p><p>Our cache language model is based on these intuitions; it includes both a standard n-gram and an added "cache" component. Given a source file, the cache will contain all the n-grams found in the local code. Thus, the combined model assigns each token candidate two probabilities, the first based on its frequency in the training corpus (the n-gram component) and its frequency in the cache (the cache component). Linear interpolation of the two probabilities produces an overall probability of each token candidate.</p><p>The main idea behind our work is to combine a large global (static) language model with a small local (dynamic) model estimated from the proximate local context. The n-gram and cache components capture different regularities: the n-gram component captures the corpus linguistic structure, and offers a good estimate of the mean probability of a specific linguistic event in the corpus; around this mean, the local probability fluctuates, as token patterns change in different localities. The cache component models these local changes, and provides variance around the corpus mean for different local contexts. The strengths of the additional cache component are reflecting the code locality by capturing:</p><p>1. Endemic n-gram patterns: The cache component captures more different n-grams by memorizing the endemic n-grams in the locality, which do not occur within the n-gram set captured from the rest of the corpus. When the endemic n-grams repeat, the cache component can offer a suggestion.</p><p>2. Specific n-gram patterns: If a token that followed the same prefix sequence has occurred often in the locality (might span several local files), it will be assigned a higher probability than when its local frequency is low. In this way, the inclusion of a cache component satisfies our goal to dynamically track the frequency-biased patterns of n-gram use in the locality.</p><p>In Section 4.2, we describe how the cache model automatically learns to interpolate between these two models. Then, we analyze several local factors that affect the cache model's performance in Section 4.3. By setting the factors appropriately, the cache component will capture the code locality with modest computational resources (as shown in Table <ref type="table" target="#tab_5">6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mathematical Treatment</head><p>The cache model is now introduced mathematically:</p><formula xml:id="formula_5">P (ti|h, cache) = λ • Pn-gram(ti|h) + (1 -λ) • Pcache(ti|h) (6)</formula><p>Here ti is the token to be predicted, h is the prefix tokens that ti follows, cache is the list of n-grams that are stored in the cache, and λ is the interpolation weight. The combined model leaves the n-gram component Pn-gram(ti|h) of the language model unchanged. Note that the traditional n-grams model is the special case of the cache model, when λ = 1. The n-gram-based probability Pn-gram(ti|h) can be regarded as a good estimate of the mean around which the value Pcache(ti|h) fluctuates, while the cache-based probability Pcache(ti|h) is the variance around that mean. This allows the estimate of Pcache(ti|h) to deviate from its average value to reflect temporary high or low values.</p><p>Probability Estimation in Cache. The cache-based probability is calculated from the frequency of ti followed the prefix h in the cache. We estimate the cache probability Pcache(ti|h) by</p><formula xml:id="formula_6">Pcache(ti|h) = count(&lt; h, ti &gt; in cache) count(h in cache)<label>(7)</label></formula><p>Here we map the prefix h to the m -1 most recent tokens. It is worth noting that the order m is not necessarily the same with the order n for the n-gram component, as discussed in Section 4.3.</p><p>Automatic Selection of Dynamic Interpolation Weights. In simple linear interpolation, the weight λ is just a single number that may be set by hand. But we can define a more general and powerful model where the weights are a function of the prefix. We assume that the cache in which more records for the prefix are found is considered more reliable. For example, if more prefix sequences are found in the cache, we want to bias ourselves more towards using the cache. To accomplish that, inspired by Knight <ref type="bibr" target="#b26">[26]</ref>, we replace λ(h) with γ γ+H , where H is the number of times that the prefix h has been observed in the cache, and γ is a concentration parameter between 0 and infinity:</p><formula xml:id="formula_7">P (ti|h, cache) = γ γ + H • Pn-gram(ti|h) + H γ + H • Pcache(ti|h)<label>(8)</label></formula><p>We can see that if the prefix occurs few times (H is small), then the n-gram model probability will be preferred. But if the prefix occurs many times (H is large), then the cache component will be preferred. This setting avoids tricky hand-tuned parameters and make the interpolation weight self-adaptive for different n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Tuning the Cache</head><p>In this section, we will discuss several factors of the locality that would affect the performance of cache language model: cache context, cache scope, cache size, and cache order.</p><p>Cache Context. Cache context, by definition, is the lexical context from which n-grams in the cache are extracted. The cache is initially designed to capture the local regularities from the preceding tokens in the current file, (the "prolog", in contrast with the succeeding tokens, or "epilog"), and is well suited for the initial development (developing new files). However, for software development the maintenance and evolution are critical activities <ref type="bibr" target="#b2">[2]</ref>, where both prolog and epilog contexts are available. We believe our cache model will benefit from more context, which means that our model would be more useful for software maintenance, as shown in Section 5.2.3.</p><p>We hypothesized that the performance of the cache component would depend on the prolog more than the epilog. Recall that given an n-gram, the probability from the cache component along with the weight depends on its frequency in the cache and the total number of n-grams that share the same prefix (i.e. H) (Eq. 8). Hence, we expected different performance for the cache component built on the prolog and epilog, since there maybe different distributions of the n-grams that share the same prefix in the early and later parts of files. This turned out to be false. This aspect of results are discussed in Section 5.4.1.</p><p>Cache Scope. Cache scope is the part of codebase used to estimate the cache component. The intuitive way is to build the cache on the scope of the current file (file cache). However, one problem with the file cache is that those initial sentences in a file may not benefit from the file cache. We use a simple heuristic to alleviate the problem: we build the cache on the previous K tokens which could span files in local grouping (i.e. same subdirectory in the codebase). 3 In other words, the underlying assumption is that software is also localized 3 The files in the same directory are ordered by the filename.</p><p>in the codebase, in the sense that the files in the same subdirectory are locally similar. In this way, our cache model incorporates a latent scope to capture the code patterns in local files.</p><p>Cache Size. Cache size is defined as the maximum number of ngrams stored in the cache. Intuitively, it is more likely to observe a given n-gram if we use a larger cache. However, there has to be a trade-off between locality and coverage when we decide the size of cache. The smaller the cache is, the more local regularities the cache component can capture, at the cost of missing more possibly useful n-grams. In contrast, a larger cache would cover more ngrams while neglecting the localness of source code. An extreme case is to build the cache on the whole project, which captures only the global regularities. Moreover, maintaining a larger cache is computationally expensive. As a consequence, we need to decide an appropriate cache size to balance the locality and coverage of the cache model.</p><p>Cache Order. Cache order refers to the maximum order m of ngrams stored in the cache. Generally, the longer the prefix, the more accurate the suggestion. A longer prefix is more specific since it has more detailed information. For example, the long prefix "for (int i=0; i&lt;10; i" is more likely to predict the correct suggestion "++" than the short prefix "; i". Given that a cache stores much fewer tokens than the n-gram component, we can employ a higher order m without increasing much complexity. Data sparseness problems arise when we use higher order n-grams in the cache. We employ back-off techniques <ref type="bibr" target="#b23">[23]</ref> when a long prefix is not matched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we try to answer the following research questions: R1. Can the cache model capture the code locality? R2. Can the strength be leveraged for code suggestion? R3. Why does the cache model help?</p><p>R4. What factors affect the performance of the cache model?</p><p>In Section 5.1, we demonstrate, using standard cross-entropy over large software corpora, that the cache model is indeed capturing the localness of source code, for that the additional cache component estimated from the code locality decreases nearly one bit over the n-gram model. (Note that entropy is log scaled; intuitively, it means the cache model renders code nearly twice as predictable.)</p><p>In Section 5.2, we evaluate our cache language model for code suggestion, and show that the cache language model greatly improves suggestion accuracy for different programming languages, thus retaining the portability and simplicity of the n-gram approach. Furthermore, our cache model is especially useful for cross-project code suggestion and software maintenance.</p><p>In Section 5.3, we point out that identifiers benefit most from the cache model and contribute most to the accuracy improvement. In addition, the improvements are due to that the cache component captures the endemic and specific n-gram patterns in the locality.</p><p>In Section 5.4, we analyze four factors (described in Section 4.3) that will influence the cache model. Our analyses show that (1) there is no difference between prolog and epilog, while combining them achieves a further improvement; (2) the localness of software is also reflected at the localized structure of codebase; (3) the suggestion accuracy increases rapidly with cache size and order, saturating around 2K tokens and 6, respectively.</p><p>Setting. We performed the 10-fold cross-validation (in term of files) on each project in Table <ref type="table" target="#tab_0">1</ref>. We tested the statistical significance  We use the same orders for both n-gram and cache models. The trend for Python is similar to Java. using sign-test <ref type="bibr" target="#b14">[14]</ref>. For the cache model, we set concentration parameter γ = 1, the cache size K = 5000 and the cache order m = 10. <ref type="foot" target="#foot_1">4</ref> Our baseline is the lexical trigram model (i.e. n = 3 for the n-gram model), which is the same with the n-gram component in our cache language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Can the Code Locality be Captured?</head><p>We first studied whether the localness of software can be captured by the cache model. We use cross-entropy to measure how good a language model captures the regularities in a specific corpus. Given a corpus S = ti . . . tN , of length N , with a probability pM (S) estimated by a language model M . The cross-entropy is calculated as:</p><formula xml:id="formula_8">HM (S) = - 1 N log 2 pM (S) = - 1 N N 1 log 2 P (ti|h)<label>(9)</label></formula><p>A good model is expected to predict with high confidence the test data drawn from the same population, thus has a low entropy. Figure <ref type="figure" target="#fig_4">3</ref> shows the averaged cross-entropies of all projects for both Java and English. The two lines above are the averaged crossentropies for English corpora, while the lines below are for Java projects. For English corpora, the cache model only outperforms the n-gram model at unigram, while not for higher order n-grams. This is because English is very flexible and n-grams (n &gt; 1) are hardly found in a very local cache (5K words). For Java projects, however, the cache language model outperforms n-gram model consistently, showing that the frequency of the n-gram in the recent past, captured by the cache component, is a good indicator to the probability estimation for programming language. This reconfirms our hypothesis that programming language is quite localized and the localness can be captured by the cache model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cache Model for Code Suggestion</head><p>We now show that the cache model can be applied to code suggestion, although this is by no means the only application. As mentioned earlier, good language models will be useful for other applications: code porting <ref type="bibr" target="#b35">[35]</ref>, coding standards checking <ref type="bibr" target="#b3">[3]</ref>, correcting syntax errors <ref type="bibr" target="#b12">[12]</ref> We emulate code suggestion in two scenarios: coding in new files where only prolog is available (Section 5.2.1 and 5.2.2), and modifying the existing files where both prolog and epilog are available (Section 5.2.3).</p><p>Metric. We use the mean reciprocal rank (MRR) <ref type="bibr" target="#b41">[41]</ref> measure, which is a standard measure of the accuracy of techniques that provide ranked lists of candidate answers. For each token in the Table <ref type="table">3</ref>: Accuracy with various settings on Lucene. Here "File cache" denotes the cache built on the previous tokens in the test file, while "Extended cache" denotes the cache built on the previous 5K tokens. Although SLAMC has a slight higher accuracy, our model is language independent while theirs is not.</p><p>Model MRR Top1 Top5  <ref type="bibr" target="#b36">[36]</ref> N/A 53.60% 66.10% SLAMC <ref type="bibr" target="#b36">[36]</ref> N/A 64.00% 78.20% test data, the models produce a list of suggestion candidates ordered by the probability. The reciprocal rank of a suggestion list is the multiplicative inverse of the rank ranki of the real token ti. <ref type="foot" target="#foot_2">5</ref> MRR averages the reciprocal ranks for all the tokens T in the test data:</p><formula xml:id="formula_9">MRR = 1 |T | |T | i=1 1 ranki<label>(10)</label></formula><p>MRR is better at differentiating between the top few ranks, than the top-n accuracy measure (how often the correct suggestion in the top n suggestions). Mean reciprocal MRR=0.5 means that one may expect the correct suggestion to appear on average at about the second suggestion, and 0.33 indicates correct answer in the top 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Impact of Components</head><p>In this experiment, we evaluated the impact of different components on code suggestion accuracy. Table <ref type="table">3</ref> shows accuracy of different configurations on Lucene, our largest Java project. The first row is the lexical n-gram model <ref type="bibr" target="#b18">[18]</ref>. The second and third rows show the accuracies of using only the cache and extended cache components, which are built on the file and previous 5000 tokens respectively. The fourth and fifth rows are the combinations of the n-gram and cache components.</p><p>Surprisingly, using only cache component built on the previous 5K tokens ("Extended cache") outperforms the n-gram model built on about 2M tokens ("N-gram"). The improvement is mainly due to the increase of top1 accuracy (+9.4%), indicating that local context is quite specific and accurate. <ref type="foot" target="#foot_3">6</ref> Using only cache component built on the current file produces worse performance, confirming our hypothesis that the localness of software also reflects at the localized scope of the codebase (Section 4.3). Combining both n-gram and cache components achieves the best performance. The absolute improvements are 16.25%, 18.81%, and 12.75% for MRR, top1 and top5 accuracies, respectively. This suggests that n-gram and cache components capture different regularities in the source code.</p><p>We also list the accuracies reported in Nguyen et al. <ref type="bibr" target="#b36">[36]</ref>. We do not claim that those results are directly comparable to ours, because of potential subtle differences in tokenization, cross-validation setting, suggestion tool implementations etc, but we present them here for reference. Even though we have a lower baseline, our cache model achieves comparable performance with the semantic language model <ref type="bibr" target="#b36">[36]</ref>. Unlike the Nguyen et al. model, our cache model is quite simple, language-independent and requires no extra information besides the tokens; so the results are quite encouraging. For Python projects, the improvement is 14.01% from 40.26% to 54.27%. All absolute improvements are statistically significant at p &lt; 0.001 using sign-test <ref type="bibr" target="#b14">[14]</ref>. Table <ref type="table" target="#tab_4">5</ref> shows the result on three other languages: C, PHP, and Javascript. As seen, our cache model significantly (p &lt; 0.001) outperforms the n-gram approach consistently, indicating that the improvement of our cache model is language-independent.</p><p>Cache Model: Resource Usage. We measured the computational resources used by the cache model. See Table <ref type="table" target="#tab_5">6</ref>; the memory usage increases slightly, and performance is 3X slower than the n-gram model. We contribute the increase to the additional maintenance of cache and the real-time calculation of cache probabilities along with their weights. Note that the probabilities from the n-gram model are calculated offline and only one inquiry is needed for each token. In contrast, the cache is dynamically updating during the whole suggestion procedure, and thus the cache probabilities depend on the dynamic counts of n-grams and are calculated online. Even so, the code suggestion based on the cache model is fast enough to be imperceptible for interactive use in an IDE. It should be emphasized that the additional memory of the cache only depend on the size of cache (not shown) while is independent of the training corpus.</p><p>Case Study. Here are some interesting cases to show why our cache model improves the performance. The variable heap is only found in the file CBZip2OutputStream.java in Ant. It occurs 70 times, following 12 different prefixes, of which 11 occur more than 1 time. The n-gram model does not suggest correctly, whereas the cache model does when the n-grams are found in the cache. In Lucene, the identifier search works as various roles (e.g. method, variable) and occurs 3430 times. The top 3 prefixes that  <ref type="bibr" target="#b8">(8)</ref>, "i &lt;" <ref type="bibr" target="#b5">(5)</ref>, "endsWith (" <ref type="bibr" target="#b5">(5)</ref>. The cache model recommends correctly when the n-grams occur again, while the n-gram model does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Cross-Project Code Suggestion</head><p>We performed another experiment to simulate a new, "greenfield" project setting, where training data can only be obtained from other projects. Most data-driven approaches, including language models, are subject to the well-known problem of lack of portability to new domains/projects. Usually there is a substantial drop in performance when testing on data from a project different from the training data. Our data suggests that the cache model can alleviate the crossproject problem. Table <ref type="table" target="#tab_6">7</ref> shows the results of cross-project code suggestion. For each project we performed 10-fold cross-validation as in the previous experiment. The difference is that we used the other eight Java projects rather than the other nine folds for training. As seen, the lexical n-gram model has a accuracy of 46.18%, which is 6.62% lower than that of the in-project setting reported in Table <ref type="table" target="#tab_3">4</ref>, which empirically reconfirms the cross-project problem of language models. Comparing the accuracies in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_6">7</ref>, we can see that the gap between the cross-project and in-project settings decreases from 6.62% (46.18% versus 52.80%) to 2.04% (65.55% versus 67.59%). We attribute this to that the cache component, built on a few n-grams in a quite local context, makes up for the loss of the project-specific code regularities to some extent. This also indicates that programming language is quite localized, since project-specific code patterns are grouped locally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Code Suggestion in Software Maintenance</head><p>In this experiment, we emulate code suggestion in software maintenance, where we change existing files rather than writing new files. When predicting a token, we build the cache on the rest tokens  <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_7">8</ref>, we found that our cache model is even more useful for software maintenance, by achieving significant improvement in suggestion accuracies (69.32% versus 67.59%) with less tokens (0.9K versus 5K). This result reconfirms our hypothesis that source code is localized at the file level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Why Does the Cache Model Help?</head><p>In this section, we investigated the reasons why the cache model works. We first checked which tokens benefit most from our cache model, then investigated whether our cache model is indeed capturing the localized regularities of both endemic n-gram patterns and frequency-biased patterns of non-endemic n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Token Abstraction Analysis</head><p>In this experiment we investigated which tokens benefit most from our cache model. We divided the tokens into 7 classes of token abstractions: method calls (ID_MCALL), type identifiers (ID_TYPE), variables (ID_VAR), literals (LIT), keywords (KW), operators (OP), and separators (SEP). The first four classes belong to open-vocabulary (unlimited number of vocabulary) while the latter three classes are close-vocabulary (limited number of vocabulary). Table <ref type="table" target="#tab_10">10</ref> lists the results for different token abstractions.</p><p>As one would expect, the identifiers have a low suggestion accuracy (ranging from 19.83% to 28.25%), compared with the overall accuracy of 52.80%. A recent study of code cross-entropy <ref type="bibr" target="#b4">[4]</ref> shows that identifiers contribute most to the uncertainty of the source code. Our results reconfirm these findings. We have found that the use of local caches drastically improves the suggestion accuracy of identifiers (+21.91% to +27.38%).</p><p>In contrast, the programming-specific abstractions (i.e., keywords, operators, and separators), have relatively higher accuracy. For example, the separators, which take a large proportion of the total tokens, have a suggestion accuracy of 75.20%. Consequently, their improvements from the cache model are relatively small (8.41% to 13.10%). This is because the regularities of these abstractions are more specific to the programming language syntax, which can be captured by the lexical n-gram models from the training data. The keyword (KW) new occurs 25459 times in Lucene, and achieves an improvement of 7.59% in MRR by using the cache model. 75% of the prefixes that it follows contain at least one identifier or literal. For example, nearly half of the prefixes are the instances of the pattern of constructing a new object and assigning it to a variable. This suggests that the keywords also benefit from the locality of identifiers and literals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">N -gram Patterns Analysis</head><p>In this experiment, we investigated whether our cache model is indeed capturing the localized regularities of endemic n-gram patterns and of non-endemic but specific n-grams. We use the terms "Endemic" and "NonEndemic" to denote the improvements from the two kinds of patterns above, respectively. We distinguished the accuracy improvement for a given token as follows. If the correct suggestion only occurs in the cache component, we attribute the improvement to the capture of endemic n-gram patterns from the cache component; otherwise, it is from the capture of non-endemic but specific n-grams, since the correct suggestion is found in both n-gram and cache components; however, it is assigned a higher rank because the cache component correctly captures the more local usage patterns.</p><p>Table <ref type="table" target="#tab_9">11</ref> lists the improvements (endemic and non-endemic) for different token abstractions on Java projects. Several observations can be made. First, the cache component indeed captures both the endemic n-grams patterns and the non-endemic (but locally specific) patterns of n-grams. They achieve the absolute accuracy improvements of +9.30% and +5.42% respectively on Java, and +9.57% and +3.85 on Python. Second, different abstractions benefit differently from the two kinds of patterns. For identifiers (ID_*) and literals (LIT), the coverage of endemic n-grams improves the most. This jives with intuition, because a large portion of new identifiers introduced by new functions and files can only be captured by the cache component. In contrast, the language-specific tokens (KW, OP, and SEP) benefit similarly from both patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">What Factors Affect the Cache Model?</head><p>Next, we investigate several factors' influence on the cache model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">Cache Context</head><p>To investigate the influence of cache context on the cache model, we build the cache on different kinds of contexts in the current file: prolog, epilog, and both. It came as a surprise to us that, in general,  there is no difference between the prolog and epilog, as shown in Table <ref type="table" target="#tab_11">12</ref>. We had naively assumed that the cache built on prolog would reflect the "burstiness" of the n-grams, and hence outperforms that on epilog. It turned out to be wrong. The results suggest that code locality holds its trend in the file. Intuitively, the cache model benefits from more context, reaffirming our finding that the cache model is especially useful for code suggestion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">Cache Scope</head><p>It should be emphasized that the cache component is built on the previous 5K tokens, which may across different files. Therefore, the performance of our model can be influenced by the order of files. Table <ref type="table" target="#tab_12">13</ref> lists the suggestion results with different file orders: "Ordinal" (the default setting in above experiments) and "Reverse" denote that all localized files in the same subdirectory are grouped together and are sorted in ascending and descending order respectively, while "Random" denotes that all files are in randomized order and the localized files are not grouped together. First, using only the cache component built on the localized files, either from ordinal or reverse order, outperforms that on the random files consistently for both languages, indicating that the localness of code is also reflected at the localized structure of codebase. The improvement for Python  is not so obvious as Java. One possible reasons is that in Python projects a file consists of multiple related classes. Second, working together with n-gram component (i.e. + n-gram), we achieve similar performances for different settings. We conjecture that all settings exploit the previous tokens in the current file, which captures most of the localized regularities. This suggests that our cache model is robust by taking advantages of both n-gram and cache components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">Cache Size and Order</head><p>The cache size and order have similar trends of the variation of MRR scores with the increase of the value, as plotted in Figure <ref type="figure" target="#fig_6">4</ref>. The MRR score increased rapidly with cache size, saturating around 2K tokens. It is worth mentioning that we have already achieved an absolute improvement of 12.1% when the cache size K = 500, which confirms our hypothesis that the local knowledge of source code is assembled through a specific contextual piece. When we employed an unlimited cache (we stored in the cache all the previous tokens in the test data), the suggestion accuracy is 68.86%, which is only 1.27% higher than when K = 5000. This suggests that localized regularities cannot be captured beyond the specific context.</p><p>Similarly, the accuracy went up sharply when the order m increased from 3 to 6, while was improved slowly when the order continue increased. One possible reason is that prefixes of 5 tokens are precise enough for the suggestion task and longer n-grams (e.g. 10-grams) that are locally repetitive are usually extended from these shorter n-grams (e.g. 6-grams).</p><p>Threats to Validity and Limitations. The most likely threat to the validity of our results is the corpus we used. Although we chose many projects with large numbers of LOCs, we cannot say for sure how representative our corpus is in practice. Nevertheless, the commonality we have seen across different programming languages gives us confidence that our results hold generally.</p><p>The threat to internal validity includes the influence of the settings for the cache model (e.g. γ and cache scope). For instance, the cache model would be affected by the different orders of the the localized files under the same subdirectory (Section 5.4.2). However, the cache model built on the current file regardless of the localized files, still achieves over 12% improvement in MRR score, which is only 2.6% lower than that built on the localized files (Table <ref type="table">3</ref>). For code suggestion in software maintenance where both prolog and epilog contexts are available, the cache model built on the current file achieves comparable performance with that built on the localized files (Section 5.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Applying NLP to Software Engineering. As more repositories of open source software have become publicly available (e.g. on GitHub and BitBucket), software engineering researchers have turned to empirical methods to study the process of developing and maintaining software <ref type="bibr">[11, 27, 38, 40, 42-45, 47, 49]</ref>. Statistical language modeling has also emerged as an approach to exploit this abundance of data <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b33">33]</ref>. Gabel and Su <ref type="bibr" target="#b16">[16]</ref> reported on the non-uniqueness of even large code fragments; Hindle et al. <ref type="bibr" target="#b18">[18]</ref> showed that n-gram models could capture the predictable properties of source code and thus support code suggestion. Along the same direction, to alleviate the data sparseness problem that n-gram models face, Allamanis and Sutton <ref type="bibr" target="#b4">[4]</ref> learned language models over more data from different domains, while Nguyen et al. <ref type="bibr" target="#b36">[36]</ref> exploited a more general unit-semantic tokens.</p><p>Our approach is complementary to theirs: it captures different regularities of source code, that is, the localness of software. The simplicity of our cache model makes it broadly applicable, and easy to incorporate into other approaches: just use an interpolation of the cache model estimated probability with that from an existing model. Notably, Allamanis and Sutton <ref type="bibr" target="#b4">[4]</ref> showed that the identifiers contribute most to the uncertainty of source code, even on a gigatoken corpus. Our approach works especially well for the identifiers.</p><p>Analysis of Identifiers. Identifiers take up the majority of source code tokens <ref type="bibr">[9]</ref>, hence play an important role in software engineering in both code cognition <ref type="bibr" target="#b31">[31]</ref> and understanding <ref type="bibr" target="#b28">[28]</ref>. It has been shown that identifiers are most difficult to predict <ref type="bibr" target="#b4">[4]</ref>. Our results confirm this finding: the n-gram approach only achieved suggestion accuracies of 22.11% to 26.05% on identifiers, which is half of other types (it is even worse in cross-project code suggestion). With the help of the cache component, it greatly improves the suggestion accuracies of identifiers.</p><p>There has been a line of research on predicting the identifiers based on the contextual information <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b48">48]</ref>. For instance, Holmes et al. <ref type="bibr" target="#b19">[19]</ref> and Kersten and Murphy <ref type="bibr" target="#b25">[25]</ref> incorporated contextual information to produce better recommendations of relevant API examples. Bruch et al. <ref type="bibr" target="#b10">[10]</ref> and Robbes and Lanza <ref type="bibr" target="#b39">[39]</ref> concerned predicting the most likely method calls from prior knowledge in similar scenarios. Nguyen et al. <ref type="bibr" target="#b34">[34]</ref> used graph algorithms to predict API which is similar to the current code. Our approach is generally complementary: the prediction can be improved using local statistics of corpus.</p><p>Local regularities have been exploited in the past to help splitting source code identifiers <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b30">30]</ref>. They focused on splitting compound identifiers based on the observation that terms composing identifiers usually appear in a local context (e.g. method or file).</p><p>Cache LM in NL Community. It is worthy emphasizing that our cache language model is different from that is used in NL community. As noted in Section 3.1, natural language is not as locally repetitive as programming languages at the levels of n-grams. Therefore, the cache component in NLP is usually a unigram model and working with class language models <ref type="bibr" target="#b29">[29]</ref>. A typical class language model <ref type="bibr" target="#b8">[8]</ref> first predicts the classes of the words (e.g. grammatical part-of-speech), then transforms the classes to the words on the basis of their frequency within the classes. Nguyen et al. <ref type="bibr" target="#b36">[36]</ref> implemented a similar cache LM as in NLP, where semantic tokens worked as word classes and the cache for variables stored all the variables that belong to the same or containing scope in the search path for each semantic token.</p><p>While their work does not explicitly use a cache language model to capture the general "localness" of software over a large corpus, the differences lie in: (1) we directly build the cache model at the granularity of n-grams for programming languages; (2) we cache the n-grams of all types of tokens, not only variables; and (3) we don't require any additional information (e.g. type information, code path) which is difficult and expensive to produce. Our work is based on the observation that programming languages are far more regular than natural language <ref type="bibr" target="#b18">[18]</ref> and code fragments of surprisingly large size tend to reoccur <ref type="bibr" target="#b16">[16]</ref>. The main advantage of our cache language model is that the cache component exploits more contextual information and makes more precise suggestions. Therefore, the probabilities of the cache component is directly interpolated with those from the n-gram component. Our additional caching component achieves better improvement in top1 accuracy than that used in Nguyen's work <ref type="bibr" target="#b36">[36]</ref> on the same data: +9.4% versus +0.7%. We contribute the improvement to the broader coverage of tokens and more expressive prefixes exploited in the cache component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>The cache language model introduced here captures the localness of software. It augments n-gram models with a cache component to capture the endemic and specific n-gram patterns in the locality. Experimental results show that the cache language model captures the localized regularities in the source code. Consequently, the suggestion engine based on the cache model improves the suggestion accuracy over the n-gram approach, especially in the cross-project setting and during software maintenance. It should be emphasized that our cache language model is quite simple and requires no additional information other than the tokens, thus is applicable to all programming languages. Furthermore, our approach is complementary to most state-of-the-art works since they capture different regularities of the source code.</p><p>Besides the simple n-gram models, our method is also applicable to other paradigms such as the semantic language model <ref type="bibr" target="#b36">[36]</ref>. Another interesting direction is to further explore the potential of the cache, such as building the cache on more semantic scopes (e.g. import the same header files or packages, or files that are changed together with the current files), or introducing a decaying factor to account for recency (i.e. n-gram distance) in the cache.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distance between repetitive n-grams.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Cross-entropies of Java (below) and English (above). We use the same orders for both n-gram and cache models. The trend for Python is similar to Java. using sign-test<ref type="bibr" target="#b14">[14]</ref>. For the cache model, we set concentration parameter γ = 1, the cache size K = 5000 and the cache order m = 10.4 Our baseline is the lexical trigram model (i.e. n = 3 for the n-gram model), which is the same with the n-gram component in our cache language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: The influence of cache size and order on the suggestion accuracy (Java). The trend for Python is similar to Java.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Java and Python projects data and English corpora. Distinct tokens constitute the vocabulary. Thus Ant has a total of 919,148 tokens, composed of 27,008 distinct tokens.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Tokens</cell></row><row><cell>Java Project</cell><cell>Version</cell><cell>Lines</cell><cell cols="2">Total Distinct</cell></row><row><cell>Ant</cell><cell cols="2">20110123 254,457</cell><cell>919,148</cell><cell>27,008</cell></row><row><cell>Batik</cell><cell cols="3">20110118 367,293 1,384,554</cell><cell>30,298</cell></row><row><cell>Cassandra</cell><cell cols="2">20110122 135,992</cell><cell>697,498</cell><cell>13,002</cell></row><row><cell>Log4J</cell><cell>20101119</cell><cell>68,528</cell><cell>247,001</cell><cell>8,056</cell></row><row><cell>Lucene</cell><cell cols="3">20100319 429,957 2,130,349</cell><cell>32,676</cell></row><row><cell>Maven2</cell><cell>20101118</cell><cell>61,622</cell><cell>263,831</cell><cell>7,637</cell></row><row><cell>Maven3</cell><cell cols="2">20110122 114,527</cell><cell>462,397</cell><cell>10,839</cell></row><row><cell>Xalan-J</cell><cell cols="3">20091212 349,837 1,085,022</cell><cell>39,383</cell></row><row><cell>Xerces</cell><cell cols="2">20110111 257,572</cell><cell>992,623</cell><cell>19,542</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Tokens</cell></row><row><cell>Python Project</cell><cell>Version</cell><cell>Lines</cell><cell cols="2">Total Distinct</cell></row><row><cell>Boto</cell><cell>20110714</cell><cell>53,969</cell><cell>393,900</cell><cell>10,164</cell></row><row><cell>Bup</cell><cell>20110115</cell><cell>17,287</cell><cell>179,645</cell><cell>5,843</cell></row><row><cell>Django-cms</cell><cell>20110824</cell><cell>37,740</cell><cell>567,733</cell><cell>7,526</cell></row><row><cell>Django</cell><cell cols="3">20120215 205,862 2,137,399</cell><cell>55,486</cell></row><row><cell>Gateone</cell><cell>20111013</cell><cell>24,051</cell><cell>230,048</cell><cell>8,212</cell></row><row><cell>Play</cell><cell cols="3">20111202 231,177 1,893,611</cell><cell>68,928</cell></row><row><cell>Reddit</cell><cell>20100721</cell><cell>52,091</cell><cell>527,815</cell><cell>14,216</cell></row><row><cell>Sick-beard</cell><cell>20110408</cell><cell cols="2">99,586 1,005,823</cell><cell>53,345</cell></row><row><cell>Tornado</cell><cell>20120130</cell><cell>16,326</cell><cell>622,715</cell><cell>5,893</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Tokens</cell></row><row><cell>English Corpus</cell><cell>Version</cell><cell>Lines</cell><cell cols="2">Total Distinct</cell></row><row><cell>Brown</cell><cell>20101101</cell><cell cols="2">81,851 1,161,192</cell><cell>56,057</cell></row><row><cell>Gutenberg</cell><cell>20101101</cell><cell cols="2">55,578 2,621,613</cell><cell>51,156</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Percentage of the endemic n-grams (only found in a single file). "Freq." denotes the frequency of the n-grams in the file. The denominator is the number of n-grams in the corpus.</figDesc><table><row><cell>Lang.</cell><cell cols="5">Freq. 1-gram 3-gram 6-gram 10-gram</cell></row><row><cell></cell><cell>1</cell><cell cols="3">4.65% 25.63% 53.98%</cell><cell>70.22%</cell></row><row><cell>Java</cell><cell>2</cell><cell cols="3">3.71% 11.13% 16.17%</cell><cell>14.34%</cell></row><row><cell></cell><cell>1</cell><cell cols="3">8.58% 41.82% 73.65%</cell><cell>83.79%</cell></row><row><cell>Python</cell><cell>2</cell><cell cols="3">5.29% 14.44% 17.52%</cell><cell>12.47%</cell></row><row><cell></cell><cell>1</cell><cell cols="4">3.28% 72.52% 99.70% 100.00%</cell></row><row><cell>English</cell><cell>2</cell><cell>1.39%</cell><cell>3.44%</cell><cell>0.65%</cell><cell>0.20%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (MRR) of code suggestion. "Improv." denotes the absolute improvement of cache language model ("Cache LM") over the n-gram model ("N -gram"). "Sig." denotes the statistical significance tests against "N -gram" using sign-test.</figDesc><table><row><cell cols="4">Java Proj. N -gram Cache LM Improv.</cell><cell>Sig.</cell><cell cols="4">Python Proj. N -gram Cache LM Improv.</cell><cell>Sig.</cell></row><row><cell>Ant</cell><cell>53.78%</cell><cell>65.07%</cell><cell cols="2">11.29% &lt; .001</cell><cell>Boto</cell><cell>45.76%</cell><cell>59.50%</cell><cell>13.74% &lt; .001</cell></row><row><cell>Batik</cell><cell>50.56%</cell><cell>68.34%</cell><cell cols="2">17.78% &lt; .001</cell><cell>Bup</cell><cell>39.43%</cell><cell>50.07%</cell><cell>10.64% &lt; .001</cell></row><row><cell cols="2">Cassandra 53.06%</cell><cell>66.73%</cell><cell cols="2">13.67% &lt; .001</cell><cell cols="2">Django-cms 63.70%</cell><cell>74.34%</cell><cell>10.64% &lt; .001</cell></row><row><cell>Log4j</cell><cell>51.74%</cell><cell>66.47%</cell><cell cols="2">14.73% &lt; .001</cell><cell>Django</cell><cell>44.06%</cell><cell>61.24%</cell><cell>17.18% &lt; .001</cell></row><row><cell>Lucene</cell><cell>51.88%</cell><cell>68.13%</cell><cell cols="2">16.25% &lt; .001</cell><cell>Gateone</cell><cell>38.94%</cell><cell>56.58%</cell><cell>17.64% &lt; .001</cell></row><row><cell>Maven2</cell><cell>53.96%</cell><cell>68.74%</cell><cell cols="2">14.78% &lt; .001</cell><cell>Play</cell><cell>42.41%</cell><cell>56.28%</cell><cell>13.87% &lt; .001</cell></row><row><cell>Maven3</cell><cell>56.79%</cell><cell>68.99%</cell><cell cols="2">12.20% &lt; .001</cell><cell>Reddit</cell><cell>40.08%</cell><cell>50.53%</cell><cell>10.45% &lt; .001</cell></row><row><cell>Xalan</cell><cell>50.86%</cell><cell>65.44%</cell><cell cols="2">14.58% &lt; .001</cell><cell>Sick-beard</cell><cell>37.89%</cell><cell>50.40%</cell><cell>12.51% &lt; .001</cell></row><row><cell>Xerces</cell><cell>52.61%</cell><cell>70.38%</cell><cell cols="2">17.77% &lt; .001</cell><cell>Tornado</cell><cell>49.33%</cell><cell>63.43%</cell><cell>14.10% &lt; .001</cell></row><row><cell>Average</cell><cell>52.80%</cell><cell>67.59%</cell><cell>14.79%</cell><cell></cell><cell>Average</cell><cell>44.62%</cell><cell>58.04%</cell><cell>13.42%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of suggestion for other languages.</figDesc><table><row><cell>Lang.</cell><cell cols="3">N -gram Cache LM Improv.</cell></row><row><cell>C</cell><cell>48.44%</cell><cell>62.14%</cell><cell>13.70%</cell></row><row><cell>PHP</cell><cell>56.98%</cell><cell>68.71%</cell><cell>11.73%</cell></row><row><cell cols="2">Javascript 49.28%</cell><cell>61.72%</cell><cell>12.44%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Speed and Memory Comparison. "µs" denotes microsecond (10 -6 second).Cache Model Portability. In this experiment, we compared our cache model with the lexical n-gram model in two data sets of Java and Python. Table4lists the comparison results. For Java projects, the cache model achieves an averaged improvement of 14.79% in MRR from 52.80% to 67.59% over the lexical n-gram model.</figDesc><table><row><cell></cell><cell cols="2">Speed (µs/token)</cell><cell cols="2">Memory (Mb)</cell></row><row><cell>Lang.</cell><cell cols="4">N -gram Cache LM N -gram Cache LM</cell></row><row><cell>Java</cell><cell>6.8</cell><cell>23.4</cell><cell>15.4</cell><cell>16.6</cell></row><row><cell>Python</cell><cell>6.9</cell><cell>21.2</cell><cell>12.6</cell><cell>13.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Cross-project code suggestion (Java).</figDesc><table><row><cell>Proj.</cell><cell cols="3">N -gram Cache LM Improv.</cell><cell>Sig.</cell></row><row><cell>Ant</cell><cell>47.13%</cell><cell>62.41%</cell><cell cols="2">15.28% &lt; .001</cell></row><row><cell>Batik</cell><cell>41.83%</cell><cell>65.65%</cell><cell cols="2">23.82% &lt; .001</cell></row><row><cell cols="2">Cassandra 41.54%</cell><cell>63.55%</cell><cell cols="2">22.01% &lt; .001</cell></row><row><cell>Log4j</cell><cell>46.13%</cell><cell>64.63%</cell><cell cols="2">18.50% &lt; .001</cell></row><row><cell>Lucene</cell><cell>40.82%</cell><cell>64.72%</cell><cell cols="2">23.90% &lt; .001</cell></row><row><cell>Maven2</cell><cell>58.48%</cell><cell>70.61%</cell><cell cols="2">12.13% &lt; .001</cell></row><row><cell>Maven3</cell><cell>53.08%</cell><cell>68.15%</cell><cell cols="2">15.07% &lt; .001</cell></row><row><cell>Xalan</cell><cell>41.28%</cell><cell>61.98%</cell><cell cols="2">20.70% &lt; .001</cell></row><row><cell>Xerces</cell><cell>45.32%</cell><cell>68.29%</cell><cell cols="2">22.97% &lt; .001</cell></row><row><cell>Average</cell><cell>46.18%</cell><cell>65.55%</cell><cell>19.37%</cell></row></table><note><p>it follows are "lucene ." (1611), "searcher ." (414), "solr ." (366). Given a specific file FrenchStemmer.java, it occurs 35 times and works as a variable (String[] search). The top3 prefixes are "[ ]"</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Maintaining code suggestion (Java). The cache is built only on the current file rather than the previous 5K tokens.</figDesc><table><row><cell>Proj.</cell><cell cols="3">N -gram Cache LM  *  Improv.</cell><cell>Sig.</cell></row><row><cell>Ant</cell><cell>53.78%</cell><cell>67.07%</cell><cell cols="2">13.29% &lt; .001</cell></row><row><cell>Batik</cell><cell>50.56%</cell><cell>70.05%</cell><cell cols="2">19.49% &lt; .001</cell></row><row><cell cols="2">Cassandra 53.06%</cell><cell>69.10%</cell><cell cols="2">16.04% &lt; .001</cell></row><row><cell>Log4j</cell><cell>51.74%</cell><cell>68.31%</cell><cell cols="2">16.57% &lt; .001</cell></row><row><cell>Lucene</cell><cell>51.88%</cell><cell>70.28%</cell><cell cols="2">18.40% &lt; .001</cell></row><row><cell>Maven2</cell><cell>53.96%</cell><cell>70.06%</cell><cell cols="2">16.10% &lt; .001</cell></row><row><cell>Maven3</cell><cell>56.79%</cell><cell>70.81%</cell><cell cols="2">14.02% &lt; .001</cell></row><row><cell>Xalan</cell><cell>50.86%</cell><cell>65.98%</cell><cell cols="2">15.12% &lt; .001</cell></row><row><cell>Xerces</cell><cell>52.61%</cell><cell>72.24%</cell><cell cols="2">19.63% &lt; .001</cell></row><row><cell>Average</cell><cell>52.80%</cell><cell>69.32%</cell><cell>16.52%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Descriptions of token abstractions. It is different from previous settings at that we incorporate both prolog and epilog contexts here rather than only exploiting the prolog context in the above experiments. Comparing Tables</figDesc><table><row><cell>Abstraction</cell><cell>Description</cell><cell>Examples</cell></row><row><cell cols="3">ID_MCALL method calls getMessage(), call()</cell></row><row><cell>ID_TYPE</cell><cell>types</cell><cell>QueryNode, ScoreMode</cell></row><row><cell>ID_VAR</cell><cell>variables</cell><cell>i, size, node</cell></row><row><cell>LIT</cell><cell>literals</cell><cell>"String", 'a', 270</cell></row><row><cell>KW</cell><cell>keywords</cell><cell>int, for, while</cell></row><row><cell>OP</cell><cell>operators</cell><cell>&gt;, &lt;, +, =, -</cell></row><row><cell>SEP</cell><cell>separators</cell><cell>{, }, (, ), [, ], ;</cell></row><row><cell>in the current file.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Improvements from different patterns captured by the cache component. "Endemic" and "NonEndemic" denote the endemic n-gram patterns and the frequency-biased patterns of non-endemic n-grams, respectively. In Lucene, the most frequent method length denotes the function of getting the length of a object, and therefore it usually follows "ID_VAR .". The method obtains improvements of 30.33%, 39.72%, and 5.28% in MRR, Top1, and Top10 accuracies respectively, indicating that most of the improvement is contributed by reranking the candidates.</figDesc><table><row><cell></cell><cell cols="2">Java</cell><cell cols="2">Python</cell></row><row><cell>Abstraction</cell><cell>Endemic</cell><cell>NonEndemic</cell><cell>Endemic</cell><cell>NonEndemic</cell></row><row><cell cols="2">ID_MCALL +21.81%</cell><cell>+5.57%</cell><cell>+12.92%</cell><cell>+0.92%</cell></row><row><cell>ID_TYPE</cell><cell>+16.74%</cell><cell>+5.61%</cell><cell>+13.67%</cell><cell>+2.81%</cell></row><row><cell>ID_VAR</cell><cell>+17.73%</cell><cell>+4.18%</cell><cell>+16.33%</cell><cell>+2.89%</cell></row><row><cell>LIT</cell><cell>+15.24%</cell><cell>+2.95%</cell><cell>+11.43%</cell><cell>+1.78%</cell></row><row><cell>KW</cell><cell>+5.06%</cell><cell>+5.61%</cell><cell>+6.15%</cell><cell>+4.35%</cell></row><row><cell>OP</cell><cell>+4.73%</cell><cell>+8.37%</cell><cell>+6.05%</cell><cell>+5.24%</cell></row><row><cell>SEP</cell><cell>+3.26%</cell><cell>+5.15%</cell><cell>+5.39%</cell><cell>+4.91%</cell></row><row><cell>ALL</cell><cell>+9.30%</cell><cell>+5.42%</cell><cell>+9.57%</cell><cell>+3.85%</cell></row><row><cell>Case Study.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Improvements for different abstractions.In-projectCross-project Abstraction Percent N -gram Cache LM Improv. N -gram Cache LM Improv.</figDesc><table><row><cell></cell><cell></cell><cell>Java</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ID_MCALL</cell><cell>6.20% 19.83%</cell><cell>47.21%</cell><cell cols="2">27.38% 12.91%</cell><cell>45.43%</cell><cell>32.52%</cell></row><row><cell>ID_TYPE</cell><cell>7.15% 23.98%</cell><cell>46.33%</cell><cell cols="2">22.35% 16.84%</cell><cell>44.62%</cell><cell>27.78%</cell></row><row><cell>ID_VAR</cell><cell>17.77% 28.25%</cell><cell>50.16%</cell><cell cols="2">21.91% 19.08%</cell><cell>48.47%</cell><cell>29.39%</cell></row><row><cell>LIT</cell><cell>7.20% 26.21%</cell><cell>44.41%</cell><cell cols="2">18.20% 22.45%</cell><cell>42.41%</cell><cell>19.96%</cell></row><row><cell>KW</cell><cell>10.35% 54.34%</cell><cell>65.00%</cell><cell cols="2">10.66% 51.07%</cell><cell>63.90%</cell><cell>12.83%</cell></row><row><cell>OP</cell><cell>14.78% 68.20%</cell><cell>81.30%</cell><cell cols="2">13.10% 59.67%</cell><cell>78.83%</cell><cell>19.16%</cell></row><row><cell>SEP</cell><cell>36.54% 75.20%</cell><cell>83.61%</cell><cell>8.41%</cell><cell>69.44%</cell><cell>81.28%</cell><cell>11.84%</cell></row><row><cell>ALL</cell><cell>100% 52.80%</cell><cell>67.59%</cell><cell cols="2">14.79% 46.18%</cell><cell>65.55%</cell><cell>19.37%</cell></row><row><cell></cell><cell></cell><cell>Python</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ID_MCALL</cell><cell>2.38% 13.44%</cell><cell>27.28%</cell><cell>13.84%</cell><cell>6.81%</cell><cell>25.74%</cell><cell>18.93%</cell></row><row><cell>ID_TYPE</cell><cell>3.62% 21.92%</cell><cell>38.40%</cell><cell>16.48%</cell><cell>8.83%</cell><cell>33.04%</cell><cell>24.21%</cell></row><row><cell>ID_VAR</cell><cell>25.03% 22.35%</cell><cell>41.57%</cell><cell cols="2">19.22% 15.18%</cell><cell>39.75%</cell><cell>24.57%</cell></row><row><cell>LIT</cell><cell>13.54% 20.15%</cell><cell>33.36%</cell><cell cols="2">13.21% 10.26%</cell><cell>30.23%</cell><cell>19.97%</cell></row><row><cell>KW</cell><cell>8.15% 39.15%</cell><cell>49.66%</cell><cell cols="2">10.51% 33.15%</cell><cell>47.09%</cell><cell>13.94%</cell></row><row><cell>OP</cell><cell>14.24% 66.84%</cell><cell>78.14%</cell><cell cols="2">11.30% 60.32%</cell><cell>76.10%</cell><cell>15.78%</cell></row><row><cell>SEP</cell><cell>33.03% 67.41%</cell><cell>77.71%</cell><cell cols="2">10.30% 59.24%</cell><cell>74.89%</cell><cell>15.65%</cell></row><row><cell>ALL</cell><cell>100% 44.63%</cell><cell>58.04%</cell><cell cols="2">13.41% 36.27%</cell><cell>55.62%</cell><cell>19.35%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>The influence of cache context on the code suggestion accuracy. We build the cache on the current file.</figDesc><table><row><cell></cell><cell cols="2">Cache Context</cell><cell></cell></row><row><cell>Lang.</cell><cell>Model Prolog</cell><cell>Epilog</cell><cell>Both</cell></row><row><cell></cell><cell cols="3">cache 41.47% 41.53% 52.98%</cell></row><row><cell>Java</cell><cell cols="3">+ n-gram 64.96% 65.00% 69.32%</cell></row><row><cell></cell><cell cols="3">cache 36.53% 36.50% 47.73%</cell></row><row><cell cols="4">Python + n-gram 56.55% 57.17% 62.43%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>The influence of cache scope on the code suggestion accuracy. We build the cache on the previous 5K tokens.</figDesc><table><row><cell></cell><cell>File Order</cell></row><row><cell>Lang.</cell><cell>Model Ordinal Reverse Random</cell></row><row><cell></cell><cell>cache 53.42% 53.44% 47.51%</cell></row><row><cell>Java</cell><cell>+ n-gram 67.59% 67.59% 66.56%</cell></row><row><cell></cell><cell>cache 42.81% 42.81% 42.01%</cell></row><row><cell cols="2">Python + n-gram 57.23% 57.22% 56.86%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>If the n-gram is found in different files, we skip the other files that the n-gram does not occur in when we calculate the distance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We will discuss the influence of the cache size and order in Section5.4, and γ in the future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>If the ti is not in the suggestion list, the reciprocal rank is 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>An increase of +3.2% is still found for top 1 accuracy when we set the same order (i.e. 3) for both n-gram and cache components.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>This material is based upon work supported by the National Science Foundation under Grant No. 1247280. We thank the three anonymous reviewers for their insightful comments on an earlier draft of the paper. We gratefully acknowledge the help of Hao Zhong, who provided us with invaluable suggestions, of Bogdan Vasilescu, who gave a careful check of typos.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated identifier completion and replacement</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Abebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tonella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSMR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The maintenance problem of application software: an empirical analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Alkhatib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software Maintenance: Research and Practice</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="83" to="104" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning natural coding conventions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM SIGSOFT FSE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining source code repositories at massive scale using language modelling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Programming by voice, VocalProgramming</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldthwaite</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASSETS</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="149" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A maximum likelihood approach to continuous speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="190" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A statistical approach to machine translation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cocke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Roossin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="79" to="85" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Class-based n-gram models of natural language</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Mercer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J D</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="479" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A holistic approach to software quality at work</title>
		<author>
			<persName><forename type="first">M</forename><surname>Broy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deißenböck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pizka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WCSQ</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from examples to improve code completion systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bruch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Monperrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mezini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="213" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Automatically documenting program changes</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Buse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Weimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Syntax errors just aren&apos;t natural: improving error reporting with language models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WCRE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="252" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Nomen est omen: Analyzing the language of function identifiers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Caprile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tonella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WCRE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="112" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The statistical sign test</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">236</biblScope>
			<biblScope unit="page" from="557" to="566" />
			<date type="published" when="1946">1946</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining source code to automatically split identifiers for software analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Enslen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MSR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A study of the uniqueness of source code</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Tidier: an identifier splitting approach using speech recognition techniques</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guerrouj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Di Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Antoniol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Guéhéneuc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Software: Evolution and Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="575" to="599" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximate structural context matching: An approach to recommend relevant examples</title>
		<author>
			<persName><forename type="first">R</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="952" to="970" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An evaluation of the strategies of sorting, filtering, and grouping API methods for code completion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Code template inference using language models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tairas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACMSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="1" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Design of a linguistic statistical decoder for the recognition of continuous speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jelinek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="250" to="256" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimation of probabilities from sparse data for the language model component of a speech recognizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1987">1987</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="400" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A spelling correction program based on a noisy channel model</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Kernighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="205" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using task context to improve programmer productivity</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bayesian inference with tears</title>
		<author>
			<persName><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Tutorial Workbook</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time specification patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H C</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic clustering: identifying topics in source code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ducasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gírba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Software Technology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="230" to="243" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A cache-based natural language model for speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="570" to="583" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quantifying identifier quality: an analysis of trends</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lawrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Binkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="388" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cognitive perspectives on the role of naming in computer programs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liblit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Begel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sweetser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPIG</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jungloid mining: helping to navigate the API jungle</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mandelin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kimelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="48" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Natural language models for predicting programming comments</title>
		<author>
			<persName><forename type="first">D</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph-based pattern-oriented, context-sensitive source code completion</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamrawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Al-Kofahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="69" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lexical statistical machine translation for language migration</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="651" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A statistical semantic language model for source code</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="532" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The modular structure of complex systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Parnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generating natural language summaries for cross-cutting source code concerns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rastkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Bradley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving code completion with program history</title>
		<author>
			<persName><forename type="first">R</forename><surname>Robbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="212" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A natural language approach for requirements engineering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rolland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Proix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Information Systems Engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="257" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Evaluating high accuracy retrieval techniques</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using natural language program analysis to locate and understand action-oriented concerns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">P</forename><surname>Fry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AOSD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="212" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using language clues to discover crosscutting concerns</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shepherd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tourwé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGSOFT Software Engineering Notes</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards automatically generating summary comments for Java methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Muppaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatically detecting and describing high level actions within methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Sridhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vijay-Shanker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Combining statistical and syntactic methods in recognizing handwritten sentences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baltus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Symposium: Probabilistic Approaches to Natural Language</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="121" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Text to software: developing tools to close the gaps in software engineering</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Tichy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Koerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FoSER</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="379" to="384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic parameter recommendation for practical API usage</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICSE</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="826" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Detecting API documentation errors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OOPSLA</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="803" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MAPO: mining and recommending API usage patterns</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECOOP</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="318" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Inferring specifications for resources from natural language API documentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automated Software Engineering</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="227" to="261" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
