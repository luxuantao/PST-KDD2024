<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep learning in generating radiology reports: A survey</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mahmoud</forename><forename type="middle">A</forename><surname>Maram</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Josiah</forename><surname>Monshi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vera</forename><surname>Poon</surname></persName>
						</author>
						<author>
							<persName><surname>Chung</surname></persName>
						</author>
						<author>
							<persName><surname>Monshi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Information Technology</orgName>
								<orgName type="institution">Taif University</orgName>
								<address>
									<settlement>Taif</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josiah</forename><surname>Poon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vera</forename><surname>Chung</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Sydney</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Artificial Intelligence In Medicine</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep learning in generating radiology reports: A survey</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1B9816C3F0DDC3C8284590C85FF7260</idno>
					<idno type="DOI">10.1016/j.artmed.2020.101878</idno>
					<note type="submission">Received Date: 16 April 2019 Revised Date: 30 April 2020 Accepted Date: 10 May 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Convolutional Neural Network Deep Learning Natural Language Processing Radiology Recurrent Neural Network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The combination of radiology images and text reports has led to research in generating text reports from images. This was inspired by recent work in generating text descriptions of natural images through inter-modal connections between language and visual features <ref type="bibr" target="#b0">[1]</ref>. Traditionally, computer-aided detection (CAD) systems interpret medical images automatically to offer an objective diagnosis and assist radiologists <ref type="bibr" target="#b1">[2]</ref>. Unlike CAD, DL is able to learn useful features that move beyond the limitations of radiology detection <ref type="bibr" target="#b2">[3]</ref>. For example, DL has been applied to mammography to discriminate between breast cancer and microcalcification <ref type="bibr" target="#b3">[4]</ref>, on ultrasounds to differentiate breast lesions (malignant and benign), and on CT lung scans to classify pulmonary nodules <ref type="bibr" target="#b4">[5]</ref>. Researchers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> noted a significant performance increase in DL models over conventional CAD systems. From a radiologist standpoint, DL helps to improve patient safety by offering more accurate diagnoses, obtains additional diagnostic criteria by generating unobservable data from imaging features, and increases efficiency by performing various tasks automatically <ref type="bibr" target="#b5">[6]</ref>.</p><p>The incapability to construct direct multimodal mapping between radiology images and reports that input an image and output a descriptive report is a well-known shortcoming of most automatic diagnosis methods. The discriminative image features hidden in radiology reports can support better diagnostic conclusion inferences instead of specific image labels. Recent research has utilized this semantic information in reports to propose effective image-text modelling.</p><p>Several recent surveys of DL applications <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> have been published in healthcare <ref type="bibr" target="#b8">[9]</ref>, electronic health records (EHR) <ref type="bibr" target="#b9">[10]</ref>, health informatics <ref type="bibr" target="#b10">[11]</ref>, medical image analysis <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>, medicine <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>, and even radiology <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. However, no existing reviews specifically address image and text analysis, let alone in radiology. As such, this is the investigative scope of this survey. Papers that cover a wide range of radiology applications and tasks based on DL were analyzed. We found that literature related to generating radiology reports using DL, however, is rare.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J o u r n a l P r e -p r o o f</head><p>In this paper, we examined the DL approaches employed in radiology reporting systems. Unlike other recent surveys that investigated DL in broad health informatics practices ranging from medicine to electronic health records (EHR), our survey focused exclusively on DL techniques tailored to radiology report generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Radiology</head><p>Radiology is a branch of medicine that can be divided into the following two subcategories: diagnostic and interventional radiology <ref type="bibr" target="#b17">[18]</ref>. Diagnostic radiologists examine medical images to diagnose the cause of a patient's symptoms, monitor treatment effects, screen for various illnesses, and then write radiology reports. On the other hand, interventional radiologists utilize radiology images to guide procedures. Currently, radiology images are interpreted by radiologists who are limited by speed, fatigue, and experience. Certified radiologists are rare due to training costs. As a result, many health-care systems outsource the task of medical image analysis. For example, there are many teleradiology companies in India <ref type="bibr" target="#b11">[12]</ref>. Delays or errors in diagnosis can cause harm to patients. Therefore, one solution is for radiology reporting to be performed by an automated, accurate, and efficient DL algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Understanding radiology text</head><p>A radiology report is a text-based document written by a certified radiologist. It contains descriptive information about a patient's history, symptoms, and interpretations of relevant radiology images <ref type="bibr" target="#b18">[19]</ref>. Normally, these reports are written in a specific radiology reporting format and divided into the following sections: comparison, indication, findings, and impressions. The findings section is the most crucial part of the report as it describes medical observations of normal/abnormal features in a presumptive order <ref type="bibr" target="#b19">[20]</ref>. Figure <ref type="figure">1</ref> shows an example in the form of an IU X-ray <ref type="bibr" target="#b20">[21]</ref> dataset. Here, each report is associated with two chest X-ray images.</p><p>A generated radiologist report must follow critical protocols including the correct use of medical terms to describe normal/abnormal diagnoses. They must also include supporting visual evidence in the form of detected disease location and key attributes of the image. There are several lexicons utilized in writing radiology reports including Metathesaurus 1 <ref type="bibr" target="#b21">[22]</ref>, RadLex 2 <ref type="bibr" target="#b22">[23]</ref>, and medical subject headings (MeSH) 3 . Metathesaurus <ref type="bibr" target="#b21">[22]</ref> is a collection of more than five million concept names and a million biomedical terms from over one-hundred controlled vocabulary systems. In contrast, RadLex contains more radiology-specific terms than Metathesaurus including imaging methods and equipment. Furthermore, MeSH offers comprehensive controlled vocabulary created by the United States National Library of Medicine (NLM) to index scientific journal articles and books. For example, <ref type="bibr" target="#b23">[24]</ref> utilized MeSH terms to mine reports in IU X-rays <ref type="bibr" target="#b20">[21]</ref>. However, brain tumors and lung diseases do not have a fixed standardized lexicon. Instead, they have a semi-standardized description system.</p><p>The use of DL has shown promising results in generating radiology reports from images <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. First, researchers generated a short descriptive sentence of a radiology image using only the image features.Then, they attempted to produce more informative reports with multiple sentences. However, this introduced new challenges in content selection and ordering. Using this method, radiology reports could include information that cannot be detected from image features, such as the nationality of the patient <ref type="bibr" target="#b23">[24]</ref>. On the other hand, this text-based DL algorithm is insufficient as it does not include specific image labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Understanding radiology images</head><p>There are different types of radiology images, including X-ray, computed tomography (CT), magnetic-resonance imaging (MRI), positron emission tomography (PET), and ultrasound (US) <ref type="bibr" target="#b27">[28]</ref>. Figure <ref type="figure">2</ref> shows an example of various radiology imaging modalities and characteristics. Globally, chest radiography is the most common imaging examination that demands correct and immediate interpretation to avoid life-threatening diseases <ref type="bibr" target="#b28">[29]</ref>. A single radiologist may need to read and report more than 100 chest X-rays per day <ref type="bibr" target="#b29">[30]</ref>. This imaging technology is starting to be employed as the first-line imaging modality by hospitals in Italy and UK to diagnose patients with the coronavirus disease 2019 (COVID-19) <ref type="bibr" target="#b30">[31]</ref>. Although chest X-ray is less sensitive than chest CT, it is easy to document and may reduce the risk of cross-infection by utilizing portable radiology units <ref type="bibr" target="#b31">[32]</ref>. Recently, several large chest x-rays datasets were released to enable researchers to advance the state-of-the-art for the proposed DL models <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref>. Consequently, chest X-rays have gained significant attention from DL researchers.</p><p>Picture archiving and communication systems (PACS) have been used since the 1990s by modern hospitals for radiology storage, management,     transmission, and processing. To enhance standards, digital imaging and communications in medicine (DICOM) was introduced in 1993. It included advanced report and result features <ref type="bibr" target="#b40">[41]</ref>. Where DICOM has assisted with many image processing procedures, PACS is an e-system mainly used for the acquisition of medical images.</p><p>From DL perspective, radiology images are pre-processed differently due to the varied processor and memory restrictions. Some images, such as Xrays, are two-dimensional (2D) while others such as CT and MRI scans are three-dimensional (3D). Currently, DL models that are trained on simple 2D images are more successful than 3D images which add an extra dimension to the problem <ref type="bibr" target="#b41">[42]</ref>. However, experience needs to be gained in applying DL to X-rays because they are 2D projections of a 3D human body <ref type="bibr" target="#b42">[43]</ref>. In other words, DL algorithms may need to be adjusted to handle the physiological structures that lie on top of each other in the X-rays. Significantly, DL, in particular CNN, can process an input of 2D and 3D images with only minor adjustments. After all, deep learning in radiology images is still an area of active ongoing research.</p><p>So far, DL has been successfully applied to medical image analysis and acknowledged as a powerful tool for image classification <ref type="bibr" target="#b43">[44]</ref>, lesion detection <ref type="bibr" target="#b44">[45]</ref>, segmentation <ref type="bibr" target="#b45">[46]</ref>, content-based image retrieval (CBIR) <ref type="bibr" target="#b46">[47]</ref>, report generation from images, and image generation and enhancement <ref type="bibr" target="#b47">[48]</ref>. To allow practitioners to rapidly implement DL solutions for image analysis tasks, NiftyNet<ref type="foot" target="#foot_0">11</ref>  <ref type="bibr" target="#b48">[49]</ref> features an open source framework for many medical imaging CNN algorithms under the Apache License. Several surveys have introduced the role of DL algorithms in medical image analysis, focusing on CNN <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. <ref type="bibr" target="#b49">[50]</ref> classifies DL models based on application area, including cardiovascular, neurology, mammography, microscopy, dermatology, gastroenterology, and pulmonary applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Text/Image radiology dataset</head><p>Table <ref type="table">1</ref> compares publicly available radiology image datasets with relevant reports in the medical informatics domain. These include the following: the Indiana University chest X-ray (IU X-ray) <ref type="bibr" target="#b20">[21]</ref>, ChestX-ray14 <ref type="bibr" target="#b33">[34]</ref>, MIMIC-CXR <ref type="bibr" target="#b32">[33]</ref>, pathology detection in chest radiographs (PadChest) <ref type="bibr" target="#b36">[37]</ref>, the digital database for screening mammography (DDSM), and the pathology education informational resource (PEIR). Researchers have employed these multimodal medical databases for developing and evaluating DL models. Nevertheless, there are few large and accessible datasets adequate for developing CNN models. In addition, researchers conduct experiments using different database subsets. This makes it difficult to compare the performance of their proposed approaches.</p><p>At present, IU X-ray <ref type="bibr" target="#b20">[21]</ref> and ChestX-ray14 <ref type="bibr" target="#b33">[34]</ref> are the most frequently used datasets by researchers in the medical informatics domain. The IU Xray <ref type="bibr" target="#b20">[21]</ref> collection consists of 7,470 chest X-rays with 3,955 radiology reports available through OpenI. OpenI is an open-source collection of literature and biomedical images. It contains IU X-ray, 2,064 orthopedic illustrations, and more than three million images from PubMed and the National Library of Medicine (NLM). Researchers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> have used this dataset to demonstrate how their proposed DL models label and describe the diseases associated with the images. However, data in IU X-ray comes from fully anonymized reports in two hospitals. As a result, some keywords, findings and images are missing. ChestX-ray14 <ref type="bibr" target="#b33">[34]</ref> is from the national institute of health (NIH) clinical center. It is an open access chest X-ray dataset that includes 112,120 Xray images with fourteen thorax disease labels (atelectasis, consolidation, infiltration, pneumothorax, edema, emphysema, fibrosis, effusion, pneumonia, pleural thickening, cardiomegaly, nodule, mass, and hernia). These labels were mined from the original radiologist reports. However, the complete text reports are not publicly available.</p><p>CheXpert <ref type="bibr" target="#b28">[29]</ref> and MIMIC-CXR <ref type="bibr" target="#b32">[33]</ref> are the latest co-released open source datasets that use the CheXpert labeler to extract annotations from unstructured radiology reports. CheXpert is a dataset that consists of 224,316 chest radiographs from 65,240 patients labeled due to the presence of 14 common chest radiographic observations. ChestX-ray14 uses an automatic labeler to extract labels from reports. On the other hand, CheXpert offers radiologists labeled validation and expert scores. The largest open access chest radiography to date is MIMIC-CXR. This includes 371,920 chest X-rays linked to 227,943 reports gathered from the Beth Israel Deaconess Medical Center. Through a limited release version of this dataset, <ref type="bibr" target="#b34">[35]</ref> conducted the first work that trained a collection of CNNs using a huge dataset to recognize thorax diseases. Then, <ref type="bibr" target="#b35">[36]</ref> used MIMIC-CXR v1.0.0. to show that processing multi-view chest X-rays simultaneously resulted in better classification performance.</p><p>PadChest <ref type="bibr" target="#b36">[37]</ref>, however, is labeled with the largest number of annotations including 174 radiology findings, 19 diagnoses, and 104 anatomic locations. This dataset contains 160,868 chest X-rays from six different views and the associated 109,931 reports collected from San Juan Hospital. It provides researchers with the opportunity to address unfinished investigations such as measuring DL model performance using the chest X-ray views <ref type="bibr" target="#b37">[38]</ref>.</p><p>Apart from X-ray collections, DDSM <ref type="bibr" target="#b38">[39]</ref> and PEIR are open source datasets of different image modality. For example, PEIR is a digital library created by the University of Alabama for medical education. It contains sentence-level descriptions of 20 different body parts, including the abdomen, adrenal, aorta, breast, chest, head, and kidneys. On the other hand, DDSM <ref type="bibr" target="#b38">[39]</ref> contains 2,620 scanned films of normal, benign, and malignant mammography studies with verified pathology information. It is supported by the University of South Florida and it has been widely used by researchers due to its scale and ground truth validation. <ref type="bibr" target="#b39">[40]</ref> selected a subset of the DDSM database that consisted of 974 images annotated with semantic descriptors to test their multi-task-loss CNN based model. This outperforms the accuracy of current techniques by up to 10% when detecting and describing lesions.</p><p>Moreover, researchers have trained their deep learning frameworks on several privately-owned datasets, including the picture archiving and communication systems (PACS) from the NIH clinical center <ref type="bibr" target="#b50">[51]</ref> and CX-CHR <ref type="bibr" target="#b61">[62]</ref>. The PACS from the NIH clinical center consists of 216,000 2D images with radiology reports that offer visual references to pathologies. The CX-CHR dataset contains chest X-rays of 35,500 patients and contains Chinese reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep learning (DL)</head><p>Currently, DL is a promising subfield of machine learning (ML) which, in turn, is a subfield of artificial intelligence (AI) (Fig. <ref type="figure">3a</ref>). Artificial intelligence occurs when a machine is composed of multiple layers, uses raw data as input, and improves the representations required for pattern recognition <ref type="bibr" target="#b51">[52]</ref>. Essentially, a linear combination of input signals adds bias to apply an affine transformation and generate the output (Fig. <ref type="figure">3b</ref>) where are the weights, and ( ) is the activation function (described in Section 3.1). This main computational element, known as the neuron or perceptron, enables the DL machine to learn from experience without the need to specify the desired knowledge. Currently, DL has already succeeded in many computerized applications including computer vision, NLP, speech processing, gaming, and cross-media retrieval. From a radiology perspective, DL models can be fed with multiple datatypes and iteratively distort them as they flow from layer to layer <ref type="bibr" target="#b8">[9]</ref> (Fig. <ref type="figure">3c</ref>). This is a particularly relevant function for radiology data as it consists of reports and linked images.</p><p>Researchers have classified DL models into three categories: supervised, unsupervised, and reinforcement learning (RL) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref>. Supervised learning mainly infers a mapping function ( ) from input to output such as multilayer perceptron (MLP), recurrent neural network (RNN), and convolutional neural network (CNN). Often RNNs are accompanied with</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. Deep learning</head><p>CNNs to generate medical image descriptions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53]</ref> (Fig. <ref type="figure">3d</ref>). In contrast, unsupervised DL takes onboard remarkable properties related to the distribution of including Boltzmann machines (BM) and autoencoders (AE). Deep RL is a semi-supervised technique for partially labeled datasets as it can act with limited input data. For instance, if a deep RL network is fed with several tumor cells, it can overinterpret an image to detect insignificant aspects <ref type="bibr" target="#b53">[54]</ref>. To enable effective and robust radiology report generation, using RL, HRGR-Agent <ref type="bibr" target="#b19">[20]</ref> trained the retrieval policy module and the generation module using sentence-level and word-level rewards, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Artificial Intelligent (AI)</head><p>Machine Learning (ML)</p><p>Deep Learning (DL)</p><p>a. DL, ML and AI J o u r n a l P r e -p r o o f</p><formula xml:id="formula_0">x 1 W k1 Input x 2 W k2 x 3 W k3 x m W km ∑ b K φ (.) V K y K Output Bias</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Activation function</head><p>An activation function is a critical element of DL as it adds nonlinearity by taking the weighted sum of inputs in one layer and converting it into an output value <ref type="bibr" target="#b15">[16]</ref>. Then, this value is conveyed to nodes in the subsequent layer. Table <ref type="table" target="#tab_5">2</ref> illustrates common activation functions including sigmoidal, hyperbolic tangent (TanH), rectified linear unit (ReLU) <ref type="bibr" target="#b54">[55]</ref>, and leaky ReLU <ref type="bibr" target="#b55">[56]</ref>. Sigmoidal is one of the earliest activation methods used in neural networks but can cause network instability or freeze network learning. The limitations of TanH are similar as it is a scaled form of the sigmoid function.</p><p>On the other hand, ReLU performs better than sigmoidal functions as it was the first to be successfully used for neural networks by <ref type="bibr" target="#b54">[55]</ref>. It converts the weighted sum of inputs to zero if they are less than zero or to the same input if they are equal to or greater than zero. Leaky ReLU is an extension of ReLU that outputs small negative numbers if the inputs are negative. If not, it produces the same outputs as ReLU. Researchers tend to begin with ReLU and then apply other activation functions if they do not obtain optimal results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overcome dead ReLU problem</head><p>All traditional CNN activation functions output a single result for a single input except Softmax. Instead, Softmax produces multiple outputs. It is useful as it converts the output of the last neural network layer into a probability distribution. In practice, Softmax is used in multiclass classifications, while sigmoid is used in binary classifications <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Convolutional neural network (CNN)</head><p>A CNN <ref type="bibr" target="#b57">[58]</ref> is a type of multi-layer neural network that uses minimal processing to recognize visual patterns from pixel images. One of the main advantages of CNN is its ability to automatically amalgamate low-level features (including lines and edges) into high-level features (such as shapes) within subsequent layers <ref type="bibr" target="#b11">[12]</ref>. For each convolutional layer , a set of kernels with biases convolve an input image to generate feature maps . These generated maps have a non-linear transform ( ) in each layer (refer to Eq. 1.).</p><formula xml:id="formula_1">( )<label>(1)</label></formula><p>There are several CNN models including deep feed-forward CNNs for images and word-embedding networks for text. The histogram of oriented gradients (HOG) and scale-invariant feature transform (SIFT) are two examples of convolutional image features. However, deep CNNs significantly outperform shallow learning frameworks and hand-crafted image features as they need larger collections of training data <ref type="bibr" target="#b58">[59]</ref>.</p><p>Recently, CNNs have become the primary frameworks for mining medical data as the number of papers published on CNN methods and applications has increased rapidly since 2015 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref>. In radiology, CNN is the most applicable DL algorithm for performing various tasks including medical image classification and segmentation <ref type="bibr" target="#b59">[60]</ref>. Interestingly, CNNs can transfer learning from a large database unrelated to the current task (e.g., ImageNet) into a related one (e.g., IU X-ray).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Architecture</head><p>The most popular CNN architectures were proposed by top competitors at the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). This includes the following architectures: AlexNet <ref type="bibr" target="#b60">[61]</ref>, ZFNet <ref type="bibr" target="#b61">[62]</ref>, Visual Geometry Group (VGG-16) <ref type="bibr" target="#b62">[63]</ref>, GoogLeNet <ref type="bibr" target="#b63">[64]</ref>, Residual Network (ResNet) <ref type="bibr" target="#b64">[65]</ref>,</p><p>ResNeXt <ref type="bibr" target="#b65">[66]</ref>, CUImage Team <ref type="bibr" target="#b66">[67]</ref>, and SENets <ref type="bibr" target="#b67">[68]</ref> (see Table <ref type="table" target="#tab_6">3</ref>). ImageNet is a project that aims to create an enormous visual database that can be utilized by researchers in the field of visual object recognition <ref type="bibr" target="#b68">[69]</ref>. It should be noted that ImageNet runs ILSVRC, an annual contest where software programmers classify and detect objects and scenes. In 2012, <ref type="bibr" target="#b60">[61]</ref> noted how AlexNet was the first model to considerably improve image classification performance. It obtained a 16.4% error rate using the ImageNet dataset. This model minimized the overfitting problem using data augmentation and dropout procedures. Two remarkable models were then proposed in 2014: the VGG-16 (7.4% error rate), which reduced the spatial size of the input in each layer, and GoogLeNet (6.67% error rate), which permitted procedures such as pooling and convolutional to run in parallel to each other. AlexNet uses eight convolutional layers, 650,000 neurons (60,000,000 parameters) and has an error rate of 16.4%. In contrast, VGG-16 consist of 16 convolutional layers, 133,000,000 parameters and 7.4% error rates <ref type="bibr" target="#b69">[70]</ref>. It is clear that VGG-16 is a significantly deeper model than AlexNet, which is why its error rate is lower.</p><p>By 2015, automatic image classification models could outperform human manual annotation with a 5% to 10% error, respectively. This first occurred when <ref type="bibr" target="#b64">[65]</ref> introduced Microsoft deep ResNet. This contains 152 layers that apply residual connections in CNNs to address the issues of vanishing gradients <ref type="bibr" target="#b70">[71]</ref> and degradation. The ILSVRC 2016 winner was the CUImage team <ref type="bibr" target="#b66">[67]</ref>, who assembled the following six architectures: Inception v3, Inception v4, Inception ResNet v2, ResNet 200, Wide Resnet 68, and Wide Resnet 3. However, the 2016 runner-up, ResNext <ref type="bibr" target="#b65">[66]</ref>, introduced a simple framework that consisted of branches in a residual block. Each branch conducted a transformation aggregated by a summation function at the end. Although this model is based on ResNet and uses less layers, it outperforms ResNet, Inception-v3 and Inception-ResNet <ref type="bibr" target="#b71">[72]</ref>. It can be generalizable by reshaping it using other models like AlexNet.</p><p>In 2017, the ILSVRC concluded as researchers considered the problem of supervised image classification solved <ref type="bibr" target="#b6">[7]</ref>. The 2017 winner was squeeze and excitation networks (SENet). This network is based on the ResNeXt-152 model and adds recalibration to adaptively reweight feature maps.</p><p>To generate radiology reports, researchers follow some ImageNet CNN network settings as well as other reliable architectures. These include network in network (NIN) <ref type="bibr" target="#b72">[73]</ref> and densely connected convolutional network (DenseNet) <ref type="bibr" target="#b73">[74]</ref> with slight modifications. For instance, <ref type="bibr" target="#b23">[24]</ref> notes that AlexNet is a complex method. Instead, they use NIN as it is a simpler and faster model. In addition, they suggest that GoogLeNet is the baseline CNN model and use it to train their data. Although AlexNet and GoogLeNet have different depths, <ref type="bibr" target="#b58">[59]</ref> utilized both to train their looped deep pseudo-task optimization network model (LDPO). When extracting features from images, VGG16 is the preferred choice for the majority of researches in the visual pattern recognition community <ref type="bibr" target="#b18">[19]</ref>. This is largely because VGG16 offers a uniform CNN architecture and publicly available weight configuration <ref type="foot" target="#foot_2">12</ref> . For example, <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53]</ref> adopt this architecture to read radiology images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Recurrent neural network (RNN)</head><p>RNN is a neural network that processes sequential information while maintaining a state vector within its hidden neurons <ref type="bibr" target="#b74">[75]</ref>. Equation ( <ref type="formula" target="#formula_2">2</ref>) is the basic RNN that preserves a hidden state at a time that is the outcome of a non-linear mapping sing its input and the previous state , where and are the shared weight matrices over time. On the other hand, CNNs are the preferable networks for pixels in an image and other clear spatial structure data. Recurrent neural networks work well with natural language and similar sequentially ordered data <ref type="bibr" target="#b9">[10]</ref>. They can predict next words based on the former ones in the language model <ref type="bibr" target="#b75">[76]</ref>. However, it is hard to save information for a long time as the weights are equal in all RNN layers. Another issue is the requirement for a backpropagation algorithm to train RNN as the gradients either grow or shrink. Consequently, variations of RNN have been introduced to overcome these limitations.</p><formula xml:id="formula_2">( )<label>(2)</label></formula><p>The most popular extensions of RNN are Long Short-Term Memory (LSTM) <ref type="bibr" target="#b76">[77]</ref> and the Gated Recurrent Unit (GRU) <ref type="bibr" target="#b77">[78]</ref>. Long short-term memory uses memory blocks to save the network temporal state and gates to monitor the information flow. On the other hand, GRU is a lighter form of RNN than LSTM in terms of topology, computation expenses, and complexity. At present, researchers must choose between the faster model offered by GRU that needs fewer parameters or the higher performing model provided by LSTM that contains sufficient data and computational power <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Software</head><p>Convolutional architecture for fast feature embedding (Caffe) <ref type="foot" target="#foot_3">13</ref>  <ref type="bibr" target="#b78">[79]</ref> is the most common software package utilized by practitioners to automate radiology reporting. Using Caffe, <ref type="bibr" target="#b50">[51]</ref> trained their deep CNN model to map X-rays into specified document categories, and <ref type="bibr" target="#b39">[40]</ref> implemented a multi task loss CNN model to describe medical images. Using Caffe, <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b79">80]</ref> acquired pre-trained CNN models on ImageNet for their radiology annotation systems.</p><p>However, there are several other software packages that support CNN and RNN implementations, including TensorFlow<ref type="foot" target="#foot_4">14</ref>  <ref type="bibr" target="#b80">[81]</ref> and PyTorch<ref type="foot" target="#foot_5">15</ref>  <ref type="bibr" target="#b81">[82]</ref>. Using both TensorFlow and Tensorpack <ref type="foot" target="#foot_6">16</ref> , <ref type="bibr" target="#b26">[27]</ref> implement a text-image embedding network (TieNet) that produces thorax diseases reports. DualNet <ref type="bibr" target="#b34">[35]</ref> and the hybrid retrieval-generation reinforced agent (HRGR-Agent) <ref type="bibr" target="#b19">[20]</ref> frameworks are based on PyTorch. These software packages are open-source projects that utilize Nvidia support to enhance performance through graphics processing unit (GPU) acceleration. To note, training DL can be accelerated through advanced GPU that facilitates parallel processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Generating radiology text</head><p>Natural language processing (NLP) explores the use of machines to process/understand human languages and carry out useful tasks. Traditional learning algorithms for NLP are often incapable of absorbing a large volumes of training data as feature engineering requires significant human expertise <ref type="bibr" target="#b82">[83]</ref>. Several years ago, NLP was brought forward by a new era of deep learning algorithms using a vision named "NLP from scratch" <ref type="bibr" target="#b83">[84]</ref>. Such DL waves have the capacity to learn representations from text through layers of nonlinear neurons for feature extraction. Since 2010, DL has been productively applied to NLP tasks <ref type="bibr" target="#b84">[85]</ref> including natural language generation (NLG) from meaning representation. This can be considered the inverse of natural language understanding <ref type="bibr" target="#b85">[86]</ref>. Through this, DL can generate fluent, communicative, and new image descriptions.</p><p>Applied to a free-form radiologist text, NLP assists with converting text into a structured report, extracting meaningful information, and classifying reports <ref type="bibr" target="#b86">[87]</ref>. A recent NLP technique is neural language modelling, which includes word embedding and recurrent language models <ref type="bibr" target="#b87">[88]</ref>. Word embedding converts words into vectors to allow less sparse data representation. Using this, DL models can be trained with smaller datasets. Advanced word embedding was applied to a large collection of radiology reports to generate word vectors of radiology image descriptions <ref type="bibr">[20, 25-27, 51, 89]</ref>. Recurrent language models predict word output based on a sequence of arbitrary past words. As such, they are not limited by fixed input dimensions.</p><p>Generally, radiology reports are semi-structured and use standardized documentation templates <ref type="bibr" target="#b32">[33]</ref>. Consequently, researchers have proposed open-source NLP tools to extract controlled vocabulary from radiology reports. Examples of these tools include NegBio labeler<ref type="foot" target="#foot_7">17</ref>  <ref type="bibr" target="#b27">[28]</ref> and CheXpert labeler <ref type="foot" target="#foot_8">18</ref> . NegBio was developed by NIH and used to annotate the ChestX-ray14 dataset. CheXpert was built by the Stanford Machine Learning Group and based on NegBio. However, CheXpert achieved a higher F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DL models for generating radiology report</head><p>Overall, the purpose of the proposed models was to generate interpretations of radiology images. During training, the input for these models was a collection of images and associated reports, as shown in Fig. <ref type="figure">4</ref>. First, researchers proposed models to align disease descriptions to the relevant visual regions using multimodal embedding. They then used the outcomes as training data for additional models. This training data allowed the additional models to learn how to generate the image descriptions.</p><p>Table <ref type="table" target="#tab_8">4</ref> categorizes the existing approaches into three main levels to summarize their main characteristics. These categories are as follows: words, sentences, and paragraphs. It is clear that the accessibility of a large volume of radiology reports and images allowed deep CNNs to become the premier learning method and address the automatic text report generation issue.</p><p>Table <ref type="table" target="#tab_9">5</ref> compares the results of the generated reports through quantitative evaluation matrices (defined in section 6.1). To the best of our knowledge, the multi-task learning model <ref type="bibr" target="#b24">[25]</ref> outperforms existing approaches in generating radiology paragraphs using the IU X-ray dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Word level</head><p>In 2015, the first text/image DL framework with a large-scale PACS was proposed by <ref type="bibr" target="#b50">[51]</ref> and used in a national research hospital. This process is explained in more detail in <ref type="bibr" target="#b18">[19]</ref>. This system uses approximately 780,000 radiology reports and around 216,000 2D images to extract and mine the semantic interactions between them. This framework is capable of matching images with their descriptions automatically using NLP. Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b89">[90]</ref> was applied to obtain the semantic interpretation of diagnostic images, and a CNN was trained to map the images into document Fig. <ref type="figure">4</ref>. Framework of the radiology reporting models categories. The weak supervision method was used to generate interpretations of radiology images, and the strict supervision method was used to detect the absence or presence of several common diseases. In the testing set, the match rate between predicted disease words and actual words in the report was 0.56. This system represents a significant step towards accurately generating radiologist reports using enormous medical image databases.</p><p>Nevertheless, the clusters in <ref type="bibr" target="#b50">[51]</ref> are highly unbalanced. This is because most images are clustered into three groups as they were derived from text modalities only (approximately 780,000 reports). On the other hand, <ref type="bibr" target="#b58">[59]</ref> created the LDPO model, which formed clusters from text reports as well as image cues to offer a more visually coherent and balanced method in terms of clusters. As such, LDPO is an iterative system that extracts deep CNN features based on finetuned radiologist topic labels and mutual information shared between discovered clusters. Afterwards, the framework either stops the iteration and outputs optimized clustering or inputs the refined cluster labels into the next iteration to fine-tune the CNN model. At the end, NLP is applied to the radiology reports to count and rank the frequency of each word. This process allocates the most common words, which are then used as the keyword labels for each cluster. To evaluate the system, a board of certified radiologists reviewed the resultant keywords and sampled images. The results of applying the LDPO model to discovery clusters were found to be visually coherent and highly balanced clusters. Nevertheless, the looped property is specific to deep CNN classification-clustering methods as other kinds of classifiers cannot learn satisfactory image characteristics simultaneously.</p><p>Using a dataset of more than 16,000 X-ray images and Chinese radiology reports, <ref type="bibr" target="#b52">[53]</ref> trained a CNN model to automatically label new images with one of ten pre-defined labels: normal, increased lung marking, aortosclerosis, increased heart shadow, pleural thickening, pulmonary interstitial hyperplasia, costophrenic angle blunting, pleural effusion, emphysema, and bronchitis. These disease labels were extracted from the reports using basic NLP techniques. In addition, this system can generate the correct label with an accuracy of 97%. However, it performed poorly in cases including increased heart shadows and pleural thickening due to the unbalanced database. In this dataset, half of the images were labelled as "normal" cases.</p><p>The above frameworks involve two separate models. Therefore, a single model trained end-to-end that can move directly from a radiology text-image database to region-level annotation has yet to be created.</p><p>CheXNet <ref type="bibr" target="#b79">[80]</ref> is one of the most popular DL models that utilized the Chest-Xray14 dataset <ref type="bibr" target="#b33">[34]</ref>. It contains more than 112,000 images from a reformed version of DenseNet with 121 convolution layers. CheXNet outperformed a panel of three radiologists when annotating pneumonia and 13 other diseases. Furthermore, it applied class activation mapping (CAM) <ref type="bibr" target="#b90">[91]</ref> to produce heatmaps that visualized the indicative regions of the disease in the image. Using the same dataset but with ResNet-152 architecture instead, ChestNet <ref type="bibr" target="#b91">[92]</ref> incorporated an additional attention branch into CNN based on gradient-weighted class activation mapping (Grad-CAM) <ref type="bibr" target="#b92">[93]</ref>. This exploited the correlation between labels and disease locations.</p><p>DualNet <ref type="bibr" target="#b34">[35]</ref> and the multi-view model <ref type="bibr" target="#b35">[36]</ref> employed the MIMIC-CXR <ref type="bibr" target="#b32">[33]</ref> dataset, which is over four times the size of Chest-Xray14 <ref type="bibr" target="#b33">[34]</ref>, to demonstrate the benefits of simultaneously processing frontal and lateral chest X-rays when detecting common thorax diseases. They used DenseNet-121 and ResNet-50, respectively. The multi-view model adopted discriminative learning rates <ref type="bibr" target="#b93">[94]</ref> and introduced the stage wise training approach to reduce training time and increase accuracy. This had an average labelling performance of 0.779 AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sentence level</head><p>In contrast to recent studies that only detected diseases in images using text/image datasets <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b91">92]</ref>, <ref type="bibr" target="#b23">[24]</ref> described the context of the disease in a similar way to a radiology report. They introduced a recurrent neural cascade model to detect and describe disease location, severity, and the affected Output (Word Level): Models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b91">92]</ref> Output (Sentence Level):</p><p>M odels <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref> Output (Par ag rapgh Level): Models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> Training Image Report J o u r n a l P r e -p r o o f organs to offer a better understanding of the disease. This system computed labels based on joint text/image contexts after initial CNN/RNN training using single object labels in a chest X-ray dataset from IU X-ray <ref type="bibr" target="#b20">[21]</ref>. Eventually, it generated image descriptions by training the RNN with the new CNN image embedding (refer to Eq. 3.), where I denotes the input image, t is the time step, N is the number of words in the annotation, Y is the output word, S is the correct word and represents the joint image/text context vector from the first iteration, .</p><formula xml:id="formula_3">( ) ∑ , ( ) * ( ) +-<label>(3)</label></formula><p>Similarly, the multi-task-loss CNN-based system generated radiologist sentences to describe tumor lesions (shape, margin, and density) in breast images <ref type="bibr" target="#b39">[40]</ref>. Essentially, this system was trained using a DDSM dataset and a private dataset of mammography and ultrasound to produce and rank the rectangular regions of interest (ROIs). The highest ROIs were fed into the remaining network layers which, in turn, generated semantic descriptions of subsequent ROIs. This system provided automatic lesion detection in breast images alongside semantic descriptions. <ref type="bibr" target="#b24">[25]</ref> added a co-attention mechanism to describe abnormal lesions by discovering visual and semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Paragraph level</head><p>The first work towards generating truly radiology reports with long and diverse topics is a multitask learning model with a co-attention mechanism. It contains a hierarchical LSTM to produce long descriptive paragraphs through capturing long-range semantics <ref type="bibr" target="#b24">[25]</ref>. Although this model achieved outstanding results when generating descriptive radiology reports using the IU X-ray J o u r n a l P r e -p r o o f  <ref type="bibr" target="#b19">[20]</ref> 2018 IU X-Ray <ref type="bibr" target="#b20">[21]</ref> CX-CHR (Chinese reports) <ref type="bibr" target="#b19">[20]</ref> PyTorch <ref type="bibr" target="#b81">[82]</ref>. DensNet <ref type="bibr" target="#b73">[74]</ref> VGG19 <ref type="bibr" target="#b62">[63]</ref> CNN Extract visual features J o u r n a l P r e -p r o o f dataset, the produced paragraphs contained repeated sentences due to a lack of contextual coherence in the hierarchical models.</p><p>On the other hand, <ref type="bibr" target="#b25">[26]</ref> generated sentences using the same dataset through an attention input of image encoding and the first generated sentence. This method maintained coherence in the resultant paragraphs as it uses CNN and LSTM in a recurrent way. As <ref type="bibr" target="#b25">[26]</ref> filtered reports without two associated images (frontal and lateral chest X-rays) and reports without complete sections from the IU X-ray dataset, the training was performed using a small dataset. As a result, the generated text was missing some abnormal descriptions and contained sentences that were different from the ones in the training set.</p><p>Using the same dataset, <ref type="bibr" target="#b26">[27]</ref> proposed a text-image embedding network (TieNet) that integrated multi-level attention with a CNN-RNN framework for classification and reporting. The CNN, RNN, and LSTM were based on ResNet-50, the visual spatial attention approach <ref type="bibr" target="#b95">[96]</ref>, and standard LSTM, respectively. Multiple RNNs may have enhanced TieNet by learning the disease attributes more efficiently which, in turn, may have improved the auto-report quality.</p><p>Recently, <ref type="bibr" target="#b19">[20]</ref> introduced the first retrieval model with a generative neural network using RL. This is called the hybrid retrieval-generation reinforced agent (HRGR-Agent). The HRGR-Agent extracts visual features of chest X-rays from the last convolutional layer of DenseNet or VGG19 and improves text generation by empowering RNN with an attention mechanism. The experiments on two medical databases, IU X-ray and CX-CHR, showed high performance in generating precise text that described rare abnormal findings. The CX-CHR database utilized was a proprietary dataset of Chinese reports and linked images. This made it difficult to compare the HRGR-Agent with other recent state-of-the-art models.</p><p>In contrast, <ref type="bibr" target="#b96">[97]</ref> used the largest public intensive care unit (ICU) patient dataset to introduce a framework that learned multiple disease labels from two types of features: medical charts and notes. Instead of considering the correlation between diseases in the same way as existing methods, this approach used diseasespecific features. However, the paper only demonstrated an intuitive implementation of the disease-specific feature construction, rather than using multiple clusters for positive and negative instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>Evaluating radiology reporting models has become increasingly essential due to the rapid introduction of DL approaches to large medical datasets. Both quantitative (machine-based) and qualitative (human-based) evaluations have been employed to compare the benchmark reporting models. Qualitative evaluation is more expensive than quantitative and is not repeatable. However, it may offer additional valuable measurement for generated reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Quantitative</head><p>The common evaluation metrics for image captioning and machine learning are bilingual evaluation understudy (BLEU) <ref type="bibr" target="#b97">[98]</ref>, recall-oriented understudy for gisting evaluation (ROUGE) <ref type="bibr" target="#b98">[99]</ref>, METEOR <ref type="bibr" target="#b99">[100]</ref>, consensus-based image description evaluation (CIDEr) <ref type="bibr" target="#b100">[101]</ref>, and semantic propositional image caption evaluation (SPICE) <ref type="bibr" target="#b101">[102]</ref>. Table <ref type="table" target="#tab_10">6</ref> compares these matrices using their original purposes, main ideas, strengths, and weaknesses.</p><p>These evaluation matrices are employed by researchers to compare their proposed models of generating radiology reports against the benchmarks. They automatically calculate an accuracy score for a new model by observing the similarity/differences between the generated captions and the radiologist's written descriptions from empirical observation. Increased performance is indicated through higher scores in BLEU, ROUGE, METEOR, CIDEr, and SPICE. The MS COCO evaluation kit <ref type="foot" target="#foot_9">19</ref> offers the implementation script for these evaluation matrices in terms of caption generation.</p><p>BLEU-n metrices <ref type="bibr" target="#b97">[98]</ref> are precision metrices for machine translation that are computed by multiplying n-gram precision scores by a penalty for short sentences. They have been employed to measure the similarity between a pair of sentences. A superior version of BLEU was proposed by <ref type="bibr" target="#b102">[103]</ref>. However, BLEU suffers from a low performance in explicit word matching.</p><p>ROUGE <ref type="bibr" target="#b98">[99]</ref> is a recall metric for summarization systems that matches intersecting n-grams, word sequences, and word pairs. ROUGE-L is a version of ROUGE that calculates the longest common sub sequences between two sentences.  CIDEr <ref type="bibr" target="#b100">[101]</ref> is an evaluation metric for image captioning that calculates cosine similarity between candidate image annotation and the associated sentences produced by humans. It works in a purely linguistic means, but its evaluations are ineffective as it sometimes provides large weight for insignificant sentence details.</p><p>SPICE <ref type="bibr" target="#b101">[102]</ref> is a recent evaluation metric for image caption that uses scene-graph tuples to parses a sentence into semantic tokens including object classes, relation types, and attribute types. Thus, the quality of the parsing determines CIDEr's performance. In some cases, this may result in failure as illustrated by an example in <ref type="bibr" target="#b103">[104]</ref>. In a similar way to METEOR, SPICE utilizes WordNet synonym matching for tuple matching.</p><p>The different design choices of evaluation metrics, such as n-gram and scene-graph, result in metrics that have different strengths and weaknesses. For example, BLEU, ROUGE, and CIDEr use only exact n-gram matches, but METEOR adds synonyms and paraphrases. Although BLEU is based on precision, METEOR and ROUGE are recall-based metrics. As a consequence, <ref type="bibr" target="#b103">[104]</ref> suggested that existing evaluation metrics should complement each other in measuring the quality, accuracy, and robustness of the generated annotations.</p><p>The original purpose of these common matrices was not to evaluate generated radiology reports. Therefore, some researchers have designed complementary metrices. For instance, a metric called keywords accuracy (KA) calculates accuracy by dividing the number of correctly generated words by the number of ground truth words from the medical text indexer (MTI) annotations <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Qualitative</head><p>Qualitative evaluation involves comparing ground truth reports with mode generated reports using content coverage, length, medical term accuracy, and text fluency. For example, <ref type="bibr" target="#b19">[20]</ref> utilized Amazon mechanical Turk (MTurk) to conduct surveys. Here, participants chose the generated report that best matched the ground truth report. <ref type="bibr" target="#b24">[25]</ref> manually compared the generated paragraphs from their co-attention model with the ground truth to establish which models captured normality and abnormality most efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and future direction</head><p>Deep learning algorithms have the potential to be used in all fields of medicine and could significantly alter the way medicine is practiced. Future DL research should utilize the wealth of medical images and relevant diagnostic reports that are available in PACS to automatically produce clinical reports <ref type="bibr" target="#b12">[13]</ref>. Recent attention has focused on generating text reports based on medical data.</p><p>Beyond traditional medical image annotation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b91">92]</ref> and single sentence-based descriptions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">40]</ref>, generating radiologist coherent paragraphs has recently attracted researchers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>. This presents a more practical and challenging application that can bridge visual medical features with radiologist interpretation. Notably, CNN and RNN have quickly become popular choices for mining radiology images and text, respectively. The main challenge now lies in how to obtain ImageNet-level semantic labels on a large collection of medical images.</p><p>Deep learning has several limitations that should be addressed to improve the task of radiology reporting. A reliable reporting system may require tens of millions of image/text samples which are not yet readily available <ref type="bibr" target="#b13">[14]</ref>. Furthermore, these samples should be structured without scattered and noisy information to facilitate the learning process for DL models. To date, there are few medical datasets that are large and accessible enough to train multimodal deep CNN. Improving the quantity and quality of radiology data remains an ongoing task.</p><p>In a radiology database, the data is unbalanced because abnormal cases are rarer than normal cases. For example, the healthy cases in the IU X-ray chest X-Ray dataset consisted of 2,696 images (37%) compared to the 840 images (12%) that represented common diseases and 655 images (9%) that showed less common diseases. <ref type="bibr" target="#b23">[24]</ref> attempted to address this issue by training CNN with different regularization methods including batch normalization and data dropout. In addition, it is challenging to automate labels for medical images as radiologist reports often include ambiguous words. This includes disease prediction rather than if it is present or not <ref type="bibr" target="#b18">[19]</ref>. It should be noted that it is difficult to compare various models as researchers conduct their experiments using diverse and sometimes private datasets.</p><p>J o u r n a l P r e -p r o o f Researchers consider DL as a black box that takes an input, such as a medical image, and generates an output to state a conclusion (e.g. "there is a 0.8 probability of melanoma") without clear explanations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b104">105]</ref>. This is unacceptable in the medical domain as radiologist need to provide findings as well as underlying justifications. For instance, researchers may attempt to provide the rationale behind the radiologist's description using their proposed models. Considerably more research will need to be conducted to offer reasonable explanations for DL model outcomes.</p><p>Most research uses CNN to apply text-image mining in medical imaging. As such, CNN has the widest variety in architecture including AlexNet, VGG-16, GoogLeNet, and ResNet. In the last three years, end-to-end trained CNNs have become the preferred approach for medical imaging interpretation. As such, this could be considered standard practice for mining medical images. In addition, it is likely that the volume of research in leveraging radiology reports for CNN training will only increase in the near future.</p><p>Creating multipurpose reporting systems for radiologists that can detect several diseases simultaneously remains an ongoing challenge. Medical findings often correlate with certain body parts such as the spread of liver metastases and lymph nodes. Despite the promising results of generating radiologist reports, several questions require addressing. For example, what are the clinically related image annotations to be defined? How should the large volume of radiologist images required for DL techniques be labeled? To what extent is the deep CNN framework generalizable for radiology images? Future work should explore valuable semantic diagnostic information and map the many well-written radiologist reports and relevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper presented a comprehensive literature survey on multimodal datasets to train deep DL models that generate radiology text from images. This field is crucial as these techniques can quickly and accurately provide additional diagnostic criteria by reporting unobservable data from the images and text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .Fig. 2 .</head><label>12</label><figDesc>Fig.1. Example of a radiology report and associated images (obtained from an IU X-ray)<ref type="bibr" target="#b20">[21]</ref> </figDesc><graphic coords="4,177.82,66.20,57.23,74.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r n a l P r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>METEOR [ 100 ]</head><label>100</label><figDesc>is a recall metric for machine translation that utilizes synonyms, paraphrase matching, precision, and unigram recall to obtain harmonic overlapping between sentences. It overcomes BLEU's weaknesses J o u r n a l P r e -p r o o f</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Radiology image/text dataset (available online)</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Description</cell><cell>Base annotation</cell><cell>Employed by</cell></row><row><cell>IU X-Ray 4</cell><cell></cell><cell>7,470 chest x-rays</cell><cell>Thorax diseases</cell><cell>[20, 24-27]</cell></row><row><cell cols="2">Demner-Fushman, et al. [21] 2015</cell><cell>3,955 radiology reports</cell><cell></cell><cell></cell></row><row><cell>ChestX-ray14</cell><cell>5</cell><cell>112,120 chest x-rays</cell><cell>Atelectasis, consolidation, infiltration, pneumothorax, edema, emphysema,</cell><cell>[20, 27, 29]</cell></row><row><cell cols="2">Wang, et al. [34] 2017</cell><cell>14 thoracic labels</cell><cell>fibrosis, effusion, pneumonia, pleural thickening, cardiomegaly, nodule,</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>mass and hernia</cell><cell></cell></row><row><cell>CheXpert 6</cell><cell></cell><cell>224,316 chest x-rays</cell><cell></cell><cell></cell></row><row><cell cols="2">Irvin, et al. [29] 2019</cell><cell>14 annotated observations</cell><cell></cell><cell></cell></row></table><note><p>No finding, enlarged cardamom, cardiomegaly, lung opacity, lung lesion, edema, consolidation, pneumonia, atelectasis, pneumothorax, pleural -4 https://openi.nlm.nih.gov/faq.php 5 https://nihcc.app.box.com/v/ChestXray-NIHCC 6 https://stanfordmlgroup.github.io/competitions/chexpert/ COMPARISON: None INDICATION: Fatigue, weakness, anterior chest pain FINDINGS: Cardiomediastinal silhouette and pulmonary vasculature are within normal limits. Lungs are clear. No pneumothorax or pleural effusion. No acute osseous findings. IMPRESSION: No acute cardiopulmonary findings.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc>Activation function for DL</figDesc><table><row><cell>Name</cell><cell cols="2">Equation</cell><cell>Plot</cell><cell>Characteristics</cell></row><row><cell>Sigmoid</cell><cell cols="2">( )</cell><cell></cell><cell>Range [0, 1]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Not zero centered</cell></row><row><cell></cell><cell>(</cell><cell>)</cell><cell></cell><cell>Have exponential</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>centered</cell></row><row><cell>TanH</cell><cell>( )</cell><cell></cell><cell></cell><cell>Range [-1, 1]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Zero centered</cell></row><row><cell></cell><cell>(</cell><cell>)</cell><cell></cell></row><row><cell>ReLU [55]</cell><cell>( )</cell><cell></cell><cell></cell><cell>It doesn't saturate</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fast</cell></row><row><cell>leaky</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReLU [56]</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>( )    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3</head><label>3</label><figDesc>CNN architectures (ILSVRC winners)</figDesc><table><row><cell cols="3">Winer by year J o u r n a l P r e -p r o o f No. of conv. layers Top-5 error rate (%)</cell></row><row><cell>2012 -AlexNet [61]</cell><cell>8</cell><cell>16.4</cell></row><row><cell>2013 -ZFNet [62]</cell><cell>8</cell><cell>11.7</cell></row><row><cell>2014 second -VGG-16 [63]</cell><cell>16</cell><cell>7.4</cell></row><row><cell>2014 first -GoogLeNet [64]</cell><cell>22</cell><cell>6.67</cell></row><row><cell>2015 -ResNet [65]</cell><cell>152</cell><cell>3.57</cell></row><row><cell>2016 second -ResNeXt [66]</cell><cell>101</cell><cell>3.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>DL models for generating radiology report</figDesc><table><row><cell>Model</cell><cell>Proposed by</cell><cell>Image</cell><cell>Dataset</cell><cell>Organ</cell><cell>Pathology</cell><cell cols="2">Software CNN</cell><cell>Base</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Modality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Architecture</cell><cell>Technique</cell><cell>Task</cell></row><row><cell>Word-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Deep mining model</cell><cell cols="2">Shin, et al. [51] 2015 CT</cell><cell>PACS of NIH clinical</cell><cell>Multiple (e.g.,</cell><cell>Multiple (e.g. adenopathy,</cell><cell>Caffe [79]</cell><cell>AlexNet [61]</cell><cell>LDA &amp; RNN</cell><cell>Generate semantic labels</cell></row><row><cell></cell><cell></cell><cell>MR</cell><cell>centre [62]</cell><cell>neck, bone, liver,</cell><cell>metastasis and sinus</cell><cell></cell><cell>VGG-16 [63]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>PET</cell><cell></cell><cell>brain and heart)</cell><cell>diseases)</cell><cell></cell><cell>VGG-19 [63]</cell><cell>CNN</cell><cell>Map from images to label spaces</cell></row><row><cell></cell><cell></cell><cell>Computed</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LDPO: looped deep</cell><cell>Wang, et al. [59]</cell><cell>radiography</cell><cell></cell><cell></cell><cell></cell><cell>Caffe [79]</cell><cell>AlexNet [61]</cell><cell>CNN</cell><cell>Initialize looped optimization</cell></row><row><cell>pseudo task optimization network</cell><cell>2016</cell><cell>Ultrasound</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GoogLeNet [64]</cell><cell>K-means/RIM</cell><cell>Cluster images</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NLP</cell><cell>Extracts semantically relevant words</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PCA</cell><cell>Reduce dimensionality</cell></row><row><cell>CNN-based</cell><cell cols="2">Dong, et al. [53] 2017 X-Ray</cell><cell>PACS of the fourth</cell><cell>Chest</cell><cell>9 diseases (e.g.</cell><cell>Caffe [79]</cell><cell>VGG-16 [63]</cell><cell>NLP</cell><cell>Extract disease labels from reports</cell></row><row><cell>classification model</cell><cell></cell><cell></cell><cell>people's hospital (Chinees reports)</cell><cell></cell><cell>emphysema &amp; bronchitis)</cell><cell></cell><cell>ResNet-101 [65]</cell><cell>CNN</cell><cell>Classify images</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RNN</cell><cell>Describe a detected disease</cell></row><row><cell>CheXNet</cell><cell>Rajpurkar, et al. [80]</cell><cell></cell><cell>ChestX-ray14 [34]</cell><cell></cell><cell>Pneumonia &amp; 13 other</cell><cell>-</cell><cell>DenseNet [74]</cell><cell>CNN</cell><cell>Classify images</cell></row><row><cell></cell><cell>2017</cell><cell></cell><cell></cell><cell></cell><cell>pathologies</cell><cell></cell><cell></cell><cell>CAM [91]</cell><cell>Produce heatmaps</cell></row><row><cell>ChestNet</cell><cell>Wang and Xia [92]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Caffe [79]</cell><cell>Resnet-152 [65]</cell><cell>CNN</cell><cell>Perform feature extraction-classification</cell></row><row><cell></cell><cell>2018</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Attention mechanism (Grad-</cell><cell>Exploits correlation between class labels</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CAM [93])</cell><cell>&amp; pathology locations</cell></row><row><cell>DualNet</cell><cell>Rubin, et al. [35]</cell><cell></cell><cell>MIMIC-CXR [33]</cell><cell></cell><cell>14 Thorax diseases (e.g</cell><cell cols="3">PyTorch [82] DenseNet-121 [74] NLP (NegBio [95])</cell><cell>Map reports into UMLS concept ids</cell></row><row><cell></cell><cell>2019</cell><cell></cell><cell></cell><cell></cell><cell>pneumonia &amp; edema)</cell><cell></cell><cell></cell><cell>CNN</cell><cell>Recognize multiple diseases</cell></row><row><cell>Multi-view model</cell><cell>Monshi, et al. [36]</cell><cell></cell><cell></cell><cell></cell><cell>12 Thorax diseases</cell><cell></cell><cell>Resnet-50 [65]</cell><cell>CNN</cell><cell>Detect diseases</cell></row><row><cell></cell><cell>2019</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>discriminative learning rates</cell><cell>Tune each layer with various learning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[94]</cell><cell>rates</cell></row><row><cell>Sentence-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recurrent neural</cell><cell cols="2">Shin, et al. [24] 2016 X-Ray</cell><cell>IU X-Ray [21]</cell><cell>Chest</cell><cell>Thorax diseases (e.g.</cell><cell>-</cell><cell>NIN [73]</cell><cell>CNN</cell><cell>Classify images</cell></row><row><cell>cascade model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cardiomegaly, and</cell><cell></cell><cell>GoogLeNet [64]</cell><cell>LSTM-RNN [77] / GRU-RNN</cell><cell>Describe disease contexts</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>granuloma)</cell><cell></cell><cell></cell><cell>[78]</cell><cell></cell></row><row><cell>Multi-task-loss CNN</cell><cell>Kisilev, et al. [40]</cell><cell>Mammograph</cell><cell>DDSM</cell><cell>Breast</cell><cell>Tumour</cell><cell>Caffe [79]</cell><cell>AlexNet (5 conv.</cell><cell>CNN</cell><cell>Produce ranked ROI</cell></row><row><cell>model</cell><cell>2016</cell><cell>Ultrasound</cell><cell>Private dataset [34]</cell><cell></cell><cell></cell><cell></cell><cell>layers) [61]</cell><cell></cell><cell>Generate semantic description</cell></row><row><cell>Multi-task learning</cell><cell cols="2">Jing, et al. [25] 2017 Multiple</cell><cell>PEIR Gross</cell><cell>21 organ categories</cell><cell>Multiple</cell><cell>-</cell><cell>VGG-19 [63]</cell><cell>CNN</cell><cell>Learn visual features</cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell><cell>(e.g. kidney)</cell><cell></cell><cell></cell><cell></cell><cell>MLC</cell><cell>Predict relevant tags</cell></row><row><cell>Paragraph-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-task learning</cell><cell cols="2">Jing, et al. [25] 2017 X-Ray</cell><cell>IU X-Ray [21]</cell><cell>Chest</cell><cell>Thorax diseases</cell><cell>-</cell><cell>VGG-19 [63]</cell><cell>CNN</cell><cell>Learn visual features</cell></row><row><cell>model</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>hierarchical LSTM</cell><cell>Generate long paragraphs</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLC</cell><cell>Predict relevant tags</cell></row><row><cell>Multimodal recurrent</cell><cell>Xue, et al. [26] 2018</cell><cell></cell><cell>IU X-Ray [21]</cell><cell></cell><cell></cell><cell>-</cell><cell>Resnet-152 [65]</cell><cell>CNN</cell><cell>Extract visual features</cell></row><row><cell>model with attention</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Single layer LSTM</cell><cell>Sentence decoding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bi-LSTM and ID CNN</cell><cell>Sentence encoding</cell></row><row><cell>TieNet: text-image</cell><cell>Wang, et al. [27]</cell><cell></cell><cell>IU X-Ray [21]</cell><cell></cell><cell></cell><cell>TensorFlow</cell><cell>ResNet-50 [65]</cell><cell>NLP</cell><cell>Mine disease labels</cell></row><row><cell>embedding network</cell><cell>2018</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[81] Tensorpack</cell><cell></cell><cell>CNN-RNN LSTM-RNN</cell><cell>Link words with image regions Produce reports</cell></row><row><cell>HRGR-Agent: hybrid</cell><cell>Li, et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>retrieval-generation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>reinforced agent</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5</head><label>5</label><figDesc>Quantitative evaluation of generated radiology reports based on DL models</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Database BLEU-1</cell><cell>BLEU-2</cell><cell>BLEU-3</cell><cell>BLEU-4</cell><cell>METEOR</cell><cell>ROUGH</cell><cell>ROUGH_L</cell><cell>CIDER</cell></row><row><cell>Sentence-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recurrent neural</cell><cell>LSTM</cell><cell>IU X-Ray</cell><cell>79.3</cell><cell>9.1</cell><cell>0.0</cell><cell>0.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>cascade model [24]</cell><cell>GRU</cell><cell>[21]</cell><cell>78.5</cell><cell>14.4</cell><cell>4.7</cell><cell>0.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Multi-task learning model [25]</cell><cell>PEIR</cell><cell>0.300</cell><cell>0.218</cell><cell>0.165</cell><cell>0.113</cell><cell>0.149</cell><cell>0.279</cell><cell>-</cell><cell>0.329</cell></row><row><cell>Paragraph-level</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Multi-task learning model [25]</cell><cell>IU X-Ray</cell><cell>0.517</cell><cell>0.386</cell><cell>0.306</cell><cell>0.247</cell><cell>0.217</cell><cell>0.447</cell><cell>-</cell><cell>0.327</cell></row><row><cell cols="2">Multimodal recurrent model with</cell><cell>[21]</cell><cell>0.464</cell><cell>0.358</cell><cell>0.270</cell><cell>0.195</cell><cell>0.274</cell><cell>0.366</cell><cell>-</cell><cell>-</cell></row><row><cell>attention [26]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TieNet [27]</cell><cell></cell><cell></cell><cell>0.2860</cell><cell>0.1597</cell><cell>0.1038</cell><cell>0.0736</cell><cell>0.1076</cell><cell>-</cell><cell>0.2263</cell><cell>-</cell></row><row><cell>HRGR-Agent [20]</cell><cell></cell><cell></cell><cell>0.438</cell><cell>0.298</cell><cell>0.208</cell><cell>0.151</cell><cell>-</cell><cell>0.322</cell><cell>-</cell><cell>0.343</cell></row><row><cell></cell><cell></cell><cell>CX-CHR</cell><cell>0.673</cell><cell>0.587</cell><cell>0.530</cell><cell>0.486</cell><cell>-</cell><cell>0.612</cell><cell>-</cell><cell>2.895</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6</head><label>6</label><figDesc>Evaluation metrics (image caption measures)</figDesc><table><row><cell>Metric</cell><cell>Purpose</cell><cell>Algorithm</cell><cell>Strengths</cell><cell>Weaknesses</cell></row><row><cell>BLEU[98] 2002</cell><cell>machine translation</cell><cell></cell><cell>Correlates with human</cell><cell>Lack of explicit word matching</cell></row><row><cell></cell><cell></cell><cell></cell><cell>judgments</cell><cell></cell></row><row><cell>ROUGE [99] 2004</cell><cell>document summarization</cell><cell></cell><cell>Favours long sentences</cell><cell>Works only in single document</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>summarization</cell></row><row><cell>METEOR [100] 2005</cell><cell>machine translation</cell><cell></cell><cell>Benefit from synonyms and</cell><cell>Lack of semantic similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell>paraphrase matching</cell><cell>capturing</cell></row><row><cell>CIDEr [101] 2015</cell><cell>image captioning</cell><cell></cell><cell>Works in linguistics means</cell><cell>May weight irrelevant sentence's</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>details</cell></row><row><cell>SPICE [102] 2016</cell><cell>image captioning</cell><cell></cell><cell>Can match noun / object</cell><cell>Reliant on the performance of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>between captions</cell><cell>parsing</cell></row><row><cell cols="5">in failing to locate semantic similarity by applying synonym matching based on WordNet. Nonetheless, observing synonyms alone may not be adequate to</cell></row><row><cell cols="2">capture semantic similarity.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_0"><p>http://www.niftynet.io J o u r n a l P r e -p r o o f</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>J o u r n a l P r e -p r o o f</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_2"><p>http://www.robots.ox.ac.uk/~vgg/research/very_deep/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_3"><p>http://caffe.berkeleyvision.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_4"><p>https://www.tensorflow.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_5"><p>http://pytorch.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_6"><p>https://github.com/ppwwyyxx/tensorpack/ J o u r n a l P r e -p r o o f</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_7"><p>https://github.com/ncbi-nlp/NegBio</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_8"><p>https://github.com/stanfordmlgroup/chexpert-labeler J o u r n a l P r e -p r o o f</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_9"><p>https://github.com/tylin/coco-caption J o u r n a l P r e -p r o o f</p></note>
		</body>
		<back>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>{ doi: https://doi.org/</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interests</head><p>The author declares that they have no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis: how to move from the laboratory to the clinic</title>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Schaefer-Prokop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">261</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="719" to="732" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Implementing machine learning in radiology practice and research</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Prevedello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Filice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Geis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Roentgenology</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="754" to="760" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Discrimination of breast cancer with microcalcifications on mammography by deep learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27327</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Computer-aided diagnosis with deep learning architecture: applications to breast lesions in US images and pulmonary nodules in CT scans</title>
		<author>
			<persName><forename type="first">J.-Z</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">24454</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning in radiology</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Mcbee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Academic radiology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Survey on Deep Learning: Algorithms, Techniques, and Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pouyanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01164</idno>
		<title level="m">The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A guide to deep learning in healthcare</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep EHR: a survey of recent advances in deep learning techniques for electronic health record (EHR) analysis</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rashidi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1589" to="1604" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning for health informatics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ravı</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="21" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning applications in medical image analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="9375" to="9389" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Learning in Medicine-Promise, Progress, and Challenges</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Casalino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khullar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA internal medicine</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Learning: Current and Emerging Applications in Medicine and Technology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Akay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning in radiology: does one size fit all?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Korfiatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Philbrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="521" to="526" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine learning in radiology: applications beyond image interpretation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lakhani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="350" to="359" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Imaging and radiology: MedlinePlus Medical Encyclopedia</title>
		<ptr target="https://medlineplus.gov/ency/article/007451.htm" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interleaved text/image deep mining on a large-scale radiology database for automated image interpretation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<ptr target="https://www.scopus.com/inward/record.uri?eid=2" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research, Article</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2016">2016. -s2.0-84989187487&amp;partnerID=40&amp;md5=83764cf16c1f8dcf723acced65ee2054</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Hybrid Retrieval-Generation Reinforced Agent for Medical Image Report Generation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08298</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Preparing a collection of radiology examinations for distribution and retrieval</title>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The UMLS Metathesaurus: representing different views of biomedical concepts</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Schuyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Hole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Sherertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Medical Library Association</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">217</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">RadLex: a new method for indexing online educational materials</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Radiological Society of North America</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2497" to="2506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">On the automatic generation of medical imaging reports</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08195</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multimodal Recurrent Model with Attention for Automated Radiology Report Generation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9049" to="9058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data Compression and Its Application in Medical Imaging</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Thanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid and Advanced Compression Techniques for Medical Images</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07031</idno>
		<title level="m">CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistics » Diagnostic Imaging Dataset</title>
		<ptr target="https://www.england.nhs.uk/statistics/statistical-work-areas/diagnostic-imaging-dataset/" />
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frequency and distribution of chest radiographic findings in COVID-19 positive patients</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y F</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="page">201160</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ACR recommendations for the use of chest radiography and computed tomography (CT) for suspected COVID-19 infection</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C O</forename><surname>Radiology</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACR website</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">MIMIC-CXR: A large publicly available database of labeled chest radiographs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07042</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weaklysupervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Large scale automated reading of frontal and lateral chest x-rays using dual convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sanghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu-Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07839</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional Neural Network to Detect Thorax Diseases from Multi-view Chest X-Rays</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M A</forename><surname>Monshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<publisher>Springer Nature Switzerland AG</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De La Iglesia-Vayá</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07441</idno>
		<title level="m">PadChest: A large chest x-ray image dataset with multi-label annotated reports</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Do Lateral Views Help Automated Chest X-ray Predictions?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hashir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08534</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The digital database for screening mammography</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kopans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th international workshop on digital mammography</title>
		<meeting>the 5th international workshop on digital mammography</meeting>
		<imprint>
			<publisher>Medical Physics Publishing</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="212" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Medical image description using multi-task-loss CNN</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kisilev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hashoul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Data Labeling for Medical Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="121" to="129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">DICOM search in medical image archive solution e-Sushrut Chhavi</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 3rd International Conference on Electronics Computer Technology</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="256" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The ultimate guide to AI in radiology</title>
		<author>
			<persName><forename type="first">O</forename><surname>Six</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Healthcare Solutions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep Learning Applications in Chest Radiography and Computed Tomography: Current State of the Art</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of thoracic imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="75" to="85" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Classification using deep learning neural networks for brain tumors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mohsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-S</forename><forename type="middle">A</forename><surname>El-Dahshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-S</forename><forename type="middle">M</forename><surname>El-Horbaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-B</forename><forename type="middle">M</forename><surname>Salem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Computing and Informatics Journal</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="71" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Retinal lesion detection with deep learning using image patches</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Investigative ophthalmology &amp; visual science</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="590" to="596" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interactive medical image segmentation using deep learning with image-specific fine tuning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1562" to="1573" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Medical image retrieval using deep convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Qayyum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Majid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">266</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Super-resolution musculoskeletal MRI using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chaudhari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance in medicine</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2139" to="2154" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">NiftyNet: a deep-learning platform for medical imaging</title>
		<author>
			<persName><forename type="first">E</forename><surname>Gibson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer methods and programs in biomedicine</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="page" from="113" to="122" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">State-of-the-art review on deep learning in medical imaging</title>
		<author>
			<persName><forename type="first">M</forename><surname>Biswas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frontiers in bioscience</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="392" to="426" />
		</imprint>
	</monogr>
	<note>Landmark edition</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interleaved text/image deep mining on a very large-scale radiology database</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1090" to="1099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Deep learning</title>
		<imprint>
			<publisher>MIT press Cambridge</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to read chest X-ray images from 16000+ examples using CNN</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies</title>
		<meeting>the Second IEEE/ACM International Conference on Connected Health: Applications, Systems and Engineering Technologies</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="51" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning: An overview</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07274</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Activation functions: Comparison of trends in practice and research for deep learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Nwankpa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ijomah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gachagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03378</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Unsupervised category discovery via looped deep pseudo-task optimization using a large scale radiology image database</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07965</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automatic segmentation of MR brain images with a convolutional neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Mendrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Benders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1252" to="1261" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Ilsvrc</surname></persName>
		</author>
		<ptr target="http://image-net.org/challenges/LSVRC/2016/results#team" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases</title>
		<author>
			<persName><forename type="first">P</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="498" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Černocký</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>in Eleventh annual conference of the international speech communication association</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05225</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">A Joint Introduction to Natural Language Processing and to Deep Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Natural Language Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Deep Learning in Natural Language Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Deep learning in natural language generation from images</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Natural Language Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="289" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Unsupervised topic modeling in a large free text radiology report repository</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hassanpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="62" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Natural Language Processing for Large-Scale Medical Image Analysis Using Deep Learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning for Medical Image Analysis</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="405" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Mdnet: A semantically and visually interpretable medical image diagnosis network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6428" to="6436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Chestnet: A deep neural network for classification of thoracic diseases on chest radiography</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03058</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradientbased localization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Universal language model fine-tuning for text classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06146</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">NegBio: a high-performance tool for negation and uncertainty detection in radiology reports</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Diagnosis labeling with disease-specific characteristics mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence in medicine</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="25" to="33" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Meteor universal: Language specific translation evaluation for any target language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Denkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth workshop on statistical machine translation</title>
		<meeting>the ninth workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="376" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics</title>
		<meeting>the 42nd Annual Meeting on Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">605</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Re-evaluating automatic metrics for image captioning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kilickaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07600</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Comprehensible reasoning and automated reporting of medical examinations based on deep learning analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hicks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM Multimedia Systems Conference</title>
		<meeting>the 9th ACM Multimedia Systems Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="490" to="493" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
