<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">T-CAT: Dynamic Cache Allocation for Tiered Memory Systems With Memory Interleaving</title>
				<funder ref="#_XWWDrJt">
					<orgName type="full">National Research Foundation of Korea</orgName>
					<orgName type="abbreviated">NRF</orgName>
				</funder>
				<funder ref="#_6vvxEwQ #_CwYAmzG">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_puQ67wj">
					<orgName type="full">Korea government (MSIT)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hwanjun</forename><surname>Lee</surname></persName>
							<email>lee.hwanjun@dgist.ac.kr</email>
							<idno type="ORCID">0009-0001-0513-2112</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">DGIST</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seunghak</forename><surname>Lee</surname></persName>
							<email>slee@dgist.ac.kr</email>
							<idno type="ORCID">0000-0003-2221-8433</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">DGIST</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yeji</forename><surname>Jung</surname></persName>
							<email>jung.yeji@dgist.ac.kr</email>
							<idno type="ORCID">0009-0008-7840-4669</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">DGIST</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daehoon</forename><surname>Kim</surname></persName>
							<email>dkim@dgist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">DGIST</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">DGIST</orgName>
								<address>
									<postCode>42988</postCode>
									<settlement>Daegu</settlement>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">T-CAT: Dynamic Cache Allocation for Tiered Memory Systems With Memory Interleaving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LCA.2023.3290197</idno>
					<note type="submission">received 21 February 2023; revised 24 May 2023; accepted 24 June 2023. Date of publication 28 June 2023; date of current version 19 July 2023.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cache partitioning</term>
					<term>memory interleaving</term>
					<term>nonuniform memory architecture (NUMA)</term>
					<term>tiered memory systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>New memory interconnect technology, such as Intel's Compute Express Link (CXL), helps to expand memory bandwidth and capacity by adding CPU-less NUMA nodes to the main memory system, addressing the growing memory wall challenge. Consequently, modern computing systems embrace the heterogeneity in memory systems, composing the memory systems with a tiered memory system with near and far memory (e.g., local DRAM and CXL-DRAM). However, adopting NUMA interleaving, which can improve performance by exploiting node-level parallelism and utilizing aggregate bandwidth, to the tiered memory systems can face challenges due to differences in the access latency between the two types of memory, leading to potential performance degradation for memory-intensive workloads. By tackling the challenges, we first investigate the effects of the NUMA interleaving on the performance of the tiered memory systems. We observe that while NUMA interleaving is essential for applications demanding high memory bandwidth, it can negatively impact the performance of applications demanding low memory bandwidth. Next, we propose a dynamic cache management, called T-CAT, which partitions the last-level cache between near and far memory, aiming to mitigate performance degradation by accessing far memory. T-CAT attempts to reduce the difference in the average access latency between near and far memory by re-sizing the cache partitions. Through dynamic cache management, T-CAT can preserve the performance benefits of NUMA interleaving while mitigating performance degradation by the far memory accesses. Our experimental results show that T-CAT improves performance by up to 17% compared to cases with NUMA interleaving without the cache management.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>core count integrated into single-processor chips is increasing to support the performance demand of data-intensive applications, leading to an imbalance between the processor and memory performance <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Consequently, demand of the memory bandwidth and capacity leads modern computing systems to embrace heterogeneity in memory systems with emerging non-DDR memory interconnect technology such as CXL <ref type="bibr" target="#b3">[4]</ref>, which is enable to plug in more DRAM modules via PCIe interface (i.e., CXL-DRAM), composing the main memory system with the tiered memory system consisting of host DRAM and CXL-DRAM.</p><p>As the tiered memory systems typically compose the main memory system with multiple CPU-less NUMA nodes, each of which has different memory access latency, data placement and management policy is the key fact of system performance. For memory-intensive applications, NUMA interleaving policy which places data across all NUMA node in the round-robin manner improves the performance considerably by handling memory requests in parallel on DIMMs of each NUMA node while utilizing the aggregate bandwidth of the multiple NUMA nodes. However, the NUMA systems composed of near and far memory are likely to suffer performance degradation with the NUMA interleaving, in particular for applications demanding low memory bandwidth, due to the access latency of far memory (e.g., CXL-DRAM) much longer than near memory (e.g., local DRAM).</p><p>Many studies propose techniques in software manners such as NUMA-aware data placement and page migration to hide far memory latency with the tiered memory system <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. These studies identify frequently accessed data (i.e., hot data), then migrate the hot data to near memory from far memory, which makes applications obtain the hot data from near memory more frequently. However, such data migration techniques require costly software implementation (i.e., memory copies, page table updates, and TLB flushing) and considerably increase tail latency by blocking each memory channels from servicing memory requests <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>. Furthermore, the migration techniques can suffer from significant performance degradation while not efficiently utilizing aggregate bandwidth when the channel bandwidth for near memory is saturated.</p><p>By tackling the issues, we first investigate the effects of the NUMA interleaving at page granularity (i.e., 4 KB) on the performance of memory-intensive applications with the tiered memory system. We observe that the interleaving may degrade the performance due to far memory accesses while improving the performance considerably by utilizing the aggregate bandwidth when memory traffic is heavy. Therefore, we propose a dynamic cache allocation technique, called T-CAT, which partitions the last-level cache (LLC) for near and far memory and re-sizes the partitions to adjust memory traffic between them. To this end, T-CAT divides cache ways into two partitions for the near and far memory, which aims to reduce the difference in the access latency to data from near and far memory. For dynamic partitioning, T-CAT measures the average access latency of each memory node's data at the first-level cache (i.e., L1 cache) and enlarges the partition for the memory type with longer latency while shrinking the partition for the other one, balancing the latency between the memory nodes. Consequently, T-CAT preserves the benefits of memory interleaving while preventing the far memory from becoming the performance bottleneck. With real machine based set-up, our experimental results show that T-CAT improves the performance of NUMA interleaving by up to 17% while showing even higher performance than the case where all data are placed on the fast local node without interleaving.</p><p>To the best of our knowledge, this is the first study that improves the performance by balancing performance difference in tiered memory systems through the dynamic cache allocation. T-CAT does not require costly implementation; note that current commercial processors already support cache partitioning (e.g., Intel Cache Allocation Technology (CAT), AMD Cache Allocation Enforcement (CAE)) and data access latency measurement (e.g., Intel Performance Counter Monitor, AMD ?Prof). Lastly, T-CAT can be easily integrated with prior data placement and migration techniques between NUMA nodes based on the data hotness for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. NUMA Management in Tiered Memory Systems</head><p>NUMA systems are prevalent in data centers, benefiting from the increase in aggregate bandwidth and scalable memory capacity they provide. However, accessing memory that is not close to the processor (i.e., far memory) can result in performance degradation. To address this issue, the placement and movement of data between near and far memory is crucial in enhancing performance in NUMA systems.</p><p>The NUMA management in default Linux kernel offers two basic placement policies: first-touch and page interleaving. The first-touch focuses on data nearness, allocating data in the nearest memory first. When the nearest memory becomes full, the first-touch allocates data in the next nearest node. Consequently, first-touch may not fully leverage the aggregate memory bandwidth, even if the far memory has sufficient available memory. Furthermore, the first-touch requires the frequent migration of pages that are frequently accessed to near memory, which can be costly in terms of overhead. In contrast, the page interleaving distributes data across all available memory nodes, rather than allocating it to near memory first. This can improve performance by leveraging aggregate memory bandwidth for applications demanding high bandwidth. However, it should be noted that far memory exhibits longer latency compared to near memory, resulting in performance degradation due to accessing data stored in far memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Effects of Memory Interleaving on Tiered Memory Systems</head><p>To investigate the effects of NUMA interleaving on the performance of the tiered memory system, we run applications from SPECCPU2006 <ref type="bibr" target="#b11">[12]</ref> and GAPBS <ref type="bibr" target="#b12">[13]</ref> on a two-socket NUMA machine equipped with 64 GB DDR4 DRAM while all CPUs on  the far node are disabled to emulate CPU-less NUMA node. To mimic the latency difference in systems with CXL interconnects, we configure the access latency of near and far memory to 65.2 ns and 165.6 ns, respectively, with a difference of 100 ns. Both nodes are equipped with the same number of memory channels and have the same capacity.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the normalized execution time when NUMA interleaving is enabled (denoted as w/ interleaving) or disabled (denoted as w/o interleaving). The results are normalized to ones of w/o interleaving. For w/o interleaving, all data is stored on the near DRAM. On the other hand, for w/ interleaving, half of the data is stored on the near DRAM while the other half is stored on CXL-DRAM. In Fig. <ref type="figure" target="#fig_0">1</ref>, the left four applications do not fully utilize the near memory bandwidth while the right four applications fully utilize the near memory bandwidth.</p><p>As plotted in Fig. <ref type="figure" target="#fig_0">1</ref>, the NUMA interleaving has a negative impact on the performance of applications that do not fully utilize the bandwidth of near DRAM since the applications obtain half of the data from the slow far memory. For example, the memory interleaving increases the execution time of cc by 36%. On the contrary, when the applications saturate the bandwidth of the near memory, the interleaving improves execution time by up to 15% compared to cases without interleaving, as it allows applications to utilize additional bandwidth from the far memory.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> shows the average memory access latency with and without NUMA interleaving. For applications (i.e., xalan, omnetpp, bfs, cc) that do not require high memory bandwidth while not saturating the bandwidth of near memory, the average access latency is longer than the latency of the near memory and shorter than the latency of the far memory of w/ interleaving case. Consequently, for those applications, the memory interleaving degrades the performance due to longer access latency of the far memory. However, for applications (i.e., milc, leslie3d, pr, pr_spmv) that saturate the near memory node bandwidth, both near and far access latency with interleaving is shorter than w/o interleaving case. For example, The far memory access latency of cc is longer by 27% than w/o interleaving and pr is shorter by 22% than w/o interleaving. Fig. <ref type="figure">3</ref> illustrates the overall architecture of T-CAT with an example assuming two memory nodes, near memory and far memory. T-CAT employs two latency monitors per L1 cache to measure the average access latency to data from near and far memory. The monitors leverage the existing implementation supported by the Intel Performance Counter Monitor (PCM). In calculating the average access latency, we exclude the latencies resulting from L1 hits, since applications with a relatively high L1 hit rate do not show a notable difference in the average access latency between near and far memory even though far memory delays the execution time considerably.</p><p>By periodically monitoring the average access latency of the near and far memory, the decision engine in T-CAT dynamically adjusts the size of the LLC partitions between the two memory nodes to maintain a balanced average latency. If the latency difference is greater than a particular threshold that we empirically obtain, the decision engine requests to allocate/deallocate LLC ways for each memory node to the cache allocator. T-CAT stops and resets the partitioning when it does not observe memory traffic.</p><p>With the tiered memory system composed of near and far memory, initially, T-CAT equally partitions the LLC ways for each memory. If the calculated average access latency to data from the far memory is longer than the latency to data from the near memory, T-CAT reclaims an LLC way of the near memory and allocates it to the far memory (? in Fig. <ref type="figure">3</ref>). Conversely, if the calculated average access latency to data from the near memory is longer, T-CAT reclaims an LLC way from the far memory's LLC and allocates it to the near memory (? in Fig. <ref type="figure">3</ref>). By allocating more LLC ways to a memory node, the number of LLC misses is reduced, thereby reducing the memory traffic for that node.</p><p>The cache allocator adjusts the size of LLC partitions for each memory node by slightly extending Intel Cache Allocation Technology (CAT) technology. In addition to existing per core bit mask that represents the allocated ways for each core in the Intel CAT, T-CAT requires a bit mask per memory node. For example, with two memory nodes as depicted in Fig. <ref type="figure">3</ref>, T-CAT requires two additional bit masks, one for near memory (i.e., near mask) and the other for far memory (i.e., far mask). This allows T-CAT to manage the way allocation for near and far memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Methodology</head><p>To investigate the quantitative potential of T-CAT on performance with the real machine setup, we emulate the tiered memory system using a dual-socket Non-Uniform Memory Access (NUMA) machine. To emulate a tiered memory system, we use a single NUMA node (i.e., local node), which includes an eight-core processor (Intel Xeon E5-2620 v4) with a 20 MB 20-way LLC and a 32 GB DDR memory while disabling all cores of another node (i.e., remote node). For the memory configuration, we dedicate one memory channel for the local memory and another channel for the remote memory, assuming the local and remote DRAM as near and far memory, respectively. We partition the LLC for near and far memory using Intel CAT. Since the existing CAT only supports way-based partitioning for each process, not for each memory node, we run two groups of four copies, each of which only accesses the near and far memory, respectively, on separate cores to partition the cache for the near and far memory. In comparison to the case of NUMA interleaving with eight copies, we do not observe any notable difference in our experimental setup in terms of execution time, memory bandwidth, and memory latency. We place data of a group of the four copies on the remote memory (i.e., far memory) while placing the other group's one on the local memory (i.e., near memory) using numactl API; we configure the access latency of local and remote memory to 65.2 ns and 165.6 ns, respectively. For experiments, we run applications from SPECCPU2006 <ref type="bibr" target="#b11">[12]</ref> and GAPBS <ref type="bibr" target="#b12">[13]</ref>. We use 500 ms as the partitioning period of T-CAT, and measure the access latency to data from the near and far memory by reading Model Specific Register (MSR) for each L1 cache, and average them out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>Fig. <ref type="figure" target="#fig_3">4</ref> compares the performance of a tiered memory system composed of near and far memory among three approaches, w/ interleaving, w/o interleaving, and T-CAT, respectively. All results are normalized to the results of w/ interleaving. For w/ interleaving, we interleave data between the near and far memory at page granularity while placing all data on the near memory for w/o interleaving. T-CAT dynamically adjusts the LLC partitions for near and far memory while interleaving data between them. For applications demanding low memory bandwidth, such as omnetpp, xalan, and gcc, w/ interleaving shows  lower performance than w/o interleaving due to accesses to the far memory. On the other hand, w/o interleaving shows lower performance than w/ interleaving with applications demanding high memory bandwidth, such as milc, libquantum, and leslie3d since w/o interleaving does not utilize the aggregated memory bandwidth of both memory nodes. Compared to w/ interleaving and w/o interleaving, T-CAT improves performance by 9% and 6% on average, respectively. T-CAT improves performance for all applications by up to 17% compared to w/ interleaving by moderating the performance degradation by far memory accesses with the NUMA interleaving while utilizing aggregated memory bandwidth of both memories. Compared to w/o interleaving, which places all data on the near memory, T-CAT also improves performance for applications by up to 37%. However, for applications that do not require high memory bandwidth, such as xalan, omnetpp, and cc, w/o interleaving outperforms T-CAT, as these applications only use the near memory without bandwidth saturation. The reason for the lower performance for those applications is that they are the applications that cannot obtain the benefits of aggregate bandwidth with memory interleaving while demanding low memory bandwidth. We expect that integrating T-CAT with software-based data placement or migration techniques between near and far memory can considerably improve the performance further for those applications. Fig. <ref type="figure" target="#fig_4">5</ref> depicts the difference in the average access latency between near and far memory with w/ interleaving and T-CAT . As shown in the figure, w/ interleaving shows a large difference in the access latency between near and far memory nodes (i.e., 20.38 ns) while T-CAT significantly reduces this difference to 6.06 ns. T-CAT effectively balances the average access latency between near and far memory through dynamic cache partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DISCUSSION</head><p>While T-CAT demonstrates its effectiveness with NUMA interleaving page placement, it is clear that there are applications that prefer uneven page placement between memory nodes along with dynamic placement/migration policies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> rather than interleaving. The dynamic page placement/migration policies typically place pages on memory nodes based on their hotness (i.e., place hot pages on near memory). Such hotness based placement/migration creates imbalance in memory traffics between nodes by increasing the number of memory requests to near memory while decreasing the number of requests to far memory. We expect that T-CAT still works with the uneven page placement by dynamic placement/migration policies since T-CAT fundamentally relies on the average L1 miss latency, which inherently takes into account performance impact of changes in memory traffics. Consequently, we expect that there is a positive synergy in performance improvement when integrating T-CAT with dynamic page placement/migration policies. By combining the cache management capabilities of T-CAT with intelligent page placement/migration policies, we can further optimize access patterns to memory nodes and enhance overall system performance. We remain it as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this letter, we propose a dynamic cache management technique called T-CAT, which can dynamically allocate and resize the LLC capacity for near and far memory. T-CAT determines the LLC capacity for near and far memory based on the average memory access latency of each memory node, balancing the average access latency between memory nodes. Consequently, T-CAT improves the performance of the tiered memory systems with NUMA interleaving by moderating performance penalty by far memory accesses while utilizing aggregate memory bandwidth of both near and far memory. Our experimental results show that T-CAT improves performance by up to 17% compared with the results of w/ interleaving.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Normalized runtime between w/ interleaving and w/o interleaving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Average memory node access latency with tiered memory system of w/ interleaving and w/o interleaving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig.</head><label></label><figDesc>Fig. The overall architecture of T-CAT .</figDesc><graphic url="image-1.png" coords="3,74.87,67.13,439.82,103.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of performance among w/ interleaving, w/o interleaving, and T-CAT .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Latency difference of w/ interleaving and T-CAT .</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 08:15:48 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by <rs type="institution">Samsung Electronics Co</rs>., Ltd under Grant <rs type="grantNumber">IO220316-09443-01</rs>, in part by <rs type="funder">National Research Foundation of Korea (NRF)</rs> under Grants <rs type="grantNumber">NRF-2020R1C1C1013315</rs> and <rs type="grantNumber">NRF-2018R1A5A1060031</rs>, and in part by the <rs type="institution">Institute of Information and communications Technology Planning and Evaluation (IITP)</rs> under Grant <rs type="grantNumber">2018-0-00503</rs> funded by the <rs type="funder">Korea government (MSIT)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XWWDrJt">
					<idno type="grant-number">IO220316-09443-01</idno>
				</org>
				<org type="funding" xml:id="_6vvxEwQ">
					<idno type="grant-number">NRF-2020R1C1C1013315</idno>
				</org>
				<org type="funding" xml:id="_CwYAmzG">
					<idno type="grant-number">NRF-2018R1A5A1060031</idno>
				</org>
				<org type="funding" xml:id="_puQ67wj">
					<idno type="grant-number">2018-0-00503</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SMT: Software-defined memory tiering for heterogeneous computing systems with CXL memory expander</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2023-04">Mar./Apr. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gen-Z: Communication at the speed of memory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Supercomputing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling of memory performance and capacity with CXL memory expander</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Hot Chips Symp</title>
		<meeting>IEEE Hot Chips Symp</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Compute Express Link</title>
		<ptr target="https://www.computeexpresslink.org" />
		<imprint>
			<date type="published" when="2023-06-25">2023. Jun. 25, 2023</date>
		</imprint>
	</monogr>
	<note>CXL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nimble page management for tiered memory systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Architectural Support Program</title>
		<meeting>ACM Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="331" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MULTI-CLOCK: Dynamic tiering for hybrid memory systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maruf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bhimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rudoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rangaswami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. High Perform</title>
		<meeting>IEEE Int. Symp. High Perform</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="925" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pond: CXL-based memory pooling systems for cloud platforms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Architectural Support Program</title>
		<meeting>ACM Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="574" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the design space of page management for multi-tiered memory systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. USENIX Annu. Tech. Conf., 2021</title>
		<meeting>USENIX Annu. Tech. Conf., 2021</meeting>
		<imprint>
			<biblScope unit="page" from="715" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">TPP: Transparent page placement for CXL-enabled tiered-memory</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Maruf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Architectural Support Program</title>
		<meeting>ACM Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="742" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">TMO: Transparent memory offloading in datacenters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Int. Conf. Architectural Support Program</title>
		<meeting>ACM Int. Conf. Architectural Support Program</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="609" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3D-Xpath: High-density managed dram architecture with cost-effective alternative paths for memory transactions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Parallel Architectures Compilation Techn</title>
		<meeting>IEEE Int. Conf. Parallel Architectures Compilation Techn</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SPECCPU2006 benchmark descriptions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The gap benchmark suite</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.03619</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
