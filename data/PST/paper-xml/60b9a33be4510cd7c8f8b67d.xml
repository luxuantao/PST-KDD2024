<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph-Based Embedding Smoothing for Sequential Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tianyu</forename><surname>Zhu</surname></persName>
							<idno type="ORCID">0000-0001-7716-938X</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Science and Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leilei</forename><surname>Sun</surname></persName>
							<email>leileisun@buaa.edu.cn</email>
							<idno type="ORCID">0000-0002-0157-1716</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Science and Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guoqing</forename><surname>Chen</surname></persName>
							<email>chengq@sem.tsinghua.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Management Science and Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph-Based Embedding Smoothing for Sequential Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TKDE.2021.3073411</idno>
					<note type="submission">received 8 July 2020; revised 10 Mar. 2021; accepted 7 Apr. 2021. Date of publication 15 Apr. 2021; date of current version 7 Dec. 2022.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender systems</term>
					<term>embedding smoothing</term>
					<term>graph convolution</term>
					<term>sequential recommendation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In real-world scenarios, a user's interactions with items could be formalized as a behavior sequence, indicating his/her dynamic and evolutionary preferences. To this end, a series of recent efforts in recommender systems aim at improving recommendation performance by considering the sequential information. However, impacts of sequential behavior on future interactions may vary greatly in different scenarios. Additionally, semantic item relations underlying item attributes have not been well exploited in sequential recommendation models, which could be crucial for measuring item similarities in recommendation. To deal with the above problems, this paper provides a general embedding smoothing framework for sequential recommendation models. Specifically, we first construct a hybrid item graph by fusing sequential item relations derived from user-item interactions with semantic item relations built upon item attributes. Second, we perform graph convolutions on the hybrid item graph to generate smoothed item embeddings. Finally, we equip sequential recommendation models with the smoothed item representations to enhance their performances. Experimental results demonstrate that with our embedding smoothing framework, the state-of-the-art sequential recommendation model, SASRec, achieves superior performance to most baseline methods on three real-world datasets. Moreover, the results show that most mainstream sequential recommendation models could benefit from our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>R ECOMMENDER System (RS) has become a crucial technol- ogy for e-commerce platforms and other online services in recent years. How to accurately capture user preferences is a core issue in recommendation. In practical applications, a user's interest is usually shifting over time and can be affected by his/her recent interactions. To model the dynamics in user preferences, recent efforts have proposed various sequential architectures (e.g., RNN, CNN, and self-attention <ref type="bibr" target="#b0">[1]</ref>) for generating sequential recommendations <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>Despite the success of sequential models in Natural Language Processing (NLP) and other areas, some issues of concern may arise when they are applied to recommendation. Specifically, although user interactions appear in the form of sequences, the sequential dependency is not necessarily held in some real-world scenarios. For example, when shopping online, a user who has bought a laptop would buy accessories (e.g., a mouse or a keyboard) later. However, when choosing restaurants for dinner, people's choices may not follow a certain order. Therefore, the sequentiality in user interactions may vary greatly in different recommendation scenarios, which may seriously affect the applicability of sequential recommendation models.</p><p>To empirically demonstrate the sequentiality in user interactions in different recommendation scenarios, we extract sequential association rules above certain thresholds of support count and confidence <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref> in the form of ðS u tÀL ; . . . ; S u tÀ1 Þ ! S u t in three real-world datasets, i.e., Amazon Books, Yelp, and Google Local (see the details of these datasets in Section 5). Fig. <ref type="figure" target="#fig_0">1</ref> summarizes the number of rules under the Markov order L with the minimum support count of 5 and the minimum confidence of 10%=30%. It can be seen that user interactions in Amazon Books/Google Local/Yelp show a high/medium/low sequentiality, respectively.</p><p>In addition, sequential models only consider sequential item relations in terms of user behavior, while neglecting semantic item relations that are crucial for measuring item similarities in recommendation. Semantic item relations refer to connections between items in terms of attributes or features (e.g., genres for books or locations for POIs). However, these relations are not necessarily captured by sequential models. For example, two movies of the same genre but with different release time may rarely appear adjacent to each other in user interaction sequences. Therefore, it is important to consider semantic item relations in sequential recommendation models, since they can complement and regularize the item similarities obtained from user interactions, thereby improving recommendation accuracy.</p><p>In this paper, we propose an effective framework to address these issues. The main idea is to smooth the item embeddings with item relation graphs to maintain the global item similarity structure in sequential models. Specifically, we first construct a hybrid item graph for embedding smoothing. We build a sequential item graph from useritem interactions and construct a semantic item graph through item side information (e.g., item attributes). Then we combine both graphs to generate a hybrid item graph that involves multiple item relations. Second, we perform graph convolutions on the hybrid item graph to generate smoothed item embeddings. We point out the drawbacks of using Graph Convolutional Network (GCN) <ref type="bibr" target="#b5">[6]</ref> directly and implement a simplified graph convolution by removing the redundant nonlinearity in GCN, one that specifically tailors for recommendation problems. Finally, the smoothed item embeddings work as the input of sequential models to generate recommendations. In this way, sequential models involve both user interacted items and their related items when predicting the next item, relaxing the sequential assumptions in these models and enhancing their generalization ability.</p><p>Furthermore, we evaluate the proposed embedding smoothing method with a state-of-the-art sequential recommendation model, SASRec <ref type="bibr" target="#b3">[4]</ref>, on three public datasets. Results of extensive experiments show that the improved SASRec outperforms all baselines on high/medium sequentiality dataset (i.e., Amazon Books/Google Local) and achieves better results on low sequentiality dataset (i.e., Yelp). We also conduct experiments to show how SASRec performs with shuffled user interaction orders and the effect of the proposed method on alleviating the performance degradation. Lastly, experiments are conducted to verify the effectiveness of the proposed method on different categories of sequential recommendation models.</p><p>The rest of this paper is organized as follows. Section 2 introduces the preliminaries, Section 3 presents the proposed method in detail, Section 4 discusses the relationship of the proposed method with existing efforts and analyzes the time complexity of the proposed method, Section 5 evaluates the performances of the compared models, and Section 6 reviews the related work. Finally, Section 7 concludes the work with possible future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>This section formulates the sequential recommendation problem and presents a brief description of the backbone model SASRec and graph convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>In sequential recommendation, let U ¼ fu 1 ; u 2 ; . . . ; u jUj g be a set of users, I ¼ fi 1 ; i 2 ; . . . ; i jIj g be a set of items, and S ðuÞ ¼ i </p><formula xml:id="formula_0">1 ; i<label>ðuÞ</label></formula><p>Then the top-N items could be recommended to user u according to the probabilities in descending order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">SASRec</head><p>Here we concisely introduce the backbone model SASRec <ref type="bibr" target="#b3">[4]</ref>. SASRec (short for Self-Attention based Sequential Recommendation) models the sequential dependency through the self-attention architecture in Transformer <ref type="bibr" target="#b0">[1]</ref>. Specifically, it first converts user interaction sequences into the vector sequences by embedding look-up. Then a trainable positional embedding is injected into the input embeddings, and a self-attention layer with two feed-forward layers is used to encode the sequences. After stacking the self-attention blocks with residual connection, the final representation vector at time step t is treated as the sequence representation. Finally, the similarity score between the sequence representation and the embedding of a candidate item is calculated by dot-product and the cross-entropy loss is adopted for model training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Convolution</head><p>Since graph convolution is one of the essential components in our proposed method, we introduce it briefly here. Early efforts defined graph convolution in the Fourier domain by computing the eigendecomposition of the normalized graph Laplacian <ref type="bibr" target="#b6">[7]</ref>. Then graph convolution can be defined as the multiplication of a signal with a parametric filter. To avoid computing the eigendecomposition of the normalized graph Laplacian, ChebNet <ref type="bibr" target="#b7">[8]</ref> approximates the filter by the Chebyshev polynomials. The widely used Graph Convolutional Network (GCN) <ref type="bibr" target="#b5">[6]</ref> simplifies ChebNet by introducing a firstorder approximation, and its matrix form can be written as</p><formula xml:id="formula_3">H ðkÞ ¼ sð ÂH ðkÀ1Þ Q ðkÞ Þ;<label>(2)</label></formula><p>where H ðkÞ 2 R NÂd and Q ðkÞ 2 R dÂd are the hidden embedding matrix and the filter parameter matrix in the kth layer, H ð0Þ ¼ X is the node feature matrix, and sðÁÞ is the activation function. Â ¼ DÀ1=2 Ã DÀ1=2 is the renormalized adjacency matrix of the graph, where</p><formula xml:id="formula_4">Ã ¼ A þ I.</formula><p>By stacking multiple such layers, GCN could aggregate the information from multi-hop neighbors into the center node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>This section introduces the proposed method. We present the framework, the construction of item graphs, and the process of graph convolution for embedding smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Framework</head><p>The framework of our proposed method is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. First, we construct a hybrid item graph by fusing the sequential item relations from user interaction sequences with the semantic item relations from item attributes. The hybrid item graph involves both sequential and semantic relations among items. Second, we perform graph convolutions on the hybrid item graph to generate smoothed item embeddings as the input of sequential recommendation models for improving their performance. Our proposed method can be considered as a plug-in module for sequential recommendation models, namely Graph-based Embedding Smoothing (GES). In this paper, we adopt SASRec as the backbone model and apply the proposed GES to it as GES-SASRec to verify its effectiveness. We will describe the main parts of our proposed method in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating Item Graphs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sequential Item Graph</head><p>Compared with graph-based recommendation models, neural models are not necessarily capturing the multi-hop transitivity of items <ref type="bibr" target="#b8">[9]</ref>. That is, they are poor at maintaining the global structure of the user-item interaction graph when generating recommendations. For instance, a user's threehop items in the user-item interaction graph should be ranked higher than his/her five-hop items, while such an order may not be kept for neural models. Hence, we consider encoding the graph structure explicitly in the item embeddings to cope with this problem.</p><p>Specifically, we aggregate all user interaction sequences to build a global sequential item graph, which reflects relations among items in terms of user behavior. Let A seq 2 R jIjÂjIj be the adjacency matrix of the sequential item graph. It contains the frequency that two items are adjacent in all user interaction sequences. An example of generating A seq is shown in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><p>Note that the sequential item graph here is built as an undirected weighted graph. Since we aim at smoothing item embeddings according to the item graph, we mainly focus on the adjacency of the items when constructing the item graph, and the sequentiality in user behavior has been modeled by the sequential architecture in SASRec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Semantic Item Graph</head><p>The sequential item graph built from user-item interactions reflects item transition patterns in terms of user behavior. However, it is worth mentioning that semantic item relations are also crucial for measuring item similarities in recommendation, which are not necessarily captured by sequential models. For example, two movies of the same genre but with different release time may rarely appear adjacent to each other in user interaction sequences. Therefore, it is important to consider semantic item relations in sequential recommendation models to complement and regularize the item similarities obtained from user interactions.</p><p>Semantic item relations refer to connections between items in terms of attributes or features, which can be obtained from item side information. For instance, books of the same genre or POIs within a small area can be considered semantically related. Then, a semantic item graph can be constructed according to the semantic item relations. Let A sem 2 R jIjÂjIj be the adjacency matrix of the semantic item graph. An example of generating A sem is shown in Fig. <ref type="figure" target="#fig_1">2</ref>. More precisely, we define a ij in A sem as a ij ¼ 2 if i has a bidirectional relation with j, 1 if i has a unidirectional relation with j, 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">&lt; :</head><p>Although semantic item relations may be unidirectional in real-world scenarios, we construct the semantic item graph as an undirected weighted graph, and the weights reflect the relation strength.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Graph Fusion</head><p>To combine both sequential and semantic item graphs for embedding smoothing, a simple idea is to propagate item embeddings in each graph and then fuse the output to obtain the final item embeddings. However, such an idea couples the two kinds of graphs loosely and neglects item transitions between the two relations. For example, if a user bought item B after item A, and item B shares similar features with item C, then item A and C may be highly related, while such relations are not modeled if we perform embedding smoothing on the item graphs respectively. To this end, we fuse the item graphs before smoothing item embeddings.</p><p>We apply a weighted sum to fusing the sequential and semantic item graphs with self-loops into a hybrid item graph, as</p><formula xml:id="formula_5">A hyb ¼ I þ aA seq þ bA sem ;<label>(3)</label></formula><p>where a and b are weight coefficients for the sequential item graph and the semantic item graph, respectively. Intuitively, the weights can be regarded as unnormalized probabilities of transition from an item to its sequential/semantic neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Convolution for Embedding Smoothing</head><p>We perform graph convolutions on the hybrid item graph to obtain smoothed item embeddings. Specifically, using GCN <ref type="bibr" target="#b5">[6]</ref> for embedding smoothing could be formulated as</p><formula xml:id="formula_6">V ðkÞ ¼ sð ÂV ðkÀ1Þ W ðkÞ Þ;<label>(4)</label></formula><p>where V ðkÞ 2 R jIjÂd and W ðkÞ 2 R dÂd are the hidden representations of items and the trainable parameter matrix in the kth layer, and Â is the adjacency matrix of the item graph after symmetric normalization.</p><p>Although GCN obtains superior performances on graphbased semi-supervised classification problems, it may be unsuitable for embedding smoothing in recommendation problems <ref type="bibr" target="#b9">[10]</ref>. In semi-supervised node classification, nodes have rich features as the input of GCN, e.g., words in articles, such that the multi-layer nonlinear transformation is helpful to improve the model expressivity. However, items are represented by trainable ID embeddings in recommendation models. Hence, performing multi-layer nonlinear transformations may affect the learning of ID embeddings, leading to poor recommendation performances.</p><p>Inspired by <ref type="bibr" target="#b10">[11]</ref>, we consider removing the nonlinear transformation in GCN layers and perform a simple graph convolution to smooth the embeddings in sequential recommendation models as</p><formula xml:id="formula_7">V ðkÞ ¼ ÂV ðkÀ1Þ ;<label>(5)</label></formula><p>where V ð0Þ is the trainable ID embeddings for items. Note that different from the simple graph convolution in <ref type="bibr" target="#b10">[11]</ref> which has a single weight matrix, we do not apply any transformation to item embeddings. Thus, the graph convolution acts as a weighted mean filter to denoise the item representations according to the graph structure. The hidden representation matrix in the last layer V ðKÞ is used as the input of sequential models.</p><p>It is worth mentioning that we could also adopt layer aggregation strategies to alleviate the over-smoothing problem of deep graph convolutional networks <ref type="bibr" target="#b11">[12]</ref>, e.g., sumpooling or concatenation. We will compare their performances in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Analysis</head><p>We can draw some insights into how graph convolutions lead to embedding smoothing. Here we take the one-layer simple graph convolution as an instance. The weights in the adjacency matrix are set to 1 for an easy analysis. For item i, its representation after graph convolution is</p><formula xml:id="formula_8">v ð1Þ i ¼ X m2N þ i 1 ffiffiffiffiffiffiffiffiffiffi ffi jN þ i j q ffiffiffiffiffiffiffiffiffiffiffi jN þ m j q v<label>ð0Þ</label></formula><formula xml:id="formula_9">m ;<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">N þ i ¼ N i [ fI i g.</formula><p>Then, the similarity score between item i and item j is calculated by dot-product (consistent with the predictive model in recommendation) as</p><formula xml:id="formula_11">v ð1Þ T i v ð1Þ j ¼ w ij X m2N þ i X n2N þ j 1 ffiffiffiffiffiffiffiffiffiffiffi jN þ m j q ffiffiffiffiffiffiffiffiffiffi ffi jN þ n j q v ð0Þ T m v ð0Þ n ¼ w ij X m2N i X n2N j 1 ffiffiffiffiffiffiffiffiffiffi ffi jN þ m j q ffiffiffiffiffiffiffiffiffiffi ffi jN þ n j q v ð0Þ T m v ð0Þ n þ 1 ffiffiffiffiffiffiffiffiffiffi ffi jN þ i j q ffiffiffiffiffiffiffiffiffiffi ffi jN þ j j q v ð0Þ T i v ð0Þ j þ X m2N i 1 ffiffiffiffiffiffiffiffiffiffi ffi jN þ m j q ffiffiffiffiffiffiffiffiffiffi ffi jN þ j j q v ð0Þ T m v<label>ð0Þ</label></formula><formula xml:id="formula_12">j þ X n2N j 1 ffiffiffiffiffiffiffiffiffiffi ffi jN þ n j q ffiffiffiffiffiffiffiffiffiffi ffi jN þ i j q v ð0Þ T n v<label>ð0Þ</label></formula><formula xml:id="formula_13">i :<label>(7)</label></formula><p>where</p><formula xml:id="formula_14">w ij ¼ 1 ffiffiffiffiffiffiffiffi jN þ i j p ffiffiffiffiffiffiffiffi jN þ j j p .</formula><p>The similarity between two items can be divided into four parts: the first part is the neighborto-neighbor similarity, which has jN i jjN j j terms; the second part is the node-to-node similarity; and the third and fourth parts are the node-to-neighbor similarity with jN i j þ jN j j terms. Therefore, the similarity between two items after the graph convolution mainly depends on the neighbor-toneighbor similarity. That is to say, the more similar their neighborhood structures are, the higher their similarity of representation vectors would be. To conclude, embedding smoothing through graph convolutions is mainly based on the second-order proximity <ref type="bibr" target="#b12">[13]</ref>.</p><p>In addition, from the perspective of graph signal processing, <ref type="bibr" target="#b10">[11]</ref> proves that the simple graph convolution with selfloops shrinks the graph spectral domain, resulting in a lowpass filter that captures low-frequency signals, corresponding to smooth features across the graph. As a result, nearby nodes tend to share similar representations after the simple graph convolution.</p><p>Applying graph convolution-based embedding smoothing to sequential recommendation models can be considered as predicting the next item i tþ1 with both the past items fi t ; i tÀ1 ; Á Á Ág and the sequentially or semantically related items fi c : c 2 N i t [ N i tÀ1 [ Á Á Ág. It relaxes the strict sequential dependency in sequential models and enhances their generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DISCUSSIONS</head><p>In this section, we discuss the connections between our proposed method and related efforts. We then analyze the time complexity of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relationship With Graph Laplacian Regularization</head><p>Graph Laplacian Regularization (GLR) <ref type="bibr" target="#b13">[14]</ref> is widely used for graph-based semi-supervised node classification problems</p><formula xml:id="formula_15">L ¼ X i lðy i ; fðx i ÞÞ þ X ði;jÞ2E a ij jjfðx i Þ À fðx j Þjj 2 ; (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where the first term is the supervised loss, and the second term is the graph Laplacian regularization that performs label propagation across the graph, assuming that connected nodes share similar labels. denotes the weight coefficient.</p><p>Semi-supervised embedding <ref type="bibr" target="#b14">[15]</ref> extends the regularization term from the prediction layer to the embedding layer. Inspired by Laplacian eigenmaps <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> proposes Graph Regularized Matrix Factorization (GRMF). The formulation of GRMF with item graph regularization can be written as</p><formula xml:id="formula_17">L ¼ X ði;jÞ lðy ij ; u T i v j Þ þ X ðm;nÞ2E a mn jjv m À v n jj 2 ¼ X ði;jÞ lðy ij ; u T i v j Þ þ trðV T LVÞ;<label>(9)</label></formula><p>where u i and v j are representation vectors for user i and item j respectively, trðÁÞ is the trace of a matrix, and L ¼ D À A is the unnormalized Laplacian matrix. GRMF can be regarded as embedding propagation on the graph, assuming that nearby items tend to share similar representations. Performing graph Laplacian regularization can also smooth the item embeddings according to the item relations, but it has the following disadvantages compared with graph convolutions:</p><p>First, graph Laplacian regularization is built on the firstorder proximity, i.e., whether there is a relation between two items. However, observed item relations in real-world datasets are often sparse and many similar items may not be explicitly linked <ref type="bibr" target="#b12">[13]</ref>. Therefore, considering the similarity of neighborhood structure (i.e., the second-order proximity) can highly enrich the relations of items and is more efficient for embedding smoothing.</p><p>Second, graph Laplacian regularization is based on the semi-supervised learning framework, and its performance could be highly sensitive to the weight . Too small or too large regularization would result in under-smoothing or over-smoothing, hurting recommendation performance. While the graph convolution uses the normalized propagation matrix and its performance is less sensitive to the weight that balances the information from the node itself and from its neighbors. <ref type="bibr" target="#b17">[18]</ref> uses the Gated Graph Neural Network (GGNN) <ref type="bibr" target="#b18">[19]</ref> to model the local dependency in session-based recommendation, which is highly related to our work. The session sequences are gathered to construct a global graph, and each session sequence is modeled as a subgraph that extracted from the global graph. Then GGNN is applied to these subgraphs, which can be formulated as <ref type="bibr" target="#b19">[20]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Relationship With SR-GNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SR-GNN</head><formula xml:id="formula_18">a in t ¼ A in t ð½v 1 ; . . . ; v n W in þ b in Þ;<label>(10)</label></formula><formula xml:id="formula_19">a out t ¼ A out t ð½v 1 ; . . . ; v n W out þ b out Þ; (<label>11</label></formula><formula xml:id="formula_20">)</formula><formula xml:id="formula_21">a t ¼ ½a in t ; a out t ;<label>(12)</label></formula><formula xml:id="formula_22">h t ¼ GRUða t ; v tÀ1 Þ;<label>(13)</label></formula><p>where A in and A out are the adjacency matrices that represent incoming and outgoing edges in the subgraph of a session, and W and b are the trainable transformation matrix and the bias vector.</p><p>It is worth mentioning that our proposed method differs from SR-GNN in two aspects:</p><p>First, SR-GNN only considers how items within a session communicate with each other according to the global graph. That is, related items not interacted in the session are excluded. On the contrary, our proposed method performs embedding smoothing on the global graph. For a user's interaction sequence, our method also considers the related items that the user has not interacted with but other users have.</p><p>Second, our proposed method is a general framework. It can take advantage of different item graphs to improve recommendation performance, such as the sequential item graph from user-item interactions, the semantic item graph from item attributes, or the hybrid item graph. More importantly, this framework can be applied to a variety of sequential models for improving their performances, such as models based on Markov Chain, RNN, CNN, or self-attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Time Complexity Analysis</head><p>Since the proposed embedding smoothing method is a plugin module for sequential recommendation models, here we only analyze the time complexity of the graph convolution in it. By using sparse matrices, the original form of GCN takes OðjEjdK þ jIjd 2 KÞ <ref type="bibr" target="#b20">[21]</ref>, where jEj is the number of edges in the item graph, jIj is the number of items, d is the embedding size, and K denotes the depth of GCN. By removing the unnecessary nonlinear transformations, embedding smoothing with the simple graph convolution takes OðjEjdKÞ. In addition, if the layer aggregation strategies are not used , the propagation matrix ÂK can be precomputed <ref type="bibr" target="#b10">[11]</ref>, and the time complexity further reduces to OðjEjdÞ. We consider that the additional time cost is acceptable for significant improvements on model performance. We can also adopt neighborhood sampling strategies to further reduce the time cost of the graph convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, data experiments were conducted to verify the effectiveness of the proposed method. The codes have been published on GitHub 1 . The experiments were designed to answer the following research questions: RQ1. Can the proposed embedding smoothing method improve the performance of SASRec?</p><p>RQ2. Can the proposed GES-SASRec outperform the stateof-the-art recommendation methods?</p><p>RQ3. How do the hyperparameters and setups affect the performance of GES-SASRec?</p><p>RQ4. How do SASRec and GES-SASRec perform under the shuffled user interaction orders? RQ5. Can the proposed embedding smoothing method apply to other categories of sequential recommendation models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We evaluated the proposed method on three real-world datasets with different domains and sparsity. The statistics of the datasets are summarized in Table <ref type="table" target="#tab_1">1</ref>. Amazon Books. 2 This is a product-review dataset crawled from Amazon.com, a well-known online shopping platform <ref type="bibr" target="#b21">[22]</ref>. The entire data is split into several subsets according to the top-level categories. Here the largest category, i.e., the "Books" category, was adopted. We used the data within 2013 and retained the items/users with at least 20/10 records. The "also-view", "also-buy", "buy after viewing" and "buy together" relations were treated as semantic item relations.</p><p>Yelp. 3 This dataset is a subset of the businesses, reviews, and user data in Yelp.com, a large location-based social platform. Here we used the review and business data within 2014-2016 in Yelp Challenge 2019 and retained the items/ users with at least 20/10 records. The nearest at most 20 businesses in the same city were regarded as semantically related items for each business.</p><p>Google Local. 4 This is a POI-based dataset crawled from Google Local, which contains user reviews and POI data <ref type="bibr" target="#b22">[23]</ref>. The datasets were preprocessed by retaining the items/ users with at least 20/10 records. The nearest (truncated by a distance threshold) at most 20 POIs were treated as semantically related items for each POI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>Evaluation Protocols. We adopted the leave-one-out scheme to evaluate the performances of the models on sequential recommendation, which has been widely used in literature <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>. Given a user's interaction sequence S ðuÞ ¼ ði ðuÞ 1 ; i ðuÞ 2 ; . . . ; i ðuÞ n u Þ, the last item i ðuÞ n u and the next-to-last item i ðuÞ nuÀ1</p><p>were used for test and validation respectively, and the remaining items ði ðuÞ 1 ; i ðuÞ 2 ; . . . ; i ðuÞ n u À2 Þ were for training. For time-saving evaluation, we followed a strategy that 999 items which have not been interacted by a user were randomly sampled to be ranked along with the ground-truth item by the models. The performance of a ranking list was judged by Hit Ratio (HR), Normalized Discounted Cumulative Gain (NDCG), and Mean Reciprocal Rank (MRR)</p><formula xml:id="formula_23">HR@N ¼ 1 jUj X u2U</formula><p>1ðr u;g u NÞ;</p><formula xml:id="formula_24">NDCG@N ¼ 1 jUj X u2U 1ðr u;g u NÞ log 2 ðr u;g u þ 1Þ ; (<label>(14)</label></formula><formula xml:id="formula_25">) MRR@N ¼ 1 jUj X u2U<label>15</label></formula><p>1ðr u;gu NÞ r u;gu ;</p><p>where g u is the ground-truth item for user u, r u;gu is the rank generated by a recommendation method for user u and item g u , and 1ðÁÞ is an indicator function. Without special mention, the ranking list was truncated at 10 for all metrics. As such, HR@10 intuitively measures whether the ground-truth item is presented on the top-10 list, while NDCG@10 and MRR@10 account for the position of the hit by assigning higher scores to hits at top ranks. The average scores of all users for three metrics were reported. Due to space limitation, we only show the performances of NDCG@10 in some figures. Baselines. To verify the effectiveness of GES-SASRec, we compare its performance with several groups of baselines. The first group contains general recommendation methods, which only use the user-item interaction data for item recommendation:</p><p>Most Popular. It is a non-personalized method that simply ranks items according to their popularity, i.e., the number of interactions by all users. BPR <ref type="bibr" target="#b24">[25]</ref>. It is an MF-based model with a pairwise ranking loss to learn from implicit feedback. Mult-DAE <ref type="bibr" target="#b25">[26]</ref>. It is a state-of-the-art item-based recommendation model built upon denoising autoencoders with multinomial conditional likelihood. LightGCN <ref type="bibr" target="#b9">[10]</ref>. It is a state-of-the-art graph convolution-based recommendation model that learns user/ item embedding by linear propagation on the useritem bipartite graph. The second group includes sequential recommendation models that capture user dynamic preferences: FPMC <ref type="bibr" target="#b26">[27]</ref>. It combines matrix factorization and factorized first-order Markov chains for sequential recommendation. CKE <ref type="bibr" target="#b28">[29]</ref>. It is an embedding-based model that combines structural, textual, and visual information in an MF-based framework. Here only the structural information was used. MCF <ref type="bibr" target="#b29">[30]</ref>. It is a relation-aware recommendation model that simultaneously factorizes the user-item rating matrix and the item-item relation matrix for rating prediction. LightGCN+. It performs LightGCN <ref type="bibr" target="#b9">[10]</ref> on the heterogeneous graph that consists of user-item interactions and item-item semantic relations to generate user and item embeddings for recommendation. The fourth group includes a recommendation model that simultaneously encodes the sequential and semantic item relations for recommendation:</p><p>MoHR <ref type="bibr" target="#b23">[24]</ref>. It adopts the translational operators to model the mixture of heterogeneous item-item relations under a multi-task learning framework. Hyperparameter Settings. We implemented the compared methods with TensorFlow. For common hyperparameters in all learning-based methods, the embedding size d was set to 64 with the random normal initializer (with a mean of 0 and standard deviation of 0.01), the l 2 regularization was tuned in {0, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3}, and the learning rate was tuned in {1e-2, 5e-3, 2e-3, 1e-3, 5e-4, 2e-4, 1e-4}. All the learning-based methods were trained with mini-batch Adam <ref type="bibr" target="#b30">[31]</ref>, in the batch size of 256. The results of baseline methods under their optimal hyperparameter settings were reported. For GRU4Rec, NARM, SASRec, and GES-SASRec, we set the maximum sequence length to 50. For GES-SAS-Rec, the number of graph convolutional layers is tuned in f1; 2; 3g.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance Comparison With SASRec (RQ1)</head><p>We report the performances of SASRec and GES-SASRec with 1-3 graph convolutional layers on the sequential/ semantic/hybrid item graph in Table <ref type="table" target="#tab_2">2</ref>. We further plot their learning curves (test performance of NDCG@10) in Fig. <ref type="figure" target="#fig_2">3</ref> to show the benefits of the proposed embedding smoothing method. Note that the optimal learning rate may be different for SASRec and GES-SASRec with different item graphs. For comparison, we use 0.001 in Fig. <ref type="figure" target="#fig_2">3</ref>. We conclude the findings as follows:</p><p>First, the effectiveness of embedding smoothing with sequential item graphs may rely on the strength of sequentiality in user interactions. For the high sequentiality dataset (Amazon Books), embedding smoothing with sequential item graphs may cause over-smoothing and harm the performance of SASRec. While for the medium and low sequentiality datasets (Yelp and Google Local), it could benefit recommendation performance.</p><p>Second, the effectiveness of embedding smoothing with semantic item graphs may depend on the reliability of semantic item relations and the sparsity of the dataset. As can be seen, embedding smoothing with semantic item graphs is more effective for Amazon Books and Yelp.</p><p>Third, using hybrid item graphs for embedding smoothing could further improve recommendation performance and achieves the best results on all datasets. This may be because the hybrid item graph is a balanced combination of the information from user behavior and from item attributes.</p><p>Fourth, the performances of GES-SASRec do not necessarily improve with more graph convolutional layers. This may be because too many graph convolutional layers would lead to over-smoothing for item embeddings, hurting the HR@10 NDCG@10 MRR@10 HR@10 NDCG@10 MRR@10 HR@10 NDCG@10 MRR@10 recommendation performance. Generally speaking, a 1-2 graph convolutional layer may be more suitable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Performance Comparison With</head><p>State-of-the-Arts (RQ2)</p><p>Table <ref type="table">3</ref> shows the performance comparison with baseline methods. We have the following main observations: First, the strength of sequentiality in user interactions may affect the performances of the methods from different categories. For the high sequentiality dataset (Amazon Books), the best sequential recommendation model SASRec outperforms the best general recommendation model Mult-DAE significantly. While for the low sequentiality dataset (Yelp), SASRec achieves much worse performance than Mult-DAE. For the medium sequentiality dataset (Google Local), the performances of SASRec and Mult-DAE are comparable.</p><p>Second, semantic item relations are significantly beneficial to the performances of recommendation methods. For MF-based methods, MCF outperforms BPR significantly. For graph convolution-based methods, LightGCN+ obtains better results than LightGCN.</p><p>Third, GES-SASRec outperforms other baselines on Amazon Books and Google Local but does not perform best on Yelp. That is, GES-SASRec shows stronger recommendation performance on high sequentiality datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyperparameter and Ablation Analyses (RQ3)</head><p>We perform detailed analyses on GES-SASRec to show how key hyperparameters and layer aggregation strategies affect its performance. We also investigate the performance of SASRec with different embedding smoothing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Effect of Embedding Size</head><p>Fig. <ref type="figure" target="#fig_3">4</ref> shows the performances of the compared methods w. r.t. the embedding size d. From a macro perspective, with the increase of embedding size, the performances of all models improve due to stronger expressivity. However, the marginal improvement is decreasing, and overfitting may occur with large embedding sizes for some models. Specifically, GES-SASRec achieves better results than SASRec under different embedding sizes on all datasets. In addition, GES-SASRec significantly outperforms the baseline methods on higher sequentiality datasets (Amazon Books and Google Local), especially when the embedding size is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Effect of Relation Coefficients</head><p>Fig. <ref type="figure">5</ref> shows the results of GES-SASRec w.r.t. the sequential/semantic relation coefficient a/b. Generally, both item relations are significant for the performance of GES-SASRec. When a and b are close to each other, the model performs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3 Performance Comparison With Embedding Size 64</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Information Amazon Books Yelp Google Local HR@10 NDCG@10 MRR@10 HR@10 NDCG@10 MRR@10 HR@10 NDCG@10 MRR@10 well. Note that the optimal ratio of a to b depends on datasets. For Amazon Books, the optimal ratio of a to b is about 0.5, that is, the optimal weight of semantic relations is twice that of sequential relations. While for Yelp and Google Local, the optimal ratio is about 2, that is, the optimal weight of sequential relations is twice that of semantic relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.3">Effect of Layer Aggregation</head><p>Fig. <ref type="figure">6</ref> shows the performances of GES-SASRec with different layer-aggregation functions. As can be seen, the performance of GES-SASRec without layer aggregations drops when the number of layers increases from 1-2 to 3, indicating that the over-smoothing problem occurs. While using layer aggregations could effectively avoid this problem, as the model performance slightly improves with the increase of layers in most cases. Moreover, GES-SASRec using 1 or 2 layers without layer aggregations achieves better performances than using layer aggregations with 3 layers in most cases. That is, using 1-2 graph convolutional layers without layer aggregations may achieve more suitable smoothness for item embeddings in recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.4">Effect of Embedding Smoothing Approach</head><p>Table <ref type="table" target="#tab_5">4</ref> shows the performances of SASRec with different embedding smoothing methods on the hybrid item graphs, including Graph Laplacian Regularization <ref type="bibr" target="#b13">[14]</ref> (GLR-SAS-Rec), Graph Convolutional Network <ref type="bibr" target="#b5">[6]</ref> (GCN-SASRec), and Simple Graph Convolution <ref type="bibr" target="#b10">[11]</ref> (GES-SASRec). We can see that all embedding smoothing methods could improve the performances of SASRec in most cases. Compared with graph Laplacian regularization, graph convolutions seem more effective. Additionally, GES-SASRec outperforms GCN-SASRec in most cases, showing that the redundant complexity of transformations in graph convolutional layers may harm recommendation performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Performance Under shuffled User Interaction Orders (RQ4)</head><p>It is known that the performance of sequential recommendation models will strongly rely on the order of user interactions.</p><p>Here experiments are conducted to show how SASRec performs when the order of user interactions is disrupted and whether the proposed embedding smoothing method could alleviate the performance degradation. In Fig. <ref type="figure">7</ref>, we show the performances of SASRec, GES-SASRec, LightGCN, and LightGCN+ under different shuffle ratios in the test process. The shuffle ratio is defined as the proportion of items whose positions are randomly shuffled in user interaction sequences. We have the following findings: First, the performances of SASRec and GES-SASRec are both sensitive to the order of user interactions, as their recommendation accuracy drops with the increase of the shuffle ratio.</p><p>Second, for Amazon Books, SASRec outperforms LightGCN when the shuffle ratio is lower than 70 percent. GES-SASRec outperforms LightGCN+ even when the shuffle ratio increases to 90 percent. Thus, embedding smoothing could help SASRec go further ahead of LightGCN on recommendation performance on high sequentiality datasets.</p><p>Third, for Yelp, LightGCN performs significantly better than SASRec even when the positions of items are not shuffled, while the performance of GES-SASRec gets closer to LightGCN and LightGCN+ owing to the proposed embedding smoothing method. That is, embedding smoothing could narrow the performance gap between SASRec and LightGCN on low sequentiality datasets.</p><p>Fourth, for Google Local, SASRec achieves comparable performances with LightGCN when the shuffle ratio equals 0, while GES-SASRec outperforms LightGCN+ when the shuffle ratio is lower than 30 percent. Hence, embedding smoothing could help SASRec outperform LightGCN on medium sequentiality datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Embedding Smoothing for Other Models (RQ5)</head><p>We also conduct experiments to show whether the proposed embedding smoothing method could apply to other categories of sequential recommendation models. Specifically, we try to apply the proposed method to other four sequential models, including Markov Chain-based model (TransRec), CNN-based model (Caser), RNN-based model (GRU4Rec), and RNN with attention-based model (NARM). We smooth the item embeddings in these models with the proposed hybrid item graph and show the results in Table <ref type="table" target="#tab_6">5</ref>. As can be seen, the proposed embedding smoothing method could improve the performances of all these sequential recommendation models significantly, especially for item-based methods (GRU4Rec and NARM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this section, we review recent efforts relating to our work, including sequential recommendation models, GNN-based recommendation models, and regularization in recommendation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sequential Recommendation Models</head><p>Efforts on sequential recommendation problems focus on how to capture user dynamic preferences. Early work adopts Markov Chains (MCs) to learn the transition patterns over items. For example, FPMC <ref type="bibr" target="#b26">[27]</ref> combines the Matrix Factorization (MF) with the first-order Markov chain to model both long-term and short-term preference for users. Fossil <ref type="bibr" target="#b31">[32]</ref> fuses the similarity-based model with high-order Markov chains to predict personalized sequential behavior. TransRec [23] adopts the translational operator to model the transitions of items.</p><p>Recently, deep learning-based sequential models (e.g., RNN, CNN, Transformer, etc.) achieve great success in many areas. Thus, some efforts seek to apply these models to sequential recommendation. For example, GRU4Rec <ref type="bibr" target="#b1">[2]</ref> exploits Gated Recurrent Unit (GRU) for session-based recommendation. NARM <ref type="bibr" target="#b27">[28]</ref> adopts GRU with an attention mechanism to capture the global and local preference of users. Caser <ref type="bibr" target="#b2">[3]</ref> considers user recent interactions as an image and uses CNNs with both horizontal and vertical convolutional filters to learn sequential patterns. RUM <ref type="bibr" target="#b32">[33]</ref> designs the item-level and feature-level memory-augmented neural networks to store and update user preferences. SHAN <ref type="bibr" target="#b33">[34]</ref> uses a two-layer hierarchical attention network to couple user's long-term and short-term preference. SASRec <ref type="bibr" target="#b3">[4]</ref> and BERT4Rec <ref type="bibr" target="#b34">[35]</ref> adopt the self-attention blocks in Transformer to encode user's sequential behavior with the left-to-right setting and the bidirectional setting, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">GNN-Based Recommendation Models</head><p>Graph Neural Networks (GNNs) have been widely used in recent efforts to deal with graph-structure data in recommendation models <ref type="bibr" target="#b35">[36]</ref>. Specifically, these graphs fall within four categories: 1) User-item bipartite graph. These models perform embedding propagation on the user-item bipartite graph with different strategies to obtain smoothed user/ item embedding for recommendation, such as GCMC <ref type="bibr" target="#b36">[37]</ref>, SpectralCF <ref type="bibr" target="#b37">[38]</ref>, NGCF <ref type="bibr" target="#b38">[39]</ref>, LR-GCCF <ref type="bibr" target="#b39">[40]</ref>, and LightGCN <ref type="bibr" target="#b9">[10]</ref>. 2) User-user social network. These models apply GNNs to the user social network to model the social influence propagating among users for social recommendation, such as DGRec <ref type="bibr" target="#b40">[41]</ref>, DANSER <ref type="bibr" target="#b41">[42]</ref>, GraphRec <ref type="bibr" target="#b42">[43]</ref>, and Diff-Net <ref type="bibr" target="#b43">[44]</ref>. 3) Item-item graph. These models attempt to leverage item relations from the training data or the side information to regularize the item embeddings for improving recommendation accuracy, such as SR-GNN <ref type="bibr" target="#b17">[18]</ref>, GC-SAN <ref type="bibr" target="#b19">[20]</ref>, and PinSage <ref type="bibr" target="#b44">[45]</ref>. 4) Heterogeneous information network/knowledge graph. These models propagate user/ item embedding in the heterogeneous information network or the knowledge graph to generate knowledge-aware recommendations, such as RippleNet <ref type="bibr" target="#b45">[46]</ref>, KGCN-LS <ref type="bibr" target="#b46">[47]</ref>, and KGAT <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Regularization in Recommendation Models</head><p>Recommendation models often suffer from data sparsity problems, which make them hard to capture user preferences and item features and lead to unsatisfactory performances. To cope with the problems, some recent efforts seek to leverage the graph data to regularize these models by making the users/items that are close in the graph have similar representations. The strategies have two schools of thought: 1) Using graphs generated from the user-item interactions. These efforts construct the user co-purchase or the item co-occurrence relations from user-item interactions, and treat these relations as global features for users/items to regularize recommendation models, such as CoFactor <ref type="bibr" target="#b48">[49]</ref>, RME <ref type="bibr" target="#b49">[50]</ref>, BiNE <ref type="bibr" target="#b50">[51]</ref>, and GRMF <ref type="bibr" target="#b16">[17]</ref>. 2) Using graphs built from the side information. These methods leverage the side information of users/items to construct user/item graphs for regularizing recommendation models <ref type="bibr" target="#b51">[52]</ref>, such as the user-user graph in SoReg <ref type="bibr" target="#b52">[53]</ref> built from user social networks, the item-item graph in CMF <ref type="bibr" target="#b53">[54]</ref> constructed upon item features, and the item-entity graph in MKR <ref type="bibr" target="#b54">[55]</ref> derived from the knowledge graph. Most of these methods follow a multi-task learning framework to regularize the main task (recommendation) with the auxiliary task (relation learning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this study, we have pointed out that the sequentiality in user interactions may vary greatly in different recommendation scenarios, which may seriously affect the performances of sequential recommendation models. Additionally, sequential models only consider sequential item relations in terms of user behavior, while neglecting semantic item relations that are crucial for measuring item similarities in recommendation. To deal with the problems, we have proposed an effective framework to enhance the performances of sequential </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Method Amazon Books Yelp Google Local HR@10 NDCG@10 MRR@10 HR@10 NDCG@10 MRR@10 HR@10 NDCG@10 MRR@10 recommendation models. Our main idea is to smooth the item embeddings in sequential models with item graphs, which consists of two essential steps: generating a hybrid item relation graph and performing graph convolutions. To construct a hybrid item relation graph, we fuse the sequential item relations derived from user-item interactions with the semantic item relations built upon item attributes. Then we perform graph convolutions on the hybrid item graph to generate smoothed item embeddings as the input of sequential recommendation models. We have evaluated the proposed embedding smoothing method with a state-of-the-art sequential recommendation model, SASRec, on three public datasets.</p><p>The experimental results show that the improved SASRec model outperforms all baselines on high/medium sequentiality dataset and achieves better results on low sequentiality dataset. We have also conducted experiments to verify the effectiveness of the proposed embedding smoothing method on different categories of sequential recommendation models. For future work, one possible effort is to explore heterogeneous semantic relations between items (e.g., movies with the same genre/director/actor), so as to design embedding smoothing strategies with these relations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The number of sequential association rules w.r.t. the Markov order L in three real-world datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The framework of graph-based embedding smoothing (GES).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Test performances of SASRec and GES-SASRec w.r.t. the training epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performances of the models w.r.t. the embedding size d.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Performance of GES-SASRec w.r.t. the item relation coefficients a and b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Statistics of the Evaluation Datasets</figDesc><table><row><cell>Statistics</cell><cell>Amazon Books</cell><cell>Yelp</cell><cell>Google Local</cell></row><row><cell>#Users</cell><cell>27,738</cell><cell>32,206</cell><cell>16,381</cell></row><row><cell>#Items</cell><cell>43,790</cell><cell>27,671</cell><cell>34,928</cell></row><row><cell>#Interactions</cell><cell>624,573</cell><cell>726,154</cell><cell>427,910</cell></row><row><cell>Sparsity</cell><cell>0.051%</cell><cell>0.081%</cell><cell>0.075%</cell></row><row><cell>Avg. Length</cell><cell>20.52</cell><cell>20.55</cell><cell>24.12</cell></row><row><cell>#Relations</cell><cell>847,258</cell><cell>268,402</cell><cell>687,397</cell></row></table><note>1. http://github.com/zhuty16/GES 2. http://jmcauley.ucsd.edu/data/amazon/index_2014.html 3. https://www.yelp.com/dataset/ 4. https://cseweb.ucsd.edu/jmcauley/datasets.htmlTransRec<ref type="bibr" target="#b22">[23]</ref>. It models each user as a translation vector between items to capture the transition patterns for sequential recommendation. GRU4Rec<ref type="bibr" target="#b1">[2]</ref>. It adopts GRU to model the session sequences for session-based recommendation. NARM<ref type="bibr" target="#b27">[28]</ref>. It uses GRU with attention mechanism to capture both sequential behavior and main purpose of users for session-based recommendation. Caser<ref type="bibr" target="#b2">[3]</ref>. It adopts horizontal and vertical convolutions to learn sequential patterns for recommendation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Performance of GES-SASRec With Different Item Graphs and Different Numbers of Graph Convolutional Layers</figDesc><table><row><cell>#Layers Item Graph</cell><cell>Amazon Books</cell><cell>Yelp</cell><cell>Google Local</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Performance of SASRec With Different Embedding Smoothing Methods</figDesc><table><row><cell>Method</cell><cell></cell><cell>Amazon Books</cell><cell></cell><cell></cell><cell>Yelp</cell><cell></cell><cell></cell><cell>Google Local</cell><cell></cell></row><row><cell></cell><cell>HR@10</cell><cell>NDCG@10</cell><cell>MRR@10</cell><cell>HR@10</cell><cell>NDCG@10</cell><cell>MRR@10</cell><cell>HR@10</cell><cell>NDCG@10</cell><cell>MRR@10</cell></row><row><cell>SASRec</cell><cell>0.4824</cell><cell>0.3069</cell><cell>0.2528</cell><cell>0.3588</cell><cell>0.1956</cell><cell>0.1461</cell><cell>0.5664</cell><cell>0.3466</cell><cell>0.2792</cell></row><row><cell>GLR-SASRec</cell><cell>0.4901</cell><cell>0.2982</cell><cell>0.2391</cell><cell>0.3708</cell><cell>0.2026</cell><cell>0.1519</cell><cell>0.6080</cell><cell>0.3760</cell><cell>0.3041</cell></row><row><cell>GCN-SASRec</cell><cell>0.5138</cell><cell>0.3296</cell><cell>0.2725</cell><cell>0.3890</cell><cell>0.2169</cell><cell>0.1645</cell><cell>0.6085</cell><cell>0.3901</cell><cell>0.3224</cell></row><row><cell>GES-SASRec</cell><cell>0.5405</cell><cell>0.3553</cell><cell>0.2991</cell><cell>0.3825</cell><cell>0.2128</cell><cell>0.1620</cell><cell>0.6257</cell><cell>0.4031</cell><cell>0.3358</cell></row><row><cell cols="2">Best results are in boldface.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Fig. 7. Performances of SASRec, GES-SASRec, LightGCN and LightGCN+ w.r.t. the shuffle ratio.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Performances of Sequential Recommendation Models With and Without the Proposed Embedding Smoothing Method</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 28,2022 at 06:45:20 UTC from IEEE Xplore. Restrictions apply.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1">IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 35, NO. 1, JANUARY 2023 Authorized licensed use limited to: Tsinghua University. Downloaded on December 28,2022 at 06:45:20 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported in part by the National Natural Science Foundation of China under Grant 71901011 and in part by the MOE Project of Key Research Institute of Humanities and Social Sciences at Universities under Grant 17JJD630006.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sessionbased recommendations with recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hidasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karatzoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baltrunas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tikk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06939</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Personalized top-n sequential recommendation via convolutional sequence embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM Int. Conf. Web Search</title>
				<meeting>11th ACM Int. Conf. Web Search</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Data Mining</title>
				<meeting>Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining sequential patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Data Eng</title>
				<meeting>11th Int. Conf. Data Eng</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural IR meets graph embedding: A ranking model for product search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Wide Web Conf</title>
				<meeting>World Wide Web Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2390" to="2400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">LightGCN: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02126</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H D</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07153</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03536</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Line: Large-scale information network embedding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th Int. Conf. World Wide Web</title>
				<meeting>24th Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Uller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks: Tricks of the Trade. Berlin</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Laplacian Eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collaborative filtering with graph information: Consistency and scalable methods</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2107" to="2115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sessionbased recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
				<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph contextualized self-attention network for session-based recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th Int. Joint Conf</title>
				<meeting>28th Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3940" to="3946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021-01">Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Int. Conf. World Wide Web</title>
				<meeting>25th Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Translation-based recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th ACM Conf. Recommender Syst</title>
				<meeting>11th ACM Conf. Recommender Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="161" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recommendation through mixtures of heterogeneous item relationships</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ACM Int</title>
				<meeting>27th ACM Int</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1143" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th Conf. Uncertainty Artif. Intell</title>
				<meeting>25th Conf. Uncertainty Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Variational autoencoders for collaborative filtering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2018 World Wide Web Conf</title>
				<meeting>2018 World Wide Web Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Factorizing personalized Markov chains for next-basket recommendation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Int. Conf. World Wide Web</title>
				<meeting>19th Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="811" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural attentive session-based recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2017 ACM Conf. Inf. Knowl. Manage</title>
				<meeting>2017 ACM Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collaborative knowledge base embedding for recommender systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</title>
				<meeting>22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Do&quot; also-viewed&quot; products help user rating prediction?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th Int. Conf. World Wide Web</title>
				<meeting>26th Int. Conf. World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fusing similarity models with Markov chains for sparse sequential recommendation</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 16th Int. Conf. Data Mining</title>
				<meeting>IEEE 16th Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="191" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sequential recommendation with user memory networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Acm Int. Conf. Web Search Data Mining</title>
				<meeting>11th Acm Int. Conf. Web Search Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="108" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequential recommender system based on hierarchical attention network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI Int. Joint Conf</title>
				<meeting>IJCAI Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3926" to="3932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 28th ACM Int. Conf. Inf. Knowl. Manage</title>
				<meeting>28th ACM Int. Conf. Inf. Knowl. Manage</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric matrix completion with recurrent multi-graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neural Inf. Process. Syst</title>
				<meeting>Neural Inf. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3697" to="3707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spectral collaborative filtering</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM Conf. Recommender Syst</title>
				<meeting>12th ACM Conf. Recommender Syst</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Int</title>
				<meeting>42nd Int</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Revisiting graph based collaborative filtering: A linear residual graph convolutional network approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf</title>
				<meeting>AAAI Conf</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Session-based social recommendation via dynamic graph attention networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th ACM Int. Conf. Web Search Data Mining</title>
				<meeting>12th ACM Int. Conf. Web Search Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="555" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dual graph attention networks for deep latent representation of multifaceted social effects in recommender systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Wide Web Conf</title>
				<meeting>World Wide Web Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2091" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Wide Web Conf</title>
				<meeting>World Wide Web Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A neural influence diffusion model for social recommendation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 42nd Int</title>
				<meeting>42nd Int</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</title>
				<meeting>24th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">RippleNet: Propagating user preferences on the knowledge graph for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ACM Int</title>
				<meeting>27th ACM Int</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Knowledge-aware graph neural networks with label smoothness regularization for recommender systems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</title>
				<meeting>25th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">KGAT: Knowledge graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</title>
				<meeting>25th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Factorization meets the item embedding: Regularizing matrix factorization with item co-occurrence</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ACM Conf. Recommender Syst</title>
				<meeting>10th ACM Conf. Recommender Syst</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="59" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Regularizing matrix factorization with user and item embeddings for recommendation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th ACM Int</title>
				<meeting>27th ACM Int</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bine: Bipartite network embedding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 41st Int</title>
				<meeting>41st Int</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="715" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bridging collaborative filtering and semi-supervised learning: A neural approach for poi recommendation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd ACM SIGKDD Inte. Conf. Knowl. Discov. Data Mining</title>
				<meeting>23rd ACM SIGKDD Inte. Conf. Knowl. Discov. Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1245" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recommender systems with social regularization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Int. Conf. Web Search. Data Mining</title>
				<meeting>4th ACM Int. Conf. Web Search. Data Mining</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="287" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Relational learning via collective matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</title>
				<meeting>14th ACM SIGKDD Int. Conf. Knowl. Discov. Data Mining</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multitask feature learning for knowledge graph enhanced recommendation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. World Wide Web Conf</title>
				<meeting>World Wide Web Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2000" to="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">His research interests include machine learning, data mining, and recommender systems</title>
	</analytic>
	<monogr>
		<title level="m">Tianyu Zhu is currently working toward the PhD with the of Management Science and Engineering</title>
				<meeting><address><addrLine>Dalian, China; Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Tsinghua University, Beijing, China ; Dalian University of Technology ; School of Computer Science, Beihang University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include machine learning and data mining</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">His research interests include data mining and business analytics, recommender systems, information extraction, fuzzy logic, and data modeling</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Belgium; Beijing, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Management Science and Engineering, Tsinghua University</orgName>
		</respStmt>
	</monogr>
	<note>For more information on this or any other computing topic. please visit our Digital Library at www.computer.org/csdl</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
