<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">What Makes Convolutional Models Great on Long Sequence Modeling?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-18">October 18, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuhong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<country>Champaign</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<country>Champaign</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Debadeepta</forename><surname>Dey</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">What Makes Convolutional Models Great on Long Sequence Modeling?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-18">October 18, 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.09298v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional models have been widely used in multiple domains. However, most existing models only use local convolution, making the model unable to handle long-range dependency efficiently. Attention overcomes this problem by aggregating global information based on the pair-wise attention score but also makes the computational complexity quadratic to the sequence length. Recently, Gu et al. [2021a]  proposed a model called S4 inspired by the state space model. S4 can be efficiently implemented as a global convolutional model whose kernel size equals the input sequence length. With Fast Fourier Transform, S4 can model much longer sequences than Transformers and achieve significant gains over SoTA on several long-range tasks. Despite its empirical success, S4 is involved. It requires sophisticated parameterization and initialization schemes that combine the wisdom from several prior works. As a result, S4 is less intuitive and hard to use for researchers with limited prior knowledge. Here we aim to demystify S4 and extract basic principles that contribute to the success of S4 as a global convolutional model. We focus on the structure of the convolution kernel and identify two critical but intuitive principles enjoyed by S4 that are sufficient to make up an effective global convolutional model: 1) The parameterization of the convolutional kernel needs to be efficient in the sense that the number of parameters should scale sub-linearly with sequence length. 2) The kernel needs to satisfy a decaying structure that the weights for convolving with closer neighbors are larger than the more distant ones. Based on the two principles, we propose a simple yet effective convolutional model called Structured Global Convolution (SGConv). SGConv exhibits strong empirical performance over several tasks: 1) With faster speed, SGConv surpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging SGConv into standard language and vision models, it shows the potential to improve both efficiency and performance. Code is available at https://github.com/ctlllll/SGConv.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Handling Long-Range Dependency (LRD) is a key challenge in long-sequence modeling tasks such as time-series forecasting, language modeling, and pixel-level image generation. Unfortunately, standard deep learning models fail to solve this problem for different reasons: Recurrent Neural Network (RNN) suffers from vanishing gradient, Transformer has complexity quadratic in the sequence length, and Convolutional Neural Network (CNN) usually only has a local receptive field in each layer.</p><p>A recently proposed benchmark called Long-Range Arena (LRA) <ref type="bibr">[Tay et al., 2020b</ref>] reveals that all existing models perform poorly in modeling LRD. Notably, on one spatial-level sequence modeling task called Pathfinder-X from LRA, all models fail except a new Structured State Space sequence model (S4) <ref type="bibr">[Gu et al., 2021a]</ref>. The S4 model is inspired by the state space model widely used in control theory and can be computed efficiently with a special parameterization based on the Cauchy kernel. The exact implementation of the S4 model can be viewed as a (depthwise) global convolutional model with an involved computation global convolution kernel. Thanks to the global receptive field of the convolution kernel, S4 is able to handle tasks that require LRD, such as Pathfinder <ref type="bibr">[Tay et al., 2020b]</ref>, where classic local CNNs fail <ref type="bibr" target="#b25">[Linsley et al., 2018</ref><ref type="bibr" target="#b23">, Kim et al., 2019]</ref>. Also, the use of Fast Fourier Transform (FFT) and techniques from numerical linear algebra make the computational complexity of S4 tractable compared to the quadratic complexity of attention. Together, S4 shows the potential of global convolutional models to model LRD and advances the SoTA on LRA.</p><p>Despite its accomplishments, the delicate design of S4 makes it unfriendly even to knowledgable researchers. In particular, the empirical success of S4 relies on 1) A Diagonal Plus Low Rank (DLPR) parameterization whose efficient implementation requires several numerical linear algebra tricks, 2) An initialization scheme based on the HiPPO matrix derived in prior work <ref type="bibr" target="#b13">[Gu et al., 2020]</ref>. Therefore, aiming to reduce the complications of the model and highlight minimal principles, we raise the following questions:</p><p>What contributes to the success of the S4 model? Can we establish a simpler model based on minimal principles to handle long-range dependency?</p><p>To answer these questions, we focus on the design of the global convolution kernel. We extract two simple and intuitive principles that contribute to the success of the S4 kernel. The first principle is that the parameterization of the global convolution kernel should be efficient in terms of the sequence length: the number of parameters should scale slowly with the sequence length. For example, classic CNNs use a fixed kernel size. S4 also uses a fixed number of parameters to compute the convolution kernel while the number is greater than classic CNNs. Both models satisfy the first principle as the number of parameters does not scale with input length. The efficiency of parameterization is also necessary because the naive implementation of a global convolution kernel with the size of sentence length is intractable for inputs with thousands of tokens. Too many parameters will also cause overfitting, thus hurting the performance. The second principle is the decaying structure of the convolution kernel, meaning that the weights for convolving with closer neighbors are larger than the more distant ones. This structure appears ubiquitously in signal processing, with the well-known Gaussian filter as an example.</p><p>The intuition is clear that closer neighbors provide a more helpful signal. S4 inherently enjoys this decaying property because of the exponential decay of the spectrum of matrix powers (See Figure <ref type="figure" target="#fig_0">2</ref>), and we find this inductive bias improves the model performance (See Section 4.1.2).</p><p>Figure <ref type="figure">1</ref>: Illustration of the parameterization used in SGConv (Eq. ( <ref type="formula" target="#formula_3">1</ref>)). The convolution kernel is composed of multi-scale sub-kernels. Parameterization Efficiency. Every larger sub-kernel doubles the size of the previous sub-kernel while the same number of parameters are used for every scale, ensuring a logarithmic dependency of the number of parameters to the input length. Decaying. We use a weighted combination of sub-kernels where the weights are decaying, and smaller weights are assigned to larger scales.</p><p>We show that these two principles are sufficient for designing a global convolutional model that captures LRD well. To verify this, we introduce a class of global convolution kernels with a simple multiscale structure, as shown in Figure <ref type="figure">1</ref>. Specifically, we compose the convolution kernel by a sequence of subkernels of increasing sizes, yet every subkernel is upsampled from the same number of parameters. This parameterization ensures that the number of parameters only scales logarithmically to the input length, which satisfies the first principle. In addition, we add a decaying weight to each scale during the combination step and fulfill the second principle. We named our methods as Structural Global Convolution kernels (SGConv). Empirically, SGConv improves S4 by more than 1% and achieves SoTA results on the LRA benchmark. On Speech Command datasets, SGConv achieves comparative results in the ten-class classification task and significantly better results in the 35-class classification task upon previous SoTA. We further show that SGConv is more efficient than S4 and can be used as a general purpose module in different domains. For example, a hybrid model of classic attention and SGConv shows promising performance on both autoregressive language modeling and sentence classification tasks. Moreover, on a large-scale image classification task, replacing the 2D convolution kernel of the ConvNext model with 1D SGConv matches the performance of the original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Efficient Transformers. The Transformer architecture <ref type="bibr" target="#b43">[Vaswani et al., 2017]</ref> has been successful across a wide range of applications in machine learning. However, the computation and memory complexity of Transformer scales quadratically with the input length, making it intractable for modeling long-range interactions in very long sequences. Therefore, several efficient variants of Transformer model have been proposed recently to overcome this issue <ref type="bibr" target="#b4">[Child et al., 2019</ref><ref type="bibr">, Wang et al., 2020</ref><ref type="bibr" target="#b24">, Kitaev et al., 2019</ref><ref type="bibr" target="#b48">, Zaheer et al., 2020</ref><ref type="bibr">, Tay et al., 2020a</ref><ref type="bibr">, Peng et al., 2021</ref><ref type="bibr" target="#b33">, Qin et al., 2021</ref><ref type="bibr" target="#b28">, Luo et al., 2021]</ref>. Nevertheless, few of these methods performed well on benchmarks such as Long Range Arena <ref type="bibr">[Tay et al., 2020b]</ref>, SCROLLS <ref type="bibr" target="#b37">[Shaham et al., 2022]</ref>, which require long-range modeling ability.</p><p>(Re-)parameterization. Parameterization is a crucial but underrated part of architecture design because different parameterizations usually provide different inductive biases. For example, weight normalization <ref type="bibr" target="#b36">[Salimans and Kingma, 2016]</ref> parameterizes the norm and direction of the weight matrices separately and thus reaches faster convergence. On the other hand, Zagoruyko and Komodakis <ref type="bibr">[2017]</ref> proposed a Dirac weight re-parameterization to train deep networks without explicit skip-connections and matched the performance of ResNets <ref type="bibr" target="#b19">[He et al., 2016]</ref>. In computer vision, a line of works <ref type="bibr" target="#b8">[Ding et al., 2019</ref><ref type="bibr" target="#b18">, Guo et al., 2020</ref><ref type="bibr" target="#b9">, Ding et al., 2021</ref><ref type="bibr" target="#b1">, Cao et al., 2022]</ref> explored using structural re-parameterization to create 2D convolution kernels. However, most of these works are limited to the vision domain and utilize only short-range convolution kernels (e.g., 7 ? 7) with only one exception <ref type="bibr" target="#b10">[Ding et al., 2022]</ref>, which scales the size of convolution to 31 ? 31 with an optimized CUDA kernel. Our SGConv kernel is a special parameterization of global convolution kernels that tackles LRD and showcases the extensibility of re-parameterized kernels.</p><p>State Space Models. The state space model (SSM) uses a set of linear differential equations to model physical systems with input, output, and state variables. It is widely used in control, neuroscience, and statistics. Recently, <ref type="bibr">Gu et al. [2021b]</ref> introduced a deep SSMbased model that can outperform prior approaches on several long sequence modeling tasks with a specially structured state transition matrix. However, the expensive computation and memory requirements make it impractical. A followup work of <ref type="bibr">Gu et al. [2021b]</ref> proposed a new parameterization of SSM <ref type="bibr">[Gu et al., 2021a]</ref>, which decomposes the state transition matrix into the sum of low-rank and normal matrices and implements SSM as a global convolutional model. Under this parameterization, the authors then combine the techniques of diagonalizing the Cauchy kernel and performing low-rank corrections with the Woodbury identity to compute the global convolution kernel. While achieving promising results, S4 is theoretically involved and practical implementations of S4 require accelerator-specific dedicated code optimization for the Cauchy kernel computation. This makes it difficult to readily implement in deep learning frameworks <ref type="bibr" target="#b0">[Abadi et al., 2016</ref><ref type="bibr" target="#b3">, Chen et al., 2015</ref><ref type="bibr" target="#b2">, Chen, 2021</ref><ref type="bibr" target="#b30">, Ma et al., 2019]</ref> and hardware targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Design of Global Convolutional Models</head><p>We summarize the design principles that enable the global convolutional model to be both efficient and effective. Then we introduce the proposed Structured Global Convolution (SGConv) based on the highlighted principles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Design Principles</head><p>The two intuitive design principles that contribute to the success of S4 are efficient parameterization and decaying structure. The values in the convolution kernel exhibit a decaying behavior. We only plot the first 4096 positions for better illustration.</p><p>Efficient Parameterization. Different from local convolution, where the kernel size is fixed, global convolution requires a kernel size that is the same as the sentence length. Naive parameterization of convolution kernel as classic local convolutions is therefore intractable for long sequences. For instance, the Pathfinder-X task has a length of 16K. It then impractically requires 4M parameters for a single layer to model the depth-wise global convolution kernel with a standard channel size of 256. Thus, an efficient convolution kernel parameterization is necessary, especially when the sentence is extremely long. For example, S4 takes a well-designed Normal Plus Low-Rank (NPLR) parameterization to model the whole kernel with two special matrices where the number of parameters is fixed.</p><p>Decaying Structure. Apart from the efficiency of the parameterization, we find that a decaying structure of the convolution kernel provides a good inductive bias to long-sequence modeling and contributes to the performance (See Section 4.1.2 for detailed ablation study). Concretely, the magnitude of the value in the convolution kernel should decay so that more weight is assigned to the close neighbors. S4 model inherently satisfies this property because the spectrum of the power of a matrix decays exponentially:</p><p>Fact 1. For a square matrix A, the spectral radius</p><formula xml:id="formula_0">?(A k ) ? ?(A) k . In particular, if ?(A) &lt; 1, ?(A k ) decays exponential to k.</formula><p>We can also directly observe the decaying structure of S4 in different tasks in Figure <ref type="figure" target="#fig_0">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SGConv</head><p>Putting the two principles altogether, we propose a simple global depth-wise convolution, dubbed Structured Global Convolution (SGConv), based on multiscale sub-kernels and weighted combinations. (See Figure <ref type="figure">1</ref>). Formally, let L be the length of the input sequence. We define the parameter set of a single channel as + 1. We use the upsample operation, implemented as linear interpolation, to form sub-kernels of different scales. We use Upsample l (x) to denote upsampling x to length l. We also introduce a normalization constant Z to ensure the convolution operation will not change the scale of the input and a coefficient ? to control the decaying speed. Now, we are ready to introduce the weighted combination scheme by concatenating a set of weighted sub-kernels k i :</p><formula xml:id="formula_1">S = w i |0 ? i &lt; log 2 L d + 1 where w i ? R d is</formula><formula xml:id="formula_2">Cat(S) = 1 Z [k 0 , k 1 , ? ? ? , k N -1 ] ,</formula><p>where</p><formula xml:id="formula_3">k i = ? i Upsample 2 max[i-1,0] d (w i ) .<label>(1)</label></formula><p>It is easy to check that Cat(S) gives the desire convolution kernel with length L and the number of parameters is N d = O(log L). The decay coefficient ?, usually chosen to be 1/2, induces the decaying structure. In the implementation, we compute the depth-wise convolution kernel and use Fast Fourier Transform (FFT) to compute the convolution in O(L log L) time. We compute the normalization constant Z such that the norm of the kernel is one at initialization and fix it during training. Due to the simplicity of the parameterization, SGConv is easy to compute and more efficient than S4, as shown in Section 4.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first test the effectiveness of SGConv on two standard long sequence modeling tasks, i.e., Long Range Arena <ref type="bibr">[Tay et al., 2020b]</ref> and Speech Commands <ref type="bibr" target="#b46">[Warden, 2018]</ref>, and compare it with S4 and other baselines. We also conduct ablation studies over the decay speed and scale dimension d and evaluate the speed of SGConv on LRA. Further, we explore the possibility of plugging the global convolutional layer into standard models as a general-purpose component for capturing long-range dependency. For language tasks, we find that replacing half of layers of Transformer with a certain strategy with SGConv block will not hurt performance, while the complexity of those layers improves from O(L 2 ) to O(L log L). On ImageNet, we replace the 7 ? 7 convolution in ConvNext <ref type="bibr" target="#b27">[Liu et al., 2022]</ref> with SGConv and show comparative or better performance.  The decay structure is crucial for getting good performance; 2) In a reasonable range, d has less impact on the performance than t.</p><p>Long Range Arena benchmark <ref type="bibr">[Tay et al., 2020b</ref>] is a suite of six tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Results</head><p>We show the experimental results in Table <ref type="table" target="#tab_0">1</ref> with several baseline methods <ref type="bibr" target="#b43">[Vaswani et al., 2017</ref><ref type="bibr" target="#b4">, Child et al., 2019</ref><ref type="bibr">, Wang et al., 2020</ref><ref type="bibr" target="#b24">, Kitaev et al., 2019</ref><ref type="bibr" target="#b48">, Zaheer et al., 2020</ref><ref type="bibr">, Gu et al., 2021a</ref><ref type="bibr">, 2022b]</ref>. SGConv achieves a 1% improvement in average accuracy upon well-tuned S4 variants introduced in <ref type="bibr">Gu et al. [2022b]</ref>. Notably, SGConv is guided by the two intuitive principles and has a much simpler structure than S4 <ref type="bibr">[Gu et al., 2022b]</ref>. The detailed implementation settings can be found in Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation Study on IMDB</head><p>We conduct ablation studies on the IMDB byte-level document classification task in the LRA benchmark. We mainly focus on two aspects: 1) The speed of decaying and 2) The parameter dimension d of each scale. For simplicity, in the standard SGConv formulation (Eq. ( <ref type="formula" target="#formula_3">1</ref>)), we fix the decay coefficient ? = 1/2 and only tune the dimension d. However, the actual decay speed as a function of the position in the kernel depends both on ? and d, making it hard to conduct ablation studies. Thus, we use a slightly different convolution kernel that disentangles the decay speed and the dimension of each scale:</p><formula xml:id="formula_4">Cat*(S) = 1 Z [k 0 , k 1 , ? ? ? , k N -1 ] 1 1 t , 1 2 t , ? ? ? , 1 L t , where k i = Upsample 2 max[i-1,0] d (w i ) . (2)</formula><p>t here then controls the decay speed, which is independent of each scale's dimension. We conduct two sets of experiments: 1) Fix d = 8, vary t from 0 (which means no decay) to 2, and 2) Fix t = 1, vary d from 1 to 64. Figure <ref type="figure" target="#fig_1">3</ref> reports the accuracies in different settings. We can observe that 1) The decay structure is crucial for getting good performance, and 2) In a reasonable range, d has less impact on the performance than t. Nevertheless, we observe a trend of performance drop when increasing d from 8 to 64. Experiments on larger d show worse performance, which can be attributed to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Speed Comparison</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we compare the speed of the S4 kernel and SGConv kernel in different settings. Due to its simplicity, SGConv is faster than S4 for any sentence length. SGConv is about 50% faster than the vanilla implementation of the S4 kernel and is 15% faster than the optimized CUDA kernel implementation without resorting to optimized CUDA kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Speech Commands</head><p>The Speech Command (SC) dataset <ref type="bibr" target="#b46">[Warden, 2018]</ref> is a 35-class dataset of 1 second (16000 HZ sampling rate) spoken words in English. However, followup works <ref type="bibr" target="#b22">[Kidger et al., 2020</ref><ref type="bibr">, Gu et al., 2021b</ref><ref type="bibr">, Romero et al., 2021b</ref>,a] adopted a smaller 10-class subset of SC. And works <ref type="bibr">[Romero et al., 2021a</ref><ref type="bibr">, Gu et al., 2021b]</ref> on the SC dataset specifically use pre-processing such as MFCC features. Our baselines are obtained from <ref type="bibr">[Gu et al., 2021a</ref><ref type="bibr">[Gu et al., , 2022a]]</ref>. Note that besides SSM-based models, there is no strong baseline for raw waveform classification using either the 10-class or the full dataset. And SSM-based methods also show the ability to perform 0-shot testing at lower sampling rate such as 8000 Hz. Table <ref type="table" target="#tab_2">3</ref> shows that the SGConv yields better results compared to the SSM-based method among 4 out of 5 tasks. Notably, for the original SC (35-class), SGConv achieves marginally higher accuracy for raw-sequence classification and significantly better results (+2.40%) compared to the existing SoTA method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10-cls</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Further Applications of SGConv</head><p>We further study SGConv as a generic network architecture drop-in component targeting tasks in language modeling and computer vision. In Section 4.3.1 we present an efficient mixture of attention and SGConv layers architecture that replaces half of the attention blocks in the Transformer with the SGConv blocks. We demonstrate the potential of utilizing such a model for long text processing. In Section 4.3.2, we incorporate SGConv (1D) into ConvNeXt <ref type="bibr" target="#b27">[Liu et al., 2022]</ref>. Surprisingly, SGConv achieves comparable or even better results compared to several SoTA CNN and Vision Transformer models by treating the 2D features as a 1D sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Language Tasks</head><p>Language modeling. We propose the SGConv block (shown in Figure <ref type="figure">6</ref>) which is similar to the Attention block in Transformer <ref type="bibr" target="#b43">[Vaswani et al., 2017]</ref>. SGConv block enjoys both O(L log(L)) time complexity and space complexity. We benchmark the inference time and GPU memory usage of both SGConv and Attention in Table <ref type="table" target="#tab_3">4</ref>. When the sequence length is 1024, SGConv block is ?2.1X faster than the Attention block. For language modeling, we utilize the feature of SGConv to directly process the long sequences. The Attention block only targets the short range data termed SAttention. We illustrate the structure in Figure <ref type="figure">4a</ref>. Furthermore, we investigate the strategy to replace the Attention blocks with SGConv blocks. We generate 50 architectures with 8 SGConv blocks and 8 Attention blocks where the order is shuffled. We denote the average depth to replace the Attention blocks as:</p><formula xml:id="formula_5">N SGConv i=0</formula><p>idx i /N total where the idx denotes the ith SGConv depth position. N SGConv = 8 and N total = 16 in this case. The results in Figure <ref type="figure">4b</ref> suggest that when fixing the number of SGConv layer, models achieve better performance by placing SGConv blocks in deeper layers. Guided by the strategy, we handcraft two Transformer-XL <ref type="bibr" target="#b5">[Dai et al., 2019]</ref> style models. (1) 16-layer: {A, A, A, C}?2 + {A, C, C, C}?2. (2) 18-layer: {A, A, C}?3 + {A, C, C}?3. A denotes SAttention and C denotes SGConv. ?N denotes repeating the order of layers for N times. We test the model on WikiText-103 <ref type="bibr" target="#b31">[Merity et al., 2016]</ref> which is a wide-used language modeling benchmark with an average length of 3.6K We show the results in Table <ref type="table" target="#tab_4">5</ref>. Our results suggest that when the attention range is short, the 16L model outperform the baseline with -1.17 perplexity. For the 18L model, our model achieves 18.70 perplexity. Note that we use a smaller and affordable batch size (16) for training. Under the same setting, our model gains slightly better perplexity than Transformer-XL (-0.05).</p><p>Our results show the potential of adopting SGConv as part of the language model for long range language sequence processing.</p><p>Sentence classification. We combine the SGConv block with the BERT model <ref type="bibr" target="#b7">[Devlin et al., 2018]</ref>. Concretely, we utilize the 12-layer {A, A, C}?2+{A, C, C}?2 model. The pretraining is conducted on BooksCorpus <ref type="bibr">[Zhu et al., 2015]</ref> and English Wikipedia <ref type="bibr">[Foundation]</ref>. We then fine-tune the model on the GLUE benchmark <ref type="bibr" target="#b44">[Wang et al., 2019]</ref>. To avoid the instability of fine-tuning on small datasets, we only test on tasks with more than 5K training samples. We follow the training and fine-tuning pipeline of <ref type="bibr" target="#b21">Ke et al. [2020]</ref> (BERT-A in Table <ref type="table" target="#tab_0">1</ref> of <ref type="bibr" target="#b21">Ke et al. [2020]</ref>) and report the average accuracy of 5 different random seeds. SGConvBERT achieves comparable performance to the original BERT model, while the SGConv layer is more efficient than the attention layer.</p><p>Mem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SGConv ?</head><p>Mem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAttn</head><p>Mem.</p><p>Long </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Image Classification</head><p>We also evaluate the adaptability of SGConv by applying it on large-scale image classification. We conduct experiments on ImageNet-1k <ref type="bibr" target="#b6">[Deng et al., 2009]</ref> which consists of more than 1.28 million high-resolution training and 50,000 validation images. We replace the 7 ? 7 2D convolutional kernels with SGConvs in ConvNeXt <ref type="bibr" target="#b27">[Liu et al., 2022</ref>] denoted as SGConvNeXt. The block designs of SGConvNeXt are shown in Figure <ref type="figure">7</ref>. Note we train the SGConveNeXt-Tiny/Small/Base/Large using hyperparameter settings from ConvNeXt 4 without any changes. By treating the 2D features as sequences, our SGConvNeXt achieves better results compared to existing SoTA methods such as EfficientNets <ref type="bibr" target="#b38">[Tan and Le, 2019]</ref>, Swin Transformers <ref type="bibr" target="#b26">[Liu et al., 2021]</ref> (shown in Figure <ref type="figure" target="#fig_2">5</ref>). Note that Vision Transformer <ref type="bibr" target="#b11">[Dosovitskiy et al., 2020]</ref> and its variants <ref type="bibr">[Touvron et al., 2021a</ref><ref type="bibr">,b, Yu et al., 2022]</ref> adopt patching techniques that can lead to a quadratic increase in complexity with image size. Also, patching is incompatible with dynamic input resolutions while SGConvNeXt processes the data globally. We list several interesting directions that can be explored for future work: 1) Optimization for the long-range convolution: we noticed that though FFT theoretically requires less FLOPs than plain convolution, the throughput drops empirically. One reason is that there is no optimized CUDA implementation for 1D long-range convolution and can be a good direction for future work. 2) Optimized hyperparameters and data augmen- tation methods: ConvNeXts' hyperparameters are tuned for maximum performance, which may not be ideal for SGConvNeXt. 3) SGConv for vision reasoning tasks: we show that SGConv is powerful for long-range synthetic reasoning tasks and large-scale classification tasks. It could be effective in visual reasoning applications such as Vision-Language Reasoning <ref type="bibr" target="#b20">[Johnson et al., 2017</ref><ref type="bibr">, Zhu et al., 2020]</ref> with great potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>In this paper, we attempt to answer the question of what makes convolutional models great again on long sequence modeling and summarize two principles contributing to the success. Based on the principles, we propose a simple and intuitive global convolutional model SGConv that has both direct implications and solid performance. Concurrent to our work there are also attempts to simplify the S4 model by restricting the state transition matrix to be diagonal <ref type="bibr">[Gu et al., 2022a]</ref>. Compared to our paper, the proposal <ref type="bibr">Gu et al. [2022a]</ref> again involves a nuanced design of parameterization and initialization schemes, which give intuition from state-space-model perspective to explain the S4. Instead, we hope our simpler principles and non-SSM-based model can open up a direction for general audiences to understand and try global convolution as a general-purpose module for tackling long-range dependency. This potential has been shown in a very recent paper <ref type="bibr" target="#b29">[Ma et al., 2022]</ref> concurrent to our work, where the authors incorporate an exponential moving average layer to a Transformer-like model and achieve promising performance over several long sequence modeling tasks. The exponential moving average layer is a particular type of global convolution layer that naturally satisfies our two principles. We believe that similar global convolutional modules will emerge in the future as long-range dependency becomes increasingly critical for sequence modeling.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of S4 kernels on (a) Pathfinder-X and (b) Speech Command 10-class.The values in the convolution kernel exhibit a decaying behavior. We only plot the first 4096 positions for better illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Ablation study on the effect of decay speed and hidden dimension of each scale on IMDB dataset. pos ? [1, L] refers to the position in the convolution kernel. We observe: 1) The decay structure is crucial for getting good performance; 2) In a reasonable range, d has less impact on the performance than t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of ImageNet-1k Top-1 accuracy with SoTA works. Left: Top-1 Accuracy vs. FLOPs. Right: Top-1 Accuracy vs. throughputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigureFigure 8 :</head><label>8</label><figDesc>Figure 7: SGConvnext</figDesc><graphic url="image-2.png" coords="20,165.77,400.15,107.27,80.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>the parameter for i-th The performance of SGConv compared to other baselines on the LRA dataset. SGConv achieves significant improvement compared to previous methods with a more straightforward structure and faster speed (See Table2)sub-kernel k i . Denote the number of scales N = log 2</figDesc><table><row><cell>Model</cell><cell cols="5">ListOps Text Retrieval Image Pathfinder Path-X Avg.</cell></row><row><cell>Transformer</cell><cell>36.37 64.27</cell><cell>57.46</cell><cell>42.44</cell><cell>71.40</cell><cell>54.39</cell></row><row><cell>Sparse Trans.</cell><cell>17.07 63.58</cell><cell>59.59</cell><cell>44.24</cell><cell>71.71</cell><cell>51.24</cell></row><row><cell>Linformer</cell><cell>35.70 53.94</cell><cell>52.27</cell><cell>38.56</cell><cell>76.34</cell><cell>51.36</cell></row><row><cell>Reformer</cell><cell>37.27 56.10</cell><cell>53.40</cell><cell>38.07</cell><cell>68.50</cell><cell>50.67</cell></row><row><cell>BigBird</cell><cell>36.05 64.02</cell><cell>59.29</cell><cell>40.83</cell><cell>74.87</cell><cell>55.01</cell></row><row><cell>S4 (original)</cell><cell>58.35 76.02</cell><cell>87.09</cell><cell>87.26</cell><cell>86.05</cell><cell>88.10 80.48</cell></row><row><cell cols="4">S4 [Gu et al., 2022b] 59.60 86.82 90.90 88.65</cell><cell>94.20</cell><cell>96.35 86.09</cell></row><row><cell>SGConv</cell><cell cols="3">61.45 89.20 91.11 87.97</cell><cell>95.46</cell><cell>97.83 87.17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>L</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>d</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the inference and backpropagation time (ms/batch) of S4 and SGConv blocks (number of channels 128, batch size 64) on CPU and GPU. Note that the parameterization in S4 requires a customized CUDA kernel to improve the efficiency (refer to opt. in the Table). Nevertheless, SGConv still always surpasses S4 even compared to the optimized CUDA kernel.</figDesc><table><row><cell cols="3">Sequence length 256</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16384</cell></row><row><cell>Inf.</cell><cell>S4</cell><cell cols="4">29.4 81.7 158.3 306.9</cell><cell>594</cell><cell cols="2">1156.9 2274.0</cell></row><row><cell>CPU</cell><cell>SGConv</cell><cell cols="7">23.8 56.2 108.7 211.3 409.3 789.5 1559.3</cell></row><row><cell>Inf.</cell><cell cols="2">S4 (w/o opt) 2.7</cell><cell>2.7</cell><cell>4.4</cell><cell>7.9</cell><cell>15.2</cell><cell>32.7</cell><cell>64.5</cell></row><row><cell>GPU</cell><cell>S4 (w. opt.) SGConv</cell><cell>1.6 1.2</cell><cell>1.9 1.3</cell><cell>3.1 2.3</cell><cell>5.4 4.4</cell><cell>10.0 8.5</cell><cell>22.3 19.8</cell><cell>44.3 39.4</cell></row><row><cell>BP</cell><cell cols="2">S4 (w/o opt) 4.1</cell><cell>5.7</cell><cell>10.2</cell><cell>19.4</cell><cell>38.1</cell><cell>80.1</cell><cell>161.2</cell></row><row><cell cols="2">GPU S4 (w. opt.)</cell><cell>3.5</cell><cell>4</cell><cell>6.6</cell><cell>11.9</cell><cell>22.6</cell><cell>48.9</cell><cell>97.8</cell></row><row><cell></cell><cell>SGConv</cell><cell>2.0</cell><cell>2.7</cell><cell>5.0</cell><cell>9.6</cell><cell>18.6</cell><cell>41.2</cell><cell>82.5</cell></row><row><cell cols="2">4.1 Long Range Arena</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>90</cell><cell>10 0</cell><cell cols="3">10 1 Dimension of each scale</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>85</cell><cell></cell><cell></cell><cell cols="2">Decay: 1/pos, Dimension: 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell cols="3">No decay, Dimension: 8</cell><cell>Dimension Decay speed</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 t, Decay speed=1/pos t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Speech Command (SC) classification results compared to existing methods. ( * ) We carefully reproduce the S4 method based on the official released code 1 .Since the latest version removed 10-class experiments settings, we utilized earlier released version. 2 .The results suggest that for the SC 35-classification, SGConv achieves SoTA on both full length task and 2X sampling rate, zero-shot task.</figDesc><table><row><cell></cell><cell cols="6">Transformer Performer NRDE CKConv WaveGAN-D S4</cell><cell>S4  *  SGConv</cell></row><row><cell>MFCC</cell><cell>90.75</cell><cell>80.85</cell><cell>89.8</cell><cell>95.3</cell><cell></cell><cell>93.96 92.05 94.91</cell></row><row><cell>16000HZ</cell><cell></cell><cell>30.77</cell><cell>16.49</cell><cell>11.6</cell><cell></cell><cell>71.66</cell><cell>98.32 97.98 97.52</cell></row><row><cell>8000HZ (0-shot)</cell><cell></cell><cell>30.68</cell><cell>15.12</cell><cell>65.96</cell><cell></cell><cell>96.30 91.83 96.03</cell></row><row><cell>35-cls</cell><cell cols="6">InceptionNet ResNet-18 XResNet-50 ConvNet S4D S4</cell><cell>S4  *  SGConv</cell></row><row><cell>16000HZ</cell><cell>61.24</cell><cell>77.86</cell><cell cols="2">83.01</cell><cell cols="2">95.51 96.25 96.08 96.27 96.42</cell></row><row><cell>8000HZ (0-shot)</cell><cell>5.18</cell><cell>8.74</cell><cell>7.72</cell><cell></cell><cell>7.26</cell><cell>91.58 91.32 91.89 94.29</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of inference time and GPU memory utilization with Attention blocks. SGConv has significantly less memory usage and faster inference speed when the sequence increases exponentially.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">256 512 1024 2048 3072</cell></row><row><cell>Attn.</cell><cell cols="5">Inf. (ms/batch) 2.6 7.3 23.2 91.7</cell></row><row><cell>Block</cell><cell>Mem. (GB)</cell><cell cols="2">2.6 3.9</cell><cell>7.9</cell><cell>23.9 OOM</cell></row><row><cell>SGConv</cell><cell cols="5">Inf. (ms/batch) 2.7 5.4 10.9 21.8 43.6</cell></row><row><cell>Block</cell><cell>Mem. (GB)</cell><cell cols="3">2.6 3.4 5.2</cell><cell>8.7</cell><cell>15.7</cell></row><row><cell></cell><cell>Model</cell><cell></cell><cell cols="3">Valid. Test</cell></row><row><cell></cell><cell>LSTM+Hebb.</cell><cell></cell><cell>29.0</cell><cell cols="2">29.2</cell></row><row><cell></cell><cell cols="2">16L Transformer-XL</cell><cell>-</cell><cell cols="2">24.0</cell></row><row><cell></cell><cell cols="2">16L SGConv+SAttn</cell><cell cols="3">21.90 22.83</cell></row><row><cell></cell><cell>Adaptive Input</cell><cell></cell><cell>-</cell><cell cols="2">18.7</cell></row><row><cell></cell><cell>S4</cell><cell></cell><cell>-</cell><cell cols="2">20.95</cell></row><row><cell></cell><cell cols="2">18L Transformer-XL</cell><cell>-</cell><cell cols="2">18.3</cell></row><row><cell></cell><cell cols="5">18L Transformer-XL  *  18.16 18.75</cell></row><row><cell></cell><cell cols="2">18L SGConv+SAttn</cell><cell cols="3">18.10 18.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison on WikiText-103. tokens per article. We set both the attention and memory length to 384 for 18L model and 192 for 16L model. The length of input sequence is 3092 which can be processed by SGConv directly.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance comparison of BERT and SGConvBERT on GLUE dataset. SGConvBERT is comparable with BERT while being more efficient. We exclude MRPC and RTE datasets in GLUE because their sizes are too small (&lt; 5K training samples).</figDesc><table><row><cell>Sequence</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10012-10022, 2020. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.</figDesc><table><row><cell>d_in</cell><cell>d_in, 2d?1d</cell></row><row><cell>7x7, groups=d_in</cell><cell>SGCONV, groups=d_in</cell></row><row><cell>LN</cell><cell>LN , 1d?2d</cell></row><row><cell>1x1, in=d_in, out=d_e</cell><cell>1x1, in=d_in, out=d_e</cell></row><row><cell>GELU</cell><cell>GELU</cell></row><row><cell>1x1, in=d_e, out=d_in</cell><cell>1x1, in=d_e, out=d_in</cell></row><row><cell>ConvNeXt Block</cell><cell>SGConv Block</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/HazyResearch/state-spaces</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/HazyResearch/state-spaces/tree/307f11bba801d5734235a1791df1859f6ae0e367</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/LanguageModeling/ Transformer-XL</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">S?bastien Bubeck</rs> for helpful comments and discussions on the paper and <rs type="person">Albert Gu</rs> and his coauthors for introducing an interesting line of works and sharing their well-organized codebase.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Long Range Arena</head><p>Here we report the detailed implementation of the LRA experiments. We use the concatenation style combination of sub-kernels in all experiments and mildly tune the dimension of each scale. Since the SGConv exhibits a strong ability to fit data, we slightly increase the dropout for some tasks to prevent overfitting. Table <ref type="table">7</ref> lists the detailed hyperparameters used in LRA. In most experiments, we set ? to 1/2, which approximately decays in speed 1/pos. Experiments on flattened 2D images require some special modification of the kernel. We hypothesize that it is because images require more subtle inductive bias. For the experiment on the Image dataset, we use the disentangled version of parameterization and combination weights as described in Section 4.1.2 and set the decay speed to be 1/pos. For the experiment on the Pathfinder-X task, we initialize convolution kernels in different channels with cosine waves with different frequencies and randomly assign ? ranging from 1 to 1/3 to different channels. Both these modifications bring about 1% improvement compared to standard fixed ? = 1/2 and random initialization. The remaining hyperparameters and experimental settings are same to <ref type="bibr">Gu et al. [2022a]</ref> which can be found in the Github repo 1 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ListOps Text Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Language Task</head><p>Our implementation for Language Task is based on the project 3 . For the 16-L model, we utilize 3072 as the sequence length for SGCONV and 192 as both the attention and memory length for SAttention. For the 18-L model, we utilize 3072 as the sequence length for SGCONV and 384 as both the attention and memory length for SAttention. The SGConv has 96 as the scale dimension. We adopt the training settings from the above mentioned project 3 except the batch size which is reduced to 64. The SGConv block is shown in Figure <ref type="figure">4</ref>.    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do-conv: Depthwise over-parameterized convolutional layer</title>
		<author>
			<persName><forename type="first">Jinming</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhe</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep Learning and Practice with MindSpore</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Springer Nature</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1911" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Repvgg: Making vgg-style convnets great again</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13733" to="13742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11963" to="11975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://dumps.wikimedia.org" />
		<title level="m">Wikimedia Foundation. Wikimedia downloads</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hippo: Recurrent memory with optimal polynomial projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1474" to="1487" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficiently modeling long sequences with structured state spaces</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00396</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining recurrent, convolutional, and continuous-time models with linear state space layers</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="572" to="585" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On the parameterization and initialization of diagonal state space models</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.11893</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">How to train your hippo: State space models with generalized orthogonal basis projections</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isys</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Timalsina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atri</forename><surname>Rudra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.12037</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Expandnets: Linear overparameterization to train compact convolutional networks</title>
		<author>
			<persName><forename type="first">Shuxuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1298" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural controlled differential equations for irregular time series</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6696" to="6707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangling neural mechanisms for perceptual grouping</title>
		<author>
			<persName><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalpit</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning long-range spatial dependencies with horizontal gated recurrent units</title>
		<author>
			<persName><forename type="first">Drew</forename><surname>Linsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junkyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Veerabadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Windolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stable, fast and accurate: Kernelized attention with relative positional encoding</title>
		<author>
			<persName><forename type="first">Shengjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianle</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinglan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="22795" to="22807" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Mega</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.10655</idno>
		<title level="m">Moving average equipped gated attention</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Paddlepaddle: An open-source deep learning platform from industrial practice</title>
		<author>
			<persName><forename type="first">Yanjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Data and Domputing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="115" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<title level="m">Pointer sentinel mixture models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=QtTKTdVrFBB" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">cosformer: Rethinking softmax in attention</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baohong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Robert-Jan</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName><surname>Bruintjes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><surname>Flexconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08059</idno>
		<title level="m">Continuous kernel convolutions with differentiable kernel sizes</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Anna</forename><surname>David W Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Kuzina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><forename type="middle">M</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><surname>Hoogendoorn</surname></persName>
		</author>
		<author>
			<persName><surname>Ckconv</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02611</idno>
		<title level="m">Continuous kernel convolution for sequential data</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weight normalization: A simple reparameterization to accelerate training of deep neural networks</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Scrolls: Standardized comparison over long language sequences</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maor</forename><surname>Ivgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avia</forename><surname>Efrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Haviv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Efficientnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.04006</idno>
		<title level="m">Long range arena: A benchmark for efficient transformers</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In the Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00388</idno>
	</analytic>
	<monogr>
		<title level="m">Sergey Zagoruyko and Nikos Komodakis. Diracnets: Training very deep neural networks without skip-connections</title>
		<imprint>
			<date type="published" when="2017">2022. 2017</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
