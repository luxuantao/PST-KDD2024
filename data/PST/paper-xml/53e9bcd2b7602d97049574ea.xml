<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Making Use of the Most Expressive Jumping Emerging Patterns for Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinyan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSSE</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3052</postCode>
									<settlement>Parkville</settlement>
									<region>Vic</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Guozhu</forename><surname>Dong</surname></persName>
							<email>gdong@cs.wright.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Dept. of CSE</orgName>
								<orgName type="institution">Wright State University</orgName>
								<address>
									<postCode>45435</postCode>
									<settlement>Dayton</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of CSSE</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<postCode>3052</postCode>
									<settlement>Parkville</settlement>
									<region>Vic</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Making Use of the Most Expressive Jumping Emerging Patterns for Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D456C3CECBFDC1E31AAC0DBAE3A367EC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Classification aims to discover a model from training data that can be used to predict the class of test instances. In this paper, we propose the use of jumping emerging patterns (JEPs) as the basis for a new classifier called the JEP-Classifier. Each JEP can capture some crucial difference between a pair of datasets. Then, aggregating all JEPs of large supports can produce more potent classification power. Procedurally, the JEP-Classifier learns the pair-wise features (sets of JEPs) contained in the training data, and uses the collective impacts contributed by the most expressive pair-wise features to determine the class labels of the test data. Using only the most expressive JEPs in the JEP-Classifier strengthens its resistance to noise in the training data, and reduces its complexity (as there are usually a very large number of JEPs). We use two algorithms for constructing the JEP-Classifier which are both scalable and efficient. These algorithms make use of the border representation to efficiently store and manipulate JEPs. We also present experimental results which show that the JEP-Classifier achieves much higher testing accuracies than the association-based classifier of <ref type="bibr" target="#b12">[8]</ref>, which was reported to outperform C4.5 in general.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Classification is an important problem in the fields of data mining and machine learning. In general, classification aims to classify instances in a set of test data, based on knowledge learned from a set of training data. In this paper, we propose a new classifier, called the JEP-Classifier, which exploits the discriminating power of jumping emerging patterns (JEPs) <ref type="bibr">[4]</ref>. A JEP is a special type of EP <ref type="bibr">[3]</ref> (also a special type of discriminant rule <ref type="bibr" target="#b10">[6]</ref>), defined as an itemset whose support increases abruptly from zero in one dataset, to non-zero in another dataset -the ratio of support-increase being ½. The JEP-Classifier uses JEPs exclusively, and is distinct from the CAEP classifier <ref type="bibr" target="#b9">[5]</ref> which mainly uses EPs with finite support-increase ratios.</p><p>The exclusive use of JEPs in the JEP-Classifier is motivated by our belief that JEPs represent knowledge which discriminates between different classes more strongly than any other type of EPs. Consider, for example, the Mushroom dataset taken from the UCI data repository <ref type="bibr">[1]</ref>. The itemset ODOR = foul is a JEP, whose support increases from ¼± in the edible class to ± in the poisonous class. If a test instance contains this particular EP, then we can claim with a very high degree of certainty that this instance belongs to the poisonous class, and not to the edible class. In contrast, other kinds of EPs do not support such strong claims. Experimental results show that the JEP-Classifier indeed gives much higher prediction accuracy than previously published classifiers.</p><p>Example 1. This simplified example illustrates how JEPs are used in the JEP-Classifier. Consider two sets of training data, ½ and ¾ , such that all instances in ½ are of Class 1, and all instances in ¾ are of Class 2. Let each instance be a subset of (see Table <ref type="table">1</ref>). Question: Which class should the test instance be classified as?</p><p>Table <ref type="table">1</ref>. Two simplified datasets containing 4 instances each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>½ ¾</head><p>Answer: Class 2. Rationale: The test instance contains the JEP from ½ to ¾ , whose support in ¾ is ¼±. Furthermore, the remaining proper subsets of -namely, , , , , and -appear in both classes of data with the same frequencies. These facts give us a higher confidence that the test instance should be classified as Class 2.</p><p>In general, a test instance Ì may contain several JEPs, and these EPs can favour different classes. Consider again the datasets in Table <ref type="table">1</ref>, this time with the test instance Ì . The instance Ì contains the following JEPs:</p><p>-the subsets and , in favour of Class 1 with supports in ½ of, respectively, ¼± and ¾ ±;</p><p>-the subset in favour of Class 2, with a support in ¾ of ¼±.</p><p>We let all three JEPs contribute an impact equal to its support in its favoured class -the final decision is reached using the collective impact, obtained as the sum of the impacts of the individual JEPs, and choosing the class with the largest collective impact as the class of the test instance. It follows that the instance should be classified as Class 1, since the collective impact in favour of Class 1 ( ¼± • ¾ ± ±) is larger than that of Class 2 ( ¼±). This aggregation of the supports of JEPs is at the core of the JEP-Classifier.</p><p>There can be a large (e.g., ½¼ ) number of JEPs in the dense and high-dimensional datasets of a typical classification problem. Obviously, the naive approach to discovering all JEPs and calculating their collective impacts is too time consuming. For the JEP-Classifier, we utilize two border-based algorithms <ref type="bibr">[3,</ref><ref type="bibr">4]</ref> to efficiently discover concise border representations of all JEPs from training dataset. The use of the border representation simplifies the identification of the most expressive JEPs. Intuitively, the most expressive JEPs are those JEPs with large support, which can be imagined as being at the "frontier" of the set of JEPs. Itemsets which are proper subsets of the boundary itemsets are not JEPs, while itemsets which are proper supersets of the boundary itemsets must have supports not larger than the largest support of the boundary itemsets. These boundary JEPs represent the essence of the discriminating knowledge in the training dataset. The use of the most expressive JEPs strengthens the JEP-Classifier's resistance to noise in the training data, and can greatly reduce its overall complexity. Borders are formally defined in Section 3.</p><p>Example 1 above deals with a simple database containing only two classes of data. To handle the general cases where the database contains more classes, we introduce the concept of pair-wise features, which describes a collection of the discriminating knowledge of ordered pairs of classes of data. Using the same idea for dealing with two classes of data, the JEP-Classifier uses the collective impact contributed by the most expressive pair-wise features to predict the labels of more than two classes of data.</p><p>Our experimental results (detailed in <ref type="bibr">Section 5)</ref> show that the JEP-Classifier can achieve much higher testing accuracy than previously published classifiers, such as the classifier proposed in <ref type="bibr" target="#b12">[8]</ref>, which generally outperforms C4.5, and the classifier in <ref type="bibr" target="#b9">[5]</ref>. In summary, the JEP-Classifier has superior performance because:</p><p>1. Each individual JEP has sharp discriminating power, and 2. Identifying the most expressive JEPs and aggregating their discriminating power leads to very strong classifying ability.</p><p>Note that the JEP-Classifier can reach a ½¼¼± accuracy on any training data. However, unlike many classifiers, this does not lead to the usual overfitting problems, as JEPs can only occur when they are supported in the training dataset. The remainder of this paper is organised as follows. In Section 2, we present an overall description of the JEP-Classifier (the learning phase and the classification procedure), and formally define its associated concepts. In Section 3, we present two algorithms for discovering the JEPs in a training dataset: one using a semi-naive approach, and the other using a border-based approach. These algorithms are complementary, each being useful for certain types of training data. In Section 4, we present a process for selecting the most expressive JEPs, which efficiently reduces the complexity of the JEP-Classifier. In Section 5, we show some experimental results using a number of databases from the UCI data repository <ref type="bibr">[1]</ref>. In Section 6, we outline several previously published classifiers, and compare them to the JEP-Classifier. Finally, in Section 7, we offer some concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The JEP-Classifier</head><p>The framework discussed here assumes that the training database is a normal relational table, consisting of AE instances defined by Ñ distinct attributes. An attribute may take categorical values (e.g., the attribute COLOUR) or numeric values (e.g., the attribute SALARY). There are Õ known classes, namely Class ½, ¡ ¡ ¡, Class Õ; the AE instances have been partitioned into Õ sets, ½ ¾ ¡ ¡ ¡ Õ , according to their classes.</p><p>To encode as a binary database, the categorical attribute values are mapped to items using bijections. For example, the two categorical attribute values, namely red and yellow, of COLOR, are mapped to two items: (COLOR = red) and (COLOR = yellow). For a numeric attribute, its value range is first discretized into intervals, and then the intervals are mapped to items using an approach similar to that for categorical attributes. In this work, the values of numeric attributes in the training data are discretized into 10 intervals with the same length, using the so-called equal-length-bin method.</p><p>Let Á denote the set of all items in the encoding. An itemset is defined as a subset of Á. The support of an itemset over a dataset ¼ is the fraction of instances in ¼ that contain , and is denoted ×ÙÔÔ ¼ ´ µ.</p><p>The most frequently used notion, JEPs, is defined as follows:</p><p>Definition 1. The JEPs from ¼ to ¼¼ , denoted JEP´ ¼ ¼¼ µ, (or called the JEPs of ¼¼ over ¼ , or simply the JEPs of ¼¼ if ¼ is understood), are the itemsets whose supports in ¼ are zero but in ¼¼ are non-zero.</p><p>They are named jumping emerging patterns (JEPs), because the supports of JEPs grow sharply from zero in one dataset to non-zero in another dataset.</p><p>To handle the general case where the training dataset contains more than two classes, we introduce the concept of pair-wise features. Note that we do not enumerate all these JEPs individually in our algorithms. Instead, we use borders to represent them. Also, the border representation mechanism facilitates the simple selection of the most expressive JEPs. The concept of border was proposed in <ref type="bibr">[3]</ref> to succinctly represent a large collection of sets. (It will be reviewed later in section 3.) Continuing with the above example, the JEPs from ½ to ¾ can be represented by the border of . Its left bound is , and its right bound is ; it represents all those sets that are supersets of some itemset in its left bound, and are subsets of some itemset in its right bound. Obviously, , the itemset in the left bound, has the largest support among all itemsets covered by the border. Similarly, the JEPs from ¾ to ½ can be represented by two borders: and . (Details will be given in Section 4.) Therefore, the most expressive JEPs in ½ and ¾ are those in the set of , the union of the left bounds of the three borders above. Observe that it is much smaller than the set of all JEPs.</p><p>In JEP-Classifier, the most expressive JEPs play a central role. To classify a test instance Ì , we evaluate the collective impact of only the most expressive JEPs that are subsets of Ì . Definition 3. Given a pair of datasets ¼ and ¼¼ and a test instance Ì , the collective impact in favour of the class of ¼ contributed by the most expressive JEPs of ¼ and of ¼¼ is defined as</p><formula xml:id="formula_0">¾MEJEP´ ¼ ¼¼ µ and Ì ×ÙÔÔ ¼ ´ µ</formula><p>where MEJEP´ ¼ ¼¼ µ is the union of the most expressive JEPs of ¼ over ¼¼ and the most expressive JEPs of ¼¼ over ¼ . The collective impact in favour of the class of ¼¼ is defined similarly.</p><p>The classification procedure of JEP-Classifier for a given test instance is a simple process as follows. Given a test instance Ì , the Õ collective impacts respectively in favour of the Õ classes are first computed. Then, the JEP-Classifier determines the class label as the class where Ì obtains the largest collective impact. When a tie occurs (i.e., the collective impacts obtained are equal), we can use popularities to break the tie.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Discovering the Pair-wise Features</head><p>As the pair-wise features in are defined as the JEPs over Õ pairs of datasets, we only need to consider how to discover the JEPs over one pair of datasets. Without loss of generality, suppose dataset consists of only two classes of data ½ and ¾ , then the pair-wise features in are the JEPs from ½ to ¾ and the JEPs from ¾ to ½ . Now, we consider how to discover the JEPs from ½ to ¾ .</p><p>The most naive way to find the JEPs from ½ to ¾ is to check the frequencies, in ½ and ¾ , of all itemsets. This is clearly too expensive to be feasible. The problem of efficiently mining JEPs from dense and high-dimensional datasets is well-solved in <ref type="bibr">[3]</ref> <ref type="bibr">[4]</ref>. The high efficiency of these algorithms is a consequence of their novel use of borders <ref type="bibr">[3]</ref>. In the following subsections we present two approaches to discovering JEPs. The first approach is a semi-naive algorithm which makes limited use of borders, while the second approach uses an efficient border-based algorithm called MBD-LLBORDER <ref type="bibr">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Borders, Horizontal Borders, and HORIZON-MINER</head><p>A border is a structure used to succinctly represent certain large collections of sets. Definition 4. <ref type="bibr">[3]</ref>. A border is an ordered pair Ä Ê such that each of Ä and Ê is an antichain collection of sets, each element of Ä is a subset of some element in Ê, and each element of Ê is a superset of some element in Ä; Ä is the left bound of the border, and Ê is its right bound.</p><p>The collection of sets represented by Ä Ê (also called the The simple HORIZON-MINER algorithm <ref type="bibr">[4]</ref> was proposed to discover the horizontal border of a dataset. The basic idea of this algorithm is to select the maximum itemsets from all instances in (an itemset is maximal in the collection of itemsets if it has no proper superset in ). HORIZON-MINER is very efficient as it requires only one scan through the dataset.</p><formula xml:id="formula_1">set interval of Ä Ê ) is Ä Ê℄ ¾ Ä ¾ Ê such</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Semi-naive Approach to Discovering JEPs</head><p>The semi-naive algorithm for discovering the JEPs from ½ to ¾ consists of the following two steps: (i) Use HORIZON-MINER to discover the horizontal border of ¾ ; (ii) Scan ½ to check the supports of all itemsets covered by the horizontal border of ¾ ; the JEPs are those itemsets with zero support in ½ . The pruned SE-tree <ref type="bibr">[3]</ref> can be used in this process to irredundantly and completely enumerate the itemsets represented by the horizontal border.</p><p>The semi-naive algorithm is fast on small databases. However, on large databases, a huge number of itemsets with non-zero support make the semi-naive algorithm too slow to be practical. With this in mind, in the next subsection, we present a method which is more efficient when dealing with large databases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Border-based Algorithm to Discover JEPs</head><p>In general, MBD-LLBORDER <ref type="bibr">[3]</ref> finds those itemsets whose supports in ¾ are some support threshold but whose support in ½ are less than some support threshold AE for a pair of dataset ½ and ¾ . Specially, this algorithm produces exactly all those itemsets whose supports are nonzero in ¾ but whose supports are zero in ½ , namely the JEPs from ½ to ¾ . In this case, MBD-LLBORDER takes the horizontal border from ½ and the horizontal border from ¾ as inputs. Importantly, this algorithm does not output all JEPs individually. Instead, MBD-LLBORDER outputs a family of borders in the form of Ä Ê , ½ ¡ ¡ ¡ , to concisely represent all JEPs.</p><p>Unlike the semi-naive algorithm, which must scan the dataset ½ to discover the JEPs, MBD-LLBORDER works by manipulating the horizontal borders of the datasets ½ and ¾ . As a result, the MBD-LLBORDER algorithm scales well to large databases. This is confirmed by the experimental results in Section 5. The MBD-LLBORDER algorithm for discovering JEPs is described in detail in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Selecting the Most Expressive JEPs</head><p>We have given two algorithms to discover the pair-wise features from the training data : the semi-naive algorithm is useful when is small, while the MBD-LLBORDER algorithm is useful when is large. As seen in the past section, the MBD-LLBORDER algorithm outputs the JEPs represented by borders. These borders can represent very large collections of itemsets. However, only those itemsets with large support contribute significantly to the collective impact used to classify a test instance. By using only the most expressive JEPs in the JEP-Classifier, we can greatly reduce its complexity, and strengthen its resistance to noise in the training data.</p><p>Consider JEP´ ½ ¾ µ JEP´ ¾ ½ µ, the pair-wise features in . Observe that JEP´ ½ ¾ µ is represented by a family of borders of the form Ä Ê , ½ ¡ ¡ ¡ , where the Ê are singleton sets (see the pseudo-code for MBD-LLBORDER in the Appendix). We believe that the itemsets in the left bounds, Ä , are the most expressive JEPs in the dataset. The reasons behind this selection include:</p><p>-By definition, the itemsets in the left bound of a border have the largest supports of all the itemsets covered by that border because the supersets of an itemset have smaller supports than that of . Then, the most expressive JEPs cover more instances (at least equal) of the training dataset than the other JEPs. -Any proper subset of the most expressive JEPs is not a JEP any more.</p><p>It follows that we can select the most expressive JEPs of JEP´ ½ ¾ µ by taking the union of the left bounds of the borders produced by MBD-LLBORDER. This union is called the LEFT-UNION of JEP´ ½ ¾ µ. So, LEFT-UNION Ä . Similarly, we can select the most expressive JEPs of JEP´ ¾ ½ µ. Combining the two LEFT-UNION, the most expressive pair-wise features in are then constructed.</p><p>Algorithmically, finding LEFT-UNION can be done very efficiently. If the MBD-LLBORDER algorithm is used, then we simply use the left bounds of the borders it produces. In practice, this can be done by replacing the last line of the pseudo code of the MBD-LLBORDER algorithm in the Appendix with return the union of the left bounds of all borders in EPBORDERS.</p><p>If the semi-naive algorithm is used, then LEFT-UNION can be updated as each new JEP is discovered.</p><p>Example 5. To illustrate several points discussed in this subsection, consider ½ and ¾ from Table <ref type="table">1</ref>. The horizontal border of ½ is ½ , and that of The LEFT-UNION of JEP´ ¾ ½ µ is the union of the left bounds of the above two borders, namely .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section we present the results of our experiments, where we run the JEP-Classifier on 30 databases (some contain up to 10 classes, some have up to 30162 instances, some have up to 60 attributes) taken from the UCI Repository of Machine Learning Databases <ref type="bibr">[1]</ref>. These experiments were carried out on a 500MHz PentiumIII PC with 512M bytes of RAM. The accuracy was obtained using the methodology of ten-fold cross-validation <ref type="bibr" target="#b14">[10]</ref> (but one fold was tested in census-income).</p><p>The experiment's pre-processes are: (i) download original datasets, say , from the UCI website; (ii) partition into class datasets ½ ¾ ¡ ¡ ¡ Õ ; (iii) randomly shuffle ½ ¡ ¡ ¡ Õ; (iv) for each , choose the first 10% instances as the testing data and the remaining 90% as the training data. Repeatedly, choose the second 10% as the testing data, and so forth; (v) if there exist continuous attributes, discretize them by our equal-length-bin method in the training datasets first, and then map the intervals to the testing data. This step is used to convert the original training and testing data into the standard binary transactional data. (These executable codes are available from the authors on request.) After pre-processing, we followed the steps illustrated in Figure <ref type="figure" target="#fig_0">1</ref> to get the results. Alternatively, MLC++ technique <ref type="bibr" target="#b11">[7]</ref> was also used to discretize continuous attributes in the glass, ionosphere, pima, sonar, and vehicle datasets. These testing accuracies are reported in Table <ref type="table" target="#tab_0">2</ref>. The main disadvantage of MLC++ technique is that it sometimes, for example in the liver dataset, produces many different instances with different labels into identical instances. Table <ref type="table" target="#tab_0">2</ref> summarizes the results. In this table, the first column lists the name of each database, followed by the numbers of instances, attributes, and classes in Column 2. The third column presents the error rate of the JEP-Classifier, calculated as the percentage of test instances incorrectly predicted. Similarly, columns 4 and 5 give the error rate of, respectively, the CBA classifier in <ref type="bibr" target="#b12">[8]</ref> and C4.5. (These results are the best results taken from Table <ref type="table">1</ref> in <ref type="bibr" target="#b12">[8]</ref>; a dash indicates that we were not able to find previous reported ½ For readability, we use as shorthand for the set . 1. Our JEP-Classifier performed perfectly (100% or above 98.5% testing accuracy) on some databases (nursery, mushroom, tic-tac-toe, soybean). 2. Among the 25 databases marked with * (indicating results of both CBA and C4.5 are available) in table 2, the JEP-Classifier outperforms both C4.5 and CBA on 15 datasets; CBA wins on 5; and C4.5 wins on 5 (in terms of the testing accuracies). 3. For the databases (with bold font), they have much larger data sizes than the remaining databases. The JEP-Classifier performs well on those datasets. 4. For unbalanced datasets (having unbalanced numbers of instances for each class), the JEP-Classifier performs well. For example, nursery dataset contains 5 classes and have respectively 4320, 2, 328, 4266, and 4044 instances in each class. Interest-ingly, we observed that the testing accuracy by the JEP-Classifier was consistently around 100% for each class. For CBA, its support threshold was set as 1%. In this case, CBA would mis-classify all instances of class 2. The reason is that CBA cannot find the association rules in class 2.</p><p>Our experiments also indicate that the JEP-Classifier is fast and highly efficient.</p><p>-Building the classifiers took approximately ¼ ¿ hours on average for the 30 cases considered here. -For databases with a small number of items, such as the iris, labor, liver, soybean, and zoo databases, the JEP-Classifier completed both the learning and testing phases within a few seconds. For databases with a large number of items, such as the mushroom, sonar, german, nursery, and ionosphere databases, both phases required from one to two hours. -In dense databases, the border representation reduced the total numbers of JEPs (by a factor of up to ½¼ or more) down to a relatively small number of border itemsets (approximately ½¼ ¿ ).</p><p>We also conducted experiments to investigate how the number of data instances affects the scalability of the JEP-Classifier. We selected ¼±, ±, and ¼± of data instances from each original database to form three new databases. The JEP-Classifier was then applied to the three new databases. The resulting run-times shows a linear dependence on the number of data instances when the number of attributes is fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Extensive research on the problem of classification has produced a range of different types of classification algorithms, including nearest neighbor methods, decision tree induction, error back propagation, reinforcement learning, and rule learning. Most classifiers previously published, especially those based on classification trees (e.g., C4.5 <ref type="bibr" target="#b13">[9]</ref>, CART <ref type="bibr">[2]</ref>), arrive at a classification decision by making a sequence of micro decisions, where each micro decision is concerned with one attribute only. Our JEP-Classifier, together with the CAEP classifier <ref type="bibr" target="#b9">[5]</ref> and the CBA classifier <ref type="bibr" target="#b12">[8]</ref>, adopts a new approach by testing groups of attributes in each micro decision. While CBA uses one group at a time, CAEP and the JEP-Classifier use the aggregation of many groups of attributes. Furthermore, CBA uses association rules as the basic knowledge of its classifier, CAEP uses emerging patterns (mostly with finite growth rates), and the JEP-Classifier uses jumping emerging patterns.</p><p>While CAEP has some common merits with the JEP-Classifier, it differs from the JEP-Classifier in several ways:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding Remarks</head><p>In this paper, we have presented an important application of JEPs to the problem of classification. Using the border representation and border-based algorithms, the most expressive pair-wise features were efficiently discovered in the learning phase. The collective impact contributed by these pair-wise features were then used to classify test instances. The experimental results have shown that the JEP-Classifier generally achieves a higher predictive accuracy than previously published classifiers, including the classifier in <ref type="bibr" target="#b12">[8]</ref>, and C4.5. This high accuracy results from the strong discriminating power of an individual JEP over a fraction of the data instances and the collective discriminating power by all the most expressive JEPs. Furthermore, our experimental results show that the JEP-Classifier scales well to large datasets.</p><p>As future work, we plan to pursue several directions. (i) In this paper, collective impact is measured by the sum of the supports of the most expressive JEPs. As alternatives, we are considering other aggregates, such as the squared sum, and adaptive methods, such as neural networks. (ii) In this paper, JEPs are represented by borders. In the worst case, the number of the JEPs in the left bound of a border can reach AE AE ¾ , where AE is the number of attributes in the dataset. We are considering the discovery and use of only some of the itemsets in the left bound, to avoid this worst-case complexity. (iii) In discovering JEPs using the MBD-LLBORDER algorithm, there are multiple uses of the BORDER-DIFF sub-routine, dealing with different borders. By parallelizing these multiple calls, we can make the learning phase of the JEP-Classifier even faster and more scalable.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. JEP-Classifier working on a database with three classes of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1 depicts how the JEP-Classifier is built from the training data, and how it is then used to classify testing data, for the case when a database contains three classes of data. In this figure, JEP´½ • ¾ ¿µ represents the JEPs from ½ ¾ to ¿ , and similarly for JEP´½ • ¿ ¾µ and JEP´¾ • ¿ ½µ. The HORIZON-MINER [4] and MBD-LLBORDER [3] algorithms, used to extract the pair-wise features from the training dataset, are outlined in Section 3. Determining the most expressive JEPs is discussed in Section 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>from ¾ to ½ are represented by two borders Ä Ê , ½ ¾, namely and . (The readers can use MBD-LLBORDER in the Appendix to derive these borders.JEPs in the left bounds have the largest supports.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Definition 2 .</head><label>2</label><figDesc>The pair-wise features in a dataset , whose instances are partitioned into Õ classes ½ ¡ ¡ ¡ Õ , consist of the following Õ groups of JEPs: those of ½ over For example, if Õ ¿, then the pair-wise features in consist of 3 groups of JEPs: those of ½ over ¾ ¿ , those of ¾ over ½ ¿ , and those of ¿ over ½ ¾ . Example 2. The pair-wise features in ½ and ¾ of Table 1 consist of the JEPs from ½ to ¾ , , and the JEPs from ¾ to ½ , .</figDesc><table /><note><p>Õ ¾ , those of ¾ over Õ ¾ , ¡ ¡ ¡, and those of Õ over Õ ½ ½ .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Test Data Collective impact in favor of Determining the class label of T</head><label></label><figDesc></figDesc><table><row><cell cols="3">Training Data (3 Classes)</cell></row><row><cell>½</cell><cell>¾</cell><cell>¿</cell></row><row><cell></cell><cell cols="2">Pair-wise Features</cell></row><row><cell>JEP´½ • ¾ ¿µ</cell><cell>JEP´½ • ¿ ¾µ</cell><cell>JEP´¾ • ¿ ½µ</cell></row><row><cell cols="3">The Most Expressive JEPs</cell></row><row><cell cols="3">Calculating the collective impacts</cell></row><row><cell cols="2">when a test case T is given</cell><cell></cell></row><row><cell>Class 1</cell><cell>Class 2</cell><cell>Class 3</cell></row></table><note><p>By MBD-LLborder or naive algorithm (after Horizon-Miner)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>that We say that Ä Ê℄ has Ä Ê as its border, and that each ¾ Ä Ê℄ is covered by Ä Ê .</figDesc><table><row><cell cols="2">Example 3. The set interval of</cell><cell>consists of</cell></row><row><cell cols="2">twelve itemsets, namely all sets that are supersets of</cell><cell>and that are subsets of either</cell></row><row><cell>or</cell><cell>.</cell></row></table><note><p>Definition 5. The horizontal border of a dataset is the border Ê that represents all non-zero support itemsets in the dataset. Example 4. The horizontal border of ½ in Table 1 is .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Accuracy Comparison. Column 6 gives the number of the most expressive JEPs used by the JEP-Classifier. The last column gives the number of CARs used in CBA.These results raise several points of interest.</figDesc><table><row><cell>Datasets</cell><cell cols="6">#inst, attri, class JEP-Cla. CBA C4.5rules # JEPs #CARs</cell></row><row><cell>anneal*</cell><cell>998, 38, 5</cell><cell>4.4</cell><cell>1.9</cell><cell>5.2</cell><cell cols="2">5059 65081</cell></row><row><cell>australian*</cell><cell cols="2">690, 14, 2 13.66</cell><cell>13.2</cell><cell>13.5</cell><cell cols="2">9806 46564</cell></row><row><cell>breast-w*</cell><cell>699, 10, 2</cell><cell>3.73</cell><cell>3.9</cell><cell>3.9</cell><cell>2190</cell><cell>399</cell></row><row><cell>census</cell><cell>30162, 16, 2</cell><cell>12.7</cell><cell>-</cell><cell>-</cell><cell>68053</cell><cell>-</cell></row><row><cell>cleve*</cell><cell cols="2">303, 13, 2 15.81</cell><cell>16.7</cell><cell>18.2</cell><cell cols="2">8633 1634</cell></row><row><cell>crx*</cell><cell cols="2">690, 15, 2 14.06</cell><cell>14.1</cell><cell>15.1</cell><cell cols="2">9880 4717</cell></row><row><cell>diabete*</cell><cell cols="2">768, 8, 2 23.31</cell><cell>24.7</cell><cell>25.8</cell><cell>4581</cell><cell>162</cell></row><row><cell>german*</cell><cell>1000, 20, 2</cell><cell>24.8</cell><cell>25.2</cell><cell>27.7</cell><cell cols="2">32510 69277</cell></row><row><cell>glass*</cell><cell>214, 9, 7</cell><cell>17.4</cell><cell>27.4</cell><cell>27.5</cell><cell>127</cell><cell>291</cell></row><row><cell>heart*</cell><cell cols="2">270, 13, 2 17.41</cell><cell>18.5</cell><cell>18.9</cell><cell>7596</cell><cell>624</cell></row><row><cell>hepatitis*</cell><cell cols="2">155, 19, 2 17.40</cell><cell>15.1</cell><cell>19.4</cell><cell cols="2">5645 2275</cell></row><row><cell>horse*</cell><cell>368, 28, 2</cell><cell>16.8</cell><cell>17.9</cell><cell>16.3</cell><cell cols="2">22425 7846</cell></row><row><cell>hypo*</cell><cell>3163, 25, 2</cell><cell>2.69</cell><cell>1.6</cell><cell>0.8</cell><cell>1903</cell><cell>493</cell></row><row><cell>ionosphere*</cell><cell>351, 34, 2</cell><cell>6.9</cell><cell>7.9</cell><cell>8.0</cell><cell cols="2">8170 10055</cell></row><row><cell>iris*</cell><cell>150, 4, 3</cell><cell>2.67</cell><cell>7.1</cell><cell>4.7</cell><cell>161</cell><cell>23</cell></row><row><cell>labor*</cell><cell>57, 16, 2</cell><cell>8.67</cell><cell>17.0</cell><cell>20.7</cell><cell>1400</cell><cell>313</cell></row><row><cell>liver</cell><cell cols="2">345, 6, 2 27.23</cell><cell>-</cell><cell>32.6</cell><cell>1269</cell><cell>-</cell></row><row><cell>lymph*</cell><cell>148, 18, 4</cell><cell>28.4</cell><cell>18.9</cell><cell>21.0</cell><cell cols="2">5652 2965</cell></row><row><cell>mushroom</cell><cell>8124, 22, 2</cell><cell>0.0</cell><cell>-</cell><cell>-</cell><cell>2985</cell><cell>-</cell></row><row><cell>nursery</cell><cell>12960, 8, 5</cell><cell>1.04</cell><cell>-</cell><cell>-</cell><cell>1331</cell><cell>-</cell></row><row><cell>pima*</cell><cell>768, 8, 2</cell><cell>20.4</cell><cell>26.9</cell><cell>24.5</cell><cell cols="2">54 2977</cell></row><row><cell>sick*</cell><cell>4744, 29, 2</cell><cell>2.33</cell><cell>2.7</cell><cell>1.5</cell><cell>2789</cell><cell>627</cell></row><row><cell>sonar*</cell><cell>208, 60, 2</cell><cell>14.1</cell><cell>21.7</cell><cell>27.8</cell><cell cols="2">13050 1693</cell></row><row><cell>soybean</cell><cell>47, 34, 4</cell><cell>0.00</cell><cell>-</cell><cell>8.3</cell><cell>1928</cell><cell>-</cell></row><row><cell>tic-tac-toe*</cell><cell>958, 9, 2</cell><cell>1.0</cell><cell>0.0</cell><cell>0.6</cell><cell cols="2">2926 1378</cell></row><row><cell>vehicle*</cell><cell>846, 18, 4</cell><cell>27.9</cell><cell>31.2</cell><cell>27.4</cell><cell cols="2">19461 5704</cell></row><row><cell>vote1*</cell><cell>433, 16, 2</cell><cell>8.53</cell><cell>6.4</cell><cell>4.8</cell><cell>5783</cell><cell>-</cell></row><row><cell>wine*</cell><cell>178, 13, 3</cell><cell>6.11</cell><cell>8.4</cell><cell>7.3</cell><cell cols="2">5531 1494</cell></row><row><cell>yeast*</cell><cell cols="2">1484, 8, 10 33.72</cell><cell>44.9</cell><cell>44.3</cell><cell>2055</cell><cell>-</cell></row><row><cell>zoo*</cell><cell>101, 16, 7</cell><cell>4.3</cell><cell>5.4</cell><cell>7.8</cell><cell>624</cell><cell>686</cell></row><row><cell>results).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We are grateful to Richard Webber for his help. We thank Bing Liu and Jiawei Han for useful discussions. We also thank the anonymous referees for helpful suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: MBD-LLBORDER for Discovering JEPs</head><p>Suppose the horizontal border of ½ is ½ ¡ ¡ ¡ Ñ and the horizontal bor- </p><p>remove all itemsets in Ä that are not minimal; return Ä Í ;</p><p>Note that given a collection of sets, the minimal sets are those ones whose proper subsets are not in . For correctness and variations of BORDER-DIFF, the readers are referred to <ref type="bibr">[3]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Basic idea. The JEP-Classifier utilizes the JEPs of large supports (the most discriminating and expressive knowledge) to maximize its collective classification power when making decisions. CAEP uses the collective classifying power of EPs with finite growth rates, and possibly some JEPs</title>
		<imprint/>
	</monogr>
	<note>in making decisions</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">In the JEP-Classifier, the most expressive JEPs are discovered by simply taking the union of the left bounds of the borders derived by the MBD-LLBORDER algorithm (specialised for discovering JEPs)</title>
		<imprint/>
	</monogr>
	<note>In the CAEP classifier, the candidate EPs must be enumerated individually after the MBD-LLBORDER algorithm in order to determine their supports and growth rates</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classification procedure. The JEP-Classifier&apos;s decision is based on the collective impact contributed by the most expressive pair-wise features, while CAEP&apos;s decision is based on the normalized ratio-support scores</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting accuracy. The JEP-Classifier outperforms the CAEP classifier in large and high dimension databases such as mushroom, ionosphere, and sonar. For small datasets such as heart, breast-w, hepatitis, and wine databases, the CAEP classifier reaches higher accuracies than the JEP-Classifier does</title>
	</analytic>
	<monogr>
		<title level="m">On 13 datasets where results are available for CAEP, the JEP-Classifier outperforms CAEP on 9 datasets</title>
		<imprint/>
	</monogr>
	<note>While our comparison to CAEP is still preliminary, we believe that CAEP and JEP-Classifiers are complementary. More investigation is needed to fully understand the advantages offered by each technique. References</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">UCI Repository of machine learning database</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Murphy</surname></persName>
		</author>
		<ptr target="http://www.cs.uci.edu/mlearn/mlrepository.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Irvine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Department of Information and Computer Science</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth International Group</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient mining of emerging patterns: Discovering trends and differences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD&apos;99 International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>ACM SIGKDD&apos;99 International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>San Diego, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering jumping emerging patterns and experiments on real datasets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Database Conference (IDC&apos;99)</title>
		<meeting>the 9th International Database Conference (IDC&apos;99)<address><addrLine>Hong Kong</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="155" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CAEP: Classification by aggregating emerging patterns</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DS-99: Second International Conference on Discovery Science</title>
		<meeting><address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploration of the power of attribute-oriented induction in data mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Piatetsky-Shapiro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Uthurusamy</surname></persName>
		</editor>
		<imprint>
			<publisher>AAAI/MIT Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="399" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MLC++: a machine learning library in C++. Tools with artificial intelligence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pfleger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="740" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Integrating classification and association rule mining</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Conference on Knowledge Discovery in Databases and Data Mining, KDD&apos;98</title>
		<meeting>the 4th International Conference on Knowledge Discovery in Databases and Data Mining, KDD&apos;98<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">C4. 5: program for machine learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On comparing classifiers: pitfalls to avoid and a recommended approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Salzberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="317" to="327" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
