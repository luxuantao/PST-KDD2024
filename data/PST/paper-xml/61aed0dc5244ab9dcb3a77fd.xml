<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-12-06">6 Dec 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
							<email>chezhu@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
							<email>siqi.sun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
							<email>jfgao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
							<email>nzeng@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Corporation</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-12-06">6 Dec 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.03254v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications.</p><p>In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledge External Attention for Reasoning (KEAR), reaches human parity on the open Common-senseQA research benchmark with an accuracy of 89.4% in comparison to the human accuracy of 88.9%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> have revolutionized many areas of AI with state-of-the-art performance in a wide range of tasks <ref type="bibr" target="#b12">(Devlin et al., 2018;</ref><ref type="bibr">Dosovitskiy et al., 2020)</ref>. The most notable and effective component in a Transformer model is the self-attention mechanism, which enables the model to dynamically leverage different parts of the input for computation with no information loss for even the most distant parts in input. With the success of pre-trained models <ref type="bibr" target="#b12">(Devlin et al., 2018;</ref><ref type="bibr" target="#b31">Liu et al., 2019)</ref>, the Transformer and its selfattention mechanism have been widely adopted as the cornerstone of foundation models trained on huge amounts of data <ref type="bibr" target="#b4">(Bommasani et al., 2021)</ref>.</p><p>One phenomenon found during the development of Transformer models is that models with larger size tend to have better learning abilities, especially when combined with large-scale data <ref type="bibr" target="#b23">(Kaplan et al., 2020)</ref>. This has prompted the recent boom of super large Transformer models, ranging from BERT <ref type="bibr" target="#b12">(Devlin et al., 2018)</ref> with 110 million parameters, to <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> with 175 billion parameters. Nevertheless, numerous studies have shown that the corresponding understanding and generation capabilities of these huge models are still behind humans <ref type="bibr" target="#b4">(Bommasani et al., 2021)</ref>. Furthermore, the sheer size of these models already poses serious practical challenges in utilization, deployment, interpretation, and environmental impact <ref type="bibr" target="#b35">(Patterson et al., 2021)</ref>. Thus, the recent "scaling-up" approach to Transformer-based NLP modeling is unsustainable and has been questioned in recent studies <ref type="bibr" target="#b4">(Bommasani et al., 2021)</ref>.</p><p>In this paper, we take a step back and examine the mechanism of current Transformer-based models. Self-attention was designed to allow the model to better analyze the inner structure of input data, and the model is trained to have its parameters grasp and memorize all the content and patterns of the training data. When the model is given a novel input X, the implicitly stored knowledge in the parameters about related information is activated to facilitate the analysis of X. This could partly explain why larger models pre-trained with more data have an advantage in performance.</p><p>While Transformer models process input by looking inward via self-attention, we propose to make the model look outward by providing it with related context and knowledge from various sources. We then let the model conduct selfattention on the input while also computing external attention to the knowledge (Figure <ref type="figure" target="#fig_0">1</ref>). As the context and knowledge can usually be stored in an non-parametric and symbolic way (e.g., plain text, knowledge graph and dictionary entries), even moderately-sized Transformer models can perform exceptionally well on NLP tasks. This approach allows one to shrink the size of Transformer-based foundation models, which is critical to the accessibility and democratization of AI technology. This approach is also analogous to the way humans conduct intelligence; we often resort to search engines, dictionaries, or information from other people in order to navigate the world.</p><p>Another benefit of the external attention is that, as the related knowledge is stored outside of the model, practitioners can easily update the knowledge source to change the behavior of their models. For example, one could add or delete entries from a knowledge graph or rewrite certain paragraphs in Wikipedia. By explicitly representing knowledge, the decision process of the model becomes much more transparent and explainable.</p><p>In this paper, we use the commonsense reasoning task CommonsenseQA <ref type="bibr" target="#b44">(Talmor et al., 2019)</ref> as a case study in leveraging external attention to obtain and integrate information related to the input. Given a commonsense question and a choice, we retrieve knowledge from three external sources: a knowledge graph (ConceptNet), dictionary (Wiktionary) and labeled training data (Com-monsenseQA and 16 related QA datasets). The retrieved knowledge is directly appended to the input and sent to the language model with no revision to the underlying architecture. We show that with the proposed external attention, the accuracy of commonsense reasoning using a DeBERTaxxlarge model <ref type="bibr" target="#b19">(He et al., 2020)</ref> can be significantly boosted from 83.8% to 90.8% on the dev set, while fine-tuned large-scale models like GPT-3 can only achieve 73.0%. The ensembled version of our model, Knowledge External Attention for Reasoning (KEAR), reaches an accuracy of 93.4% on the dev set and 89.4% on the test set, surpassing human performance (88.9%) for the first time <ref type="bibr" target="#b44">(Talmor et al., 2019)</ref>.</p><p>The benefits of our approach extend beyond commonsense reasoning. First, the external attention dramatically reduces our system's dependence on large-scale models, i.e., achieving human parity with models up to 1.5B parameters. Second, the external information is obtained via computationally efficient methods, such as information retrieval and word matching, adding little computational cost to the main model. Third, the text-level concatenation of input and knowledge leads no change to the Transformer model, enabling existing systems to easily adopt this new external attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We first describe our external attention framework in Sec 2.1. Next, we describe our external knowledge sources in Sec 2.2. Last, we present additional modeling techniques for improving commonsense reasoning in Sec 2.3. We present empirical results of our techniques in Sec 3.</p><p>Problem Formulation. We focus on the multiple-choice question answering task in this paper, where the goal is to select the correct answer from a given list c 1 , c 2 , ..., c n for a commonsense question q. The output of the model is a distribution P on {1, 2, ..., n}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Attention</head><p>Self Attention. The majority of recent language models are based on the Transformer architecture <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref>. One of the most important components in Transformer is the self-attention mechanism, which can be formulated as</p><formula xml:id="formula_0">Q = H l W q , K = H l W k , V = H l W v , A = QK T √ d , H l+1 = softmax(A)V, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where H l ∈ R N ×d is the input hidden vectors to the l-th Transformer layer, W q , W k , W v ∈ R d×d are projection matrices, N is the input length and d is the hidden vector's dimension. The inputs to the first Transformer layer are usually the embeddings of the tokenized input text, denoted as</p><formula xml:id="formula_2">H 0 = X = [x 1 , x 2 , ..., x N ] 1 .</formula><p>In the multi-choice question answering context, the input text is a concatenation of the question and a specific choice.</p><p>External Attention. For commonsense question answering, the required information needed to answer the question is usually absent from the input. Thus, we need to integrate external knowledge into the model. In this work, we denote the extra knowledge in text format as</p><formula xml:id="formula_3">K = [x K 1 , x K 2 , ..., x K N k ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>There are many ways to integrate the external</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question &amp; Candidate</head><p>𝑬 <ref type="bibr">[𝑪𝑳𝑺]</ref> 𝑬 𝟎 𝑬 𝑵 … knowledge into the model. In this paper we concatenate the knowledge to the input text:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head><formula xml:id="formula_4">H 0 = [X; K] = [x 1 , ..., x N , x K 1 , ..., x K N k ].</formula><p>The advantage of this input-level integration is that the existing model architecture does not need to be modified. Then, applying self-attention on H 0 can make the model freely reason between the knowledge text and the question/choices, therefore equipping the model with enhanced reasoning capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Knowledge Retrieval</head><p>The knowledge to append to the input for external attention is crucial for getting the correct prediction. For commonsense reasoning, we collect three external knowledge sources to complement the input questions and choices.</p><p>Knowledge Graph. Knowledge graphs (KG) contain curated facts that can help with commonsense reasoning. We follow KCR (Lin, 2020) to retrieve a relevant relation triple in the ConceptNet graph <ref type="bibr" target="#b43">(Speer et al., 2017)</ref>. Suppose the question entity is e q and the choice contains entity e c<ref type="foot" target="#foot_1">2</ref> . If there is a direct edge r from e q to e c in ConceptNet, we choose this triple (e q , r, e c ). Otherwise, we retrieve all the triples originating from e c . We score each triple j by the product of its confidence w j (provided by ConceptNet) and the defined relation type weight t r j :</p><formula xml:id="formula_5">s j = w j • t r j = w j • N Nr j</formula><p>, where r j is the relation type of j, N is the total number of triples originating from e c , N r j is the number of triples with relation r j among these triples. We then choose the triple with highest weight. Finally, if the selected triple is (e 1 , r, e 2 ), we format the knowledge from the KG as</p><formula xml:id="formula_6">K KG = [e 1 r e 2 ].</formula><p>Dictionary. Although pre-trained language models are exposed to large-scale text data, the long tail distribution of words means that the quality of a word's representation is highly dependent on that word's frequency in the pre-training corpus.</p><p>Dictionaries, on the other hand, can provide accurate semantic explanation of words regardless of their frequency in datasets. To help understand key concepts in the question and answer, we follow DEKCOR <ref type="bibr" target="#b50">(Xu et al., 2021)</ref> to use the Wiktionary definitions of the question and answer concepts as external knowledge. For every concept, we fetch the first (most frequent) definition from Wiktionary using its closest lexical match. Let d q be the definition text for e q and d c be the definition text for e c , we format the dictionary knowledge as</p><formula xml:id="formula_7">K dict = [e q : d q ; e c : d c ].</formula><p>Training Data. Although recent language models are giant in terms of the number of parameters, recent studies show that they cannot perfectly memorize all the details of their training data (Anonymous, 2022).</p><p>To tackle this challenge, we propose to retrieve relevant questions and answers from the training data as additional knowledge. We use BM25 <ref type="bibr" target="#b41">(Schütze et al., 2008)</ref> to retrieve top M relevant questions and answers from the training data. We build the query and index using the concatenation of question, ConceptNet triples and Wiktionary definitions. For each retrieved question from the training data, we drop the knowledge part and employ the retrieved question and its ground-truth answer as external knowledge. During training, for query x, we filter itself from the retrieved results to avoid data leakage. Suppose the retrieved questions and answers are {(x 1 , c 1 ), (x 2 , c 2 ), ..., (x M , c M )}, we format the knowledge from training data as</p><formula xml:id="formula_8">K train = [x 1 c 1 ; x 2 c 2 ; • • • ; x M c M ].</formula><p>Different from <ref type="bibr" target="#b0">Anonymous (2022)</ref> where the retrieval questions are only obtained from the same dataset, we experiment with three sources of training data for retrieval: i) CSQA training data, ii) CSQA+OBQA+RiddleSense, a small collection of datasets focusing on ConceptNet knowledge, and iii) a pool of 17 datasets focusing on commonsense reasoning (we describe details of these 17 datasets in the Appendix).</p><p>Finally, we concatenate the retrieved knowledge from our three sources to form a final knowledge input: K = [K KG ; K dict ; K train ]. In practice, the semicolon is replaced by the separator token (e.g., <ref type="bibr">[SEP]</ref>). We name our knowledge retrieval and integration technology as Knowledge External Attention for Reasoning (KEAR), shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">General Methods to Improve Commonsense Reasoning</head><p>Prior works have proposed other methods to improve general NLU performance, and it is therefore natural to wonder if these methods also works for commonsense reasoning. Here, we explore two general methods for improving commonsense reasoning performance: i) using different text encoders and ii) virtual adversarial learning.</p><p>Text Encoders. Previous methods for natural language understanding (NLU) <ref type="bibr" target="#b50">(Xu et al., 2021;</ref><ref type="bibr" target="#b15">Yan et al., 2020;</ref><ref type="bibr" target="#b48">Wang et al., 2020;</ref><ref type="bibr">Khashabi et al., 2020)</ref> have tried using BERT <ref type="bibr" target="#b12">(Devlin et al., 2018)</ref>, RoBERTa <ref type="bibr" target="#b31">(Liu et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b26">(Lan et al., 2019)</ref>, T5 <ref type="bibr" target="#b37">(Raffel et al., 2019)</ref>, ELECTRA <ref type="bibr" target="#b10">(Clark et al., 2020)</ref> and DeBERTa <ref type="bibr" target="#b19">(He et al., 2020)</ref> as the text encoder, achieving state-of-the-art performance on the GLUE benchmark <ref type="bibr" target="#b47">(Wang et al., 2019)</ref>. Thus, we evaluate these models as encoders for the commonsense reasoning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual Adversarial Training (VAT).</head><p>Previous works show that virtual adversarial training (VAT, Miyato et al. ( <ref type="formula">2018</ref>)) can improve the performance for general NLU and question answering tasks <ref type="bibr" target="#b22">(Jiang et al., 2020;</ref><ref type="bibr" target="#b9">Cheng et al., 2021)</ref>. In the multiple-choice commonsense reasoning task, the goal is to minimize the cross-entropy loss:</p><formula xml:id="formula_9">min θ E (x,y)∼D [CE(f (x; θ), y)]<label>(2)</label></formula><p>where f produces the model prediction (distribution P on the choices), θ represents the model parameters, y is the one-hot ground-truth answer vector, CE is cross entropy, and D is the empirical data distribution. VAT first finds the update δ that leads to the largest change in the predicted distribution, subject to a L p -norm constraint. Then, a consistency regularization loss term is added to minimize the difference in the function's output when compared to the input variation δ:</p><formula xml:id="formula_10">min θ E (x,y)∼D [CE(f (x; θ), y)+ (3) α max δ 2 ≤ε CE(f (x; θ), f (x + δ; θ))],<label>(4)</label></formula><p>where α and ε are hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup</head><p>Data. We focus on the CommonsenseQA (CSQA, <ref type="bibr" target="#b44">Talmor et al., 2019)</ref> benchmark. Common-senseQA is a widely used multiple-choice question answering dataset that requires commonsense knowledge. It contains 12k questions created using ConceptNet <ref type="bibr" target="#b43">(Speer et al., 2017)</ref>. For an edge (subject, relation, object) in ConceptNet, <ref type="bibr" target="#b44">Talmor et al. (2019)</ref> retrieves other object concepts with the same subject and relation as distractors for a question. A human worker is then asked to i) write a question containing the subject and with the object as the correct answer, ii) pick the most distractive answer from the retrieved concepts, and iii) write another distractor for the question. The final question contains 5 choices, with one correct choice, two random retrieved concepts, one human-picked concept, and one human-curated answer.</p><p>Model Setup. We feed the input text into a pretrained text encoder (e.g., DeBERTa) and take the representation v ∈ R d of the [CLS] token, where d is the dimension of the encoder. We set the segment id as 0 for the question and answer text, and 1 for the appended knowledge text. The final prediction is computed via softmax(v T b), where b ∈ R d is a parameter vector and the softmax is computed over all five choices for a question. We then minimize the cross entropy error during training.</p><p>Implementation Details. We finetune the model using the AdamW optimizer. The batch size is set to 48 or smaller to fit the batch on to a single GPU. We train the model for 10 epochs and take the best result on the dev set. We choose best the weight decay in {0, 0.01, 0.1}. The learning rate are chosen from {1e − 5, 2e − 5, 3e − 6} for all encoders except for DeBERTa; following the DeBERTa paper <ref type="bibr" target="#b19">(He et al., 2020)</ref> we use a smaller learning rate, chosen from {4e − 6, 6e − 6, 9e − 6}.</p><p>We use the DeBERTa v2 model from Huggingface Transformers <ref type="bibr" target="#b49">(Wolf et al., 2020)</ref>, and choose from the pretrained model or model finetuned on MNLI.</p><p>For VAT, we choose α ∈ {0.1, 1.0, 10.0} and set ε = 1e − 5. For VAT on DeBERTa-xxlarge, we follow SiFT <ref type="bibr" target="#b19">(He et al., 2020)</ref> that normalizes the word vectors before adding the perturbation δ, and set ε = 1e − 4. For knowledge from training data, we choose the best from the three retrieval source datasets. We run each experiment with 3 different seeds and present results from the best run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Effects of Individual Components</head><p>Effect of the Encoders. As shown in ing abilities on CommonsenseQA. Notice that the fine-tuned GPT-3 model with 175 billion parameters could only achieve 73.0% on the dev set of CommonsenseQA. Based on these results, we choose ELECTRA-large and DeBERTa variants <ref type="bibr" target="#b19">(He et al., 2020</ref><ref type="bibr" target="#b18">(He et al., , 2021) )</ref> as the encoders for subsequent experimentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Virtual Adversarial Training.</head><p>Table <ref type="table" target="#tab_2">2</ref> shows that VAT can improve commonsense reasoning accuracy for of the models under consideration. ELECTRA-large exhibits the largest increase in accuracy (0.8%). Thus, we apply VAT to ELECTRA-large for the following experiments.</p><p>Effect of External Attention. As shown in Table 3, all of the proposed knowledge sources bring gains in commonsense reasoning accuracy across all base encoder models. The dictionary, knowledge graph and training data bring 0.5%, 2.1% and 2.5% improvement, respectively, when DeBERTaV3-large <ref type="bibr" target="#b18">(He et al., 2021)</ref>   <ref type="bibr" target="#b44">(Talmor et al., 2019)</ref>.  <ref type="bibr" target="#b50">(Xu et al., 2021;</ref><ref type="bibr" target="#b8">Chen et al., 2020;</ref><ref type="bibr" target="#b32">Lv et al., 2020)</ref>. <ref type="bibr" target="#b2">Bhakthavatsalam et al. (2020)</ref> combine the knowledge from ConceptNet, WordNet and other corpora to form 3.5M generic statements and show that this knowledge can help boost accuracy and explanation quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining the Techniques</head><p>Recently, there are approaches to generate facts from pretrained language models to complement missing facts in the external knowledge source. <ref type="bibr" target="#b5">Bosselut et al. (2019)</ref> and <ref type="bibr" target="#b21">Hwang et al. (2020)</ref> finetunes a pretrained model on ATOMIC for commonsense knowledge graph completion. <ref type="bibr" target="#b30">Liu et al. (2021)</ref> directly prompts the GPT-3 model <ref type="bibr">(Brown et al., 2020)</ref> to get knowledge for reasoning.</p><p>Beyond commonsense reasoning, external knowledge can also help boost performance on other language processing tasks like open domain question answering <ref type="bibr" target="#b53">(Yu et al., 2021)</ref>, relation classification <ref type="bibr" target="#b54">(Yu et al., 2020a)</ref>, dialog response generation <ref type="bibr" target="#b17">(Ghazvininejad et al., 2018)</ref>, conversational QA <ref type="bibr" target="#b36">(Qin et al., 2019</ref><ref type="bibr">), multilingual NLU (Fang et al., 2021</ref>) and text generation <ref type="bibr" target="#b55">(Yu et al., 2020b)</ref>. Compared with prior work that uses extra modules (e.g., GNNs) or extra models (e.g., GPT-3), our external attention framework is extremely lightweight. It operates via a combination of non-parametric retrieval and text concatenation, which we show is highly effective, able to surpass human parity on the CommonsenseQA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose external attention as a lightweight framework for retrieving and integrating external knowledge for language understanding. Compared with self-attention which benefits from ever increasing model sizes, external attention can bring related information from external sources to supplement the input. We demonstrate that this strategy can lead to considerable gains in performance with little additional computational cost. By leveraging knowledge from knowledge graphs, dictionaries and training data, we show that our technology, KEAR, achieves human parity on the Common-senseQA benchmark task for the first time. For future work, we will apply the technique to other NLP tasks to improve language model performance with external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>We use a combination of 17 datasets for our largestscale training data retrieval. The datasets include αNLI <ref type="bibr" target="#b1">(Bhagavatula et al., 2019)</ref>, SWAG <ref type="bibr" target="#b56">(Zellers et al., 2018)</ref>, RACE <ref type="bibr" target="#b25">(Lai et al., 2017)</ref> (we only use the middle-school subset), CODAH <ref type="bibr" target="#b7">(Chen et al., 2019</ref><ref type="bibr">), RiddleSense (Lin et al., 2021)</ref>, SciTail <ref type="bibr" target="#b24">(Khot et al., 2018)</ref>, Com2Sense <ref type="bibr" target="#b42">(Singh et al., 2021)</ref>, AI2 Science Questions <ref type="bibr">(Clark et al., 2019)</ref>, Wino-Grade <ref type="bibr" target="#b38">(Sakaguchi et al., 2019)</ref>, CommonsenseQA <ref type="bibr" target="#b44">(Talmor et al., 2019</ref><ref type="bibr">), CommonsenseQA2.0 (Talmor et al., 2021)</ref>, ASQ <ref type="bibr" target="#b16">(Fu et al., 2019)</ref>, OBQA <ref type="bibr" target="#b33">(Mihaylov et al., 2018)</ref>, PhysicalIQA <ref type="bibr" target="#b3">(Bisk et al., 2020)</ref>, SocialIQA <ref type="bibr" target="#b40">(Sap et al., 2019b)</ref>, CosmosQA <ref type="bibr" target="#b20">(Huang et al., 2019)</ref> and HellaSWAG <ref type="bibr" target="#b57">(Zellers et al., 2019)</ref>. We present details of the datasets that we use for training data retrieval in Table <ref type="table" target="#tab_6">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Task </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Retrieval Sources</head><p>We present results comparing different sources of training data retrieval in </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our proposed method of Knowledge External Attention for Reasoning (KEAR). Related knowledge is retrieved from external sources, e.g., knowledge graph, dictionary and training data, using the input as key and then integrated with the input. While additional external attention layers can be added to the Transformer blocks, we adopt text-level concatenation for external attention, incurring no structural change to the model architecture.</figDesc><graphic url="image-1.png" coords="3,246.12,303.32,62.14,75.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CSQA dev set accuracy for various encoders. We append the accuracy on MNLI dataset (in-domain) for each encoder as a reference. MNLI scores are from the corresponding GitHub repositories. 1 : from<ref type="bibr" target="#b30">Liu et al. (2021)</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on virtual adversarial training.</figDesc><table><row><cell></cell><cell>Dev Acc(%)</cell></row><row><cell>Baselines</cell><cell></cell></row><row><cell>ELECTRA-large</cell><cell>81.3</cell></row><row><cell>DeBERTa-xxlarge</cell><cell>83.8</cell></row><row><cell>DeBERTaV3-large</cell><cell>84.6</cell></row><row><cell>With VAT</cell><cell></cell></row><row><cell>ELECTRA-large + VAT</cell><cell>82.1</cell></row><row><cell>DeBERTa-xxlarge + SiFT</cell><cell>84.4</cell></row><row><cell>DeBERTaV3-large + VAT</cell><cell>85.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>is the base encoder model. We find that the best training data CSQA dev set results with different encoders and ensembles.retrieval source depends on the exact encoders, and we present a detailed comparison in the Appendix. This demonstrates the effectiveness of our proposed knowledge retrieval and concatenation methods.</figDesc><table><row><cell>Method</cell><cell cols="3">E-l+V D-xxl DV3-l</cell></row><row><cell>Base</cell><cell>82.1</cell><cell>83.8</cell><cell>84.6</cell></row><row><cell>+ KG</cell><cell>85.2</cell><cell>86.4</cell><cell>86.7</cell></row><row><cell>+ Dictionary</cell><cell>83.8</cell><cell>84.0</cell><cell>85.1</cell></row><row><cell>+ Training data</cell><cell>84.0</cell><cell>86.4</cell><cell>87.1</cell></row><row><cell cols="4">Table 3: Applying external attention to different knowl-</cell></row><row><cell cols="4">edge sources. E-l+V stands for ELECTRA-large with</cell></row><row><cell cols="4">VAT, D-xxl stands for DeBERTa-xxlarge, DV3-l stands</cell></row><row><cell>for DeBERTaV3-large.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>Dev Acc(%)</cell></row><row><cell cols="2">ELECTRA-large + KEAR</cell><cell></cell><cell>88.7</cell></row><row><cell cols="2">DeBERTa-xlarge + KEAR</cell><cell></cell><cell>89.5</cell></row><row><cell cols="2">DeBERTa-xxlarge + KEAR</cell><cell></cell><cell>90.8</cell></row><row><cell cols="2">DeBERTaV3-large + KEAR</cell><cell></cell><cell>91.2</cell></row><row><cell cols="3">Ensemble (39 models w/ KEAR)</cell><cell>93.4</cell></row><row><cell>Method</cell><cell cols="3">Single Ensemble</cell></row><row><cell>BERT+OMCS</cell><cell cols="2">62.5</cell><cell>-</cell></row><row><cell>RoBERTa</cell><cell cols="2">72.1</cell><cell>72.5</cell></row><row><cell cols="2">RoBERTa+KEDGN</cell><cell>-</cell><cell>74.4</cell></row><row><cell>ALBERT</cell><cell></cell><cell>-</cell><cell>76.5</cell></row><row><cell cols="3">RoBERTa+MHGRN 75.4</cell><cell>76.5</cell></row><row><cell>ALBERT + HGN</cell><cell cols="2">77.3</cell><cell>80.0</cell></row><row><cell>T5</cell><cell cols="2">78.1</cell><cell>-</cell></row><row><cell>UnifiedQA</cell><cell cols="2">79.1</cell><cell>-</cell></row><row><cell>ALBERT+KCR</cell><cell cols="2">79.5</cell><cell>-</cell></row><row><cell>ALBERT + KD</cell><cell cols="2">80.3</cell><cell>80.9</cell></row><row><cell>ALBERT + SFR</cell><cell></cell><cell>-</cell><cell>81.8</cell></row><row><cell>DEKCOR</cell><cell cols="2">80.7</cell><cell>83.3</cell></row><row><cell>Human</cell><cell></cell><cell>-</cell><cell>88.9</cell></row><row><cell>KEAR (ours)</cell><cell cols="2">86.1</cell><cell>89.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on test set from the leaderboard. The human performance is ensemble of 5 workers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows the results of KEAR, which com-</cell></row><row><cell>bines the best techniques in previous experiments,</cell></row><row><cell>i.e., VAT and external attention to all knowledge</cell></row><row><cell>sources, to further boost the performance. The</cell></row><row><cell>best single model (DeBERTaV3-large + KEAR)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The datasets used for training data retrieval. NLI stands for natural language inference, MC is multiple choice, MRC is machine reading comprehension, CLF is classification, NSP is next sentence prediction.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">#Train #Label</cell></row><row><cell>αNLI</cell><cell>NLI</cell><cell>170k</cell><cell>2</cell></row><row><cell>SWAG</cell><cell>MC</cell><cell>73.5k</cell><cell>4</cell></row><row><cell cols="2">RACE-Middle MRC</cell><cell>87.9k</cell><cell>4</cell></row><row><cell>CODAH</cell><cell>MC</cell><cell>1672</cell><cell>4</cell></row><row><cell>RiddleSense</cell><cell>MC</cell><cell>3512</cell><cell>5</cell></row><row><cell>SciTail</cell><cell>NLI</cell><cell>23.6k</cell><cell>2</cell></row><row><cell>Com2Sense</cell><cell>MC</cell><cell>808</cell><cell>2</cell></row><row><cell>AI2Science</cell><cell>MC</cell><cell>1232</cell><cell>4</cell></row><row><cell>WinoGrade</cell><cell cols="2">CoRef 40.4k</cell><cell>2</cell></row><row><cell>CSQA</cell><cell>MC</cell><cell>9741</cell><cell>5</cell></row><row><cell>CSQA2.0</cell><cell>CLF</cell><cell>9264</cell><cell>2</cell></row><row><cell>ASQ</cell><cell>MC</cell><cell>8872</cell><cell>2</cell></row><row><cell>OBQA</cell><cell>MC</cell><cell>4960</cell><cell>4</cell></row><row><cell>PhysicalIQA</cell><cell>MC</cell><cell>16.1k</cell><cell>2</cell></row><row><cell>SocialIQA</cell><cell>MC</cell><cell>33.4k</cell><cell>3</cell></row><row><cell>CosmosQA</cell><cell>MRC</cell><cell>25.3k</cell><cell>4</cell></row><row><cell>HellaSWAG</cell><cell>NSP</cell><cell>39.9k</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>The best choice of retrieval source varies with encoders and the techniques applied; in general the 17-dataset pool achieve the best performance for DeBERTa, but for ELECTRA retrieving from the CSQA training set alone can get the best performance.</figDesc><table><row><cell>Model</cell><cell cols="3">CSQA 3-Data 17-Data</cell></row><row><cell>E-l+V</cell><cell>84.0</cell><cell>82.9</cell><cell>82.8</cell></row><row><cell>D-xxl</cell><cell>86.2</cell><cell>86.1</cell><cell>86.4</cell></row><row><cell>DV3-l</cell><cell>87.0</cell><cell>87.1</cell><cell>87.1</cell></row><row><cell>E-l+V, best</cell><cell>88.5</cell><cell>88.2</cell><cell>87.1</cell></row><row><cell>D-xxl, best</cell><cell>89.8</cell><cell>90.5</cell><cell>90.8</cell></row><row><cell>DV3-l, best</cell><cell>91.0</cell><cell>91.2</cell><cell>91.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Performance on CSQA dev set of model w.r.t source of training data retrieval. "Best" means our best model combining all the techniques. E-l+V stands for ELECTRA-large with VAT, D-xxl stands for DeBERTa-xxlarge, DV3-l stands for DeBERTaV3large.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We do not differentiate between tokens and their embeddings in the following discussion. Following previous work, we prepend a [CLS] token to the input.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">In CommonsenseQA dataset, both eq and ec are provided. Otherwise, we can use entity linking to find related knowledge graph nodes to the input text.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers for their comments on the paper of our earlier system DECKOR. We thank Reid Pryzant for proof-reading the paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Training data is more valuable than you think: A simple and effective method by retrieving from training data</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Under review</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05739</idno>
		<title level="m">Abductive commonsense reasoning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Genericskb: A knowledge base of generic statements</title>
		<author>
			<persName><forename type="first">Sumithra</forename><surname>Bhakthavatsalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Anastasiades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00660</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Michael S Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05317</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Codah: An adversarially-authored question answering dataset for common sense</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike D'</forename><surname>Arcy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for NLP</title>
				<meeting>the 3rd Workshop on Evaluating Vector Space Representations for NLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="63" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving commonsense question answering by graph-based iterative retrieval over multiple knowledge sources</title>
		<author>
			<persName><forename type="first">Qianglong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.232</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
				<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2583" to="2594" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Posterior differential regularization with f-divergence for improving model robustness</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lis</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.85</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1078" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Electra: Pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dalvi</forename><surname>Bhavana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01958</idno>
	</analytic>
	<monogr>
		<title level="j">Niket Tandon</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2019. From&apos;f&apos;to&apos;a&apos;on the ny regents science exams: An overview of the aristo project</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
				<meeting><address><addrLine>Bert</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<imprint>
			<publisher>Matthias</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Georg</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<idno>arXiv:2110.08462</idno>
	</analytic>
	<monogr>
		<title level="m">Yuwei Fang, Shuohang Wang, Yichong Xu, Ruochen Xu, Siqi Sun, Chenguang Zhu, and Michael Zeng. 2021. Leveraging knowledge in multilingual commonsense reasoning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yanlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00646</idno>
		<title level="m">Scalable multi-hop relational reasoning for knowledgeaware question answering</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Asking the right question: Inferring advice-seeking intentions from personal narratives</title>
		<author>
			<persName><forename type="first">Liye</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01587</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A knowledge-grounded neural conversation model</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09543</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deberta: Decoding-enhanced bert with disentangled attention</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cosmos QA: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs</title>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05953</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smart: Robust and efficient fine-tuning for pretrained natural language models through principled regularized optimization</title>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2177" to="2190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<idno>arXiv:2005.00700</idno>
		<title level="m">Unifiedqa: Crossing format boundaries with a single qa system</title>
				<editor>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Scaling laws for neural language models</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SciTail: A textual entailment dataset from science question answering</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02151</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge</title>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Ho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP 2021): Findings</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics (ACL-IJCNLP 2021): Findings</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Knowledge chosen by relations</title>
		<ptr target="https://github.com/jessionlin/csqa/blob/master/Model_details.md" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alisa</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.08387</idno>
		<title level="m">Generated knowledge prompting for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph-based reasoning over heterogeneous external knowledge for commonsense question answering</title>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semisupervised learning</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conversing by reading: Contentful neural conversation with on-demand machine reading</title>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>William B Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5427" to="5436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Winogrande: An adversarial winograd schema challenge at scale</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10641</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Social iqa: Commonsense reasoning about social interactions</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hannah Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prabhakar</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge University Press Cambridge</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">COM2SENSE: A commonsense reasoning benchmark with complementary sentences</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Te-Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.78</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
				<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="883" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI conference on artificial intelligence (AAAI)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Commonsenseqa: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>North American Chapter</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">CommonsenseQA 2.0: Exposing the limits of AI through gamification</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In the Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Connecting the dots: A knowledgeable path generator for commonsense question answering</title>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fusing context into knowledge graph for commonsense question answering</title>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrigank</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Handong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nedim</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12873</idno>
		<title level="m">Learning contextualized knowledge structures for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Qagnn: Reasoning with language models and knowledge graphs for question answering</title>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06378</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04330</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Jaket: Joint pre-training of knowledge graph and language understanding</title>
		<author>
			<persName><forename type="first">Donghan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00796</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaitang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04389</idno>
		<title level="m">A survey of knowledge-enhanced text generation</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Swag: A large-scale adversarial dataset for grounded commonsense inference</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">HellaSwag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1472</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4791" to="4800" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
