<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An End-to-End Compression Framework Based on Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feng</forename><surname>Jiang</surname></persName>
							<email>fjiang@hit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Wen</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
							<email>shliu@hit.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xun</forename><surname>Guo</surname></persName>
							<email>xunguo@microsoft.com</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Debin</forename><surname>Zhao</surname></persName>
							<email>dbzhao@hit.edu.cn.dr.liu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Tech-nology</orgName>
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<postCode>150001</postCode>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An End-to-End Compression Framework Based on Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">52F784C33A96492ED8365DCAC565BFA9</idno>
					<idno type="DOI">10.1109/TCSVT.2017.2734838</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 2 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 3 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 4 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 5</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>compression framework</term>
					<term>compact representation</term>
					<term>convolutional neural networks (CNNs)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning, e.g., convolutional neural networks (CNNs), has achieved great success in image processing and computer vision especially in high level vision applications such as recognition and understanding. However, it is rarely used to solve low-level vision problems such as image compression studied in this paper. Here, we move forward a step and propose a novel compression framework based on CNNs. To achieve high-quality image compression at low bit rates, two CNNs are seamlessly integrated into an end-to-end compression framework. The first CNN, named compact convolutional neural network (ComCNN), learns an optimal compact representation from an input image, which preserves the structural information and is then encoded using an image codec (e.g., JPEG, JPEG2000 or BPG). The second CNN, named reconstruction convolutional neural network (RecCNN), is used to reconstruct the decoded image with high-quality in the decoding end. To make two CNNs effectively collaborate, we develop a unified end-to-end learning algorithm to simultaneously learn ComCNN and RecCNN, which facilitates the accurate reconstruction of the decoded image using RecCNN. Such a design also makes the proposed compression framework compatible with existing image coding standards. Experimental results validate that the proposed compression framework greatly outperforms several compression frameworks that use existing image coding standards with state-of-the-art deblocking or denoising post-processing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, image compression attracts increasing interest in image processing and computer vision due to its potential applications in many vision systems. The aim of image compression is to reduce irrelevance and redundancy of an image in order to store or transmit the image at low bit rates <ref type="bibr" target="#b0">[1]</ref>. Traditional image coding standards <ref type="bibr" target="#b1">[2]</ref> (such as JPEG and JPEG2000) attempt to distribute the available bits for every nonzero quantized transform coefficient in the whole image. While the compression ratio increases, the bits per pixel (BPP) decreases as a result of the use of bigger quantization steps, which will cause the decoded image to have blocking artifacts or noises. To overcome this problem, a lot of efforts have been devoted to improving the quality of the decoded image using a post-processing deblocking or denoising method. Zhai et al. <ref type="bibr" target="#b2">[3]</ref> propose an effective deblocking method for JPEG images through post-filtering in shifted windows of image blocks. Foi et al. <ref type="bibr" target="#b3">[4]</ref> develop an image deblocking filtering based on shape-adaptive DCT, in conjunction with the anisotropic local polynomial approximation-intersection of confidence intervals technique. Inspired by the success of nonlocal filters and bilateral filters for image debolcking, several nonlocal filters have been proposed for image deblocking <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Recently, Zhang et al. <ref type="bibr" target="#b7">[8]</ref> propose a constrained non-convex low-rank model for image deblocking. Although desired performance is achieved, these post-processing methods are very time-consuming because solving the optimal solutions involves computationally expensive iterative processes. Therefore, it is difficult to apply them to practical applications.</p><p>Motivated by the excellent performance of convolutional neural networks (CNNs) in low level computer vision <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref> in recent years and the fact that existing image codecs are extensively Meanwhile, the RecCNN handle the task of reconstructing the decoded image. Co(•) represents an image codec, encoding and decoding the compact representation produced by the ComCNN, which consists of a transform coding stage followed by quantization and entropy coding. In this work, JPEG, JPEG2000 and BPG are adopted. used across the world, we propose an end-to-end compression framework, which consists of two CNNs and an image codec as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. The first CNN, named compact convolutional neural network (ComCNN), learns an optimal compact representation from an input image, which is then encoded using an image codec (e.g., JPEG, JPEG2000 or BPG). The second CNN, named reconstruction convolutional neural network (RecCNN), is used to reconstruct the decoded image with high quality in the decoding end. Existing image coding standards usually consists of transformation, quantization and entropy coding. Unfortunately, the rounding function in quantization is not differentiable, which brings great challenges to train deep neural networks when performing the backpropagation algorithm. To address this problem, we present a simple but effective learning algorithm to train the proposed end-to-end compression framework by simultaneously learning ComCNN and RecCNN to facilitate the accurately reconstruction of the decoded image using RecCNN. An example of image compression is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, from which we can see that the proposed framework achieves much better quality with more visual details. In addition, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the compact representation obtained by ComCNN preserves the structural information of the image, therefore, an image codec can be effectively utilzed to compress the compact representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The contributions of this work are summarized as follows:</head><p>The restoration oriented methods regard the compression operation as a distortion process and reduce artifacts by restoring the clear images. Sun et al. <ref type="bibr" target="#b14">[15]</ref> model the quantization distortion as Gaussian noises and use field of experts as image priors to restore the images. Zhang et al. <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> propose to utilize similarity priors of image blocks to reduce compression artifacts by estimating the transform coefficients of overlapped blocks from non-local blocks. Recently, Zhang et al. <ref type="bibr" target="#b7">[8]</ref> develop a novel algorithm for image deblocking using a constrained non-convex lowrank model, which formulates image deblocking as an optimization problem within maximum a posteriori framework.</p><p>In the aforementioned methods, image prior models play important roles in both the deblocking oriented and restoration oriented methods. However, these methods involve computationally expensive iterative processes when solving the optimal solutions with complex formula derivations. Therefore, they may be not suitable for practical applications. In short, all the related methods reviewed above attempt to improve image quality only from the perspective of image postprocessing. In other words, the connection between the encoder front-end processing and the decoder back-end processing is ignored. We attempt to jointly optimize the encoder and decoder joint optimization to improve the compression performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image Super-Resolution Based on Deep Learning</head><p>Recently, CNNs have been used successfully for image super-resolution (SR) especially when residual learning <ref type="bibr" target="#b17">[18]</ref> and gradients-based optimization algorithms <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> are proposed to train deeper network efficiently. Dong et al. propose a CNN based SR method <ref type="bibr" target="#b21">[22]</ref> named SRCNN, which consists of three layers: patch extraction, non-linear mapping and reconstruction. Although Dong et al. conclude in their paper that deeper networks do not result in better performance in some cases, other researchers argue that increasing depth significantly boosts performance. For example, VDSR <ref type="bibr" target="#b22">[23]</ref> shows a significant improvement in accuracy, which uses 20 weight layers. DRCN <ref type="bibr" target="#b23">[24]</ref> has a very deep recursive layer (up to 16 recursions) and outperforms previous methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Compression Based on Deep Learning</head><p>Recently, deep learning has been used both for lossy and lossless image compression and achieved competitive performance. For the lossy image compression, Toderici et al. <ref type="bibr" target="#b24">[25]</ref>  on convolutional and deconvolutional LSTM recurrent networks. Further, Toderici et al. <ref type="bibr" target="#b25">[26]</ref> proposed a neural network which is competitive across compression rates on images of arbitrary size. For a given compression rate, both methods learn the compression models by minimizing the distortion. Theis et al. <ref type="bibr" target="#b26">[27]</ref> propose compressive autoencoders, which uses a smooth approximation of the discrete of the rounding function and upper-bound the discrete entropy rate loss for continuous relaxation. Ballé et al. <ref type="bibr" target="#b27">[28]</ref> make use a generalized divisive normalization (GDN) for joint nonlinearity and replace rounding quantization with additive uniform noise for continuous relaxation. Li et al. <ref type="bibr" target="#b28">[29]</ref> proposed a content-weighted compression method with the importance map of image. For the lossless image compression, the methods proposed by Theis et al. <ref type="bibr" target="#b29">[30]</ref> and van den Oord et al. <ref type="bibr" target="#b30">[31]</ref> achieves state-of-the-art results.</p><p>Overall, although the image compression methods based on deep learning achieve competitive performance, they ignored the compatibility with existing image codecs, which limits their use in some existing systems. In contrast, the proposed compression framework takes into account both compression performance and compatibility with existing image codecs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED COMPRESSION FRAMEWORK</head><p>In this section, we first introduce the architecture of the proposed compression framework and then present the detailed learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Architecture of End-to-End Compression Framework</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Algorithm</head><p>According to the proposed architecture, both ComCNN and RecCNN try to make the reconstructed image as similar as possible to the original image. Therefore, the end-to-end optimization goal can be formulated as Unfortunately, the Co(•) in Eq.( <ref type="formula" target="#formula_0">1</ref>) involves a rounding function, which is not differentiable when performing the back propagation algorithm. To solve this problem, we designed an iterative optimization learning algorithm. By fixing θ 2 , we can get θ1 = arg min</p><formula xml:id="formula_0">θ1 , θ2 = arg min θ 1 ,θ 2 Re (θ 2 , Co (Cr (θ 1 , x))) -x 2 ,<label>(1)</label></formula><formula xml:id="formula_1">θ 1 Re θ2 , Co (Cr (θ 1 , x)) -x 2 ,<label>(2)</label></formula><p>and by fixing θ 1 , we can obtain θ2 = arg min </p><p>After combining Eq.( <ref type="formula" target="#formula_2">4</ref>) and Eq.( <ref type="formula">3</ref>), we can obtain θ2 = arg min</p><formula xml:id="formula_3">θ 2 Re (θ 2 , xm ) -x 2 . (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>2) Updating the Parameters θ 1 of ComCNN: From Eq.( <ref type="formula" target="#formula_1">2</ref>), we can see that it is not a trivial task to obtain the optimal θ 1 since the Co(•) is an inherently non-differentiable operation when performing back propagation. To solve this problem, we define an auxiliary variable x * m as the optimal input of RecCNN:</p><formula xml:id="formula_5">x * m = arg min xm Re θ2 , xm -x 2 .<label>(6)</label></formula><p>Here we make a reasonable and general assumption that Re θ2 , • is monotonic with respect to x * m , which can be expressed as</p><formula xml:id="formula_6">τ -x * m 2 ≥ ϕ -x * m 2 , if and only if Re θ2 , τ -x 2 ≥ Re θ2 , ϕ -x 2 . (<label>7</label></formula><formula xml:id="formula_7">)</formula><p>Let θ1 be the solution of arg min</p><formula xml:id="formula_8">θ 1</formula><p>Co (Cr (θ, x)) -xm 2 , i.e., for any possible θ 1 , it satisfies that</p><formula xml:id="formula_9">Co (Cr (θ 1 , x)) -x * m 2 ≥ Co Cr θ1 , x -x * m 2 .<label>(8)</label></formula><p>Following assumption <ref type="bibr" target="#b6">(7)</ref>, we can obtain that</p><formula xml:id="formula_10">Re θ2 , Co (Cr (θ 1 , x)) -x 2 ≥ Re θ2 , Co Cr θ1 , x -x 2 .<label>(9)</label></formula><p>Accordingly, θ1 = arg min</p><formula xml:id="formula_11">θ 1 Re θ2 , Co (Cr (θ 1 , x)) -x 2 . (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>Combining with Eq.( <ref type="formula" target="#formula_1">2</ref>), we can get θ1 = θ1 , which is Since Co(•) is an codec, a reasonable solution of Eq.( <ref type="formula" target="#formula_15">12</ref>) is θ1 ≈ arg min</p><formula xml:id="formula_13">θ1 = arg min θ 1 Co (Cr (θ 1 , x)) -x * m 2 . (<label>11</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">θ 1 Cr (θ 1 , x) -x * m 2 . (<label>12</label></formula><formula xml:id="formula_16">)</formula><p>Combine Eq.( <ref type="formula" target="#formula_17">13</ref>) and the assumption (7) above, it arrives θ1 = arg min</p><formula xml:id="formula_17">θ 1 Re θ2 , Cr (θ 1 , x) -x 2 . (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Here, we get Eq.( <ref type="formula" target="#formula_17">13</ref>) with a reasonable assumption and rigorous derivations, which is the approximation of Eq.( <ref type="formula" target="#formula_1">2</ref>). In this paper, we use Eq.( <ref type="formula" target="#formula_17">13</ref>) to train ComCNN instead of Eq.( <ref type="formula" target="#formula_1">2</ref>).</p><p>We can obtain the optimal θ 1 and θ 2 by iteratively optimizing Eq.( <ref type="formula" target="#formula_3">5</ref>) and Eq.( <ref type="formula" target="#formula_17">13</ref>), respectively.</p><p>In light of all derivations above, the complete description of the proposed algorithm is given in Algorithm 1. </p><formula xml:id="formula_19">L 2 (θ 2 ) = 1 2N N k=1 res (Co (x m k ) , θ 2 ) -(Co (x m k ) -x k ) 2 , (<label>15</label></formula><formula xml:id="formula_20">)</formula><p>where θ 2 represents the trainable parameter. res(•) represents the residual learned by RecCNN.</p><p>Clearly, it looks somewhat different from Eq.( <ref type="formula" target="#formula_3">5</ref>), but they are not contradictory. Actually, they are essentially identical, and Eq.( <ref type="formula" target="#formula_19">15</ref>) is just expresses Eq.( <ref type="formula" target="#formula_3">5</ref>) as the form of the residual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To evaluate the performance of the proposed compression framework, we conduct experimental comparisons against standard compression methods (e.g., JPEG, JPEG 2000 and BPG) with a post-processing deblocking or denoising method. Five representative image deblocking methods, i.e. Sun's <ref type="bibr" target="#b14">[15]</ref>, DicTV <ref type="bibr" target="#b33">[34]</ref>, Zhang's <ref type="bibr" target="#b16">[17]</ref>, Ren's [35], Zhang's <ref type="bibr" target="#b7">[8]</ref> and two representative image denoising methods, i.e. BM3D <ref type="bibr" target="#b35">[36]</ref> and WNNM <ref type="bibr" target="#b36">[37]</ref> are chosen due to their state-of-the-art performance. Moreover, ARCNN <ref type="bibr" target="#b8">[9]</ref> is also chosen since it is a landmark deblocking method based on deep learning and achieves the state-of-the-art performance. Meanwhile, in order to demonstrate the effectiveness of ComCNN, we remove ComCNN in the framework and just using</p><p>RecCNN to reconstruct the decoded image. Similarly, we remove the RecCNN to examine the effect of ComCNN using bicubic interpolation to obtain the reconstructed image of the same size as the original image. The results of all the compared methods are obtained by running the source codes of the original authors with the optimal parameters. Through this section, we use the name of the post-processing method to denote a compared method.</p><p>We use the MatConvNet package <ref type="bibr" target="#b37">[38]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Initialization</head><p>We initialize the weights of ComCNN using the method in <ref type="bibr" target="#b39">[40]</ref> and use Adam algorithm <ref type="bibr" target="#b20">[21]</ref> with α = 0.001, β 1 = 0.9, β 2 = 0.999 and ε = 10 -8 . We train ComCNN for 50 epochs using a batch size of 128. The learning rate is decayed exponentially from 0.01 to 0.0001 for 50 epochs.</p><p>The weights initialization and gradient updating of RecCNN is the same as ComCNN. RecCNN is also trained for 50 epochs using the same batch size with ComCNN. The learning rate is decayed exponentially from 0.1 to 0.0001 for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results</head><p>In our experiments, we set different quality factors (QF) to achieve the same bits per pixel (bpp) for both the proposed and compared methods. For the proposed method, we first manually adjust the QF for the compression of compact representation by JPEG to achieve almost the same bpp with the compared image enhancement methods. Then we compare the PSNR and SSIM of the proposed method with the compared methods. The comparison results for all test images with QF = 5 and QF = 10 are provided in Table <ref type="table">I</ref> and Table <ref type="table">II</ref>, respectively, where the best results are highlighted in bold.</p><p>As seen from Table <ref type="table">I</ref> and Table <ref type="table">II</ref>, in the case of QF = 5, the proposed compression framework achieves 1.20dB gains in PSNR and 0.0227 gains in SSIM compared against Zhang's <ref type="bibr" target="#b7">[8]</ref>, which is state-of-the-art in the compared methods. It is worth mentioning that the proposed framework outperforms all the compared image enhancement methods including ARCNN <ref type="bibr" target="#b8">[9]</ref>, which is a milestone based on CNN. Meanwhile, in the case of QF = 10, the proposed compression framework achieves 0.43dB and 0.0067 gains in PSNR and SSIM, respectively, compared against ARCNN <ref type="bibr" target="#b8">[9]</ref>. The visual quality comparisons in the case of QF = 5 for Lena is provided in Fig. <ref type="figure" target="#fig_10">6</ref>.</p><p>We can see that the blocking artifacts are obvious in the image decoded directly by the standard JPEG. DicTV <ref type="bibr" target="#b33">[34]</ref>, Sun's <ref type="bibr" target="#b14">[15]</ref>, WNNM <ref type="bibr" target="#b36">[37]</ref> and BM3D <ref type="bibr" target="#b35">[36]</ref> remove the artifacts partially, but there are still some artifacts visible in the reconstructed image. Zhang's <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr">Ren's [35]</ref> generate better results than Sun's <ref type="bibr" target="#b14">[15]</ref> and BM3D <ref type="bibr" target="#b35">[36]</ref>. However, the blur effects along the edges are generated at the same time. Zhang's <ref type="bibr" target="#b7">[8]</ref> achieves a better PSNR and SSIM, but it makes the image over-smoothing and discards some details in image edges. ARCNN <ref type="bibr" target="#b8">[9]</ref> and ReCNN achieve better visual quality than other compared methods. The proposed compression framework not only removes most of the artifacts significantly, but also preserves more details on both edges and textures than all the compared methods. In order to verify the effect of We also evaluate our framework on JPEG 2000 and BPG (Better Portable Graphics) 4 and achieve excellent performance. BPG compression is based on the High Efficiency Video Coding (HEVC), which is considered as a major advance in compression techniques. For JPEG2000, we test the proposed compression framework at different bit rates (from 0.1bpp to 0.4bpp) and compare it with JPEG 2000.  .</p><p>in terms of both PSNR and SSIM. For bpp from 0.1 to 0.4, the proposed framework achieves on average 3.06dB, 2.45dB, 1.34dB, 1.09dB and 0.1047, 0.0709, 0.0525, 0.0435 gains in PSNR and SSIM compared against JPEG2000. In Fig. <ref type="figure" target="#fig_12">8</ref>, one can see that the proposed compression framework achieves much better subjective performance than JPEG2000, especially at very low bit rate. Our framework preserves more high-frequency information and recovers sharp edges and pure textures in the reconstructed image. For BPG, we test the BPG codec at QP (quality  <ref type="table" target="#tab_6">IV</ref>. One can see that, if we treat RecCNN as a post-processing method, RecCNN achieves on average 0.81dB and 0.0168 gains in PSNR and SSIM. And the proposed compression framework achieves on average 0.99dB and 0.0218 gains in PSNR and SSIM while saving 5.22% bit-rates. It is worth noting that the performance of our proposed compression framework on BPG is not so obvious on JPEG and JPEG2000 when compared, because BPG is already a very good compression method, which might not be significantly improved further.</p><p>In order to show the effectiveness of the proposed compression framework, we test our method on Set5 <ref type="bibr" target="#b22">[23]</ref>, Set14 <ref type="bibr" target="#b22">[23]</ref>, LIVE1 <ref type="bibr" target="#b40">[41]</ref> and General-100 <ref type="bibr" target="#b41">[42]</ref> datasets. It is worth mentioning that the General-100 dataset contains 100 bmp-format images with no compression, which are very suitable for compression task. Results are shown in Table <ref type="table" target="#tab_8">VI</ref>, from which we can see that the performance of our proposed compression exceeds JPEG and JPEG2000 by a larger margin for all four testing datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Running Time</head><p>The running time of all compared methods when dealing with a 256 × 256 grayscale image in CPU or GPU are shown in Table <ref type="table" target="#tab_7">V</ref>. It should be noted that it is not possible to test the running time in GPU for all other compared methods. As we can see from Table <ref type="table" target="#tab_7">V</ref>, the proposed framework needs only 1.56s and 0.017s in CPU and GPU, respectively. Our compression framework is faster than other post-processing methods. In addition, we also calculate the running time of our sub-network RecCNN, which almost takes the entire running time of our method. Because RecCNN has 20 layers, which is much deeper than ComCNN with only 3 layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we propose an effective end-to-end compression framework based on two CNNs, one of which is used to produce compact intermediate representation for encoding using an image encoder. The other CNN is used to reconstruct the decoded image with high quality.</p><p>These two CNNs collaborate each other and are trained using a unified optimization method.</p><p>Experimental results demonstrate that the proposed compression framework achieves state-ofthe-art performance and is much faster than most post-processing algorithms. Our work indicates that the performance of the proposed compression framework can be significantly improved by applying the proposed framework, which will inspire other researchers to design better deep neural networks for image compression along this orientation.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Left: the JPEG-coded image (PSNR = 27.33 dB) with QF = 5 , where we could see blocking artifacts,ring effects and blurring on the eyes, abrupt intensity changes on the face. Right: the decoded image (PSNR = 31.14 dB) by our proposed compression framework at the same bit rate with left, where the compression artifacts vanished and generate more details.</figDesc><graphic coords="2,193.68,66.56,224.63,108.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed novel compression framework. The ComCNN preserves more useful information for reconstruction.</figDesc><graphic coords="3,142.20,66.56,327.60,179.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 ,</head><label>3</label><figDesc>, the proposed compression framework consists of two CNNs and an image codec. The compact representation CNN (ComCNN) is used to generate a compact representation of the input image for the encoding, which preserves structural information of the image and therefore facilitates the accurate reconstruction of high-quality images. The reconstruction CNN (RecCNN) is used to enhance the quality of the decoded image. These two CNNs collaborate with each other and are optimized simultaneously to achieve high-quality image compression at low bit rates. 1) Compact Representation Convolutional Neural Network (ComCNN): As shown in Fig. ComCNN has 3 weight layers, which maintain the spatial structure of the original image and therefore facilitate the accurate reconstruction of the decoded image using RecCNN 1 . The</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The architecture of the proposed ComCNN and feature maps of different layers.Upscaled image is obtained by Bicubic interpolation on Decoded image.</figDesc><graphic coords="7,142.20,66.56,327.60,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The architecture of the proposed RecCNN and feature maps of different layers. Upscaled image is obtained by bicubic interpolation on decoded image. The network predicts a residual image, then the sum of the input and the residual gives the high quality output.</figDesc><graphic coords="8,102.42,66.56,407.16,170.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>where x represents the original image, θ 1 and θ 2 are the parameters of ComCNN and RecCNN, respectively. Cr(•) and Re() represent the ComCNN and RecCNN, respectively. Co(•) represents an image codec (e.g. JPEG, JPEG2000 or BPG). From this objective function, we can see that an original image x passes through the compression pipeline, including ComCNN, image codec and RecCNN, and finally outputs the reconstructed image x. Such a process is an end-to-end compression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>θ 2 Re θ 2 , 1 )</head><label>221</label><figDesc>Co Cr θ1 , x -x Updating the Parameters θ 2 of RecCNN: According to the network topology, an auxiliary variables xm is introduced and defined as the decoded compact representation of x, which can be formulated as xm = Co Cr θ1 , x .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 1 and θ0 2 3: 1 )</head><label>121</label><figDesc>The Proposed Compression Framework for Training Sub-Networks 1: Input: The original image x 2: Initialization:Random initial θ0 1 for t = 1 → T do For ComCNN training: Given a set of original images x and trained parameters θ 2 , we use mean squared error (MSE) as the loss function L 1 (θ 1 ) = 1 2N N k=1 Re θ2 , Cr (θ 1 , x k ) -x k 2) For RecCNN training: Having obtained a set of compact representation xm from ComCNN and original images x, we use MSE as the loss function:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The used test images.</figDesc><graphic coords="12,72.00,66.56,468.00,95.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>ComCNN, we remove ComCNN and use RecCNN alone to reconstruct the decoded image. Similarly, we remove RecCNN and only use ComCNN and bicubic interpolation to examine the effect of RecCNN. As shown in Table I and Table II, worse performances are obtained only with ComCNN or RecCNN. In addition, we show examples of the compact representation produced by ComCNN in Fig.7. It can be seen that the compact representation maintains the structural information of the original image, but it is different from traditional down sampling methods. In a nutshell, both ComCNN and RecCNN play key roles in the proposed compression framework. Due to the collaboration of ComCNN and RecCNN, the compact representation preserves more useful information for the final image reconstruction. Our testing codes are available in GitHub 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visual quality comparison of image deblocking for Lena in the case of QF = 5. From left to right and top to bottom: original image, JPEG(PSNR = 27.33 dB, SSIM = 0.7367), the deblocking results by Sun's (PSNR = 28.87 dB, SSIM = 0.8061), Zhang's (PSNR = 29.00 dB, SSIM = 0.8035), Ren's (PSNR = 29.07 dB, SSIM = 0.8010), BM3D (PSNR = 28.63 dB, SSIM = 0.7837), DicTV (PSNR = 28.07 dB, SSIM = 0.7744), WNNM (PSNR = 28.95 dB, SSIM = 0.7947), Zhang's (PSNR = 29.31 dB, SSIM = 0.8169), ARCNN (PSNR = 29.31 dB, SSIM = 0.8142), RecCNN (PSNR = 29.63 dB, SSIM = 0.8195) and the proposed framework (PSNR = 31.14 dB, SSIM = 0.8486)</figDesc><graphic coords="14,72.00,66.56,468.00,377.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visual quality comparison of the down sampling using bicubic interpolation and the compact representation produced by ComCNN.</figDesc><graphic coords="15,192.61,66.56,226.78,226.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Subjective performance comparison of JPEG2000 when the bit rate from 0.1bpp to 0.4bpp for Parrot. From left to right and top to bottom, the corresponding PSNR(in dB) and SSIM values are (26.05, 0.7853), (31.22, 0.8895), (29.96, 0.8589), (32.72, 0.9242), (32.42, 0.8897), (33.48, 0.9382), (34.42, 0.9150) and (35.09, 0.9480).</figDesc><graphic coords="16,72.00,66.56,468.02,286.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>propose a general framework for variable-rate image compression and a novel architecture based 1051-8215 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 10</figDesc><table /><note><p>1051-8215 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table III presents the comparison results with JPEG 2000. It can seen that our framework significantly outperforms JPEG2000 on all test bit-rates of all test images</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 23</figDesc><table><row><cell>Proposed Proposed</cell><cell>26.23 28.60</cell><cell>26.53 27.44</cell><cell>31.45 33.25</cell><cell>31.14 33.11</cell><cell>30.84 31.83</cell><cell>25.52 28.77</cell><cell>30.12 31.11</cell><cell>28.83 30.59</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SSIM SSIM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JPEG JPEG</cell><cell>0.7378 0.8325</cell><cell>0.7283 0.7965</cell><cell>0.7733 0.8183</cell><cell>0.7367 0.8183</cell><cell>0.7087 0.7839</cell><cell>0.7775 0.8609</cell><cell>0.7581 0.8336</cell><cell>0.7456 0.8206</cell></row><row><cell>Sun's [15] Sun's [15]</cell><cell>0.8321 0.8871</cell><cell>0.7687 0.8358</cell><cell>0.8113 0.8504</cell><cell>0.8061 0.8590</cell><cell>0.7931 0.8322</cell><cell>0.8380 0.9138</cell><cell>0.8323 0.8783</cell><cell>0.8104 0.8652</cell></row><row><cell>Zhang's [17] Zhang's [17]</cell><cell>0.8313 0.8923</cell><cell>0.7672 0.8329</cell><cell>0.8141 0.8513</cell><cell>0.8035 0.8597</cell><cell>0.7895 0.8317</cell><cell>0.8548 0.9212</cell><cell>0.8308 0.8804</cell><cell>0.8130 0.8671</cell></row><row><cell>Ren's [35] Ren's [35]</cell><cell>0.8419 0.9010</cell><cell>0.7666 0.8259</cell><cell>0.8197 0.8526</cell><cell>0.8010 0.8571</cell><cell>0.7876 0.8300</cell><cell>0.8720 0.9309</cell><cell>0.8310 0.8775</cell><cell>0.8171 0.8679</cell></row><row><cell>BM3D [36] BM3D [36]</cell><cell>0.8184 0.8896</cell><cell>0.7607 0.8240</cell><cell>0.8082 0.8492</cell><cell>0.7837 0.8549</cell><cell>0.7639 0.8250</cell><cell>0.8510 0.9207</cell><cell>0.8118 0.8749</cell><cell>0.7997 0.8626</cell></row><row><cell>DicTV [34] DicTV [34]</cell><cell>0.7769 0.8699</cell><cell>0.6658 0.8046</cell><cell>0.7963 0.8484</cell><cell>0.7744 0.8559</cell><cell>0.7456 0.8244</cell><cell>0.8104 0.9032</cell><cell>0.8005 0.8741</cell><cell>0.7671 0.8544</cell></row><row><cell>WNNM [37] WNNM [37]</cell><cell>0.8445 0.9019</cell><cell>0.7674 0.8248</cell><cell>0.8178 0.8531</cell><cell>0.7947 0.8571</cell><cell>0.7827 0.8303</cell><cell>0.8749 0.9325</cell><cell>0.8287 0.8775</cell><cell>0.8158 0.8681</cell></row><row><cell>Zhang's [8] Zhang's [8]</cell><cell>0.8667 0.9142</cell><cell>0.7666 0.8401</cell><cell>0.8285 0.8609</cell><cell>0.8169 0.8661</cell><cell>0.8031 0.8358</cell><cell>0.8882 0.9406</cell><cell>0.8460 0.8842</cell><cell>0.8308 0.8774</cell></row><row><cell>ARCNN [9] ARCNN [9]</cell><cell>0.8741 0.9237</cell><cell>0.7674 0.8389</cell><cell>0.8209 0.8591</cell><cell>0.8142 0.8711</cell><cell>0.7961 0.8434</cell><cell>0.8983 0.9495</cell><cell>0.8446 0.8942</cell><cell>0.8308 0.8828</cell></row><row><cell>ComCNN ComCNN</cell><cell>0.7488 0.8796</cell><cell>0.7662 0.8154</cell><cell>0.8119 0.8497</cell><cell>0.8042 0.8573</cell><cell>0.7966 0.8296</cell><cell>0.7885 0.9095</cell><cell>0.8377 0.8766</cell><cell>0.7934 0.8597</cell></row></table><note><p>1051-8215 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV BPG</head><label>IV</label><figDesc>: PSNR (DB) AND SSIM RESULTS OF BPG, BPG + RECCNN AND THE PROPOSED METHOD .</figDesc><table><row><cell></cell><cell></cell><cell>BPG</cell><cell cols="2">BPG + RecCNN</cell><cell cols="3">ComCNN + BPG + RecCNN</cell></row><row><cell>Test Images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="7">Size(Bytes) Size(Bytes) PSNR/SSIM Size(Bytes) PSNR/SSIM Bits Saving</cell></row><row><cell></cell><cell>752</cell><cell>25.95/0.7723</cell><cell>752</cell><cell>26.55/0.7867</cell><cell>642</cell><cell>26.70/0.7887</cell><cell>14.63%</cell></row><row><cell>Cameraman</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1251</cell><cell>28.12/0.8188</cell><cell>1251</cell><cell>28.77/0.8288</cell><cell>1105</cell><cell>28.85/0.8325</cell><cell>11.67%</cell></row><row><cell></cell><cell>389</cell><cell>29.06/0.8023</cell><cell>389</cell><cell>29.61/0.8121</cell><cell>384</cell><cell>30.21/0.8274</cell><cell>1.29%</cell></row><row><cell>House</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>592</cell><cell>31.35/0.8349</cell><cell>592</cell><cell>32.02/0.8433</cell><cell>524</cell><cell>32.31/0.8489</cell><cell>11.49%</cell></row><row><cell></cell><cell>1616</cell><cell>28.79/0.7896</cell><cell>1616</cell><cell>29.20/0.8016</cell><cell>1585</cell><cell>29.47/0.8066</cell><cell>1.92%</cell></row><row><cell>Lena</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2737</cell><cell>30.97/0.8351</cell><cell>2737</cell><cell>31.46/0.8456</cell><cell>2702</cell><cell>31.74/0.8529</cell><cell>1.28%</cell></row><row><cell></cell><cell>1277</cell><cell>24.43/0.8335</cell><cell>1277</cell><cell>25.79/0.8691</cell><cell>1203</cell><cell>25.65/0.8648</cell><cell>5.79%</cell></row><row><cell>Butterfly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1999</cell><cell>26.86/0.8875</cell><cell>1999</cell><cell>28.03/0.9069</cell><cell>1934</cell><cell>28.09/0.9076</cell><cell>3.25%</cell></row><row><cell></cell><cell>1797</cell><cell>28.93/0.7759</cell><cell>1797</cell><cell>29.59/0.7944</cell><cell>1722</cell><cell>29.75/0.7978</cell><cell>4.17%</cell></row><row><cell>Peppers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2747</cell><cell>30.81/0.8108</cell><cell>2747</cell><cell>31.38/0.8220</cell><cell>2699</cell><cell>31.43/0.8287</cell><cell>1.75%</cell></row><row><cell></cell><cell>1651</cell><cell>24.55/0.8670</cell><cell>1651</cell><cell>25.88/0.9046</cell><cell>1564</cell><cell>25.97/0.9090</cell><cell>5.27%</cell></row><row><cell>Leaves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2492</cell><cell>27.17/0.9195</cell><cell>2492</cell><cell>28.71/0.9405</cell><cell>2423</cell><cell>28.83/0.9476</cell><cell>2.77%</cell></row><row><cell></cell><cell>621</cell><cell>27.94/0.8235</cell><cell>621</cell><cell>28.61/0.8389</cell><cell>595</cell><cell>28.85/0.8483</cell><cell>4.19%</cell></row><row><cell>Parrots</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1017</cell><cell>30.25/0.8562</cell><cell>1017</cell><cell>30.95/0.8680</cell><cell>980</cell><cell>31.19/0.8716</cell><cell>3.64%</cell></row><row><cell>Average</cell><cell>-</cell><cell>28.23/0.8305</cell><cell>-</cell><cell>29.04/0.8473</cell><cell>-</cell><cell>29.22/0.8523</cell><cell>5.22%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V RUNNING</head><label>V</label><figDesc>TIME (S) OF COMPARED METHODS IN CPU (/GPU)TESTED ON A 256 × 256 GRAYSCALE IMAGE. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TCSVT.2017.2734838, IEEE Transactions on Circuits and Systems for Video Technology ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY 25</figDesc><table><row><cell>Sun's</cell><cell>Zhang's</cell><cell>Ren's</cell><cell>BM3D</cell><cell>DicTV</cell><cell>WNNM</cell><cell>Zhang's</cell><cell>RecCNN</cell><cell>Proposed</cell></row><row><cell>[15]</cell><cell>[17]</cell><cell>[35]</cell><cell>[36]</cell><cell>[34]</cell><cell>[37]</cell><cell>[8]</cell><cell></cell><cell></cell></row><row><cell>132/-</cell><cell>251/-</cell><cell>36/-</cell><cell>3.40/-</cell><cell>53/-</cell><cell>230/-</cell><cell>442/-</cell><cell>1.50/0.015</cell><cell>1.56/0.017</cell></row></table><note><p>1051-8215 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI AVERAGE</head><label>VI</label><figDesc>PSNR(DB)/SSIM RESULTS OF JPEG AND THE PROPOSED METHOD FOR QUALITY FACTORS 5 AND 10, JPEG2000 AND THE PROPOSED METHOD FOR BPP 0.1, 0.2, 0.3 FOR SET5, SET14, LIVE1 AND GENERAL-100</figDesc><table><row><cell></cell><cell></cell><cell>JPEG</cell><cell></cell></row><row><cell></cell><cell>Quality</cell><cell>JPEG</cell><cell>Proposed</cell></row><row><cell>DataSets</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Factors</cell><cell>PSNR/SSIM</cell><cell>PSNR/SSIM</cell></row><row><cell></cell><cell>5</cell><cell>26.13/0.7206</cell><cell>29.20/0.8387</cell></row><row><cell>Set5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>28.99/0.8109</cell><cell>31.40/0.8854</cell></row><row><cell></cell><cell>5</cell><cell>24.90/0.6686</cell><cell>26.89/0.7914</cell></row><row><cell>Set14</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>27.49/0.7762</cell><cell>28.91/0.8336</cell></row><row><cell></cell><cell>5</cell><cell>24.60/0.6666</cell><cell>26.78/0.7934</cell></row><row><cell>LIVE1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10</cell><cell>27.02/0.7720</cell><cell></cell></row><row><cell>Genreal-</cell><cell>5</cell><cell>25.93/0.7228</cell><cell>27.29/0.8447</cell></row><row><cell>100</cell><cell>10</cell><cell>28.92/0.8199</cell><cell>30.16/0.8767</cell></row><row><cell></cell><cell></cell><cell>JPEG2000</cell><cell></cell></row><row><cell></cell><cell></cell><cell>JPEG2000</cell><cell>Proposed</cell></row><row><cell cols="2">DataSets Rate(bpp)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>PSNR/SSIM</cell><cell>PSNR/SSIM</cell></row><row><cell></cell><cell>0.1</cell><cell>26.09/0.7176</cell><cell>29.00/0.8163</cell></row><row><cell>Set5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>29.11/0.8157</cell><cell>31.94/0.8859</cell></row><row><cell></cell><cell>0.3</cell><cell>31.06/0.8629</cell><cell>33.32/0.9086</cell></row><row><cell></cell><cell>0.1</cell><cell>25.23/0.6554</cell><cell>27.88/0.7911</cell></row><row><cell>Set14</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>27.80/0.7534</cell><cell>28.58/0.8273</cell></row><row><cell></cell><cell>0.3</cell><cell>29.57/0.8063</cell><cell>30.42/0.8867</cell></row><row><cell></cell><cell>0.1</cell><cell>25.39/0.6612</cell><cell>27.14/0.7478</cell></row><row><cell>LIVE1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell>27.58/0.7478</cell><cell>28.54/0.8183</cell></row><row><cell></cell><cell>0.3</cell><cell>29.19/0.7991</cell><cell>30.07/0.8544</cell></row><row><cell>General-</cell><cell>0.1</cell><cell>26.45/0.7179</cell><cell>27.82/0.8048</cell></row><row><cell>100</cell><cell>0.2</cell><cell>29.88/0.8153</cell><cell>30.86/0.8761</cell></row><row><cell></cell><cell>0.3</cell><cell>32.00/0.8639</cell><cell>32.88/0.9083</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We have tried deeper networks to obtain better performance, but only negligible improvements at the expense of a lot of training time and costs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>ACCEPTED TO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/compression-framework/compression framwork for tesing</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is partially funded by the Major State Basic Research Development Program of China (973 Program 2015CB351804), the Science and Technology Commission of China No.17-H863-03-ZT-003-010-01 and the Natural Science Foundation of China under Grant No. 61572155 and 61672188.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The jpeg still picture compression standard</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Wallace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on consumer electronics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="xviii" to="xxxiv" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Standard codecs: Image compression to advanced video coding</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanbari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iet</title>
		<imprint>
			<biblScope unit="issue">49</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient image deblocking based on postfiltering in shifted windows</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="122" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image postprocessing by non-local kuans filter</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="251" to="262" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A generic post-deblocking filter for block based image compression algorithms</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>De Faria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="985" to="997" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive non-local means filter for image deblocking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Concolor: constrained non-convex low-rank model for image deblocking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1246" to="1259" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep dual-domain based fast restoration of jpegcompressed images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-learning-based post-processing for image/video deblocking via sparse representation</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><forename type="middle">F</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="891" to="903" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Post-processing for blocking artifact reduction based on inter-block correlation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Ra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1536" to="1548" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data-driven soft decoding of compressed images in dual transform-pixel domain</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1649" to="1659" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Postprocessing of low bit-rate block dct coded images based on a fields of experts prior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2743" to="2751" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing blocking artifacts in compressed images via transform-domain non-local coefficients estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="836" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compression artifact reduction by overlapped-block transform coefficient estimation with block similarity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4613" to="4626" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04587</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<idno type="arXiv">arXiv:1511.04491</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Variable rate image compression with recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>O'malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06085</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Full resolution image compression with recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Covell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05148</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Lossy image compression with compressive autoencoders</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00395</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end optimized image compression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ballé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Laparra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01704</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning convolutional networks for content-weighted image compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10553</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial lstms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1927" to="1935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reducing artifacts in jpeg decompression via a learned dictionary</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on signal processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image blocking artifacts reduction via patch clustering and low-rank minimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Compression Conference (DCC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="516" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2862" to="2869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.02848</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Live image quality assessment database release 2</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">He is now an Associated Professor in the Department of Computer Science, HIT and a visiting scholar in the School of Electrical Engineering, Princeton University. His research interests include computer vision, image and video processing and pattern recognition</title>
		<author>
			<orgName type="collaboration">Feng Jiang received the B.S., M.S., and Ph</orgName>
		</author>
		<imprint>
			<date type="published" when="2001">2001, 2003, and 2008</date>
			<pubPlace>Harbin, China</pubPlace>
		</imprint>
	</monogr>
	<note>D. degrees in computer science from Harbin Institute of Technology (HIT)</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">He is now working towards the M.S. degree at School of Computer Science and Technology, HIT. His current research interests are in image processing and computer vision</title>
	</analytic>
	<monogr>
		<title level="m">TABLE I JPEG: QF = 5, PSNR (DB) AND SSIM RESULTS OF ALL COMPETITIVE ALGORITHMS GRAYSCALE IMAGE DEBLOCKING AND DENOISING</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Wen Tao received the B.S. degrees in computer science from Harbin Institute of Technology(HIT), Harbin</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><surname>Table</surname></persName>
		</author>
		<author>
			<persName><surname>Jpeg</surname></persName>
		</author>
		<title level="m">QF = 10, PSNR (DB) AND SSIM RESULTS OF ALL COMPETITIVE ALGORITHMS GRAYSCALE IMAGE DEBLOCKING AND DENOISING</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
