<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Copperhead: Compiling an Embedded Data Parallel Language</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
							<email>catanzar@cs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
							<email>mgarland@nvidia.com</email>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
							<email>keutzer@cs.berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Copperhead: Compiling an Embedded Data Parallel Language</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F568B6F0CCEC8D3E38E21FEC4D489F58</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.4 [Programming Languages]: Processors -Code generation, Compilers</term>
					<term>D.3.2 [Language Classifications]: Parallel languages</term>
					<term>D.1.3 [Concurrent Programming]: Parallel Programming General Terms Algorithms, Design, Performance Python, Data Parallelism, GPU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern parallel microprocessors deliver high performance on applications that expose substantial fine-grained data parallelism. Although data parallelism is widely available in many computations, implementing data parallel algorithms in low-level languages is often an unnecessarily difficult task. The characteristics of parallel microprocessors and the limitations of current programming methodologies motivate our design of Copperhead, a high-level data parallel language embedded in Python. The Copperhead programmer describes parallel computations via composition of familiar data parallel primitives supporting both flat and nested data parallel computation on arrays of data. Copperhead programs are expressed in a subset of the widely used Python programming language and interoperate with standard Python modules, including libraries for numeric computation, data visualization, and analysis.</p><p>In this paper, we discuss the language, compiler, and runtime features that enable Copperhead to efficiently execute data parallel code. We define the restricted subset of Python which Copperhead supports and introduce the program analysis techniques necessary for compiling Copperhead code into efficient low-level implementations. We also outline the runtime support by which Copperhead programs interoperate with standard Python modules. We demonstrate the effectiveness of our techniques with several examples targeting the CUDA platform for parallel programming on GPUs. Copperhead code is concise, on average requiring 3.6 times fewer lines of code than CUDA, and the compiler generates efficient code, yielding 45-100% of the performance of hand-crafted, well optimized CUDA code.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As the transition from sequential to parallel processing continues, the need intensifies for high-productivity parallel programming tools and methodologies. Manual implementation of parallel programs using efficiency languages such as C and C++ can yield Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. PPoPP <ref type="bibr">'11, February 12-16, 2011</ref>, San Antonio, Texas, USA. Copyright c 2011 ACM 978-1-4503-0119-0/11/02. . . $10.00 high performance, but at a large cost in programmer productivity, which some case studies show as being 2 to 5 times worse than productivity-oriented languages such as Ruby and Python <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b24">25]</ref>. Additionally, parallel programming in efficiency languages is often viewed as an esoteric endeavor, to be undertaken by experts only. Without higher-level abstractions that enable easier parallel programming, parallel processing will not be widely utilized. Conversely, today's productivity programmers do not have the means to capitalize on fine-grained, highly parallel microprocessors, which limits their ability to implement computationally intensive applications. To enable widespread use of parallel processors, we need to provide higher level abstractions which are both productive and efficient.</p><p>Although the need for higher-level parallel abstractions seems clear, perhaps less so is the type of abstractions which should be provided, since there are many potential abstractions to choose from. In our view, nested data parallelism, as introduced by languages such as NESL <ref type="bibr" target="#b3">[4]</ref>, is particularly interesting. Nested data parallelism is abundant in many computationally intensive algorithms. It can be mapped efficiently to parallel microprocessors, which prominently feature hardware support for data parallelism. For example, mainstream x86 processors from Intel and AMD are adopting 8-wide vector instructions, Intel's Larrabee processor used 16-wide vector instructions, and modern GPUs from NVIDIA and AMD use wider SIMD widths of 32 and 64, respectively. Consequently, programs which don't take advantage of data parallelism relinquish substantial performance, on any modern processor.</p><p>Additionally, nested data parallelism as an abstraction clearly exposes parallelism. In contrast to traditional auto-parallelizing compilers, which must analyze and prove which operations can be parallelized and which can not, the compiler of a data-parallel language needs only to decide which parallel operations should be performed sequentially, and which should be performed in parallel. Accordingly, nested data parallel programs have a valid sequential interpretation, and are thus easier to understand and debug than parallel programs that expose race conditions and other complications of parallelism.</p><p>Motivated by these observations, Copperhead provides a set of nested data parallel abstractions, expressed in a restricted subset of the widely used Python programming language. Instead of creating an entirely new programming language for nested data parallelism, we repurpose existing constructs in Python, such as map and reduce. Embedding Copperhead in Python provides several important benefits. For those who are already familiar with Python, learning Copperhead is more similar to learning how to use a Python package than it is to learning a new language. There is no need to learn any new syntax, instead the programmer must learn only what subset of Python is supported by the Copperhead language and runtime. The Copperhead runtime is implemented as a standard Python extension, and Copperhead programs are invoked through the Python interpreter, allowing them to interoperate with the wide variety of existing Python libraries for numeric computa-tion, file manipulation, data visualization, and analysis. This makes Copperhead a productive environment for prototyping, debugging, and implementing entire applications, not just their computationally intense kernels.</p><p>Of course, without being able to efficiently compile nested data parallel programs, Copperhead would not be of much use. To this end, the Copperhead compiler attempts to efficiently utilize modern parallel processors. Previous work on compilers for nested data parallel programming languages has focused on flattening transforms to target flat SIMD arrays of processing units. However, today's processors support a hierarchy of parallelism, with independent cores containing tightly coupled processing elements. Accordingly, the Copperhead compiler maps nested data parallel programs to a parallelism hierarchy, forgoing the use of flattening transforms. We find that in many circumstances, this is substantially more efficient than indiscriminate application of the flattening transform.</p><p>The current Copperhead compiler targets CUDA C++, running on manycore Graphics Processors from NVIDIA. We generate efficient, scalable parallel programs, performing within 45-100% of well optimized, hand-tuned CUDA code. Our initial implementation focus on CUDA does not limit us to a particular hardware platform, as the techniques we have developed for compiling dataparallel code are widely applicable to other platforms as well. In the future, we anticipate the development of Copperhead compiler backends for other parallel platforms. Additionally, the compiler techniques we propose are suitable for compiling other data parallel languages, such as NESL and Data Parallel Haskell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is an extensive literature investigating many alternative methods for parallel programming. Data parallel approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13]</ref> have often been used, and have historically been most closely associated with SIMD and vector machines, such as the the CM-2 and Cray C90, respectively. Blelloch et al. designed the NESL language <ref type="bibr" target="#b3">[4]</ref>, demonstrating a whole-program transformation of nested data parallel programs into flat data parallel programs. The flattening transform has been extended to fully higher-order functional languages in Data Parallel Haskell <ref type="bibr" target="#b6">[7]</ref>. In contrast to these methods, we attempt to schedule nested procedures directly onto the hierarchical structure of the machines we target.</p><p>The CUDA platform <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26]</ref> defines a blocked SPMD programming model for executing parallel computations on GPUs. Several libraries, such as Thrust <ref type="bibr" target="#b13">[14]</ref>, provide a collection of flat data parallel primitives for use in CUDA programs. Copperhead uses selected Thrust primitives in the code it generates.</p><p>Systems for compiling flat data parallel programs for GPU targets have been built in a number of languages, including C# <ref type="bibr" target="#b27">[28]</ref>, C++ <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, and Haskell <ref type="bibr" target="#b18">[19]</ref>. Such systems typically define special data parallel array types and use operator overloading and metaprogramming techniques to build expression trees describing the computation to be performed on these arrays. The Ct <ref type="bibr" target="#b10">[11]</ref> library adopts a similar model for programming multicore processors. However, these systems have not provided a means to automatically map nested data parallel programs to a hierarchically nested parallel platform.</p><p>Rather than providing data parallel libraries, others have explored techniques that mark up sequential loops to be parallelized into CUDA kernels <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30]</ref>. In these models, the programmer writes an explicitly sequential program consisting of loops that are parallelizable. The loop markups, written in the form of C pragma preprocessor directives, indicate to the compiler that loops can be parallelized into CUDA kernels.</p><p>There have also been some attempts to compile various subsets of Python to different platforms. The Cython compiler <ref type="bibr" target="#b8">[9]</ref> compiles a largely Python-like language into sequential C code. Clyther <ref type="bibr" target="#b7">[8]</ref> takes a similar approach to generating OpenCL kernels. In both cases, the programmer writes a Python program which is transliterated into an isomorphic C program. Rather than transliterating Python programs, PyCUDA <ref type="bibr" target="#b17">[18]</ref> provides a metaprogramming facility for textually generating CUDA kernel programs, as well as Python/GPU bindings. Theano <ref type="bibr" target="#b28">[29]</ref> provides an expression tree facility for numerical expressions on numerical arrays. Garg and Amaral <ref type="bibr" target="#b9">[10]</ref> recently described a technique for compiling Python loop structures and array operations into GPU-targeted code.</p><p>These Python/GPU projects are quite useful; in fact Copperhead's runtime relies on PyCUDA. However, to write programs using these tools, programmers must write code equivalent to the output of the Copperhead compiler, with all mapping and scheduling decisions made explicit in the program itself. Copperhead aims to solve a fairly different problem, namely compiling a program from a higher level of abstraction into efficiently executable code. This @cu decorator declares the associated procedure to be a Copperhead procedure. These procedures must conform to the requirements of the Copperhead language, and they may be compiled for and executed on any of the parallel platforms supported by the Copperhead compiler. Once defined, Copperhead procedures may be called just like any other Python procedure, both within the program body or, as shown below, from an interactive command line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Copperhead Language</head><p>&gt;&gt;&gt; add _ vectors(range(10), [2] * 10) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> The @cu decorator interposes a wrapper object around the native Python function object that is responsible for compiling and executing procedures on a target parallel platform.</p><p>Copperhead is fundamentally a data parallel language. It provides language constructs, specifically map, and a library of primitives such as reduce, gather, and scatter that all have intrinsically parallel semantics. They operate on 1-dimensional arrays of data that we refer to as sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Restricted Subset of Python</head><p>The Copperhead language is a restricted subset of the Python 2.6 language. Every valid Copperhead procedure must also be a valid Python procedure. Portions of the program outside procedures marked with the @cu decorator are normal Python programs that will be executed by the standard Python interpreter, without any restrictions imposed by the Copperhead runtime or compiler. The Copperhead language supports a very restricted subset of Python in order to enable efficient compilation.</p><p>Copperhead adopts the lexical and grammatical rules of Python. In summarizing its restricted language, we denote expressions by E and statements by S. We use lower-case letters to indicate identifiers and F and A to indicate function-valued and array-valued expressions, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Expressions</head><p>The most basic expressions are literal values, identifier references, tuple constructions, and accesses to array elements.</p><formula xml:id="formula_0">E : x | (E1, . . . , En) | A[E] | True | False | integer | floatnumber</formula><p>The basic logical and arithmetic operators are allowed, as are Python's and, or, and conditional expressions. In addition to these grammatical restrictions, Copperhead expressions must be statically well-typed. We employ a standard Hindley-Milner style polymorphic type system. Static typing provides richer information to the compiler that it can leverage during code generation, and also avoids the run-time overhead of dynamic type dispatch.</p><formula xml:id="formula_1">| E1 + E2 | E1 &lt; E2 | not E | . . . | E1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Statements</head><p>The body of a Copperhead procedure consists of a suite of statements: one or more statements S which are nested by indentation level. Each statement S of a suite must be of the following form:</p><formula xml:id="formula_2">S : return E | x1, . . . , xn = E | if E: suite else: suite | def f (x1, . . . ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>xn): suite</head><p>Copperhead further requires that every execution path within a suite must return a value, and all returned values must be of the same type. This restriction is necessary since every Copperhead procedure must have a well-defined static type. All data in a Copperhead procedure are considered immutable values. Thus, statements of the form x = E bind the value of the expression E to the identifier x; they do not assign a new value to an existing variable. All identifiers are strictly lexically scoped.</p><p>Copperhead does not guarantee any particular order of evaluation, other than the partial ordering imposed by data dependencies in the program. Python, in contrast, always evaluates statements from top to bottom and expressions from left to right. By definition, a Copperhead program must be valid regardless of the order of evaluations, and thus Python's mandated ordering is one valid ordering of the program.</p><p>Because mutable assignment is forbidden and the order of evaluation is undefined, the Copperhead compiler is free to reorder and transform procedures in a number of ways. As we will see, this flexibility improves the efficiency of generated code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Parallel Primitives</head><p>Copperhead is a data parallel language. Programs manipulate data sequences by applying aggregate operations, such as map, or reduce. The semantics of these primitives are implicitly parallel: they may always be performed by some parallel computation, but may also be performed sequentially. Table <ref type="table">1</ref> summarizes the main aggregate operations used by the examples in this paper. They mirror operations found in most other data parallel languages. With two minor exceptions, these functions will produce the same result regardless of whether they are performed in parallel or sequentially. The permute primitive is the primary exception. If the indices array given to permute contains one or more repeated index values, the resulting sequence is non-deterministic since we guarantee no particular order of evaluation. The other exception is reduce. If given a truly commutative and associative operation, then its result will always be identical. However, programs often perform reductions using floating point values whose addition, for instance, is not truly associative. These problems are commonly encountered in general parallel programming, and are not unique to Copperhead.</p><p>To demonstrate our use of these operators, Figure <ref type="figure" target="#fig_0">1</ref> shows a simple Copperhead procedure for computing the sparse matrixvector product (SpMV) y = Ax. Here we assume that A is stored in Compressed Sparse Row (CSR) format-one of the most frequently used representation for sparse matrices-and that x is a dense vector. The matrix representation simply records each row of the matrix as a sequence containing its non-zero values along with a corresponding sequence recording the column index of each value. A simple example of this representation is: <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>, <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref></p><formula xml:id="formula_3">A =    1 7 0 0 0 2 8 0 5 0 3 9 0 6 0 4    vals = [</formula><formula xml:id="formula_4">] cols = [[0,1],[1,2],[0,2,3],[1,3]]</formula><p>The body of spmv _ csr applies a sparse dot product procedure, spvv, to each row of the matrix using map. The sparse dot product itself produces the result yi for row i by forming the products Aijxj for each column j containing a non-zero entry, and then summing these products together. It uses gather to fetch the necessary values xj from the dense vector x and uses sum, a convenient special case of reduce where the operator is addition, to produce the result.</p><p>This simple example illustrates two important issues that are a central concern for our Copperhead compiler. First, it consists of a number of potentially parallel aggregate operations, which are underlined. Second, these aggregate operators are nested: those within spvv are executed within an enclosing map operation invoked by spmv _ csr. One of the principal tasks of our compiler is to decide how to schedule these operations for execution. Each of them may be performed in parallel or by sequential loops, and the nesting of operations must be mapped onto the hardware execution units in an efficient fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Compiler</head><p>The Copperhead compiler is a source-to-source compiler responsible for converting a suite of one or more Copperhead procedures y = replicate(a, n) Return an n-element sequence whose every element has value a. y = map(f, x1, ..., xn)</p><formula xml:id="formula_5">Returns sequence [f(x1[0], ..., xn[0]), f(x1[1], ..., xn[1]), ...]. y = zip(x1, x2) Return sequence of pairs [(x1[0],x2[0]), (x1[1],x2[1]), ...]. y = gather(x, indices) Produce sequence with y[i] = x[indices[i]]. y = permute(x, indices) Produce y where y[indices[i]] = x[i]. s = reduce(⊕, x, prefix) Computes prefix ⊕ x[0] ⊕ x[1] ⊕ .</formula><p>.. for a commutative and associative operator ⊕.</p><formula xml:id="formula_6">y = scan(f, x) Produce y such that y[0]=x[0] and y[i] = f(y[i-1], x[i]</formula><p>). Function f must be associative.</p><p>Table <ref type="table">1</ref>. Selected data-parallel operations on sequences. </p><formula xml:id="formula_7"># spvv :: ([a], [Int], [a]) → [a]</formula><p>def spvv(Ai, j, _ k0):</p><formula xml:id="formula_8">z 0 = gather( _ k0, j) tmp 0 = map(lambda0, Ai, z 0 ) return sum(tmp 0 ) # spmv _ csr :: ([[a]], [[Int]], [a]) → [a]</formula><p>def spmv _ csr(vals, cols, x): return map(closure([x], spvv), vals, cols) into a code module that can execute on the target parallel platform. We have designed it to support three basic usage patterns.</p><p>First is what we refer to as JIT (Just-In-Time) compilation. When the programmer invokes a @cu-decorated function either from the command line or from a Python code module, the Copperhead runtime may need to generate code for this procedure if none is already available. Second is batch compilation, where the Copperhead compiler is asked to generate a set of C++ code modules for the specified procedures. This code may be subsequently used either within the Copperhead Python environment or linked directly with external C++ applications. The third common scenario is one where the compiler is asked to generate a collection of variant instantiations of a Copperhead procedure in tandem with an autotuning framework for exploring the performance landscape of a particular architecture.</p><p>In the following discussion, we assume that the compiler is given a single top level Copperhead function-referred to as the "entry point"-to compile. It may, for instance, be a function like spmv _ csr that has been invoked at the Python interpreter prompt. For the CUDA platform, the compiler will take this procedure, along with any procedures it invokes, and produce a single sequential host procedure and one or more parallel kernels that will be invoked by the host procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Front End</head><p>After parsing the program text using the standard Python ast module, the compiler front end converts the program into a simplified abstract syntax tree (AST) form suitable for consumption by the rest of the compiler. It applies a sequence of fairly standard program transformations to: lift all nested lambdas and procedures into top level definitions, make closures explicit, convert all statements to single assignment form with no nested expressions, and infer static types for all elements of the program. Programs that contain syntax errors or that do not conform to the restricted language outlined in Section 3 are rejected.</p><p>For illustration, Figure <ref type="figure" target="#fig_2">2</ref> shows a program fragment representing the AST for the Copperhead procedure shown in Figure <ref type="figure" target="#fig_0">1</ref>. Each procedure is annotated with its most general polymorphic type, which we determine using a standard Hindley-Milner style type inference process. Lexically captured variables within nested procedures are eliminated by converting free variables to explicit arguments-shown as variables _ k0, _ k1, etc.-and replacing the original nested definition point of the function with a special closure([k1, ..., kn], f) form where the ki represent the originally free variables. In our example, the local variable x was free in the body of the nested spvv procedure, and is thus converted into an explicit closure argument in the transformed code. Single assignment conversion adds a unique subscript to each local identifier (e.g., z0 in spvv), and flattening nested expressions introduces temporary identifiers (e.g., tmp0) bound to the value of individual sub-expressions. Note that our single assignment form has no need of the φ-functions used in SSA representations of imperative languages since we disallow loops and every branch of a conditional must return a value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scheduling Nested Parallelism</head><p>The front end of the compiler carries out platform independent transformations in order to prepare the code for scheduling. The middle section of the compiler is tasked with performing analyses and scheduling the program onto a target platform.</p><p>At this point in the compiler, a Copperhead program consists of possibly nested compositions of data parallel primitives. A Copperhead procedure may perform purely sequential computations, in which case our compiler will convert it into sequential C++ code. Copperhead makes no attempt to auto-parallelize sequential codes. Instead, it requires the programmer to use primitives that the compiler will auto-sequentialize as necessary. Our compilation of sequential code is quite straightforward, since we rely on the host C++ compiler to handle all scalar code optimizations, and our restricted language avoids all the complexities of compiling a broad subset of Python that must be addressed by compilers like Cython <ref type="bibr" target="#b8">[9]</ref>.</p><p>Copperhead supports both flat and nested data parallelism. A flat Copperhead program consists of a sequence of parallel primitives which perform purely sequential operations to each element of a sequence in parallel. A nested program, in contrast, may apply parallel operations to each sequence element. Our spmv _ csr code provides a simple concrete example. The outer spmv _ csr procedure applies spvv to every row of its input via map. The spvv procedure itself calls gather, map, and sum, all of which are potentially parallel. The Copperhead compiler must decide how to map these potentially parallel operations onto the target hardware: for example, they may be implemented as parallel function calls or sequential loops.</p><p>One approach to scheduling nested parallel programs, adopted by NESL <ref type="bibr" target="#b3">[4]</ref> and Data Parallel Haskell <ref type="bibr" target="#b6">[7]</ref> among others, is to apply a flattening (or vectorization) transform to the program. This converts a nested structure of vector operations into a sequence of flat operations. In most cases, the process of flattening replaces the nested operations with segmented equivalents. For instance, in our spmv _ csr example, the nested sum would be flattened into a segmented reduction.</p><p>The flattening transform is a powerful technique which ensures good load balancing. It also enables data parallel compilation for flat SIMD processors, where all lanes in the SIMD array work in lockstep. However, we take a different approach. Flattening transformations are best suited to machines that are truly flat. Most modern machines, in contrast, are organized hierarchically. The CUDA programming model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, for instance, provides a hierarchy of execution formed into four levels:</p><p>1. A host thread running on the CPU, which can invoke 2. Parallel kernels running on the GPU, which consist of 3. A grid of thread blocks each of which is comprised of 4. Potentially hundreds of device threads running on the GPU.</p><p>The central goal of the Copperhead compiler is thus to map the nested structure of the program onto the hierarchical structure of the machine.</p><p>Recent experience with hand-written CUDA programs suggests that direct mapping of nested constructions onto this physical machine hierarchy often yields better performance. For instance, Bell and Garland <ref type="bibr" target="#b0">[1]</ref> explored several strategies for implementing SpMV in CUDA. Their Coordinate (COO) kernel is implemented by manually applying the flattening transformation to the spmv _ csr algorithm. Their other kernels represent static mappings of nested algorithms onto the hardware. The flattened COO kernel only delivers the highest performance in the exceptional case where the distribution of row lengths is extraordinarily variable, in which case the load balancing provided by the flattening transform is advantageous. However, for 13 out of the 14 unstructured matrices they examine, applying the flattening transform results in performance two to four times slower than the equivalent nested implementation.</p><p>Experiences such as this lead us to the conclusion that although the flattening transform can provide high performance in certain cases where the workload is extremely imbalanced, the decision to apply the transform should be under programmer control, given the substantial overhead the flattening transform imposes for most workloads.</p><p>Our compiler thus performs a static mapping of nested programs onto a parallelism hierarchy supported by the target parallel platform.</p><p>Returning to our spmv _ csr example being compiled to the CUDA platform, the map within its body will become a CUDA kernel call. The compiler can then chose to map the operations within spvv to either individual threads or individual blocks. Which mapping is preferred is in general program and data dependent; matrices with very short rows are generally best mapped to threads while those with longer rows are better mapped to blocks.</p><p>Because this information is data dependent, we currently present the decision of which execution hierarchy to target as a compiler option. The programmer can arrange the code to specify explicitly which is preferred, an autotuning framework can explore which is better on a particular machine, or a reasonable default can be chosen based on any static knowledge about the problem being solved. In the absence of a stated preference, the default is to map nested operations to sequential implementations. This will give each row of the matrix to a single thread of the kernel created by the call to map in spmv _ csr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Synchronization Points and Fusion</head><p>The compiler must also discover and schedule synchronization points between aggregate operators. We assume that our target parallel platform provides a SPMD-like execution model, and therefore synchronization between a pair of parallel operations is performed via barrier synchronization across parallel threads. Similarly, a synchronization point between a pair of operators that have been marked as sequentialized implies that they must be implemented with separate loop bodies.</p><p>The most conservative policy would be to require a synchronization point following every parallel primitive. However, this can introduce far more synchronization, temporary data storage, and memory bandwidth consumption than necessary to respect data dependencies in the program. Requiring a synchronization point after every parallel primitive can reduce performance by an order of magnitude or more, compared with equivalent code that synchronizes only when necessary. In order to produce efficient code, our compiler must introduce as few synchronization points as possible, by fusing groups of parallel primitives into single kernels or loop bodies.</p><p>In order to determine where synchronization points are necessary, we need to characterize the data dependencies between individual elements of the sequences upon which the parallel primitives are operating. Consider an aggregate operator of the form x = f(y1, ..., yn) where x and the yi are all sequences. Since the elements of x may be computed concurrently, we need to know what values of yi it requires.</p><p>We call the value x[i] complete when its value has been determined and may be used as input to subsequent operations. The array x is complete when all its elements are complete.</p><p>We categorize the bulk of our operators into one of two classes. The first class are those operators which perform induction over the domain of their inputs. This class is essentially composed of permute, scatter, and their variants. We cannot in general guarantee that any particular x[i] is complete until the entire operation is finished. In other words, either the array x is entirely complete or entirely incomplete.</p><p>The second class are those operators which perform induction over the domain of their outputs. This includes almost all the rest of our operators, such as map, gather, and scan. Here we want to characterize the portion of each yj that must be complete in order to complete x[i]. We currently distinguish three broad cases:</p><p>• None: x[i] does not depend on any element of yj • Local: completing x[i] requires that yj [i] be complete</p><p>• Global: completing x[i] requires that yj be entirely complete With deeper semantic analysis, we could potentially uncover more information in the spectrum between local and global completion requirements. However, at the moment we restrict our attention to this simple classification.</p><p>Each primitive provided by Copperhead declares its completion requirements on its arguments (none, local, global) as well as the completion of its output. Completing the output of class 1 operators requires synchronization, while the output of class 2 operators is completed locally. The compiler then propagates this information through the body of user-defined procedures to compute their completion requirements. For example, a synchronization point is required between a class 1 primitive and any point at which its output is used, or before any primitive that requires one of its arguments to be globally complete. We call a sequence of primitives that are not separated by any synchronization points a phase of the program. All operations within a single phase may potentially be fused into a single parallel kernel or sequential loop by the compiler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Shape Analysis</head><p>Shape analysis determines, where possible, the sizes of all intermediate values. This allows the back end of the compiler to statically allocate and reuse space for temporary values, which is critical to obtaining efficient performance. For the CUDA backend, forgoing shape analysis would require that every parallel primitive be executed individually, since allocating memory from within a CUDA kernel is not feasible. This would lead to orders of magnitude lower performance.</p><p>Our internal representation gives a unique name to every temporary value. We want to assign to each a shape of the form [d1, ..., dn], s where di is the array's extent in dimension i and s is the shape of each of its elements. Although Copperhead currently operates on one-dimensional sequences, the shape analysis we employ applies to higher dimensional arrays as well. The shape of a 4-element, 1-dimensional sequence [5,4,8,1] would, for example, be <ref type="bibr" target="#b3">[4]</ref>, Unit where Unit= [], • is the shape reserved for indivisible types such as scalars. Nested sequences are not required to have fully determined shapes: in the case where subsequences have differing lengths, the extent along that dimension will be undefined, for example: <ref type="bibr" target="#b1">[2]</ref>, [ * ], Unit . Note that the dimensionality of all values are known as a result of type inference. It remains only to determine extents, where possible.</p><p>We approach shape analysis of user-provided code as an abstract interpretation problem. We define a shape language consisting of Unit, the shape constructor D, s described above, identifiers, and shape functions extentof(s), elementof(s) that extract the two respective portions of a shape s. We implement a simple environment-based evaluator where every identifier is mapped to either (1) a shape term or (2) itself if its shape is unknown. Every primitive f is required to provide a function, that we denote f.shape, that returns the shape of its result given the shapes of its inputs. It may also return a set of static constraints on the shapes of its inputs to aid the compiler in its analysis. Examples of such shape-computing functions would be:</p><formula xml:id="formula_9">gather.shape(x, i) = extentof(i), elementof(x) zip.shape(x1, x2) = extentof(x1), (elementof(x1),elementof(x2)) with extentof(x1)==extentof(x2)</formula><p>The gather rule states that its result has the same size as the index array i while having elements whose shape is given by the elements of x. The zip augments the shape of its result with a constraint that the extents of its inputs are assumed to be the same. Terms such as extentof(x) are left unevaluated if the identifier x is not bound to any shape.</p><p>To give a sense of the shape analysis process, consider the spvv procedure shown in Figure <ref type="figure" target="#fig_2">2</ref>. Shape analysis will annotate every binding with a shape like so: def spvv(Ai, j, _ k0): z 0 :: extentof(j), elementof( _ k0) tmp 0 :: extentof(Ai), elementof(Ai) return sum(tmp 0 ) :: Unit</p><p>In this case, the shapes for z0, tmp0, and the return value are derived directly from the shape rules for gather, map, and sum, respectively. Shape analysis is not guaranteed to find all shapes in a program. Some identifiers may have data dependent shapes, making the analysis inconclusive. For these cases, the compiler must treat computations without defined shapes as barriers, across which no fusion can be performed, and then generate code which computes the actual data dependent shape before the remainder of the computation is allowed to proceed.</p><p>Future work involves allowing shape analysis to influence code scheduling: in an attempt to better match the extents in a particular nested data parallel problem to the dimensions of parallelism supported by the platform being targeted. For example, instead of implementing the outermost level of a Copperhead program in a parallel fashion, if the outer extent is small and the inner extent is large, the compiler may decide to create a code variant which sequentializes the outermost level, and parallelizes an inner level. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">CUDA C++ Back End</head><p>The back end of the Copperhead compiler generates platform specific code. As mentioned, we currently have a single back end, which generates code for the CUDA platform. Figure <ref type="figure" target="#fig_3">3</ref> shows an example of such code for our example spmv _ csr procedure. It consists of a sequence of function objects for the nested lambdas, closures, and procedures used within that procedure. The templated function spmv _ csr _ phase0 corresponds to the Copperhead entry point.</p><p>Not shown here is the Copperhead generated C++ host code that invokes the parallel kernels. It is the responsibility of the host code to marshal data where necessary, allocate any required temporary storage on the GPU, and make the necessary CUDA calls to launch the kernel.</p><p>We generate templated CUDA C++ code that makes use of a set of sequence types, such as stored _ sequence and nested _ sequence types, which hold sequences in memory. Fusion of sequential loops and block-wise primitives is performed through the construction of compound types. For example, in the spvv functor shown above, the calls to gather, and transform perform no work, instead they construct gathered _ sequence and transformed _ sequence structures that lazily perform the appropriate computations upon dereferencing. Work is only performed by the last primitive in a set of fused sequential or block-wise primitives. In this example, the call to seq::sum introduces a sequential loop which then dereferences a compound sequence, at each element performing the appropriate computation. When compiling this code, the C++ compiler statically eliminates all the indirection present in this code, yielding machine code which is as efficient as if we had generated the fused loops directly.</p><p>We generate fairly high level C++ code, rather than assembly level code, for two reasons. Firstly, existing C++ compilers provide excellent support for translating well structured C++ to efficient machine code. Emitting C++ from our compiler enables our compiler to utilize the vast array of transformations which existing compilers already perform. But more importantly, it means that the code generated by our Copperhead compiler can be reused in external C++ programs. We believe that an important use case for systems like Copperhead is to prototype algorithms in a high-level language and then compile them into template libraries that can be used by a larger C++ application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Runtime</head><p>Copperhead code is embedded in standard Python programs. Python function decorators indicate which procedures should be executed by the Copperhead runtime. When a Python program calls a Copperhead procedure, the Copperhead runtime intercepts the call, compiles the procedure, and then executes it on a specified execution place. The Copperhead runtime uses Python's introspection capabilities to gather all the source code pertaining to the procedure being compiled. This model is inspired by the ideas from Selective, Embedded, Just-In-Time Specialization <ref type="bibr" target="#b5">[6]</ref>.</p><p>The Copperhead compiler is fundamentally a static compiler that may be optionally invoked at runtime. Allowing the compiler to be invoked at runtime matches the no-compile mindset of typical Python development. However, the compiler does not perform dynamic compilation optimizations specific to the runtime instantiation of the program, such as treating inputs as constants, tracing execution through conditionals, etc. Forgoing these optimizations enables the results of the Copperhead compiler to be encapsulated as a standard, statically compiled binary, and cached for future reuse or incorporated as libraries into standalone programs which are not invoked through the Python interpreter.</p><p>Consequently, the runtime compilation overhead we incur is analogous to the build time of traditional static compilers, and does not present a performance limitation. The Copperhead compiler itself typically takes on the order of 300 milliseconds to compile our example programs, and the host C++ and CUDA compilers typically take on the order of 20 seconds to compile a program. Both of these overheads are not encountered in performance critical situations, since Copperhead's caches obviate recompilation. The actual runtime overhead, compared with calling an equivalent C++ function from within C++ code, is on the order of 100 microseconds per Copperhead procedure invocation.</p><p>For each Copperhead procedure, the CUDA runtime generates a host C++ function which coordinates the launch and synchronization between multiple parallel phases, as well as allocates data for temporary results. Copperhead uses PyCUDA <ref type="bibr" target="#b17">[18]</ref> and CodePy <ref type="bibr" target="#b16">[17]</ref> to provide mechanisms for compiling, persistent caching, linking and executing CUDA and C++ code. The Copperhead runtime uses Thrust <ref type="bibr" target="#b13">[14]</ref> to implement fully parallel versions of certain data parallel primitives, such as reduce and variations of scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Places</head><p>In order to manage the location of data and kernel execution across multiple devices, the Copperhead runtime defines a set of places that represent these heterogeneous devices. Data objects are created at a specific place. Calling a Copperhead procedure will execute a computation on the current target place, which is controlled utilizing the Python with statement.</p><p>Currently we support two kinds of places: CUDA capable GPUs and the native Python interpreter. Copperhead is designed to allow other types of places, with corresponding compiler back ends to be added. For instance, multi-core x86 back end would be associated with a new place type.</p><p>To faciliate interoperability between Python and Copperhead, all data is duplicated, with a local copy in the Python interpreter, and a remote copy which resides at the place of execution. Data is lazily transferred between the local and remote place as needed by the program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Structures</head><p>Copperhead adopts a common technique for representing arbitrarily nested sequences as a flat sequence of data, along with a descriptor sequence for each level of nesting. The descriptor sequences provide the necessary information to build a view of each subsequence, including empty subsequences.</p><p>In addition to supporting arbitrarily nested sequences, Copperhead also allows programmers to construct uniformly nested sequences, which support the important special case where the shape is completely defined. For such sequences, a set of strides and lengths are sufficient to describe the nesting structure -descriptor sequences are not required. Uniformly nested sequences also allow the data to be arbitrarily ordered: when the programmer creates a uniformly nested sequence, they either specify the data ordering or provide a tuple of strides directly. This allows the programmer to express data layouts analogous to row-and column-major ordering for doubly nested sequences, as well as their analogues for more deeply nested sequences. The programmer may provide the strides directly, which allows the subsequences to be aligned arbitrarily. This can provide important performance benefits when data access patterns with standard nested sequences are not matched well to the processor's memory hierarchy.</p><p>We have seen have seen two to three fold performance improvements by using the correct nested data structure type. Future work may investigate autotuning over alignments and data layouts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Examples</head><p>In this section, we investigate the performance of Copperhead code in several example programs. As we compare performance, we compare Copperhead compiled code to published, well-optimized, hand-crafted CUDA implementations of the same computation. Our compiler can't apply all the transformations which a human can, so we don't expect to achieve the same performance as welloptimized code. Still, we aim to show that high-level data parallel computations can perform within striking distance of human optimized efficiency code.</p><p>The Copperhead compiler and runtime is freely available at http://code.google.com/p/copperhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Sparse Matrix Vector Multiplication</head><p>Continuing with the Sparse Matrix Vector Multiplication example, we examine the performance of Copperhead generated code for three different SpMV kernels: compressed sparse row, vector compressed sparse row, and ELL. The CSR kernel is generated by compiling the Copperhead procedure for CSR SpMV onto the standard parallelism hierarchy, which distributes computations along the outermost parallelism dimension to independent threads. Data parallel operations are sequentialized into loops inside each thread.</p><p>For the CSR kernel, each row of the matrix is then processed by a different thread, and consequently, adjacent threads are processing widely separated data. On the CUDA platform, this yields suboptimal performance for most datasets.</p><p>The vector CSR kernel improves on the performance of the CSR kernel by mapping to a different parallelism hierarchy: one where the outermost level distributes computations among independent thread blocks, and subsequent data parallel operations are then sequentialized to block-wise operations. As mentioned earlier, the Copperhead programmer can choose which hierarchy to map to, or can choose to autotune over hierarchies. In this case, mapping to the block-wise hierarchy improves memory performance on the CUDA platform, since adjacent threads inside the same thread block are processing adjacent data, allowing for vectorized memory accesses.</p><p>The ELL representation stores the nonzeros of the matrix in a dense M -by-K array, where K bounds the number of nonzeros per row, and rows with fewer than K nonzeros are padded. The array is stored in column-major order, so that after the computation has been mapped to the platform, adjacent threads will be accessing adjacent elements of the array. For example: ELL generally performs best on matrices where the variance of the number of non-zeros per row is small. If exceptional rows with large numbers of non-zeros exist, ELL will perform badly due to the overhead resulting from padding the smaller rows.  We compare against Cusp <ref type="bibr" target="#b0">[1]</ref>, a C++ library for Sparse Matrix Vector multiplication, running on an NVIDIA GeForce GTX 480 GPU. We use a suite of 8 unstructured matrices which were used by Bell and Garland <ref type="bibr" target="#b0">[1]</ref>, and which are amenable to ELL storage. Copperhead generated code achieves identical performance for the scalar CSR kernel, and on average provides 45% and 79% of Cusp performance for the vector CSR and ELL kernels, respectively.</p><formula xml:id="formula_10">A =   </formula><p>Our relatively low performance on the vector CSR kernel is due to a specialized optimization which the Cusp vector CSR implementation takes advantage of, but the Copperhead compiler does not: namely the ability to perform "warp-wise" reductions without synchronization, packing multiple rows of the SpMV computation into a single thread block. This optimization is an important workaround for the limitations of today's CUDA processors, but we consider it too special purpose to implement in the Copperhead compiler. The Conjugate Gradient method is widely used to solve sparse systems of the form Ax = b. We examine performance on a preconditioned conjugate gradient solver written in Copperhead, which forms a part of an fixed-point non-linear solver used in Variational Optical Flow methods <ref type="bibr" target="#b26">[27]</ref>. Figure <ref type="figure" target="#fig_7">5</ref> was generated using the matplotlib Python library, operating directly from the Copperhead solver, highlighting the ability of Copperhead programs to interoperate with other Python modules, such as those for plotting.</p><p>Conjugate gradient performance depends strongly on matrixvector multiplication performance. Although we could have used a preexisting sparse-matrix library and representation, in this case we know some things about the structure of the matrix, which arises from a coupled 5-point stencil pattern on a vector field. Taking advantage of this structure, we can achieve significantly better performance than any library routine by creating a custom matrix format and sparse matrix-vector multiplication routine. This is an ideal scenario for Copperhead, since the productivity gains enable programmers to write custom routines that take advantage of special knowledge about a problem, as opposed to using a prebuilt library.</p><p>In addition to writing a custom sparse-matrix vector multiplication routine, practically solving this problem requires the use of a preconditioner, since without preconditioning, convergence is orders of magnitude slower. We utilize a block Jacobi preconditioner. Figure <ref type="figure">6</ref> shows the Copperhead code for computing the preconditioner, which involves inverting a set of symmetric 2x2 matrices, with one matrix for each point in the vector field, as well as applying the preconditioner, which involes a large number of symmetric 2x2 matrix multiplicaitons.</p><p>We implemented the entire solver in Copperhead. The custom SpMV routine for this matrix runs within 10% of the hand-coded CUDA version, achieving 49 SP GFLOP/s on a GTX 480, whereas a hand-tuned CUDA version achieves 55 SP GFLOP/s on the same hardware. Overall, for the complete preconditioned conjugate gradient solver, the Copperhead generated code yields 71% of the custom CUDA implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Quadratic Programming: Nonlinear Support Vector Machine Training</head><p>Support Vector Machines are a widely used classification technique from machine learning. Support Vector Machines classify multidimensional data by checking where the data lies with respect to a decision surface. Nonlinear decision surfaces are learned via a Quadratic Programming optimization problem, which we implement in Copperhead. We make use of the Sequential Minimal Optimization algorithm, with first-order variable selection heuristic <ref type="bibr" target="#b15">[16]</ref>, coupled with the RBF kernel, which is commonly used for nonlinear SVMs. Computationally, this is an iterative algorithm, where at each step the majority of the work is to evaluate distance functions between all data points and two active data points, update an auxiliary vector and then to select the extrema from data-dependent subsets of this vector. More details can be found in the literature <ref type="bibr" target="#b4">[5]</ref>. Space does not permit us to include the Copperhead code that implements this solver, but we do wish to point out the RBF kernel evaluation code, shown in Figure <ref type="figure">9</ref>, is used both on its own, where the compiler creates a version that instantiates the computation across the entire machine, as well as nested as an inner computation within a larger parallel context, where the compiler fuses it into a sequential loop. We compare performance against GPUSVM, a publically available CUDA library for SVM training. Figure <ref type="figure">8</ref> shows the throughput of the Copperhead implementation of SVM training over four datasets. On average, the Copperhead implementation attains 51% of hand-optimized, well tuned CUDA performance, which is quite good, given the Copperhead implementation's simplicity. The main advantage of the hand coded CUDA implementation on this example compared to the Copperhead compiled code is the use of on-chip memories to reduce memory traffic, and we expect that as the Copperhead compiler matures, we will be able to perform this optimization in Copperhead as well, thanks to our existing shape analysis, which can inform data placement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Productivity</head><p>Productivity is difficult to measure, but as a rough approximation, Table <ref type="table" target="#tab_2">2</ref> provides the number of lines of code needed to implement the core functionality of the examples we have put forth, in C++/CUDA as well as in Python/Copperhead. On average, the Copperhead programs take about 4 times fewer lines of code than their C++ equivalents, which suggests that Copperhead programming is indeed more productive than the manual alternatives. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Future Work</head><p>There are many avenues for improving the Copperhead runtime and compiler. The first set involves improving the quality of code which the compiler emits. For example, the compiler currently does not reorder parallel primitives to increase fusion and reduce synchronization. The compiler also does not attempt to analyze data reuse and place small, yet heavily reused sequences in onchip memories to reduce memory bandwidth, nor does it attempt to take advantage of parallelism in the dataflow graph or parallel primitives. Another avenue for future work involves broadening the scope of programs which can be compiled by the Copperhead compiler, such as supporting multi-dimensional arrays, adding new data parallel primitives, and supporting forms of recursion other than tail recursion. We also intend to create other back-ends for the Copperhead compiler, in order to support other platforms besides CUDA. Some potential choices include support for multicore x86 processors and OpenCL platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper has shown how to efficiently compile a nested data parallel language to modern parallel microprocessors. Instead of using flattening transforms by default, we take advantage of nested parallel hardware by mapping data parallel computations directly onto the hardware parallelism hierarchy. This mapping is enabled by synchronization and shape analyses, which which we introduce. Instead of creating a new data parallel language, we have embedded Copperhead into a widely used productivity language, which makes Copperhead programs look, feel and operate like Python programs, interoperating with the vast array of Python libraries, and lowering the amount of investment it takes for a programmer to learn data parallel programming.</p><p>We then demonstrated that our compiler generates efficient code, yielding from 45 to 100% of the performance of hand crafted, well-optimized CUDA code, but with much higher programmer productivity. On average, Copperhead programs require 3.6 times fewer lines of code than CUDA programs, comparing only the core computational portions.</p><p>Large communities of programmers choose to use productivity languages such as Python, because programmer productivity is often more important than attaining absolute maximum performance. We believe Copperhead occupies a worthwhile position in the tradeoff between productivity and performance, providing performance comparable to that of hand-optimized code, but with a fraction of the programmer effort.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Procedure for computing Ax for a matrix A in CSR form and a dense vector x. Underlined operations indicate potential sources of parallel execution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>#</head><label></label><figDesc>lambda0 :: (a, a) → a def lambda0(Aij, xj): return op _ mul(Aij, xj)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. SpMV procedure from Figure 1 after transformation by the front end compiler.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Sample CUDA C++ code generated for spmv _ csr. Ellipses (...) indicate incidental type and argument information elided for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>,<ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>,[ * , * ,9, * ]] cols = [[0,1,0,1],[1,2,2,3],[ * , * ,3, * ]]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Average Double Precision Sparse Matrix Vector Multiply Performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Left: closeup of video frame. Right: gradient vector field for optical flow</figDesc><graphic coords="8,329.37,75.10,95.96,96.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Forming and applying the block Jacobi preconditioner</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Support Vector Machine Training Performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>A Copperhead program is executed, like any other Python program, by executing the sequence of its top-level statements. Selected procedure definitions within the body of the program may be marked with the Copperhead decorator, as in the following:</figDesc><table><row><cell>A Copperhead program is a Python program that imports the Cop-</cell></row><row><cell>perhead language environment:</cell></row><row><cell>from copperhead import *</cell></row><row><cell>@cu</cell></row><row><cell>def add _ vectors(x, y):</cell></row><row><cell>return map(lambda xi,yi: xi+yi, x, y)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>and E2 | E1 or E2 | E1 if Ep else E2</head><label></label><figDesc></figDesc><table><row><cell>Expressions that call and define functions must use only positional</cell></row><row><cell>arguments. Optional and keyword arguments are not allowed.</cell></row><row><cell>| F (E1, . . . , En) | lambda x1, . . . , xn: E</cell></row><row><cell>Copperhead relies heavily on map as the fundamental source of</cell></row><row><cell>parallelism and elevates it from a built-in function (as in Python) to</cell></row><row><cell>a special form in the grammar. Copperhead also supports a limited</cell></row><row><cell>form of Python's list comprehension syntax, which is de-sugared</cell></row><row><cell>into equivalent map constructions during parsing.</cell></row><row><cell>| map(F, A1, . . . , An)</cell></row><row><cell>| [E for x in A]</cell></row><row><cell>| [E for x1, . . . , xn in zip(A1, . . . , An)]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Number of Lines of Code for Example Programs</figDesc><table><row><cell>Example</cell><cell cols="2">CUDA &amp; Copperhead &amp;</cell></row><row><cell></cell><cell>C++</cell><cell>Python</cell></row><row><cell>Scalar CSR</cell><cell>16</cell><cell>6</cell></row><row><cell>Vector CSR</cell><cell>39</cell><cell>6</cell></row><row><cell>ELL</cell><cell>22</cell><cell>4</cell></row><row><cell>PCG Solver</cell><cell>172</cell><cell>79</cell></row><row><cell cols="2">SVM Training 429</cell><cell>111</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgments</head><p>Research partially supported by Microsoft (Award #024263) and Intel (Award #024894) funding and by matching funding from U.C. Discovery (Award #DIG07-10227). Additional support comes from Par Lab affiliates National Instruments, NEC, Nokia, NVIDIA, Samsung, and Sun Microsystems. Bryan Catanzaro was supported by an NVIDIA Graduate Fellowship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Implementing sparse matrix-vector multiplication on throughput-oriented processors</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC &apos;09: Proc. Conference on High Performance Computing Networking, Storage and Analysis</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vector models for data-parallel computing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<ptr target="http://www.cs.cmu.edu/~guyb/papers/Ble90.pdf" />
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Programming parallel algorithms</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="85" to="97" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Implementation of a portable nested data-parallel language</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Hardwick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sipelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zagha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="1994-04">Apr. 1994</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="4" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast Support Vector Machine Training and Classification on Graphics Processors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">SEJITS: Getting Productivity and Performance With Selective Embedded JIT Specialization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fox</surname></persName>
		</author>
		<idno>UCB/EECS-2010-23</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Berkeley</pubPlace>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data parallel Haskell: a status report</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M T</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leshchinskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2007 Workshop on Declarative Aspects of Multicore Programming</title>
		<meeting>2007 Workshop on Declarative Aspects of Multicore Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Clyther: Python language extension for opencl</title>
		<author>
			<persName><surname>Clyther</surname></persName>
		</author>
		<ptr target="http://clyther.sourceforge.net/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cython: C-extensions for python</title>
		<author>
			<persName><surname>Cython</surname></persName>
		</author>
		<ptr target="http://cython.org/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compiling python to a hybrid execution environment</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Amaral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPGPU &apos;10: Proc. 3rd Workshop on General-Purpose Computation on Graphics Processing Units</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="19" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Ct: A Flexible Parallel Programming Model for Tera-scale Architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ghuloum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sprangle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>White Paper, Intel Corporation</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">hiCUDA: a high-level directivebased language for GPU programming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Abdelrahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GPGPU-2: Proc. 2nd Workshop on General Purpose Processing on Graphics Processing Units</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data parallel algorithms</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Steele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<idno type="ISSN">0001-0782</idno>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1170" to="1183" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Thrust: A parallel template library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoberock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bell</surname></persName>
		</author>
		<ptr target="http://www.meganewtons.com/.Version1.2" />
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Haskell vs. Ada vs. C++ vs. Awk vs...an experiment in software prototyping productivity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hudak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Jones</surname></persName>
		</author>
		<idno>YALEU/DCS/RR-1049</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
			<pubPlace>New Haven, CT</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yale University Department of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improvements to Platt&apos;s SMO Algorithm for SVM Classifier Design</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Keerthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R K</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<idno type="ISSN">0899-7667</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="649" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Klöckner</surname></persName>
		</author>
		<author>
			<persName><surname>Codepy</surname></persName>
		</author>
		<ptr target="http://mathema.tician.de/software/codepy" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Pycuda: Gpu run-time code generation for high-performance computing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klöckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fasih</surname></persName>
		</author>
		<idno>CoRR, abs/0911.3456</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GPU kernels as data-parallel array computations in Haskell</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M T</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Exploiting Parallelism using GPUs and other Hardware-Assisted Methods</title>
		<imprint>
			<publisher>EPAHM</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">OpenMP to GPGPU: a compiler framework for automatic translation and optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eigenmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
		<meeting>14th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Data-parallel programming on the Cell BE and the GPU using the RapidMind development platform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mccool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GSPx Multicore Applications Conference</title>
		<meeting>GSPx Multicore Applications Conference</meeting>
		<imprint>
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Shader metaprogramming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Graphics Hardware</title>
		<meeting>Graphics Hardware</meeting>
		<imprint>
			<publisher>Eurographics Association</publisher>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scalable parallel programming with CUDA</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nickolls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="53" />
			<date type="published" when="2008-04">Mar/Apr 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nvidia Cuda C Programming</surname></persName>
		</author>
		<author>
			<persName><surname>Guide</surname></persName>
		</author>
		<ptr target="http://www.nvidia.com/CUDA.Version3.1" />
		<imprint>
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
		<respStmt>
			<orgName>NVIDIA Corporation</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Are Scripting Languages Any Good? A Validation of Perl, Python, Rexx, and Tcl against C, C++, and Java</title>
		<author>
			<persName><forename type="first">L</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computers</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="205" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">CUDA by Example: An Introduction to General-Purpose GPU Programming</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kandrot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-07">July 2010</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dense Point Trajectories by GPU-accelerated Large Displacement Optical Flow</title>
		<author>
			<persName><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010-09">September 2010</date>
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Accelerator: using data parallelism to program GPUs for general-purpose uses</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarditi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oglesby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="325" to="335" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Theano: A python array expression library</title>
		<author>
			<persName><surname>Theano</surname></persName>
		</author>
		<ptr target="http://deeplearning.net/software/theano/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Implementing the PGI accelerator model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Workshop on General-Purpose Computation on Graphics Processing Units</title>
		<meeting>3rd Workshop on General-Purpose Computation on Graphics essing Units</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
