<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Generalization and Adaptation using Low Rank Exemplar SVMs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wen</forename><surname>Li</surname></persName>
							<email>liwen@vison.ee.ethz.ch</email>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
							<email>dai@vison.ee.ethz.ch</email>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><forename type="middle">Van</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">W</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
							<email>vangool@vison.ee.ethz.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Laboratory</orgName>
								<address>
									<addrLine>ETH Z ürich</addrLine>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">the University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Generalization and Adaptation using Low Rank Exemplar SVMs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6532D6B4463075536C5B0B916B8F7E9D</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2704624</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2704624, IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>latent domains</term>
					<term>domain generalization</term>
					<term>domain adaptation</term>
					<term>exemplar SVMs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaptation between diverse source and target domains is a challenging research problem, especially in the real-world visual recognition tasks where the images and videos consist of significant variations in viewpoints, illuminations, qualities, etc. In this paper, we propose a new approach for domain generalization and domain adaptation based on exemplar SVMs. Specifically, we decompose the source domain into many subdomains, each of which contains only one positive training sample and all negative samples. Each subdomain is relatively less diverse, and is expected to have a simpler distribution. By training one exemplar SVM for each subdomain, we obtain a set of exemplar SVMs. To further exploit the inherent structure of source domain, we introduce a nuclear-norm based regularizer into the objective function in order to enforce the exemplar SVMs to produce a low-rank output on training samples. In the prediction process, the confident exemplar SVM classifiers are selected and reweigted according to the distribution mismatch between each subdomain and the test sample in the target domain. We formulate our approach based on the logistic regression and least square SVM algorithms, which are referred to as low rank exemplar SVMs (LRE-SVMs) and low rank exemplar least square SVMs (LRE-LSSVMs), respectively. A fast algorithm is also developed for accelerating the training of LRE-LSSVMs. We further extend Domain Adaptation Machine (DAM) to learn an optimal target classifier for domain adaptation, and show that our approach can also be applied to domain adaptation with evolving target domain, where the target data distribution is gradually changing. The comprehensive experiments for object recognition and action recognition demonstrate the effectiveness of our approach for domain generalization and domain adaptation with fixed and evolving target domains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D OMAIN adaptation techniques, which aim to reduce the domain distribution mismatch when the training and testing samples come from different domains, have been successfully used for a broad range of vision applications such as object recognition and video event recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. As a related research problem, domain generalization differs from domain adaptation, because it assumes that the target domain samples are not available during the training process. Without focusing on the generalization ability on the specific target domain, domain generalization techniques aim to better classify testing data from any unseen target domain <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Please refer to Section 2 for a brief review of existing domain adaptation and domain generalization techniques.</p><p>For visual recognition, most existing domain adaptation methods treat a whole dataset as one domain <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, the real world visual data is usually quite diverse. The images and videos could be captured from arbitrary viewpoints, under different illuminations, and using different equipments. In other words, the distribution of visual domain is complex, thus it is challenging to reduce the distribution mismatch between different domains.</p><p>Several recent works proposed to partition the source domain into multiple hidden domains <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. While those works showed the benefits to exploit latent domains in the source data for improving domain adaptation performance, it is a non-trivial task to discover the characteristic latent domains by explicitly partitioning the training samples into multiple clusters because many factors (e.g., pose and illumination) overlap and interact in images and videos in complex ways <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this work, we propose a new approach for domain generalization and adaptation by exploiting the intrinsic structure of positive samples from latent domains without explicitly partitioning the training samples into multiple clusters/domains. Our work builds up the recent ensemble learning method exemplar SVMs, in which we aim to train a set of exemplar classifiers with each classifier learnt by using one positive training sample and all negative training samples. Under the context of domain adaptation, the training set for learning each exemplar classifier (i.e., one positive training sample and all negative training samples) can be regarded as one subdomain, which is relatively less diverse and with a simpler distribution. So each learnt exemplar classifier is expected to have good generalization capability for one certain data distribution. When predicting the test sample from an arbitrary distribution, those exemplar classifiers are then combined to properly fit the target domain distribution and produce good prediction results.</p><p>To further enhance the discriminative capability of the learnt exemplar classifiers, we exploit the intrinsic latent structure in the source domain, as inspired by the recent latent domain discovery works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In particular, positive samples may come from multiple latent domains characterized by different factors. For the positive samples captured under similar conditions (e.g., frontal-view poses), their predictions from each exemplar classifier are expected to be similar to each other. Using the predictions from all the exemplar classifiers as the feature of each positive sample, we assume the prediction matrix consisting of the features of all positive samples should be low-rank in the ideal case. Based on this assumption, we formulate a new objective function by introducing a nuclear norm based regularizer on the prediction matrix into the objective function of exemplar SVMs in order to learn a set of more robust exemplar classifiers for domain generalization and domain adaptation. Therefore, we refer to our new approach as Low Rank Exemplar SVMs, or LRE-SVMs in short.</p><p>During the testing process, we can directly use the whole or a selected set of learnt exemplar classifiers for the domain generalization task when the target domain samples are not available during the training process. For the domain adaptation problem where the unlabeled target domain data is available, we propose an effective method to re-weight the selected set of exemplar classifiers based on the Maximum Mean Discrepancy (MMD) criterion, and further extend the Domain Adaptation Machine (DAM) method to learn an optimal target classifier.</p><p>In our preliminary conference paper <ref type="bibr" target="#b10">[11]</ref>, we have formulated our LRE-SVMs approach by using the logistic regression method to learn each exemplar classifier, which is computational expensive as we need to learn one classifier for each positive training sample. To improve the efficiency in the training process, in this paper, we formulate a new objective function based on the least square SVM method, and develop a fast algorithm for learning each exemplar classifier. For the testing process, we also extend our method to address the domain adaptation problem with evolving target domain, where the test data comes one by one, and the target data distribution may change gradually <ref type="bibr" target="#b11">[12]</ref>. We conduct comprehensive experiments for various visual recognition tasks using three benchmark datasets, and the results clearly demonstrate the effectiveness of our approach for domain generalization, and domain adaptation with fixed and evolving target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Traditional domain adaptation methods can be roughly categorized into feature based approaches and classifier based approaches. The feature based approaches aim to learn domain invariant features for domain adaptation. Kulis et al. <ref type="bibr" target="#b0">[1]</ref> proposed a distance metric learning method to reduce domain distribution mismatch by learning asymmetric nonlinear transformation. Gopalan et al. <ref type="bibr" target="#b1">[2]</ref> and Gong et al. <ref type="bibr" target="#b2">[3]</ref> proposed two domain adaptation methods by interpolating intermediate domains. To reduce domain distribution mismatch, some recent approaches learnt a domain invariant subspace <ref type="bibr" target="#b12">[13]</ref> or aligned two subspaces from both domains <ref type="bibr" target="#b13">[14]</ref>.</p><p>Moreover, with the advance of deep learning techniques, a few works have also proposed to learn domain invariant features based on the convolutional neural network (CNN) for image recognition <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. <ref type="bibr">Long et al.</ref> proposed to learn CNN features while minimizing the MMD of the features between two domains <ref type="bibr" target="#b17">[18]</ref>. Ganin and Lempitsky proposed to simultaneously minimize the classification loss and maximize the domain confusion with a CNN for unsupervised domain adaptation <ref type="bibr" target="#b15">[16]</ref>, while Tzeng et al. employed soft-label generated from source domain to train the CNN for target samples <ref type="bibr" target="#b14">[15]</ref>. Li et al. <ref type="bibr" target="#b16">[17]</ref> proposed to apply the batch normalization method for domain adaptation with the CNN.</p><p>Our work is different from those works in two folds. First, those works focus on learning domain-adaptive features for a specifical target domain, while our approaches can be applied for domain generalization problem where the target domain is unseen during the training process. Second, our approaches and those methods are at different stages for visual recognition. Instead of learning feature representations, our approaches aim to learn robust classifiers, thus the learnt feature representations from their method can be used as input of our approaches if there still exists domain distribution mismatch.</p><p>Classifier based approaches directly learn the classifiers for domain adaptation, among which SVM based approaches are the most popular ones. Duan et al. <ref type="bibr" target="#b3">[4]</ref> proposed Adaptive MKL based on multiple kernel learning (MKL), and a multi-domain adaptation method by selecting the most relevant source domains <ref type="bibr" target="#b18">[19]</ref>. The work in <ref type="bibr" target="#b19">[20]</ref> developed an approach to iteratively learn the SVM classifier by labeling the unlabeled target samples and simultaneously removing some labeled samples in the source domain. For the unsupervised domain adaptation scenario, training domain adaptive classifiers by reweighing the source training samples has been widely exploited for reducing the distribution mismatch between the source and target domains. Various approaches have been proposed to learn the weights (a.k.a. density ratios) based on different strategies and criteria in the literature, which includes Kernel Mean Machting (KMM) <ref type="bibr" target="#b20">[21]</ref>, the MaxNet based methods <ref type="bibr" target="#b21">[22]</ref>, Kullback-Leibler Importance Estimation Procedure (KLIEP) <ref type="bibr" target="#b22">[23]</ref>, the Logistic Regression based method (LogReg) <ref type="bibr" target="#b23">[24]</ref>, and Large Scale KLIEP (LS-KLIEP) <ref type="bibr" target="#b24">[25]</ref>.</p><p>There are a few works specifically designed for domain generalization. <ref type="bibr">Muandet et al.</ref> proposed to learn domain invariant feature representations <ref type="bibr" target="#b6">[7]</ref>. Given multiple source datasets/domains, Khosla et al. <ref type="bibr" target="#b7">[8]</ref> proposed an SVM based approach, in which the learnt weight vectors that are common to all datasets can be used for domain generalization. Ghifary et al. <ref type="bibr" target="#b25">[26]</ref> proposed an approach for learning domain-invariant features based on the auto-encoder method. Inspired by our preliminary conference work <ref type="bibr" target="#b10">[11]</ref>, Niu et al. <ref type="bibr" target="#b26">[27]</ref> proposed an approach for multi-view domain generalization.</p><p>Our work is more related to the recent works for discovering latent domains <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, a clustering based approach is proposed to divide the source domain into different latent domains. In <ref type="bibr" target="#b9">[10]</ref>, the MMD criterion is used to partition the source domain into distinctive latent domains. However, their methods need to decide the number of latent domains beforehand. In contrast, our method exploits the low-rank structure from latent domains without requiring the number of latent domains. Moreover, we directly learn the exemplar classifiers without partitioning the data into clusters/domains.</p><p>Our work builds up the recent work on exemplar SVMs <ref type="bibr" target="#b27">[28]</ref>. In contrast to <ref type="bibr" target="#b27">[28]</ref>, we introduce a nuclear norm based regularizer on the prediction matrix in order to exploit the low-rank structure from latent domains for domain generalization. Recently, inspired by our preliminary conference work <ref type="bibr" target="#b10">[11]</ref>, Xu et al. <ref type="bibr" target="#b28">[29]</ref> also employed nuclear norm to exploit the low-rank structure for sub-categorization. In multitask learning, the nuclear norm based regularizer is also introduced to enforce the related tasks share similar weight vectors when learning the classifiers for multiple tasks <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>. However, their works assume the training and testing samples come from the same distribution without considering the domain generalization or domain adaptation tasks. Moreover, our regularizer is on the prediction matrix such that we can better exploit the structure of positive samples from multiple latent domains. Domain adaptation with changing distribution also attracts attentions from computer vision researchers. Lee et al. <ref type="bibr" target="#b31">[32]</ref> studied the change of car styles through the past decades. Hoffman et al. <ref type="bibr" target="#b11">[12]</ref> studied the domain adaptation problem with evolving target domain, where the distribution of target domain is gradually changing. They proposed an approach to learn the subspaces along manifold for domain adaptation. Lampert <ref type="bibr" target="#b32">[33]</ref> studied the time-varying data problem, where the source domain data distribution is varying, and proposed a method for predicting the data in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LOW RANK EXEMPLAR SVMS</head><p>In this section, we introduce the formulation of our low rank exemplar SVMs as well as the optimization algorithm. For ease of presentation, in the remainder of this paper, we use a lowercase/uppercase letter in boldface to represent a vector/matrix. The transpose of a vector/matrix is denoted by using the superscript ′ . A = [a ij ] ∈ R m×n defines a matrix A with a ij being its (i, j)-th element for i = 1, . . . , m and j = 1, . . . , n. The element-wise product between two matrices</p><formula xml:id="formula_0">A = [a ij ] ∈ R m×n and B = [b ij ] ∈ R m×n is defined as C = A • B, where C = [c ij ] ∈ R m×n and c ij = a ij b ij .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Exemplar SVMs</head><p>The exemplar SVMs model was first introduced in <ref type="bibr" target="#b27">[28]</ref> for object detection. In exemplar SVMs, each exemplar classifier is learnt by using one positive training sample and all the negative training samples. Let X s = X + ∪X -denote the set of training samples, in which X + = {x + 1 , . . . , x + n } is the set of positive training samples, and X -= {x - 1 , . . . , x - m } is the set of negative training samples. Each training sample x + or x -is a d-dimensional column vector, i.e., x + , x -∈ R d . We first develop our LRE-SVMs approach based on the logistic regression method, and then introduce a variant based on the formulation of least square SVMs for improving the efficiency of the training process. Specifically, given any sample x ∈ R d , the prediction function using logistic regression can be written as:</p><formula xml:id="formula_1">p(x|w i ) = 1 1 + exp(-w ′ i x) , (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>where w i ∈ R d is the weight vector in the i-th exemplar classifier trained by using the positive training sample x + i and all negative training samples 1 . By defining a weight matrix W = [w 1 , . . . , w n ] ∈ R d×n , we formulate the learning problem as follows,</p><formula xml:id="formula_3">min W ∥W∥ 2 F + C 1 n ∑ i=1 l(w i , x + i ) + C 2 n ∑ i=1 m ∑ j=1 l(w i , x - j ),<label>(2)</label></formula><p>where ∥•∥ F is the Frobenius norm of a matrix, C 1 and C 2 are the tradeoff parameters analogous to C in SVM, and l(w, x) is the logistic loss, which is defined as:</p><formula xml:id="formula_4">l(w i , x + i ) = log(1 + exp(-w ′ i x + i )), (3) l(w i , x - j ) = log(1 + exp(w ′ i x - j )).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Exemplar SVMs for Domain Generalization and Adaptation</head><p>Each exemplar classifier is learnt to discriminate a unique exemplar positive training sample from all the negative samples, so the differences between exemplar SVM classifiers represent the differences between positive training samples. In other words, the exemplar classifier also learns the unique property of each exemplar positive training sample. Under the context of domain adaptation, the training data for learning each exemplar SVM classifier (i.e., one positive training sample and all negative training samples) can be considered as a small subdomain. By learning an exemplar classifier for each subdomain, we expect that the classifier encodes not only the corresponding category information of the exemplar positive sample, but also the domain property (e.g., pose, background, illumination, etc.) of the exemplar positive training sample. In the testing process, for any given target sample or target domain, we combine the exemplar classifiers with proper weights, and expect the domain properties of training data are also similar to those of target samples, thus producing better predictions by using the combined classifiers (see Section 5.2 for more details). Formally, let us denote D s i as the underlying distribution of a subdomain formed by {x + i , x - j | m j=1 }, and denote f i as the learnt exemplar classifier. We also denote the distribution of the test data as D t . It has been shown in the previous works <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> that the error of f i on the test data can be bounded as</p><formula xml:id="formula_5">err t (f i ) ≤ err s (f i ) + dist(D s i , D t ) + ∆, where err t (f i ) is the test error, err s (f i ) is the training error of f i , dist(D s i , D t )</formula><p>is the distance between two distributions under certain measurement, and ∆ is a constant term. The distribution distance term dist(D s i , D t ) plays an important role in the generalization bound for domain adaptation. Given two different exemplar classifiers f i and f j , their training sets share the same negative training samples, so the difference between the underlying distributions D i and D j is from the two exemplar positive training samples x i and x j . This means that dist(D s i , D t ) (resp., dist(D s j , D t )) tends to be decided by whether x i (resp., x j ) is closer to the distribution of the test data.</p><p>The exemplar SVMs can be directly used for domain generalization, as the target domain unlabeled data is not required in the training stage. Generally, in the domain 1. Although we do not explicitly use the bias term in the prediction formulation, in our experiments we append 1 to the feature vector of each training sample.</p><p>generalization task, the test data is assumed to come from an arbitrary target domain that is unseen in the training procedure. However, based on the above analysis, the generalization error of exemplar classifiers tends to be huge if the target domain distribution is considerably different from those of all the subdomains (i.e., dist(D s i , D t ) is huge ∀i). For domain adaptation, the traditional domain adaptation methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> usually aim to learn a common subspace or an adaptive classifier to handle the domain distribution mismatch. Those methods can be regarded as using the global distribution information from the source and target domains for domain adaptation. However, for visual recognition applications where the training and test data are with large variances, the data distribution of one domain is usually complex, so it might be challenging to match two distributions by using global distribution information. In contrast, our approach focuses on each positive sample by using exemplar SVMs, and learns the local information of each exemplar. Each exemplar classifier encodes certain local distribution information related to this exemplar. So it is more flexible to match a single target sample or even a target domain with different data distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Low-Rank Exemplar SVMs</head><p>A drawback of exemplar SVMs is that training exemplar classifier with only one exemplar positive training sample might be sensitive to the noise. For example, the exemplar classifiers of two images with similar objects might produce different predictions for the same test image. To this end, we consider to discover the intrinsic latent structure in the source training data to improve discriminate capacity of exemplar classifiers, as inspired by the recent latent domain discovery methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>Intuitively, if there are multiple latent domains in the training data (e.g., poses, backgrounds, illuminations, etc.), the positive training samples should also come from several latent domains. For the positive samples captured under similar conditions (e.g., frontal-view poses), the prediction from each exemplar classifier is expected to be similar to each other. Using the predictions from all the exemplar classifiers as the feature of each positive sample, we assume the prediction matrix consisting of the predictions of all positive samples should be low-rank in the ideal case. Formally, we denote the prediction matrix as</p><formula xml:id="formula_6">G(W) = [g ij ] ∈ R n×n , where each g ij = p(x + i |w j</formula><p>) is the prediction of the i-th positive training sample by using the j-th exemplar classifier. We also denote the objective of exemplar SVMs in (2) as</p><formula xml:id="formula_7">J(W) = ∥W∥ 2 F + C 1 ∑ n i=1 l(w i , x + i ) + C 2 ∑ n i=1 ∑ m j=1 l(w i , x - j ).</formula><p>To exploit those latent domains, we thus enforce the prediction matrix G(W) to be low-rank when we learn those exemplar SVMs, namely, we arrive at the following objective function,</p><formula xml:id="formula_8">min W J(W) + λ∥G(W)∥ * , (<label>5</label></formula><formula xml:id="formula_9">)</formula><p>where we use the nuclear norm based regularizer ∥G(W)∥ * to approximate the rank of G(W). It has been shown that the nuclear norm is the best convex approximation of the rank function over the unit ball of matrices <ref type="bibr" target="#b35">[36]</ref>. However, it is a nontrivial task to solve the problem in ( <ref type="formula" target="#formula_8">5</ref>), because </p><formula xml:id="formula_10">J(W) + λ 1 ∥F∥ * + λ 2 ∥F -G(W)∥ 2 F ,<label>(6)</label></formula><p>which can be solved by alternatingly optimizing two subproblems w.r.t. W and F. Specifically, the optimization problem w.r.t. W does not contain the nuclear norm based regularizer, which makes the optimization much easier. Also, the nuclear norm based regularizer only depends on the intermediate matrix F rather than a non-linear term w.r.t. W (i.e., the prediction matrix G(W)) as in <ref type="bibr" target="#b4">(5)</ref>, thus the optimization problem w.r.t. F can be readily solved by using the Singular Value Threshold (SVT) method <ref type="bibr" target="#b36">[37]</ref> (see Section 3.4 for the details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussions:</head><p>The existing latent domain discovery methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> aim to explicitly partition the source domain data into several clusters, each corresponding to one latent domain. However, it is nontrivial to determine the number of latent domains, or to cluster the training samples into multiple subsets, because many factors (e.g., pose and illumination) overlap and interact in images and videos in complex ways <ref type="bibr" target="#b9">[10]</ref>. In contrast, our proposed approach exploits the latent domain structure in an implicit way based on the low-rank regularizer defined on the prediction matrix.</p><p>To better understand the effect of the low-rank regularizer, in Figure <ref type="figure" target="#fig_0">1</ref>, we show an example of the learnt prediction matrix G(W) from the "check watch" category in the IXMAS multi-view dataset by using Cam 0 and Cam 1 as the source domain. After using the nuclear norm based regularizer, we observe that the block diagonal property of the prediction matrix G(W) in Figure <ref type="figure" target="#fig_0">1</ref>(a). In Figure <ref type="figure" target="#fig_0">1(b)</ref>, we also display some frames from the videos corresponding to the two blocks with large values in G(W). We observe that the videos sharing higher values in the matrix G(W) are also visually similar to each other. For example, the first two rows in Figure <ref type="figure" target="#fig_0">1</ref>(b) are the videos from similar poses.</p><p>More interestingly, we also observe that our algorithm can group similar videos from different views into one block (e.g., the last three rows in Figure <ref type="figure" target="#fig_0">1</ref>(b) are the videos from the same actor), which demonstrates it is beneficial to exploit the latent source domains by using our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>In this section, we discuss how to optimize the problem in <ref type="bibr" target="#b5">(6)</ref>. We optimize (6) by iteratively updating W and F. The two subproblems w.r.t. W and F are described in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Update W:</head><p>When F is fixed, the subproblem w.r.t. W can be written as,</p><formula xml:id="formula_11">min W J(W) + λ 2 ∥G(W) -F∥ 2 F , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where the matrix F is obtained at the k-th iteration, and G(W) is defined as in Section 3.3. We optimize the above subproblem by using the gradient descent technique. Let us respectively define</p><formula xml:id="formula_13">X + = [x + 1 , . . . , x + n ] ∈ R d×n and X -= [x - 1 , . . . , x - m ] ∈ R d×m</formula><p>as the data matrices of positive and negative training samples, and also denote</p><formula xml:id="formula_14">H(W) = ∥G(W) -F∥ 2 F .</formula><p>Then, the gradients of the two terms in ( <ref type="formula" target="#formula_11">7</ref>) can be derived as follows,</p><formula xml:id="formula_15">∂J(W) ∂W = 2W + C 1 X + (P + -I) + C 2 X -P -, ∂H(W) ∂W = 2X + (G(W) • (11 ′ -G(W)) • (G(W) -F)) ,</formula><p>where</p><formula xml:id="formula_16">P + = diag ( p(x + i |w i )</formula><p>) ∈ R n×n is a diagonal matrix with each diagonal entry being the prediction on one positive sample by using its corresponding exemplar classifier, P -= [p(x - i |w j )] ∈ R m×n is the prediction matrix on all negative training samples by using all exemplar classifiers, I ∈ R n×n is an identity matrix, and 1 ∈ R n is a vector with all entries being 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Update F:</head><p>When W is fixed, we calculate the matrix G = G(W) at first, then the subproblem w.r.t. F becomes,</p><formula xml:id="formula_17">min F λ 1 ∥F∥ * + λ 2 ∥F -G∥ 2 F ,<label>(8)</label></formula><p>which can be readily solved by using the singular value thresholding (SVT) method <ref type="bibr" target="#b36">[37]</ref>. Specifically, let us denote the singular value decomposition of G as G = UΣV ′ , where U, V ∈ R n×n are two orthogonal matrices, and Σ = diag(σ i ) ∈ R n×n is a diagonal matrix containing all the singular values. The singular value thresholding operator on G can be represented as UD(Σ)V ′ , where</p><formula xml:id="formula_18">D(Σ) = diag((σ i -λ1 2λ2 ) + )</formula><p>, and ( • ) + is a thresholding operator by assigning the negative elements to be zeros.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Algorithm:</head><p>We summarize the optimization procedure in Algorithm 1 and name our method as Low-rank Exemplar SVMs (LRE-SVMs). Specifically, we first initialize the weight matrix W as W 0 , where W 0 is obtained by solving the traditional exemplar SVMs formulation in <ref type="bibr" target="#b1">(2)</ref>. Then we calculate the prediction matrix G(W) by applying the learnt classifiers Calculate the prediction matrix G(W) based on the current W.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Solve for F by optimizing the problem in <ref type="bibr" target="#b7">(8)</ref> with the SVT method. Update W by solving the problem in <ref type="bibr" target="#b6">(7)</ref> with the gradient descent method. 6: until The objective converges or the maximum number of iterations is reached. Output: The weight matrix W. on all positive samples. Next, we obtain the matrix F by solving the problem in <ref type="bibr" target="#b7">(8)</ref> with the SVT method. After that, we use the gradient descent method to update the weight matrix W. The above steps are repeated until the algorithm converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LOW-RANK EXEMPLAR LEAST SQUARE SVMS</head><p>While being able to exploit subdomains, LRE-SVMs is computationally inefficient. This is inherited from the exemplar SVMs method, in which an exemplar SVM is required to be trained for each positive training sample. In this section, we develop an efficient algorithm with a new LRE-SVMs formulation based on the least square SVMs, which is referred to as Low-Rank Exemplar Least Square SVMs or LRE-LSSVMs in short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Formulation</head><p>In the same spirit of exemplar SVMs, we learn an SVM classifier with the least square SVM method by using each positive sample and all the negative samples. In particular, let us denote the decision function as g(x) = w ′ ϕ(x). Based on the least square SVM method, we formulate the exemplar least square SVMs problem as follows, min W,ηi,ξi,j</p><formula xml:id="formula_19">1 2 ∥W∥ 2 + 1 2 C 1 n ∑ i=1 η 2 i + 1 2 C 2 n ∑ i=1 m ∑ j=1 ξ 2 i,j<label>(9)</label></formula><p>s.t.</p><formula xml:id="formula_20">w ′ i ϕ(x + i ) + η i = 1 w ′ i ϕ(x - j ) + ξ i,j = -1,</formula><p>where W = [w 1 , . . . , w n ] ∈ R d×n , and w i ∈ R d is the weight vector for the decision function of the i-th exemplar least square SVM.</p><p>Let us denote the objective as</p><formula xml:id="formula_21">J 2 (W) = 1 2 ∥W∥ 2 + 1 2 C 1 ∑ n i=1 η 2 i + 1 2 C 2 ∑ n i=1 ∑ m j=1 ξ 2 i,j</formula><p>. By substituting it into the objective function of <ref type="bibr" target="#b5">(6)</ref>, we arrive at our new LRE-LSSVMs formulation based on the exemplar least square SVMs as follows, min</p><formula xml:id="formula_22">W,F J 2 (W) + λ 1 ∥F∥ * + λ 2 ∥F -G(W)∥ 2 F , (<label>10</label></formula><formula xml:id="formula_23">)</formula><p>where G(W) consists of the decision values of the exemplar classifiers on all positive training samples, i.e.,</p><formula xml:id="formula_24">G(W) = [g ij ] ∈ R n×n with g ij = w ′ j ϕ(x + i ).</formula><p>Similarly, we also opti- mize the new problem by alternatingly update two matrices F and W. When updating F, the algorithm is the same as that in Section 3.4.2 based on the new definition of the matrix G, so we omit the details here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Fast Solution for Exemplar LSSVM</head><p>Now we discuss how to update W, which is also the most computationally expensive part for training LRE-SVMs. When F is fixed, the subproblem for updating W can be written as</p><formula xml:id="formula_25">min W J(W) + λ 2 ∥G(W) -F∥ 2 F ,<label>(11)</label></formula><p>which can be further decomposed into n independent subproblems, with each subproblem for one exemplar SVM. Specifically, the k-th subproblem can be written as</p><formula xml:id="formula_26">min w k ,η k ,ξ k,j 1 2 ∥w k ∥ 2 + 1 2 C 1 η 2 k + 1 2 C 2 m ∑ j=1 ξ 2 k,j + 1 2 λ 2 n ∑ i=1 ζ 2 i (12) s.t. w ′ k ϕ(x + k ) + η k = 1 (13) w ′ k ϕ(x - j ) + ξ k,j = -1, (<label>14</label></formula><formula xml:id="formula_27">)</formula><formula xml:id="formula_28">w ′ k ϕ(x + i ) + ζ i = f i,k , (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>where f i,k is the (i, k)-th entry of F. Moreover, the first constraint is on the exemplar sample x + k (i.e., the k-th positive training sample), the second constraint is on each negative training sample x - j , and the last constraint is on the i-th positive training sample x + i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let us introduce the dual variable vector</head><formula xml:id="formula_30">α = [α + , α - 1 , . . . , α - m , α f 1 , . . . , α f n ] ′ ∈ R n+m+1</formula><p>, where each of α + , α - j 's and α f i 's are the dual variables for those three types of constraints in ( <ref type="formula">13</ref>), ( <ref type="formula" target="#formula_26">14</ref>) and <ref type="bibr" target="#b14">(15)</ref>, respectively. We also use</p><formula xml:id="formula_31">a vector ỹ = [+1, -1, . . . , -1, f 1,k , . . . , f n,k ] ′ ∈ R n+m+1</formula><p>to denote the values on the right hand side of the above constraints. Then, the dual problem of ( <ref type="formula">12</ref>) can be written as,</p><formula xml:id="formula_32">( K + D)α = ỹ<label>(16)</label></formula><p>where the matrix K ∈ R (n+m+1)×(n+m+1) is an extended kernel matrix. Specifically, the elements in the first row of K are the inner products between the exemplar training sample and all samples, i.e.,</p><formula xml:id="formula_33">[ϕ(x + k ) ′ ϕ(x + k ), ϕ(x + k ) ′ ϕ(x - 1 ), . . . , ϕ(x + k ) ′ ϕ(x - m ), ϕ(x + k ) ′ ϕ(x + 1 ), . . . , ϕ(x + k ) ′ ϕ(x + n )].</formula><p>Similarly, the elements in the subsequent m rows are similarly defined as the inner products between all the negative samples and all samples, and the elements in remaining n rows are the inner products between all the positive training samples and all samples. The matrix D is a diagonal matrix, with the first element as 1 C1 , the following m elements as 1 C2 , and the remaining n elements ans 1 λ2 . The solution can be obtained in close form as α = ( K + D) -1 ỹ. For the linear kernel case, the weight vector in the decision function can be recovered as</p><formula xml:id="formula_34">w k = Xα, where X = [x k , x - 1 , . . . , x - m , x + 1 , . . . , x + n ].</formula><p>Although there is a closed form solution, it is still expensive to iteratively solve w for each exemplar SVM. The main cost is from the matrix inverse operation Calculate the prediction matrix G(W) based on the current W.</p><formula xml:id="formula_35">( K + D) -1 , which is O((n + m + 1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Solve for F by optimizing the problem in <ref type="bibr" target="#b7">(8)</ref> with the SVT method. Update W by iteratively solving n subproblems in <ref type="bibr" target="#b11">(12)</ref> based on the precomputed M -1 . 6: until The objective converges or the maximum number of iterations is reached. Output: The weight matrix W. matrices M's are only different in the elements from the first row and the first column, which makes it possible to efficiently calculate the inverse of those matrices M's. In particular, each matrix M can be decomposed as</p><formula xml:id="formula_36">M = [ m 11 m ′ 1 m 1 M ] (<label>17</label></formula><formula xml:id="formula_37">)</formula><p>where m 11 is the (1, 1)-th element of M, m 1 ∈ R n+m is the vector consists of the remaining elements in the first column of M, and M ∈ R (n+m)×(n+m) is a submatrix of M by eliminating the elements in the first row and the first column. Using the block matrix inverse lemma, we arrive at</p><formula xml:id="formula_38">M-1 = [ µ -µ(M -1 m 1 ) ′ -µM -1 m 1 M -1 + µM -1 m 1 m ′ 1 M -1 ] (<label>18</label></formula><formula xml:id="formula_39">)</formula><p>where µ =</p><formula xml:id="formula_40">1 m11-m ′ 1 M -1 m1</formula><p>. After calculating M -1 , the time complexity of calculating M-1 for each exemplar SVM is reduce to O((n + m) 2 ) only.</p><p>We summarize the optimization algorithm for LRE-LSSVMs in Algorithm 2. It is similar to Algorithm 1, except that we optimize a set of exemplar least square SVMs with the aforementioned fast algorithm at step 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ENSEMBLE EXEMPLAR CLASSIFIERS</head><p>After training the low-rank exemplar SVMs, we obtain n exemplar classifiers. To predict the test data, we discuss how to effectively use those learnt classifiers in three situations. The first one is the domain generalization scenario, where the target domain samples are not available during the training process. The second one is the domain adaptation scenario with fixed target domain, where we have unlabeled data in the target domain during the training process. And the third one is the domain adaptation scenario with evolving target domain, where the target test sample comes one by one, and is sampled from an unknown and gradually changing data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Domain Generalization</head><p>In the domain generalization scenario, we have no prior information about the target domain. A simple way is to equally fuse those n exemplar classifiers. Given any test sample x, the prediction p(x|W) can be calculated as,</p><formula xml:id="formula_41">p(x|W) = 1 n n ∑ i=1 p(x|w i ),<label>(19)</label></formula><p>where p(x|w i ) is the prediction from the i-th exemplar classifier. It can be the likelihood value (resp., the decision value) when using the logistic regression method (resp., the least square SVM method) for learning the exemplar classifier.</p><p>Recalling the training samples may come from several latent domains, a more practical way is to only use the exemplar classifiers from the latent domain that the test data likely belongs to. As aforementioned, each exemplar classifier encodes certain local information of the exemplar positive training sample. As a result, an exemplar classifier tends to output relatively higher prediction scores for the positive samples from the same latent domain, and relatively lower prediction scores for the positive samples from different latent domains. On the other hand, all exemplar classifiers are expected to output low prediction scores for the negative samples.</p><p>Therefore, given the test sample x during the test process, it is beneficial to fuse only the exemplar classifiers that output higher predictions, such that we output a higher prediction score if x is positive, and a low prediction score if x is negative. Let us define T (x) = { i | p(x|w i ) is one of the top K prediction scores for x} as the set of indices of those selected exemplar classifiers, then the prediction on this test sample can be obtained as,</p><formula xml:id="formula_42">p(x|W) = 1 K ∑ i:i∈T (x) p(x|w i ), (<label>20</label></formula><formula xml:id="formula_43">)</formula><p>where K is the predefined number of exemplar classifiers that output high prediction scores for the test sample x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Domain Adaptation with Fixed Target Domain</head><p>When we have unlabeled data in the target domain during the training process, we can further assign different weights to the learnt exemplar classifiers and better fuse the exemplar classifiers for predicting the test data from the target domain. Intuitively, when the training data of one exemplar classifier is closer to the target domain, we should assign a higher weight to this classifier and vice versa.</p><p>Let us denote the target domain samples as {x 1 , . . . , x u }, where u is the number of samples in the target domain. Based on the Maximum Mean Discrepancy (MMD) criterion <ref type="bibr" target="#b37">[38]</ref>, we define the distance between the target domain and the training samples for learning each exemplar classifier as follows,</p><formula xml:id="formula_44">d i = ∥ 1 n + m   nϕ(x + i ) + m ∑ j=1 ϕ(x - j )   - 1 u u ∑ j=1 ϕ(x j )∥ 2 , (<label>21</label></formula><formula xml:id="formula_45">)</formula><p>where ϕ(•) is a nonlinear feature mapping function induced by the Gaussian kernel. We assign a higher weight n to the positive sample x + i when calculating the mean of source domain samples, since we only use one positive sample for training the exemplar classifier at each time. In other words, we duplicate the positive sample x + i for n times and then combine the duplicated positive samples with all the negative samples to calculate the distance with the target domain.</p><p>With the above distance, we then obtain the weight for each exemplar classifier by using the RBF function as v i = exp(-d i /σ), where σ is the bandwidth parameter, and is set to be the median value of all distances. Then, the prediction on a test sample x can be obtained as,</p><formula xml:id="formula_46">p(x|W) = ∑ i:i∈T (x) ṽi p(x|w i ),<label>(22)</label></formula><p>where T (x) is defined as in Section 5.1, and ṽi = v i / ∑ i:i∈T (x) v i . One potential drawback with the above ensemble method is that we need to perform the predictions for n times, and then fuse the top K prediction scores. Inspired by Domain Adaptation Machine <ref type="bibr" target="#b18">[19]</ref>, we also propose to learn a single target classifier on the target domain by leveraging the predictions from the exemplar classifiers. Specifically, let us denote the target classifier as f (x) = w′ ϕ(x) + b. We formulate our learning problem as follows,</p><formula xml:id="formula_47">min w,b,ξi,ξ * i ,f 1 2 ∥ w∥ 2 + C u ∑ i=1 (ξ i + ξ * i ) + λ 2 Ω(f ),<label>(23)</label></formula><formula xml:id="formula_48">s.t. w′ ϕ(x i ) + b -f i ≤ ϵ + ξ i , ξ i ≥ 0, (24) f i -w′ ϕ(x i ) -b ≤ ϵ + ξ * i , ξ * i ≥ 0,<label>(25)</label></formula><p>where f = [f 1 , . . . , f u ] ′ is an intermediate variable, λ and C are the tradeoff parameters, ξ i and ξ * i are the slack variables in the ϵ-insensitive loss similarly as in SVR, and ϵ is a predifined small positive value in the ϵ-insensitive loss. The regularizer Ω(f ) is a smoothness function defined as follows,</p><formula xml:id="formula_49">Ω(f ) = u ∑ j=1 ∑ i:i∈T (xj ) ṽi (f j -p(x j |w i )) 2 , (<label>26</label></formula><formula xml:id="formula_50">)</formula><p>where we enforce each intermediate variable f j to be similar to the prediction scores from the selected exemplar classifiers in T (x j ) for the target sample x j . In the above problem, we use the ϵ-insensitive loss to enforce the prediction score from target classifier f (x j ) = w′ ϕ(x j ) + b to be close to the intermediate variable f j . At the same time, we also use a smoothness regularizer to enforce the intermediate variable f j to be close to the prediction scores of the selected exemplar classifiers in T (x j ) on the target sample x j . Intuitively, when ṽi is large, we enforce the intermediate variable f j to be closer to p(x j |w i ), and vice versa. Recall the weight ṽi models the importance of the i-th exemplar classifier for predicting the target sample, so we expect the learnt classifier f (x) performs well for predicting the target domain samples. By introducing the dual variables α = [α 1 , . . . , α u ] ′ and α * = [α * 1 , . . . , α * u ] ′ for the constraints in ( <ref type="formula">24</ref>) and ( <ref type="formula" target="#formula_48">25</ref>), we arrive at its dual form as follows,</p><formula xml:id="formula_51">min α,α * 1 2 (α -α * ) K(α -α * ) + p ′ (α -α * ) +ϵ1 ′ u (α + α * ),<label>(27)</label></formula><p>s.t.</p><formula xml:id="formula_52">1 ′ α = 1 ′ α * , 0 ≤ α, α * ≤ C1, (<label>28</label></formula><formula xml:id="formula_53">)</formula><p>where K = K + 1 λ I ∈ R u×u with K being the kernel matrix of the target domain samples, p = [p(x 1 |W), . . . , p(x u |W)] ′ with each entry p(x j |W) defined in <ref type="bibr" target="#b21">(22)</ref> being the "virtual label" for the target sample x j as in DAM <ref type="bibr" target="#b18">[19]</ref>. In DAM <ref type="bibr" target="#b18">[19]</ref>, the virtual labels of all the target samples are obtained by fusing the same set of source classifiers. In contrast, we use the predictions from different selected exemplar classifiers to obtain the virtual labels of different target samples. Therefore, DAM can be treated as a special case of our work by using the same classifiers for all test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Domain Adaptation with Evolving Target Domain</head><p>The domain adaptation with evolving target domain is a learning problem between domain generalization and domain adaptation. On one hand, the unlabeled target data is also unseen during the training process. On the other hand, we have prior information that the unlabeled target samples arrive sequentially, which are assumed to be sampled from an unknown distribution that changes gradually in the sequential order.</p><p>As discussed in <ref type="bibr" target="#b11">[12]</ref>, the traditional domain adaptation methods are not applicable to this problem, because we usually cannot access a set of unlabeled target samples, which are required by those methods for reducing the domain distribution mismatch. Moreover, even if we use the existing unlabeled target domain samples as input, the traditional domain adaptation methods may not work well, because the underlying data distribution is dynamically changing.</p><p>Since the target domain samples are not available during the training process, a possible way is to treat it as a domain generalization problem. More interestingly, recall our LRE-SVMs and LRE-LSSVMs encode the local domain properties into those exemplar classifiers. For any given test target sample, by using the ensemble method in (20), we can dynamically select multiple exemplar classifiers with appropriate domain properties that well match the target sample, so our LRE-SVMs and LRE-LSSVMs approaches are also expected to perform well on the domain adaptation task with evolving target domain by simply treating it as an domain generalization problem, which is verified in our experiments (see Section 6.3).</p><p>However, if we simply apply LRE-SVMs or LRE-LSSVMs, we would have ignored the prior information that the underlying distribution of target test samples is gradually changing. Therefore, to further improve the stability of domain adaptation, we propose to learn a single classifier to predict the test sample in the target domain, and gradually update the classifier when the test samples in the target domain come sequentially. In particular, we first consider the traditional domain adaptation problem where we are given a batch of unlabeled target samples, and then adapt it to the scenario when the unlabeled target samples come one by one.</p><p>Given a set of unlabeled target samples {x 1 , . . . , x u }, we employ the similar formulation in <ref type="bibr" target="#b22">(23)</ref> to learn a unified classifier f (x) = w′ x for predicting target domain samples, except replacing the ϵ-insensitive loss with the least square loss. In particular, we arrive at the following objective func- </p><formula xml:id="formula_54">if t = 1 then 4: Calculate H -1 = (x t x ′ t + 1 C I) -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Calculate w = H -1 x t p t .</p><p>6:</p><formula xml:id="formula_55">else 7: Calculate w ← w + H -1 xt(pt-w′ xt) 1+x ′ t H -1 xt 8: Calculate H -1 ← H -1 - H -1 xtx ′ t H -1 1+x ′ t H -1 xt . 9: end if 10:</formula><p>Predict the test sample f (x t ) = w′ x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Set t ← t + 1. 12: until The end of the input sequence. Output: Decision values {f (x 1 ), . . .}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>tion,</head><formula xml:id="formula_56">min w 1 2 ∥ w∥ 2 + C u ∑ i=1 ( w′ x i -p i ) 2 , (<label>29</label></formula><formula xml:id="formula_57">)</formula><p>which has a closed form solution w = (XX ′ + 1 C I) -1 Xp, where X = [x 1 , . . . , x u ] ∈ R d×u is the data matrix of the unlabled target samples, I ∈ R d×d is an identity matrix, and p = [p 1 , . . . , p u ] ′ with each p i = p(x i |W) being the prediction from LRE-SVMs as defined in <ref type="bibr" target="#b21">(22)</ref>.</p><p>When the target domain samples come sequentially, the problem in <ref type="bibr" target="#b28">(29)</ref> can also be solved in an online fashion. In particular, let us denote X t = [x 1 , . . . , x t ] ∈ R d×t as the data matrix consisting of the first t samples, and denote w t as the the weight vector of the prediction function at the tth time. We also define H t = X t X t ′ + 1 C I. According to the Woodbury formula, the inverse of H t+1 can be written as</p><formula xml:id="formula_58">H t+1 -1 = H t -1 - H -1 t x t+1 x ′ t+1 H -1 t 1 + x ′ t+1 H t -1 x t+1 . (<label>30</label></formula><formula xml:id="formula_59">)</formula><p>Therefore, the weight vector wt+1 at time (t + 1), can be updated as,</p><formula xml:id="formula_60">wt+1 = (X t+1 X ′ t+1 + 1 C I) -1 X t+1 p t+1 = wt + H -1 t x t+1 (p t+1 -w′ t x t+1 ) 1 + x ′ t+1 H -1 t x t+1 . (<label>31</label></formula><formula xml:id="formula_61">)</formula><p>We observe that the calculation of wt+1 and H t+1 only relies on the (t + 1)-th sample x t+1 and the matrix H t . Therefore, we can gradually update H t and wt , and predict the target sample sequentially without storing those target samples. We summarize the algorithm for domain adaptation with evolving target domain in Algorithm 3. Specifically, for the first target sample, we directly learn the least square SVM classifier using the closed form solution for <ref type="bibr" target="#b28">(29)</ref>. For the subsequent target samples, we update w and H -1 using the equations in <ref type="bibr" target="#b29">(30)</ref> and <ref type="bibr" target="#b30">(31)</ref>. The target sample is predicted by using the latest classifier. The process is repeated until no more target samples arrive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we evaluate our two low-rank exemplar SVMs (LRE-SVMs) approaches for domain generalization, and domain adaptation with fixed and evolving target domains, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Domain Generalization</head><p>In domain generalization scenario, only the source domain samples are available in the training process. The task is to train classifiers that generalize well to the unseen target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Experimental Setup</head><p>Following the work in <ref type="bibr" target="#b9">[10]</ref>, we use the Office-Caltech dataset <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b38">[39]</ref> for visual object recognition and the IXMAS dataset <ref type="bibr" target="#b39">[40]</ref> for multi-view action recognition.</p><p>Office-Caltech <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b38">[39]</ref> dataset contains the images from four domains, Amazon (A), Caltech (C), DSLR (D), and Webcam (W), in which the images are from Amazon, Caltech-256, and two more datasets captured with digital SLR camera and webcam, respectively. The ten common categories among the 4 domains are utilized in our evaluation. Following the recent works on unsupervised domain adaptation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b40">[41]</ref>, we extract the AlexNet f c7 feature <ref type="bibr" target="#b41">[42]</ref> for the images in the Office-Caltech dataset.</p><p>IXMAS dataset <ref type="bibr" target="#b39">[40]</ref> contains the videos from eleven actions captured by five cameras (Cam 0, Cam 1, . . . , Cam 4) from different viewpoints. Each of the eleven actions is performed three times by twelve actors. To exclude the irregularly performed actions, we keep the first five actions (check watch, cross arms, scratch head, sit down, get up) performed by six actors (Alba, Andreas, Daniel, Hedlena, Julien, Nicolas), as suggested in <ref type="bibr" target="#b9">[10]</ref>. We extract the dense trajectories features <ref type="bibr" target="#b42">[43]</ref> from the videos, and use K-means clustering to build a codebook with 1, 000 clusters for each of the five descriptors (i.e., trajectory, HOG, HOF, MBHx, MBHy). The bag-of-words features are then concatenated to a 5, 000 dimensional feature for each video sequence.</p><p>Following <ref type="bibr" target="#b9">[10]</ref>, we treat the images from different sources in the Office-Caltech dataset as different domains, and treat the videos from different viewpoints in the IXMAS dataset as different domains, respectively. In our experiments, we mix several domains as the source domain for training the classifiers and use the remaining domains as the target domain for testing. For the domain generalization task, the samples from the target domain are not available during the training process.</p><p>We compare our two low-rank exemplar SVMs approaches with the recent two latent domain discovering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We additionally report the results from the discriminative sub-categorization(Sub-C) method <ref type="bibr" target="#b43">[44]</ref>, as it can also be applied to our application. For all methods, we mix multiple domains as the source domain for training the classifiers following the experimental setup in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>For those two latent domain discovering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, after partitioning the source domain data into different domains using their methods, we train an SVM classifier on each domain, and then fuse those classifiers for predicting the test samples. We employ two strategies to fuse the learnt classifiers as suggested in <ref type="bibr" target="#b9">[10]</ref>, which are referred to as the ensemble strategy and the match strategy, respectively. The ensemble strategy is to re-weight the decision values from different SVM classifiers by using the domain probabilities learnt with the method in <ref type="bibr" target="#b8">[9]</ref>. In the match strategy, we first select the most relevant domain based on the MMD criterion, and then use the SVM classifier from this domain to predict the test samples. Moreover, we also report the results from the baseline SVM method, which is trained by using all training samples in the source domain. The results from exemplar SVMs (E-SVMs) (resp., exemplar least square SVMs (E-LSSVMs)) are also reported, which is a special case of our proposed LRE-SVMs (resp., LRE-LSSVMs) without considering the nuclearnorm based regularizer, and we also use the method in <ref type="bibr" target="#b19">(20)</ref> to fuse the selected top K exemplar classifiers for the prediction. We fix K = 5 for all our experiments. For other learning parameters, we fix the regularizer parameters as λ 1 = λ 2 = 10, and set the loss weight parameter C 1 = 10 and C 2 = 1 on the Office-Caltech dataset, and C 1 = 0.1 and C 2 = 0.001 on the IXMAS dataset. For the baseline methods, we choose the optimal parameters according to their recognition accuracies on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Experimental Results</head><p>The experimental results on two datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>. For the two latent domain discovering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, we observe that the recently published method by Gong et al. <ref type="bibr" target="#b9">[10]</ref> achieves quite competitive results when using the ensemble strategy (i.e., <ref type="bibr" target="#b9">[10]</ref>(Ensemble) in Table <ref type="table" target="#tab_0">1</ref>). It achieves better results in five cases when compared with SVM, which demonstrates it is beneficial to discover latent domains in the source domain. However, the method in <ref type="bibr" target="#b8">[9]</ref> is not as effective as <ref type="bibr" target="#b9">[10]</ref>. We also observe the match strategy generally achieves worse results than the ensemble strategy for those latent domain discovering methods, although the target domain information is used to select the most relevant discovered source domain in the testing process. Moreover, the Sub-C method also achieves better results on five cases when compared with SVM, because it also exploits the latent structure within each category, which could be helpful for improving the generalization ability similarly as the latent domain discovering methods. Compared with the baselines, our proposed LRE-SVMs and LRE-LSSVMs approaches achieve the best results in all six cases on two datasets, which clearly demonstrates the effectiveness of our method for domain generalization by exploiting the low-rank structure in the source domain. We also observe that our special cases (i.e., exemplar SVMs (E-SVMs) and exemplar least square SVMs (E-LSSVMs)) also achieve comparable or better results than SVM. Note we also apply the prediction method using ( <ref type="formula" target="#formula_42">20</ref>) for E-SVMs and E-LSSVMs. By selecting the most relevant classifiers, we combine a subset of exemplar classifiers for predicting each test sample, leading to good results. By further exploiting the low-rank structure in the source domain, we implicitly employ the information from latent domains in our LRE-SVMs and LRE-LSSVMs methods. In this way, the selected top K exemplar classifiers are more likely from the same latent domain that the test sample belongs to. Thus, our LRE-SVMs (resp., LRE-LSSVMs) method outperforms its special case E-SVMs (resp., E-LSSVMs) in all six cases for domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Effect of the Low-Rank Regularizer</head><p>In this section, we provide some qualitative results to better understand the effect of the low-rank based regularizer in our our proposed LRE-LSSVMs and LRE-SVMs methods. Intuitively, the low-rank regularizer aims to enforce the predictions on positive training samples from the exemplar classifiers to be low-rank. In Figure <ref type="figure" target="#fig_3">2</ref>, we show the prediction matrix G(W) for our LRE-LSSVMs method and its corresponding special case the E-LSSVMs method in the setting of "C, D, W A" on the Office-Caltech dataset for the category "laptop computer". As shown in Figure <ref type="figure" target="#fig_3">2(b)</ref>, the prediction matrix G of E-LSSVMs roughly exhibits the block diagonal property, which verifies our motivation that the predictions using the exemplar classifiers from the same latent domains tend to be similar. In Figure <ref type="figure" target="#fig_3">2</ref>(a), we also observe that the low-rank property of the matrix G becomes more obvious after using the low-rank regularizer in our LRE-LSSVMs method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Parameter Sensitivity Analysis</head><p>We show the performance changes of our LRE-LSSVMs method with respect to different parameters in Figure <ref type="figure" target="#fig_4">3</ref>. In particular, we take the Office-Caltech dataset with the case D, W → A, C as a study case, and run our method by varying each of the four parameters (i.e., C 1 , C 2 , λ 1 , and λ 2 ) in a certain range when fixing the others. In particular, the parameters can be divided into two groups, the loss </p><formula xml:id="formula_62">λ 1 ∈ [10 -2 , 10 -1 , 10 0 , 10 1 , 10 2 ], λ2 ∈ [2 -1 , 2 0 , 2 1 , 2 2 , 2 3 ].</formula><p>It can be observed that our method is relatively stable when varying each parameter in a certain range. While we fix the parameter as described above, further tuning the parameter will give better performance. For example, the performance goes up when we set a smaller C1 or C 2 . Generally, it is still an open problem to automatically select the optimal parameters for domain adaptation and domain generalization methods, because of the lack of validation data in the target domain. How to use the cross-validation strategy to choose the optimal parameters for our proposed methods without having any labeled target domain data will be an interesting research issue in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.5">Comparison of Two LRE-SVMs Approaches</head><p>Compared with the LRE-SVMs method developed in our preliminary conference work <ref type="bibr" target="#b10">[11]</ref>, our newly developed LRE-LSSVMs method uses the least square loss to replace the logistic regression loss for learning each exemplar classifier. We have also developed a fast algorithm for solving a batch of exemplar least square SVMs. In terms of recognition accuracies, we observe from Table <ref type="table" target="#tab_0">1</ref> that our new LRE-LSSVMs method generally achieve comparable results with the original LRE-SVMS method, where LRE-LSSVMs is better than LRE-SVMS in four cases, and worse in two cases only.</p><p>Besides the recognition accuracy comparison, the main advantage of the newly proposed LRE-LSSVMs method over the LRE-SVMS method is the time cost for training exemplar SVMs. We take the first case of the IXMAS dataset as an example to compare the training time of two approaches. Both methods are implemented with MATLAB, and we conduct the experiments on a workstation with Intel(R) Core i7-3770K CPU@3.50GHz and 16GB RAM. For each method, the total training time over all five actions is recorded. The experiments are repeated for 10 times for each method, and the average training time is reported in Table <ref type="table" target="#tab_4">2</ref>. It can be observed that the newly proposed LRE-LSSVMs approach is much faster (more than 80 times) than the original LRE- SVM approach on the IXMAS dataset for the case (Cam 0,1 → Cam 2,3,4). This shows the efficiency of our fast algorithm for learning the exemplar least square SVMs when updating W at the step 5 of Algorithm Therefore, we conduct the following experiments by using LRE-LSSVMs due to its efficiency and effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Domain Adaptation with Fixed Target Domain</head><p>In this section, we further evaluate our proposed method for the domain adaptation task, in which the unlabeled samples from the target domain are available in the training process.</p><p>For domain adaptation, we adopt the approach proposed in Section 5.2 to fuse the exemplar classifiers learnt by using our LRE-LSSVMs method, and we refer to our approach for domain adaptation as LRE-LSSVMs-DA. We compare our method with other domain adaptation methods on two tasks, action recognition and object recognition. Because the deep transfer learning methods are assumed to take images as the input, they were mainly applied to the image based object recognition task <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. We divide our experiments into two parts, comparisons with traditional domain adaptation methods for the video based action recognition on the IXMAS dataset, and comparisons with the CNN based domain adaptation methods for image based object recognition the Office-Caltech dataset <ref type="bibr" target="#b2">[3]</ref>. The experiments for image recognition on the benchmark Office dataset can also be found in the Supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Video Based Action Recognition</head><p>We first investigate the state-of-the-art unsupervised domain adaptation methods, including Kernel Mean Matching (KMM) <ref type="bibr" target="#b20">[21]</ref>, Sampling Geodesic Flow (SGF) <ref type="bibr" target="#b1">[2]</ref>, Geodesic Flow Kernel (GFK) <ref type="bibr" target="#b2">[3]</ref>, Selective Transfer Machine (STM) <ref type="bibr" target="#b44">[45]</ref>, Domain Invariant Projection (DIP) <ref type="bibr" target="#b12">[13]</ref>, and Subspace Alignment (SA) <ref type="bibr" target="#b13">[14]</ref>. For all those methods, we combine the videos captured from multiple cameras to form one combined source domain, and use the remaining samples as the target domain. Then we apply all the methods for domain adaptation. For the feature-based approaches (i.e., SGF, GFK, DIP and SA), we train an SVM classifier after obtaining the domain invariant features/kernels with those methods. We also select the best parameters for those baseline methods according to the test results. The results of those baseline methods are reported in Table <ref type="table" target="#tab_2">3</ref>. We also include the baseline SVM method trained by using all the source domain samples as training data for the comparison. Cross-view action recognition is a challenging task. As a result, most unsupervised domain adaptation methods cannot achieve promising results on this dataset, and they are worse than SVM in many cases. The recently proposed methods SA <ref type="bibr" target="#b13">[14]</ref> and DIP <ref type="bibr" target="#b12">[13]</ref> achieve relatively better results, which are better than SVM in two out of three cases. We further investigate the latent domain discovering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. We use their methods to divide the source domain into several latent domains. Then, we follow <ref type="bibr" target="#b9">[10]</ref> to perform the GFK <ref type="bibr" target="#b2">[3]</ref> method between each discovered latent domain and the target domain to learn a new kernel for reducing the domain distribution mismatch, and train SVM classifiers using the learnt kernels. Then we also use the two strategies (i.e., ensemble and match) to fuse the SVM classifiers learnt from different latent domains. Moreover, as the SA method achieves better results than GFK on the combined source domain, we further use the SA method to replace the GFK method for reducing the domain distribution mismatch between each latent domain and the target domain. The other steps are the same as those when using the GFK method. We report the results using latent domain discovering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> combined with GFK and SA in Table <ref type="table" target="#tab_2">3</ref>, which are denoted as GFK(latent) and SA(latent), respectively. As our method is also related to the DAM method <ref type="bibr" target="#b18">[19]</ref>, we also report the results of the DAM method by treating the discovered latent domains with <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> as multiple source domains, which is referred to as DAM(latent).</p><p>From Table <ref type="table" target="#tab_2">3</ref>, we observe GFK(latent) using the latent domains discovered by <ref type="bibr" target="#b9">[10]</ref> is generally better when compared with GFK(latent) using the latent domains discovered by <ref type="bibr" target="#b8">[9]</ref>. By using the latent domains discovered by <ref type="bibr" target="#b9">[10]</ref>, the results of GFK(latent) using both match and ensemble strategies are better than those of GFK on the combined source domain. However, most results from GFK(latent) are still worse than SVM, possibly because the GFK method cannot effectively handle the domain distribution mismatch between each discovered latent domain and the target domain. When using the SA method to replace GFK, we observe the results from SA(latent) in all three cases are improved when compared with their corresponding results from GFK(latent) by using the latent domains discovered by <ref type="bibr" target="#b9">[10]</ref>. Moreover, we also observe DAM(latent) outperforms SVM in all cases or most cases when using the latent source domains discovered by <ref type="bibr" target="#b9">[10]</ref> or <ref type="bibr" target="#b8">[9]</ref>. Our method achieves the best results in all three cases, which again demonstrates the effectiveness of our proposed LRE-LSSVMs-DA for exploiting the low-rank structure in the source domain. Moreover, our method LRE-LSSVMs-DA outperforms LRE-LSSVMs on three cases (see Table <ref type="table" target="#tab_0">1</ref>). Note LRE-LSSVMs does not use the target domain unlabeled samples during the training process. The results further demonstrate effectiveness of our domain adaptation approach LRE-LSSVMs-DA for coping with the domain distribution mismatch in the domain adaptation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Image Based Object Recognition</head><p>We compare our LRE-LSSVMs-DA method with the recently proposed deep transfer learning methods DAN <ref type="bibr" target="#b17">[18]</ref>, and Re-verseGrad <ref type="bibr" target="#b15">[16]</ref> for object recognition on the Office-Caltech dataset. We strictly follow the experimental setup in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. We first fine-tune pretrained AlexNet model based on the ImageNet dataset by using the labeled samples in the source domain, and then use the fine-tuned CNN model to extract the features from the images in both source and target domains. For DAN and ReverseGrad, we use their released source codes, and fine-tune the pretrained AlexNet model by using the suggested parameters in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>The results are shown in Table <ref type="table" target="#tab_6">4</ref>, in which the results of the baseline SVM method are also included for comparison. Comparing with the baseline SVM method in Table <ref type="table" target="#tab_0">1</ref> that directly uses AlexNet f c7 features, fine-tuning the networks using the labeled data from the source domain does not gain much improvements on this dataset. We also observe that DAN and ReverseGrad generally achieve quite good results when using multiple datasets as one source domain. However, there is no consistent winner when comparing these two methods and our method LRE-LSSVMs-DA. In particular, our method achieves the best result on the last case, while the DAN method and the ReverseGrad method win on the first and second case, respectively.</p><p>Moreover, the deep transfer learning methods are proposed to learn domain-invariant features, while our proposed LRE-SVMs and LRE-LSSVMs methods aim to improve the cross-domain generalization ability by learning robust exemplar classifiers, namely, their methods focus on feature learning, while our work focuses on classification. So our proposed LRE-LSSVMs method can be used to further improve the recognition accuracies by learning the exemplar classifiers with the features extracted by DAN and ReveseGrad, which are denoted by LRE-LSSVMs(DAN) and LRELSSVMs(ReverseGrad), respectively. From the last two rows of Table <ref type="table" target="#tab_6">4</ref>, it can be observed that LRE-LSSVMs(DAN) consistently outperforms DAN, while LRELSSVMs(ReverseGrad) is consistently better than ReveseGrad, which demonstrates that our LRELSSVMs method is complementary to the two deep transfer learning methods DAN and ReveseGrad by exploiting the local statistics to further enhance the generalization ability across domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Domain Adaptation with Evolving Target Domain</head><p>In the domain adaptation scenario with evolving target domain, unlabeled samples in the target domain are provided sequentially. Each target sample is assumed to be sampled from an unknown and gradually changing distribution. In other words, unlabeled samples in the target domain are unseen when learning the classifiers using labeled data in the source domain, and the adaptation is performed in an online fashion during the testing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Experimental Setup</head><p>Similarly as in <ref type="bibr" target="#b32">[33]</ref>, we conduct experiments on the CarEvolution dataset <ref type="bibr" target="#b45">[46]</ref> <ref type="foot" target="#foot_0">2</ref> , which consists of 1, 086 images of different cars. Each car is annotated with one of three manufactories (i.e., BMW, Mercedes or VW), as well as the year in which the car model was introduced (from 1972 to 2013). The task is to predict the manufactory of the car in each image.</p><p>As discussed in <ref type="bibr" target="#b31">[32]</ref>, the car style is gradually varying during the past decades. Therefore, the car images in the CarEvolution dataset can be assumed to be sampled from an underlying distribution, which is gradually changed chronologically. Different from the task in <ref type="bibr" target="#b32">[33]</ref>, we aim to investigate the classification performance when the target data is varying, so we split as many images as possible into the test set. We conduct the experiments in two settings. In the first setting, we use the images of cars after 1980 as the target domain and the remaining images of cars before 1980 as the source domain (i.e., ≤ 1980 →&gt; 1980), while in the second setting we use the images of cars after 1990 as the target domain and the remaining images of cars before 1990 as the source domain (i.e., ≤ 1990 →&gt; 1990). The target images are ordered chronologically, and are assumed to be provided one by one.</p><p>We extract the CNN features using the output from the "fc6" layer of the CAFFE reference model <ref type="bibr" target="#b46">[47]</ref>, which leads to a 4, 096 dimension feature vector for each image. Each feature vector is further normalized such that its ℓ 2 norm equals 1. When predicting the target samples, we employ Algorithm 3 based on the pre-learnt LRE-LSSVMs classifiers trained based the training samples from the source domain, and refer to our approach as LRE-LSSVMS-EDA. We compare our work with the recently proposed Continuous Manifold Adaptation (CMA) method <ref type="bibr" target="#b47">[48]</ref>  <ref type="foot" target="#foot_1">3</ref> . Following <ref type="bibr" target="#b47">[48]</ref>, we integrate the CMA approach with two subspace based domain adaptation methods, the GFK method and the SA method, and refer to them as CMA+GFK and CMA+SA, respectively. The SVM method is used to train the classifiers for predicting the target samples after applying CMA+GFK or CMA+SA. We also include the SVM method without domain adaptation as a baseline for comparison. For the baseline methods, the optimal parameters are chosen according to their best recognition accuracies on the test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Experimental Results</head><p>The recognition accuracies of all methods on the CarEvolution dataset are summarized in Table <ref type="table" target="#tab_7">5</ref>. Generally, the task become more challenging, when we split more samples into the test set. For example, the recognition accuracies of all methods in the first setting <ref type="bibr">(≤ 1980 →&gt; 1980</ref>) are lower than their corresponding results in the second setting <ref type="bibr">(≤ 1990 →&gt; 1990)</ref>. The CMA+GFK method achieves better results than the baseline SVM method in both settings by considering the evolving distribution of target domain samples, while the CMA+SA method does not perform well in the second setting. Our LRE-LSSVMs method achieves better results than two CMA methods, which demonstrates excellent generalization ability of our approach by combining the exemplar classifiers for domain generalization. By further considering the distribution changing on the target domain, our LRE-LSSVMs-EDA method achieves the best results in both settings, which clearly demonstrates the effectiveness of our LRE-LSSVMs-EDA approach for domain adaptation with evolving target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we have proposed a new approach called Low-rank Exemplar SVMs (LRE-SVMs) for domain generalization by exploiting the low-rank structure of positive training samples from multiple latent source domains. Specifically, based on the recent work on exemplar SVMs, we propose to exploit the low-rank structure in the source domain by introducing a nuclear-norm based regularizer on the prediction matrix consisting of the predictions of all positive samples from all exemplar classifiers. We develop a new LRE-SVMs approach based on the least square SVMs (referred to as LRE-LSSVMs), which is much faster than the original LRE-SVMs method. To additionally handle the domain distribution mismatch between the training and test data, we further develop an effective method to reweight the selected set of exemplar classifiers based on the Maximum Mean Discrepancy (MMD) criterion, and extend the Domain Adaptation Machine (DAM) method to learn a unified target classifier. We also develop a new algorithm for the domain adaptation problem with evolving target domain, where the data distribution of target domain is gradually changing. The comprehensive experiments have demonstrated the effectiveness of our approach for domain generalization, and domain adaptation with fixed and evolving target domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the prediction matrix G(W), where we observe the block diagonal property of G(W) in (a). The frames from the videos corresponding to the two blocks with large values in G(W) are also visually similar to each other in (b). the last term is a nuclear norm based regularizer on the prediction matrix G(W) and G(W) is non-linear w.r.t. W. To solve the optimization problem in (5), we introduce an intermediate matrix F ∈ R n×n to model the ideal G(W) such that we can decompose the last term in (5) into two parts: on one hand, we expect the intermediate matrix F should be low-rank as we discussed above; on the other hand, we enforce the prediction matrix G(W) to be close to the intermediate matrix F. Therefore, we reformulate the objective function as follows, min W,F</figDesc><graphic coords="4,309.02,40.67,283.83,123.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Visualization of the prediction matrix G(W).</figDesc><graphic coords="10,70.52,44.12,87.78,87.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The performance of our LRE-LSSVMs method when varying different parameters: (a) varying parameters C1 and λ 1 , and (b) varying parameters C 2 and λ2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1</head><label>1</label><figDesc>Optimization Algorithm for Low-rank Exemplar SVMs (LRE-SVMs) Input: Training data X s , and the parameters C 1 , C 2 , λ 1 , λ 2 .</figDesc><table /><note><p>1: Initialize W ← W 0 , where W 0 is obtained by solving (2). 2: repeat 3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Optimization Algorithm for Low-rank Least Square Exemplar SVMs (LRE-LSSVMs) Input: Training data X s , and the parameters C 1 , C 2 , λ 1 , λ 2 .1: Initialize W ← W 0 , where W 0 is obtained by solving<ref type="bibr" target="#b8">(9)</ref>, and calculate the matrix M -1 .</figDesc><table /><note><p><p>3 </p>) in time complexity. Let us define M = K + D. For different exemplar samples, the resultant Algorithm 2 2: repeat 3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Algorithm 3</head><label>3</label><figDesc>Low-rank Exemplar SVMs (LRE-SVMs) for Domain Adaptation with Evolving Target Domain Input: Sequential target data {x 1 , . . .}, weight matrix of LRE-SVMs W, and the tradeoff parameter C.</figDesc><table /><note><p>1: Set t = 1. 2: repeat 3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1</head><label>1</label><figDesc>Recognition accuracies (%) of different methods for domain generalization. Our LRE-SVMs and LRE-LSSVMs approaches do not require domain labels or target domain data during the training process. The results of our LRE-SVMs and LRE-LSSVMs approaches are denoted in boldface.</figDesc><table><row><cell>Source</cell><cell cols="5">A,C D,W C,D,W Cam 0,1 Cam 2,3,4 Cam 0,1,2,3</cell></row><row><cell>Target</cell><cell>D,W A,C</cell><cell>A</cell><cell cols="2">Cam 2,3,4 Cam 0,1</cell><cell>Cam 4</cell></row><row><cell>SVM</cell><cell cols="2">84.96 84.09 92.28</cell><cell>71.70</cell><cell>63.83</cell><cell>56.61</cell></row><row><cell cols="3">Sub-C [44] 85.28 84.33 92.17</cell><cell>78.11</cell><cell>76.90</cell><cell>64.04</cell></row><row><cell cols="3">[9](Ensemble) 83.41 83.37 89.67</cell><cell>71.55</cell><cell>51.02</cell><cell>49.70</cell></row><row><cell cols="3">[9](Match) 81.86 79.29 88.10</cell><cell>63.81</cell><cell>60.04</cell><cell>48.91</cell></row><row><cell cols="3">[10](Ensemble) 85.07 84.39 91.82</cell><cell>75.04</cell><cell>68.98</cell><cell>57.64</cell></row><row><cell cols="3">[10](Match) 84.71 84.22 92.14</cell><cell>71.59</cell><cell>60.73</cell><cell>55.37</cell></row><row><cell>E-SVMs</cell><cell cols="2">85.24 84.64 92.47</cell><cell>76.86</cell><cell>68.04</cell><cell>72.98</cell></row><row><cell cols="3">LRE-SVMs 85.29 85.01 92.66</cell><cell>79.96</cell><cell>80.15</cell><cell>74.97</cell></row><row><cell cols="3">E-LSSVMs 85.85 84.02 92.46</cell><cell>80.68</cell><cell>70.99</cell><cell>71.58</cell></row><row><cell cols="3">LRE-LSSVMs 87.56 84.72 92.99</cell><cell>81.05</cell><cell>81.81</cell><cell>72.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Average training time and standard deviation (in seconds) of LRE-SVMs and LRE-LSSVMs over 10 rounds of experiments on the IXMAS dataset (Cam 0,1 → Cam 2,3,4).</figDesc><table><row><cell>LRE-SVMS</cell><cell>LRE-LSSVMs</cell></row><row><cell>Time 236.34 ±1.93</cell><cell>2.70 ±0.03</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Recognition accuracies (%) of different methods for domain adaptation. The best results are denoted in boldface.</figDesc><table><row><cell></cell><cell>Source</cell><cell>Cam 0,1</cell><cell cols="2">Cam 2,3,4 Cam 0,1,2,3</cell></row><row><cell></cell><cell>Target</cell><cell>Cam 2,3,4</cell><cell>Cam 0,1</cell><cell>Cam 4</cell></row><row><cell></cell><cell>SVM</cell><cell>71.70</cell><cell>63.83</cell><cell>56.61</cell></row><row><cell></cell><cell>KMM</cell><cell>73.92</cell><cell>42.22</cell><cell>52.57</cell></row><row><cell></cell><cell>SGF</cell><cell>60.37</cell><cell>69.04</cell><cell>28.66</cell></row><row><cell></cell><cell>GFK</cell><cell>64.87</cell><cell>55.53</cell><cell>42.16</cell></row><row><cell></cell><cell>STM</cell><cell>68.69</cell><cell>70.53</cell><cell>51.05</cell></row><row><cell></cell><cell>DIP</cell><cell>65.20</cell><cell>70.03</cell><cell>62.92</cell></row><row><cell></cell><cell>SA</cell><cell>73.35</cell><cell>77.92</cell><cell>49.59</cell></row><row><cell></cell><cell>[9] (Match)</cell><cell>61.33</cell><cell>58.77</cell><cell>46.62</cell></row><row><cell>GFK</cell><cell>[9] (Ensemble)</cell><cell>65.32</cell><cell>55.01</cell><cell>42.09</cell></row><row><cell>(latent)</cell><cell>[10] (Match)</cell><cell>65.32</cell><cell>64.43</cell><cell>47.22</cell></row><row><cell></cell><cell>[10] (Ensemble)</cell><cell>69.12</cell><cell>68.87</cell><cell>51.30</cell></row><row><cell></cell><cell>[9] (Match)</cell><cell>58.49</cell><cell>56.27</cell><cell>55.87</cell></row><row><cell>SA</cell><cell>[9] (Ensemble)</cell><cell>63.01</cell><cell>62.05</cell><cell>62.69</cell></row><row><cell>(latent)</cell><cell>[10] (Match)</cell><cell>66.27</cell><cell>67.00</cell><cell>63.01</cell></row><row><cell></cell><cell>[10] (Ensemble)</cell><cell>71.04</cell><cell>76.64</cell><cell>72.26</cell></row><row><cell>DAM</cell><cell>[9]</cell><cell>77.92</cell><cell>76.99</cell><cell>53.76</cell></row><row><cell>(latent)</cell><cell>[10]</cell><cell>77.32</cell><cell>73.94</cell><cell>62.47</cell></row><row><cell cols="2">LRE-LSSVMs-DA</cell><cell>81.79</cell><cell>81.84</cell><cell>73.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Recognition accuracies (%) of different methods for domain adaptation on the Office-Caltech dataset. The best results are denoted in bold.</figDesc><table><row><cell>Source</cell><cell cols="2">A,C D,W C,D,W</cell></row><row><cell>Target</cell><cell>D,W A,C</cell><cell>A</cell></row><row><cell>SVM</cell><cell cols="2">85.40 83.76 92.28</cell></row><row><cell>DAN [18]</cell><cell cols="2">92.92 82.08 92.48</cell></row><row><cell>ReverseGrad [16]</cell><cell cols="2">92.25 88.90 93.11</cell></row><row><cell>LRE-LSSVMs-DA</cell><cell cols="2">89.55 88.15 93.27</cell></row><row><cell>LRE-LSSVMs(DAN)</cell><cell cols="2">94.54 83.24 93.07</cell></row><row><cell cols="3">LRE-LSSVMs(ReverseGrad) 92.55 89.32 93.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 5</head><label>5</label><figDesc>Recognition accuracies (%) of different methods for domain adaptation with evolving target domain, where the target domain distribution is gradually changing. The best results are denoted in boldface.</figDesc><table><row><cell></cell><cell cols="2">Setting 1 Setting 2</cell></row><row><cell>Source</cell><cell>≤ 1980</cell><cell>≤ 1990</cell></row><row><cell>Target</cell><cell>&gt; 1980</cell><cell>&gt; 1990</cell></row><row><cell>SVM</cell><cell>39.66</cell><cell>46.08</cell></row><row><cell>CMA+GFK</cell><cell>42.95</cell><cell>47.93</cell></row><row><cell>CMA+SA</cell><cell>42.73</cell><cell>44.39</cell></row><row><cell>LRE-LSSVMs</cell><cell>43.61</cell><cell>50.53</cell></row><row><cell>LRE-LSSVMs-EDA</cell><cell>44.33</cell><cell>51.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://homes.esat.kuleuven.be/ ∼ krematas/VisDA/ CarEvolution.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Codes are downloaded from http://cma.berkeleyvision.org/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Wen Li is a postdoctoral researcher with the Computer Vision Laboratory, ETH Z ürich, Switzerland. He received the Ph.D. degree from the Nanyang Technological University, Singapore in 2015. Before that, he received the B.S. and M.Eng degrees from the Beijing Normal University, Beijing, China, in 2007 and 2010, respectively. His main interests include transfer learning, multi-view learning, multiple kernel learning, and their applications in computer vision. Luc Van Gool got a degree in electromechanical engineering at the Katholieke Universiteit Leuven in 1981. Currently, he is a professor at the Katholieke Universiteit Leuven in Belgium and the ETH in Z ürich, Switzerland. He leads computer vision research at both places, where he also teaches computer vision. He has authored over 200 papers in this field. He has been a program committee member of several major computer vision conferences. His main interests include 3D reconstruction and modeling, object recognition, tracking, and gesture analysis. He received several Best Paper awards. He is a co-founder of 5 spin-off companies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Zheng</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What you saw is not what you get: Domain adaptation using asymmetric kernel transforms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual event recognition in videos by learning from web data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Patter Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1667" to="1680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Patter Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="465" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Patter Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1134" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reshaping visual datasets for domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure from latent domains for domain generalization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Coference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno>arXiv [cs.CV]</idno>
		<imprint>
			<date type="published" when="2016-03-15">15 Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources: A domain-dependent regularization approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</title>
		<imprint>
			<date type="published" when="2012-03">March 2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="504" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation problems: A DASVM classification technique and a circular validation strategy</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marconcini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Patter Analysis and Machine Intelligence (T-PAMI)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="770" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Phillip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bnau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative learning for differing training and test distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Direct density ratio estimation for large-scale covariate shift adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JIP</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="138" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Coference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-view domain generalization for visual recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-SVMs for object detection and beyond</title>
		<author>
			<persName><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure for discriminative sub-categorization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Integrating low-rank and groupsparse structures for robust multi-task learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Style-aware mid-level representation for discovering visual connections in space and time</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Predicting the future behavior of a time-varying probability distribution</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalization bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Guaranteed minimum rank solutions to linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Cands</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Gorgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Reserarch (JMLR)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3d exemplars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1412.3474</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Discriminative sub-categorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selective transfer machine for personalized facial action unit detection</title>
		<author>
			<persName><forename type="first">W.-S</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Does evolution cause a domain shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Continuous manifold based adaptation for evolving visual domains</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
