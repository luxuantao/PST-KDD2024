<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<address>
									<country>BNRist</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">KIRC</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<address>
									<country>BNRist</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cloud BU</orgName>
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangming</forename><surname>Pan</surname></persName>
							<email>liangmingpan@u.nus.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<addrLine>5 ETH Zürich 6 Noah&apos;s Ark Lab</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Huawei Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lunyiu</forename><surname>Nie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<address>
									<country>BNRist</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Xiang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
							<email>houlei@</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<address>
									<country>BNRist</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">KIRC</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
							<email>lijuanzi@tsinghua.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<address>
									<country>BNRist</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">KIRC</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bin</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Complex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including ~120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro serves for both KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/ KQAPro_Baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Thanks to the recent advances in deep models, especially large-scale unsupervised representation learning <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, question answering of simple questions over knowledge base (Simple KBQA), i.e., single-relation factoid questions <ref type="bibr" target="#b6">(Bordes et al., 2015)</ref>, begins to saturate <ref type="bibr" target="#b31">(Petrochuk and Zettlemoyer, 2018;</ref><ref type="bibr">Wu et al., 2019;</ref><ref type="bibr" target="#b17">Huang et al., 2019)</ref>. However, tackling complex questions (Complex KBQA) is still an ongoing challenge, due to the unsatisfied capability of compositional reasoning. As shown in Table <ref type="table" target="#tab_1">1</ref>, to promote the community development, several benchmarks are proposed for Complex KBQA, including LC-QuAD2.0 <ref type="bibr" target="#b13">(Dubey et al., 2019)</ref>, ComplexWebQuestions <ref type="bibr" target="#b42">(Talmor and Berant, 2018)</ref>, MetaQA <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref>, CSQA <ref type="bibr" target="#b34">(Saha et al., 2018)</ref>, CFQ <ref type="bibr" target="#b20">(Keysers et al., 2020)</ref>, and so on. However, they suffer from the following problems:</p><p>1) Most of them only provide QA pairs without explicit reasoning processes, making it challenging for models to learn compositional reasoning. Some researchers try to learn the reasoning processes with reinforcement learning <ref type="bibr" target="#b25">(Liang et al., 2017;</ref><ref type="bibr" target="#b33">Saha et al., 2019;</ref><ref type="bibr">Ansari et al., 2019)</ref> and searching <ref type="bibr" target="#b16">(Guo et al., 2018)</ref>. However, the prohibitively huge search space hinders both the performance and speed, especially when the question complexity increases. For example, <ref type="bibr" target="#b33">Saha et al. (2019)</ref> achieved a 96.52% F1 score on simple questions in CSQA, whereas only 0.33% on complex questions that require comparative count. We think that intermediate supervision is needed for learning the compositional reasoning, mimicking the learning process of human beings <ref type="bibr" target="#b17">(Holt, 2017)</ref>.</p><p>2) Questions are not satisfactory in diversity and scale. For example, MetaQA <ref type="bibr" target="#b46">(Zhang et al., 2018)</ref> questions are generated using just 36 templates, and they only consider relations between entities, ignoring literal attributes; LC-QuAD2.0 <ref type="bibr" target="#b13">(Dubey et al., 2019)</ref> and ComplexWebQuestions <ref type="bibr" target="#b42">(Talmor and Berant, 2018)</ref> have fluent and diverse humanwritten questions, but their scale is less than 40K.</p><p>To address these problems, we create KQA Pro, a large-scale benchmark for Complex KBQA. In KQA Pro, we define a Knowledge-oriented Programming Language (KoPL) to explicitly describe When did Cleveland Cavaliers pick up LeBron James SELECT <ref type="bibr">DISTINCT ?qpv WHERE { ?e_1 &lt;pred:name&gt; "LeBron James" . ?e_2 &lt;pred:name&gt; "Cleveland Cavaliers" . ?e_1 &lt;drafted by&gt; ?e_2 . [ &lt;pred:fact_h&gt; ?e_1 ;</ref><ref type="bibr">&lt;pred:fact_r&gt; &lt;winner&gt; ;</ref><ref type="bibr">&lt;pred:fact_t&gt; ?e_2 ]</ref>  the reasoning process for solving complex questions (see Fig. <ref type="figure">1</ref>). A program is composed of symbolic functions, which define the basic, atomic operations on KBs. The composition of functions well captures the language compositionality (Baroni, 2019). Besides KoPL, following previous works <ref type="bibr">(Yih et al., 2016;</ref><ref type="bibr" target="#b39">Su et al., 2016)</ref>, we also provide the corresponding SPARQL for each question, which solves a complex question by parsing it into a query graph. Compared with SPARQL, KoPL 1) provides a more explicit reasoning process. It divides the question into multiple steps, making human understanding easier and the intermediate results more transparent; 2) allows humans to control the model behavior better, potentially supporting human-in-the-loop. When the system gives a wrong answer, users can quickly locate the error by checking the outputs of intermediate functions. We believe the compositionality of KoPL and the graph structure of SPARQL are two complementary directions for Complex KBQA.</p><p>To ensure the diversity and scale of KQA Pro, we follow the synthesizing and paraphrasing pipeline in the literature <ref type="bibr" target="#b45">(Wang et al., 2015a;</ref><ref type="bibr" target="#b7">Cao et al., 2020)</ref>, first synthesize large-scale (canonical question, KoPL, SPARQL) triples, and then paraphrase the canonical questions to natural language questions (NLQs) via crowdsourcing. We combine the following two factors to achieve diversity in questions: (1) To increase structural variety, we leverage a varied set of templates to cover all the possible queries through random sampling and recursive composing; (2) To increase linguistic variety, we filter the paraphases based on their edit distance with the canonical utterance. Finally, KQA Pro consists of 117,970 diverse questions that involve varied reasoning skills (e.g., multi-hop reasoning, value comparisons, set operations, etc.). Besides a QA dataset, it also serves as a semantic parsing dataset. To the best of our knowledge, KQA Pro is currently the largest corpus for NLQ-to-SPARQL and NLQ-to-Program tasks.</p><p>We reproduce the state-of-the-art KBQA models and thoroughly evaluate them on KQA Pro. From the experimental results, we observe significant performance drops of these models compared with on existing KBQA benchmarks. It indicates that Complex KBQA is still challenging, and KQA Pro could support further explorations. We also treat KQA Pro as a diagnostic dataset for analyzing a model's capability of multiple reasoning skills, and discover weaknesses that are not widely known, e.g., current models struggle on comparisonal reasoning for lacking of literal knowledge (i.e., (Le-Bron James, height, 206 centimetre)), or perform poorly on questions whose answers are not obverseved in the training set. We hope all contents of KQA Pro could encourage the community to make further breakthroughs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Complex KBQA aims at answering complex questions over KBs, which requires multiple reasoning capabilities such as multi-hop inference, quantitative comparison, and set operation <ref type="bibr" target="#b23">(Lan et al., 2021)</ref>. Current methods for Complex KBQA can be grouped into two categories: 1) semantic parsing based methods <ref type="bibr" target="#b25">(Liang et al., 2017;</ref><ref type="bibr" target="#b16">Guo et al., 2018;</ref><ref type="bibr" target="#b33">Saha et al., 2019;</ref><ref type="bibr">Ansari et al., 2019)</ref>, which parses a question to a symbolic logic form (e.g., λcalculus <ref type="bibr" target="#b1">(Artzi et al., 2013)</ref>, λ-DCS <ref type="bibr" target="#b26">(Liang, 2013;</ref><ref type="bibr" target="#b29">Pasupat and Liang, 2015;</ref><ref type="bibr">Wang et al., 2015b;</ref><ref type="bibr" target="#b30">Pasupat and Liang, 2016)</ref>, SQL <ref type="bibr" target="#b47">(Zhong et al., 2017)</ref>, AMR <ref type="bibr" target="#b2">(Banarescu et al., 2013)</ref>, SPARQL <ref type="bibr" target="#b41">(Sun et al., 2020)</ref>, and etc.) and then executes it against the KB and obtains the final answers; 2) information retrieval based methods <ref type="bibr" target="#b28">(Miller et al., 2016;</ref><ref type="bibr" target="#b35">Saxena</ref>   <ref type="bibr">et al., 2020;</ref><ref type="bibr" target="#b36">Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b46">Zhang et al., 2018;</ref><ref type="bibr" target="#b16">Zhou et al., 2018;</ref><ref type="bibr" target="#b32">Qiu et al., 2020;</ref><ref type="bibr" target="#b38">Shi et al., 2021)</ref>, which constructs a question-specific graph extracted from the KB and ranks all the entities in the extracted graph based on their relevance to the question.</p><p>Compared with information retrieval based methods, semantic parsing based methods provides better interpretability by generating expressive logic forms, which represents the intermediate reasoning process. However, manually annotating logic forms is expensive and labor-intensive, and it is challenging to train a semantic parsing model with weak supervision signals (i.e., question-answer pairs). Lacking logic form annotations turns out to be one of the main bottlenecks of semantic parsing.</p><p>Table <ref type="table" target="#tab_1">1</ref> lists the widely-used datasets in Complex KBQA community and their features. MetaQA and CSQA have a large number of questions, but they ignore literal knowledge, lack logic form annotations, and their questions are written by templates. Query graphs (e.g., SPARQLs) are provided in some datasets to help solve complex questions. However, SPARQL is weak in describing the intermediate procedure of the solution, and the scale of existing question-to-SPARQL datasets is small.</p><p>In this paper, we introduce a novel logic form KoPL, which models the procedure of Complex KBQA as a multi-step program, and provides a more explicit reasoning process compared with query graphs. Furthermore, we propose KQA Pro, a large-scale semantic parsing dataset for Complex KBQA, which contains ~120k diverse natural language questions with both KoPL and SPARQL annotations. It is the largest NLQ-to-SPARQL dataset as far as we know. Compared with these existing datasets, KQA Pro serves as a more well-rounded benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KB Definition</head><p>Typically, a KB (e.g., Wikidata <ref type="bibr" target="#b44">(Vrandečić and Krötzsch, 2014)</ref>) consists of: Entity, the most basic item in KB. Concept, the abstraction of a set of entities, e.g., basketball player. Relation, the link between entities or concepts. Entities are linked to concepts via the relation instance of. Concepts are organized into a tree structure via relation subclass of. Attribute, the literal information of an entity. An attribute has a key and a value, which is one of four types<ref type="foot" target="#foot_0">1</ref> : string, number, date, and year. The number value has an extra unit, e.g., 206 centimetre. Relational knowledge, the triple with form (entity, relation, entity), e.g., (LeBron James Jr., father, LeBron James). Literal knowledge, the triple with form (entity, attribute key, attribute value), e.g., (LeBron James, height, 206 centimetre). Qualifier knowledge, the triple whose head is a relational or literal triple, e.g., <ref type="bibr">((LeBron James, drafted by, Cleveland Cavaliers), point in time, 2003)</ref>. A qualifier also has a key and a value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KoPL Design</head><p>We design KoPL, a compositional and interpretable programming language to represent the reasoning process of complex questions. It models the complex procedure of question answering with a program of intermediate steps. Each step involves a function with a fixed number of arguments. Every program can be denoted as a binary tree. As shown in Fig. <ref type="figure">1</ref>, a directed edge between two nodes represents the dependency relationship between two functions. That is, the destination function takes the output of the source function as its argument. The tree-structured program can also be serialized by post-order traversal, and formalized as a sequence with n functions. The general form is shown below. Each function f i takes in a list of textual arguments a i , which need to be inferred according to the question, and a list of functional arguments b i , which come from the output of previous functions. We analyze the generic, basic operations for Complex KBQA, and design 27 functions 2 in KoPL. They support KB item manipulation (e.g., Find, Relate, FilterConcept, QueryRelationQualifier, etc.), various reasoning skills (e.g., And, Or, etc.), and multiple question types (e.g., QueryName, SelectBetween, etc.). By composing the finite functions into a KoPL program 3 , we can model the reasoning process of infinite complex questions.</p><formula xml:id="formula_0">f 1 (a 1 , b 1 )f 2 (a 2 , b 2 )...f n (a n , b n ) (1)</formula><p>Note that qualifiers play an essential role in disambiguating or restricting the validity of a fact <ref type="bibr" target="#b14">(Galkin et al., 2020;</ref><ref type="bibr" target="#b27">Liu et al., 2021)</ref>. However, they have not been adequately modeled in current KBQA models or datasets. As far as we know, we are the first to explicitly model qualifiers in Complex KBQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">KQA Pro Construction</head><p>To build KQA Pro dataset, first, we extract a knowledge base with multiple kinds of knowledge (Section 4.1). Then, we generate canonical questions, corresponding KoPL programs and SPARQL queries with novel compositional strategies (Section 4.2). In this stage, we aim to cover all the possible queries through random sampling and recursive composing. Finally, we rewrite canonical questions into natural language via crowdsourcing (Section 4.3). To further increase linguistic variety, 2 The complete function instructions are in Appendix A. 3 The grammar rules of KoPL are in Appendix B.</p><p>we reject the paraphrases whose edit distance with the canonical question is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Knowledge Base Extraction</head><p>We took the entities of FB15k-237 <ref type="bibr" target="#b43">(Toutanova et al., 2015)</ref> as seeds, and aligned them with Wikidata via Freebase IDs<ref type="foot" target="#foot_1">4</ref> . The reasons are as follows: 1) The vast amount of knowledge in the full knowledge base (e.g., full Freebase <ref type="bibr" target="#b5">(Bollacker et al., 2008)</ref> or Wikidata contains millions of entities) may cause both time and space issues, while most of the entities may never be used in questions. 2) FB15k-237 is a high-quality, dense subset of Freebase, whose alignment to Wikidata produces a knowledge base with rich literal and qualifier knowledge. We added 3,000 other entities with the same name as one of FB15k-237 entities to increase the disambiguation difficulty. The statistics of our final knowledge base are listed in Table <ref type="table" target="#tab_4">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Question Generation Strategies</head><p>To generate diverse complex questions in a scalable manner, we propose to divide the generation into two stages: locating and asking. In locating stage we describe a single entity or an entity set with various restrictions, while in asking stage we query specific information about the target entity or entity set. We define several strategies for each stage. By sampling from them and composing the two stages, we can generate large-scale and diverse questions with a small number of templates. Fig. <ref type="figure">2</ref> gives an example of our generation process.</p><p>For locating stage, we propose 7 strategies and show part of them in the top section of Table <ref type="table" target="#tab_6">3</ref>. We can fill the placeholders of templates by sampling from KB to describe a target entity. We support quantitative comparisons of 4 operations: equal, not equal, less than, and greater than, indicated by "&lt;OP&gt;" of the template. We support optional qualifier restrictions, indicated by "(&lt;QK&gt; is &lt;QV&gt;)",  which can narrow the located entity set. In Recursive Multi-Hop, we replace the entity of a relational condition with a more detailed description, so that we can easily increase the hop of questions. For asking stage, we propose 9 strategies and show some of them in the bottom section of Table <ref type="table" target="#tab_6">3</ref>. Our SelectAmong is similar to argmax and argmin operations in λ-DCS. The complete generation strategies are shown in Appendix D due to space limit. Our generated instance consists of five elements: question, SPARQL query, KoPL program, 10 answer choices, and a golden answer. Choices are selected by executing an abridged SPARQL<ref type="foot" target="#foot_2">5</ref> , which randomly drops one clause from the complete SPARQL. With these choices, KQA Pro supports both multiple-choice setting and open-ended setting. We randomly generate lots of questions, and only preserve those with a unique answer. For example, since Akron has different populations in different years, we will drop questions like What is the population of Akron, unless the time constraint (e.g., in 2010) is specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Question Paraphrasing and Evaluation</head><p>After large-scale generation, we release the generated questions on Amazon Mechanical Turk (AMT) and ask the workers to paraphrase them without changing the original meaning. For the convenience of paraphrasing, we visualize the KoPL flowcharts like Fig. <ref type="figure">1</ref> to help workers understand complex questions. We allow workers to mark a question as confusing if they cannot understand it or find logical errors. These instances will be removed from our dataset.</p><p>After paraphrasing, we evaluate the quality by 5 other workers. They are asked to check whether the paraphrase keeps the original meaning and give a fluency rating from 1 to 5. We reject those paraphrases which fall into one of the following cases:</p><p>(1) marked as different from the original canonical question by more than 2 workers; (2) whose average fluency rating is lower than 3; (3) having a very small edit distance with the canonical question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Dataset Analysis</head><p>Our KQA Pro dataset consists of 117,970 instances with 24,724 unique answers. Fig. <ref type="figure" target="#fig_2">3(a)</ref> shows the question type distribution of KQA Pro. Within the 9 types, SelectAmong accounts for the least fraction (4.6%), while others account for more or less than 10%. <ref type="bibr">Fig. 3(b)</ref> shows that multi-hop questions cover 73.7% of KQA Pro, and 4.7% questions even require at least 5 hops. We compare the question length distribution of different Complex KBQA Figure 2: Process of our question generation. First, we sample a question type from asking strategies and sample a target entity from KB. Next, we sample a locating strategy and detailed conditions to describe the target entity. Finally, we combine intermediate snippets into the complete question and check whether the answer is unique. Note that the snippets of canonical question, SPARQL, and KoPL are operated simultaneously.. A more detailed explanation of this example is in Appendix F. datasets in Fig. <ref type="figure" target="#fig_2">3(c</ref>). We observe that our KQA Pro has longer questions than others on average. In KQA Pro, the average length of questions/programs/SPARQLs is 14.95/4.79/35.52 respectively. More analysis is included in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The primary goal of our experiments is to show the challenges of KQA Pro and promising Complex KBQA directions. First, we compare the performance of state-of-the-art KBQA models on current datasets and KQA Pro, to show whether KQA Pro is challenging. Then, we treat KQA Pro as a diagnostic dataset to investigate fine-grained reasoning abilities of models, discuss current weakness and promising directions. We further conduct an experiment to explore the generation ability of our  proposed model. Last, we provide a case study to show the interpretablity of KoPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>Benchmark Settings. We randomly split KQA Pro to train/valid/test set by 8/1/1, resulting in three sets with 94,376/11,797/11,797 instances. About 30% answers of the test set are not seen in training. Representative Models. KBQA models typically follow a retrieve-and-rank paradigm, by constructing a question-specific graph extracted from the KB and ranks all the entities in the graph based on their relevance to the question <ref type="bibr" target="#b28">(Miller et al., 2016;</ref><ref type="bibr" target="#b35">Saxena et al., 2020;</ref><ref type="bibr" target="#b36">Schlichtkrull et al., 2018;</ref><ref type="bibr" target="#b46">Zhang et al., 2018;</ref><ref type="bibr" target="#b16">Zhou et al., 2018;</ref><ref type="bibr" target="#b32">Qiu et al., 2020)</ref>; or follow a parse-then-execute paradigm, by parsing a question to a query graph <ref type="bibr" target="#b4">(Berant et al., 2013;</ref><ref type="bibr">Yih et al., 2015)</ref> or program <ref type="bibr" target="#b25">(Liang et al., 2017;</ref><ref type="bibr" target="#b16">Guo et al., 2018;</ref><ref type="bibr" target="#b33">Saha et al., 2019;</ref><ref type="bibr">Ansari et al., 2019)</ref> through learning from question-answer pairs.</p><p>Experimenting with all methods is logistically challenging, so we reproduce a representative subset of mothods: KVMemNet <ref type="bibr" target="#b28">(Miller et al., 2016)</ref>, a well-known model which organizes the knowledge into a memory of key-value pairs, and iteratively reads memory to update its query vector. EmbedKGQA <ref type="bibr" target="#b35">(Saxena et al., 2020)</ref>, a state-of-the art model on MetaQA, which incorporates knowledge embeddings to improve the reasoning performance. SRN <ref type="bibr" target="#b32">(Qiu et al., 2020)</ref>, a typical path search model to start from a topic entity and predict a sequential relation path to find the target entity. RGCN <ref type="bibr" target="#b36">(Schlichtkrull et al., 2018)</ref>, a variant of graph convolutional networks, tackling Complex KBQA through the natural graph structure of knowledge base. Our models. Since KQA Pro provides the annotations of SPARQL and KoPL, we directly learn our parsers using supervised learning by regarding the semantic parsing as a sequence-to-sequence task. We explore the widely-used sequence-to-sequence model-RNN with attention mechanism <ref type="bibr" target="#b11">(Dong and Lapata, 2016)</ref>, and the pretrained generative language model-BART <ref type="bibr" target="#b24">(Lewis et al., 2020)</ref>, as our SPARQL and KoPL parsers.</p><p>For KoPL learning, we design a serializer to translate the tree-structured KoPL to a sequence. For example, the KoPL program in Fig. <ref type="figure">2</ref> is serialized as: Find ⟨arg⟩ LeBron James ⟨f unc⟩ Relate ⟨arg⟩ drafted by ⟨arg⟩ backward ⟨f unc⟩ Filter-Concept ⟨arg⟩ team ⟨f unc⟩ QueryName. Here, ⟨arg⟩ and ⟨f unc⟩ are special tokens we designed to indicate the structure of KoPL.</p><p>To compare machine with Human, we sample 200 instances from the test set, and ask experts to answer them by searching our knowledge base. Implementation Details. For our BART model, we used the bart-base model of HuggingFace<ref type="foot" target="#foot_3">6</ref> . We used the optimizer Adam (Kingma and Ba, 2015) for all models. We searched the learning rate for BART paramters in {1e-4, 3e-5, 1e-5}, the learning rate for other parameters in {1e-3, 1e-4, 1e-5}, and the weight decay in {1e-4, 1e-5, 1e-6}. According to the performance on validation set, we finally used learning rate 3e-5 for BART parameters, 1e-3 for other parameters, and weight decay 1e-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Difficulty of KQA Pro</head><p>We compare the performance of KBQA models on KQA Pro with MetaQA and WebQSP (short for WebQuestionSP), two commonly used benchmarks in Complex KBQA. The experimental results are in Table <ref type="table">4</ref>, from which we observe that:</p><p>Although the models perform well on MetaQA and WebQSP, their performances are significantly lower and not satisfying on KQA Pro. It indicates that our KQA Pro is challenging and the Complex KBQA still needs more research efforts. Actually, 1) Both MetaQA and WebQSP mainly focus on relational knowledge, i.e., multi-hop questions. Therefore, previous models on these datasets are designed to handle only entities and relations. In comparison, KQA Pro includes three types of knowledge, i.e., relations, attributes, and qualifiers, thus is much more challenging. 2) Compared with MetaQA which contains template questions, KQA Pro contains diverse natural language questions and can evaluate models' language understanding abilities. 3) Compared with WebQSP which contains 4,737 fluent and natural questions, KQA Pro covers more question types (e.g., verification, counting) and reasoning operations (e.g., intersect, union).  <ref type="bibr" target="#b40">(Sun et al., 2018)</ref>. The BART results on MetaQA 3-hop WebQSP results are from <ref type="bibr" target="#b18">(Huang et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Analyses on Reasoning Skills</head><p>KQA Pro can serve as a diagnostic dataset for indepth analyses of reasoning abilities (e.g., counting, comparision, logical reasoning, etc.) for Complex KBQA, since KoPL programs underlying the questions provide tight control over the dataset.</p><p>We categorize the test questions to measure finegrained ability of models. Specifically, Multi-hop means multi-hop questions, Qualifier means questions containing qualifier knowledge, Comparison means quantitative or temporal comparison between two or more entities, Logical means logical union or intersection, Count means questions that ask the number of target entities, Verify means questions that take "yes" or "no" as the answer, Zero-shot means questions whose answer is not seen in the training set. The results are shown in Table <ref type="table" target="#tab_9">5</ref>, from which we have the following observations:</p><p>(1) Benefits of intermediate reasoning supervision. Our RNN and BART models outperform current models significantly on all reasoning skills. This is because KoPL program and SPARQL query provide intermediate supervision which benefits the learning process a lot. As <ref type="bibr" target="#b12">(Dua et al., 2020)</ref> suggests, future dataset collection efforts should set aside a fraction of budget for intermediate annotations, particularly as the reasoning required Existing models perform poorly in situations requiring comparison capability. This is because they only focus the relational knowledge, while ignoring the literal and qualifier knowledge. We hope our dataset will encourage the community to pay more attention to multiple kinds of knowledge in Complex KBQA.</p><p>(3) Generalization to novel questions and answers. For zero-shot questions, current models all have a close to zero performance. This indicates the models solve the questions by simply memorizing their training data, and perform poorly on generalizing to novel questions and answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Compositional Generalization</head><p>We further use KQA Pro to test the ability of KBQA models to generalize to questions that contain novel combinations of the elements observed during training. Following previous works, we conduct the "productivity" experiment (Lake and <ref type="bibr" target="#b22">Baroni, 2018;</ref><ref type="bibr" target="#b37">Shaw et al., 2021)</ref> Natural Language Question:</p><p>Who is the human that hold the position Prime Minister of the United Kingdom (the successor of this statement is David Lloyd George)</p><p>Figure <ref type="figure">4</ref>: Predicted SPARQL and KoPL by BART. We show the natural language question and canonical question before human rewriting. We mark the error corrections of the wrong predictions in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Case Study</head><p>To further understand the quality of logical forms predicted by the BART parser, we show a case in Fig. <ref type="figure">4</ref>, for which the SPARQL and KoPL parsers both give wrong predictions. The SPARQL parser fails to understand prior to David Lloyd George and gives a totally wrong prediction for this part.</p><p>The KoPL parser gives a function prediction which is semantically correct but very different from our generated golden one. It is a surprising result, revealing that the KoPL parser can understand the semantics and learn multiple solutions for each question, similar to the learning process of humans. We manually correct the errors of predicted SPARQL and KoPL and mark them in red. Compared to SPARQLs, KoPL programs are easier to be understood and more friendly to be modified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>In this work, we introduce a large-scale dataset with explicit compositional programs for Complex KBQA. For each question, we provide the corresponding KoPL program and SPARQL query so that KQA Pro can serve for both KBQA and semantic parsing tasks. We conduct a thorough evaluation of various models, discover weaknesses of current models and discuss future directions. Among these models, the KoPL parser shows great interpretability. As shown in Fig. <ref type="figure">4</ref>, when the model predicts the answer, it will also give a reasoning process and a confidence score (which is ommited in the figure for simplicity). When the parser makes mistakes, humans can easily locate the error through reading the human-like reasoning process or checking the outputs of intermediate functions. In addition, using human correction data, the parser can be incrementally trained to improve the performance continuously. We will leave this as our future work. A Function Library of KoPL </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Grammar Rules of KoPL</head><p>As shown in Table <ref type="table" target="#tab_14">7</ref>, the supported program space of KoPL can be defined by a synchrounous contextfree grammar (SCFG), which is widely used to generate logical forms paired with canonical questions <ref type="bibr" target="#b45">(Wang et al., 2015a;</ref><ref type="bibr" target="#b19">Jia and Liang, 2016;</ref><ref type="bibr">Wu et al., 2021)</ref>. The programs are meant to cover the desired set of compositional functions, and the canonical questions are meant to capture the meaning of the programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Knowledge Base Extraction</head><p>Specifically, we took the entities of FB15k-237 <ref type="bibr" target="#b43">(Toutanova et al., 2015)</ref>, a popular subset of Freebase, as seeds, and then aligned them with Wikidata via Freebase IDs<ref type="foot" target="#foot_5">7</ref> , so that we could extract their rich literal and qualifier knowledge from Wikidata. Besides, we added 3,000 other entities with the same name as one of FB15k-237 entities, to further increase the difficulty of disambiguation.</p><p>For the relational knowledge, we manually merged the relations of FB15k-237 (e.g., /people/person-/spouse_s./people/marriage/spouse) and Wikidata (e.g., spouse), obtaining 363 relations totally. Finally, we manually filtered out useless attributes (e.g., about images and Wikidata pages) and entities (i.e., never used in triples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Generation Strategies</head><p>Table <ref type="table">8</ref> list the complete generation strategies, including 7 locating and 9 asking strategies. In locating stage we describe a single entity or an entity set with various restrictions, while in asking stage we query specific information about the target entity or entity set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E SPARQL Implementation Details</head><p>We build a SPARQL engine with Virtuoso<ref type="foot" target="#foot_6">8</ref> to execute generated SPARQLs. To denote qualifiers, we create a virtual node for those literal and relational triples. For example, to denote the point in time of (LeBron James, drafted by, Cleveland Cavaliers), we create a node _BN which connects to the subject, the relation, and the object with three special edges, and then add (_BN, point in time, 2003) into the graph. Similarly, we use virtual node to represent the attribute value of number type, which has an extra unit. For example, to represent the height of LeBron James, we need (LeBron James, height, _BN), <ref type="bibr">(_BN,</ref><ref type="bibr">value,</ref><ref type="bibr">206)</ref>, <ref type="bibr">(_BN, unit, centimetre)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Generation Examples</head><p>Consider the example of Fig. <ref type="figure">2</ref> in Section 4.2 in the main text, following is a detailed explanation. At the first, the asking stage samples the strategy QueryName and samples Cleveland Cavaliers from the whole entity set as the target entity. The corresponding textual description, SPARQL, and KoPL of this stage is "Who is &lt;E&gt;", "SELECT ?e WHERE { }", and "QueryName", respectively. Then we switch to the locating stage to describe the target entity Cleveland Cavaliers. We sample the strategy, Concept + Relational, to locate it. For the concept part, we sample team from all concepts of Cleveland Cavaliers. The corresponding textual description, SPARQL, and KoPL is "team", "?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "team" .", and "FilterConcept(team)", respectively. For the relation part, we sample (LeBron James, drafted by) from all triples of Cleveland Cavaliers. The corresponding textual description, SPARQL, and KoPL is "drafted LeBron James", "?e_1 &lt;drafted  by&gt; ?e . ?e_1 &lt;pred:name&gt; 'LeBron James' .", "Find(LeBron James) → Relate(drafted by, backward)", respectively. The locating stage combines the concept and the relation, obtaining the entity description "the team that drafted LeBron James" and the corresponding SPARQL and KoPL. Finally, we combine the results of the two stages and output the complete question. Figure <ref type="figure">8 and 9</ref> show more examples of KQA Pro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Data Analysis</head><p>There are 24,724 unique answers in KQA Pro. We show the top 20 most frequent answers and their fractions in Fig. <ref type="figure" target="#fig_3">5</ref>. "yes" and "no" are the most frequent answers, because they cover all questions of type Verify. "0", "1", "2", "3", and other quantity answers are for questions of type Count, which accounts for 11.5%. Fig. <ref type="figure" target="#fig_4">6</ref> shows the Program length distribution. Most of our problems (28.42%) can be solved by 4 functional steps. Some extreme complicated ones (1.24%) need more than 10 steps. Fig. <ref type="figure" target="#fig_5">7</ref> shows sunburst for first 4 words in questions. We can see that questions usually start with "what", "which", "how many", "when", "is" and "does". Frequent topics include "person", "movie", "country", "university", and etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Baseline Implementation Details</head><p>KVMemNet. For literal and relational knowledge, we concatenated the subject and the attribute/relation as the memory key, e.g., "LeBron James drafted by", leaving the object as the memory value.  Table <ref type="table">6</ref>: Details of our 27 functions. Each function has 2 kinds of inputs: the functional inputs come from the output of previous functions, while the textual inputs come from the question.</p><p>For high-level knowledge, we concatenated the fact and the qualifier key as the memory key, e.g., "Le-Bron James drafted by Cleveland Cavaliers point in time". For each question, we pre-selected a small subset of the KB as its relavant memory. Following <ref type="bibr" target="#b28">(Miller et al., 2016)</ref>, we retrieved 1,000 key-value pairs where the key shares at least one word with the question with frequency &lt; 1000 (to ignore stop words). KVMemNet iteratively updates a query vector by reading the memory attentively. In our experiment we set the update steps to 3. SRN. SRN can only handle relational knowledge. It must start from a topic entity and terminate with a predicted entity. So we filtered out questions that contain literal knowledge or qualifier knowledge, retaining 5,004 and 649 questions as its training set and test set. Specifically, we retained the questions with Find as the first function and QueryName as the last function. The textual input of the first Find was regarded as the topic entity and was fed into the model during both training and testing phase.</p><p>EmbedKGQA. EmbedKGQA utilizes knowledge graph embedding to improve multi-hop reasoning.</p><p>To adapt to existing knowledge embedding techniques, we added virtual nodes to represent the qualifier knowledge of KQA Pro. Different from SRN, we applied EmbedKGQA on the entire KQA Pro dataset, because its classification layer is more flexible than SRN and can predict answers outside the entity set. The topic entity of each question was extracted from the golden program and then fed into the model during both training and testing.</p><p>RGCN. To build the graph, we took entities as nodes, connections between them as edges, and relations as edge labels. We concatenated the literal attributes of an entity into a sequence as the node description. For simplicity, we ignored the qualifier knowledge. Given a question, we first initialized node vectors by fusing the information of node descriptions and the question, then conducted RGCN to update the node features, and finally aggregated features of nodes and the question to predict the answer via a classification layer. Our RGCN imple-mentation is based on DGL,<ref type="foot" target="#foot_7">9</ref> a high performance Python package for deep learning on graphs. Due to the memory limit, we set the graph layer to 1 and set the hidden dimension of nodes and edges to 32. RNN-based KoPL and SPARQL Parsers. For KoPL prediction, we first parsed the question to the sequence of functions, and then predicted textual inputs for each function. We used Gated Recurrent Unit (GRU) <ref type="bibr" target="#b8">(Cho et al., 2014;</ref><ref type="bibr" target="#b9">Chung et al., 2014)</ref>, a well-known variant of RNNs, as our encoder of questions and decoder of functions. At-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Distribution of question hops. 73.7% of our questions require multiple-hops.(c) Question length distribution of Complex KBQA datasets. We can see that KQA Pro questions have a wide range of lengths and are longer on average than all others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Question statistics of KQA Pro.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Top 20 most occurring answers in KQA Pro. The most frequent one is "yes", which is the answer of about half of type Verify.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of program lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Distribution of first 4 question words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Entity) × (Pred, QKey) → (QValue) Return the qualifier value of the fact (Entity, Pred, Entity) QueryRelationQualifier(drafted by, point in time)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>&lt;point_in_time&gt; ?qpv . } Who is taller, LeBron James Jr. or his father?</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Knowledge Base</cell></row><row><cell cols="2">date of birth: 6 October mass: 80 kilogram; height: 188 centimetre; LeBron James Jr.</cell><cell></cell><cell>199,110 population: Akron</cell><cell>knowledge relational knowledge entity with literal</cell></row><row><cell>2004;</cell><cell></cell><cell></cell><cell>point in time:2010</cell><cell>between entities</cell></row><row><cell>father</cell><cell>child</cell><cell>p la c e o f b ir th</cell><cell></cell><cell>qualifier knowledge about facts</cell></row><row><cell cols="2">LeBron James height: 206 centimetre;</cell><cell></cell><cell>drafted by</cell><cell>Cleveland Cavaliers incept: 1970;</cell></row><row><cell cols="2">mass: 113 kilogram;</cell><cell></cell><cell></cell><cell>Social media followers:</cell></row><row><cell cols="2">Work period (start): 2003</cell><cell cols="2">point in time: 26 June 2003</cell><cell>3,242,471 point in time:6 January 2021</cell></row><row><cell></cell><cell>Find</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">LeBron James</cell><cell cols="2">QueryRelationQualifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">drafted by</cell></row><row><cell></cell><cell>Find</cell><cell></cell><cell cols="2">point in time</cell></row><row><cell></cell><cell cols="2">Cleveland Cavaliers</cell><cell></cell></row><row><cell cols="5">Question 2: SPARQL: SELECT ?e WHERE { { ?e &lt;name&gt; "LeBron James Jr." . } UNION { ?e_1</cell></row><row><cell></cell><cell cols="4">&lt;name&gt; "LeBron James Jr." . ?e_1 &lt;father&gt; ?e . } ?e &lt;height&gt; ?v . }</cell></row><row><cell></cell><cell cols="3">ORDER BY DESC(?v) LIMIT 1</cell></row><row><cell>KoPL:</cell><cell></cell><cell cols="2">Find LeBron James Jr.</cell><cell>SelectBetween height</cell></row><row><cell></cell><cell>Find</cell><cell></cell><cell>Relate</cell><cell>greater</cell></row><row><cell></cell><cell cols="2">LeBron James Jr.</cell><cell>father</cell></row></table><note>Figure1: Example of our KB and questions. Our KB is a dense subset of Wikidata<ref type="bibr" target="#b44">(Vrandečić and Krötzsch, 2014)</ref>, including multiple types of knowledge. Our questions are paired with executable KoPL programs and SPARQL queries.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with existing datasets of Complex KBQA. The column multiple kinds of knowledge means whether the dataset considers multiple types of knowledge or just relational knowledge (introduced in Sec.3.1). The column natural language means whether the questions are in natural language or written by templates.</figDesc><table><row><cell>Dataset</cell><cell>multiple kinds of knowledge</cell><cell>number of questions</cell><cell>natural language</cell><cell>query graphs</cell><cell>multi-step programs</cell></row><row><cell>WebQuestions (Berant et al., 2013)</cell><cell>✓</cell><cell>5,810</cell><cell>✓</cell><cell>×</cell><cell>×</cell></row><row><cell>WebQuestionSP (Yih et al., 2016)</cell><cell>✓</cell><cell>4,737</cell><cell>✓</cell><cell>✓</cell><cell>×</cell></row><row><cell>GraphQuestions (Su et al., 2016)</cell><cell>✓</cell><cell>5,166</cell><cell>✓</cell><cell>✓</cell><cell>×</cell></row><row><cell>LC-QuAD2.0 (Dubey et al., 2019)</cell><cell>✓</cell><cell>30,000</cell><cell>✓</cell><cell>✓</cell><cell>×</cell></row><row><cell>ComplexWebQuestions (Talmor and Berant, 2018)</cell><cell>✓</cell><cell>34,689</cell><cell>✓</cell><cell>✓</cell><cell>×</cell></row><row><cell>MetaQA (Zhang et al., 2018)</cell><cell>×</cell><cell>400,000</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>CSQA (Saha et al., 2018)</cell><cell>×</cell><cell>1.6M</cell><cell>×</cell><cell>×</cell><cell>×</cell></row><row><cell>CFQ (Keysers et al., 2020)</cell><cell>×</cell><cell>239,357</cell><cell>×</cell><cell>✓</cell><cell>✓</cell></row><row><cell>GrailQA (Gu et al., 2021)</cell><cell>✓</cell><cell>64.331</cell><cell>✓</cell><cell>✓</cell><cell>×</cell></row><row><cell>KQA Pro (ours)</cell><cell>✓</cell><cell>117,970</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>.</figDesc><table><row><cell># Con.</cell><cell># Ent.</cell><cell cols="3"># Name # Pred. # Attr.</cell></row><row><cell>794</cell><cell>16,960</cell><cell>14,471</cell><cell>363</cell><cell>846</cell></row><row><cell cols="5"># Relational facts # Literal facts # High-level facts</cell></row><row><cell>415,334</cell><cell></cell><cell>174,539</cell><cell></cell><cell>309,407</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics of our knowledge base. The top lists the numbers of concepts, entities, unique entity names, predicates, and attributes. The bottom lists the numbers of different types of knowledge.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Literal the &lt;C&gt; whose &lt;K&gt; is &lt;OP&gt; &lt;V&gt; (&lt;QK&gt; is &lt;QV&gt;) the basketball team whose social media followers is greater than 3,000,000 (point in time is 2021) Concept + Relational the &lt;C&gt; that &lt;P&gt; &lt;E&gt; (&lt;QK&gt; is &lt;QV&gt;) the basketball player that was drafted by Cleveland Cavaliers</figDesc><table><row><cell>Strategy</cell><cell>Template</cell><cell>Example</cell></row><row><cell></cell><cell>Locating Stage</cell><cell></cell></row><row><cell>Entity Name</cell><cell>-</cell><cell>LeBron James</cell></row><row><cell>Concept + Recursive Multi-Hop</cell><cell>unfold &lt;E&gt; in a Concept + Relational description</cell><cell>the basketball player that was drafted by the basketball team</cell></row><row><cell></cell><cell></cell><cell>whose social media followers is greater than 3,000,000 (point in</cell></row><row><cell></cell><cell></cell><cell>time is 2021)</cell></row><row><cell>Intersection</cell><cell>Condition 1 and Condition 2</cell><cell>the basketball players whose height is greater than 190</cell></row><row><cell></cell><cell></cell><cell>centimetres and less than 220 centimetres</cell></row><row><cell></cell><cell>Asking Stage</cell><cell></cell></row><row><cell>QueryName</cell><cell>What/Who is &lt;E&gt;</cell><cell>Who is the basketball player whose height is equal to 206</cell></row><row><cell></cell><cell></cell><cell>centimetres?</cell></row><row><cell>Count</cell><cell>How many &lt;E&gt;</cell><cell>How many basketball players that were drafted by Cleveland</cell></row><row><cell></cell><cell></cell><cell>Cavaliers?</cell></row><row><cell>SelectAmong</cell><cell>Among &lt;E&gt;, which one has the largest/smallest &lt;K&gt;</cell><cell>Among basketball players, which one has the largest mass?</cell></row><row><cell>Verify</cell><cell>For &lt;E&gt;, is his/her/its &lt;K&gt; &lt;OP&gt; &lt;V&gt; (&lt;QK&gt; is &lt;QV&gt;)</cell><cell>For the human that is the father of LeBron James Jr., is his/her</cell></row><row><cell></cell><cell></cell><cell>height greater than 180 centimetres?</cell></row><row><cell>QualifierRelational</cell><cell>&lt;E&gt; &lt;P&gt; &lt;E&gt;, what is the &lt;QK&gt;</cell><cell>LeBron James was drafted by Cleveland Cavaliers, what is the</cell></row><row><cell></cell><cell></cell><cell>point in time?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Representative templates and examples of our locating and asking stage. Placeholders in template have specific implication: &lt;E&gt;-description of an entity or entity set; &lt;C&gt;-concept; &lt;K&gt;-attribute key; &lt;OP&gt;operator, selected from {=, !=, &lt;, &gt;}; &lt;V&gt;-attribute value; &lt;QK&gt;-qualifier key; &lt;QV&gt;-qualifier value; &lt;P&gt;-relation description, e.g., was drafted by. The complete instruction is in Appendix D.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Which team picked LeBron James?</figDesc><table><row><cell cols="2">Asking</cell><cell></cell><cell></cell><cell></cell><cell>Locating</cell></row><row><cell cols="2">Strategies</cell><cell></cell><cell></cell><cell cols="2">Strategies</cell></row><row><cell>sample</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>sample</cell></row><row><cell cols="2">QueryName</cell><cell></cell><cell></cell><cell cols="2">Concept + Relational</cell></row><row><cell cols="2">Who/What is &lt;E&gt;</cell><cell></cell><cell>team</cell><cell></cell><cell>LeBron James</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">sports team working</cell><cell>drafted by</cell></row><row><cell cols="2">LeBron James JR.</cell><cell></cell><cell>basketball team</cell><cell>group</cell><cell>Cleveland Cavaliers country</cell></row><row><cell cols="2">United States of America</cell><cell cols="3">Cleveland Cavaliers</cell><cell>United States of America</cell></row><row><cell cols="2">LeBron James</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Cleveland Cavaliers</cell><cell></cell><cell>sample</cell><cell></cell><cell>sample</cell></row><row><cell></cell><cell></cell><cell></cell><cell>team</cell><cell></cell><cell>drafted by, LeBron James</cell></row><row><cell>sample</cell><cell></cell><cell cols="3">?e &lt;pred:instance_of&gt; ?c .</cell><cell>?e_1 &lt;drafted_by&gt; ?e .</cell></row><row><cell></cell><cell></cell><cell cols="3">?c &lt;pred:name&gt; "team" .</cell><cell>?e_1 &lt;pred:name&gt; "LeBron James".</cell></row><row><cell cols="2">Cleveland Cavaliers</cell><cell></cell><cell cols="2">FilterConcept team</cell><cell>Find LeBron James</cell><cell>Relate backward drafted by</cell></row><row><cell cols="2">Who/What is &lt;E&gt;</cell><cell></cell><cell cols="3">the team that drafted LeBron James</cell></row><row><cell cols="2">SELECT ?e WHERE { }</cell><cell></cell><cell cols="3">?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "team" . ?e_1 &lt;drafted by&gt; ?e . ?e_1 &lt;pred:name&gt; "LeBron James".</cell></row><row><cell cols="2">QueryName</cell><cell></cell><cell cols="2">Find LeBron James</cell><cell>Relate backward drafted by</cell><cell>FilterConcept team</cell></row><row><cell>CanonicalQ:</cell><cell cols="5">What is the team that drafted LeBron James</cell></row><row><cell cols="6">SPARQL: SELECT ?e WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "team" .</cell></row><row><cell></cell><cell cols="5">?e_1 &lt;drafted by&gt; ?e . ?e_1 &lt;pred:name&gt; "LeBron James". }</cell></row><row><cell>KoPL:</cell><cell cols="2">Find LeBron James</cell><cell>Relate backward drafted by</cell><cell cols="2">FilterConcept team</cell><cell>QueryName</cell></row><row><cell>Choices:</cell><cell cols="5">Miami Heat; Los Angeles Lakers; Cleveland Cavaliers; Charlotte</cell></row><row><cell></cell><cell cols="5">Hornets; Washington Wizards; Denver Nuggets; Los Angeles Clippers;</cell></row><row><cell></cell><cell cols="5">Houston Rockets; Chicago Bulls; New York Knicks</cell></row><row><cell>Answer:</cell><cell cols="3">Cleveland Cavaliers</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">Answer Uniqueness Checking</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Crowdsourced Paraphrasing</cell></row><row><cell>NaturalQ:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>…</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Accuracy of different models on KQA Pro test set. BART KoPL CG denotes the BART based KoPL parser on the compositional generalization experiment (see Section 5.4) becomes more complex. We hope our dataset KQA Pro with KoPL and SPARQL annotations will help guide further research in Complex KBQA. (2) More attention to literal and qualifier knowledge.</figDesc><table><row><cell>Model</cell><cell>Overall</cell><cell>Multi-</cell><cell cols="2">Qualifier Compari-</cell><cell>Logical</cell><cell>Count</cell><cell>Verify</cell><cell>Zero-</cell></row><row><cell></cell><cell></cell><cell>hop</cell><cell></cell><cell>son</cell><cell></cell><cell></cell><cell></cell><cell>shot</cell></row><row><cell>KVMemNet</cell><cell>16.61</cell><cell>16.50</cell><cell>18.47</cell><cell>1.17</cell><cell>14.99</cell><cell>27.31</cell><cell>54.70</cell><cell>0.06</cell></row><row><cell>SRN</cell><cell>-</cell><cell>12.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EmbedKGQA</cell><cell>28.36</cell><cell>26.41</cell><cell>25.20</cell><cell>11.93</cell><cell>23.95</cell><cell>32.88</cell><cell>61.05</cell><cell>0.06</cell></row><row><cell>RGCN</cell><cell>35.07</cell><cell>34.00</cell><cell>27.61</cell><cell>30.03</cell><cell>35.85</cell><cell>41.91</cell><cell>65.88</cell><cell>0.00</cell></row><row><cell>RNN SPARQL</cell><cell>41.98</cell><cell>36.01</cell><cell>19.04</cell><cell>66.98</cell><cell>37.74</cell><cell>50.26</cell><cell>58.84</cell><cell>26.08</cell></row><row><cell>RNN KoPL</cell><cell>43.85</cell><cell>37.71</cell><cell>22.19</cell><cell>65.90</cell><cell>47.45</cell><cell>50.04</cell><cell>42.13</cell><cell>34.96</cell></row><row><cell>BART SPARQL</cell><cell>89.68</cell><cell>88.49</cell><cell>83.09</cell><cell>96.12</cell><cell>88.67</cell><cell>85.78</cell><cell>92.33</cell><cell>87.88</cell></row><row><cell>BART KoPL</cell><cell>90.55</cell><cell>89.46</cell><cell>84.76</cell><cell>95.51</cell><cell>89.30</cell><cell>86.68</cell><cell>93.30</cell><cell>89.59</cell></row><row><cell>BART KoPL CG</cell><cell>77.86</cell><cell>77.86</cell><cell>61.46</cell><cell>93.61</cell><cell>77.88</cell><cell>79.17</cell><cell>89.01</cell><cell>76.04</cell></row><row><cell>Human</cell><cell>97.50</cell><cell>97.24</cell><cell>95.65</cell><cell>100.00</cell><cell>98.18</cell><cell>83.33</cell><cell>95.24</cell><cell>100.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>fact_h&gt; ?e ; &lt;pred:fact_r&gt; &lt;position_held&gt; ; &lt;pred:fact_t&gt; ?e_1 ] &lt;replaced_by&gt; ?qpv . ?qpv &lt;pred:value&gt; "David Lloyd George" . follow Canonical Question:</head><label></label><figDesc></figDesc><table><row><cell cols="2">Golden SPARQL: SELECT DISTINCT ?e WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "human" . ?e &lt;position_held&gt; ?e_1 . ?e_1 &lt;pred:name&gt; "Prime Minister of the United Kingdom" . [ &lt;pred:fact_h&gt; ?e ; &lt;pred:fact_r&gt; &lt;position_held&gt; ; &lt;pred:fact_t&gt; ?e_1 ] &lt;replaced_by&gt; ?qpv . ?qpv &lt;pred:value&gt; "David Lloyd George" .</cell><cell>Predicted SPARQL: SELECT DISTINCT ?e WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "human" . ?e &lt;position_held&gt; ?e_1 . ?e_1 &lt;pred:name&gt; "Prime Minister of the United Kingdom" . ?e_2 &lt;position_held&gt; ?e . ?e_2 &lt;pred:name&gt; "David Lloyd George" . } United Kingdom prior to David Lloyd [ &lt;pred:Who was the Prime Minister of the George?</cell></row><row><cell>}</cell><cell></cell><cell></cell></row><row><cell>Golden KoPL:</cell><cell></cell><cell>Predicted KoPL:</cell></row><row><cell cols="2">Find Prime Minister of the United Kingdom</cell><cell>Find David Lloyd George</cell></row><row><cell>Relate</cell><cell>QFilterStr</cell><cell>QueryRelationQualifier</cell></row><row><cell>position held</cell><cell>replaced by</cell><cell>position held</cell></row><row><cell>backward</cell><cell>David Lloyd George</cell><cell>followed by</cell></row><row><cell>FilterConcept human</cell><cell>QueryName</cell><cell>Find Prime Minister of the United Kingdom</cell></row><row><cell>, which focuses on generaliza-</cell><cell></cell><cell></cell></row><row><cell>tion to longer sequences or to greater composi-</cell><cell></cell><cell></cell></row><row><cell>tional depths than have been seen in training (for</cell><cell></cell><cell></cell></row><row><cell>example, from a length 4 program to a length 5</cell><cell></cell><cell></cell></row><row><cell>program). Specifically, we take the instances with</cell><cell></cell><cell></cell></row><row><cell>short programs as training examples, and those</cell><cell></cell><cell></cell></row><row><cell>with long programs as test and valid examples, re-</cell><cell></cell><cell></cell></row><row><cell>sulting in three sets including 106,182/5,899/5,899</cell><cell></cell><cell></cell></row><row><cell>examples. The performance of BART KoPL drops</cell><cell></cell><cell></cell></row><row><cell>from 90.55% to 77.86%, which indicates learning</cell><cell></cell><cell></cell></row><row><cell>to generalize compositionally for pretrained lan-</cell><cell></cell><cell></cell></row><row><cell>guage models requires more research efforts. Our</cell><cell></cell><cell></cell></row><row><cell>KQA Pro provides an environment for further ex-</cell><cell></cell><cell></cell></row><row><cell>perimentation on compositional generalization.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>YushiWang, Jonathan Berant, and Percy Liang. 2015b.Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1332-1342, Beijing, China. Association for Computational Linguistics.</figDesc><table><row><cell>Peng Wu, Shujian Huang, Rongxiang Weng, Zaixiang</cell></row><row><cell>Zheng, Jianbing Zhang, Xiaohui Yan, and Jiajun</cell></row><row><cell>Chen. 2019. Learning representation mapping for</cell></row><row><cell>relation detection in knowledge base question an-</cell></row><row><cell>swering. In Proceedings of the 57th Annual Meet-</cell></row><row><cell>ing of the Association for Computational Linguistics,</cell></row><row><cell>pages 6130-6139, Florence, Italy. Association for</cell></row><row><cell>Computational Linguistics.</cell></row><row><cell>Shan Wu, Bo Chen, Chunlei Xin, Xianpei Han, Le Sun,</cell></row><row><cell>Weipeng Zhang, Jiansong Chen, Fan Yang, and Xun-</cell></row><row><cell>liang Cai. 2021. From paraphrasing to semantic pars-</cell></row><row><cell>ing: Unsupervised semantic parsing via synchronous</cell></row><row><cell>semantic decoding. In Proceedings of the 59th An-</cell></row><row><cell>nual Meeting of the Association for Computational</cell></row><row><cell>Linguistics and the 11th International Joint Confer-</cell></row><row><cell>ence on Natural Language Processing (Volume 1:</cell></row><row><cell>Long Papers), pages 5110-5121, Online. Association</cell></row><row><cell>for Computational Linguistics.</cell></row><row><cell>Wen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-</cell></row><row><cell>feng Gao. 2015. Semantic parsing via staged query</cell></row><row><cell>graph generation: Question answering with knowl-</cell></row><row><cell>edge base. In Proceedings of the 53rd Annual Meet-</cell></row><row><cell>ing of the Association for Computational Linguistics</cell></row><row><cell>and the 7th International Joint Conference on Natu-</cell></row><row><cell>ral Language Processing (Volume 1: Long Papers),</cell></row><row><cell>pages 1321-1331, Beijing, China. Association for</cell></row><row><cell>Computational Linguistics.</cell></row><row><cell>Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-</cell></row><row><cell>Wei Chang, and Jina Suh. 2016. The value of se-</cell></row><row><cell>mantic parse labeling for knowledge base question</cell></row><row><cell>answering. In Proceedings of the 54th Annual Meet-</cell></row><row><cell>ing of the Association for Computational Linguistics</cell></row><row><cell>(Volume 2: Short Papers), pages 201-206, Berlin,</cell></row><row><cell>Germany. Association for Computational Linguis-</cell></row><row><cell>tics.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table6shows our 27 functions and their explanations. Note that we define specific functions for different attribute types (i.e., string, number, date, and year), because the comparison of these types are quite different. Following we explain some necessary items in our functions. Entities/Entity: Entities denotes an entity set, which can be the output or functional input of a function.When the set has a unique element, we get an Entity. Name: A string that denotes the name of an entity or a concept. Key/Value: The key and value of an attribute. Op: The comparative operation. It is one of {=, ̸ =, &lt;, &gt;} when comparing two values, one of {greater, less} in SelectBetween, and one of {largest, smallest} in SelectAmong. Pred/Dir: The relation and direction of a relation. Fact: A literal fact, e.g., (LeBron James, height, 206 centimetre), or a relational fact, e.g., (LeBron James, drafted by, Cleveland Cavaliers). QKey/QValue: The key and value of a qualifier.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>SCFG rules for producing KoPL program and canonical question pairs. "|" matches either expression in a group. "?" denotes the expression preceding it is optional. Key_Text, QKey_Text, and Pred_Text denote the annotated template for attribute keys, qualifier keys, and relations. For example, for Pred place of birth, the Pred_Text is "was born in".</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Wikidata also has other types like geographical and time. We omit them for simplicity and leave them for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">The detailed extracting process is in Appendix C.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">The SPARQL implementation details are shown in Appendix E.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">https://github.com/huggingface/transformers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4">Mantong Zhou,Minlie Huang, and Xiaoyan Zhu. 2018.   An interpretable reasoning network for multi-relation question answering. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2010-2022, Santa Fe, New Mexico, USA. Association for Computational Linguistics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5">Wikidata provides the Freebase ID for most of its entities, but the relations are not aligned.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6">https://github.com/openlink/virtuoso-opensource</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">https://github.com/dmlc/dgl</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is founded by the National Key Research and Development Program of China (2020AAA0106501), the Institute for Guo Qiang, Tsinghua University (2019GQB0003), Huawei Noah's Ark Lab and Beijing Academy of Artificial Intelligence.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>tention mechanism <ref type="bibr" target="#b11">(Dong and Lapata, 2016)</ref> was applied by focusing on the most relavant question words when predicting each function and each textual input. The SPARQL parser used the same encoder-decoder structure to produce SPARQL token sequences. We tokenized the SPARQL query by delimiting spaces and some special punctuation symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I BART KoPL Accuracy of different #hops</head><p>In Table <ref type="table">9</ref>, we presents the BART KoPL accuracy of different #hops. Note that KQA Pro not only consider multi-hop relations, but also consider attributes and qualifiers. We count all of them into the hop number. So in KQA Pro, given a question with "4-hops", it does not mean 4 relations, but may be 1 relations + 2 attributes + 1 comparison. E.g., "Who is taller, LeBron James Jr. or his father?". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>Which area has higher elevation (above sea level), Baghdad or Jerusalem (the one whose population is 75200)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPARQL:</head><p>SELECT ?e WHERE { { ?e &lt;pred:name&gt; "Baghdad" . } UNION { ?e &lt;pred:name&gt; "Jerusalem" . ?e &lt;population&gt; ?pv_1 . ?pv_1 &lt;pred:unit&gt; "1" . ?pv_1 &lt;pred:value&gt; "75200"^^xsd:double . } ?e &lt;elevation_above_sea_level&gt; ?pv . ?pv &lt;pred:value&gt; ?v . } ORDER BY DESC(?v) LIMIT 1</p><p>KoPL: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>When did the big city whose postal code is 54000 have a population of 104072? SPARQL:</p><p>SELECT DISTINCT ?qpv WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "big city" . ?e &lt;postal_code&gt; ?pv_1 . ?pv_1 &lt;pred:value&gt; "54000" . ?e &lt;population&gt; ?pv . ?pv &lt;pred:unit&gt; "1" . <ref type="bibr">?pv &lt;pred:value&gt; "104072"^^xsd:double . [ &lt;pred:fact_h&gt; ?e ; &lt;pred:fact_r&gt;</ref>  ASK { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "city" . ?e &lt;capital_of&gt; ?e_1 . ?e_1 &lt;pred:name&gt; <ref type="bibr">"Guyana" . ?e &lt;elevation_above_sea_level&gt; ?pv . ?pv &lt;pred:unit&gt; "metre" . ?pv &lt;pred:value&gt; ?v . FILTER ( ?v &lt; "130"^^xsd:double )</ref>  In KQA Pro, each instance consists of 5 components: the textual question, the corresponding SPARQL, the corresponding KoPL, 10 candidate choices, and the golden answer. Choices are separated by semicolons in this figure. For questions of Verify type, the choices are composed of "yes", "no", and 8 special token "unknown" for padding.</p><p>Question:</p><p>Among the feature films with a publication date after 2003, which one has the smallest duration?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPARQL:</head><p>SELECT ?e WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "feature film <ref type="bibr">" . ?e &lt;publication_date&gt; ?pv_1 . ?pv_1 &lt;pred:year&gt; ?v_1 . FILTER ( ?v_1 &gt; 2003 )</ref> .  <ref type="bibr">1955-12-01; 1966-04-18; 2005-12-31; 1375; 1995-12-19; 1980-10-01; 1944-01-01; 1885-01-01; 1976-12-</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>What number of animated movies were published after 1940?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SPARQL:</head><p>SELECT (COUNT(DISTINCT ?e) AS ?count) <ref type="bibr">WHERE { ?e &lt;pred:instance_of&gt; ?c . ?c &lt;pred:name&gt; "animated film" . ?e &lt;publication_date&gt; ?pv . ?pv &lt;pred:year&gt; ? v . FILTER ( ?v &gt; 1940 )</ref> . }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KoPL:</head><p>FindAll Choices: <ref type="bibr">35; 36; 37; 38; 39; 40; 41; 42; 43</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural program induction for KBQA without gold programs or query annotations</title>
		<author>
			<persName><forename type="first">Ghulam</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishwajeet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Bhambhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/679</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
				<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10">2019. August 10-16, 2019</date>
			<biblScope unit="page" from="4890" to="4896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic parsing with Combinatory Categorial Grammars</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials)</title>
				<meeting>the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials)<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract Meaning Representation for sembanking</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
				<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linguistic generalization and compositionality in modern artificial neural networks</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions of the Royal Society B</title>
		<imprint>
			<biblScope unit="volume">375</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno>abs/1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised dual paraphrasing for two-stage semantic parsing</title>
		<author>
			<persName><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.608</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6806" to="6817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-4012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</title>
				<meeting>SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<title level="m">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Language to logical form with neural attention</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="33" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Benefits of intermediate annotations in reading comprehension</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.497</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5627" to="5634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Lc-quad 2.0: A large dataset for complex question answering over wikidata and dbpedia</title>
		<author>
			<persName><forename type="first">Mohnish</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debayan</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Abdelkawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="69" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Message passing for hyper-relational knowledge graphs</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.596</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7346" to="7359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Beyond i.i.d.: Three levels of generalization for question answering on knowledge bases</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><forename type="middle">E</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dialog-to-action: Conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="2946" to="2955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding based question answering</title>
		<author>
			<persName><forename type="first">John</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uk</forename><forename type="middle">Xiao</forename><surname>Hachette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3290956</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019</title>
				<meeting>the Twelfth ACM International Conference on Web Search and Data Mining, WSDM 2019<address><addrLine>Melbourne, VIC, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017. 2019. February 11-15, 2019</date>
			<biblScope unit="page" from="105" to="113" />
		</imprint>
	</monogr>
	<note>How children learn</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unseen entity handling in complex question answering over knowledge base via language generation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung-Jae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowei</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.50</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
				<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="547" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Measuring compositional generalization: A comprehensive method on realistic data</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Schärli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Scales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hylke</forename><surname>Buisman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Furrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergii</forename><surname>Kashubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Momchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danila</forename><surname>Sinopalnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Stafiniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tibor</forename><surname>Tihon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Tsarkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Van Zee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><surname>Baroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10">2018. July 10-15, 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2879" to="2888" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on complex knowledge base question answering: Methods, challenges and solutions</title>
		<author>
			<persName><forename type="first">Yunshi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaole</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kenneth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="23" to="33" />
		</imprint>
	</monogr>
	<note>Forbus, and Ni Lao</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4408</idno>
		<title level="m">Lambda dependency-based compositional semantics</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Role-aware modeling for n-ary relational knowledge bases</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>Proceedings of the Web Conference</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Key-value memory networks for directly reading documents</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1147</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inferring logical forms from denotations</title>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="23" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sim-pleQuestions nearly solved: A new upperbound and baseline approach</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrochuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1051</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="554" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stepwise reasoning for multi-relation question answering over knowledge graph with weak supervision</title>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371812</idno>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
				<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-02-03">2020. February 3-7, 2020</date>
			<biblScope unit="page" from="474" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Complex program induction for querying knowledge bases in the absence of gold programs</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghulam</forename><surname>Ahmed Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Laddha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00262</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="185" to="200" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vardaan</forename><surname>Pahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarath</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="705" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving multi-hop question answering over knowledge graphs using knowledge base embeddings</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditay</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.412</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4498" to="4507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Compositional generalization and natural language variation: Can a semantic parsing approach handle both?</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.75</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="922" to="938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TransferNet: An effective and transparent framework for multi-hop question answering over relation graph</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shulin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.341</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On generating characteristic-rich question sets for QA evaluation</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mudhakar</forename><surname>Srivatsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izzeddin</forename><surname>Gür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenghui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1054</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="562" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1455</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">SPARQA: skeleton-based semantic parsing for complex questions over knowledge bases</title>
		<author>
			<persName><forename type="first">Yawei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020-02-07">2020. February 7-12, 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8952" to="8959" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1059</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>New Orleans</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representing text for joint embedding of text and knowledge bases</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pallavi</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1174</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon</addrLine></address></meeting>
		<imprint>
			<publisher>Portugal. Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1499" to="1509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<title level="m">Wikidata: a free collaborative knowledge base</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Building a semantic parser overnight</title>
		<author>
			<persName><forename type="first">Yushi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1332" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="6069" to="6076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
