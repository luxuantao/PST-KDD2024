<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A PARALLEL LANGUAGE AND ITS COMPILATION TO MU~TIPROCESSOR MACHINES OR VLSI</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marina</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<address>
									<addrLine>Yale Unlverslty New Haven</addrLine>
									<postCode>06520</postCode>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A PARALLEL LANGUAGE AND ITS COMPILATION TO MU~TIPROCESSOR MACHINES OR VLSI</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">822A7BABF55264565037CD13086B7E10</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A language Crystal and its compiler for parallel programming is presented. The goal of Crystal is to help programmers in seeking etificient parallel implementations of an Mgorithm, and managing the complexity that might arise in dealing with hundreds of thousands of autonomous parMlel processes. In Crystal, a program consists of a system of recursion equations and is interpreted as a parallel system. Crystal views a large complex system as consisting of a hierarchy of parallel sub-systems, built upon a set of Crystal programs by composition and abstraction. There is no mention of explicit communications in a Crystal program. The Crystal compiler automatically incorporates pipelining into programs, and generates a parallel program that is optimal with respect to an algorithm. Each optimizing compiler, targeted for a particular machine, determines the appropriate granular size of parallelism and attains a balance between computations and communications. Based on the language, a unified theory for understanding and generating any systolic design has been devised and it eonstitues a part of the compiler.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The effective exploitation of the parMlelism provided by multiprocessor machines or speciM purpose YLSI designs clearly relies on a suitable parallel programming language and a powerful compiler ~o help in <ref type="bibr" target="#b0">(1)</ref> seeking efficient parallel implementations of an algorithm, and (2) managing the complexity that might arise in dealing with hundreds of thousands of autonomous parallel processes. Toward these zwo goals, several issues concerning language arise:</p><p>1. What kind of languages support expression of concurrency implicitly and free programmers from specifying explicit communications? 2. Balanced commumeations and computations are central ~o an efficient parallel implementation of a program. Does a language allow the relationship between the two ~o be clearly reflected m a program so that an optimizing compiler can perform zrade-offs and come up with an efficient implementation?</p><p>Permission to copy without fee all or part of this material is granted provided that the copies are nol made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permlssmn.</p><p>Â© 1986 ACM-0-89791-175-X-1/86-0131 $00.75</p><p>3. Is the language general purpose? Does it allow a network topology to be easily expressed? 4. Does a language provide constructs that encourage the exploitation of large-scale parallelism, and simultaneously, promote the clarity of programs and ease of ensuring correctness? Crystal*, along with its compiler, is an attempt to provide answers to these questions. In Crystal, a program consists of a system of remtrsion equations (e.g. in <ref type="bibr">Manna[i5]</ref>) similar to those appearing in applicative languages, but it is interpreted completely differently from the applicative interpretation (or its parallel version). Crystal's parallelism is shown to be more efficient than the applicative parallelism. There is no mention of explicit communications in a Crystal program. The Crystal compiler automatically incorporates pipelining into programs, and generates a parallel program that is optimal with respect to an algorithm. Each optimizing compiler, targeted for a particular machine, determines the appropriate granular size of parallelism and attains a balance between computations and communications.</p><p>Compared with other high-level programming environments that allow parallel implementations such as Shpiro's systolic programming in Concurrent Prolog <ref type="bibr" target="#b18">[20]</ref> and Snyder% Poker programming environment, Crystal goes one step further by automating the process of decomposing computations co par-Mlel processes. Efficient parallel programs are generated from algorithms by taking into account the dominant cost in parallel procesmng: the cost of communications. Moreover. a generated parallel program is expressed in ~erms also of recursion equations, the same language as that for an algorithm Hence in Crystal, the equivalence of an algorithm and its parallel implementation can be shown formally and guaranteed mechanically.</p><p>tn Section 2, the language Crystal and its interpretation are introduced. The second stage of Crystal's compilation process, space-time mapping, is presented in Section 3. Space-time mapping generates programsthat are systolic, in other words, it syatheizes systolic designs from an algorithm specified in a high level language. The third stage of compilation concerns with finite sizes of machines. Techniques for an optimizing compiler and the first stages of Crystal% compilation process are discussed in Section 4. In the last section, a few efficient parallel implementations of Crystal programs are mentioned. Finally, various language issues are revisited to review the capabtility of Crystal.</p><p>~Ig stands for Concurrent Representation of Your Spaee~Time Algorithms [21, a language for parallel computing. It bears no relationship with the distributed operating system Crystat Nugget 9], nor with the VLS1 tlming verifier Crystal 17] nor the Crystalline software on the Cosmic Cube <ref type="bibr" target="#b17">[19]</ref>, nor witch the character portrayed by Lind~ Evans on the popular television soap opera Dynasty.</p><p>.~o The Parallel Langaage Crystal ~.1o A high level parallel program Each parallel program in Crystal consists of a system of recursion equation(s), for example, tile number of partitions of an integer /c into integers less allan or equal to m can be obtained by applying the following recursively defined function C to the pair of integers (m,/c): {f~ = ~ --~ l( i &gt; j~ c(i -:t,j) c(&lt;j): i&gt;l-~l~: ]-~c(i-~'j)+ i &lt; j -~ c(i-1,j) + c(i,j-., i).</p><p>(2.1)</p><p>~o2o Applicative parallelism Equation (2.1) is a recursive definition that is common i~ any applicative programming language. Its applicative into&gt; proration can be illustrated by a tree as shown in Figure <ref type="figure">t</ref>. Certainly, in such an applicative view, the calls, or invocations of the recursive definition C(i,j) need not be made sequ.entially; paraitelism can be extracted from, for instance, tile parallel evaluation of the left branch C(i -1,j) and ~he right, branch C(i,j --i) of the last expression in the equation. ~o3o CrystaPs parallelism In Crystal, however, a completely different interpretation is given to the equation, and parallism of a different kind is introduecd. Instead of being considered as actual p~rametees at a call, the pair (i,j) is considered as an index pair for a process in an ensemble of parallel processes. For each pair (i~j) in the set D de:~ {(i,j) : 0 &lt; i &lt; m,O &lt; j &lt; k}, there corresponds a process. The process structure of this program can be seen as a DAG (Directed Acyclic Graph), which is shown in Figure <ref type="figure">2</ref>.</p><p>The DAG depicts the data dependency of each C(i,j) %r all (i,j) ~ D. It consists of aodes~ where each node corresponds to an index pair (i,j) in the set D, and directed edges, each of which comes out of a node whose index pair appears on the right hand side of Equation (2.1), and goes into a node whose index pair appears on the left hand side of the equation. The directed edges of the DAG define dle data dependency relation of the algorithm. We say that a node u precedes v {u ~ v), or v depends on u i v &gt; u), K there is a directed edge from u ~o v. The transitive closure of this relation on all such pairs is a partial order, and there is no infinite decreasing chain from any pair (i,j)o Those nodes that, have no incoming edges are called 8ogrces.</p><p>In Crystal, processes are parallel in nature. A corapueatioa starts at the sources which are properly initialized, and is %llowed by other processes eack of which star~ execution when all of its required inputs, or dependent data, become available.</p><p>If the applicative h~terpretation is demand-driven, where a call Based on its data-driven view of parallelism, Crystal's compiler fu.rther increases the rossaces utilization by reducing the nurnbet of processors needed from, say O(n '~) to O(n ~-1 ) without increasing the time complexity, where n is the problem size and ]Â¢ some constant. When a computing system becomes space limited due to problem sizes~ the dramatic Savings in the space requirements of Crystal's parallelism over that of the applicative parallelism translates to savings in time, because invocations must be se-quen~ialized under such limited circumsgances~ The exponential space requirement in the applicative parallel evaluation stems from the evaluations of the same sub-tree by multiple branches, independent of one another. This wasteful situation may be avoided by checkin.g for duplicated evaluations at run time, but not with.out a significant cost. tt might also be avoided if a programmer were to rewrite the recursive program using iterative constructs instead, but at the expense of the conceptual elegance provided by recursion equations.</p><p>The advantage of Crystal's parallelism goes beyond a particular example. As stated in Section 2, Crystal uses a style of recursion which is course-of-values recursion with unbounded minimaIization, hence any program can be written in the above style. The more elements in the process structur% the more the parallelism. In contrast, the only parallelism that can be gained from the app]icative scheme is by parallel evaluations of two or more independent branches due to the command-driven interpretation, which always requires an exponential number of processes with respect to the problem size. The growth is limited onty in the case of a single branch, but then there is no paraiteli~m that can be extracted. To utilize applicative par-atlelb~m, one would like to en.courage more branches in a program to allow nmre parallel evaluations, yet this very act causes quk:ker exponential growth in the number of parallel processes, whict~ in tunl forces them to be sequentialised due to tile inevitable space limitation in physical realilty. Such dilemma is a fundamental flaw in the applicative parallei interpretation scheme, In contrast, Crystal's interpretation scheme results in emcent parallelism as well as retains the elegance of functional programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Comparison with CSP-1ike Languages</head><p>Niany languages proposed for parallel programming, such as Conmmnication Sequential Processes (CSP) <ref type="bibr">[11]</ref> or Oceam <ref type="bibr" target="#b10">[12]</ref>, are based on imperative seqential programming languages augmented with communication commands for expressing concurrency explicitly. There is no doubt that such a language is general purpose and flexible enough for programmers ~o specify any tesirable parallelism: the question is whether it supporvs parallel programming a~ a suitable level. Taking any program wrwGen in CSF. for instance, one can see very clearly the cmnputation of each individual process and. locally, how it explicit communicaces with )thers. However. such a program presenvs no gIimpse of what tile global structure of parallelism might be.</p><p>In fact, due vo the inherent asynchronous style in which comnrunication commands are used. i~ is often necessary vo simulate the program ~o see wha* a process s~rucvure is. The lack of global structure makes programs written in such language error prone and di~icult to debug, a situation analogous ~o assembly language programs in sequential programming, except, that the issue of correcvness becomes a wen more severe problem due tc the complexity of large scale parallelism. Any sequential language augmented with a sev of communication commands serves well as a parallel Ianguage, but only a~ the object language level [or a cmnpiler ~o generate code in. rather than be used by programmers directly. For instance, a Crystal compiler targeted for Intei's iPSC hypercube multiprocessor uses the programming language C augmented with iPSO's communication commands as an object, language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6.">Comparison with data flow languages</head><p>The computation model of Crystal differs from those of tl~e data flow languages <ref type="bibr" target="#b6">[8]</ref> in that the flow graph (the process structure is a directed acyelic graph (DAG), where each node in a DAG represents, an abstract process (invocation of a function). In contrast, the flow graph in data flow languages is often cyclic wh.ere iterative loops are represented by cycles~ and each node in a flow grra~ph represents. . some operation or functional unit (a processor'l ratller than an invocation. Tokens that flow through the graph are then necessary to model the invocations, By comparison, CrystaPs model of computation is at an abstract level rather than being bound to some implementation as in the model of data flow languages, Furthermore, Crystal% synthesis method finds one or more mappings of processes (nodes in the DAG) to processors (organized in some network topology) to yield efficient parallel programs of an algorithm.</p><p>Besides of its independence from implementations, Cryso taFs style of programming with data streams defined over a process structure encourages a much larger scale of parallelism than that occurs in data flow systems [I0].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.7.">Reeursion equations</head><p>In Equation (2,1), we call i and j recursion variables and C a functional variable of the equation. In general, a parallel program is defined by a system of recursion equations Fi, processing functions 01, and communication functions rii~ where 1 &lt; i,j &lt;_ n. The discrete structure A is in general some n-dimensionM space. It ranges from the degenerate case (0 dimensional), where a piece of straight code (without any iteration or recursion construct] is executed on a uniprocessor. to a two-dimensionM case with a systolic program running on a linear array, or to a multLdimensionM case with programs running on butterfly, tree. or hypercube machines. A set of communication functions is often associated with each particular process structure A. Each processing function Â¢i is a composition of base functions (e.g. "+" in (2.1)) from some value domain (Di, g) ~o another value domain (D~, G).</p><p>The outermost base function in the composition is often a case function, as is the one with four cases {flattened from the nested conditionals) in Equation (2.1). Each equation in (2.2) can be written structurally in more detail as k cases:</p><p>where Pi~,..., Pi~ are boolean predicates.</p><p>The number of cases ~ depends on the homogeneity of the processing functions and the uniformity of the communication functions over structure A. Since the predicates are functions of Ni's, both tile processing and communication (or data flow) of each elemeat in A might be data dependent and change dynamicaIly during a computation. The classification of parallel machines can be borrowed here: For the single instruction multiple data (SIMD) type of machine architectures, the processing and communication functions of all elements of the process structure are tile same, hence tile recursion equations are of a simple form. For the MIMD type of architectures, the recursion equations are usually composed of many cases.</p><p>Note that the system (2~2) is in t:he %rm of simultaneous, course-of-vahes reeursion, whi&amp; is primitive recursive. Thus unbounded mlnimalization is a construct for specifying the computation of general recursive functions. This construct is particularly useful in describing the process of relaxation in various simulation tasks. ~o8Â° Correctness of a Crystal program In Crystal, correctness is easy to ensure, as the behavior of a program is the least fixed point of the system of recursion equations which can be verified by structural induction [1} on the DAG. 2o9~ Structured programmlng in Crystal Similarly to any sequential programming language, Crystat provides modular constructs for structured programming. Processing functions and communication functions can be de-fined separately, also as recursion equations, and then used in a program. In Crystal's view, a large complex system consists of a hierarchy of parallel sub-systems. At a given level, each sub-system is modularized depending on what is natural to a particular problem. The system of recursion equations defined for a sub-system at one level is used as a function (its least fixed-point) at the level above it. In such a philosophy of computlng, computations are defined independently of how they are going to be implemented --sequentially or in parallel. A Crystal compiler, specialized for each target machine, is responsible for determining the Ievel beyond which paralleI implementation is used and the level below whi&amp; a sequential implementation of Crystal's interpretation of recursion equations is used. Such closure in composition of CKystal programs and capability in Mleviating programmers from implementation details are essential to complex systems with massive parallelism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2o10o Space-time recurslen equations</head><p>From an implem.entation point of view, a process Â°e in a recursion equation (2.2) witl be mapped to some physlcal processor s during execntion, and once it is terminated, another process can be mapped to the same processor. We call each execution of a process by a processor an invocation of the procosset. Let t be an index :for labeling the invocations so that the processes executed in the same processor can be differen.tinted, and these invocations be labeled by strictly increasing non-negative integers. Then ibr a given implementation of a program., each process "v has an alias [,% t], telling when (which invocation) and where (in which proces~sor) it is executed.</p><p>How shall the name of a process be related to its alb.&lt;s? The key to an efficient implemer~tation of a parallel program is to find a suitable one-to-one function that maps a name to its alias. Once such a fmmtion is obtained, it can be substituted into the program to yield another system of recursion equations which has the property that (1) the time component s must always be non-negative, and (2) the time component of any invocation on the right hand side of an equation must be strictly less than the time component of the invocation on the leg hand side of ghe equation. Such equations are called space-time recursion equations (STREQ) [3] and the f'unction a space-time mapping.</p><p>A system of space4ime recursion equations specifies completely what code each processor executes at each invocation (its processing function), how the processor communicates with ether processors (its communication functions), how communicatimls and computations are controlled locally at each processor (its predicates), how each processor shmdd be initialized (its specification at time zero), and how input streams shonld be supplied (its specification at the boundary processors of a mnltlproeessor machine).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~. Compilation Method</head><p>The goal of the Crystal compiler is to generate, from a given algorithm or definition specified by a system of recursion equations, a system of space-time recursion equations that is optimal for executing on the target machine. In what follows~ we call such a parallel implementation described by a system of space-time recursion equations a deaign. The procedure of Crystal's compilation method is shown in Figure <ref type="figure">6</ref>. In the for ]owing, the second stage of the compilation method, a sys~hesis method which automatically incorporates pipelining, is illustrated by deriving a design to sob,'e the integer partition problem. An optimal design which uses an array of m processors and computes C(m, k) in total k + rn time steps with latency rn is generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>gÂ°l. Process structure and communication functions</head><p>As mentioned earlier, the process structure D is of two dimensions. The communication functions consist of q (i, j) = (i -1,j) and r2~(i,j) = (i,j -i). Note that for each different i, there is associated a different communication function r~i. This variation suggests that the partition problem may need to be solved by a design that is not quite regular.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3,2~ Difference vectors</head><p>The discrete process structure can be embedded in a vector space over the rationals; thus each ~process id" can be treated as a vector. A difference vector is defined to be the difference of (i,j), the left hand side index pair of Equation (2.1) and q(i,j) or r~i(i,j), which appear on the right hand side of tile equation. The difference vectors obtained from this equation. are (t,0), and (0, i) for 0 &lt; i &lt; rn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~3Â° Uniformity of" a parallel program</head><p>The concept of uniformity is introduced to characterize parallel algorithms so that an expedient procedure can be applied to a uniform algorithm to find the space-time mapping. A uniform program is defined to be:</p><p>De.finltion 3A. A uniform program is one in which a single set of basis vectors can be chosen so as to satisfy the following mapping condition: each difference vector appearing in the program can be expressed as a linear combination of the chosen basis vectors with non-negative coordinates.</p><p>The mapping condition is motivated by the possibility of using as the space-time mapping a linear transform from the basis difference vectors to the basis communication vectors. A communication w~ctor [k~s, At] is just a difference vector of a system of STREQ, which always has a positive time component to another processor s+-&amp;s in &amp;t thne steps. If the mapping between the two bets of bums vectors is determined, then the mapping of each difference vector is also determined: its mapping is the ~ame liw,ar combination as itself, except in terms of the corr,,st&gt;ending ba:ds communication vectors. If a difference vector has a term with a negative coordi~ates in its linear combina-t%n. then th,-msl)l)h~ of this term to space-time tel)resents a communication that takes negative time steps, a situation that is not feasible in a~ty physical implementation. In the case of a non-uniform program, more titan one set of basis difference vectors must be chosen so as to satisfy the mapping condition, and the space-time mapping may turn out to be non-linear. An inductive mapping procedure described must be used to find the space-time mapping for non-uniform algorithms.</p><p>The program (Equation <ref type="formula">2</ref>.1) for the partition problem is a uniform one. as all difference vectors appearing in Equation (2.1) can be expressed as linear combinations of basis vectors (1.0) and (0, 1) with nonq~egative coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Basis communication vectors</head><p>Eacb network often comes with a set of basis communication vectors. /'or instance, in an n dimensional hypercube, a processor ha~s n connections to its nearest neighboring processors. Each of the n communication vectors (one for each connection), has n + i components: the first n ones indicating the movement in space, the last in time, which is always positive (counting invocations). These n communication vectors, togetiler witil the communication vector [0,0,...,0,11, representing the processors's communication of its current state to its next state, form the basis communication vectors. In an u-dimensional network, there can be more than one set of basis communication vectors. Taking a two-dimensional network as an example, connections other than the nearest neighbor connection are used, as in a i~exagonal systolic array, where a diagonal connection has a communication vector <ref type="bibr">[1, t. 1]</ref>. Since in general there can be more than two processors along each dimension of a network, the commumcation vector [1, 1, 1] ~s different from but as valid a basis communication vector as [-1. -1, 1]. In fact, different sets of basis communication vectors result in lifferent designs. The Crystal compiler finds all of these designs by enumerating all possible sets of basis communication vectors, so that the most suitable one for the target algorithm can be chosen.</p><p>For the partition ~roblem. its process structure is twodimensional and its design can be realized in a one-dimensional network of processors. The symmetry of a one-dimensional network can be described by the symmetry group (7 ~f {E'(identity), R~ 180 tegree rotation)}.</p><p>Since the different directions of data flow at each processor have different symmetry properties, all possible directions of data flow at each processor can be obtained by enumerating the subgroups of G. This enurneration of possible directions of data flow at each processor by symmetry group <ref type="bibr" target="#b12">[14]</ref> can be generalized to an n dimensional network. As shown in Figure <ref type="figure">3</ref>, each subgroup is identified with a set of basis communication vectors that describes the data flow at each processor.</p><p>The sets of communication vectors, each of which corresponds to a subgroup of G, consist of <ref type="bibr">[1, 11, [-1,1]</ref>} ( O' itself as the subgroup), and {[0, 11 [t 1]} (subgroup {El). All possible mappings from a set of basis difference vectors to a. set of basis communication vectors are ms follows where the transpose of , ' q" de,Z 1 ~ def 0 each of the bas,s vectors ,s defined as~ = (O),Y = (i),</p><p>Since the program is uniform, tile mapping between the two basis sets gives the linear mapping from (i,j) to a space-time index pair [s,t] immediately. For instance, linear transform Ti results in the following mapping: Proposition ~.1. Linear transform T1 results in a function f, f(i,j) Â¢~f [i,i + j] that maps process (i,j) to invocations of processors [s,t], for 0 &lt; i _&lt; m and 0 &lt; j _&lt; k'.</p><p>Since f is a 1 to 1 function, its inverse f 1 maps each invocation [s,t] to a process (i,j). Let Q[s,t] d$~ C(i,j), then Q = Cf -1 . Algebraically, the following STREQ for the partition problem is obtained by substitution of the recursion variabies i and j for s and i, difference vectors (1,0) and (0,0 for communication vectors <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref> and [0, s], and renaming C(i,j) ms Q <ref type="bibr">[s,t]</ref> in Equation <ref type="formula">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Compiling STREQ to machine code or YLSI</head><p>Tile resulting design described by the STREQ is ready co be compiled locally to each processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~.5.1. Processor and time requiremengs</head><p>The design consists of a cue-dimensional array of m processors which complete the computation in k-~m--1 time steps, as indicated by the range of s and t. The network of processors ~nd a few execution traces are shown in Figure <ref type="figure" target="#fig_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2.">Input ports and storage requirements</head><p>In the STREQ, if a communication vector has a non-zero space component, then the corresponding datum is an input such as Q[s -t,t -1]; otherwise it is a stored value such as Q[s, t -s]. Processor ,~ (0 &lt; s &lt; m) must be incorporated with s registers because it must hold Q[s,t] for s time steps. Thus a total 6f O(m ~) distributed memory is used by this design. The ~non-llaearity" of this design shows up as the different numbers of registers needed by the processors. At each invocation, one of' the s registers is accessed and updated, and all of them are accessed by successive invocations in round--robin fashion. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3,5.3, Control reqltirement~</head><p>The guards of conditionals in the STREQ are implemented differently, depending on different target implementation media. In a rauhiproeessor machine, the ~processor id" ~ is stored and an **invoeation counter" keeps track of t until the condition t &gt; 2s is satisfied. In a VLSI implementation, however, it is too costly to store these two integers of length !og(k + m) and perform arithematic operations on them. The fo!Iowing optimization of STREQ is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ao5.4. Optimizatior~ performed on STREQ</head><p>For any guard that is not time dependent, such as s = 1, can be "hard-wired" in a design, however, a time dependent guard such as ~ = 2s envolves keeping track of L Any movement of a data stream takes time, hence it can be used to compute a time-dependent guard. We call such an added data stream a eonfro~ s~reara. Equation 3.2 is a control stream which computes : 2s A better design can thus be obtained by replacing the expensive computations of the guards by transferring a one-bit contro] signal, and using a 2-bit control state to record which function should be performed when a guard is true. The optimized system of STREQ derived from Equation (a.1) is the following, where Bee, gl is a data stream that carries the control signs!, and Pea, t I is the stored control state: else--~ P[a, t-1] Note that both control streams themselves do not use any time dependent guard, except for those that compare with some constant. Central to an optimizing compiler for parallel programs is making trade-offs between communications and computations, as illustrated here and in Section 4 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ao6o Other designs for solving the partitim~ problem</head><p>The design using mappings T~ can be obtained similarly ms above, and the space-time mapping function f is also a linear transform defined as [s,*] = f(i,j):= [j,i + j] for 0 &lt; i S m and 0 &lt; j 5 A:. Such a design ha.s k processors. For different i, the communication path at each processor s changes because the difference vector (0,i) is mapped to a communication vector [*,i] whose first component is non-zero. Figure <ref type="figure" target="#fig_7">5(a)</ref> shows the execution traces for such a program. This systolic program can only be efficiently implemented by using switch networks due to the requirement for reconfigurable connections. Its direct implementation in VLSI is too costly in area because the connections provided for potential communications in each processor are of O(rn). One way to cut down on the number of connections is to decompose <ref type="bibr">[i,i]</ref> into compositions of ?-: [1, 1] so that C(i,j -i) is sent to the processor where C(i,j) is computed via other processors. This systolic program, however, requires that the bandwlth of each channel for communication be proportional to .m.~ still not a good solution in terms off VLSI implementation. Figure <ref type="figure" target="#fig_7">5</ref>(a) shows the execution traces of the modified design. The same implementation issties arise if it is to tie implemented on a multiprocessor machine.</p><p>In summary, the time complexity of this systolic program is k + m, the same as derived from T1, but the latency is k. The total number of processors required is k, and the total (b) The program derived from mappings Ts has problems similar to the one for 7Â½ since the difference vector (O,i) is also mapped to a communication vector that is not stationary inn space. The design is even less efficient then that obtained from T~ because some of the invocations do not have processes mapping to them, hence tb.ey perform "no-op" instead of useful computations.</p><formula xml:id="formula_0">P2 P3 Pk &lt;%,%,%<label>(a)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Limited number of processors and network dimensions</head><p>The space-time mapping above does not address the problems of a limited number of processors and a limited degree of connectivity at each processor. The first problem can be solved by modifying the space-time mapping to take the limitation into account. For example, for the partition problem, if the number of processor in the network is l, and 1 &lt; m, the following space-time mapping f* can be used:</p><formula xml:id="formula_1">is, t] = f*(i,j) ~e [i(mod l),i + j + (i/l) x m],</formula><p>where "/" is the integer division operation. This mapping based on f is obtained by (1) first '~folding around" the network of size l to obtain a function is,t] =f'(i,j) d=ef[i(rnod 1),~ +j], which is not one-to-one, and then (2) shuming the processes that are mapped to the same invocation is, t]. The shuffllng can be performed in many different ways, as long as it respects ~&lt;', the data dependency relation on the DAG. Note that in CrystaFs parallelism, toad balancing during execution does not become a problem for programs having data independent flows (such as this example). In contrast to the case of a program interpreted by applicative parallelism where mappings of computation to smaller networks create exponential congestion in some of the processors, and load balancing becomes a series problem, as shown in Martin's Torus <ref type="bibr" target="#b14">[16]</ref>.</p><p>A similar modification that performs projection followed by shuffling can be applied in the case where the dimensionatity of the basis communication vectors exceeds that of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4~ Optlmlzing Compilers</head><p>A compiler for a sequential programming language must allocate resources such as registers in an intelligent way. Analogously, a compiler for a parallel programming language must allocate resources~ this time processors and communication channels, rather than those resources within a processor. The central concern of a paralleI compiler is the trade-off between the computations local ~o each processor and the communications between processors.</p><p>Beyond the optimization performed on the space-time reeursion equations mentioned above, an algorithm can be further improved~ or optimized a~ the source level to yield better parallel programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>~Â°Io Optimal dlmenslonatlty</head><p>As illus{rated above, a program with a k-dimensional process structure is compiled to a design using a (k-1)-dimensional network. If the dimensionality of the actual machine ~-s-i-e;s than k-i~ then the design must be projected down to be fitted on the machine. If, on the other hand, the dimensionallty of the machine is greater, one might suspect whether or not the power of the machine is fully utilized. The notions of fized fan-ins and fixed fan-outs, motivated by the limitation on the bandwidth for communication at each procsssor~ are used to characterize whether or not the process structure of a program is an optimal one. The number of fan-ins of a program is the maximum number of terms appearing on the rlght-hand side of any of the recursion equations of the programÂ° The number of famouts of a program is the maximum of the total number of times a particular datum appears on the rlght-hand side of all of the reeursion equations. In any physical implementation, the bandwidth of communication channels is fixed, not proportional to a problem size, and therefore the growth of fan-ins (or fan-outs) with the problem size must be avoided. When a program does not have fixed fan-ins or fixed fan-outs, the dimensionMity of its process structure must be increased to allow a greater degree of connectivity in the hope of bringing the number of fan-ins and fan-outs down to constants. An illustration of such optimization (stage 1 of the compilation process) by transforming algebraically the definition of a problem to a system of recursion equation that has low fan-in and famout degrees and is low order, i.e., using only local communication~ is illustrated in <ref type="bibr" target="#b3">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4Â°2Â° Detecting common expensive computations</head><p>Similar to the idea of detecting common sub-expressions in conventional compilation, detecting common expensive corn. pntation can reduce the cost of implementation. An expensive computation is one that takes more time than the computation (or communication) time in an otherwise balanced implementation. Hence more powerful computing resources are dedicated to this operation to keep up the performance. The total cost of the computation can be reduced if the results computed by the process using the expensive resource can be transferred to other processes rather than having the common expensive com~ putation be carried out by many processes. Such an optimization technique can be applied to STI2EQ at the source level as illustrated by the following example. The progrann for LU decomposition of a matriÃ consists of recursion equations for three data streams: stream a for the matrix to be decomposed, and streams l and u containing respectively the lower and upper triangular matrices to be obtained The system of recursion equations is defined on process ssruc~ure N 3, where N ~-~.f {0, 1.2,... on}. The Table <ref type="table">1</ref> contains "the pars of the program that is transformed ~o yield a better design by reducing the number of division operations needed from order leh hand side conditions right hand side before after  </p><formula xml:id="formula_2">l(i,j,k) (j=k) A(i&gt;k) ~(i,j,k-l) x[u(i,j,k)] -~ a(i,j,k-1) xb(i,j,k) u(i,j,k) (j=k) Ati=k) afi,j,k-1% a(i,j,k-1) (j=k) A(i &gt; k) ufi-1,j,k) u(i-l,j,k) b(i,j,k) (i : k) A (i : k) (j = t~) A (i &gt; k) [a(i,j,k-1)] -I b(i -I., j, k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S. Conch~sions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Results</head><p>The language of recursion equations provides a simpl&lt; straight4orward~ and conceptually clean way of describing algo-rRhms~ When interpreted by Crystal's parallelism, it naturally discloses the communications between processes and the con&gt; putadon within an individual process. The Crystal compiler takes these equations and automatically generates an e~cient parallel program. Among the parallel programs, or VLSI a&gt; chRecture generated are: a design for LU decomposition of rnatrk'es [6] that is three thnes more ef~cient tha, n that ia [13], a class of fast multipliers for the long multiplication algorithm [4], and new and efficient parallel hnpiementations for integer p.'.~r-tltlon% transitive closure, and dynamic programming <ref type="bibr" target="#b5">[7]</ref>. Thus based on language Crystal, a unified theory for understanding and generating any systolic design comes into existence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.~Â° Lang~mge 1states</head><p>To the questions raised in the very beginning of this paper, Crystal provides Rs answers: A programmer need not specify expticit communications in a program; the specification is a functional definRion that is natural to the probleni rather than tailored to a machine. In a Crystal program~ cost of cemnmnications can be extracted from difference vectors, as they are mapped to commuaicadoa vectors each of which has a cost associated with the target machine and technology. Simib.n']y, the cost of" computation can be extracted from the processing function, which is a composRion of a set of hawse functions, and each has a cost depending on the in~plementation medium. Thus computation/cnlrmlui~ieation trade-offs c.an be expressed forreally as transformations of Crystal programs, and carried out by an optimizing compiler. As discussed in Secti6n 2, recurs[on equations of the form (2.2) with unbounded minilnalization are general purpose, and a process structure can ilave an arbitrary topology. Last but not least, Crystal does encourage large scale parallelism; in fact its phylosopby of computing hnposes such parallelism, it allows composRion and abstraction of Crystal progra:ms in which no hnplementation details are of any concern </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Â®</head><p>Such capability makes Crystal appropriate not only as a programming language for problem solving, but also as a system programming language for multi-user operating systems and a specification language for interfacing systems consisting of dissimilar sub-systems. A multi-user operating system can be viewed as a parallel system consisting of loosely coupled user tasks and system resources. The implementation of such an operating system can certainly be described by recursion equations as the interaction of a collection of processes, each being the abstraction of a user program which, at its highest level, consists of a system of recursion equations. Hence the problems of efficient resource allocation at the operating system level becomes very similar to that of Crystal's optimizing compiler, and both can be tackled in a uniform framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>I would like to thank Martin Rein for suggesting the integer partition problem as an example for illustrating both the language and the Synthesis method. My thanks go to Andrea Pappus and Chris Hatchell for their assistance with the manuscript and, in particular, Andrea for all of her art work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FigL~re 1 :</head><label>1</label><figDesc>FigL~re 1: The applicative interpretation of Equation (2A) for c(4,r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>; 9 .</head><label>9</label><figDesc>4. Advantage of CrystaFs parallelism Is there any advantage to Crystal's interpretation of recursion equations? The asymptotic complexity for computing C(m, k) by applicative parallel evaluation amounts to O(m+k) 0 .~: m for the time requirement and ((E~g(Y&amp;-~)) for tl~e space requirement. The time complexly is obtained from counting the longest path from the root of the tree C(rn, k) to a leaf node, i.e., C(1, *), and the space complexiW is obtained by noticing that if k &lt; m then C(m,,[:) = C(k -t,h) + 1 and then from {k]m-1 for ]c &gt; m, where each the sum 1 + ~ + (~)~ + + ,;~, term @)i-1 is the asymptotic space complexity for computing the right branch of C(i, ~'), In contrast a naive implementation of Crystal's parallelism requires only k x m number of processors (the number of nodes in the DAG) and k + m time steps (the longest path from a source to node (m,k))~ This implementation is to use one processor for each process in the DAG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F~</head><label></label><figDesc>f Iv1, v2,, .., vn] E A, A is a discrete structur% which is a Cartesian product A1 x A 2 Ã ... x An where each Ai, say, is a subset of the set of integers~ or the set of rationals, and (Ai,E_) is a flat lattice, where "~" is the approximation order<ref type="bibr" target="#b16">[18]</ref>. function ~ii maps from A to A. * functional variables F1, F2,..., F,, range over continuous functions mapping from (A,E) to (D,~), where D is some value domain such as the set of integers~ reals, etc, and (D, E) is a flat lattice. e functions ~ol, ~,..., ~n, are continuous functions over the value domains (D, E). Thus (2.2) is a system of fixed-point equations, and on its right hand side. ~(r~, F2 ..... ~,).~v.[Â¢~ (...), Â¢2('" "),. -â¢, Â¢,('" ")l is a continuous function mapping from E n ~o E n. where E de_f [(A,~) (D,~)], the domain of continuous functions from (A, _if) ~o (D, E). Such a system of equations embodies a parallel compu-~mion on a discrete structure A with multiple data streams</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>. 1 .</head><label>1</label><figDesc>Proposition 3.2. The space-time recurs/on equation for the partition problem is where0&lt; s &lt;_ m and s&lt; t &lt; k + m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure ~ :</head><label>~</label><figDesc>Figure ~: Symmetry groups and the sets of communication vectors that describe the data flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A systolic implementation for the partition problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig~.tre 5 :</head><label>5</label><figDesc>Fig~.tre 5: (a) The systolic program lining linear transfbrm ~, which needs re&lt;:onfigurz~ble connections. (b) A modified systolic design whleh needs channels of bandwidd~ O(rn)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>n ~ to n. The modification consists of moving the processes performing divisions in a region of the half plane { &gt; k = j to those in a segment of the line { = j = ~; by using an extra data stream b which flows along the same direction as stream ~. Stream I~ can be sent on the same channel as ~.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>F[</head><label></label><figDesc>Fig~lre 6: The compiiatioa steps of a Crystal program.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table i :</head><label>i</label><figDesc>Code optimization for LU decomposition</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Proving Properties of Programs by Stuctural Induction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Burstall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="48" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Characterization of Deadlock-Free Resource Contention</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rum</surname></persName>
		</author>
		<idno>4684</idno>
		<imprint>
			<date type="published" when="1982-01">January 1982</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Generation of a Class of Multipliers: a Synthesis Approach to the Design of Highly Parallel Algorithms in VLSI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen~</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tEEE International Conference on Computer Design: VLSI in Computers</title>
		<meeting>the tEEE International Conference on Computer Design: VLSI in Computers</meeting>
		<imprint>
			<date type="published" when="1983-05">May 1983. {4. October 1985</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
	<note>Space-time Algorithm.s: Semantics and Methodology</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Automatic Generation of VLSI Architectures: Synthesis by Algorithm Transformation</title>
		<idno>427</idno>
		<imprint>
			<date type="published" when="1985-10">October 1985</date>
		</imprint>
		<respStmt>
			<orgName>Yale University,</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Synthesizing</forename><surname>Systolic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Designs</forename></persName>
		</author>
		<title level="m">Proceedings of the Second International Symposium on VLSI Technology, Systems, and Applications</title>
		<meeting>the Second International Symposium on VLSI Technology, Systems, and Applications</meeting>
		<imprint>
			<date type="published" when="1985-05">May 1985</date>
			<biblScope unit="page" from="209" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Synthesis Method for Systolic Designs</title>
		<imprint>
			<date type="published" when="1985-01">January 1985</date>
		</imprint>
		<respStmt>
			<orgName>Yale University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report 334</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data Flow Program Graphs</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">M</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="26" to="41" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Crystal Mlticomputer: Design and Implementation Experience</title>
		<author>
			<persName><forename type="middle">D J</forename><surname>Devitt</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">R</forename><surname>Pinkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno>553</idno>
		<imprint>
			<date type="published" when="1984-09">September 1984</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Second Opinion on Data Flow Machines and Languages</title>
		<author>
			<persName><surname>Gajski</surname></persName>
		</author>
		<author>
			<persName><surname>Padua</surname></persName>
		</author>
		<author>
			<persName><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="58" to="69" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A R</forename><surname>Hoare</surname></persName>
		</author>
		<title level="m">Communicating Sequential Processes, Communication of ACM</title>
		<imprint>
			<date type="published" when="1978">1978</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="666" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">OCCAM PrOgramming Manual</title>
		<author>
			<persName><surname>Inmos Limited</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International series in computer science</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Algorithms for VLSI Processor Arrays, Introduction to VLS~ Systems by Mead and Conway</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Application of Group Theory in Classifying Systolic Arrays</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Mead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Display File</title>
		<imprint>
			<biblScope unit="volume">5006</biblScope>
			<date>Mar t982</date>
		</imprint>
	</monogr>
	<note>Caltech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Mathematical Theory of Computation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Manna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Toms: An Exercise in Constructing a Processing Surface</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Conference on VLSI: Architecture, Designs Fabrications</title>
		<meeting>Conference on VLSI: Architecture, Designs Fabrications</meeting>
		<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="52" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Crystal: A Timing Analyzer for aMOS VLSI Circuits, Third Calteeh conference on Very Large Scale Integration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ousterhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Computer Science Press</publisher>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward a Mathematical Semantics for Computer Languages</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strachey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Computers and</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Fox</surname></persName>
		</editor>
		<meeting>the Symposium on Computers and<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Automata~ Polytechnic Institute of Brooklyn Press</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="page" from="19" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Cosmic Cube</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">L</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="page" from="22" to="33" />
			<date type="published" when="1985-01">January (1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Systolic Programming: A Paradigm of Parallel Processing</title>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Shapiro</surname></persName>
		</author>
		<idno>csg4-16</idno>
	</analytic>
	<monogr>
		<title level="s">Department of Applied Mathematics</title>
		<imprint>
			<date type="published" when="1985-01">January 1985</date>
			<publisher>The Weizmann Institute of Science</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
