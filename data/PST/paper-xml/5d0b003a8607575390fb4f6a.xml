<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Encoding Strategy Based Word-Character LSTM for Chinese NER</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hefei Innovation Research Institute</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tongge</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hefei Innovation Research Institute</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Cyber Science and Technology</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qinghua</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiayu</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Hefei Innovation Research Institute</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yueran</forename><surname>Zu</surname></persName>
							<email>yueranzu@buaa.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Beihang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An Encoding Strategy Based Word-Character LSTM for Chinese NER</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A recently proposed lattice model has demonstrated that words in character sequence can provide rich word boundary information for character-based Chinese NER model. In this model, word information is integrated into a shortcut path between the start and the end characters of the word. However, the existence of shortcut path may cause the model to degenerate into a partial word-based model, which will suffer from word segmentation errors. Furthermore, the lattice model can not be trained in batches due to its DAG structure. In this paper, we propose a novel wordcharacter LSTM(WC-LSTM) model to add word information into the start or the end character of the word, alleviating the influence of word segmentation errors while obtaining the word boundary information. Four different strategies are explored in our model to encode word information into a fixed-sized representation for efficient batch training. Experiments on benchmark datasets show that our proposed model outperforms other stateof-the-arts models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Name Entity Recognition(NER) is a basic task of many NLP systems including Information Retrieval <ref type="bibr" target="#b30">(Virga and Khudanpur, 2003)</ref>, Relationship Extraction <ref type="bibr" target="#b22">(Miwa and Bansal, 2016)</ref>, Question Answering <ref type="bibr" target="#b23">(Mollá et al., 2006)</ref>. The main task of NER is to identify named entities such as person, location, organization, etc. in given text. Various methods have been proposed to tackle this problem, including Hidden Markov Models(HMMs) <ref type="bibr" target="#b27">(Saito and Nagata, 2003)</ref>, Maximum Entropy Models(ME) <ref type="bibr" target="#b3">(Chieu and Ng, 2003)</ref>, Support Vector Machines(SVM) <ref type="bibr" target="#b8">(Ekbal and Bandyopadhyay, 2010)</ref> and Conditional Random Fields(CRF) <ref type="bibr" target="#b9">(Feng et al., 2006)</ref>. → "水(Water)", the model incorrectly predicts that "江(River)" and "水(Water)" belong to the same entity. Red labels(without underline) denote predicted labels, and blue labels(with underline) denote gold labels.</p><p>works <ref type="bibr" target="#b13">(Huang et al., 2015;</ref><ref type="bibr" target="#b15">Lample et al., 2016;</ref><ref type="bibr" target="#b11">Habibi et al., 2017)</ref> have been introduced to NER task. To avoid the segmentation errors, most of neural Chinese NER models are character-based.</p><p>Although character-based method has achieved good performance, it does not exploit word information in character sequence. Entity boundaries usually coincide with some word boundaries, which suggests that words in character sequence can provide rich boundary information for character-based model. To integrate words information into character-based model, <ref type="bibr" target="#b40">Zhang and Yang (2018)</ref> propose a lattice-structured LSTM model to encode a sequence of input characters as well as all potential words that match a lexicon. Their model is an extension of characterbased LSTM-CRF model and uses extra "shortcut paths" to link the memory cell between the start and the end characters of a word for utilizing word information. And the gated recurrent unit is used to control the contribution of shortcut paths and path between adjacent characters. However, as the study of <ref type="bibr">(Yang et al., 2018)</ref> shown, the gate mechanism fails to choose the right path sometimes. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, wrong choices may cause lattice model to degenerate into a partial word-based model, which suffers from word segmentation errors. In addition, due to the variable length of words, the length of the whole path is not fixed. Besides, each character is bounded with a variable-sized candidate word sets, which means the amount of incoming and outcoming paths is not fixed either. In this case, lattice LSTM model is deprived of the power of batch training, and hence it is highly inefficient.</p><p>To address the above problems, we propose a novel word-character LSTM(WC-LSTM) to integrate word information into character-based model. To prevent our model from degenerating into a partial word-based model, we assign word information to a single character and ensure that there are no shortcut paths between characters. Specifically, word information is assigned to its end character and start character in forward WC-LSTM and backward WC-LSTM respectively. We introduce four strategies to extract fixed-sized useful information from different words, which ensures that our proposed model can perform batch training without losing word information.</p><p>We demonstrate the effectiveness of our architecture on four widely used datasets. Experimental results show that our proposed model outperforms other state-of-the-art models on the four datasets.</p><p>Our contributions of this paper can be concluded as follows:</p><p>• We propose a novel word-character LSTM(WC-LSTM) to incorporate word information into character-based model.</p><p>• We explore four different strategies to encode word information into a fixed-sized vector, which enables our proposed model to be trained in batches and adapted to various application scenarios.</p><p>• Our proposed model outperforms other models and achieves new state-of-the-art over four Chinese NER datasets. We release the source code for further research<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Neural Networks have been shown to achieve impressive results on Name Entity Recognition task <ref type="bibr" target="#b10">(Gregoric et al., 2018;</ref><ref type="bibr" target="#b17">Lin and Lu, 2018)</ref>. Based on the level of granularity, most of the models can be divided into three categories: word-based models, character-based models, and hybrid models.</p><p>Word-Based Models. <ref type="bibr" target="#b5">Collobert and Weston (2008)</ref> propose one of the first word-based models for NER, with feature constructed from orthographic features, dictionaries and lexicons <ref type="bibr" target="#b33">(Yadav and Bethard, 2018)</ref>. <ref type="bibr" target="#b6">Collobert et al. (2011)</ref> replace the hand-crafted features with word embeddings. <ref type="bibr" target="#b13">Huang et al. (2015)</ref> propose a BiLSTM-CRF model for NER and achieves good performance. <ref type="bibr" target="#b21">Ma and Hovy (2016)</ref> and <ref type="bibr" target="#b4">Chiu and Nichols (2016)</ref> use CNN to capture spelling characteristics and <ref type="bibr" target="#b15">Lample et al. (2016)</ref> use LSTM instead. When applied to Chinese NER, the above models all suffer from segmentation errors, since Chinese word segmentation is compulsory for those models.</p><p>Character-Based Models. <ref type="bibr" target="#b24">Peng and Dredze (2015)</ref> propose to add segmentation features for better recognition of entity boundary. <ref type="bibr" target="#b7">Dong et al. (2016)</ref> integrate radical-level features into character-based model. To eliminate the ambiguity of character, <ref type="bibr" target="#b29">Sun and He (2017)</ref> take the position of character into account. Although the above models have achieved good results, they all ignore word information in character sequence.</p><p>Hybrid Models. Some efforts have been made to integrate word boundary information into character-based models. Motivated by the success of multi-task learning for Natural Language Processing <ref type="bibr" target="#b19">(Liu et al., 2016</ref><ref type="bibr" target="#b20">(Liu et al., , 2017;;</ref><ref type="bibr">Zhang et al., 2018)</ref>, <ref type="bibr" target="#b25">Peng and Dredze (2016)</ref> first proposed to jointly train Chinese NER with Chinese word segmentation(CWS) task. <ref type="bibr" target="#b0">Cao et al. (2018)</ref> apply adversarial transfer learning framework to integrate the task-shared word boundary information into Chinese NER task. Another way to obtain word boundary information is proposed by <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, using a lattice LSTM to integrate word information into character-based model, which is similar to what is proposed in Where "&lt;PAD&gt;" denotes padding value; "Stgy" denotes a certain encoding strategy and ⊕ denotes concatenation operation.</p><p>this paper. The main differences are as follows.</p><p>Firstly, they exploit word information by a DAGstructured LSTM, while we use a chain-structured LSTM. Secondly, instead of integrating to the hidden state of LSTM, our model add word information into the input vector. Finally, our model can be trained in batches and is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The architecture of our proposed model is shown in Figure <ref type="figure" target="#fig_1">2</ref>. Same as the widely used neural Chinese NER model, we use LSTM-CRF as our main network structure. The differences between our model and a standard LSTM-CRF model are mainly on the embedding layer and LSTM and can be summarized as follows. First, we represent a Chinese sentence as a sequence of character-words pairs to integrate word information into each character. Second, to enable our model to train in batches and to meet different application requirements, we introduce four encoding strategies to extract fixed-sized but different information from words. Finally, a chain-structured word-character LSTM is used to extract features from both character and word for better predicting.</p><p>Next, we will explain the main ideas for each component, including word-character embedding layer, word encoding strategy, and word-character LSTM.</p><p>Formally, we denote a Chinese sentence as s = {c 1 , c 2 , ..., c n }, where c i denotes the i th character. We use c b,e to denote a character subsequence in s, which begins with b th character and ends with e th character. Take the sentence in Figure <ref type="figure" target="#fig_1">2</ref> for example, c 1,2 is "上涨(Rise)". We use −→ ws i to denote words assigned to i th character in forward WC-LSTM, which are a set of character subsequences c b,i , where b &lt; i and c b,i matches a word in lexicon D. The lexicon D is the same as the one used in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, which is built by using automatically segmented large raw text. Similarly, we use ←− ws i to denote the words for i th character in backward WC-LSTM, which are a set of character subsequences c i,e , where e &gt; i and c i,e matches a word in lexicon D. Finally, the sentence s is represented as</p><formula xml:id="formula_0">− → rs = {(c 1 , − − → ws 1 ), (c 2 , − − → ws 2 ), ..., (c n , − − → ws n )} in our model, and its reverse representation is ← − rs = {(c n , ← − − ws n ), (c n−1 , ← −−− − ws n−1 ), ..., (c 1 , ← − − ws 1 )}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Word-Character Embedding Layer</head><p>In our model, Each position i in − → rs consists of two parts: i th character c i and the assigned words −→ ws i . The origin number of words in −→ ws i is s t i , and words are sorted by their length. We ensure each −→ ws i has the same number s p i 2 in the whole batch by padding. We embed each character c i in dis-tributional space as x c i :</p><formula xml:id="formula_1">x c i = e c (c i ) (1)</formula><p>where e c denotes a pre-trained character embedding lookup table. Similarly, for each −→ ws</p><formula xml:id="formula_2">i = { − → w i1 , ..., − − → w is p i }, the l th word − → w il in −→ ws i is repre- sented using x − → w il = e w ( − → w il )<label>(2)</label></formula><p>where e w denotes a pre-trained word embedding lookup table. As a result, the distributional representation of words −→ ws i is {x</p><formula xml:id="formula_3">− → w i1 , ..., x − → w is p i }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Words Encoding Strategy</head><p>Although the number of assigned words s p i for each character c i is same in one batch, the number varies from batch to batch. As a result, the size of input to the model is not fixed, which is not conducive to batch training. To acquire fixedsized input, we introduce four different encoding strategies in this section. And we use x − → ws i to denote the final representation of word information for position i in following sections.</p><p>Shortest Word First: For each word set −→</p><formula xml:id="formula_4">ws i = { − → w i1 , ..., − − → w is p i },</formula><p>we simply select word whose length is the shortest, i.e. − → w i1 . Then</p><formula xml:id="formula_5">x − → ws i = x − → w i1</formula><p>(3)</p><p>Longest Word First: Contrary to the shortest word first, we select word whose length is the longest, i.e. − − → w is t i . Note that s t i may be 0, in this case, we set it to 1. Then</p><formula xml:id="formula_6">x − → ws i = x − → w is t i (4)</formula><p>Average: While the first two strategies can only use the information of partial words, we introduce an average strategy to utilize all word information. As its name indicates, the average strategy computes the centroid of the embeddings of all elements except paddings in word set , i.e. { − → w i1 , ..., − − → w is t i }. If s t i = 0, we simply average all the padding value in the word set. Then</p><formula xml:id="formula_7">x − → ws i =    1 s t i s t i l=1 x − → w il , if s t i &gt; 0 1 s p i s p i l=1 x − → w il , if s t i = 0</formula><p>(5)</p><p>Self-Attention: Inspired by self-attention mechanism applied to sentence embedding <ref type="bibr" target="#b18">(Lin et al., 2017)</ref>, we exploit self-attention to better capture useful information from assigned words. For simplicity, we denote all the x − → w il as W i , which has the size s p i -by-d w , where d w denotes the dimensionality of word embedding e w .</p><formula xml:id="formula_8">W i = (x − → w i1 , ..., x − → w is p i ) (6)</formula><p>We use self-attention mechanism to obtain a linear combination of s p i word embeddings in W i . The attention mechanism takes W i as input, and generates a weight vector a i .</p><formula xml:id="formula_9">a i = sof tmax(w 2 tanh(W 1 W T i )) (7)</formula><p>W 1 is a weight matrix with the size of d a -by-d w and w 2 is a d a dimensional vector, where d a is a hyperparameter. Both of them are trainable parameters. If s t i &gt; 0, we use the mask to exclude the padding values; otherwise we reserve them. Finally, we use a i to get the weighted sum of all words.</p><p>x</p><formula xml:id="formula_10">− → ws i = s t i l=1 a il x − → w il , if s t i &gt; 0 s p i l=1 a il x − → w il , if s t i = 0<label>(8)</label></formula><p>where a il denotes the l th value in a i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Word-Character LSTM(WC-LSTM)</head><p>Inspired by the way character bigram is integrated into sequence labeling model <ref type="bibr" target="#b2">(Chen et al., 2015;</ref><ref type="bibr" target="#b36">Yang et al., 2017)</ref>, we concatenate each x c i with x − → ws i to utilize word information. And this is quite different from the way used in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, since they use extra shortcut paths to integrate word information into the hidden layer of LSTM. By concatenating, there is no shortcut path in our model and information can only flow between adjacent characters, which ensures that our model will not degenerate into a partial wordbased model. Then the WC-LSTM functions are:</p><formula xml:id="formula_11">    ci o i i i f i     =     tanh σ σ σ     W p x i h i−1 + b p (9) x i = x c i ⊕ x − → ws i c i = ci i i + c i−1 f i h i = o i tanh(c i )<label>(10)</label></formula><p>where o i , i i and f i denote output gate, input gate and forget gate respectively. W p and b p are parameters of affine transformation; σ denotes the logistic sigmoid function; ⊕ denotes concatenation operation and denotes elementwise multiplication.</p><p>The bidirectional WC-LSTM is applied in our model to leverage both information from the past and the future. To get the future information, we use a second WC-LSTM that reads the reverse representation of − → rs, i.e., ← − rs = {(c n , ← − − ws n ), (c n−1 , ← −−− − ws n−1 ), ..., (c 1 , ← − − ws 1 )}. And the following operations to get each backward WC-LSTM hidden vector ← − h i is the same as the one in the forward WC-LSTM. Finally, the update of each bidirectional WC-LSTM unit can be written as follows:</p><formula xml:id="formula_12">− → x i = x c i ⊕ x − → ws i ← − x i = x c i ⊕ x ← − ws i − → h i = − −−−−−−−− → WC − LSTM( − − → h i−1 , − → x i ) ← − h i = ← −−−−−−−− − WC − LSTM( ← − − h i+1 , ← − x i ) h i = − → h i ⊕ ← − h i<label>(11)</label></formula><p>where − → h i and ← − h i are hidden states at position i of forward and backward WC-LSTM respectively, and ⊕ denotes concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Decoding and Training</head><p>Considering the dependencies between successive labels, we use a CRF layer to make sequence tagging. We define matrix O to be scores calculated based on the output H = {h 1 , h 2 , ..., h n }:</p><formula xml:id="formula_13">O = W o H + b o (12)</formula><p>For a label sequence y = {y 1 , y 2 , ..., y n }, we define its probability to be: While decoding, we use the Viterbi algorithm to find the label sequences that obtained the highest score:</p><formula xml:id="formula_14">p(y|s) = exp i O i,y i + T y i−1 ,y i ỹ exp i O i,ỹ i + T ỹi−1 ,ỹ i<label>(</label></formula><formula xml:id="formula_15">y * = arg max y∈ỹ i O i,y i + T y i−1 ,y i<label>(14)</label></formula><p>Given N manually labeled data {(s j , y j )}| N j=1 , we minimize the sentence-level negative loglikelihood loss to train the model:  <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>.</p><formula xml:id="formula_16">L = − j log(p(y j |s j ))<label>(15)</label></formula><p>For OntoNotes, we use the same training, development and test splits as <ref type="bibr" target="#b1">(Che et al., 2013)</ref>. For other datasets which have already been split, and we don't change them. We summarize the datasets in Table <ref type="table" target="#tab_0">1</ref>. Implementation Details. We utilize the character and word embeddings used in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, both of which are pre-trained on Chinese Giga-Word using word2vec model. Following <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>, we use the word embedding dictionary as Lexicon D in our model. For characters and words that do not appear in the pretrained embeddings, we initialize them with a uniform distribution<ref type="foot" target="#foot_2">3</ref> . When training the model, character embeddings and word embeddings are updated along with other parameters.</p><p>For hyper-parameter configurations, we mostly refer to the settings in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>. We set both character embedding size and word embedding size to 50. The dimensionality of each unidirectional multi-input LSTM hidden states is 100 for Weibo NER and Chinese Resume, and 200 for OntoNote 4 and MSRA. For self-attention strategy, we set the d a to 50. To avoid overfitting, we apply dropout to both embeddings and LSTM with a rate of 0.5. We use SGD to optimize all the trainable parameters. Learning rate is set to 0.015 initially and decays during training at a rate For evaluation, we use the Precision(P), Recall(R) and F1 score as metrics in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head><p>OntoNotes. Table <ref type="table">2</ref> shows the experimental results on OntoNote 4 dataset. The "Input" column shows the representation of input sentence, where "Gold seg" means a sequence of words with goldstandard segmentation, and "No seg" means a sequence of character without any segmentation.</p><p>The first block in Table <ref type="table">2</ref> are the results of wordbased models <ref type="bibr" target="#b31">(Wang et al., 2013;</ref><ref type="bibr" target="#b1">Che et al., 2013;</ref><ref type="bibr" target="#b34">Yang et al., 2016)</ref>. By using gold-standard segmentation and external labeled data, all of them achieve good performance. But the only resource used in our model are pretrained character and word embeddings.</p><p>The first two rows in the second block show the performance of the lattice model and characterbased model. The character baseline denotes the original character-based BiLSTM-CRF model. <ref type="bibr" target="#b40">Zhang and Yang (2018)</ref> propose a lattice LSTM to exploit word information in character sequence, giving the F1 score of 73.88%. Compared with the character baseline, lattice model gains 8.92% improvement in F1 score, which shows the importance of word information in character sequence.</p><p>In the last four rows, we list the results of our proposed model. The results show that all of our models outperform other character-based models, and the one with self-attention strategy achieves the best result. Without gold-standard segmentation and external labeled data, our model gives competitive results to the word-based models on this dataset. Compared with the character baseline, our model with self-attention obtains 9.48% improvement in F1 score, which proves the effectiveness of our way to integrating word information. Compared with lattice model, all of our models achieve better results, which shows that our Models P R F1 <ref type="bibr" target="#b39">Zhang et al. (2006)</ref> 92.20 90.18 91.18 <ref type="bibr" target="#b41">Zhou et al. (2013)</ref> 91.86 88.75 90.28 <ref type="bibr" target="#b7">Dong et al. (2016)</ref> 91.28 90.62 90.95 <ref type="bibr" target="#b0">Cao et al. (2018)</ref> 91.73 89.58 90.64 Lattice <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>   approach to integrating word information is more reasonable than lattice model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSRA.</head><p>Table <ref type="table" target="#tab_3">3</ref> shows the results on MSRA dataset. <ref type="bibr" target="#b39">Zhang et al. (2006)</ref> and <ref type="bibr" target="#b41">Zhou et al. (2013)</ref> use the statistical model with rich hand-crafted features. <ref type="bibr" target="#b7">Dong et al. (2016)</ref> exploit radical features in Chinese character. <ref type="bibr" target="#b0">Cao et al. (2018)</ref> joint train Chinese NER task with Chinese word segmentation, in which adversarial learning and selfattention mechanism are applied for better performance. We can observe that our proposed models outperformance the above models and the one with average strategy achieves new state-of-the-art performance.</p><p>Weibo. Table <ref type="table" target="#tab_4">4</ref> shows the results<ref type="foot" target="#foot_3">4</ref> on Weibo dataset. The "NE", "NM" and "Overall" columns denote F1-score for named entities, nominal entities(excluding named entities) and both respectively. We can see that WC-LSTM model with longest word first strategy achieves new state-ofthe-art performance. Multi-task learning <ref type="bibr">(Peng and</ref><ref type="bibr">Dredze, 2015, 2016;</ref><ref type="bibr" target="#b0">Cao et al., 2018)</ref> and semi-supervised learning <ref type="bibr" target="#b29">(Sun and He, 2017;</ref><ref type="bibr" target="#b12">He and Sun, 2017)</ref>   for Weibo NER task due to the small amount of training data. All of the above models require additional cross-domain or semi-supervised data.</p><p>Compared with those models, our model does not need additional labeled data; we only exploit pretrained character and word embeddings.</p><p>Resume. Table <ref type="table" target="#tab_6">5</ref> shows the results on Chinese Resume dataset. Consistent with the previous results, our models outperform lattice model <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref>. The above experimental results strongly verify that our method to utilize word information is more effective than the lattice model.</p><p>Our proposed model has achieved state-of-theart results on various domains such as news, social media, and Chinese resume.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Efficiency</head><p>To further explore the efficiency of our model, we conduct some comparative experiments on training time and convergence speed. The lattice model proposed in <ref type="bibr" target="#b40">(Zhang and Yang, 2018)</ref> is our principal comparison object, since it also utilizes the word information in character sequence. Our model is an extension of the character-based model, so we also report the results on characterbased model as character baseline. We only conduct our experiments on OnteNotes dataset due to space limitation. And we choose the model with the self-attention strategy for the comparative experiments, as it outperforms other strategies on OntoNotes dataset.</p><p>The training time of each epoch for all models is shown in Table <ref type="table" target="#tab_7">6</ref>. The lattice model needs the most training time for each epoch, since it can only Figure <ref type="figure" target="#fig_3">3</ref> shows the learning curve of the models in Table <ref type="table" target="#tab_7">6</ref>    <ref type="bibr" target="#b14">(Ju et al., 2018;</ref><ref type="bibr" target="#b28">Sohrab and Miwa, 2018)</ref>. Short word first is good at identifying inner nested entities due to the short word information, while longest word first tends to identify flat entities with the help of long word information. Taking "长江三角洲(Yangtze River Delta)" as an example, shortest word first recognizes "长 江(Yangtze)" and "三角洲(Delta)" as entities, but longest word first tend to think that they are part of the entity "长江三角洲(Yangtze River Delta)". Both results are reasonable, but the right result depends on specific needs. The average and self-attention strategies are the combination of all words information and can use more information. Intuitively, they should outperform the shortest word first and the longest word first. But results on Weibo NER(Table <ref type="table" target="#tab_4">4</ref>) and Resume(Table <ref type="table" target="#tab_6">5</ref>) show the opposite effect. We conjecture that this is caused by the small amount of training data since more word information but small dataset will lead to overfitting. The average strategy is a special case of the self-attention strat-  egy where all weights are the same, so we would like to see the latter outperforms the former when training data is sufficient. Surprisingly, the average strategy achieves higher F1 score than the selfattention strategy in MSRA dataset(Table <ref type="table" target="#tab_3">3</ref>). We carefully analyze the experimental results and find that there are a large number of informal texts in the MSRA test set. Specifically, the MSRA test set contains some very long sentences, in which there are a series of Chinese person name without delimiter. As shown in Table <ref type="table" target="#tab_11">8</ref>, when applied to such informal text, the self-attention strategy fails to determine the entity boundary sometimes while the average strategy correctly recognizes the entities. And we conjecture that, with more trainable parameters, the self-attention strategy can better fit the formal text in the training set but cannot adapt well to the informal data in the test set, so it performs worse than the average strategy. Finally, the application scenarios of different strategies can be summarized as followings. If the training data is sufficient, we recommend using self-attention for formal texts and average strategy for informal texts. If there is only a very small amount of annotated data, we recommend using the shortest words first for inner nested entities and longest word first strategy for flat entities. Lexicon and Embeddings. To further analyze the contribution from word lexicon and pretrained word embeddings, we conduct some comparative experiments by using the same word lexicon with and without pretrained embeddings. We choose the strategy that achieving the best performance for each dataset. We estimate the contribution of the lexicon by replacing pretrained word embeddings with randomly initialized embeddings<ref type="foot" target="#foot_4">5</ref> . As shown in  Where "init" and "pretrain" denote without and with pretrained embeddings respectively. "+" denotes the boost value to baseline. and pretrained word embeddings are useful to our model. However, different from the result in lattice model <ref type="bibr">(Yang et al., 2018)</ref>, pretrained word embeddings contribute more than lexicon to our model. Taking the result on Ontonote for example, the contribution of pretrained embeddings can be estimated as (9.48% − 2.86%) = 6.62%, which is higher than the contribution of lexicon 2.86%. The results show that our model relies more on pretrained embeddings instead of the lexicon, which explains the excellent performance of our model in different domains.</p><formula xml:id="formula_17">的 东 北 亚 大 陆 桥 O O B-LOC M-LOC E-LOC O O O Character 新 的 东 北 亚 大 陆 桥 O O O O O O O O Lattice 新 的 东 北 亚 大 陆 桥 O O B-LOC M-LOC M-LOC M-LOC M-LOC E-LOC Shortest 新 的 东 北 亚 大 陆 桥 O O B-LOC M-LOC E-LOC O O O Longest 新 的 东 北 亚 大 陆 桥 O O B-LOC M-LOC E-LOC O O O Average 新 的 东 北 亚 大 陆 桥 O O B-LOC M-LOC E-LOC O O O Self-attention 新 的 东 北 亚 大 陆 桥 O O B-LOC M-LOC E-LOC O O O</formula><formula xml:id="formula_18">维 中 经 叔 平 B-PER M-PER E-PER B-PER M-PER E-PER Average 房 维 中 经 叔 平 B-PER M-PER E-PER B-PER M-PER E-PER Self-attention 房 维 中 经 叔 平 B-PER M-PER M-PER E-PER B-PER E-PER</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose a novel method to utilize word information in character sequence for Chinese NER. Four encoding strategies are introduced to extract fixed-sized but different information for batch training. By using WC-LSTM to extract features from the character vector and word vector, our model can effectively exploit word boundary information and mitigate the influence of word segmentation errors. Experiments on datasets in different domains show that our model is more efficient and faster than the lattice model and also outperforms other state-of-the-art models.</p><p>In the future, we plan to further improve and perfect the proposed method, such as exploring some strategies to handle OOV words. Also, the proposed methods can be further extended to other Chinese NLP tasks, such as CWS, Text Classification, and Sentiment Analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of the lattice model degenerates into a partial word-based model. Due to the shortcut path "江(River)" → "江 水(River Water)"→ "水(Water)", the model incorrectly predicts that "江(River)" and "水(Water)" belong to the same entity. Red labels(without underline) denote predicted labels, and blue labels(with underline) denote gold labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The architecture of our unidirectional model. The blue part can be seen as a standard character-based model but with a word-character LSTM(WC-LSTM), and the red part indicates the process of encoding word information into a fixed-size representation. Word information is integrated into the end character of the word. Where "&lt;PAD&gt;" denotes padding value; "Stgy" denotes a certain encoding strategy and ⊕ denotes concatenation operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>13) Where W o and b o are paramters to calculate O; T is a transition score matrix and ỹ denotes all possible tag sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Convergence curve of models. Our model can converge within the same epochs as lattice model does. "1" and "8" denotes batch size. Lattice model can only be trained with batch size=1 due to its DAG structure.</figDesc><graphic url="image-1.png" coords="7,307.28,62.80,218.27,163.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. As we can see from the figure, whether with batch size=1 or 8, our model can converge within the same epochs as lattice model does. But compared with the lattice model, our model with batch size=8 only takes about 1/7 of their training time per epoch. Besides, we can observe from Figure 3, both our model and lattice model significantly outperform the character baseline, which shows the importance of the word information again.4.4 Detailed AnalysisCase Study. Word information is very useful for Chinese NER task, since it can provide rich word boundary information. To verify that our model can better utilize the boundary information, we analyze an example from OntoNotes dataset. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train sent Dev sent Test sent</cell></row><row><cell>OntoNotes</cell><cell>15724</cell><cell>4301</cell><cell>4346</cell></row><row><cell>MSRA</cell><cell>46364</cell><cell>-</cell><cell>4365</cell></row><row><cell>Weibo NER</cell><cell>1350</cell><cell>270</cell><cell>270</cell></row><row><cell>Chinese resume</cell><cell>3821</cell><cell>463</cell><cell>477</cell></row><row><cell cols="2">4 Experiments</cell><cell></cell><cell></cell></row><row><cell cols="2">4.1 Experimental Settings</cell><cell></cell><cell></cell></row><row><cell cols="4">Dataset. We evaluate our model on four datasets,</cell></row><row><cell cols="4">including OntoNotes4 (Weischedel et al., 2011),</cell></row><row><cell cols="4">MSRA (Levow, 2006), Weibo NER (Peng and</cell></row><row><cell cols="4">Dredze, 2015) and a Chinese resume dataset</cell></row><row><cell cols="4">(Zhang and Yang, 2018). Both OntoNotes4 and</cell></row><row><cell cols="4">MSRA datasets are news in simplified Chinese.</cell></row><row><cell cols="4">Weibo NER dataset is social media data, which</cell></row><row><cell cols="4">is drawn from the Sina Weibo. Chinese resume</cell></row><row><cell cols="4">dataset consists of resumes of senior executives,</cell></row><row><cell cols="2">which is annotated by</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on MSRA</figDesc><table><row><cell>Models</cell><cell>NE</cell><cell cols="2">NM Overall</cell></row><row><cell>Peng and Dredze (2015)</cell><cell cols="2">51.96 61.05</cell><cell>56.05</cell></row><row><cell>Peng and Dredze (2016)</cell><cell cols="2">55.28 62.97</cell><cell>58.99</cell></row><row><cell>Sun and He (2017)</cell><cell cols="2">54.50 62.17</cell><cell>58.23</cell></row><row><cell>He and Sun (2017)</cell><cell cols="2">50.60 59.32</cell><cell>54.82</cell></row><row><cell>Cao et al. (2018)</cell><cell cols="2">54.34 57.35</cell><cell>58.70</cell></row><row><cell cols="3">Lattice (Zhang and Yang, 2018) 53.04 62.25</cell><cell>58.79</cell></row><row><cell>Character baseline</cell><cell cols="2">47.98 57.94</cell><cell>52.88</cell></row><row><cell>WC-LSTM + shortest</cell><cell cols="2">52.99 65.75</cell><cell>59.20</cell></row><row><cell>WC-LSTM + longest</cell><cell cols="2">52.55 67.41</cell><cell>59.84</cell></row><row><cell>WC-LSTM + average</cell><cell cols="2">53.19 64.17</cell><cell>58.67</cell></row><row><cell>WC-LSTM + self-attention</cell><cell cols="2">49.86 65.31</cell><cell>57.51</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on Weibo NER</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>are the most common methods<ref type="bibr" target="#b40">Zhang and Yang, 2018)</ref> 94.81 94.11 94.46 Character baseline 93.26 93.44 93.35 WC-LSTM + shortest 94.97 94.91 94.94 WC-LSTM + longest 95.27 95.15 95.21 WC-LSTM + average 95.09 94.97 95.03 WC-LSTM + self-attention 95.14 94.79 94.96</figDesc><table><row><cell>Models</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Lattice (</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Results on Chinese Resume</figDesc><table><row><cell></cell><cell>Time(s)/epoch</cell></row><row><cell>Character baseline (batch size=1)</cell><cell>880</cell></row><row><cell>Character baseline (batch size=8)</cell><cell>253</cell></row><row><cell>Lattice</cell><cell>2245</cell></row><row><cell>WC-LSTM (batch size=1)</cell><cell>980</cell></row><row><cell>WC-LSTM (batch size=8)</cell><cell>350</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Time per epoch of models</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Table 7, the character-based model cannot detect the existence of the entity "东北 亚(Northeast Asia)" without word information. The lattice model incorrectly recognizes "东北亚</figDesc><table><row><cell>Sentence</cell><cell>新的</cell><cell>东北亚</cell><cell>大陆桥</cell></row><row><cell>(truncated)</cell><cell>New</cell><cell>Northeast Asian</cell><cell>Continental Bridge</cell></row><row><cell></cell><cell cols="3">东北, 东北亚, 北亚, 亚大, 亚大陆, 大陆, 大陆桥, 陆桥</cell></row><row><cell>Latent words</cell><cell cols="3">Northest, Northeast Asia, North Asia, Second largest, Subcontinent,</cell></row><row><cell></cell><cell cols="3">Continent, Continental bridge, Land bridge</cell></row><row><cell>Gold labels</cell><cell>新</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: An example of that our models can mitigate</cell></row><row><cell>the influence of wrong boundary information while uti-</cell></row><row><cell>lizing word information. "Latent words" denotes all</cell></row><row><cell>words in character sequences; "Character" denotes the</cell></row><row><cell>character-based model; "Lattice" denotes lattice model</cell></row><row><cell>and the last four rows are our models with different en-</cell></row><row><cell>coding strategies.</cell></row><row><cell>大陆桥(Northeast Asian Continental Bridge)" as</cell></row><row><cell>an entity, which is caused by the wrong selection</cell></row><row><cell>of paths. Different from the lattice model, our</cell></row><row><cell>models are not disturbed by the wrong boundary</cell></row><row><cell>information and make the correct predictions.</cell></row><row><cell>Strategies Analysis. In this part, we analyze</cell></row><row><cell>the difference between strategies. The applica-</cell></row><row><cell>tion scenarios of shortest word first and longest</cell></row><row><cell>word first can be explained by Nested Name Entity</cell></row><row><cell>Recognition</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>An example of our model applied to informal text using the average strategy and self-attention strategy. "Latent words" denotes all words in character sequences.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table 9, both lexicon +2.86) 94.51(+1.16) 90.68(+2.31) 54.94(+2.06) WC-LSTM + pretrain 74.43(+9.48) 95.21(+1.86) 93.74(+5.37) 59.84(+6.96)</figDesc><table><row><cell></cell><cell>OntoNotes</cell><cell>Resume</cell><cell>MSRA</cell><cell>Weibo</cell></row><row><cell>Character baseline</cell><cell>64.95</cell><cell>93.35</cell><cell>88.37</cell><cell>52.88</cell></row><row><cell>WC-LSTM + init</cell><cell>67.81(</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Comparison F1 scores between our proposed model with and without pretrained word embeddings.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/liuwei1206/CCW-NER</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The number depends on the maximum s t i in the whole batch, and it can not be less than 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The range is − 3 dim , + 3 dim , where dim demotes the size of embedding.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The results of(Peng and Dredze, 2015, 2016)  are taken from<ref type="bibr" target="#b26">(Peng and Dredze, 2017)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">Same initialization strategy as in "Implement Details".</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work is supported by the No. BHKX-17-07 project of Hefei Innovation Research Institute, Beihang University. We would like to thank Jie Yang for his open-source toolkit "NCRF++" (Yang and Zhang, 2018). Moreover, we sincerely thank all anonymous reviewers for their valuable comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial transfer learning for chinese named entity recognition with selfattention mechanism</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings</title>
				<meeting><address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Westin Peachtree Plaza Hotel</publisher>
			<date type="published" when="2013-06-09">2013. June 9-14, 2013</date>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Long short-term memory neural networks for chinese word segmentation</title>
		<author>
			<persName><forename type="first">Xinchi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17">2015. 2015. September 17-21, 2015</date>
			<biblScope unit="page" from="1197" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with a maximum entropy approach</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Leong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chieu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Hwee Tou</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003</title>
				<meeting>the Seventh Conference on Natural Language Learning, CoNLL 2003, Held in cooperation with HLT-NAACL 2003<address><addrLine>Edmonton, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-05-31">2003. May 31 -June 1, 2003</date>
			<biblScope unit="page" from="160" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><forename type="middle">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Characterbased LSTM-CRF with radical-level features for chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Chuanhai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Understanding and Intelligent Applications</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Named entity recognition using support vector machine: A language independent approach</title>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Electrical and Electronics Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="155" to="170" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chinese word segmentation and named entity recognition based on conditional random fields models</title>
		<author>
			<persName><forename type="first">Yuanyong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhua</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing</title>
				<meeting>the Fifth SIGHAN Workshop on Chinese Language Processing</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Named entity recognition with parallel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Zukov Gregoric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Coope</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning with word embeddings improves biomedical named entity recognition</title>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Habibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><forename type="middle">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luis Wiegandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Leser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="37" to="38" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified model for cross-domain and semi-supervised named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="3216" to="3222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A neural layered model for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Meizhi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1446" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Chinese Language Processing</title>
				<meeting>the Fifth Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07-22">2006. July 22-23, 2006</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
	<note>SIGHAN@COLING/ACL 2006</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural adaptation layers for cross-domain named entity recognition</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4. 2018</date>
			<biblScope unit="page" from="2012" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cícero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep multi-task learning with shared memory for text classification</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-01">2016. 2016. November 1-4, 2016</date>
			<biblScope unit="page" from="118" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial multi-task learning for text classification</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-07">2016. August 7-12, 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1105" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Named entity recognition for question answering</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Mollá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menno</forename><surname>Van Zaanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Australasian Language Technology Workshop (ALTW2006)</title>
				<meeting>the 2006 Australasian Language Technology Workshop (ALTW2006)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="51" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-17">2015. 2015. September 17-21, 2015</date>
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving named entity recognition for chinese social media with word segmentation representation learning</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Supplementary results for named entity recognition on chinese social media with an updated dataset</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multilanguage named-entity recognition system based on hmm</title>
		<author>
			<persName><forename type="first">Kuniko</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition</title>
				<meeting>the ACL 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep exhaustive model for nested named entity recognition</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Golam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sohrab</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4. 2018</date>
			<biblScope unit="page" from="2843" to="2849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">F-score driven max margin neural network for named entity recognition in chinese social media</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangfeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
				<meeting>the 15th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="713" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transliteration of proper names in cross-lingual information retrieval</title>
		<author>
			<persName><forename type="first">Paola</forename><surname>Virga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Effective bilingual constraints for semi-supervised learning of named entity recognizers</title>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Seventh AAAI Conference on Artificial Intelligence<address><addrLine>Bellevue, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-07-14">2013. July 14-18, 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Taylor Nianwen Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<title level="m">Ontonotes release 4.0 ldc2011t03. dvd</title>
				<editor>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohammed</forename><surname>El-Bachouti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Robert</forename><surname>Belvin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ann</forename><surname>Houston</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>Linguistic Data Consortium</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on recent advances in named entity recognition from deep learning models</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
				<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2145" to="2158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combining discrete and neural features for sequence labeling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing -17th International Conference</title>
				<meeting><address><addrLine>Konya, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-04-03">2016. 2016. April 3-9, 2016</date>
			<biblScope unit="page" from="140" to="154" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers, Part I</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ncrf++: An opensource neural sequence labeling toolkit</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
				<meeting>ACL 2018, System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="74" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural word segmentation with rich pretraining</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-30">2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Subword encoding in lattice LSTM for chinese word segmentation</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuailong</forename><surname>Liang</surname></persName>
		</author>
		<idno>abs/1810.12594</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural networks incorporating dictionaries for chinese word segmentation</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
				<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5682" to="5689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Word segmentation and named entity recognition for SIGHAN bakeoff3</title>
		<author>
			<persName><forename type="first">Suxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Chinese Language Processing</title>
				<meeting>the Fifth Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07-22">2006. July 22-23, 2006</date>
			<biblScope unit="page" from="158" to="161" />
		</imprint>
	</monogr>
	<note>SIGHAN@COLING/ACL 2006</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Chinese ner using lattice lstm</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chinese named entity recognition via joint identification and categorization</title>
		<author>
			<persName><forename type="first">Junsheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiguang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese journal of electronics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="230" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
