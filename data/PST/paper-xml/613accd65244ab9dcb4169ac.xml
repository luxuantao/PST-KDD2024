<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PPT: Pre-trained Prompt Tuning for Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-14">14 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The CoAI group</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science and Technology Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The THUNLP group</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science and Technology Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">The THUNLP group</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science and Technology Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Beijing Academy of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">BAAI</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
							<email>aihuang@tsinghua.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department">The CoAI group</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute for Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">State Key Lab of Intelligent Technology and Systems</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Beijing National Research Center for Information Science and Technology Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Beijing Academy of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit2">BAAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PPT: Pre-trained Prompt Tuning for Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-14">14 Sep 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2109.04332v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Prompts for pre-trained language models (PLMs) have shown remarkable performance by bridging the gap between pre-training tasks and various downstream tasks. Among these methods, prompt tuning, which freezes PLMs and only tunes soft prompts, provides an efficient and effective solution for adapting largescale PLMs to downstream tasks. However, prompt tuning is yet to be fully explored. In our pilot experiments, we find that prompt tuning performs comparably with conventional full-model fine-tuning when downstream data are sufficient, whereas it performs much worse under few-shot learning settings, which may hinder the application of prompt tuning in practice. We attribute this low performance to the manner of initializing soft prompts. Therefore, in this work, we propose to pretrain prompts by adding soft prompts into the pre-training stage to obtain a better initialization. We name this Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT, we formulate similar classification tasks into a unified task form and pretrain soft prompts for this unified task. Extensive experiments show that tuning pre-trained prompts for downstream tasks can reach or even outperform full-model fine-tuning under both full-data and few-shot settings. Our approach is effective and efficient for using largescale PLMs in practice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-tuning pre-trained language models (PLMs) <ref type="bibr" target="#b4">(Devlin et al., 2019;</ref><ref type="bibr" target="#b20">Radford et al., 2019;</ref><ref type="bibr" target="#b21">Raffel et al., 2020)</ref> has made great progress in recent years. By fine-tuning the entire parameters of PLMs, the versatile knowledge acquired from large-scale unlabeled corpora can be adapted to handle various NLP tasks and outperform the approach of learning models from scratch <ref type="bibr" target="#b7">(Han et al., 2021a)</ref>. For simplicity, we name this fullmodel tuning as "FT". As shown in Figure <ref type="figure">1</ref> (b) and (c), there are two mainstream FT approaches. The first one is task-oriented fine-tuning, where a task-specific head is added on top of PLMs, and the entire model is then fine-tuned by optimizing task-specific learning objectives on task-specific training data.</p><p>The second one is prompt-oriented finetuning <ref type="bibr" target="#b22">(Schick and Schütze, 2021a)</ref>, which is inspired by the recent works utilizing language prompts to stimulate the knowledge of PLMs <ref type="bibr" target="#b17">(Petroni et al., 2019;</ref><ref type="bibr" target="#b1">Brown et al., 2020)</ref>. In prompt-oriented fine-tuning, data samples are converted to linearized sequences containing prompt tokens, and all downstream tasks are formalized as language modeling problems. As shown in <ref type="bibr">Figure 1 (c)</ref>, by adding the prompt "It was X ." to a sentence, we can determine whether the sentence is positive or negative with PLMs predicting "great" or "terrible" at the mask position. As shown in Figure <ref type="figure">1</ref>, compared to task-oriented fine-tuning, prompt-oriented fine-tuning is more similar to pretraining in terms of objectives (masked language modeling), thereby helping to better use knowledge in PLMs and often obtaining better performance.</p><p>Although the above-mentioned FT methods have shown promising results, with the rapid growth of model scale, fine-tuning a full large model for each downstream task becomes more and more expensive. To address this challenge, <ref type="bibr" target="#b12">Lester et al. (2021)</ref> propose prompt tuning (PT) to adapt large PLMs to downstream tasks cheaply, as shown in Figure <ref type="figure">1 (d)</ref>. Specifically, PT uses soft prompts composed of continuous embeddings instead of hard prompts (discrete language phrases). These continuous prompt embeddings are generally randomly initialized and learned end-to-end. To avoid storing the entire model for each downstream task, PT freezes all parameters of PLMs and merely tunes soft prompts, without adding any intermediate layers and task-specific components.</p><p>PT has two promising advantages: first, soft prompts can be learned end-to-end in comparison to hard prompts. Second, PT is an efficient and effective paradigm for the practical use of largescale PLMs. However, as shown in Figure <ref type="figure">2</ref>(b), we find that PT performs much worse than FT under few-shot settings, which may hinder the application of PT in various low-resource scenarios.</p><p>Hence, in this paper, we extensively explore how to use PLMs for few-shot learning in an efficient and effective manner through PT. More specifically, we conduct pilot experiments to empirically analyze the effectiveness of PT on large-scale PLMs for few-shot learning in Section 2, which is ignored by most existing works. Our discoveries are as follows: (1) the choice of verbalizer has a large impact on the performance; (2) simply initializing soft prompts with concrete word embeddings can not improve the performance, yet (3) combining soft and hard prompts is helpful; and (4) all these methods cannot handle few-shot prompt tuning problems well. The above observations reveal that finding suitable prompts for large-scale PLMs is not trivial, and carefully designed initialization of soft prompt tokens is crucial.</p><p>To help the model to find suitable prompts, we pre-train these tokens using self-supervised tasks on large-scale unlabeled corpora. To ensure the generalization of pre-trained prompts, we group typical classification tasks into three formats: sentence-pair classification, multiple-choice classification, and single-text classification, each format corresponding to one self-supervised pre-training task. In addition, we find multiple-choice classification is more general among these formats and we can unify all downstream classification tasks to this format. We name this Pre-trained Prompt Tuning (PPT) framework "PPT". We evaluate PPT on several datasets using three 11B PLMs: T5-XXL <ref type="bibr" target="#b21">(Raffel et al., 2020)</ref>, mT5-XXL <ref type="bibr">(Xue et al., 2021)</ref> and CPM-2 <ref type="bibr" target="#b29">(Zhang et al., 2021b)</ref>. Experiments show that PPT can not only improve few-shot PT by a large margin, reaching or even outperforming FT methods, but also reduce the variance of few-shot learning. Besides the effectiveness, PPT also retains the parameter efficiency of existing PT methods, which is valuable for future applications on large-scale PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pilot Experiments</head><p>In this section, we present several pilot experiments of PT under few-shot settings. We empirically ana-  <ref type="bibr" target="#b16">(Perez et al., 2021)</ref>. We follow <ref type="bibr" target="#b29">Zhang et al. (2021a)</ref> and <ref type="bibr" target="#b5">Gao et al. (2021)</ref>  Hybrid Prompt Tuning In hybrid prompt tuning, both soft and hard prompt tokens are used <ref type="bibr" target="#b14">(Liu et al., 2021;</ref><ref type="bibr" target="#b8">Han et al., 2021b)</ref>. However, previous works train soft prompts jointly with the entire model. In the setting of PT where only prompt tokens are tunable, the effectiveness of using hybrid prompts is under-explored. In Table <ref type="table">1</ref>, we show the results of combining soft prompts P with three manually designed hard prompts and two auto-generated hard prompts <ref type="bibr" target="#b5">(Gao et al., 2021)</ref> on the sentiment classification task <ref type="bibr" target="#b25">(Socher et al., 2013)</ref>. We can see that hard prompts improve PT, 1 Using 100 soft prompt tokens achieves the best performance in <ref type="bibr" target="#b12">Lester et al. (2021</ref> but still underperform FT. Furthermore, different hard templates affect the performance remarkably, therefore much human labor for prompt design and selection is needed.</p><p>Verbalizer Selection Verbalizer maps taskspecific labels to concrete tokens, or instance, in Figure <ref type="figure">1</ref> (c) and (d), the verbalizer maps "great" to the label "Positive". Verbalizer selection is yet to be explored. From Table <ref type="table">1</ref> we can see that different choices of verbalizers influence the performance remarkably. In general, common words that explain the meaning of corresponding labels work well. This also guides our verbalizer selection for PPT in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Word Initialization</head><p>In real word initialization, we use the embeddings of concrete words to initialize the soft prompt tokens and test four initialization strategies. The effectiveness of this approach has been verified on small PLMs (fewer than 3B parameters) in previous works <ref type="bibr" target="#b12">(Lester et al., 2021;</ref><ref type="bibr" target="#b13">Li and Liang, 2021)</ref>. However, from the experiments on SST-2 <ref type="bibr" target="#b25">(Socher et al., 2013)</ref> and BoolQ <ref type="bibr" target="#b2">(Clark et al., 2019)</ref> (Table <ref type="table" target="#tab_2">2</ref>), we find that for the model with 11B parameters, real word initialization has little or even negative impact on the performance under few-shot settings. This suggests that observations on small models can not be directly adapted to large models and finding a good initialization for soft prompt tokens is yet to be explored.</p><p>To summarize, although the above prompt enhancement strategies cannot help PT achieve comparable results with FT under few-shot settings, the pilot experiments demonstrate that they are the key factors to the success of PT. In the following sections, we describe our PPT framework and show in experiments that PPT not only provides a good prompt initialization but also takes advantage of the good verbalizer, and is complementary to hybrid prompts.</p><p>3 Pre-trained Prompt Tuning (PPT)</p><p>In this section, we describe the whole framework of PPT, including how to pre-train prompts and use these pre-trained prompts for specific tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Following the approach of T5 <ref type="bibr" target="#b21">(Raffel et al., 2020)</ref> and PT <ref type="bibr" target="#b12">(Lester et al., 2021)</ref>, we solve all downstream tasks in a text-to-text format. As shown in Figure <ref type="figure">1 (d)</ref>, to reduce the objective gap between pre-training and downstream tasks, promptoriented fine-tuning converts downstream tasks into some cloze-style objectives. Taking classification for example, given an input sentence x ∈ V * and its label y ∈ Y, a pattern mapping f : V * → V * is first applied to convert x into a new token sequence f (x), where V is the vocabulary of PLMs. f (x) not only adds some prompt tokens as hints, but also preserves at least one masking token X to let PLMs predict tokens at the masked positions. Then, a verbalizer v : Y → V * is used to map y to a sequence of label tokens v(y). With f (•) and v(•), a classification task can be represented by a pattern-verbalizer pair (f, v):</p><formula xml:id="formula_0">arg max θ x log p y|x; θ = arg max θ x log p X = v(y)|f (x); θ ,<label>(1)</label></formula><p>where θ indicates all tunable parameters, especially the parameters of PLMs. For convenience, we use "PVP" to denote this pattern-verbalizer pair <ref type="bibr" target="#b22">(Schick and Schütze, 2021a)</ref>.</p><p>In PT <ref type="bibr" target="#b12">(Lester et al., 2021)</ref>, a set of soft prompt tokens P are concatenated to the beginning of the sequence and the model input becomes [P ; f (x)], where [•; •] is the concatenation operation. By tuning P alone with other parameters fixed, Eq. ( <ref type="formula" target="#formula_0">1</ref>) is replaced by</p><formula xml:id="formula_1">arg max P x log p X = v(y) | [P ; f (x)]; P . (2)</formula><p>Owing to the power of large-scale PLMs, Eq. ( <ref type="formula">2</ref>) is verified to be comparable to these FT methods under full-data settings. However, we find that learning effective soft prompts is not easy, which may result in low performance under various fewshot settings. The parameter initialization usually has a large impact on the difficulty of learning models. Besides randomly initializing p, some works sample word embeddings from the vocabulary of PLMs V for initialization. However, our pilot experiments have shown that existing initialization strategies and their simple variants have little or even negative impact on the performance of largescale PLMs. We refer more details of these pilot experiments to Section 4.</p><p>Recently, pre-training has been proven to be an effective method to find a good model initialization. Inspired by this, we propose to pre-train soft prompts. We notice that some groups of downstream tasks are related to certain self-supervised tasks built on unlabeled pre-training corpora. For instance, some tasks in the form of sentence-pair classification, such as natural language inference and sentence similarity, are similar to the next sentence prediction (NSP) <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> task used in the pre-training stage. As shown in Figure <ref type="figure">3</ref>, these tasks all take two sentences as input and compare their semantic meanings. Therefore, soft prompts pre-trained by NSP can be a good initialization for these sentence-pair tasks.</p><p>Formally, suppose we can divide downstream tasks into m groups {T 1 , T 2 , ..., T m }, where T i is the set containing n i downstream tasks:</p><formula xml:id="formula_2">{PVP 1 i , PVP 2 i , ..., PVP n i i }, where PVP k i = (f k i , v k i ).</formula><p>For each group, we design one corresponding pre-training task PVP pre i = (f pre i , v pre i ). After pre-training soft prompts on these pretraining tasks with all model parameters fixed, we get m pre-trained prompts {P 1 , P 2 , ..., P m }. After pre-training, for each task PVP k i in T i , we continue to optimize Eq. ( <ref type="formula">2</ref>) by using P i as the initialization of soft prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Designing Pattern-Verbalizer Pairs for Pre-training</head><p>In this section, we take several typical classification tasks as an example to describe the design of pattern-verbalizer pairs PVP pre i for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Sentence-Pair Classification</head><p>Sentence-pair classification tasks such as natural language inference and sentence similarity take two sentences x = (s 1 , s 2 ) as the input. To design a PVP for these tasks, we extend the next sentence prediction in <ref type="bibr" target="#b4">Devlin et al. (2019)</ref> to a 3-class classification with labels Y = {0, 1, 2} as the pretraining task. These labels in Y can respectively indicate that the semantic relation between two sentences is coherent (with label 2), similar (1) and irrelevant (0). To construct signal from unlabeled documents, we set the two sentences next to each other as label 2, those from the same document but not true next sentence as 1, and those from different document as 0. We consider the label set |Y| ≤ 3 since this covers most sentence pair tasks.</p><formula xml:id="formula_3">PVP pre i = (f pre i , v pre i ) is given as f pre i (x) = "s1 X .s2", v pre i (Y) = [no, maybe, yes].</formula><p>(3)</p><p>Designing PVP k i = (f k i , v k i ) according to PVP pre i is simple. s 1 and s 2 can be replaced by the input sentence pair. If a task outputs two labels, then we take v k i (Y) = [no, yes]. If a task outputs three labels, we set v k i = v pre i . If a task requires to measure the similarity between two sentences, the probability over {no, yes} can serve for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multiple-Choice Classification</head><p>Many tasks can be formulated as multiple-choice classification, which takes a query and several answer candidates as the input. We design a next sentence selection task to pre-train the prompt. Given a sentence as the query s q , the model is trained to select the adjacent sentence from six candidates, denoted as s 1 ∼ s 6 and thus the label set is Y = {1, 2, 3, 4, 5, 6}. These candidates consist of the right answer, one sentence from the same document but are not adjacent to the query, and four sentences from other documents. For x = (s q , s 1 , s 2 , • • • , s 6 ), (f pre i , v pre i ) is given as</p><formula xml:id="formula_4">f pre i (x) = "sq? A.s1 • • • F.s6.Answer is X .", v pre i (Y) = [A, B, C, D, E, F].<label>(4)</label></formula><p>Most multiple-choice tasks can use {f pre i , v pre i } directly as their PVPs. For tasks like reading comprehension, the input may contain a passage and a question. We concatenate them to form a query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Single-Sentence Classification</head><p>For single-sentence classification, we create pseudo labels for prompt pre-training. Taking sentiment classification as an example, we use another small model to annotate sentiment labels for the sentences from the pre-training corpus and filter those with low classification probability. In practice, we use a RoBERTa BASE <ref type="bibr" target="#b15">(Liu et al., 2019</ref>) model finetuned on a 5-class sentiment classification dataset other than the few-shot datasets we test on. Then with a sentence s from the corpus, we have the input x = (s) and the label set Y = {1, 2, 3, 4, 5}. (f pre i , v pre i ) is given as</p><formula xml:id="formula_5">f pre i (x) = "s. X .", v pre i (Y) = [terrible, bad, maybe, good, great].</formula><p>(5)</p><p>For sentiment classification tasks with 5 labels, we can use PVP k i = PVP pre i . For those tasks with fewer than 5 labels, we choose a subset from v pre i (Y) as labels.</p><p>Although the above method improves the model performance, we have to point out that it is still limited to generalize to other single-text classifications in different domains and with different numbers of labels. Therefore, the method described in the following section is proposed to solve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unifying Task Formats</head><p>The above-mentioned PVPs for pre-training can be unified to a single format: multiple-choice classification. Specifically, for sentence-pair classification, the query is the concatenation of the two sentences and there are three options: no, maybe, and yes. For single-sentence classification, the query is the input sentence and the options are the concrete labels. Note that in this way, the pre-trained PVPs can be used in single text classification tasks from arbitrary domains and with several labels.</p><p>Constructing a unified PVP is similar to the idea of MultiQA <ref type="bibr" target="#b26">(Talmor and Berant, 2019)</ref> and Uni-fiedQA <ref type="bibr" target="#b11">(Khashabi et al., 2020)</ref>. Recently, <ref type="bibr" target="#b30">Zhong et al. (2021a)</ref> use some hard prompts to unify several tasks as a meta question answering task. They tune the entire model with this meta task on a collection of QA datasets and then transfer to other classification tasks in low-resource settings. However, our PPT focuses on only tuning soft prompts  with the main body of PLMs fixed and our pretraining is conducted on fully unsupervised data, rather than the collection of supervised datasets.</p><p>Since different tasks may have different candidate numbers and lengths, we construct pretraining samples with option numbers varying from 2 to 16<ref type="foot" target="#foot_0">2</ref> and option lengths from 50 to 20. We use the PVP in Section 3.2.2 for pre-training, and then apply pre-trained soft prompts to cover sentencepair classification, multiple-choice classification, and single-sentence classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>We conduct experiments on both Chinese and English tasks (see Table <ref type="table" target="#tab_4">3</ref>). As described in Section 2, for tasks with less than 5 labels, we construct the training and validation set with 32 samples from the original training data and ensure the number of labels is balanced. For tasks with more than 5 labels like TNews and YahooAnswer, it is hard to compose a dataset with balanced samples across labels. Therefore, we randomly select 8 samples for each label.</p><p>For English datasets, we use T5-XXL with 11B parameters as our base model to do PT since previous work <ref type="bibr" target="#b12">(Lester et al., 2021;</ref><ref type="bibr" target="#b29">Zhang et al., 2021b)</ref> have shown that, T5-XXL is comparable with FT in the full-data setting. We also do FT experiments on various sizes of T5 to verify that T5-XXL performs better than other sizes in few-shot scenarios and improving prompt tuning based on T5-XXL is meaningful. For Chinese datasets, we do PT based on CPM-2. Since CPM-2 does not provide models with other sizes, we compare it with mT5 <ref type="bibr">(Xue et al., 2021)</ref> of various sizes.</p><p>Consistently, we use 100 soft tokens for PT. As a result, the tunable parameters is only 100×4096 = 4.1 × 10 5 = 410K. Compared with the 11B (1.1 × 10 10 ) parameters of FT, PT only needs to store 30000 times smaller parameters for each task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>The main results of English and Chinese datasets are shown in Table <ref type="table" target="#tab_5">4</ref>. In the block FT, we present the full-model fine-tuning results of the T5 model of various sizes. In the block PT, we show the results of PPT and other baselines. The first baseline is Vanilla PT, where the soft tokens are randomly initialized from a normal distribution. The second is the hybrid strategy in Section 2. We also consider LM Adaption used in <ref type="bibr" target="#b12">Lester et al. (2021)</ref> in which the T5 model is further pre-trained for 10K steps with language modeling to reduce the gap between the pre-training and the fine-tuning. We also test two variants of PPT: Hybrid PPT, in which carefully designed hard prompts are combined with pre-trained soft prompt, and Unified PPT, in which all tasks are unified in the multiple-choice format.</p><p>Effectiveness From the Table <ref type="table" target="#tab_5">4</ref> we have four observations. First, larger models achieve better overall performance, which means large-scale models still help under the few-shot setting. Therefore, considering the intractable parameter scaler, we study PT on the large-scale pre-trained model. Note that for Chinese experiments, CPM-2 and mT5-XXL share the same parameter scale. Since CPM-2 outperforms mT5-XXL across all tasks, we use CPM-2 as the base model.</p><p>Second, PPT outperforms Vanilla PT and LM Adaption across most datasets significantly. Although PPT is worse than Hybrid PT on BoolQ, simply combining PPT and hard template (Hybrid PPT) outperforms all baselines. This means pretraining prompts and using hybrid prompts are complementary. Similar phenomenons are observed on other datasets like RACE-m, LCQMC, and C 3 , where adding hard templates to PPT continues to improve results.</p><p>Third, PPT outperforms FT with 11B models on all Chinese datasets and most English datasets. This indicates that there still remains a gap between masked language modeling and downstream tasks. Pre-training soft prompt bridges this gap to some extend. Based on this observation, an intuitive extension of our method is to further pre-train the entire parameters using each PVP i pre and fine-tune the model to the corresponding downstream tasks. However, since we focus on prompt-tuning in this paper, we leave this idea as future work.</p><p>Fourth, PPT results in lower variances on most of the datasets. Few-shot learning is notorious for its instability, which becomes very obvious in Vanilla PT. For some datasets like SST-2, the variance reaches 15.5 which means the model does not perform better than random guesses under some random seeds. Combining with hard prompt or further pre-training with language modeling can alleviate this problem to some extent. But on some datasets like CCPM, Hybrid PT increases the variance and LM Adaption does not guarantee the average performance. With the help of pre-training, the variance remains at a low level across all datasets.</p><p>Unified PPT Unifying all formats to multiplechoice format is another variant of PPT. In Table <ref type="table" target="#tab_5">4</ref>, we can see that Unified PPT reaches comparable performance as PPT and Hybrid PPT, still outperforming soft-prompt tuning baselines. However, all the datasets we have considered so far have fewer than 5 classification labels. For tasks with more labels, especially single-text classification in which pseudo label pre-training is also not appropriate for cross-domain adaption, Unified PPT can be a good alternative. In Table <ref type="table">5</ref>, we test Unified PPT on datasets with more than 5 labels. For PT and FT, we use a verbalizer to map the intuitively selected words to the classification labels. PT (MC) means we solve the task in a multiple-choice format without pre-training the prompt. We do not use PPT for single-sentence classification in Section 3.2.3 because it is hard to find other suitable datasets to train the pseudo label annotator. However, we can  see that Unified PPT still achieves the best performance, even exceeding FT by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sample Efficiency</head><p>We discuss how the performance of FT, PT, and PPT varies when the number of training samples increases. In Figure <ref type="figure" target="#fig_2">4</ref>, we show the trend of these methods on the RACE-m and CB datasets. We can see that for 32 to 128 samples, PPT is consistently better than Vanilla PT, and the performances of the three methods gradually converge when the number grows to 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>PLMs and Task-oriented Fine-tuning Recently, various powerful PLMs have been proposed, such as GPT <ref type="bibr" target="#b19">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> and T5 <ref type="bibr" target="#b21">(Raffel et al., 2020)</ref>. To adapt these PLMs to downstream NLP tasks, task-oriented fine-tuning has been proposed, where researchers use PLMs as the backbone and add some task-specific heads to optimize task-specific objectives. Then, all param-eters of both PLMs and additional heads are tuned using task-specific data. Results have shown that task-oriented fine-tuning can outperform models trained from scratch on a series of NLP tasks.</p><p>Prompt-oriented Fine-tuning Most existing PLMs are pre-trained with the objectives of language modeling, yet the objectives of downstream tasks are quite different. To overcome the objective gap between pre-training and downstream tasks, prompt-oriented fine-tuning has been introduced. In prompt-oriented fine-tuning, downstream tasks are also formalized as some objectives of language modeling by leveraging language prompts, and the results of language modeling can correspond to the solutions of downstream tasks.</p><p>Knowledge probing <ref type="bibr" target="#b17">(Petroni et al., 2019;</ref><ref type="bibr" target="#b27">Trinh and Le, 2018;</ref><ref type="bibr" target="#b3">Davison et al., 2019)</ref> is the seminal work that stimulates the development of prompts, in which language triggers are used to induce PLMs to generate relational facts. These pioneering works demonstrate that language prompts can effectively stimulate the knowledge from PLMs. Encouraged by this, manually designing hard prompts which consist of discrete words is first used for prompt-oriented fine-tuning <ref type="bibr">Schick and Schütze (2021a,b)</ref>; <ref type="bibr" target="#b1">Brown et al. (2020)</ref>. Considering manually designing prompts is both time-consuming and difficult to find the best choice, later works <ref type="bibr" target="#b5">(Gao et al., 2021;</ref><ref type="bibr" target="#b10">Jiang et al., 2020;</ref><ref type="bibr" target="#b24">Shin et al., 2020)</ref> proposed to generate prompts automatically. However, these works still restrict auto-generated prompts to discrete spaces which are usually sub-optimal.</p><p>To overcome the shortcomings of discrete spaces, Li and Liang (2021); <ref type="bibr" target="#b14">Liu et al. (2021)</ref>; <ref type="bibr" target="#b8">Han et al. (2021b)</ref>; <ref type="bibr" target="#b6">Hambardzumyan et al. (2021)</ref>; <ref type="bibr" target="#b31">Zhong et al. (2021b)</ref> explore to combine hard prompts and soft prompts. Different from hard prompts using concrete and discrete tokens, soft prompts are composed of several continuous learnable embeddings, and these embeddings are randomly initialized. To step forward, some works <ref type="bibr" target="#b13">(Li and Liang, 2021;</ref><ref type="bibr" target="#b18">Qin and Eisner, 2021;</ref><ref type="bibr" target="#b12">Lester et al., 2021)</ref> propose to only tune soft prompts and fix the entire PLM parameters. When models are large enough, this method can be comparable to full-model tuning.</p><p>Few-shot Learning with PLMs Since long-tail distribution is common in real-world applications, few-shot learning is quite useful for the stability and effectiveness of PLMs, thereby attracts much attention recently. Apart from GPT-3 <ref type="bibr" target="#b1">(Brown et al., 2020)</ref> and PET <ref type="bibr" target="#b22">(Schick and Schütze, 2021a)</ref> which have demonstrated the superiority of PLMs in fewshot scenarios, some later works <ref type="bibr" target="#b16">Perez et al. (2021)</ref>; <ref type="bibr" target="#b0">Bragg et al. (2021)</ref> also discuss reasonable few-shot settings by restricting the size of validation set and proposing a unified framework to evaluate few-shot performance. There is also work <ref type="bibr" target="#b9">(IV et al., 2021)</ref> pointing out the low performance of PT for fewshot learning. But they mostly conduct experiments on PLMs with less than 400M parameters. In this paper, we study few-shot learning on large-scale PLMs (around 11B parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present PPT, a framework that improves prompt tuning for few-shot learning. We propose to firstly unify downstream tasks to several formats. Then, we design self-supervised pretraining tasks for each format and pre-train prompts on these tasks. Finally, we do prompt tuning on downstream tasks based on the initialization of the corresponding pre-trained prompts. Extensive experiments show that our method significantly outperforms other prompt tuning baselines, performing comparable or even better than full-model tuning.</p><p>There are two important directions for future work: (1) Designing unified task formats and the corresponding pre-training objectives for other kinds of tasks such as language generation and relation extraction. (2) Beyond the soft prompt, whether unified task pre-training helps the pretrained language models itself.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Figure 1: Paradigms of pre-training (masked language modeling), full-model tuning (task-oriented fine-tuning and prompt-oriented fine-tuning), and prompt tuning. The verbalizer is a function to map the concrete words to the classification labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>…Iron</head><label></label><figDesc>Figure 3: An example of PPT used in sentence pair tasks. P denotes soft prompt. X means the mask of typical encoder-decoder model like T5 and CPM-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison between full-model fine-tuning (FT), vanilla prompt tuning (Vanilla PT), and pretrained prompt tuning (PPT) when different numbers training samples are available. For the small number of training samples, PPT is consistently the best. When the number grows, the performance of these methods becomes closer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>ensure that |D train | = |D dev | to simulate the few-shot learning setting</head><label></label><figDesc></figDesc><table><row><cell>SST-2</cell><cell></cell><cell></cell></row><row><cell>Hard Prompt</cell><cell cols="2">Verbalizer Accuracy</cell></row><row><cell>None</cell><cell>good/bad</cell><cell>70.515.5</cell></row><row><cell>Man #1: P s. It was X .</cell><cell>good/bad</cell><cell>87.66.6</cell></row><row><cell>Man #2: P Just X ! s</cell><cell>good/bad</cell><cell>86.08.1</cell></row><row><cell cols="2">Man #3: P s. All in all, it was X . good/bad</cell><cell>83.48.3</cell></row><row><cell>Gen #1: P .s. a X .</cell><cell>good/bad</cell><cell>81.613.8</cell></row><row><cell>Gen #2: P s. A X one.</cell><cell>good/bad</cell><cell>81.22.2</cell></row><row><cell>Man #1: P s. It was X .</cell><cell cols="2">great/terrible 86.97.9</cell></row><row><cell>Man #1: P s. It was X .</cell><cell>dog/cat</cell><cell>60.07.6</cell></row><row><cell>Man #1: P s. It was X .</cell><cell>bad/good</cell><cell>76.311.7</cell></row><row><cell>Full-Model Tuning</cell><cell>good/bad</cell><cell>91.40.8</cell></row><row><cell cols="3">Table 1: The impact of hard prompt and verbalizer</cell></row><row><cell cols="3">when doing PT for few-shot learning (32 samples). P</cell></row><row><cell cols="3">represents soft prompt tokens. s denotes the input sen-</cell></row><row><cell cols="3">tence. "Man" means manually designed hard prompts</cell></row><row><cell cols="3">and "Gen" means auto-generated hard prompts. The</cell></row><row><cell cols="3">choice of hard prompt and verbalizer has a significant</cell></row><row><cell>influence on model performance.</cell><cell></cell><cell></cell></row><row><cell cols="3">lyze the effectiveness of three strategies including</cell></row><row><cell cols="3">hybrid prompt tuning, verbalizer selection, and real</cell></row><row><cell cols="3">word initialization. We follow Lester et al. (2021)</cell></row><row><cell cols="3">to test PT with T5-XXL (11B parameters) and use</cell></row><row><cell cols="2">100 tunable soft-prompt tokens 1 .</cell><cell></cell></row><row><cell cols="3">Following Schick and Schütze (2021a) and</cell></row><row><cell cols="3">Schick and Schütze (2021b), we randomly select 32</cell></row><row><cell cols="3">samples to construct the training set D train from the</cell></row><row><cell cols="3">original training data and keep the samples across</cell></row><row><cell cols="3">labels balanced. To tune the hyper-parameters, we</cell></row><row><cell cols="3">compose a validation set D dev from the original</cell></row><row><cell>training data and</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>to use the original validation set as the test set D test , which means |D test | |D train | = |D dev |.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>). Few-shot learning performance with different strategies for choosing concrete words for prompt initialization in PT. "Label Init": use the embeddings of the label words. "Vocab Sampling": randomly sample words from the entire vocabulary. "Top-1000 Sampling": randomly sample words from the most frequent 1000 words in the pre-training corpus. "Task-Related": randomly sample words from the downstream data. We use the classification accuracy (%) of SST-2 and BoolQ for evaluation.</figDesc><table><row><cell></cell><cell>SST-2</cell><cell>BoolQ</cell></row><row><cell>Random Init.</cell><cell cols="2">70.515.5 61.05.3</cell></row><row><cell>Label Init.</cell><cell>58.92.7</cell><cell>63.00.4</cell></row><row><cell>Vocab Sampling</cell><cell>57.04.0</cell><cell>58.44.9</cell></row><row><cell>Top-1000 Sampling</cell><cell>57.94.2</cell><cell>57.73.9</cell></row><row><cell cols="2">Task-Related Sampling 58.53.8</cell><cell>58.24.0</cell></row><row><cell>Full-Model Tuning</cell><cell cols="2">91.40.8 80.82.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The dataset we evaluated in this work. The "Format" column means the pre-training format of each dataset. SSC stands for single-sentence classification, MCC for multiple-choice classification, and SPC for sentence-pair classification. n class means the number of labels for each task.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Classification results. The experiments are conducted with 32 training samples and 32 validation samples on each dataset. FT means full-model tuning, where the entire model (with about 11B parameters) should be tuned on each dataset. PT means prompt tuning, where only 410K parameters are trained. We report the mean and the standard deviation over 5 random seeds. The score marked as bold means the best performance among all the methods. The score marked with an underline means the best one among prompt tuning (PT) methods.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">English Tasks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell>Method</cell><cell>SST-2 Acc.</cell><cell>SST-5 Acc.</cell><cell cols="2">RACE-m RACE-h Acc. Acc.</cell><cell>BoolQ Acc.</cell><cell>RTE Acc.</cell><cell>CB F1</cell></row><row><cell></cell><cell>T5-Small</cell><cell>-</cell><cell>72.83.1</cell><cell>31.10.4</cell><cell>26.40.6</cell><cell>26.30.5</cell><cell>59.20.6</cell><cell>54.01.7</cell><cell>70.14.6</cell></row><row><cell>FT</cell><cell>T5-Base</cell><cell>-</cell><cell>74.62.7</cell><cell>28.81.8</cell><cell>27.20.5</cell><cell>26.70.2</cell><cell>61.92.1</cell><cell>56.12.3</cell><cell>70.42.6</cell></row><row><cell>(11B)</cell><cell>T5-Large</cell><cell>-</cell><cell>89.12.2</cell><cell>42.41.2</cell><cell>48.21.6</cell><cell>43.21.7</cell><cell>74.60.9</cell><cell>64.43.4</cell><cell>82.32.2</cell></row><row><cell></cell><cell>T5-XL</cell><cell>-</cell><cell>89.63.2</cell><cell>38.45.1</cell><cell>55.02.8</cell><cell>50.92.6</cell><cell>77.22.1</cell><cell>62.36.8</cell><cell>81.99.0</cell></row><row><cell></cell><cell>T5-XXL</cell><cell>-</cell><cell>91.40.8</cell><cell>40.62.0</cell><cell>62.93.9</cell><cell>54.83.0</cell><cell>80.82.4</cell><cell>64.12.0</cell><cell>86.55.3</cell></row><row><cell></cell><cell></cell><cell>Vanilla PT</cell><cell>70.515.5</cell><cell>32.38.3</cell><cell>34.78.2</cell><cell>31.63.5</cell><cell>61.05.3</cell><cell>53.53.5</cell><cell>50.74.1</cell></row><row><cell></cell><cell></cell><cell>Hybrid PT</cell><cell>87.66.6</cell><cell>40.92.7</cell><cell>53.58.2</cell><cell>44.26.4</cell><cell>79.81.5</cell><cell>56.82.6</cell><cell>66.57.2</cell></row><row><cell cols="2">PT (410K) T5-XXL</cell><cell>LM Adaption PPT</cell><cell>77.67.5 93.50.3</cell><cell>36.23.6 50.20.7</cell><cell>27.30.2 60.01.2</cell><cell>26.50.4 53.00.4</cell><cell>62.00.3 66.435.7</cell><cell>55.31.0 58.91.6</cell><cell>61.21.7 71.26.2</cell></row><row><cell></cell><cell></cell><cell>Hybrid PPT</cell><cell>93.80.1</cell><cell>50.10.5</cell><cell>62.50.9</cell><cell>52.20.7</cell><cell>82.01.0</cell><cell>59.83.2</cell><cell>73.27.0</cell></row><row><cell></cell><cell></cell><cell>Unified PPT</cell><cell>94.40.3</cell><cell>46.01.3</cell><cell>58.00.9</cell><cell>49.91.3</cell><cell>76.02.7</cell><cell>65.82.1</cell><cell>82.25.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Chinese Tasks</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Model</cell><cell>Method</cell><cell cols="2">ChnSent Amazon Acc. Acc.</cell><cell>CCPM Acc.</cell><cell>C 3 Acc.</cell><cell cols="2">LCQMC CMNLI Acc. Acc.</cell><cell>OCNLI Acc.</cell></row><row><cell></cell><cell>mT5-Small</cell><cell>-</cell><cell>76.12.6</cell><cell>29.91.9</cell><cell>31.91.2</cell><cell>29.60.5</cell><cell>52.42.5</cell><cell>36.50.2</cell><cell>34.91.3</cell></row><row><cell></cell><cell>mT5-Base</cell><cell>-</cell><cell>78.20.6</cell><cell>36.40.9</cell><cell>40.46.8</cell><cell>29.40.6</cell><cell>50.91.0</cell><cell>36.30.5</cell><cell>35.40.6</cell></row><row><cell>FT</cell><cell>mT5-Large</cell><cell>-</cell><cell>79.10.6</cell><cell>31.01.4</cell><cell>46.04.0</cell><cell>29.90.8</cell><cell>52.10.6</cell><cell>35.81.2</cell><cell>35.21.1</cell></row><row><cell>(11B)</cell><cell>mT5-XL</cell><cell>-</cell><cell>82.72.6</cell><cell>35.51.7</cell><cell>68.35.1</cell><cell>29.71.2</cell><cell>52.92.4</cell><cell>36.81.6</cell><cell>35.60.5</cell></row><row><cell></cell><cell>mT5-XXL</cell><cell>-</cell><cell>83.61.5</cell><cell>42.10.8</cell><cell>79.71.1</cell><cell>37.23.3</cell><cell>53.11.0</cell><cell>39.00.4</cell><cell>37.41.2</cell></row><row><cell></cell><cell>CPM-2</cell><cell>-</cell><cell>86.11.8</cell><cell>42.52.0</cell><cell>81.81.6</cell><cell>38.43.7</cell><cell>58.81.8</cell><cell>40.71.0</cell><cell>38.51.5</cell></row><row><cell></cell><cell></cell><cell>Vanilla PT</cell><cell>62.13.1</cell><cell>30.34.8</cell><cell>31.09.7</cell><cell>28.20.4</cell><cell>51.53.4</cell><cell>35.40.5</cell><cell>37.00.5</cell></row><row><cell></cell><cell></cell><cell>Hybrid PT</cell><cell>79.24.0</cell><cell>39.13.8</cell><cell>46.615.0</cell><cell>29.20.5</cell><cell>54.62.3</cell><cell>37.10.6</cell><cell>37.81.4</cell></row><row><cell cols="2">PT (410K) CPM-2</cell><cell>LM Adaption PPT</cell><cell>74.35.2 90.10.8</cell><cell>35.22.4 48.60.6</cell><cell>33.712.8 85.40.6</cell><cell>30.21.5 43.82.2</cell><cell>51.42.9 59.10.6</cell><cell>35.10.3 43.00.5</cell><cell>38.01.1 40.10.4</cell></row><row><cell></cell><cell></cell><cell>Hybrid PPT</cell><cell>89.50.3</cell><cell>48.82.0</cell><cell>83.90.5</cell><cell>46.00.5</cell><cell>67.30.9</cell><cell>41.30.8</cell><cell>38.70.6</cell></row><row><cell></cell><cell></cell><cell>Unified PPT</cell><cell>90.70.2</cell><cell>44.61.1</cell><cell>83.40.9</cell><cell>50.20.6</cell><cell>55.00.4</cell><cell>40.60.4</cell><cell>41.51.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">We set 16 labels in this paper as they can cover most benchmarks, but more labels are applicable for other tasks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the National Science Foundation for Distinguished Young Scholars (with No. 62125604) and the NSFC projects (Key project with No. 61936010 and regular project with No. 61876096). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2019GQG1 and 2020GQG0005.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">FLEX: Unifying evaluation for few-shot nlp</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno>arxiv:2107.07170</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Language models are fewshot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
				<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BoolQ: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">WARP: Word-level adversarial reprogramming</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021-05">May. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07139</idno>
		<title level="m">Pretrained models: Past, present and future</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">PTR: prompt tuning with rules for text classification</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno>arxiv:2105.11259</idno>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno>arxiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-06">Jun Araki, and Graham Neubig. 2020</date>
		</imprint>
	</monogr>
	<note>Transaction of TACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UnifiedQA: Crossing format boundaries with a single qa system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Prefix-tuning: Optimizing continuous prompts for generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<title level="m">GPT understands, too</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>arxiv:2105.11447</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning how to ask: Querying lms with mixtures of soft prompts</title>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NACCL-HTL</title>
				<meeting>NACCL-HTL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OpenAI Technical report</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">OpenAI Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploiting cloze questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
				<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MultiQA: An empirical investigation of generalization and transfer in reading comprehension</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02847</idno>
		<title level="m">A simple method for commonsense reasoning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CPM-2: Large-scale costeffective pre-trained language models</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi ; Yuxian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><surname>Han</surname></persName>
		</author>
		<idno>arxiv:2106.10715</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR. Zhengyan Zhang</title>
				<meeting>ICLR. Zhengyan Zhang</meeting>
		<imprint>
			<date type="published" when="2021">2021a. 2021b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Revisiting fewsample bert fine-tuning</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections</title>
		<author>
			<persName><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
				<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Factual probing is [mask]: Learning vs. learning to recall</title>
		<author>
			<persName><forename type="first">Zexuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HTL</title>
				<meeting>NAACL-HTL</meeting>
		<imprint>
			<date type="published" when="2021">2021b</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
