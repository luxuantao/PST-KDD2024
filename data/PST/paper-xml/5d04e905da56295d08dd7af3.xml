<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Binarized Collaborative Filtering with Distilling Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-05">5 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yong</forename><surname>Ge</surname></persName>
							<email>yongge@email.arizona.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Binarized Collaborative Filtering with Distilling Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-05">5 Jun 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1906.01829v1[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The efficiency of top-K item recommendation based on implicit feedback are vital to recommender systems in real world, but it is very challenging due to the lack of negative samples and the large number of candidate items. To address the challenges, we firstly introduce an improved Graph Convolutional Network (GCN) model with highorder feature interaction considered. Then we distill the ranking information derived from GCN into binarized collaborative filtering, which makes use of binary representation to improve the efficiency of online recommendation. However, binary codes are not only hard to be optimized but also likely to incur the loss of information during the training processing. Therefore, we propose a novel framework to convert the binary constrained optimization problem into an equivalent continuous optimization problem with a stochastic penalty. The binarized collaborative filtering model is then easily optimized by many popular solvers like SGD and Adam. The proposed algorithm is finally evaluated on three real-world datasets and shown the superiority to the competing baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays, recommender systerms are widely used in people's daily life <ref type="bibr" target="#b9">[Liu et al., 2011;</ref><ref type="bibr" target="#b8">Lian et al., 2016;</ref><ref type="bibr" target="#b8">Li et al., 2018a]</ref>, but a growing scale of users and products renders recommendation challenging. Because implicit feedback is more common and easier to collect than explicit feedback, we concentrate on how to accelerate top-K recommendation based on implicit feedback. However, there are two challengings to address. Firstly, compared with explicit feedback, implicit feedback is more difficult to utilize because of the lack of negative samples <ref type="bibr" target="#b10">[Pan et al., 2008]</ref>. Secondly, generating top-k preferred items for each user is extremely time-consuming.</p><p>For the first problem, recently, Spec-tralCF <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref> combined collaborative filtering model with graph convolutional network <ref type="bibr" target="#b6">[Henaff et al., 2015]</ref> to mine hidden interactions between users and items from spectral domain, which showed enormous potential for implicit feedback problem <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref>. However, SpectralCF ignores high-order feature interaction.</p><p>For the second problem, for extracting top-K preferred items for each user, the time complexity of recommendation is O(M N D + M N logK) when there are M users, N items and D dimensions in the latent space. Therefore, this is a critical efficiency bottleneck. However, it is necessary to timely update recommendation algorithms and the recommendation list because user interest evolves frequently. Fortunately, hash technique, encoding real-valued vectors/matrices into binary codes(e.g., {0, 1}, {−1, 1}), is promising to address this challenge because inner product can be efficiently computed between binary codes via bit operation. Finding approximate top-K items can be even finished in sublinear or logarithmic time <ref type="bibr" target="#b10">[Wang et al., 2012;</ref><ref type="bibr" target="#b10">Muja and Lowe, 2009]</ref> by making use of index technique.</p><p>Several methods applied hash techniques to recommendation. Some two-stage approximation methods like BCCF <ref type="bibr">[Zhou and Zha, 2012]</ref>, PPH <ref type="bibr" target="#b12">[Zhang et al., 2014]</ref>, CH <ref type="bibr" target="#b10">[Liu et al., 2014]</ref> incur large quantization loss <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref>, and a direct optimization model DCF <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> is easy to fall into a local optimum because it is based on local search. To this end, to improve the accuracy of hashing-based recommender systems for implicit feedback, we propose a binarized collaborative filtering framework with distilling graph convolutional network. In the framework, we firstly train a CF-based GCN model (GCN-CF) which can capture high-order feature interaction via cross operation. Following that, we distill the ranking information from the trained GCN-CF model into a binarized model (DGCN-BinCF) with knowledge distillation technique (KD <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>). To be more specific, we introduce a novel distillation loss, which penalizes not only the discrepancy between distributions of positive items defined by GCN-CF and that defined by Bin-CF, but also the the discrepancy between distributions of sampled negative items. Noting that learning hash codes is generally NP-hard <ref type="bibr" target="#b6">[Håstad, 2001]</ref>, approximation methods are appropriate choices but it may incur the loss of information during the training process. To this end, inspired by <ref type="bibr" target="#b3">[Dai et al., 2016]</ref>, we transform the binary optimization problem to an equivalent continuous optimization problem by imposing a stochastic penalty term. Therefore, any gradient-based solver can optimize the overall loss with ranking-based loss with knowledge distillation loss.</p><p>Our contributions are summarized as follows:</p><p>• We propose a novel framework DGCN-BinCF to distill the ranking information from the proposed GCN-CF model into the binary model. To the best of our knowledge, DGCN-BinCF is the first model utilizing knowledge distilling to improve the performance of binarized model. We also improve GCN via adding a cross operation to aggregate users and items' own high-order features.</p><p>• We propose a generic method to relax the binary constraint problem to an equivalent bound-constrained continuous optimization problem. Hence, we can optimize the original problem by popular solvers directly. • Through extensive experiments performed on three realworld datasets, we show the superiority of the proposed framework to the state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we review several works related to our task including GCN for recommender systems, recent hashingbased collaborative filtering methods and distilling knowledge techniques for ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GCN for Recommender Systems</head><p>How to take advantage of the rich linkage information from the user-item bipartite graph is crucial for implicit feedback. Some work used GCN to solve it such as SpectralCF <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref>, GCMC <ref type="bibr" target="#b0">[Berg et al., 2017]</ref>, <ref type="bibr">RMGCNN [Monti et al., 2017]</ref>, GCNWSRS <ref type="bibr" target="#b11">[Ying et al., 2018]</ref>, <ref type="bibr">LGCN [Gao et al., 2018]</ref>, etc. (1)SpectralCF was the first model to learn from the Spectral domain of the user-item bipartite graph directly based on collaborative filtering. Because it could discover deep connections between users and items, it may alleviate cold-start problem. (2)GCMC combined GCN model with a graph auto-encoder to learn users' and items' latent factors.</p><p>(3)RMGCNN proposed a matrix completion architecture combining a multi-graph convolutional neural network with a recurrent neural network. (4)GCNWSRS focused on how to apply GCN model for web-scale recommendation tasks effectively, like billion of items and hundreds of millions of users. (5)LGCN proposed a sub-graph training strategy to save memory and computational resource requirements greatly. Its experiments showed it was more efficient as compared to prior approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discrete Hashing for Collaborative Filtering</head><p>A pioneer work was to exploit Locality-Sensitive Hashing <ref type="bibr" target="#b5">[Datar et al., 2004]</ref> to generate binary codes for Google News readers according to their click history <ref type="bibr" target="#b4">[Das et al., 2007]</ref>. Then <ref type="bibr">[Karatzoglou et al., 2010]</ref> proposed a method mapping users and items' latent factors into Hamming space to obtain binary representation. Later, following this, some two stage methods <ref type="bibr">[Zhou and Zha, 2012;</ref><ref type="bibr" target="#b12">Zhang et al., 2014]</ref> which relaxed binary constraints at first and then quantified binary codes.</p><p>Nonetheless, <ref type="bibr" target="#b12">[Zhang et al., 2016]</ref> proposed that those two-stage methods suffered from large quantization loss. Therefore, DCF proposed a method which could optimize binary codes directly. However, DCF optimizes binary codes via searching neighborhoods with the distance one. So it is easy to fall into local optima.</p><p>2.3 Distilling Knowledge for Ranking <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref> was the first one that proposed method "Knowledge Distilling", which trained a complex neural network firstly and then transferred the complex model to a small model. The role of the complex model is similar to a teacher, and the role of the small model is similar to a student. Following this, DarkRank <ref type="bibr" target="#b2">[Chen et al., 2018]</ref> proposed a method combining deep metric learning and "Learning to rank" technique with KD to solve pedestrian re-identification, image retrieval and image clustering tasks. In addition, <ref type="bibr">[Tang and Wang, 2018]</ref> applied KD with point-wise ranking on recommendation task. Unfortunately, it did not focus on implicit feedback problem and how to transfer unobserved interaction information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Definitions and Preliminaries</head><p>Throughout the paper, we denote vectors by boldfaced lowercase letters and matrices by boldfaced uppercase letters. All vectors are considered as column vectors. Next, we define the following definitions in this paper:</p><p>Definition 1 (Bipartite Graph)A user-item bipartite graph with M + N vertices and E edges is defined as G = {U, I, E}, where U and I are two disjoint vertex sets of user and item, and M = |U|, N = |I|. For each edge e ∈ E, it has the form that e = (u, i), where u ∈ U and i ∈ I, which shows that there exists an interaction between user u and item i in the training set.</p><p>Definition 2 (Laplacian Matrix)Given a bipartite graph with M +N vertices and E edges, the laplacian matrix L is defined as L = D −1/2 AD −1/2 , where A is the adjacent matrix and D is the (M + N ) × (M + N ) diagonal degree matrix defined as D nn = j A nj .</p><p>Our work focuses on recommendation based on implicit feedback, where we only observe whether a user has viewed or clicked an item. We denote I + i as the set of all items clicked by user i and denote I − i as the set of remaining items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Binary Collaborative Filtering</head><p>Matrix factorization maps users and items onto a joint Ddimensional latent space, where user embedding matrix is represented by</p><formula xml:id="formula_0">P = [p 1 , • • • , p M ] ′ ∈ R M×D and item em- bedding matrix is represented by Q = [q 1 , • • • , q N ] ′ ∈ R N ×D .</formula><p>However, binary collaborative filtering (Bin-CF) maps users and items onto a joint D-dimensional Hamming space.</p><formula xml:id="formula_1">Denoting Φ = [φ 1 , • • • , φ M ] ′ ∈ {−1, 1} M×D and Ψ = [ψ 1 , • • • , ψ M ] ′ ∈ {−1, 1} N ×D</formula><p>as user and item's binary codes respectively, for implicit feedback, the Bin-CF problem is formulated as follows:</p><formula xml:id="formula_2">arg min Φ,Ψ L Bin−CF = (i,j,j ′ )∈D −lnσ(p T i (q j − q j ′ )) s.t.Φ = H(P), Ψ = H(Q) (1)</formula><p>where H(•) is a hash function:R → {−1, 1}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Binary-Continuous Equivalent Transformation</head><p>Let us consider the following generic binary program firstly,</p><formula xml:id="formula_3">minf (x) s.t.x ∈{−1, 1} d (2)</formula><p>and a transformed problem,</p><formula xml:id="formula_4">min f (x) + βg(x) s.t.x ∈[−1, 1] d (3)</formula><p>where g(•):R d → R is a penalty term for f (x) and β is its penalty coefficient. <ref type="bibr">[Giannessi and Tardella, 1998;</ref><ref type="bibr" target="#b10">Lucidi and Rinaldi, 2010]</ref> show that the above two problems are equivalent when certain conditions hold.</p><p>Lemma 1 Denote || • || be a chosen norm. Suppose the following conditions hold:</p><formula xml:id="formula_5">1) When x ∈ [−1, 1] d , f (x) is bounded. In addition, there exists an open set A ⊃ {−1, 1} d and real positive num- ber η, such that for ∀x 1 , x 2 ∈ A, the following H ölder condition is satisfied: |f (x 1 ) − f (x 2 )| ≤ η||x 1 − x 2 || (4) 2) g(•) satisfies: (a) g(•) is continuous on [−1, 1] d (b) ∀x ∈ {−1, 1} d , g(x) = 0; ∀x ∈ (−1, 1) d , g(x) &gt; 0 (c) ∀y ∈ {−1, 1} d ,</formula><p>there exits a neighborhood S(y) of y and a real positive number ǫ(y), such that:</p><formula xml:id="formula_6">∀x ∈ S(y) ∩ (−1, 1) d , g(x) ≥ ǫ(y)||x − y|| (5)</formula><p>Then there exits a real value η 0 , such that ∀η &gt; η 0 , problem 2 and problem 3 are equivalent.</p><p>It can be verified that g(x) = |||x| − 1|| 2 F satisfies above conditions, and we adopt it as the penalty term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Binarized Collaborative Filtering with Distilling Graph Convolutional Network</head><p>For binarized collaborative filtering for implicit feedback problem as shown in Eqn.1, there are three problems to solve. Firstly, the interaction information between users and items is extremely sparse. Secondly, a lot of information is lost during learning binary codes. Thirdly, binary optimization is general NP-hard, so we must adopt an efficient approximate method to solve it. We propose a novel framework-Binarized Collaborative Filtering with Distilling Graph Convolutional Network to deal with the aforementioned problems. Because GCN model can mine hidden connection information between users and items in user-item graph spectral domain, we train a GCN-based collaborative filtering model (GCN-CF) to solve the first problem. Then we utilize knowledge distillation to transfer the ranking information from GCN-CF into the binary model to make up for information loss. Finally, we propose a method to transform the binary optimization to a continuous optimization problem to solve the binary optimization problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">GCN-based Collaborative Filtering</head><p>Following SpectralCF, our graph convolutional operation is shown as the following:</p><formula xml:id="formula_7">U (k+1) V (k+1) = ρ((I M+N + L) U (k) V (k) Θ (k) ) (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>where L is the Laplacian matrix of the bipartite graph G. I M+N is an identity matrix, ρ(•) is an activation function and Θ (k) is a layer-specific trainable filter parameter. The proposed convolution operation as shown in Eqn. ( <ref type="formula" target="#formula_7">6</ref>) is denoted as sp(X; L, Θ).In this model, we set it as a two-layer GCN. According to Eqn. (1), similarity to matrix factorization (MF) methods, it does not take advantage of the user's own and the item's own high-order interaction, which limits the performance of GCN. Inspired by <ref type="bibr">CrossNet [Wang et al., 2017]</ref>, we define the cross operation(cross op) to fix the problem.</p><p>The cross operation can be formulated as</p><formula xml:id="formula_9">x k+1 = x k w T k x k + x k (7)</formula><p>where x k is a user or item's embedding vector and w k is a parameter vector. The term x k w T k x k takes the place of the term x 0 w T k x k in CrossNet, which leads to obtaining higher-order interactions than CrossNet when setting the same iterations. In addition, the time complexity of the proposed cross operation is still the same as CrossNet's. The improved GCN model can be vectorized as follows:</p><formula xml:id="formula_10">U (1) V (1) = sp( U (0) V (0) ; L, Θ (0) ) (8) U (2) V (2) = diag(( U (1) V (1) • W 1 )1) U (1) V (1) + U (1) V (1)<label>(9)</label></formula><formula xml:id="formula_11">U (3) V (3) = diag(( U (2) V (2) • W 2 )1) U (2) V (2) + U (2) V (2) (10) U (4) V (4) = sp( U (3) V (3) ; L, Θ (1) )<label>(11)</label></formula><p>where "•" represents Hadamard product, "1" is a column vector whose elements are all 1 and W 1 , W 2 ∈ R (M+N )×D are weight matrices. Moreover, we add batch normalization [Ioffe and Szegedy, 2015] before Eqn.8 and Eqn.11.</p><p>In order to make full use of features from every layer of GCN, we follow SpectralCF and concatenate them into the final latent factors of users and items as:</p><formula xml:id="formula_12">U T = [U (0) , U (1) , U (4) ]<label>(12)</label></formula><formula xml:id="formula_13">V T = [V (0) , V (1) , V (4) ]<label>(13)</label></formula><p>In terms of the loss function, we employ the popular and effective BPR loss <ref type="bibr" target="#b10">[Rendle et al., 2009]</ref>. In particular, given a user matrix U and an item matrix V as shown in Eqn.12 and Eqn.13, the loss function of GCN-CF is given as</p><formula xml:id="formula_14">L GCN −CF = (i,j,j ′ )∈D −lnσ(u T i (v j − v j ′ )) + λ(||U|| 2 F + ||V|| 2 F )<label>(14)</label></formula><p>where u i and v j denote ith and jth rows of U and V respectively; λ is the regularization coefficient. Negative sample j ′ is sampled from I − i randomly and the training data D is generated as</p><formula xml:id="formula_15">D = {(i, j, j ′ )|i ∈ U ∧ j ∈ I + i ∧ j ′ ∈ I − i }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilling GCN into Binarized Collaborative Filtering</head><p>In this model, we distill the ranking information in GCN-CF model and transfer it to a simple binary collaborative filtering model via a mixed objective function. The model is denoted by DGCN-BinCF for short. The key motivation of the distillation in DGCN-BinCF is two-fold. On one hand, we consider the distribution of positive (negative) samples in the binary model should be close to that in the GCN-CF. On the other hand, the differences between positive and negative samples should become far enough in DGCN-BinCF. However, because the BPR model can guarantee that positive samples have higher scores than negative samples', negative samples are assigned much lower probability than positive samples' if we consider the distribution of both positive and negative samples at the same time. Thus, we consider positive and negative samples in GCN-CF separately.</p><p>Specifically, to distill the ranking information in GCN-CF, we hope the positive (negative) items of one user have the approximately same order in binary model and continuous model. For instance, if user u's preference for the three items i, j, k is ranked as [i, j, k] in GCN-CF model, we hope that the rank keeps [i, j, k] in the binary model. According to ListNet <ref type="bibr" target="#b1">[Cao et al., 2007]</ref>, we can characterize sorting information in the following ways</p><formula xml:id="formula_16">L rank = (i,j)∈D + − exp( u T i vj T ) j∈D + exp( u T i vj T ) log( exp( p T i q j T ) j∈D + exp( p T i q j T ) ) + (i,j)∈D − − exp( u T i vj T ) j∈D − exp( u T i vj T ) log( exp( p T i q j T ) j∈D − exp( p T i q j T ) )<label>(15)</label></formula><p>where</p><formula xml:id="formula_17">D + = {(i, j)|i ∈ U ∧ j ∈ I + i }, D − = {(i, j ′ )|i ∈ U ∧ j ′ ∈ I − i }</formula><p>and T is the temperature parameter. In Eqn.15, we convert the items' score list to probability distributions via softmax function, and utilize cross entropy for penalizing the discrepancy. According to <ref type="bibr" target="#b7">[Hinton et al., 2015]</ref>, combining L Bin−CF with L rank as a multi-task learing problem can transfer the ranking knowledge to the binary model. It's worth mentioning that since the magnitudes of the gradients produced by the L rank scale as 1/T 2 , it is necessary to multiply them by T 2 when mixing L rank and L Bin−CF . So the loss function of DGCN-BinCF is formulated as L DGCN −BinCF is denoted as L(P, Q; U, V, α, T ) for short. P and Q are user and item embedding matrices in DGCN-BinCF respectively, and U, V are trained user and item embedding matrices of GCN-CF. The loss L rank encodes the first motivation and the loss L Bin−CF encodes the second motivation.</p><formula xml:id="formula_18">L DGCN −BinCF = L Bin−CF + αT 2 L rank (16)</formula><p>For the binary optimization problem, it is a direct method to use tanh(x/t) to approximate sign function, where t is a small temperature. But <ref type="bibr" target="#b8">[Li et al., 2018b]</ref> points that setting a small temperature will harm the optimization process. <ref type="bibr" target="#b2">[Courbariaux et al., 2015]</ref> mentions that generating binary codes stochastically is a finer and more correct averaging process than generating binary codes via sign function. Hence, we generate binary codes via sampling from the Bernoulli distribution. More specifically, given x ∈ (−1, 1) d , its corresponding binary code is x = x + ε, where ε is a random variable which only can be 1 − x and −1 − x, and</p><formula xml:id="formula_19">P (ε i = 1 − x i ) = sigmoid(x i /τ ), P (ε i = −1 − x i ) = 1 − sigmoid(x i /τ ).</formula><p>Here, τ is temperature parameter. To force the noise to be small, we add the expectation of noise as a penalty term. Therefore, the DGCN-BinCF can be transformed into the following optimization problem: min L(tanh(P)</p><formula xml:id="formula_20">+ ε P , tanh(Q) + ε Q ; U, V, α, T) + ν(E(||ε P || 2 F ) + E(||ε Q || 2 F )) (17) where (ε P ) ij ∼ Bernoulli(tanh(P ij )), (ε Q ) ij ∼ Bernoulli(tanh(Q ij )).</formula><p>We use the tanh function to bound the value of P, Q between -1 and 1.</p><p>According to Lemma 1, Eqn 17 can be rewritten as minL(tanh(P), tanh(Q); U, V, α, T)</p><formula xml:id="formula_21">+ ν(E(||ε P || 2 F ) + E(||ε Q || 2 F )) + β i g tanh(p i ) + j g tanh(q j )<label>(18)</label></formula><p>Here we adopt g(p i ) = |||p i | − 1|| 2 2 which can be validated satisfying the conditions as the penalty term. Eqn.18 can be optimized by any gradient-based optimization methods directly. The whole training processing is summarized in Al-gorithm1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we evaluate our proposed DGCN-BinCF framework with the aim of answering the following research questions.</p><p>1. Does the recommendation performance of the proposed DGCN-BinCF framework outperforms the state-of-theart hashing-based recommendation methods?</p><p>2. Whether our proposed GCN-CF is effective?</p><p>3. Whether distilling ranking information helps learning binary model? 4. Whether this proposed framework can converge well?</p><p>We introduce the experimental settings firstly and then answer the above questions in following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings Dataset</head><p>We use three public real datasets including MovieLens1M, MovieLens10M and Yelp to evaluate the proposed algorithm. Because the three datasets are explicit feedback data, to convert them into implicit feedback data, we set all ratings as positive samples. In addition, due to the extreme sparsity of them, we then filter users who have less than 20 ratings and remove items that are rated by less than 20 users. Table <ref type="table" target="#tab_1">1</ref> summaries the filtered datasets. For each user, we sampled randomly 50% positive samples as training and the remaining as test. We repeated five random splits and reported the averaged results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison Methods</head><p>To evaluate the performance of DGCN-BinCF for hashingbased recommender systems, we compare DGCN-BinCF with 3 very popular and state-of-art methods: DCF, BCCF and PPH. DCF solves the binary optimization problem directly via bit-wise optimization. BCCF and PPH are twostage methods.</p><p>To measure the effectiveness of the improved GCN model, we compare GCN-CF with SpectralCF. And we compare DGCN-BinCF with the binary model L Bin−CF . To show the role of KD loss in binary optimization, L Bin−CF is optimized by our proposed relaxation method as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metric</head><p>To evaluate the recommendation system performance, we choose four widely used ranking-based metric:</p><p>(1) NDCG (Normalized Discounted Cumulative Gain), (2) Recall, , and (3) MAP (Mean Average Precision). We predicted the top-K preferred items from test set for each user in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Settings</head><p>In our experiments, we set the regularization coefficient λ = 0.001 in GCN-CF in all dataset. For DGCN-BinCF, we set temperature T = 1, τ = 0.2, the weight α = 10, and the penalty coefficients β = 0.001, ν = 0.001 in the three datasets. In addition, we set the dimension of users and items' latent factor of GCN-CF 16 in MovieLens10M and 64 in the other two datasets. The learned matrices U ,V in GCN-CF are used as the initialization of P ,Q in DGCN-BinCF. All parameters of SpectralCF are set according to <ref type="bibr" target="#b13">[Zheng et al., 2018]</ref>.</p><p>Besides, for DCF, BCCF and PPH, we held-out evaluation means on splits of training data randomly to tune the optimal hyper-parmenters via grid search. α and β in DCF are tuned among the set {1e − 4, 1e − 3, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison with Baselines</head><p>Although hashing-based recommendation has significant advantages of both time and storage, it often incurs low accuracy recommendation because binary codes have limited representation ability and lose a lot of information compared with real-valued recommender systems. DGCN-BinCF is to improve the accuracy of recommendation.</p><p>In this part we will answer the first question. We compare the recommendation accuracy of DGCN-BinCF with three state-of-art binary recommendation methods including DCF, BCCF and PPH on the three datasets. Table <ref type="table">2</ref>, Table <ref type="table">3</ref> and Table <ref type="table">4</ref> summary the results.</p><p>The three tables show that DGCN-BinCF has much better performance than all baselines on the three datasets. This is because we train the improved GCN model GCN-CF firstly to discover the deep interactions between users and items and then transfer the ranking information to a binary model, DGCN-BinCF loses less information than baseline models.</p><p>In addition, the binary optimization problem is optimized directly by the proposed penalty terms, which leads to less quantization loss. Therefore, DGCN-BinCF has great advantages over DCF, BCCF and PPH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Effectiveness of GCN-CF</head><p>It is mentioned that SpectralCF did not consider aggregating users and items' own high-order feature, which may limit its representation ability. In this part, we will answer the second question.</p><p>We implement the experiment in MovieLens1M dataset. We utilize three metrics to evaluate the performance of Spec-tralCF and GCN-CF respectively. Figure <ref type="figure" target="#fig_0">1</ref> shows the results. In two histograms, the orange column is the performance of GCN-CF and the blue one represents the results of SpectralCF. From the histograms, it is clear to observe that GCN-CF has great improvement (over 20%) for every metric compared with SpectralCF, which shows the effectiveness of GCN-CF.</p><p>Recall@100 MAP@100 NDCG@100 0 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">The Effectiveness of Distillation</head><p>Because lots of useful information loses during learning the binary representation, it is vital to utilize the ranking information from the trained GCN-CF model as supplements for learning discrete codes. In this part, we investigate the role of ranking information for binary optimization. To implement the experiment, we consider optimizing Eqn.1 directly by adding the proposed penalty term. In the other word, we set α = 0 in L DGCN −BinCF , and compare its results with DGCN-BinCF. We test the two methods in the MovieLens1M dataset and evaluate them via the four ranking metrics.</p><p>Figure <ref type="figure">2</ref> reports the comparison results. The blue bar represents BinCF and the orange bar means DGCN-BinCF model.</p><p>The two histograms show that for all evaluation indicators, DGCN-BinCF outperforms BinCF by 10%. Thus we conclude that the distillation method helps the model learn high quality binary representation.</p><p>Recall@100 MAP@100 NDCG@100 0  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Convergence</head><p>In this section, we will answer the forth question. Because deep models and discrete optimization may diverge, we test the convergence of GCN-CF and DGCN-BinCF model.</p><p>To test the convergence of our proposed model GCN-CF and DGCN-BinCF, we implement the experiment on Movie-Lens1M. We record the value of Eqn.14 and Eqn.18 with the change of epoch respectively. In this experiment, we set the maximum number of iterations 200. Figure <ref type="figure" target="#fig_2">3</ref> shows the convergence of two models. It is observed that loss value of GCN-CF decreases and the DGCN-BinCF converges greatly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a hash-based method DGCN-BinCF to accelerate implicit feedback recommendation. Because implicit feedback lacks negative samples and learning binary codes loses a lot of information, we train the model GCN-CF, which aggregates users' and items' own high-order feature, to mine rich connection information, and then distill the ranking information from GCN-CF into the binary model. In addition, we propose a method utilizing penalty terms to learning binary codes based on gradient descent directly. The experiments on three real-world datasets show the great superiority of our framework.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The performance of SpectralCF and GCN-CF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: The comparison between BinCF and DGCN-BinCF</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The left one is the loss-epoch figure of GCN-CF model;the right one is the loss-epoch figure of DGCN-BinCF model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets</figDesc><table><row><cell></cell><cell>#User #Item</cell><cell cols="2">#Rating Density</cell></row><row><cell cols="2">MovieLens1M 6,022 3,043</cell><cell>995,154</cell><cell>5.43%</cell></row><row><cell cols="4">MovieLens10M 69,878 10,681 10,000,054 1.34%</cell></row><row><cell>Yelp</cell><cell>9,235 7,353</cell><cell>423,354</cell><cell>0.62%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>• • • , 1e1}. λ in BCCF is tuned among the set {0.01, 0.03, • • • , 0.09} and λ in PPH is tuned among the set {0.01, 0.5, 1, 2, 4, 8, 16}.</figDesc><table><row><cell></cell><cell cols="3">Recall@100 MAP@100 NDCG@100</cell></row><row><cell>DCF</cell><cell>0.0416</cell><cell>0.0101</cell><cell>0.0558</cell></row><row><cell>BCCF</cell><cell>0.1234</cell><cell>0.0720</cell><cell>0.1724</cell></row><row><cell>PPH</cell><cell>0.0277</cell><cell>0.0027</cell><cell>0.0249</cell></row><row><cell>DGCN-BinCF</cell><cell>0.3059</cell><cell>0.1187</cell><cell>0.3061</cell></row><row><cell cols="4">Table 2: Item Recommendation Results(MovieLens1M)</cell></row><row><cell></cell><cell cols="3">Recall@100 MAP@100 NDCG@100</cell></row><row><cell>DCF</cell><cell>0.0791</cell><cell>0.0180</cell><cell>0.0809</cell></row><row><cell>BCCF</cell><cell>0.0789</cell><cell>0.0267</cell><cell>0.0869</cell></row><row><cell>PPH</cell><cell>0.0978</cell><cell>0.0123</cell><cell>0.0695</cell></row><row><cell>DGCN-BinCF</cell><cell>0.2405</cell><cell>0.0537</cell><cell>0.1895</cell></row><row><cell cols="4">Table 3: Item Recommendation Results(MovieLens10M)</cell></row><row><cell></cell><cell cols="3">Recall@100 MAP@100 NDCG@100</cell></row><row><cell>DCF</cell><cell>0.0661</cell><cell>0.0053</cell><cell>0.0382</cell></row><row><cell>BCCF</cell><cell>0.0966</cell><cell>0.0122</cell><cell>0.0658</cell></row><row><cell>PPH</cell><cell>0.0627</cell><cell>0.0051</cell><cell>0.0361</cell></row><row><cell>DGCN-BinCF</cell><cell>0.1738</cell><cell>0.0192</cell><cell>0.1008</cell></row><row><cell cols="4">Table 4: Item Recommendation Results(Yelp)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The work was supported in part by grants from the National Natural Science Foundation of China (Grant No. U1605251, 61832017, 61631005 and 61502077).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02263</idno>
		<title level="m">Graph convolutional matrix completion</title>
				<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML&apos;07</title>
				<meeting>ICML&apos;07</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Matthieu Courbariaux, Yoshua Bengio</title>
				<imprint>
			<date type="published" when="2015">2018. 2018. 2015. 2015</date>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
	<note>Proceedings of NeurIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Binary optimized hashing</title>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MM&apos;16</title>
				<meeting>MM&apos;16</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1247" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Google news personalization: scalable online collaborative filtering</title>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW&apos;07</title>
				<meeting>WWW&apos;07</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="271" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Giannessi and Tardella, 1998] Franco Giannessi and Fabio Tardella. Connections between nonlinear programming and discrete optimization</title>
		<author>
			<persName><surname>Datar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twentieth annual symposium on Computational geometry</title>
				<meeting>the twentieth annual symposium on Computational geometry</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998">2004. 2004. 2018. 2018. 1998</date>
			<biblScope unit="page" from="149" to="188" />
		</imprint>
	</monogr>
	<note>Handbook of combinatorial optimization</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Some optimal inapproximability results</title>
		<author>
			<persName><forename type="first">Johan</forename><surname>Håstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Håstad</surname></persName>
		</author>
		<author>
			<persName><surname>Henaff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
	</analytic>
	<monogr>
		<title level="m">Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured data</title>
				<imprint>
			<date type="published" when="2001">2001. 2001. 2015. 2015</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="798" to="859" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Alexandros Karatzoglou, Alex Smola, and Markus Weimer. Collaborative filtering on a budget</title>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<idno>arXiv:1502.03167</idno>
	</analytic>
	<monogr>
		<title level="m">Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
				<imprint>
			<date type="published" when="2010">2015. 2015. 2015. 2015. 2010</date>
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning from history and present: Nextitem recommendation via discriminatively exploiting user behaviors</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02988</idno>
	</analytic>
	<monogr>
		<title level="m">Towards binary-valued gates for robust lstm training</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2018a. 2018. 2018b. 2018. 2016. 2016</date>
			<biblScope unit="page" from="1023" to="1028" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of ICDM&apos;16</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Personalized travel package recommendation</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICDM&apos;11</title>
				<meeting>ICDM&apos;11</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="407" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ranking distillation: Learning compact ranking models with high performance for recommender system</title>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</editor>
		<meeting><address><addrLine>Wang</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2014. 2014. 2010. 2010. 2017. 2017. 2009. 2009. 2008. 2008. 2009. 2009. 2018. 2018. 2012. Jun. 2012. 2017. 2017</date>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
		</imprint>
	</monogr>
	<note>Semi-supervised hashing for large-scale search. Bin Fu, Gang Fu, and Mingliang Wang. Deep &amp; cross network for ad click predictions. In Proceedings of the ADKDD&apos;17, page 12</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Ying</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD&apos;18</title>
				<meeting>KDD&apos;18</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Xiangnan He, Huanbo Luan, and Tat-Seng Chua. Discrete collaborative filtering</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR&apos;14</title>
				<meeting>SIGIR&apos;14</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014. 2014. 2016. 2016</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
	<note>Proceedings of SIGIR&apos;16</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning binary codes for collaborative filtering</title>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RecSys&apos;18</title>
				<meeting>RecSys&apos;18</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2018. 2018. 2012. 2012</date>
			<biblScope unit="page" from="498" to="506" />
		</imprint>
	</monogr>
	<note>Proceedings of KDD&apos;18</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
