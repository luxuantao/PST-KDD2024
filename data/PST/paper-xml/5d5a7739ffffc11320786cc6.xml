<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with graph attention mechanism</title>
				<funder>
					<orgName type="full">ACS Paragon Plus Environment Journal of Medicinal Chemistry ACS Paragon Plus Environment Journal of Medicinal Chemistry ACS Paragon Plus Environment Journal of Medicinal Chemistry</orgName>
				</funder>
				<funder>
					<orgName type="full">ACS Paragon Plus Environment Journal of Medicinal Chemistry TOC</orgName>
				</funder>
				<funder>
					<orgName type="full">ACS Paragon Plus Environment Journal of Medicinal Chemistry</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhaoping</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>No.19A Yuquan Road</addrLine>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dingyan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>No.19A Yuquan Road</addrLine>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feisheng</forename><surname>Zhong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>No.19A Yuquan Road</addrLine>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaozhe</forename><surname>Wan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>No.19A Yuquan Road</addrLine>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xutong</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>No.19A Yuquan Road</addrLine>
									<postCode>100049</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaojun</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaomin</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaixian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hualiang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Life Science and Technology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>200031</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingyue</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Acs</forename><surname>Paragon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Plus</forename><surname>Environment</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Drug Discovery and Design Center</orgName>
								<orgName type="department" key="dep2">Shanghai Institute of Materia Medica</orgName>
								<orgName type="institution" key="instit1">State Key Laboratory of Drug Research</orgName>
								<orgName type="institution" key="instit2">Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>555 Zuchongzhi Road</addrLine>
									<postCode>201203</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Journal of Medicinal Chemistry ACS Paragon Plus Environment Journal of Medicinal Chemistry ACS Paragon Plus Environment</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pushing the boundaries of molecular representation for drug discovery with graph attention mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1021/acs.jmedchem.9b00959</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hunting for chemicals with favourable pharmacological, toxicological and pharmacokinetic properties remains a formidable challenge for drug discovery. Deep learning provides us with powerful tools to build predictive models that are appropriate for the rising amounts of data, but the gap between what these neural networks learn and what human beings can comprehend is growing. Moreover, this gap may induce distrust and restrict deep learning applications in practice. Here, we introduce a new graph neural network architecture called Attentive FP for molecular representation that uses a graph attention mechanism to learn from relevant drug discovery datasets. We demonstrate that Attentive FP achieves state-of-the-art predictive performances on a variety of datasets and that what it learns is interpretable. The feature visualization for Attentive FP suggests that it automatically learns non-local intramolecular interactions from specified tasks, which can help us gain chemical insights directly from data beyond human perception.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Efficient medicinal chemistry relies on associative reasoning and pattern recognition for molecular structures. However, the use of empirical 'drug-likeness' rules and 'privileged' chemical (sub)structures are failing because the low-hanging fruit has become scarcer. Even the most experienced medicinal chemists will have diverse preferences when prioritizing compounds to the next stage. The challenge for finding chemicals with favourable pharmacological, toxicological and pharmacokinetic properties stems not only from the uncertainty of body biological systems but also from the intricate meanings of the information in chemical molecular systems, because humans are incapable of determining those properties directly from chemical structures. A molecular structure is usually composed of many-body interactions and complex electronic configurations, which make constructing their comprehensive representation a non-trivial issue. Given the rising amount of data and the complexity of chemical and biological systems, medicinal chemists have been working 'at the edge of chaos' and in desperate need of augmented intelligence from AI <ref type="bibr" target="#b0">1</ref> .</p><p>In the last few years, a large volume of data concerning the biological effects of chemical compounds has been accumulated and made publicly accessible <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> . In addition, an increasing number of large-scale high-quality quantum-chemical calculation results have been shared with the research community due to recent rapid advances in high performance computing (HPC). The increasing rate of data generation across all research disciplines related to drug discovery has provided unprecedented opportunities for understanding properties or actions that are useful for molecular design and for generating mechanistic hypotheses. Therefore, building machine learning models that can fit and predict big data from expensive biological assays and quantum-chemical calculations is of great interest. Many successful applications of machine learning methods have been reported <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> that outline the future of artificial intelligence in academia and industry. However, in contrast with fields such as image or voice recognition, generating suitable representations of chemical structures to extract the most relevant information regarding the properties of interest remains challenging. In this respect, molecular representation can be defined as a logical or mathematical procedure that transforms chemical information encoded within a molecular structure into a matrix of values. Molecular descriptors or "fingerprints" are frequently used for molecular representations. To date, over 5,000 molecular descriptors have been designed to characterize chemical meaning <ref type="bibr">16</ref> . The conventional machine learning approaches for QSAR/QSPR have revolved around feature engineering for these molecular descriptors <ref type="bibr" target="#b15">[17]</ref><ref type="bibr" target="#b16">[18]</ref><ref type="bibr" target="#b17">[19]</ref><ref type="bibr" target="#b18">[20]</ref><ref type="bibr" target="#b19">[21]</ref> in which the goal is to select a subset of the relevant descriptors for use in model construction. According to their raw input form, these molecular representations can be divided into graph-based and geometry-based representations. Graph-based representations take only the information concerning the topological arrangement of atoms as input, while geometry-based representations employ the molecular geometry information, including bond lengths, bond angles and torsional angles. In addition to the molecular descriptors or fingerprints designed by chemists, increasing numbers of molecular representations have been automatically generated by deep learning models from simple raw inputs.</p><p>For example, there has been a surge in molecular representations learned from deep neural network models by fitting the quantum-chemical calculations to simple raw inputs <ref type="bibr" target="#b20">[22]</ref><ref type="bibr" target="#b21">[23]</ref><ref type="bibr" target="#b22">[24]</ref><ref type="bibr" target="#b23">[25]</ref> .</p><p>Although molecular representation would seem to benefit from a priori knowledge of a molecule's three-dimensional (3D) conformation, pragmatic considerations such as calculation cost, alignment invariance, and uncertainty in conformation generation limit the use of geometry-based representations. For example, for most drug discovery applications, the active conformation of a small molecule in a given binding process is usually unknown. In such cases, graph-based molecular representations are more suitable; however, the gaps between these two classes of molecular representations usually lack transferability and cannot predict properties interchangeably. Therefore, the question arises whether a neural network architecture applied to a molecular graph might bridge this gap and make molecular representations more generalizable.</p><p>Molecular structures usually involve many-body interactions and complex electronic structures, but molecular graphs reduce the representation complexity, with nodes and edges representing atoms and bonds, respectively. The molecular graph assumes that the key interactions among nuclei and electrons in a molecule can be implicitly captured by a graph that provides a source of insight into the geometries, functions and properties of the molecule. Recently, substantial progress has been made in designing neural network architectures that learn representations from graph structured data <ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref> . The underlying principle of these architectures is to learn a form of mapping (also called an embedding) of nodes and edges that fully captures the graph information, particularly for inferring relations between nodes. Compared with previous graph topology representation approaches, the recent neural network approaches are much more powerful at capturing the nonprominent patterns, and they require less feature engineering effort.</p><p>Fig. <ref type="figure" target="#fig_7">1a</ref> encapsulates the recent neural graph representation for molecules. Given a target node, the grey nodes indicate the probability of neighbouring nodes impacting the target node in different graph-based molecular representation schemes. The darker the node colour is, the greater the chance of it impacting the target node. For the Neural FP and GCN models <ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b25">27,</ref><ref type="bibr" target="#b27">29</ref> , the neighbour nodes' chances to influence the target decrease with topological distance during the recursive propagation procedure. In chemical molecules, atomic pairs that are topologically distant may also have significant interactions and hence affect the overall molecular properties. A desirable molecular graph representation framework should be capable of capturing the information contained among even distant atoms in a molecule, such as intra-molecular hydrogen bonding. More recently, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph attention mechanism</head><p>An attention mechanism allows a method to focus on task-relevant parts of a neural network.</p><p>It has become a routine to apply the attention mechanism for tasks with sequence-structured data to allow the model to focus on the most relevant parts of the inputs and achieve a better prediction <ref type="bibr" target="#b29">31</ref> .</p><p>Recently, Velickovic and Bengio et al. proposed graph attention networks (GATs) that extend the attention mechanism to graph-structured data for node classification tasks <ref type="bibr" target="#b30">32</ref> . The core idea of applying the attention mechanism to the graph is to obtain a context vector for the target node by focusing on its neighbours and local environment. The process can be categorized into three operations: 1) Alignment, 2) Weighting, and 3) Context, as formulated below:</p><formula xml:id="formula_0">Alignment ? ? ?? = ?????_????(? ? [? ? , ? ? ])<label>(1)</label></formula><p>Weighting ? ? ?? = ???????(? ?? ) = exp (? ?? )</p><formula xml:id="formula_1">? ? ? ?(?) exp (? ?? ) (2) Context ? , ? ? = ???( ? ? ? ?(?) ? ?? ? ? ? ? ? ) (3)</formula><p>where is the target node (a specific atom), and  <ref type="bibr" target="#b32">34</ref> . GNNs encompass an iterative procedure using recursive neural networks (RNNs) that agglomerates the "messages" of nodes from nearby to distant. As per the existing GNN architectures <ref type="bibr" target="#b33">35</ref> which includes a messaging phase and a readout phase, our Attentive FP are formulated as follows: </p><formula xml:id="formula_2">is</formula><formula xml:id="formula_3">Messaging ? ? ? -1 ? = ? ? ? ?(?) ? ? -? (? ? -1 ? ,? ? -1 ? ) (4) Readout ? , ? ? ? = ??? ? -? (? ? -1 ? ,? ? -1 ? )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Molecular featurization</head><p>Node features first need to be defined before encoding a graph. Here, we use a total of 9 types of atomic features and 4 types of bond features to characterize atoms and their local environment (Table <ref type="table" target="#tab_2">1</ref>). Most of these features are encoded in a one-hot fashion, except for formal charge and radical electron number, which are encoded as integers due to their additive nature. To create a onehot encoding feature, all the candidate categorical variables of the feature are listed and marked as either 1 or 0 (one-hot or all null) by their matches to those variables. For example, a vector of 16 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive FP network architecture</head><p>Here, we propose a new graph neural network architecture for molecular representation, called</p><p>Attentive FP, which introduces an attention mechanism for extracting non-local effects at the intramolecular level. This attention mechanism allows a method to focus on the most relevant parts of the inputs to achieve better prediction <ref type="bibr" target="#b29">31</ref> . Fig. <ref type="figure" target="#fig_0">1</ref> summarizes the architecture of the Attentive FP network: (1) We assume that a molecule, its bond features and its atomic features are extracted with RDkit and encoded according to Table <ref type="table" target="#tab_2">1</ref>. Because the model is atom-centric, each atom has its own neighbour features that concatenate both neighbouring atoms and the connecting bond features.</p><p>Notably, the vectors of the atomic features and the neighbouring atomic features do not have the same length; consequently, linear transformation and non-linear activation were performed to unify</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACS Paragon Plus Environment</head><p>Journal of Medicinal Chemistry the vector length. This procedure actually forms a fully connected layer and generates initial state vectors ("embeddings") for each atom and its neighbours. (2) Then, to include more information from the local environment, those initial state vectors are further embedded with stacked attentive layers for node embedding, allowing the atom to progressively aggregate "messages" from its neighbourhoods using an attention mechanism allowing an atom to focus on the most relevant "messages" in its neighbourhood. In each node embedding attentive layer, a new state vector is generated for each atom. After passing through several stacked attentive layers, the state vector includes more neighbourhood information. (3) To combine the individual atom state vectors into a full-molecule state vector, we treat the entire molecule as a super-virtual node that connects every atom in a molecule, and is embedded using the same atom embedding attention mechanism. This process is performed on stacked attentive layers for molecule embedding and generates a state vector for the whole molecule. (4) The final state vector is the learned representation that encodes structural information about the molecular graph, followed by a task-dependent layer for prediction.</p><p>The entire network is trained in an end-to-end fashion, obtaining either a set of specific network weight parameters for a specific task or for multiple tasks simultaneously. (Supplementary Table <ref type="table" target="#tab_5">3</ref> summarizes the algorithm implementation pseudo-code and the formulas for the Attentive FP neural network).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attentive Layers on a graph</head><p>The full network architecture for the attentive layers is illustrated in Fig. <ref type="figure" target="#fig_7">1d</ref> and<ref type="figure" target="#fig_7">1e</ref>. As shown, the Attentive FP molecular representation scheme uses two stacks of attentive layers to extract information from the molecular graph. Specifically, one stack (with k layers) is for atom embedding (Fig. <ref type="figure" target="#fig_7">1c</ref>), and the other (with t layers) is for full-molecule embedding (Fig. <ref type="figure" target="#fig_7">1e</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>More intuitively, the graph attention mechanism in a single attentive layer is illustrated in Fig. <ref type="figure" target="#fig_7">1c</ref>. When applying attention to atom 3, the state vector of atom 3 is aligned with the state vector of its neighbours 2, 4 and 5, in which the features of connecting bonds have also been embedded by a fully connected layer. Then, the weight that measures how much attention we want to assign to the neighbours is calculated by a softmax function. Next, a weighted sum of the neighbourhood information is obtained as the attention context vector of atom 3. These attention operations ? 3 help the model to focus on task-relevant information from the local environment of the target atom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lastly,</head><p>(the attention context of atom 3) is fed into a GRU recurrent network unit together with ? 3 the state vector of atom 3. The GRU used here is a variant of an LSTM (Long Short Term ? 3 Memory) recurrent network unit, which has shown good performance in retaining and filtering information using simplified update and reset gates <ref type="bibr" target="#b34">36</ref> (Supplementary Figure <ref type="figure" target="#fig_0">1</ref>). This scheme allows relevant information to be passed down without too much attrition <ref type="bibr" target="#b35">37</ref> , which, in our case, means that the implicit linkages among distant atoms can still make a difference as long as they are related to the learning task. This property is what we desired to achieve for molecular representation.</p><p>Overall, in our well-designed attentive layer, the attention mechanism allows the target node to focus on the most related information from its neighbours, and the GRU recurrent network unit ensures that the information gets passed down to the neighbourhoods efficiently during the update iterations. To perform molecule-level embedding, we assume a virtual super-node that connects to all the atoms in the molecular graph; thus, the entire molecule can be embedded in the same fashion as the individual atoms (Fig. <ref type="figure" target="#fig_7">1e</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sets and benchmark tests</head><p>For comparison purposes, we trained and tested our Attentive FP model with datasets that have previously been benchmarked. The first collection of datasets was benchmarked by Duvenaud et al. <ref type="bibr" target="#b24">26</ref> and Kearnes el al. <ref type="bibr" target="#b26">28</ref> and includes three different datasets spanning solubility, malaria bioactivity and photovoltaic efficiency. The second collection of datasets was benchmarked by Wu et al. <ref type="bibr" target="#b28">30</ref> in MoleculeNet. We tested all the physical chemistry, biophysics and physiology datasets collected by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACS Paragon Plus Environment</head><p>Journal of Medicinal Chemistry that paper except for the PDBbind datasets, which require a representation for the interaction between ligand and receptor and thus is out of the scope of this work (Supplementary Table <ref type="table" target="#tab_2">1</ref>).</p><p>The third collection of datasets comes from quantum mechanical calculations. We tested our model on the largest qm9 dataset <ref type="bibr" target="#b36">38,</ref><ref type="bibr" target="#b37">39</ref> -the most comprehensive accessible quantum mechanical datasetwhich has been tested and benchmarked by previous models <ref type="bibr" target="#b28">30</ref> (Supplementary Table <ref type="table" target="#tab_4">2</ref>).</p><p>The measurements in these datasets can be quantitative or qualitative. Generally, we built regression models for the quantitatively measured datasets and classification models for the qualitatively measured datasets. In addition, we adopted different performance metrics to use in the comparisons with previous benchmarks. Practically, regression models are evaluated by MAE (mean absolute error), MSE (mean-square error) or RMSE (root-mean-square error), and classification models are evaluated by AUC (area under the ROC (receiver operating characteristic) curve) or by the PRC (precision-recall curve). Here, we evaluated all the classification models evaluated by the ROC, except for the models built on MUA (maximum unbiased validation) datasets, in which each task has 30 structure-distinct active compounds and each active compound has 500 structure-similar inactive compounds (the number of inactive compounds is 500 times that of the active compounds). These datasets are also evaluated using the PRC, which is a better metric of an algorithm's performance on highly imbalanced datasets <ref type="bibr" target="#b38">40</ref> . The predictive model can be single-task or multi-task. For the multi-task models, we calculate the performance metrics for each individual task and report their average values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian optimization for hyper-parameter search</head><p>Tuning hyper-parameters is a challenging step in deep neural network modelling, especially for complex network architectures. Here, we use Bayesian optimization (BO) to find the appropriate sets of hyper-parameters due to its efficiency and much reduced time consumption when faced with the increasing flexibility of neural network architectures <ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42</ref> . A BO search mimics the manual search approach using Gaussian process simulation. The intuition underlying BO is to select the next set of hyper-parameters to evaluate based on past results, similar to an expert with domain expertise.</p><p>Thus BO focuses the search process on the most promising sets of hyper-parameters. We use the Python package pyGPGO 43 in this study. For each unrelated dataset, we performed a new BO-based hyper-parameter search in which we optimized the following 6 hyper-parameters simultaneously:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACS Paragon Plus Environment</head><p>Journal of Medicinal Chemistry K (the number of attentive layers for atom embedding), T (the number of attentive layers for molecule embedding), fingerprint dimension, L2 weight decay, learning rate and dropout rate. We used the Matern32 kernel as the covariance function and UCB (upper-confidence bound) as the acquisition strategy <ref type="bibr" target="#b41">44</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Early stopping</head><p>Early stopping <ref type="bibr" target="#b42">45</ref>  Supplementary Figure <ref type="figure" target="#fig_2">2</ref> illustrates the rationale for early termination. The BO algorithm returns the best performance metric on the validation set is returned to the BO algorithm to evaluate the current set of hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training protocol</head><p>Attentive FP was trained with the Pytorch <ref type="bibr" target="#b43">46</ref> framework using the Adam 47 optimizer for gradient descent optimization. The best set of hyper-parameters for each category of tasks obtained from the previous Bayesian optimization process was used to train the most predictive model. We used MSELoss and CrossEntropyLoss, which measure mean squared error and cross entropy as loss functions for the regression tasks and classification tasks, respectively. All the models were trained until an early termination criterion was reached, indicating that the performance improvement had converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of concept experiments with the datasets benchmarked by Duvenaud et al.</head><p>Molecular representations are usually evaluated by their predictive performances. A good molecular representation is expected to extract intrinsic and useful information that improves the predictive performance on a variety of tasks. To evaluate the predictive performance of Attentive</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACS Paragon Plus Environment</head><p>Journal of Medicinal Chemistry FP, we first tested it on a collection of three distinct datasets benchmarked by Duvenaud et al. <ref type="bibr" target="#b24">26</ref> , whose molecular properties span solubility, malaria bioactivity and photovoltaic efficiency.</p><p>Because these data are quantitative, we built regression models using MSE as the evaluation metric as in previous research. BO is used to search for the hyper-parameters for each dataset to minimize the MSE (exemplified in Supplementary Table <ref type="table" target="#tab_7">4</ref>). Using the best set of hyper-parameters, we performed three independent runs with different random seeds to train the model. The results are shown according to their originally reported forms for comparison.  <ref type="table" target="#tab_4">2</ref> and Supplementary Figure <ref type="figure" target="#fig_5">4</ref> summarize the predictive performances of Attentive FP and previous models. Neural FP outperforms the simple neural network models that take ECFP as input features, which demonstrates the potential of graph neural networks in predictive tasks.</p><p>However, as described in the original paper <ref type="bibr" target="#b24">26</ref> , Neural FP has a few limitations, such as the information propagation across the molecular graph and an inability to distinguish stereoisomers.</p><p>In contrast, Weave and MPNN models use an edge network to construct virtual links between every pair of atoms in a molecule. This approach helps to detect implicit interactions between distant atoms and their predictive performance is considerably improved compared to Neural FP. To better distinguish the impacts of different atoms, a GRU (Gated Recurrent Unit) was introduced in the MPNN model to control information flow during iteration, contributing to a further performance improvement. In our work, Attentive FP uses a simple scheme to extract atomic features and topological relationships in the molecular graph under the premise of distinguishing molecules using Page 13 of 33</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACS Paragon Plus Environment</head><p>Journal of Medicinal Chemistry features that are as concise as possible. For example, Attentive FP distinguishes stereoisomers by adding chirality to atomic features and stereo to bond features but eliminates partial charge and ring size features that are empirical or can be derived easily from other features. More importantly, the introduction of the graph attention mechanism allows our model to focus on task-related information from neighbourhoods, while the GRUs and the state update function are helpful in filtering out unrelated information during the iteration process. Together, these network architecture designs contribute to the state-of-the-art predictive performance that Attentive FP achieves on those datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Predicting different bioactivities and properties for drug discovery</head><p>While Attentive FP showed impressive results on the three benchmark datasets from Duvenaud et al., there are many more challenging molecular machine-learning tasks for drug discovery. These include (but are not limited to) bioactivity, physical chemistry and quantum-mechanical properties <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">20</ref> . In this section, we summarize the predictive results of Attentive FP on datasets covering a variety of molecular properties and bioactivities. As recommended previously <ref type="bibr" target="#b28">30</ref> , HIV, BACE, and BBBP datasets were evaluated under scaffold splitting, and the rest datasets were randomly splitted, with the ratio of train:valid:test at 8:1:1. As described above, BO was used for the hyper-parameter search for each dataset. Using the best set of hyper-parameters, three independent runs with different random seeds were performed to train the models, and the results are shown according to their originally reported forms for comparison purposes. (For more information, see Supplementary Table <ref type="table">6</ref>)</p><p>Physical chemistry properties measured by water solubility, solvation free energy and lipophilicity greatly influence the pharmacokinetic profile of a drug, such as its absorption and distribution in the human body. Desirable physical chemistry properties are the premise of a successful drug, and predicting those properties quickly in silico with high precision can significantly reduce the experimental cost of drug development. As reported in The values of the starred models in Table <ref type="table" target="#tab_7">4</ref> are taken from the original papers <ref type="bibr" target="#b28">30</ref> , and can be reproduced in Deepchem. The best values are highlighted in bold. Data is randomly splitted into train, validation and test sets with 80%, 10% and 10% of whole datasets, respectively (training size ~104k). The MAD (Mean Absolute Deviation) of a sample can also be interpreted as the MAE by predicting directly from the mean value of the samples. (U0 here is referred to as internal energy rather than atomization energy that many papers referred to).</p><p>Furthermore, fitting the quantum mechanical calculations with machine learning methods has attracted considerable interest because of the huge computing costs involved when using DFT methods. We tested Attentive FP on the qm9 quantum mechanical datasets benchmarked by MoleculeNet, which include 12 calculated quantum properties for 134k stable small organic molecules composed of up to 9 heavy atoms (C, O, N and F). As shown in Table <ref type="table" target="#tab_7">4</ref>, our graph-based Attentive FP molecular representation scheme outcompetes other models on 10 out of 12 tasks in the qm9 datasets. DTNN showed the best results on task R2 (Electronic Spatial Extent) and task mu (Norm of the dipole moment). Note this test result is quite encouraging because Attentive FP achieves an overall performance comparable to that of the geometry-based models, indicating that Attentive FP implicitly learned 3D conformation-related information. Thus, the molecular representation by attentive FP may provide a valuable workaround that addresses many of the problems involving explorations of the large conformational space of molecules. (For more information, see Supplementary Table <ref type="table">5</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature visualization and interpretation</head><p>The models using Attentive FP achieved state-of-the-art performance on a variety of tests; therefore, it is worth exploring the interpretability issue. Interpretability matters for two main First, we aim to explore why Attentive FPs achieve superior performances compared with those of previous models using conventional chemical descriptors as input. Therefore, we compared the automatically learned hidden features ("fingerprints") with hand-crafted chemical descriptors.</p><p>Specifically, the learned feature for solubility prediction is a 200-dimensional embedding (vector), in which each dimension has its own chemical implication. Here, we calculated the Pearson's correlation coefficient between each feature dimension and the chemical descriptors, for example, the SA (Synthesis Accessibility) Score or Drug Likeness. Learning the hidden environment </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACS Paragon Plus Environment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Journal of Medicinal Chemistry</head><p>In addition to the molecule-level learned features, each atom has its own state vector in each hidden layer for node embedding. To investigate how the atom state vectors evolved during the learning process, we obtained the similarity coefficient between atom pairs by calculating the Pearson correlation coefficient for those state vectors. Then, we plotted heat maps of the atom similarity matrix for the compound to observe the pattern changes. Taking the molecule structure of Iprodione as an example (Fig. <ref type="figure" target="#fig_4">3</ref>), before training, the visual pattern in the heat maps of the similarity matrix show similar levels of chaos across different layers. After training, however, the higher order layer shows a distinct pattern in a specific order. Zooming in on the heat map of Layer 2, we find that the atoms in Iprodione are clearly separated into three clusters-an isopropyl group (atoms 0-2), a dioxoimidazolidine-carboxamide linkage (atoms 3-12, highlighted in pink), and a dichlorophenyl group (atoms 13-20)-which strongly agrees with our chemical intuition regarding the Iprodione structure. Moreover, this pattern clearly suggests that Attentive FP has learned a representation related to molecular solubility. For example, both the isopropyl and the dichlorophenyl groups are hydrophobic and have low polarity, and the correlation between atoms of these two moieties tends to be positive. In contrast, their correlation with the dioxoimidazolidinecarboxamide group, the flexible and polar moiety, is low and negative. Another interesting finding is that atom N3 shows a higher correlation with O11 (Pearson's r ? 0.9) than with its nearer neighbour N6 (Pearson's r ? 0.7). This result is counter-intuitive, because from a graph representation perspective, two nodes with similar node features and topologically close to each other (i.e., N3 and N6 in this case) should also share a higher similarity in the "embedded" hidden space. Revisiting the chemical structure of Iprodione, we infer that the high correlation between N3 and O11, which do not have a similar chemical environment, may represent the presence of intramolecular hydrogen bonds between these two atoms. Clearly, the formation of intramolecular hydrogen bonds has been proven to contribute directly to solubility. This observation demonstrates that Attentive FP has indeed successfully extracted relevant information by learning from a specific task, and it also highlights the advantage of the attention mechanism introduced here for capturing non-local effects among atoms. (For more examples, see Supplementary Figure <ref type="figure" target="#fig_2">2</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning aromaticity</head><p>To further explore how Attentive FP can learn the non-local effects within chemical structures, we constructed a task to predict the number of aromatic atoms in a molecule, which is slightly different from the study of Matlock et al. <ref type="bibr" target="#b45">48</ref> that learns aromaticity and conjugated systems under a series of supervision tasks. In our setting, the learning is more challenging from the standpoint of machine intelligence as it is only supervised by one integer per molecule. A total of 3,945 molecules with 0-40 aromatic atoms were sampled from the PubChem BioAssay dataset for this analysis. All bond features and all atom aromatic features were excluded from the molecular featurization procedure to eliminate any prior knowledge of aromaticity, i.e., the featurization process generated only 38 bits for each atom and no bits for bonds. We also compared our Attentive FP with GCN and MPNN implemented in Deepchem 1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We gratefully acknowledge financial support from the National Natural Science     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 |</head><label>1</label><figDesc>Fig. 1 | Molecular graph representation network architecture. a, Compared to previous graph-based molecular representations, Attentive FP is more differentiable in evaluating the impact of neighbour atoms. b, Overview of Attentive FP network architecture. c, Illustration of graph attention mechanism on atom 3 in aspirin. d, The framework that generates state vector (embedding) for a target atom . ? ? ? ? and are the state vector and attention context vector at time step for atom , respectively. In ? ? ?</figDesc><graphic url="image-3.png" coords="6,90.00,74.69,416.06,524.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>was used to avoid overfitting and reduce training time consumption. When searching hyper-parameters using BO, a training process is required to obtain the best performance that the current set of hyper-parameters can reach. In this training process, we set a maximum epoch of 800, and if the performance metric had not improved in 10 epochs on the training set and in 15 epochs on the validation set, the training process was terminated early. However, these two early termination criteria are empirical and could change based on the data volume and tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 |</head><label>2</label><figDesc>Fig. 2 | Comparing Attentive FP learned features with hand-crafted descriptors in the task of water solubility prediction. a, The correlations of Attentive FP-learned features with two taskunrelated descriptors: SA (Synthesis Accessibility) Score and Drug Likeness. The Pearson's r distributions do not change much before and after training, implying a weak relevance of the SA Score and Drug Likeness to water solubility. Conversely, b, the distribution of correlations to TPSA and LogP are skewed towards the extreme ends after training, suggesting the high relevance of TPSA and LogP to water solubility.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 2a shows that the correlation distribution does not change significantly before and after training, implying that these chemical descriptors have only weak relevance to water solubility. In contrast, the correlation distributions of TPSA (Topological Polar Surface Area) and LogP (lipophilicity) skew towards extreme values after training (Fig. 2b), indicating the high relevance of TPSA and LogP to water solubility. We can clearly observe a growing number of learned features that show high positive correlations with TPSA or high negative correlations with LogP, which conforms to the chemical intuition that TPSA is positively correlated and LogP is negatively correlated with water solubility. As annotated, the most important hidden features of TPSA and LogP have Pearson's r values of 0.95 and -0.911, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 |</head><label>3</label><figDesc>Fig. 3 | Heat maps of the atom similarity matrix for the compound Iprodione. The similarity scores are annotated in the corresponding squares and indicated by a colour scheme. The atoms in Iprodione are automatically separated into three clusters during the learning process.</figDesc><graphic url="image-5.png" coords="20,90.00,417.95,411.84,274.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 |</head><label>4</label><figDesc>Fig. 4 | Learning an aromatic system. a, The predictive performance of GCN, MPNN and Attentive FP for predicting the numbers of aromatic atoms in molecules. The upper histogram indicates the distribution of the number of aromatic atoms. b, The attention weights learned from the Attentive FP model are used to highlight the atoms and conform exactly to the well-defined aromaticity. c, The molecules with adversarial modifications (a small bond or atom change that breaks or forms aromaticity) can be precisely distinguished.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Foundation</head><label></label><figDesc>of China (81773634 to M.Z. and 81430084 to K.C.), National Science &amp; Technology Major Project "Key New Drug Creation and Manufacturing Program", China (Number: 2018ZX09711002 to H.J.), and "Personalized Medicines-Molecular Signaturebased Drug Discovery and Development", Strategic Priority Research Program of the Chinese Academy of Sciences (XDA12050201 to M.Z.), and the open fund of state key laboratory of Pharmaceutical Biotechnology, Nanjing University, China (KF-GN-201706 to H.J.).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 1 |</head><label>1</label><figDesc>Fig. 1 | Molecular graph representation network architecture. a, Compared to previous graph-based molecular representations, Attentive FP is more differentiable in evaluating the impact of neighbour atoms. b, Overview of Attentive FP network architecture. c, Illustration of graph attention mechanism on atom 3 in aspirin. d, The framework that generates state vector (embedding) for a target atom v. h_v^k and C_v^kare the state vector and attention context vector at time step k for atom v, respectively. In higher time steps, target node embedding will include information from further nodes recursively. A darker the dash line implicates higher attention weight of the neighbor node. e, Similar framework that generates the entire molecular graph embedding by assuming a super node connecting to all atoms in molecular graph.152x190mm (300 x 300 DPI)</figDesc><graphic url="image-9.png" coords="31,126.38,111.60,359.25,450.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 2 |</head><label>2</label><figDesc>Fig. 2 | Comparing Attentive FP learned features with hand-crafted descriptors in the task of water solubility prediction. a, The correlations of Attentive FP-learned features with two task-unrelated descriptors: SA (Synthesis Accessibility) Score and Drug Likeness. The Pearson's r distributions do not change much before and after training, implying a weak relevance of the SA Score and Drug Likeness to water solubility. Conversely, b, the distribution of correlations to TPSA and LogP are skewed towards the extreme ends after training, suggesting the high relevance of TPSA and LogP to water solubility. 188x190mm (300 x 300 DPI)</figDesc><graphic url="image-10.png" coords="32,118.50,111.60,375.00,379.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3 |</head><label>3</label><figDesc>Fig. 3 | Heat maps of the atom similarity matrix for the compound Iprodione. The similarity scores are annotated in the corresponding squares and indicated by a colour scheme. The atoms in Iprodione are automatically separated into three clusters during the learning process. 254x172mm (300 x 300 DPI)</figDesc><graphic url="image-11.png" coords="33,118.50,111.60,375.00,254.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 |</head><label>4</label><figDesc>Fig. 4 | Learning an aromatic system. a, The predictive performance of GCN, MPNN and Attentive FP for predicting the numbers of aromatic atoms in molecules. The upper histogram indicates the distribution of the number of aromatic atoms. b, The attention weights learned from the Attentive FP model are used to highlight the atoms and conform exactly to the well-defined aromaticity. c, The molecules with adversarial modifications (a small bond or atom change that breaks or forms aromaticity) can be precisely distinguished. 159x190mm (300 x 300 DPI)</figDesc><graphic url="image-12.png" coords="34,118.50,111.60,375.00,447.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-4.png" coords="19,90.00,101.89,406.42,398.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-6.png" coords="23,90.00,75.46,414.36,492.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-7.png" coords="29,90.00,492.46,408.59,152.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-8.png" coords="30,118.50,111.60,375.00,139.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 | Initial atomic and bond fteatures.</head><label>1</label><figDesc>to encode atomic symbols, and a vector of 6 bits is defined to encode the hybridization state. Of note is that atomic chirality is encoded in three different bits: one indicates whether the atom is a chiral centre, and the other two bits define whether it is in R-or S-form. Moreover, stereo types of double bonds are represented by a feature that distinguishes their potential E/Z configurations.</figDesc><table><row><cell>Page 7 of 33</cell></row></table><note><p><p><p><p><p><p><ref type="bibr" target="#b1">2</ref> </p>, sp</p><ref type="bibr" target="#b2">3</ref> </p>, sp</p><ref type="bibr" target="#b2">3</ref> </p>d, sp 3 d 2 , other] (one-hot) Aromaticity 1 Whether the atom is part of an aromatic system [0/1] (one-hot) Hydrogens 5 Number of connected hydrogens [0,1,2,3,4] (one-hot) Chirality 1 Whether the atom is chiral centre [0/1] (one-hot) Chirality type 2 [R, S] (one-hot) Bond Feature Size Description Bond type 4 [Single, double, triple, aromatic] (one-hot) Conjugation 1 Whether the bond is conjugated [0/1] (one-hot) Ring 1 Whether the bond is in Ring [0/1] (one-hot) Stereo 4 [StereoNone, StereoAny, StereoZ, StereoE] (one-hot)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>That state vector is then subjected to alignment and weighting to obtain the attention context ( ) of atom . The output attention context is fed into the GRU (Gated Recurrent Unit) together ? 0</figDesc><table><row><cell>Page 9 of 33</cell><cell></cell><cell></cell><cell cols="3">Journal of Medicinal Chemistry</cell></row><row><cell>inputs. ?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">with the target atom's current state vector</cell><cell>? 0 ?</cell><cell>, producing the updated state vector</cell><cell>? 1 ?</cell><cell>of atom .</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>). For the molecule</cell></row><row><cell cols="6">embedding, all the atom embeddings are aggregated by assuming a super virtual node that connects</cell></row><row><cell cols="6">all the atoms of the molecule. The attention mechanism is introduced in both the individual-atom</cell></row><row><cell cols="6">embedding and the full-molecule embedding steps. For atom embedding, given a target atom , its ?</cell></row><row><cell cols="2">initial atom state vector</cell><cell>? 0 ?</cell><cell cols="3">is generated by a fully connected layer that includes only the initial</cell></row><row><cell cols="6">atom and bond features. To better represent atom , we introduce a graph attention mechanism at ?</cell></row><row><cell cols="6">each layer that incorporates information from its neighbourhoods</cell><cell>?(?)</cell><cell>. The graph attention</cell></row><row><cell cols="6">mechanism in layer 0 takes the current state vector of target atom</cell><cell>? ? ?</cell><cell>and its neighbours</cell><cell>? ? ?</cell><cell>as</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">ACS Paragon Plus Environment</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 | Prediction performance on three various tasks. Datasets Unit Solubility Log 10 (Mol/L) Malaria bioactivity Log e (?Mol/L)</head><label>2</label><figDesc></figDesc><table><row><cell>Photovoltaic efficiency</cell></row><row><cell>Percent (%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 ,</head><label>3</label><figDesc>AttentiveFP achieves the lowest RMSE on all previous benchmarked physical chemistry datasets, including water solubility (ESOL), solvation free energy (FreeSolv) and lipophilicity (Lipop).</figDesc><table><row><cell>Page 14 of 33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 | Predictive performances on datasets relevant to drug discovery Category Datasets #Compounds Metrics Splitting Previous Best* Attentive FP</head><label>3</label><figDesc></figDesc><table><row><cell>Page 15 of 33</cell><cell></cell><cell cols="3">Journal of Medicinal Chemistry</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ESOL</cell><cell>1,128</cell><cell>RMSE</cell><cell>Random</cell><cell>MPNN: 0.58</cell><cell>0.503</cell></row><row><cell>Physical Chemistry</cell><cell>FreeSolv</cell><cell>643</cell><cell>RMSE</cell><cell>Random</cell><cell>MPNN: 1.15</cell><cell>0.736</cell></row><row><cell></cell><cell>Lipop</cell><cell>4,200</cell><cell>RMSE</cell><cell cols="2">Random Neural FP: 0.655</cell><cell>0.578</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PRC</cell><cell>Random</cell><cell>MultiTask: 0.184</cell><cell>0.221</cell></row><row><cell></cell><cell>MUV</cell><cell>93,127</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ROC</cell><cell>Random</cell><cell>GC: 0.775</cell><cell>0.843</cell></row><row><cell>Bioactivity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HIV</cell><cell>41,913</cell><cell>ROC</cell><cell>Scaffold</cell><cell>SVM: 0.792</cell><cell>0.832</cell></row><row><cell></cell><cell>BACE</cell><cell>1,522</cell><cell>ROC</cell><cell>Scaffold</cell><cell>RF: 0.867</cell><cell>0.850</cell></row><row><cell></cell><cell>BBBP</cell><cell>2,053</cell><cell>ROC</cell><cell>Scaffold</cell><cell>SVM: 0.729</cell><cell>0.920</cell></row><row><cell></cell><cell>Tox21</cell><cell>8,014</cell><cell>ROC</cell><cell cols="2">Random Neural FP: 0.829</cell><cell>0.858</cell></row><row><cell>Physiology or Toxicity</cell><cell>ToxCast</cell><cell>8,615</cell><cell>ROC</cell><cell>Random</cell><cell>Weave: 0.742</cell><cell>0.805</cell></row><row><cell></cell><cell>SIDER</cell><cell>1,427</cell><cell>ROC</cell><cell>Random</cell><cell>RF: 0.684</cell><cell>0.637</cell></row><row><cell></cell><cell>ClinTox</cell><cell>1,491</cell><cell>ROC</cell><cell>Random</cell><cell>Weave: 0.832</cell><cell>0.940</cell></row><row><cell cols="7">Physical chemistry properties and biophysics bioactivities only indicate how likely small</cell></row><row><cell cols="7">molecules are to have an effect on living bodies, while physiology and toxicity datasets represent</cell></row><row><cell cols="7">the effects a molecule has in living bodies, such as blood brain barrier permeability (BBBP), adverse</cell></row><row><cell cols="7">effects (SIDER) or toxicities (Tox21, Toxcast, ClinTox). The Attentive FP models outcompete</cell></row><row><cell cols="7">previous models on the BBBP, SIDER, Tox21, Toxcast and ClinTox datasets-the only exception</cell></row><row><cell cols="6">is SIDER, on which the random forest model still results in a slightly higher performance.</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">ACS Paragon Plus Environment</cell><cell></cell><cell></cell></row></table><note><p><p><p><p>The bioactivity data summarized here describe either direct or indirect effects of chemical compounds towards different targets that are key to drug discovery. Vast amounts of such data have been accumulated in the public domain; thus, learning from these data provides a cost-effective way to perform drug candidate screening. Table</p>3</p>also presents the predictive performances of the classification models on biophysics datasets (MUV, HIV, BACE). Attentive FP also shows noticeable improvements in terms of the ROC metric.</p>Overall, Attentive FP achieves the new state-of-the-art performance on 10 out of 12 drug discovery-related datasets, suggesting that it is a promising molecular representation scheme for</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Predictive performances on the qm9 datasets quantum properties (MAE)</figDesc><table><row><cell></cell><cell></cell><cell>Sample</cell><cell cols="2">Geometry-based</cell><cell></cell><cell cols="2">Graph-based</cell><cell></cell></row><row><cell>Task</cell><cell>Unit</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MAD</cell><cell cols="3">CM* DTNN* ECFP*</cell><cell cols="3">GC* MPNN* AttentiveFP</cell></row><row><cell>mu</cell><cell>D</cell><cell>1.189</cell><cell>0.519</cell><cell>0.244</cell><cell>0.602</cell><cell>0.583</cell><cell>0.358</cell><cell>0.451</cell></row><row><cell>alpha</cell><cell>Bohr^3</cell><cell>6.299</cell><cell>0.85</cell><cell>0.95</cell><cell>3.1</cell><cell>1.37</cell><cell>0.89</cell><cell>0.492</cell></row><row><cell>HOMO</cell><cell>Hartree</cell><cell cols="3">0.016 0.00506 0.00388</cell><cell cols="3">0.0066 0.00716 0.00541</cell><cell>0.00358</cell></row><row><cell>LUMO</cell><cell>Hartree</cell><cell cols="6">0.039 0.00645 0.00513 0.00854 0.00921 0.00623</cell><cell>0.00415</cell></row><row><cell>gap</cell><cell>Hartree</cell><cell>0.040</cell><cell>0.0086</cell><cell>0.0066</cell><cell>0.01</cell><cell>0.0112</cell><cell>0.0082</cell><cell>0.00528</cell></row><row><cell>R2</cell><cell cols="2">Bohr^2 202.017</cell><cell>46</cell><cell>17</cell><cell>125.7</cell><cell>35.9</cell><cell>28.5</cell><cell>26.839</cell></row><row><cell>ZPVE</cell><cell>Hartree</cell><cell cols="6">0.026 0.00207 0.00172 0.01109 0.00299 0.00216</cell><cell>0.00120</cell></row><row><cell>U0</cell><cell>Hartree</cell><cell>31.072</cell><cell>2.27</cell><cell>2.43</cell><cell>15.1</cell><cell>3.41</cell><cell>2.05</cell><cell>0.898</cell></row><row><cell>U</cell><cell>Hartree</cell><cell>31.072</cell><cell>2.27</cell><cell>2.43</cell><cell>15.1</cell><cell>3.41</cell><cell>2</cell><cell>0.893</cell></row><row><cell>H</cell><cell>Hartree</cell><cell>31.072</cell><cell>2.27</cell><cell>2.43</cell><cell>15.1</cell><cell>3.41</cell><cell>2.02</cell><cell>0.893</cell></row><row><cell>G</cell><cell>Hartree</cell><cell>31.072</cell><cell>2.27</cell><cell>2.43</cell><cell>15.1</cell><cell>3.41</cell><cell>2.02</cell><cell>0.893</cell></row><row><cell cols="2">Cv cal/mol/K</cell><cell>3.204</cell><cell>0.39</cell><cell>0.27</cell><cell>1.77</cell><cell>0.65</cell><cell>0.42</cell><cell>0.252</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ConclusionAn ambitious goal for drug design is to read properties directly from chemical structure, however, it remains an open question as to what extent and how accurately information can be extracted. Other related tasks, such as reaction outcome and yield predictions, retrosynthesis analysis and synthetic planning, can also gain essential benefits from better molecular representations for property prediction. Molecular representation with a deep learning approach provides a viable option, which can not only help establish a predictive model for molecular properties but also recreate knowledge from existing data and even form new theories to describe chemical systems<ref type="bibr" target="#b47">50</ref> . In this direction, much more effort is still required not only to improve the predictive power of the resulting model, but also to interpret the model rather than simply accepting the "black box" results. Feature visualization, instructions and code for Attentive FP are available at ACS Paragon Plus Environment Journal of Medicinal Chemistry MSE, mean of squared error; RMSE, root of mean squared error; AUC, area under curve; ROC, receiver operating characteristics curve; PRC, precise-recall curve.</figDesc><table><row><cell>ASSOCIATED CONTENT</cell></row><row><cell>Supporting Information.</cell></row></table><note><p>In this work, we proposed Attentive FP, a small molecule representation framework based on a graph neural network. The adoption of graph attention mechanisms at both the atom and molecule levels allows this new representation framework to learn both local and non-local properties of a given chemical structure. Accordingly, it captures subtle substructure patterns such as intramolecular hydrogen bonding and aromatic systems, contributing to its excellent learning capability for a wide range of different molecular properties. Moreover, inverting the Attentive FP model by extracting the hidden layers or attention weights provides access to the model's interpretation, which will help chemists gain insights into the skyrocketing volume and complexity of drug discovery data. https://github.com/OpenDrugAI/AttentiveFP More detailed model tests and validations are in separate supporting information file. (PDF) Molecular formula strings (CSV)</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>ACS Paragon Plus Environment</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>ACS Paragon Plus EnvironmentJournal of Medicinal Chemistry</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="funder">ACS Paragon Plus Environment Journal of Medicinal Chemistry ACS Paragon Plus Environment Journal of Medicinal Chemistry ACS Paragon Plus Environment Journal of Medicinal Chemistry</rs></p></div>
			</div>
			<div type="funding">
<div><head>Table of Contents graphic</head><p>Page 28 of 33 <rs type="funder">ACS Paragon Plus Environment Journal of Medicinal Chemistry TOC</rs> Graphic 252x93mm (300 x 300 DPI) Page 29 of 33 <rs type="funder">ACS Paragon Plus Environment Journal of Medicinal Chemistry</rs></p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR INFORMATION Corresponding Author</head><p>* E-mail: hljiang@simm.ac.cn Phone: +86-21-50806600-1303. (Hualiang Jiang) *.E-mail: myzheng@simm.ac.cn Phone: +86-21-50806600-1308 (Mingyue Zheng).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORCID</head><p>Mingyue Zheng: 0000-0002-3323-3092</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head><p>Z. Xiong, M. Zheng, K. Chen and H. Jiang conceived and designed the algorithm and visualization, as well as wrote the paper. D. Wang, X. Liu, F. Zhong, X. Wan, Z. Li, X. Li and X. Luo performed the computational experiments with benchmarked datasets. All authors read and approved the final manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notes</head><p>The authors declare no competing financial interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head><p>FP, fingerprints; GAT, graph attention networks; HPC, high performance computing; GRU, gated recurrent unit; GNNs, graph neural network; GCN, graph convolutional network; MPNN, message passing neural network; DTNN, deep tensor neural network; CM, coulomb matrix; SA, synthesis accessibility; TPSA, topological polar surface area; BO, Bayesian optimization; MAD, mean of absolute deviation; MAE, mean of absolute error;</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mind and Machine in Drug Design</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="128" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Gaulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hersey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nowotka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Bento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mendez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutowo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Bellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cibri?n-Uhalte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="945" to="D954" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modelling the Tox21 10 K Chemical Profiles for in Vivo Toxicity Prediction and Mechanism Characterization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakamuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Attene-Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simeonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<date type="published" when="2016">2016, 7, 10425</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The SIDER Database of Drugs and Side Effects</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Letunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1075" to="D1079" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Thiessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Bryant</surname></persName>
		</author>
		<author>
			<persName><surname>Pubchem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Integrated Platform of Small Molecules and Biological Activities</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">217</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PubChem&apos;s BioAssay Database</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">O</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Karapetyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dracheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Shoemaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="400" to="D412" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Molecular and Materials Science</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low Data Drug Discovery with One-Shot Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feedback GAN for DNA Optimizes Protein Functions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Retrosynthetic Reaction Prediction Using Neural Sequence-to-Sequence Models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kawthekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Luu Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sloane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Cent Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1103</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Planning Chemical Syntheses with Deep Neural Networks and Symbolic AI</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H S</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Preuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">555</biblScope>
			<biblScope unit="page">604</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reconstructing Quantum States with Generative Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carrasquilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Torlai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Melko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Aolita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting Reaction Performance in C-N Cross-Coupling Using Machine Learning</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Ahneman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Dreher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6385</biblScope>
			<biblScope unit="page" from="186" to="190" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Human-Level Concept Learning through Probabilistic Program Induction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="page">1332</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reinforcement Learning in Artificial and Biological Systems</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Neftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Averbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">QSAR -How Good Is It in Practice? Comparison of Descriptor Sets on an Unbiased Cross Section of Corporate Data Sets</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gedeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rohde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="1924">2006. 1924</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inverse QSPR/QSAR Analysis for Chemical Structure Generation (from y to x)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Funatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">286</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Braga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F B</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Muratov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fourches</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tropsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tuning HERG Out: Antitarget QSAR Models for Drug Development</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1399" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantum-Chemical Descriptors in QSAR/QSPR Studies</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Lobanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Katritzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem. Rev</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1027" to="1044" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Interpretation and Temporal Evaluation of a Global QSAR of HERG Electrophysiology Screening Data</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Gavaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Arnby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Blomberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Strandlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><surname>Development</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Aided Mol. Des</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="189" to="206" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Quantum-Chemical Insights from Deep Tensor Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Commun</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unified Representation of Molecules and Crystals for Machine Learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06439</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical Modeling of Molecular Energies Using a Deep Neural Network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lubbers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Barros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Phys</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241715</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SchNet -A Deep Learning Architecture for Molecules and Materials</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Chem. Phys</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">241722</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.09292</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional Embedding of Attributed Molecular Graphs for Physical Property Prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">1757</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Molecular Graph Convolutions: Moving Beyond Fingerprints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Comput-Aided Mol Des</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">595</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph Convolution: A High-Order and Adaptive Approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09916</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MoleculeNet: A Benchmark for Molecular Machine Learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chem Sci</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Rectified Activations in Convolutional Network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00853</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs: Methods and Applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng Bull</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Light Gated Recurrent Units for Speech Recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10225</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Enumeration of 166 Billion Organic Small Molecules in the Chemical Universe Database GDB-17</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page">2864</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Von Lilienfeld</surname></persName>
		</author>
		<title level="m">Quantum Chemistry Structures and Properties of 134 Kilo Molecules. Sci Data</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Relationship Between Precision-Recall and ROC Curves</title>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goadrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Machine Learning</title>
		<meeting>the 23rd International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Nevada, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Hyperparameters Optimization in Deep Convolutional Neural Network / Bayesian Approach with Gaussian Process Prior</title>
		<author>
			<persName><forename type="first">P</forename><surname>Murugan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07233</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Parallel Gaussian Process Optimization with Upper Confidence Bound and Pure Exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Contal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Buffoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vayatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases; Blockeel</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Kersting</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Nijssen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>?elezn?</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="225" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">On Early Stopping in Gradient Descent Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Caponnetto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constr. Approx</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="289" to="315" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://github.com/pytorch/pytorch" />
		<imprint>
			<date type="published" when="2017-10-08">Oct 8, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv E-Prints</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning a Local-Variable Model of Aromatic and Conjugated Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Matlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Cent. Sci</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Learning for the Life Sciences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Shroff Publishers &amp; Distributors Pvt. Ltd: Mumbai, India</publisher>
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning Atoms for Materials Discovery</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Natl. Acad. Sci</title>
		<meeting>Natl. Acad. Sci</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="6411" to="E6417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
