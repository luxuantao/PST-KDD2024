<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning with Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-10-22">22 Oct 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chih-Hui</forename><surname>Ho</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<email>nvasconcelos@ucsd.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">34th Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2020)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning with Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-10-22">22 Oct 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2010.12050v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning (CL) is a popular technique for self-supervised learning (SSL) of visual representations. It uses pairs of augmentations of unlabeled training examples to define a classification task for pretext learning of a deep embedding. Despite extensive works in augmentation procedures, prior works do not address the selection of challenging negative pairs, as images within a sampled batch are treated independently. This paper addresses the problem, by introducing a new family of adversarial examples for constrastive learning and using these examples to define a new adversarial training algorithm for SSL, denoted as CLAE. When compared to standard CL, the use of adversarial examples creates more challenging positive pairs and adversarial training produces harder negative pairs by accounting for all images in a batch during the optimization. CLAE is compatible with many CL methods in the literature. Experiments show that it improves the performance of several existing CL baselines on multiple datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep networks have enabled significant advances in many machine learning tasks over the last decade. However, this usually requires supervised learning, based on large and carefully curated datasets. Self-supervised learning (SSL) <ref type="bibr" target="#b24">[25]</ref> aims to alleviate this limitation, by leveraging unlabeled data to define surrogate tasks that can be used for network training. Early advances in SSL were mostly due to the introduction of many different surrogate tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b32">33]</ref>, including solving image <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b26">27]</ref> or video <ref type="bibr" target="#b2">[3]</ref> puzzles, filling image patches <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b43">44]</ref> or discriminating between image rotations <ref type="bibr" target="#b17">[18]</ref>. Recently, there have also been advances in learning techniques specifically tailored to SSL, such as contrastive learning (CL) <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b38">39]</ref>, which is the focus of this paper. CL is based on a surrogate task that treats instances as classes and aims to learn an invariant instance representation. This is implemented by generating a pair of examples per instance, and feeding them through an encoder, which is trained with a constrastive loss. This encourages the embeddings of pairs generated from the same instance, known as positive pairs, to be close together and embeddings originated from different instances, known as negative pairs, to be far apart.</p><p>The design of positive pairs is one of the research focuses of CL <ref type="bibr" target="#b4">[5]</ref>. These pairs can include data from one or two modalities. For single-modality approaches, a common procedure is to rely on data augmentation techniques. For example, instances from image datasets are frequently subject to transformations such as rotation, color jittering, or scaling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b38">39]</ref> to generate the corresponding pairs. This type of data augmentation has been shown critical for the success of CL, with different augmentation approaches having a different impact on SSL performance <ref type="bibr" target="#b11">[12]</ref>. For video datasets, positive pairs are usually derived from temporal coherence constraints <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21]</ref>. For multi-view data, augmentations can be more elaborate. For example, <ref type="bibr" target="#b62">[63]</ref> considers augmentations of color channels, depth or surface normal and shows that the performance of CL improves as the augmentations increase in diversity. Multi-modal CL approaches tend to rely on audio and video from a common video clip to design positive pairs <ref type="bibr" target="#b51">[52]</ref>. In general, CL benefits from definitions of positive pairs that pose a greater challenge to the learning of the invariant instance representation.</p><p>Unlike the plethora of positive pair selection proposals, the design of negative pairs has received less emphasis in the CL literature. Since CL resembles approaches such as noise contrastive estimation (NCE) <ref type="bibr" target="#b19">[20]</ref> and N-pair <ref type="bibr" target="#b58">[59]</ref> losses, it can leverage hard negative pair mining schemes proposed for these approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b58">59]</ref>. However, because they treat each instance independently, previous SSL works do not consider CL algorithms that select difficult negative pairs within a batch. This is unlike CL methods based on metric learning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62]</ref>, which seek to construct batches with challenging negative samples.</p><p>In this work, we seek a general algorithm for the generation of diverse positive and challenging negative pairs. This is framed as the search for instance augmentation sets that induce the largest optimization cost for CL. A natural approach to synthesize these sets is to leverage adversarial examples <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b75">76]</ref>, which are crafted to attack the network and can thus be seen as the most challenging examples that it can process. We note that the goal is not to enhance robustness to adversarial attacks, but to produce a better representation for SSL. This is in line with recent studies in the adversarial literature, showing that adversarial examples can be used to improve supervised <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b76">77]</ref> and semi-supervised <ref type="bibr" target="#b40">[41]</ref> learning. We explore whether the same benefits can ensue for SSL. One difficulty is, however, that no attention has been previously devoted to the design of adversarial attacks for SSL, where no class labels are available. In fact, for SSL embeddings trained by CL, the classical definition of adversarial attack does not even apply, since CL operates on pairs of examples. We show, however, that it is possible to leverage the interpretation of CL as instance classification to produce a sensible generalization of classification attacks to the CL problem. The new attacks are then combined with recent techniques from the adversarial literature <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b76">77]</ref>, which treat adversarial training as multi-domain training, to produce more invariant representations for SSL.</p><p>Overall, the paper makes three contributions. First, we show that adversarial data augmentation can be used to improve the performance of SSL learning. Second, we propose a novel procedure for training Contrastive Learning with Adversarial Examples (CLAE) for SSL models. Unlike the attacks classically developed in the supervised learning literature, the new attacks produce pairs of examples that account for both the positive and negative pairs in a batch to maximize contrastive loss. To the best of our knowledge neither the use of attacks to improve SSL nor the design of adversarial examples with this property have been previously discussed in the literature. Finally, extensive experiments demonstrate that <ref type="bibr" target="#b0">(1)</ref> adversarial examples can indeed be leveraged to improve CL, and (2) CLAE boosts the performance of several CL baselines across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Since this work focuses on image classification tasks, our survey of previous work concentrates on contrastive learning (CL) and adversarial examples for image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Contrastive learning</head><p>Contrastive learning has been widely used in the metric learning literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b53">54]</ref> and, more recently, for self-supervised learning (SSL) <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b22">23]</ref>, where it is used to learn an encoder in the pretext training stage. Under the SSL setting, where no labels are available, CL algorithms aim to learn an invariant representation of each image in the training set. This is implemented by minimizing a contrastive loss evaluated on pairs of feature vectors extracted from data augmentations of the image. While most CL based SSL approaches share this core idea, multiple augmentation strategies have been proposed <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b38">39]</ref>. Typically, augmentations are obtained by data transformation (i.e. rotation, cropping, random grey scale and color jittering) <ref type="bibr" target="#b77">[78,</ref><ref type="bibr" target="#b11">12]</ref>, but there have also been proposals to use different color channels, depth, or surface normals as the augmentations of an image <ref type="bibr" target="#b62">[63]</ref>. Another approach is to use an augmentation dictionary composed of the embedding vectors from the previous epoch <ref type="bibr" target="#b73">[74]</ref> or obtained by forwarding an image through a momentum updated encoder <ref type="bibr" target="#b21">[22]</ref>. This diversity of approaches to the synthesis of augmentations reflects the critical importance of using semantically similar example pairs in CL <ref type="bibr" target="#b4">[5]</ref>. This has also been studied empirically in <ref type="bibr" target="#b11">[12]</ref>, showing that stronger data augmentations improve CL performance. Despite this wealth of augmentation proposals for SSL, most CL algorithms fail to mine hard negative pairs or relate the image instances within a batch. While <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b65">66]</ref> have mentioned the importance of selecting negative pairs, they do not propose a systematic algorithm to do this. Since CL is inspired by the noise contrastive estimation (NCE) <ref type="bibr" target="#b19">[20]</ref> and N-pair <ref type="bibr" target="#b58">[59]</ref> loss methods from metric learning, it inherits the well known difficulties of hard negative mining in this literature <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">60]</ref>. For metric learning methods <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b58">59]</ref>, the number of possible positive and negative pairs increases dramatically (for example, cubically when the triplet loss is used <ref type="bibr" target="#b53">[54]</ref>) as the dataset increases. A solution used by NCE is to draw negative samples from a noise distribution that treats all negative samples equally <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Unlike all these prior efforts, this work proposes to use "adversarial augmentations" as challenging training pairs that maximize the contrastive loss. However, unlike hard negative mining in metric learning, no class labels are provided in SSL. Hence, the consideration of how all images in the batch relate to each other is necessary for generating hard negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Adversarial examples</head><p>Adversarial examples are created from clean examples to produce adversarial attacks that induce a network in error <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19]</ref>. They have been used in many supervised learning scenarios, including image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b60">61]</ref>, object detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b81">82]</ref> and segmentation <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b3">4]</ref>. Typically, to defend against such attacks and increase network robustness, the network is trained with both clean and adversarial examples, a process referred as adversarial training <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b76">77]</ref>. It is also possible to leverage SSL to increase robustness against unseen attacks <ref type="bibr" target="#b44">[45]</ref>. While adversarial training is usually effective as a defense mechanism, there is frequently a decrease in the accuracy of the classification of clean examples <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>This effect has been attributed to overfitting to the adversarial examples <ref type="bibr" target="#b34">[35]</ref> but remains somewhat of a paradox, since the increased diversity of adversarial examples could, in principle, improve standard training <ref type="bibr" target="#b23">[24]</ref>, e.g. by enabling models trained on adversarial examples to generalize better to unseen data <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b35">36]</ref>. In summary, while adversarial examples could assist learning, it remains unclear how to do this. Recently, <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b76">77]</ref> have made progress along this direction, by introducing a procedure, denoted AdvProp, that treats clean and adversarial examples as samples from different domains, and uses a different set of batch normalization (BN) layers for each domain. This aims to align the statistics of the embedding of clean and adversarial samples, such that both can contribute to the network learning, and has been previously shown successful for multi-domain classification problems <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>The proposed framework CLAE is inspired by recent advances in the adversarial example literature, yet parallel to them. Unlike these methods, we aim to leverage the strength of adversarial example for SSL, where no labels are available, and the focus on pairs rather than single examples requires an altogether different definition of adversaries. Our aims is to use adversarial training to compensate the limitation of current CL algorithms, by both generating challenging positive pairs and mining effective hard negative pairs for the optimization of the contrastive loss. Note that the goal is to produce better embeddings for CL, rather than robustifying CL embeddings against attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Leveraging adversarial examples for improved contrastive learning</head><p>In this section, we introduce the approach proposed to create adversarial examples for CL, and a novel training scheme CLAE that leverages these examples for improved contrastive learning (CL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Supervised learning</head><p>A classifier maps example x ∈ X into label y ∈ {1, . . . , C}, where C is a number of classes. A deep classifer is implemented by the combination of an embedding f θ (x) of parameters θ and a softmax regression layer that predicts the posterior class probabilities</p><formula xml:id="formula_0">P Y |X (i|x) = e w T i f θ (x) k e w T k f θ (x) ,<label>(1)</label></formula><p>where w i is the vector of classification parameters of class i. Given a training set D = {x i , y i }, the parameters θ and W = {w i } C i=1 are learned by minimizing the risk R = i L(x i , y i ; W, θ) defined by the cross-entropy loss</p><formula xml:id="formula_1">L ce (x, y; W, θ) = − log e w T y f θ (x) k e w T k f θ (x) .<label>(2)</label></formula><p>Given the learned classifier, the untargeted adversarial example x adv of a clean example x is</p><formula xml:id="formula_2">x adv = x + δ s.t. ||δ|| p &lt; and arg max i w T i f θ (x) = arg max i w T i f θ (x adv ),<label>(3)</label></formula><p>where δ is an adversarial perturbation of L p norm smaller than . The optimal perturbation for x is usually found by maximizing the cross-entropy loss, i.e.</p><formula xml:id="formula_3">δ * = arg max δ L ce (x + δ, y; W, θ) s.t. ||δ|| p &lt; ,<label>(4)</label></formula><p>although different algorithms <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b9">10]</ref> use different strategies to solve this optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contrastive learning</head><p>In SSL, the dataset is unlabeled, i.e. U = {x i }, and each example x is mapped into an example pair (x p , x q ). In this work, we consider applications where x is an image and the pair is generated by data augmentation. This consists of applying a transformation p (q) in some set of transformations T (e.g. spatial transformations, color transformations, etc.) to x, to produce the augmentation x p (x q ). CL seeks to learn an invariant representation of image x i by minimizing the risk defined by the loss</p><formula xml:id="formula_4">L cl (x pi i , x qi i ; θ, T ) = − log exp(f θ (x pi i ) T f θ (x qi i )/τ ) B k=1 exp(f θ (x p k k ) T f θ (x qi i )/τ ) , p i , q i ∼ T<label>(5)</label></formula><p>where f is an embedding parameterized by θ, τ is the temperature, B is the batch size and x pi i , x qi i are augmentations of x i under transformations p i , q i randomly sampled from T . Previous works on CL have considered many possibilities for the set of transformations T . While <ref type="bibr" target="#b11">[12]</ref> has shown that the choice of T has a critical role on SSL performance, most prior works do not give much consideration to the individual choice of p i and q i , which are simply uniformly sampled over T . In this work, we seek to go beyond this and select optimal transformations for each image x i . More precisely, we seek augmentations that maximize the risk defined by the loss of (5), i.e.</p><formula xml:id="formula_5">{p * i , q * i } = arg max {pi,qi}∈T i L cl (x pi i , x qi i ; θ, T ).<label>(6)</label></formula><p>This is, in general, an ill-defined problem since, for each example x i , what matters is the difference between the two transformations, not their absolute values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial augmentation</head><p>This ambiguity can be eliminated by fixing one of the transformations of (5), i.e. solving instead</p><formula xml:id="formula_6">{p * i } = arg max {pi}∈T i L cl (x pi i , x qi i ; θ, T ), q i ∼ T<label>(7)</label></formula><p>for a given set of {q i } sampled randomly from T . However, it is usually difficult to search over the set T efficiently. Instead, we proposed to replace x pi i by an adversarial perturbation of x qi i , i.e. find</p><formula xml:id="formula_7">x ri i = arg max x∈A(x q i i ) L cl (x, x qi i ; θ, T ), q i ∼ T<label>(8)</label></formula><p>where A(x qi i ) is a set of adversarial perturbations of x qi i , defined by</p><formula xml:id="formula_8">A(x) = {x |x = x + δ, ||δ|| p &lt; }.<label>(9)</label></formula><p>In summary, given a set of transformations T and a set of augmentations x qi i , the goal is to learn the adversaries x ri i = x qi i + δ * i , by finding the perturbations δ * i that lead to the most diverse positive pairs (x ri i , x qi i ). The rationale is that the use of these pairs in (5) increases the challenge of unsupervised learning, encouraging the learning algorithm to produce a more invariant representation.  Algorithm 1 Pseudocode of contrastive learning with adversarial example (CLAE) in a batch <ref type="bibr" target="#b14">(15)</ref> with {x qi i } B i=1 and W q to obtain W * 5: Compute L aug of ( <ref type="formula" target="#formula_16">16</ref>) with {x qi i } B i=1 and W p , and L adv of ( <ref type="formula" target="#formula_16">16</ref>) with {x qi i } B i=1 and W * 6: Minimize ( <ref type="formula" target="#formula_16">16</ref>) with hyperparameter α</p><formula xml:id="formula_9">1: Input X = {x i } B i=1 , AUG:= Data Augmentation, Hyperparameter α 2: W p = {x pi i } B i=1 = AUG(X ) 3: W q = {x qi i } B i=1 = AUG(X ) 4: Compute</formula><p>To optimize (8), we start by noting that the contrastive loss of ( <ref type="formula" target="#formula_4">5</ref>) can be written as the cross-entropy loss of (2)</p><formula xml:id="formula_10">L cl (x pi i , x qi i ; θ, T ) = − log exp(w T i f θ (x qi i )) B k=1 exp(w T k f θ (x qi i )) = L ce (x qi i , i; W p , θ)<label>(10)</label></formula><p>by defining the classifier parameters W p as w k = f θ (x p k k )/τ . As above, we can replace each x pi i by the optimal adversarial perturbation x ri i ∈ A(x qi i ), to obtain an optimal set of perturbed parameters</p><formula xml:id="formula_11">W * = {f θ (x r k k )/τ } by solving {x r k k } = arg max {x k ∈A(x q k k )} i L ce (x qi i , i; {f θ (x k )/τ }, θ), q i ∼ T .<label>(11)</label></formula><p>Note that this requires the determination of the optimal adversarial perturbation x ri i for the augmentation x qi i of each example x i in the batch. Using the definition of adversarial set of ( <ref type="formula" target="#formula_8">9</ref>) in <ref type="bibr" target="#b10">(11)</ref>, results in the optimization</p><formula xml:id="formula_12">{δ * k } = arg max {δ k } i L ce (x qi i , i; {f θ (x q k k + δ k )/τ }, θ) s.t. ||δ k || p &lt; , q i ∼ T .<label>(12)</label></formula><p>This is an optimization similar to (4), but with a significant difference. While in (4) δ is a perturbation of the classifier input, in <ref type="bibr" target="#b11">(12)</ref> it is a perturbation of the classifier weights. This implies that δ k appears in the denominator of (10) for all x qi i in the batch and forces the optimization to account for all images simultaneously. In result, the embedding is more strongly encouraged to bring together the positive pairs (x ri i , x qi i ) and separate all perturbations of different examples, i.e. the optimization seeks both challenging positive and negative pairs, performing hard negative mining as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">FSGM attacks for unsupervised learning</head><p>The optimization of (12) can be performed for any set of transformations T . Hence, the procedure can be applied to most CL methods in the literature. The optimization can also be implemented with most adversarial techniques in the literature. In this work, we rely on untargeted attacks with the popular fast gradient sign method (FGSM) <ref type="bibr" target="#b18">[19]</ref>. For supervised learning, an untargeted FGSM attack consists of</p><formula xml:id="formula_13">x adv = x + δ = x + sign(∇ x L ce (x, y; W, θ)) ||δ|| 2 &lt; ,<label>(13)</label></formula><p>where L ce is the cross-entropy loss of (2). Similarly, to obtain δ * k of ( <ref type="formula" target="#formula_12">12</ref>), the first order derivative of ( <ref type="formula" target="#formula_12">12</ref>) is computed at x q k k to obtain x r k k with</p><formula xml:id="formula_14">x r k k = x q k k + δ * k = x q k k + sign ∇ x q k k i L ce (x qi i , i; {f θ (x q k k )/τ }, θ)<label>(14)</label></formula><formula xml:id="formula_15">= x q k k + sign ∇ x q k k i L ce (x qi i , i; W q , θ) , ||δ k || 2 &lt;<label>(15)</label></formula><p>Note that, due to this, the optimal set of adversarial augmentations W * = {f θ (x r k k )/τ } takes into consideration the relationship between all instances within the sampled batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Adversarial training with contrastive loss</head><p>To perform adversarial training, we adopt the training scheme of AdvProp <ref type="bibr" target="#b74">[75]</ref>, which uses two separate batch normalization (BN) layers for clean and adversarial examples. Unlike <ref type="bibr" target="#b74">[75,</ref><ref type="bibr" target="#b76">77]</ref>, we set the momentum for the two BN layers differently. The momentum of the BN layer associated with clean examples is fixed to the value used by the original CL algorithm, while a larger momentum is used empirically for the BN layer associated with adversarial examples. The overall loss function is obtained by combining ( <ref type="formula" target="#formula_10">10</ref>) and ( <ref type="formula" target="#formula_15">15</ref>),</p><formula xml:id="formula_16">arg min θ B i=1 L ce (x qi i , i; W p , θ) + α B i=1 L ce (x qi i , i; W * , θ) = arg min θ L aug + αL adv ,<label>(16)</label></formula><p>where α balances between contrastive loss L aug parametrized with W p and L adv parametrized with W * . The proposed procedure for CLAE is summarized in Algorithm 1 and visualized in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Given augmentations x qi i , adversarial examples are created by backpropagating the gradients of the contrastive loss to the network input. Contrastive loss terms are then computed for standard augmentation pairs and pairs composed by the augmentations x qi i and their adversarial examples, and combined with <ref type="bibr" target="#b15">(16)</ref> to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Generalization to other contrastive learning approaches</head><p>While Algorithm 1 is based on the plain contrastive loss of ( <ref type="formula" target="#formula_4">5</ref>), CLAE can be generalized to many other CL approaches in the literature. For example, UEL <ref type="bibr" target="#b77">[78]</ref> uses an extra objective function to discourage different images from being recognized as the same instance. SimCLR <ref type="bibr" target="#b11">[12]</ref> uses a projection head to avoid loss of information during pretext training and discards the projection head when optimizing the downstream task. To generalize Algorithm 1 to these approaches, it suffices to replace the plain contrastive loss with the loss functions on which they are based.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we discuss an experimental evaluation of adversarial contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Experiments are performed on CIFAR10 <ref type="bibr" target="#b28">[29]</ref>, CIFAR100 <ref type="bibr" target="#b28">[29]</ref> or tinyImagenet <ref type="bibr" target="#b0">[1]</ref>, using three different contrastive loss baselines: the loss of (5) (denoted as "Plain"), UEL <ref type="bibr" target="#b77">[78]</ref> and SimCLR <ref type="bibr" target="#b11">[12]</ref>. Unless otherwise noted, a Resnet18 encoder is trained using Algorithm 1 with α = 1, standard Pytorch augmentation, and an adversarial batchnorm momentum of 0.01 <ref type="foot" target="#foot_0">1</ref> . Two evaluation protocols are used, both based on a downstream classification task using features extracted by the learned encoder. These are implemented with a k = 200 nearest neighbor (kNN) classifier, and a logistic regression layer (LR). The encoder is trained with batch size 256 (128) and LR is trained for 1000 (200) epochs for CIFAR10 and CIRFAR100 (tinyImagenet). See supplementary for more details. Clean Adv.</p><p>Diff. Clean Adv. Diff.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Influence of adversarial example</head><p>In this section, we study the effect of adversarial examples on both the SSL surrogate and downstream tasks. We compare the adversarial attacks of Algorithm 1 to random additive perturbations of the same magnitude. Fig. <ref type="figure">3</ref>(a) shows the magnitude of the contrastive loss of the pretext task, on CIFAR10. For a given perturbation strength , adversarial perturbations (blue) elicit a larger loss than random ones (red). Fig. <ref type="figure">3</ref>(b) shows that, when the adversarial augmentations are fed into the downstream classification model, they elicit a larger cross entropy loss than random noise. This shows that adversarial augmentation produces more challenging pairs than random perturbations. Examples of adversarial augmentation are visualized in Fig. <ref type="figure">2</ref>.</p><p>Nevertheless, there are some significant differences to the typical behaviour of adversarial examples in the supervised setting. First, the effect of adversarial attacks is weaker for SSL. In the SSL setting, adversarial examples only degrade the downstream classification accuracy by 20%. This is much weaker than previously reported for the supervised setting, using the adversarial examples of (4). Second, as illustrated in Fig. <ref type="figure" target="#fig_2">4</ref>, the statistics of the batch normalization layers differ less between clean and adversarial examples than reported for supervised learning in <ref type="bibr" target="#b76">[77]</ref>. Both of these observations are explained by the fact that, while the classifier parameters of (2) remain stable across training, those of (10) vary between batches. This creates uncertainty in the perturbation direction of <ref type="bibr" target="#b14">(15)</ref>, decreasing the differences between clean and adversarial attacks under the SSL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to contrastive learning baselines</head><p>In this section, we investigate the gains of using CLAE framework for SSL, and the consistency of these gains across CL approaches. Table <ref type="table" target="#tab_1">1</ref> shows the downstream classification accuracy for the two classifiers and three CL methods considered in these experiments. Note that adversarial training reduces to these methods when = 0, in which case no adversarial examples are used. It can be seen that adversarial training improves the performance of all CL algorithms on all datasets, with a gain that is consistent across downstream classifiers. While best performance is achieved by using = 0.03 in (15), = 0.07 still beats the baseline in most cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>An ablation study was conducted using the combination of SimCLR <ref type="bibr" target="#b11">[12]</ref> and LR classifier. The study was performed on both CIFAR100 and tinyImagenet, with qualitatively identical results. We present CIFAR100 results here and tinyImagenet on the supplementary.</p><p>Batch size Fig. <ref type="figure" target="#fig_3">5</ref>(a) shows that larger batch sizes improve the performance of both baseline and adversarial training. While this is consistent with the conclusions of previous studies on the importance of batch size <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>, adversarial training makes large batch sizes less critical. Besides outperforming the baseline for all batch sizes, it frequently achieves better performance with smaller sizes. For  example, adversarial training with = 0.03 and batch size 64 (54%) outperforms the baseline with batch size 256 (53.79%). This suggests that adversarial training is both more robust and efficient.</p><p>Embedding dimension impact is evaluated in Fig. <ref type="figure" target="#fig_3">5(b</ref>). This dimension does not seem to affect the performance of both baseline and adversarially trained model.</p><p>Architecture The impact of different architectures is evaluated in Fig. <ref type="figure" target="#fig_3">5</ref>(c), showing that larger networks have better performance. However, the improvement is much more dramatic with adversarial training (red bar), where it can be as high as 5% (ResNet101 over ResNet18), than for the baseline (green bar), where it is at most 1%. This is likely because larger networks have higher learning capacity and can benefit from the more challenging examples produced by adversarial augmentation. Nevertheless, the fact that the ResNet18 with adversarial training beats the ResNet101 baseline shows that there is always a benefit to more challenging training pairs, even for smaller models.</p><p>Hyperparameter α weights the contributions of the two losses (L aug and L adv ) of ( <ref type="formula" target="#formula_16">16</ref>). Fig. <ref type="figure" target="#fig_4">6</ref>(a) shows downstream classification accuracy when α varies from 0 to 2, for = 0.03. Accuracy starts to increase with α = 0.2 and reaches its peak around α = 0.8, but performance is fairly stable for α &gt; 0.2. There is no significant accuracy drop when the importance of adversarial examples is weighted by as much as twice that of clean examples (α = 2). This shows that the features derived from the adversarial examples benefit learning.</p><p>Attack strength is studied in Fig. <ref type="figure" target="#fig_4">6</ref>  augmentation is more efficient. At 300 epochs, it achieves results close to those of the baseline with 400 epochs; with 400 epochs, it outperforms the baseline at 500 epochs. This is similar to previous observations in metric learning, where challenging training pairs are known to improve the both convergence speed and final embedding performance.</p><p>Transfer to other downstream datasets Transfer performance compares how encoders learned by different SSL approaches generalize to various downstream datasets. Following the linear evaluation protocol of <ref type="bibr" target="#b11">[12]</ref>, we consider the 8 datasets <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46]</ref> shown in Table2. Both the encoder of SimCLR <ref type="bibr" target="#b11">[12]</ref> and CLAE are trained on ImageNet100 (an ImageNet subset sampled by <ref type="bibr" target="#b62">[63]</ref>), using a ResNet18. On ImageNet100, CLAE achieved 62.4±0.02 classification accuracy, outperforming SimCLR (61.7±0.02). This indicates that it can scale to large datasets. On the remaining datasets of Table2, it outperformed SimCLR on 7 out of the 8 datasets. This suggests that it generalizes better across downstream datasets. Since ImageNet100 does not contain any classes related to cars and airplanes, the performance on these 2 datasets is worse than on the others. In any case, CLAE beat <ref type="bibr" target="#b11">[12]</ref> on several fine-grained datasets, such as Cars <ref type="bibr" target="#b27">[28]</ref>, Aircraft <ref type="bibr" target="#b37">[38]</ref> and Flowers <ref type="bibr" target="#b45">[46]</ref>.</p><p>Attack methods While FGSM <ref type="bibr" target="#b18">[19]</ref> is used in the above experiments, CLAE can be integrated with multiple attack approaches. To demonstrate this property, various attack methods (R-FGSM <ref type="bibr" target="#b64">[65]</ref>, F-FGSM <ref type="bibr" target="#b71">[72]</ref>, PGD <ref type="bibr" target="#b36">[37]</ref>) were evaluated, with = 0.03, on CIFAR100. As shown in Figure <ref type="figure">7</ref>, R-FGSM and F-FGSM have performance comparable to FGSM, while PGD is slightly weaker. However, all these attack methods beat the baseline, indicating that the proposed framework learns a better representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In self-supervised learning (SSL), approaches based on contrastive learning (CL) do not necessarily optimize on hard negative pairs. In this work, we have proposed a new algorithm (CLAE) that generates more challenging positive and hard negative pairs, on-the-fly, by leveraging adversarial examples. Adversarial training with the proposed adversarial augmentations was demonstrated to improve performance of several CL baselines. We hope this work will inspire further research on the use of adversarial examples in SSL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Broader Impact</head><p>This work advances the general use of deep learning technology, especially in the case that dataset annotations are difficult to obtain, and could have many applications. It advances several state of the art solutions on self-supervised learning (SSL), where no labels are provided. Moreover, while prior works in SSL suggest training with larger network, larger batch size and longer training epochs, the experiments in this works demonstrates that these factors are less critical by optimizing on effective training pairs. This can be beneficial in the scenario where time and gpu resource are limited. While this work mainly focuses on the study of image recognition, we hope this work can be extended to other application domains of SSL in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left) Generation of adversarial augmentations in step 4 of Algorithm 1 (Right) Adversarial training with contrastive loss in step 5 of Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Adversarial examples computed with Algorithm 1 on tinyImagenet. The difference is amplified for visualization purpose.</figDesc><graphic url="image-1.png" coords="7,115.70,84.50,52.13,52.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Running mean (a) and variance (b) of batch normalization for clean and adversarial examples, from randomly selected channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation study of (a) batch sizes ,(b) embedding dimensions and (c) ResNet architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation study for (a) hyperparameter α, (b) attack strength and (c) longer pretext training.</figDesc><graphic url="image-7.png" coords="8,115.84,621.80,118.80,81.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Downstream classification accuracy for three SSL methods, with and without ( = 0) adversarial augmentation, on different datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">kNN</cell><cell></cell><cell></cell><cell></cell><cell>LR</cell></row><row><cell cols="3">Method</cell><cell></cell><cell></cell><cell>Cifar10</cell><cell></cell><cell cols="2">Cifar100</cell><cell cols="2">Cifar10</cell><cell>Cifar100</cell><cell>tinyImageNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="7">82.78±0.20 54.73±0.20 79.65±0.43 51.82±0.46</cell><cell>31.71±0.23</cell></row><row><cell></cell><cell cols="2">Plain</cell><cell cols="8">0.03 83.09±0.19 55.28±0.12 79.94±0.28 52.04±0.32</cell><cell>32.82±0.10</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">0.07 83.04±0.18 54.96±0.12 79.85±0.16 52.14±0.21</cell><cell>32.71±0.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="7">83.63±0.14 55.23±0.28 80.63±0.18 52.99±0.25</cell><cell>32.32±0.30</cell></row><row><cell></cell><cell></cell><cell>UEL</cell><cell>0.03</cell><cell></cell><cell>84±0.15</cell><cell cols="5">55.96±0.06 80.94±0.13 54.27±0.40</cell><cell>33.72±0.30</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">0.07 83.72±0.19 55.36±0.22 80.82±0.12 53.90±0.11</cell><cell>33.16±0.36</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="7">75.92±0.26 34.94±0.25 83.27±0.17 53.79±0.21</cell><cell>40.11±0.34</cell></row><row><cell cols="3">SimCLR</cell><cell cols="8">0.03 76.45±0.32 38.89±0.25 83.32±0.26 55.52±0.30</cell><cell>41.62±0.20</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="8">0.07 76.70±0.36 38.41±0.21 83.13±0.22 54.96±0.20</cell><cell>41.46±0.22</cell></row><row><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>60</cell><cell></cell><cell></cell><cell></cell><cell>60</cell></row><row><cell></cell><cell></cell><cell>ϵ=0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ϵ=0</cell><cell></cell><cell>ϵ=0</cell></row><row><cell></cell><cell>58</cell><cell>ϵ=0.03</cell><cell></cell><cell></cell><cell></cell><cell>58</cell><cell cols="2">ϵ=0.03</cell><cell></cell><cell>58</cell><cell>ϵ=0.03</cell></row><row><cell>Accuracy</cell><cell>54 56</cell><cell>ϵ=0.07</cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>54 56</cell><cell cols="2">ϵ=0.07</cell><cell></cell><cell>Accuracy</cell><cell>54 56</cell><cell>ϵ=0.07</cell></row><row><cell></cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell>52</cell></row><row><cell></cell><cell>50</cell><cell>64</cell><cell cols="2">128 Batch size</cell><cell>256</cell><cell>50</cell><cell>32</cell><cell cols="2">64 Embedding size 128</cell><cell>256</cell><cell>50</cell><cell>18 ResNet Architecture 50 101</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of transfer learning performance with linear evaluation to other image datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>57</cell></row><row><cell cols="3">CIFAR10 CIFAR100 SimCLR 72.8±0.3 45.3±0.1 CLAE 73.6±0.3 47.0±0.2</cell><cell>Cars 12.25 ±0.0 12.60 ±0.1</cell><cell>Aircraft 14.8±0.9 16.2±0.6</cell><cell>Accuracy</cell><cell>53 54 55 56</cell></row><row><cell cols="3">DTD SimCLR 50.6 ±1.2 44.4 ±0.4 Pets</cell><cell>Caltech-101 68.2 ±0.3</cell><cell>Flowers 46.0 ±0.3</cell><cell></cell><cell>52</cell><cell>Baseline (No attack)</cell><cell>Attack approach Fgsm FFgsm RFgsm</cell><cell>PGD</cell></row><row><cell>CLAE</cell><cell>51.5±0.5</cell><cell>44.4±0.0</cell><cell>69.1±0.3</cell><cell>47.1±0.8</cell><cell cols="3">Figure 7: Comparison of different attack methods.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Or, equivalently 0.99 for Tensorflow implementation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was partially funded by NSF awards IIS-1637941, IIS-1924937, and NVIDIA GPU donations. We also acknowledge and thank the use of the Nautilus platform for some of the experiments discussed above.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<ptr target="https://tiny-imagenet.herokuapp.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015-12">Dec 2015</date>
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Unaiza Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><forename type="middle">A</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName><surname>Essa</surname></persName>
		</author>
		<idno>CoRR, abs/1808.07507</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the robustness of semantic segmentation models to adversarial attacks</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<idno>CoRR, abs/1902.09229</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Universal representations: The missing link between faces, text, planktons, and cat breeds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial contrastive estimation</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
		<idno>CoRR, abs/1805.03642</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Compositional hard negatives for visual semantic embeddings via an adversary</title>
		<author>
			<persName><forename type="first">Joey</forename><surname>Avishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanshuai</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
		<idno>CoRR, abs/1608.04644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial attacks and defences: A survey</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaar</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishal</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debdeep</forename><surname>Mukhopadhyay</surname></persName>
		</author>
		<idno>CoRR, abs/1810.00069</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust physical adversarial attack on faster R-CNN object detector</title>
		<author>
			<persName><forename type="first">Cory</forename><surname>Shang-Tse Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cornelius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duen</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chau</forename><surname>Horng</surname></persName>
		</author>
		<idno>CoRR, abs/1804.05810</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Physical adversarial examples for object detectors</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earlence</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atul</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadayoshi</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno>CoRR, abs/1807.07769</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>CoRR, abs/1803.07728</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
				<editor>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mike</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Sardinia, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2010-05">May 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
	<note>Chia Laguna Resort</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Large Scale Holistic Video Understanding, ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised feature extraction by time-contrastive learning and nonlinear ica</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Morioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, number NIPS 2016 in Advances in neural information processing systems</title>
				<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<meeting><address><addrLine>United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3772" to="3780" />
		</imprint>
	</monogr>
	<note>Neural Information Processing Systems Foundation</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Self-supervised visual feature learning with deep neural networks: A survey</title>
		<author>
			<persName><forename type="first">Longlong</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingli</forename><surname>Tian</surname></persName>
		</author>
		<idno>CoRR, abs/1902.06162</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning image representations by completing damaged jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
		<idno>CoRR, abs/1802.01880</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
				<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Vijay Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
		<idno>CoRR, abs/1704.01285</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR, abs/1611.01236</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno>CoRR, abs/1603.06668</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by sorting sequences</title>
		<author>
			<persName><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno>CoRR, abs/1708.01246</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adversarial vertex mixup: Toward better adversarially robust generalization</title>
		<author>
			<persName><forename type="first">Saehyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun-Gyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
		<idno>ArXiv, abs/2003.02484</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transferable adversarial training: A general approach to adapting deep classifiers</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Unsupervised learning using sequential verification for action recognition</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<idno>CoRR, abs/1603.08561</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1979" to="1993" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno>CoRR, abs/1610.08401</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">Seyed-Mohsen</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
		<idno>CoRR, abs/1511.04599</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Improvements to context based self-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Nathan</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno>CoRR, abs/1711.06379</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A selfsupervised approach for adversarial robustness</title>
		<author>
			<persName><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
				<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno>CoRR, abs/1603.09246</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Berkay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananthram</forename><surname>Swami</surname></persName>
		</author>
		<idno>CoRR, abs/1511.07528</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
				<editor>
			<persName><forename type="first">Xianghua</forename><surname>Xie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gary</forename><forename type="middle">K L</forename><surname>Tam</surname></persName>
		</editor>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="41" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>CoRR, abs/1604.07379</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.12943" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName><forename type="first">S-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<idno>CoRR, abs/1503.03832</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Selfsupervised learning from multi-view observation</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1704.06888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Time-contrastive networks: Selfsupervised learning from multi-view observation</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corey</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>CoRR, abs/1704.06888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>CoRR, abs/1904.12843</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno>CoRR, abs/1511.06452</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kouichi</forename><surname>Sakurai</surname></persName>
		</author>
		<idno>CoRR, abs/1710.08864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Stochastic class-based hard example mining for deep metric learning</title>
		<author>
			<persName><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Adversarial training and robustness for multiple perturbations</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">1904.13000, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">K</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.13625</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. arXiv: Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="5334" to="5344" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fast is better than free: Revisiting adversarial training</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Krähenbühl</surname></persName>
		</author>
		<idno>CoRR, abs/1706.07567</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>CoRR, abs/1805.01978</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>ArXiv, abs/1911.09665</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>CoRR, abs/1703.08603</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Intriguing properties of adversarial training</title>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>CoRR, abs/1906.03787</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno>CoRR, abs/1904.03436</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Adversarial examples: Attacks and defenses for deep learning</title>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qile</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Li</surname></persName>
		</author>
		<idno>CoRR, abs/1712.07107</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Theoretically principled trade-off between robustness and accuracy</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaodong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiantao</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>CoRR, abs/1901.08573</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno>CoRR, abs/1603.08511</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Practical adversarial attack against object detector</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qintao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruigang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhi</forename><surname>Zhang</surname></persName>
		</author>
		<idno>CoRR, abs/1812.10217</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
