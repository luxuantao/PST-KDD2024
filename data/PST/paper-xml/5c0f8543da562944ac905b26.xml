<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Florin</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Bogdan</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Yefeng</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sasa</forename><surname>Grbic</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Maier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joachim</forename><surname>Hornegger</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">search Scale-Space Representation Search Model Input Image Image Database</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Scale Deep Reinforcement Learning for Real-Time 3D-Landmark Detection in CT Scans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E46FBBE1D97DF70440D014100F560E6</idno>
					<idno type="DOI">10.1109/TPAMI.2017.2782687</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2782687, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2017.2782687, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 2</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>deep reinforcement learning</term>
					<term>medical image analysis</term>
					<term>multi-scale</term>
					<term>scale-space modeling</term>
					<term>three-dimensional (3D) object detection</term>
					<term>real-time detection</term>
					<term>intelligent localization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust and fast detection of anatomical structures is a prerequisite for both diagnostic and interventional medical image analysis. Current solutions for anatomy detection are typically based on machine learning techniques that exploit large annotated image databases in order to learn the appearance of the captured anatomy. These solutions are subject to several limitations, including the use of suboptimal feature engineering techniques and most importantly the use of computationally suboptimal search-schemes for anatomy detection. To address these issues, we propose a method that follows a new paradigm by reformulating the detection problem as a behavior learning task for an artificial agent. We couple the modeling of the anatomy appearance and the object search in a unified behavioral framework, using the capabilities of deep reinforcement learning and multi-scale image analysis. In other words, an artificial agent is trained not only to distinguish the target anatomical object from the rest of the body but also how to find the object by learning and following an optimal navigation path to the target object in the imaged volumetric space. We evaluate our approach on 1487 3D-CT volumes from 532 patients, totaling over 500,000 image slices and show that we significantly outperform state-of-the-art solutions on detecting several anatomical structures with no failed cases from a clinical acceptance perspective, while also improving the detection accuracy by 20-30%. Most importantly, we improve the detection-speed of the reference methods by 2-3 orders of magnitude, achieving unmatched real-time performance on large 3D-CT scans.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE detection of anatomical landmarks represents a pre- requisite for medical image analysis. Many applications for clinical support require the precise, automatic detection of anatomical structures to initialize and constrain mathematical models for volumetric organ segmentation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, image-to-image registration <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, structure tracking <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, advanced bio-physical modeling and mechanical simulations <ref type="bibr" target="#b7">[8]</ref>. As such, enabling accurate and efficient anatomical landmark detection can assist the physician with automated measurements for a more effective and streamlined image reading. We focus on 3D-CT, an imaging technology widely used both interventionally and for diagnosis, e.g. for disease screening, detection of brain hemorrhages, bone fractures, etc. <ref type="bibr" target="#b8">[9]</ref>.</p><p>The current solutions for anatomical landmark detection are typically based on machine learning concepts, which effectively exploit large annotated medical image databases <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. For this purpose, the detection task is typically decoupled into two independent and sequential stages: the learning of an appearance model and the object search. In the first stage, an appearance model is designed to capture the underlying image information F. Ghesu is with Medical Imaging Technologies, Siemens Healthineers, Princeton, NJ 08540 USA and with the Pattern Recognition Lab, Friedrich-Alexander-Universität Erlangen-N ürnberg, 91058 Erlangen, Germany (email: florin.c.ghesu@fau.de) B. Georgescu, S. Grbic, Y. Zheng and D. Comaniciu are with Medical Imaging Technologies, Siemens Healthineers, Princeton, NJ 08540 USA. A. Maier and J. Hornegger are with the Pattern Recognition Lab, Friedrich-Alexander-Universität Erlangen-N ürnberg, 91058 Erlangen, Germany.</p><p>and use it as evidence to identify the anatomical landmark. To enable this, traditional machine learning systems rely on precise feature engineering strategies, using human ingenuity to understand and model the image information <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Instead, deep learning solutions propose to learn the image features to better disentangle explanatory attributes of the observed data <ref type="bibr" target="#b14">[15]</ref>. This enables the learning models to better capture the complex anatomical variation, ensuring an increased performance and better generalization to unseen data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. However, in the second stage of the task, concerning the object search, the aforementioned solutions rely on suboptimal search strategies, e.g. exhaustive scanning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, one-shot displacement estimation <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> or end-to-end image mapping techniques <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>. These strategies lead in many cases to false-positive detection results and unreasonably high computation times.</p><p>In this work, we propose a method which follows a different paradigm, based on the reformulation of the detection task as a behavioral problem for an artificial agent. Using deep reinforcement learning <ref type="bibr" target="#b18">[19]</ref> and scale-space theory <ref type="bibr" target="#b19">[20]</ref>, we learn optimal search strategies for finding anatomical structures, based on the image information at multiple scales. A search strategy generates multi-scale navigation trajectories, which evolve through the voxel-grid of the image at different spatial resolutions, i.e. the scale-space representation of the image <ref type="bibr" target="#b19">[20]</ref>, and converge to the sought landmark location (see Figure <ref type="figure" target="#fig_0">1</ref>). This formulation exploits in a systematic and natural way the different levels of abstractness encoded in the scalespace representation of an image. The search starts at the coarsest scale level with a global context and continues across scales, capturing increased levels of details when transitioning to finer scales <ref type="bibr" target="#b19">[20]</ref>. Such details are used as additional evidence to guide the search. We propose to use independent search models on each level of the scale-space in order to adapt the search to the most discriminative image features, visible on that level. These search models are based on the mechanism of deep reinforcement learning, which we reformulated for the context of anatomical landmark detection in previous work <ref type="bibr" target="#b20">[21]</ref>. In this paper, we revisit our formulation <ref type="bibr" target="#b20">[21]</ref> and present the following additional contributions:</p><p>• We extend the original solution using scale-space theory to exploit multi-scale image representations for a robust and efficient multi-scale object search in 3D-CT scans of arbitrary size in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We present a simple modification at the core of the DRL system which enables faster and better training (see second paragraph in section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We evaluate the performance of our method on detecting 8 landmarks from different types of anatomy, i.e. bone, non-rigid organs, vessel bifurcations, using a set of 1487 3D-CT scans from 532 patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We demonstrate the performance of our method on an additional dataset of 506 3D-MR scout scans acquired from 506 patients. In contrast to the 3D-CT scans, the magnetic resonance images also show a constrained angular variation in the axial plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We provide a detailed comparison with 5 solutions for landmark detection based on the probabilistic boosting tree <ref type="bibr" target="#b2">[3]</ref>, extremely randomized trees <ref type="bibr" target="#b9">[10]</ref> and on deep learning systems <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref> -showing that our method outperforms these solutions in terms of number of failures and accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We provide a runtime analysis, demonstrating that our method is 20 to 150 times faster compared to these methods, reaching real-time detection speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We include an empirical convergence analysis of our algorithm and discuss the detection of outliers and how to handle cases of missing objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We include a detailed analysis from computer vision perspective, discussing related methods and potential applications of our ideas in this field.</p><p>The remaining paper is organized as follows. In section 2 we review previous work on object localization and high-light the limitations of existing technology. In section 3 we present our solution for object detection using multi-scale deep reinforcement learning. Experiments are presented in section 4, and section 5 covers the conclusion of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>A variety of applications for medical image analysis can benefit from the automatic and accurate detection of anatomical landmarks. In the following, we present several existing solutions for anatomical landmark detection and highlight the main challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Object Localization in 3D: Challenges</head><p>Scanning-based Systems represent the main category of detection solutions, specifically in the context of 3D data. In this case, object detection is formulated as a patchwise classification problem. A set of discrete hypotheses H, in the form of local volumetric boxes of image intensities, is extracted from any image I from the training set: ∀h ∈ H, h ∼ I. This set of hypotheses can be partitioned in a subset of positive hypotheses H + which are centered at the landmark location, and negative hypotheses H -from the rest of the image: H = H + ∪ H -. A classifier is trained on this set of hypotheses -essentially learning to distinguish the appearance of the sought object from the rest of the sampled anatomy. One can use traditional machine learning models, e.g. extremely randomized trees <ref type="bibr" target="#b9">[10]</ref> and probabilistic boosting trees <ref type="bibr" target="#b22">[23]</ref>; or deep learning, e.g. deep convolutional neural networks <ref type="bibr" target="#b23">[24]</ref> and sparse adaptive deep neural networks <ref type="bibr" target="#b0">[1]</ref>. For the final result, hypotheses aggregation using Hough regression <ref type="bibr" target="#b9">[10]</ref> or averaging <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b23">[24]</ref> can be applied. However, in the case of training with a large number of high-resolution whole body 3D-CT scans, the number of hypotheses used for training can surpass the memory capabilities of current GPU-based systems. In addition, at testing time the classifier is used to scan the entire space of hypotheses of a given image -a set which grows exponentially with the dimensionality of the image. In the case of a 200 × 200 × 200 volume, with a hypothesis size of 15 × 15 × 15, the sampling complexity reaches the order of billions. Moreover, this approach can also be sensitive to false-positive classifier responses and is thus often combined with different techniques, such as cascade filtering <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b24">[25]</ref>, filter-decomposition <ref type="bibr" target="#b17">[18]</ref>, simultaneous network propagation <ref type="bibr" target="#b21">[22]</ref> or active scheduling <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, to enable effective training and detection in 3D data.</p><p>End-to-End Systems, also called image-to-image systems, are inspired by the fully convolutional network (FCN) architectures and learn the mapping between original image and segmentation multi-masks <ref type="bibr" target="#b27">[28]</ref> or image codes highlighting the locations of anatomical landmarks <ref type="bibr" target="#b15">[16]</ref>. Recently, Dai et al. <ref type="bibr" target="#b28">[29]</ref> have proposed a region-based FCN approach for efficient object detection. While these solutions enable pixel/voxel-wise classification and support simultaneous detection of multiple landmarks, the training is very complex in terms of both memory management and processing time. For example, a forward-propagation of a single CT scan containing 200 × 200 × 200 voxels through a typical FCN can surpass a memory footprint of 3-4 GB -severely limiting the size of the sampled batch of images during training. Despite memory issues, these challenges are being addressed within this rapidly advancing line of research.</p><p>Regression-based Systems exploit the anatomical context around the landmark in order to learn relative displacement vectors pointing at the landmark location. One can use random regression forests <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, random-ferns <ref type="bibr" target="#b31">[32]</ref>, deep convolutional neural networks <ref type="bibr" target="#b32">[33]</ref> or modern spatial-transformer neural networks <ref type="bibr" target="#b33">[34]</ref> to learn the mapping. While such solutions significantly improve the efficiency of scanning-based systems, they are typically difficult to train and not robust to variations in the range of the scan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Atlas-based Systems</head><p>Atlas-based registration <ref type="bibr" target="#b34">[35]</ref>, as well as multi-atlas-based registration methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b35">[36]</ref>, can also be applied to solve localization tasks. However, the application to large 3D-CT scans poses significant computational challenges with decreased model scalability. Potesil et al. <ref type="bibr" target="#b36">[37]</ref> address this limitation using graphical models with dense parts-based matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Localization in 2D: Related Work</head><p>Similar concepts are used to approach object localization also for 2D data (e.g. color images or depth maps). While the lower dimensionality represents a clear advantage in terms of processing complexity, the lack of structure and alignment in comparison to 3D medical data changes the solution perspective. Modeling the intrinsic configuration of object points that need to be detected is thus required. In this context, shape model matching with random forest voting <ref type="bibr" target="#b37">[38]</ref> has been proposed. Deformable part models in combination with latent SVMs for partially labeled data <ref type="bibr" target="#b38">[39]</ref> have achieved competitive results on the PASCAL challenge. Recently, region-based models using deep convolutional neural networks have achieved state-of-the-art results on the PASCAL VOC dataset <ref type="bibr" target="#b39">[40]</ref>. Ren et al. <ref type="bibr" target="#b40">[41]</ref> further optimized the network architecture by selecting to share the features between the region proposal and the detection network, thereby reducing the computation time. Additional runtime improvements have been achieved by using FCNs <ref type="bibr" target="#b28">[29]</ref>. Deep learning solutions have also achieved stateof-the-art results for the task of human pose estimation. Two recent examples are the Convolutional Pose Machines <ref type="bibr" target="#b41">[42]</ref> for sequential prediction and the convolutional Stacked Hourglass Networks <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>To address the challenges of interpreting 3D data in large CT scans, we propose to reformulate anatomical object detection as a behavioral problem for an intelligent artificial agent that can teach itself how to search <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b43">[44]</ref>. With the keyword intelligent we describe the capability of our system to explore and learn the process of finding an object, as opposed to following predefined exhaustive search schemes. To achieve this, we combine concepts of cognitive modeling based on reinforcement learning with scale-space analysis and deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Deep Learning: An Overview</head><p>Established as a key technological innovation in the field of machine learning, with significant improvements in results for image parsing tasks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, deep learning systems have replaced the traditional feature handcrafting step with hierarchical, multi-layer models for automatic feature learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b46">[47]</ref>. We use the deep convolutional neural network (CNN) as an image feature extractor <ref type="bibr" target="#b47">[48]</ref> and universal non-linear function approximator <ref type="bibr" target="#b14">[15]</ref>. The network is parametrized by θ = [W, b], where W denotes the inter-neural connection weights organized as (multichannel) filter kernels, and b defines the set of neuron bias values. The architecture is inspired by the feed-forward type of information processing observable in the early visual cortex of animals <ref type="bibr" target="#b48">[49]</ref>. Convolutional layers exploit local spatial correlations of image voxels to learn translation-invariant convolutional kernels, which capture discriminative image features. Consider a multi-channel signal representation M k in layer k, i.e. a channel-wise concatenation of signal representations M k,c with c ∈ N. One can generate a signal representation in layer k + 1 as:</p><formula xml:id="formula_0">M k+1,l = φ (M k * w k,l + b k,l ),</formula><p>where w k,l ∈ W represents a convolutional kernel with the same number of channels as M k , the value b k,l ∈ b represents the bias, l denotes the channel index, and * denotes a convolution operation. The function φ represents the nonlinear activation function, which is applied point-wise. We use rectified linear unit (ReLU) activations <ref type="bibr" target="#b44">[45]</ref>. Depending on the problem, the final network layer is typically fullyconnected. In a supervised regression setup, given training data D = [(X 1 , y 1 ), . . . , (X N , y N )], i.e. N independent pairs of volumetric image observations with value assignments, one can define the network response function as R( • ; θ), and use Maximum Likelihood Estimation to estimate the optimal network parameters (L denotes the likelihood):</p><formula xml:id="formula_1">θ = arg max θ L (θ; D) = arg min θ N i=1 (R(X i ; θ) -y i ) 2 . (1)</formula><p>This optimization problem is typically solved with stochastic gradient descent (SGD) combined with the backpropagation algorithm to compute the network gradients <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Deep Reinforcement Learning for Intelligent Search</head><p>Our approach for intelligent anatomical landmark detection combines the representational power of modern CNN architectures and cognitive modeling through reinforcement learning (RL). Initially, the combination of these two elements was introduced in the literature under the name of deep reinforcement learning (DRL), a technique used to train artificial agents to master different ATARI games <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Anatomy Detection: A Behavior Learning Problem</head><p>We propose to reformulate anatomy detection as a cognitive learning task for an artificial agent. Given a volumetric image I : Z 3 → R and the location of an anatomical structure of interest p GT ∈ R 3 within I, the task is to learn a navigation strategy to p GT in image space, i.e. the voxel grid of the scan. In other words, we seek voxel-based navigation trajectories from any arbitrary starting point p 0 to p k within image I, with the property that p k -p GT is  minimal <ref type="bibr" target="#b20">[21]</ref>. Reinforcement learning allows us to model this problem using a Markov Decision Process (MDP) <ref type="bibr" target="#b50">[51]</ref> M := (S, A, T , R, γ), where:</p><p>• S represents a finite set of states, s t ∈ S being the state of the agent at time t. To encode the location of the agent in the imaged volumetric space at time t, we define s t = I( p t ), which denotes an axis-aligned box of image intensities extracted from I and centered at the voxel-position p t in image space.</p><p>• A represents a finite set of actions allowing the agent to interact with the environment defined by I, where a t ∈ A is the action the agent performs at time t. We propose a discrete voxel-wise navigation model allowing the agent to move from any voxel position p t to an adjacent voxel position p t+1 in image space (see Figure <ref type="figure" target="#fig_2">2</ref> for details).</p><p>• T : S × A × S → [0; 1] is a stochastic transition function, where T s s,a describes the probability of arriving in state s , after performing action a in state s.</p><p>• R : S × A × S → R is a scalar reward function which drives the behavior of the agent, where R s s,a ∈ R denotes the expected reward after a state transition. For a state transition s → s at time</p><formula xml:id="formula_2">t from p t → p t+1 , we define R s s,a = p t -p GT 2 2 -p t+1 -p GT 2 2</formula><p>. Intuitively this represents a distance-based feedback, which is positive if the agent gets closer to the target structure and negative otherwise.</p><p>• γ is the discount factor controlling the importance of future versus immediate rewards.</p><p>Considering the proposed reward-scheme and an arbitrary trajectory T = [ p 0 , p 1 , . . . , p k ] in image space, at any time t ∈ {0, . . . , k} the associated cumulative future discounted reward is defined as: R t = k t= t γ t-tr t , where the immediate reward at time t is denoted by r t . In RL theory this is also considered a finite-horizon learning episode of length k <ref type="bibr" target="#b50">[51]</ref>. The target is to find optimal trajectories that maximize the associated cumulative future reward (see Figure <ref type="figure" target="#fig_2">2</ref>). To achieve this, we define the optimal action-value function Q * (•, •), which encodes the maximum expected fu-ture discounted reward when starting in state s, performing action a, and acting optimally thereafter:</p><formula xml:id="formula_3">Q * (s, a) = max π E [R t |s t = s, a t = a, π] ,<label>(2)</label></formula><p>where π is an action policy, in other words a probability distribution over actions in any given state. The optimal action-value function gives us the optimal action policy, defining the optimal behavior of the agent in any state:</p><formula xml:id="formula_4">∀s ∈ S : π * (s) = arg max a∈A Q * (s, a).<label>(3)</label></formula><p>One important relation satisfied by the optimal action-value function Q * is the Bellman optimality equation <ref type="bibr" target="#b51">[52]</ref>, which represents a recursive formulation of Equation <ref type="formula" target="#formula_3">2</ref>:</p><formula xml:id="formula_5">Q * (s, a) = s T s s,a R s s,a + γ max a Q * (s , a ) = E s r + γ max a Q * (s , a ) ,<label>(4)</label></formula><p>where s defines a possible state visited after s, a the corresponding action and r = R s s,a represents a compact notation for the current, immediate reward. Viewed as an operator τ , the Bellman equation defines a contraction mapping. In previous work <ref type="bibr" target="#b52">[53]</ref> the following property was proven: ∀Q, lim n→∞ τ (n) (Q) = Q * , which gave rise to the modelbased policy iteration algorithm <ref type="bibr" target="#b50">[51]</ref>.</p><p>This standard approach is however not feasible in our case, where the state space is defined by high-dimensional image data. As such, we propose to use a model-free approach based on a non-linear parametrization of Q * with a deep convolutional neural network. In literature, this is called a deep Q-network (DQN) <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b52">[53]</ref> and is used as a non-linear approximator for the optimal action-value function: Q(s, a; θ) ≈ Q * (s, a), where θ = [W, b] are the parameters of the network. Similar to the temporal difference Q-Learning algorithm <ref type="bibr" target="#b52">[53]</ref>, a deep Q-network can be trained in a RL setup using an iterative approach to minimize the mean squared error based on the Bellman optimality equation (see Equation <ref type="formula" target="#formula_5">4</ref>). At any training-iteration i, we can approximate the optimal expected target value for the action-value function using a set of reference parameters θ(i) := θ (i ) , based on a previous training iteration i &lt; i:</p><formula xml:id="formula_6">y = r + γ max a Q(s , a ; θ(i) ).<label>(5)</label></formula><p>As such, we obtain a sequence of well-defined optimization problems, driving the evolution of the network parameters.</p><p>The error function at each training step i is defined as:</p><formula xml:id="formula_7">θ(i) = arg min θ (i) E s,a,r,s y -Q(s, a; θ (i) ) 2 . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>This is a supervised setup for DL ,which can be approached as described in the beginning of this section. In our framework, we periodically apply stochastic gradient descent steps, approximating the gradient using random sampling:</p><formula xml:id="formula_9">∇ θ (i) = E s,a,r,s y -Q(s, a; θ (i) ) ∇ θ (i) Q(s, a; θ (i) ) .<label>(7</label></formula><p>) Figure <ref type="figure">3</ref> shows an example of a learned trajectory defined by the optimal action-value function Q * . This highlights the difference between our approach and the concept of exhaustive hypotheses scanning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Exhaustive Search</head><p>Learned Search-Path Fig. <ref type="figure">3</ref>. Visualization of the differences between exhaustive scanning and our proposed method, which learns the search-process. Solutions based on exhaustive scanning, e.g. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b17">[18]</ref>, typically test all hypotheses extracted from the volumetric input and then apply a form of aggregation/clustering of high-probability candidates to obtain a final result. In contrast, our approach learns not only the appearance of the anatomy but also the strategy of how to find a target anatomical landmark. The search starts at a given point p 0 and defines a 3D trajectory in image space, visualized as a white curve, converging to the sought anatomical landmark location (here the right kidney).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Learning to Search vs. Exhaustive Search</head><p>Learning the action-value function Q * enables the agent to effectively search for objects in the image, as opposed to scanning the volumetric space exhaustively (see Figure <ref type="figure">3</ref>). This learning process is based on an adequate exploration of the environment, which we ensure through an off-policy -greedy approach <ref type="bibr" target="#b18">[19]</ref>. The variable ∈ [0, 1] controls the randomness in the exploration. This means that during training, actions are selected either uniformly at random with probability , or deterministically using the current policy with probability 1 -. In our experiments, we linearly anneal from 1.0 to 0.05. Another important strategy to ensure the training stability is the decorrelation of the training samples using the concept of experience replay <ref type="bibr" target="#b53">[54]</ref>.</p><p>During training, the agent maintains an active memory of episodic trajectories M = [T 1 , T 2 , . . .], which is constantly expanded and uniformly sampled to estimate the learning gradient (see Equation <ref type="formula" target="#formula_9">7</ref>).</p><p>To further accelerate the training, we propose to use an adaptive episode length. Through empirical analysis we observed that by gradually reducing the episode length during training using linear decay, we improve the space exploration by sampling increasing numbers of trajectories that are stored in the active memory. This simple modification not only increased the robustness of the trained policy, but also reduced the training time on average by around 30%. This is due to the fact that as the policy improves during training, sampled trajectories also converge faster to the ground-truth, eliminating the need for long-horizon episodes which can bias the stochastic sampling for the gradient estimation. In our case the initial length of the episode is 1000 and is gradually reduced to 50 steps.</p><p>Given this system definition, one can observe a major limitation related to the modeling of the state space S, more specifically to the size of the acquired state representation s ∈ S, in the form of a box of image-intensities. Acquiring a small-volume box, containing only local information, improves the sampling efficiency, but also increases the complexity of the learning task by disregarding global context. Such context is required to learn an effective navigation policy and avoid local optima. On the contrary, extracting a very large box to represent the state poses significant computational challenges in the 3D space. This trade-off indicates the inability of our preliminary approach to properly exploit the image information at different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">A Scale-space Theoretical Perspective</head><p>We propose to address this limitation by using scale-space theory <ref type="bibr" target="#b19">[20]</ref>. Given an arbitrary discrete image signal in 3D, defined as: I : Z 3 → R, the axiomatic formulation of the continuous scale-space of this signal is:</p><formula xml:id="formula_10">L(x; t) = ξ∈Z 3 T (ξ; t) I(x -ξ),<label>(8)</label></formula><p>where t ∈ R + denotes the continuous scale level, x ∈ Z 3 , L(x; 0) = I(x) and T defines a one-parameter family of kernels, used to generate the scale-space by convolution.</p><p>The main property of a scale-space signal representation L, also called the image scale-space, is the non-enhancement of local extrema, which ensures the causality of structure across scales <ref type="bibr">[20, p. 103</ref>]. This means that local maximum/minimum points in the image signal at any scale level t 0 , do not increase/decrease their value at any higher scale t &gt; t 0 . Based on this property, as well as the semi-group structure of the family of kernels T , it has been shown that within the class of linear transformations, the scale-space representation L is differentiable, satisfying the differential equation:</p><formula xml:id="formula_11">∂ t L = A ScSp L,<label>(9)</label></formula><p>where A ScSp is an infinitesimal scale-space generator, based on discrete approximations of the Laplace operator <ref type="bibr" target="#b19">[20]</ref>. In this case, one can formulate the change in scale level t as an effect of actions of the agent, e.g. by introducing a new action to increase, decrease or maintain the scale level. In RL theory this can also be formulated as a continuous action that needs to be learned as a step ∆t ∈ R, which specifies the change in scale level at each navigation step.</p><p>To achieve this, one can redefine the optimal action-value function Q * , by conditioning the state-representation s and model parameters θ on the scale-space representation L and the current scale level t: <ref type="bibr" target="#b9">(10)</ref> where t ∈ R + represents the scale level after executing action a. This implies that the object search would occur in continuous image scale-space, allowing the system to exploit structures on different scales. However, since the image dimensionality is preserved across scales, we are still left with the task of effectively addressing the aforementioned trade-off: sampling efficiency versus global context. In addition, recall that the scale-space parameter t ∈ R + is continuous. Since the model parameters θ depend on the scale, one would need to design a learning model that could capture not only the variability in image space but also the variability in scale-space. To avoid this complexity, we propose a discrete approximation L d of the continuous scale-space L, defined as:</p><formula xml:id="formula_12">Q * (s, a | L, t) = E s r + γ max a Q * (s , a | L, t ) ,</formula><formula xml:id="formula_13">L d (t) = Ψ ρ (σ(t -1) * L d (t -1)),<label>(11)</label></formula><p>where t ∈ N denotes the discrete scale level, * denotes a convolution, σ represents a scale-dependent Gaussianlike smoothing function and Ψ ρ denotes a signal operator, reducing the spatial resolution with factor ρ using downsampling <ref type="bibr" target="#b19">[20]</ref>. Similarly to the continuous case, L d (0) = I. While down-sampling the signal can introduce aliasing effects, they do not affect the learning process, enabling the system state to capture global context on coarse scale and local context on fine scale with similar sampling complexity (see Figure <ref type="figure" target="#fig_3">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning Multi-Scale Search Strategies</head><p>Given this discrete scale-space definition, we design a navigation model for each scale level:</p><formula xml:id="formula_14">Θ = [θ 0 , θ 1 , . . . , θ M -1 ],</formula><p>where M is the number of different scales. While lowlevel features could arguably be shared across scales to determine a single multi-scale search model, we empirically observed that training a different model on each scale yields optimal results. The motivation for this is that different scale levels are described by different image structures that can be used as robust evidence for the search. Across scales we clone all meta-parameters defining each model: for all scale levels 0 ≤ t &lt; M do 8:</p><formula xml:id="formula_15">Q(•, •; θ t | L d , t), ∀t &lt; M ,</formula><p>Select random image and starting-point Train Q(•, •; θ t | L d , t) according to Equation <ref type="formula" target="#formula_16">12</ref>12:</p><p>end for 13:</p><p>Decay -reduce randomness 14: end while 15: Output Θ = θ0 , θ1 , . . . , θM-1 -estimated models scale level the field-of-view of the agent is very large, acquiring sufficient global context to ensure an effective navigation. Upon convergence, the scale level is changed to M -2 and the search continued from the convergence point at level M -1. The process is repeated on the following scales until convergence on the finest scale. Note that on all scales, except the coarsest level M -1, the exploration range can be constrained, given the convergent behavior on coarser scales. These search-ranges are robustly determined during training (see Figure <ref type="figure" target="#fig_3">4</ref>). Based on the definition of the discrete scale-space L d and the independence of the search models across scales, we can rewrite Equation 6 and train on each scale level 0 ≤ t &lt; M according to:</p><formula xml:id="formula_16">θ(i) t = arg min θ (i) t E s,a,r,s y -Q s, a; θ (i) t | L d , t 2 ,<label>(12)</label></formula><p>with i ∈ N denoting the training iteration. The reference estimate y is determined similarly as in the single-scale solution, using a set of model parameters θ(i) t := θ (i ) t from a previous training iteration i &lt; i:</p><formula xml:id="formula_17">y = r + γ max a Q s , a ; θ(i) t | L d , t .<label>(13)</label></formula><p>Algorithm 1 describes the training steps for our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Empirical Convergence Criterion</head><p>Given a test volume I, a discrete scale-space definition L d and a set of trained multi-scale search models Θ, two important questions arise: At which location in the image does the search process start? When does the agent know that it has found the object of interest?</p><p>The starting point p 0 is defined based on the expected relative position r of the anatomical landmark, which is computed on the training set. Given N training images I 1 , I 2 , . . . , I N , we define r ∈ [0, 1] 3 as ∀d ∈ {1, 2, 3}, r(d) = Please note that the choice of the starting point is a question of algorithm design, and is not a limitation. As we will explain later in the experiments section, most of the scans are focused on the thorax and abdomen. Using the above definition, the starting point is expected to be close to the landmark location on such cases, considerably reducing the expected detection time. We also demonstrate that similar performance can be achieved by starting in the center of the scan (see Appendix C).</p><p>The question of trajectory convergence is implicitly related to the convergence properties of the system. However, the literature shows that there are no theoretical guarantees of global convergence when using a non-linear policy approximator, such as a deep neural network <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b46">[47]</ref>. In practice, several heuristic techniques such as memory replay, delayed updates or random-exploration ensure training stability and convergence. In this context, we formulate our trajectory convergence criterion as follows: given a searchtrajectory T = [ p 0 , p 1 , . . .] , ∃k, k ∈ N, with k &gt; k ≥ 0 such that p k = p k with l = k -k minimal. In other words trajectories converge on small, oscillatory-like cycles. Once such a cycle is identified at detection time, we stop the search and yield p k as detection result. We observed that this stopping criterion is robust in practice, that trajectories do not converge on long cycles. We provide an empirical analysis showing that the probability of converging on large cycles is exponentially small (see Appendix A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Object Not in the Scan Range?</head><p>In order to reliably use a machine in a clinical scenario to detect structures and derive automatic measurements that support the radiologist in reading 3D-CT scans, one needs to consider all the different types of such scans, e.g. cardiac, thoracic and abdominal scans, CT scans of the legs and pelvis or head-neck scans. In this general setting, an important question becomes whether the system is capable of recognizing the absence of an anatomical landmark from the captured field-of-view, i.e. the scanned region of the body. In practice, one cannot exclusively rely on meta-information about the scan acquisition to find an answer. Depending on the type of investigation, the medical technical radiology assistant (MTRA) can decide to either increase or decrease the field-of-view when acquiring the scan. To the best of our knowledge, previous solutions for object detection <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b32">[33]</ref> do not consider this scenario. For example, scanning solutions can impose a fixed threshold on the hypothesis probability, and use it as a decision criterion. In our experience, this heuristic is not always accurate.</p><p>Our formulation of generic object detection as a search problem represents a principled step towards addressing this challenge. Given an image I, and a structure of interest located outside the field-of-view, i.e. the image space, one can recognize the absence of this structure by following navigation trajectories, which attempt to leave the image space in the direction where the structure was supposed to be located, had the scan captured the whole body. By training the system on differently cropped images, we empirically observed this consistent behavior (see Figure <ref type="figure">5</ref>). Empirical results are included in section 4 . Example slices of CT scans that do not capture the left kidney Fig. <ref type="figure">5</ref>. Visualization of the search-path followed to find the left kidney in a thorax CT scan, which does not capture the left kidney (marked by an x). The trajectory leaves the image space, signaling that the organ is missing from the field-of-view. On the right we show several slices of other CT scans from the test set, which do not capture the left kidney. The image on the left is a slice from a CT scan of the legs. On the upper right we show a slice of a thorax CT scan, acquired for lung cancer screening. The lower right slice is from a cardiac CT scan with contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>For the experimental evaluation and comparison, we selected 5 reference methods: scanning with probabilistic boosting trees <ref type="bibr" target="#b2">[3]</ref> (PBT), extremely randomized trees with Hough regression <ref type="bibr" target="#b9">[10]</ref> (ExtRTrees), the Overfeat method adapted from public source-code to 3D data <ref type="bibr" target="#b21">[22]</ref>, 3D deep learning with filter decomposition <ref type="bibr" target="#b17">[18]</ref> (3D-DL) and scanning with cascaded sparse-adaptive deep neural networks <ref type="bibr" target="#b0">[1]</ref> (SADNN). All these methods were implemented and evaluated on detecting several anatomical landmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>The dataset contains 1487 3D-CT volumes from 532 patients, covering a wide range of scan types with different field-ofviews, e.g. cardiac CT scans (with contrast), thoracic scans, abdominal scans, CT scans of the legs and pelvis or CT scans of the head and neck. A large subset of these scans is focused on the thoracic and abdominal region and around 20% cover the whole body. In practice, whole body scans are acquired only rarely, most often to support the fast assessment of injuries in cases of polytrauma patients <ref type="bibr" target="#b8">[9]</ref>. In general, different types of CT scans are associated with different diagnostic routines, e.g. head CT scans can be used for diagnosis of brain hemorrhages, brain tumors and aneurysms, lower-limb CT scans for detecting complex bone fractures and tumors in the legs, CT scans of the thorax, abdomen and pelvis for cancer screening, etc. <ref type="bibr" target="#b8">[9]</ref>. As such, most of the scans capture challenging anatomical malformations, i.e. large tumors or anomalies. In the preprocessing stage, all volumes were resampled to an isotropic resolution of 2 mm for the finest scale level in the scale-space representation. For our scale-space we used 3 additional levels at isotropic spatial resolutions of 4 mm, 8 mm and 16 mm. We clipped the voxel values to the useful 0-800 HU interval and then normalized this interval to unit-range, i.e. [0, 1].</p><p>We selected a set of 8 anatomical landmark points, covering different types of structures, including bone, non-rigid organ, vessel bifurcation and respiratory tract bifurcation. These are the center of the left and right kidneys, the front corner of the left and right hip bones, the bronchial bifurcation, as well as three vessel bifurcations between the aortic arch and the left subclavian artery, the left common carotid artery and the brachiocephalic artery (see Figure <ref type="figure" target="#fig_8">6</ref>). This set of landmarks was selected to test the robustness of the solutions to the type/contrast of the anatomical structure, as well as their ability to cope with large variation of nonrigid organs and the confusion of nearby vessel bifurcations with similar appearance. Ground-truth annotations were provided by radiologists. Vessel bifurcation and bone landmarks are anatomically defined very precisely, allowing for very accurate annotations at original resolution. For bone landmarks the average inter-observer variability was reported in the literature at 1 mm, while the intra-observer variability was 0.9 mm <ref type="bibr" target="#b54">[55]</ref> (using three expert annotators). This precision level is in agreement with the findings of the study of Chien P.C. et al. <ref type="bibr" target="#b55">[56]</ref>. In contrast, the variation in the annotation of the kidney center is higher, proportional to the size of the structure. Also for such landmarks, the inter-user variability is in general low in 3D-CT <ref type="bibr" target="#b36">[37]</ref>.</p><p>The validation setup is based on a random split of the annotated volumes in approximately 80% training and 20% unseen testing examples for each landmark. The split was performed at patient level, meaning that all scans of a given patient are either in the training set or the test set. The total number of available ground-truth annotations per landmark are: 1438 (center of left kidney), 1432 (center of right kidney), 552 (front corner of left hip-bone), 1054 (front corner of right hip-bone), 1046 (bronchial bifurcations), 1028 (bifurcation of left subclavian artery), 1048 (bifurcation of left common carotid artery and left subclavian artery), 1048 (bifurcation of left common carotid artery and brachiocephalic artery).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">System Training</head><p>We used a scale-space of 3 scale levels at 4 mm (fine) -8 mm -16 mm (coarse) isotropic spatial resolutions for  the detection of each kidney and include one additional scale level at 2 mm isotropic spatial resolution for the remaining landmarks. We empirically observed that using a finest resolution of 4 mm in this case, yields optimal results both in terms of speed and accuracy. In order to cope with the intensive memory requirements, the SADNN and Overfeat methods were trained on all landmarks using a finest resolution of 4 mm. All other methods, including ours, were trained on a finest isotropic resolution of 4 mm for the kidneys and 2 mm for the remaining landmarks.</p><p>We optimized all meta-parameters on the example of one arbitrary landmark -here the front corner of the right hipbone. Using a patient-based split in training (70%), validation (10%) and testing (20%) sets, we selected the optimal algorithm meta-parameters and the network architecture using a systematic search. Shared on each scale (except the search-range on coarsest scale), the meta-parameters are specified in Table <ref type="table" target="#tab_0">1</ref>. The CNN used to encode the search policy per scale is defined as: conv-layer (32 kernels: 4×4×4, ReLU), pooling (2×2×2), conv-layer (46 kernels: 3×3×3), pooling (2×2×2) and four fully-connected layers (512×256×128×6 units, ReLU). Our implementation is based on the Theano library <ref type="bibr" target="#b56">[57]</ref>. The training time per landmark averaged to 4 hours on an Nvidia Titan X GPU. We trained all models in a 16-GPU cluster in around 3 hours.</p><p>The training criterion was the Bellman error <ref type="bibr" target="#b52">[53]</ref>, which measures the quality of the policy on each scale level. Figure <ref type="figure" target="#fig_9">7</ref> shows the evolution of the Bellman error during training for the front corner of the right hip-bone. Overlayed in the same plot is the randomness of the exploration denoted by the variable . Please note that the target is a low Bellman-error with minimal exploration randomness, i.e. near-deterministic on-policy search. Choosing the randomness decay-rate too high causes the system to fail to train, while a too low decay-rate can lead to overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">3D Landmark Detection in CT-Scans</head><p>Given a trained multi-scale set of search-models for one landmark on M different scales Θ = [ θ0 , θ1 , . . . , θM-1 ], the search process starts on the coarsest scale M -1 from the expected location of the landmark, computed on the training set. Using the search model θM-1 , the search is executed on the corresponding scale until reaching the convergence point (recall from section 3 the formal definition of convergence point). The process is repeated analogously on all following scales M -2, . . . , 0, using at each scale t as starting point the convergence point from scale t + 1.</p><p>To assess the success rate of each algorithm from an anatomical validity perspective, we imposed hard thresholds which were set to 30 mm for the center of each kidney and 10 mm for the remaining landmarks. Any detection above this limit was considered an outlier. Using this failure criterion, we first computed a failure percentage rate for all algorithms. Within the set of failed cases, we show the median and maximum error to give a sense of the error magnitude. For the remaining detections, we computed regular statistics (mean, median and standard deviation) to measure the accuracy. Table <ref type="table" target="#tab_2">2</ref> shows all these metrics for all algorithms and each considered landmark.</p><p>Our method achieves 0% failure rate on all considered landmarks, improving the average accuracy of the reference methods by 20-30% (see error distribution in Figure <ref type="figure" target="#fig_5">9</ref>). Through the multi-scale search our solution mimics a natural focusing mechanism, starting from global context to low-level image details on the finest scale. This helps to gain robustness to structures with similar anatomical appearance and thereby considerably reduce the number of miss-detections. Please note that in the implementation of the method of Donner et al. <ref type="bibr" target="#b9">[10]</ref>, we did not apply the MRF graph-matching step for hypothesis selection, which exploits the spatial relationship between the landmarks. This was not possible in our case of incomplete data, where several landmarks might be missing from the field-of-view. Instead, we applied an averaging scheme.</p><p>For a deeper insight into the performance of additional existing solutions in comparison to our method, please see Table <ref type="table">3</ref>. Please note that we did not implement these methods, but simply show their performance, as reported in the literature. The comparison in Table <ref type="table">3</ref> demonstrates that our method not only achieves the highest voxel-accuracy, but also the quickest detection time for the use-case of 3D-CT. More importantly, these results are obtained on the largest dataset reported in literature, consisting of 1487 CT scans from 532 patients. We also measured the performance of our method on an additional dataset of 506 3D-MR scans from 506 patients (see Appendix C). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Runtime Analysis</head><p>In terms of detection speed, the 3D deep learning solution proposed in <ref type="bibr" target="#b17">[18]</ref> clocked the highest runtime, averaging 61.56 seconds on CPU to detect one landmark. This high runtime is explained by the fact that scanning was performed on the complete volume on finest resolution. The Overfeat method <ref type="bibr" target="#b21">[22]</ref> improved this number by applying an efficient convolutional forward-propagation scheme, running in under 2.5 seconds. A further increase in speed to an average runtime of less than 0.7 seconds was achieved by the SADNN solution <ref type="bibr" target="#b0">[1]</ref>, by using a cascade of efficient sparse shallow models to pre-filter the large number of negative hypotheses. We emphasize that these results were achieved at a finest isotropic resolution of 4 mm, meaning that the test volumes were 8 times smaller than the same volumes at 2 mm. The PBT-based solution heuristically aggregates multi-scale hypotheses to constrain the search range on fine resolution. This strategy increased the average detection speed by around 50 times, reaching 1 second. In comparison, the method proposed by Donner et al. <ref type="bibr" target="#b9">[10]</ref> achieved a median detection time of 4.7 seconds. However, none of these methods could match the detection speed of our solution. Learning the multi-scale search trajectory and evaluating samples only on a single path, as opposed to scanning the image space, led to a median runtime of 33 milliseconds (slowest runtime: 85 ms) -an unmatched real-time performance for landmark detection in 3D-CT. The detection speed was also in similar range on CPU. The improvement against the reference methods was around 2-3 orders of magnitude (see Table <ref type="table" target="#tab_3">4</ref>).</p><p>In addition, the detection speed of our method has the property of scaling sublinearly with respect to the scan size. While the runtime of all reference methods increases linearly with the volume size N , i.e. the number of voxels in the volume, in our case the increase is proportional to 3 √ N (for more details please see Figure <ref type="figure" target="#fig_10">8</ref> and proof in Appendix B). As such, our method can also easily be applied on higher resolutions, e.g. less than 0.5 mm spatial resolution, where it can achieve similar detection speed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Object Not in the Scan Range?</head><p>We empirically estimated the accuracy of our algorithm in recognizing the absence of the landmark from the fieldof-view of the scan. For this, we selected the bronchial bifurcation landmark and 100 random images from the 188 test-images. Each image was randomly cropped along the Z-axis to eliminate the landmark from the field-of-view. The image cut was performed at a distance of at least 1 cm from the landmark. During navigation, we investigated whether the search trajectory leaves the image space on any of the scale levels through the correct volume border. We empirically found that, on at least 97% (an average of 99.2%) of the images, the search trajectory leaves the image space, regardless of the selected starting point. Similar percentages are achieved also for the hip-bones. i.e. 97.8%, respectively 98.2%. In contrast, for the kidney center the accuracy is lower, i.e. 92.2% for left and 90.5% for the right kidney. The reason for this decrease are border cases. For many thoracic CT scans, the kidney-centers are used as lower-limits for the field-of-view. This results in many challenging test examples in which the kidney center is very close to the border. Note that solutions based on scanning show a linear correlation between volume size and execution time. We remind the reader that for technical reasons the SADNN and Overfeat solutions are evaluated at a finest isotropic resolution of 4 mm while the other methods are evaluated at 2 mm. Our approach not only improves the average speed, but also scales sublinearly with respect to the size of the input volume. In parallel work, we address this challenge by enforcing the spatial coherence of the visible landmarks using robust statistical shape models. Particularly for border-cases, we investigate the benefits of explicitly training the navigation models on cases where target objects are outside the fieldof-view. These ideas fall out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">A Computer Vision Perspective</head><p>Although motivated in the context of anatomical landmark detection in large 3D-CT scans, our method can also be applied to different 2D problems from both the medical domain <ref type="bibr" target="#b20">[21]</ref> and computer vision -where data is unstructured and objects are often occluded. Several very recent publications demonstrate competitive results on a variety of computer vision tasks: Object Localization Using convolutional neural networks as pre-trained feature extractors, deep Q-learning can be applied to learn a policy for hierarchical object localization <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Using tree-structured parsing schemes, Jie et al. <ref type="bibr" target="#b59">[60]</ref> propose to use DRL for the sequential search of multiple objects. Reinforcement learning is also used to learn stochastic policies for how and where to apply detectors for object localization <ref type="bibr" target="#b60">[61]</ref>. Reported results on the PASCAL VOC 2012 dataset show significant speedimprovements over conventional solutions.</p><p>Tracking Trajectory learning can also be formulated for image sequences over time, e.g. video frames, to support active object tracking <ref type="bibr" target="#b61">[62]</ref>. By using deep recurrent models, the learned tracking policy captures motion patterns over time. Reinforcement learning is also used to model the lifetime and visibility of objects in online tracking <ref type="bibr" target="#b62">[63]</ref>.</p><p>Visual Navigation Deep actor-critic models are effectively applied to visual navigation tasks <ref type="bibr" target="#b63">[64]</ref>, such as robot navigation or autonomous driving. In this formulation the actor model learns the action selection policy using the feedback from the critic, which estimates the long-term reward.</p><p>The principles of multi-scale deep reinforcement learning, introduced in this work, might address the limitations of the aforementioned methods and increase their performance. First, using an explicit scale-space model to represent the state space allows for an improved system scalability for different object scales. This property is important for localization, tracking and navigation tasks based on photographs, for which there is typically no prior information about scale <ref type="bibr" target="#b19">[20]</ref>. In addition, as demonstrated in our applications, modeling the object search across scales significantly increases the effectiveness of the exploration, leading to a more robust and globally consistent navigation policy, that is invariant to ambiguous local image information. This is particularly important e.g. for visual navigation for autonomous driving or tracking in high-dynamic scenes <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b63">[64]</ref>. Second, coupling the concept of navigation with a scale-space representation increases the detection speed, which scales sub-linearly with respect to the scan size. This enables the effective exploitation of raw high-resolution data to increase the detection accuracy while maintaining realtime performance -an important requirement for tracking and online navigation systems. Finally, training the system end-to-end leads to a high policy performance. This was emphasized in <ref type="bibr" target="#b18">[19]</ref> in the context of game playing, and also empirically observed in our experiments on medical image data. In contrast, Caicedo et al. <ref type="bibr" target="#b57">[58]</ref> and Bellver et al. <ref type="bibr" target="#b58">[59]</ref> apply pre-trained feature extractors to get an embedding of the state, which is used as input for the policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we presented a novel method for accurate realtime 3D anatomical landmark detection in CT scans. Based on the reformulation of the problem as a generic behavioral learning task, we combine the concept of deep reinforcement learning with multi-scale image analysis to enable an artificial agent to systematically learn optimal strategies for finding anatomical structures. Experiments show that our method is robust against outliers and achieves an average accuracy improvement of 20-30% over the selected reference solutions. At the same time, the detection speed of our algorithm is 2-3 orders of magnitude faster, reaching realtime performance on high-resolution 3D-CT volumes. Our For each of the considered anatomical landmarks our method reduces the number of failed detections to zero and improves the average and median error by around 20-30%. Note that the plot displays the distribution of detection errors that are smaller than 70 mm and does not show very large outliers above this value (noted in Table <ref type="table" target="#tab_2">2</ref>).</p><p>solution can also elegantly handle the case of absent objects and can be extended to support the simultaneous detection of multiple objects. Through high robustness and real-time performance, the proposed method might represent an important component of next-generation clinical technologies that contribute to better, faster and more reproducible patient diagnosis, therapy and disease management.</p><p>Disclaimer This feature is based on research, and is not commercially available. Due to regulatory reasons its future availability cannot be guaranteed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic overview of the proposed machine learning-based paradigm for anatomical landmark detection. The detection problem is reformulated to learning a navigation strategy, which exploits the scalespace representation of a given image. In other words, an artificial agent learns how to search for an anatomical structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Schematic visualization of the decision-based search model in state s.Six possible actions allow for voxel-wise movement in the volumetric image space. In this synthetic example the optimal decision with respect to the cumulative future reward is to go left, to state s . The dashed red line represents the optimal search-trajectory to the anatomical landmark, while the circles represent neighboring voxels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Visualization of the detection pipeline for the right kidney. The search starts on the coarsest scale level L d (2). On each scale L d (k), k ≥ 0, the agent navigates until convergence. This convergence point (see definition in section 3.4.1) is used as a starting point for the subsequent scale level L d (k -1) (red dashed arrows indicate the change of scale). The process continues analogously on the following scale levels, with the convergence point on the finest scale marked as the detection result. We visualize with white arrows the optimal 3D search trajectories navigated at each scale. Along the trajectories we visualize the sequence of states, represented as 3D boxes of image context, centered at the location of the agent. The orange frame represents the constrained region sampled and explored during training on each scale. On the coarsest scale, this region always covers the entire 3D volume, with decreasing range on finer scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>including the range of the staterepresentation, i.e. the size of the extracted box of image intensities. The search starts at the coarsest scale level M -1, where the search-model Q(•, •; θ M -1 | L d , M -1) is trained for convergence from any starting point in the image (we define the convergence criterion in section 3.4.1). On this Training Multi-Scale DRL for Detection 1: Given N training 3D-CT scans: I 1 , I 2 , . . . , I N 2: Define discrete scale-space: L d (t)| 0≤t&lt;M 3: Initialize system memory: M(0, . . . , M -1) = [ ] 4: Initialize exploration factor: = 1.0 5: Initialize model parameters θ t | 0≤t&lt;M randomly 6: while &gt; 0.05 do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9 :</head><label>9</label><figDesc>Sample -greedy pathT with Q(•, •; θ t | L d , t) 10: M(t) ← M(t) ∪ [T ]11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>gtruth[I k ] d size[I k ] d , where size[I k ] d ∈ N denotes the image size, i.e. number of voxels in dimension d, and gtruth[I k ] d ∈ R + denotes the coordinate of the ground-truth landmark annotation in dimension d. Based on r, we define the starting point as p 0 = size[I] r, where the operator denotes an element-wise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Visualization of all anatomical landmarks used for evaluation. On the left we visualize a section of a whole body 3D-CT scan including neck, thorax, abdomen and pelvis. On the right we highlight the different anatomical structures and mark the individual landmark points: bronchial bifurcation (1), bifurcation of left subclavian artery (2), bifurcation of left common carotid artery and left subclavian artery (3), bifurcation of left common carotid artery and brachiocephalic artery (4), center of right kidney (5), center of left kidney (6), front corner of right hip-bone (7), front corner of left hip-bone (8).</figDesc><graphic coords="8,52.80,43.70,108.18,206.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Visualization of the mean squared error in the Bellman equation on each scale level during training of the right hip-bone landmark. The plot also visualizes the progression of the variable during training -this value controls the randomness in the exploration and decays from 1 to 0.05. Note that does not measure an error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Fig.8. Scatter plot showing the correlation between scan size and detection speed for all considered methods on the right hip-bone landmark. Note that solutions based on scanning show a linear correlation between volume size and execution time. We remind the reader that for technical reasons the SADNN and Overfeat solutions are evaluated at a finest isotropic resolution of 4 mm while the other methods are evaluated at 2 mm. Our approach not only improves the average speed, but also scales sublinearly with respect to the size of the input volume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>RFig. 9 .</head><label>9</label><figDesc>Fig.9. Comparison of the four best performing solutions (regarding accuracy and failures). For each of the considered anatomical landmarks our method reduces the number of failed detections to zero and improves the average and median error by around 20-30%. Note that the plot displays the distribution of detection errors that are smaller than 70 mm and does not show very large outliers above this value (noted in Table2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Values of all meta-parameters required to train our system.</figDesc><table><row><cell></cell><cell cols="2">Description</cell><cell></cell><cell></cell><cell cols="2">Value</cell></row><row><cell></cell><cell cols="3">Training rounds</cell><cell></cell><cell cols="2">500</cell></row><row><cell></cell><cell cols="4">Episode length (linear decay)</cell><cell cols="2">1000 → 50</cell></row><row><cell></cell><cell cols="2">State size</cell><cell></cell><cell></cell><cell cols="3">25×25×25 vox.</cell></row><row><cell></cell><cell cols="3">Max. search range</cell><cell></cell><cell cols="2">±10 voxels</cell></row><row><cell></cell><cell cols="4">Optimal scale-space factor</cell><cell>2</cell><cell></cell></row><row><cell></cell><cell cols="3">Initial/Final exploration</cell><cell></cell><cell></cell><cell cols="2">= 100% / 5%</cell></row><row><cell></cell><cell cols="3">Exploration decay</cell><cell></cell><cell cols="2">200000</cell></row><row><cell></cell><cell cols="4">Network update frequency</cell><cell cols="2">14</cell></row><row><cell></cell><cell cols="4">Replay memory size / batch size</cell><cell cols="2">100000 / 256</cell></row><row><cell></cell><cell cols="3">Reference-freeze interval</cell><cell></cell><cell cols="2">10000</cell></row><row><cell></cell><cell cols="4">Min. required memory size</cell><cell cols="2">10000</cell></row><row><cell></cell><cell cols="3">Discount factor</cell><cell></cell><cell cols="2">0.9</cell></row><row><cell></cell><cell cols="3">Optimization method</cell><cell></cell><cell cols="3">RMS-prop [19]</cell></row><row><cell></cell><cell cols="2">Learning rate</cell><cell></cell><cell></cell><cell cols="2">0.0005</cell></row><row><cell></cell><cell cols="3">RMS-decay/epsilon</cell><cell></cell><cell cols="2">0.95 / 0.01</cell></row><row><cell></cell><cell cols="3">Nesterov momentum</cell><cell></cell><cell>0</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ε exploration</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">16-mm scale</cell></row><row><cell>Bellman MSE</cell><cell>0.4 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8-mm scale 4-mm scale 2-mm scale</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>0</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Training Iterations [x1000]</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 3 Table showing a</head><label>3showing</label><figDesc>general comparison against different solutions for anatomical landmark detection in large high-resolution scans. The criteria are the average detection-accuracy and runtime (on CPU), as well as the size of the evaluation set, i.e. number of patients / scans, and data type (CT or MR, i.e. magnetic resonance).</figDesc><table><row><cell>Solution</cell><cell>Dataset Size (Data/Patients)</cell><cell>Accuracy (mm)</cell><cell>Speed (seconds)</cell></row><row><cell>Zhan et al. [27]</cell><cell>18/18 CT</cell><cell>4.72</cell><cell>4</cell></row><row><cell>Fenchel et al. [35]</cell><cell>31/31 MR</cell><cell>22.4</cell><cell>20</cell></row><row><cell>Criminisi et al. [12]</cell><cell>100/-CT</cell><cell>17.60</cell><cell>1</cell></row><row><cell>Pauly et al. [32]</cell><cell>33/33 MR</cell><cell>14.95</cell><cell>0.8</cell></row><row><cell>Cuingnet et al. [11]</cell><cell>233/89 CT</cell><cell>10.5</cell><cell>2.8</cell></row><row><cell>Donner et al. [10]</cell><cell>20/20 CT</cell><cell>5.25</cell><cell>120</cell></row><row><cell>Criminisi et al. [31]</cell><cell>400/-CT</cell><cell>13.50</cell><cell>4</cell></row><row><cell>Chu et al. [30]</cell><cell>10/10 CT</cell><cell>1.90 1</cell><cell>30</cell></row><row><cell>Potesil et al. [37]</cell><cell>83/83 CT</cell><cell>4.70</cell><cell>N/A</cell></row><row><cell>de Vos et al. [24]</cell><cell>100/-CT</cell><cell>4.80</cell><cell>10</cell></row><row><cell>Ours</cell><cell>1487/532 CT</cell><cell>4.19 2</cell><cell>0.061</cell></row><row><cell cols="4">1 Evaluated only on vertebrae localization with strong priors.</cell></row><row><cell cols="4">2 With no failures of clinical significance. All other solutions did not</cell></row><row><cell cols="2">provide any information in this respect.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2 Table</head><label>2</label><figDesc>showing results on different anatomical landmarks. The first three columns indicate the percentage of failed cases, as well as the median and maximum error within this group. The accuracy is measured on successful detections (excluding failed cases). The error is measured in mm.</figDesc><table><row><cell>Failed Cases</cell><cell>Accuracy (excl. failed cases)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 Table</head><label>4</label><figDesc>showing the runtime performance on the example of the right hip-bone landmark. For this landmark the scans cover at least 60% of the patient height at an average size of 250 × 250 × 500 voxels at 2 mm isotropic spatial resolution. The speed is measured in seconds.</figDesc><table><row><cell cols="2">DETECTION TIME per Volume [seconds]</cell><cell></cell><cell></cell></row><row><cell>Method Platform</cell><cell>Median</cell><cell>Min</cell><cell>Max</cell></row><row><cell>PBT [3] CPU 8-core</cell><cell>1.051</cell><cell>0.14</cell><cell>16.20</cell></row><row><cell>ExtRTrees [10] CPU 8-core</cell><cell>4.714</cell><cell>0.272</cell><cell>44.535</cell></row><row><cell>Overfeat 1 [22] GPU Titan X</cell><cell>2.175</cell><cell>0.331</cell><cell>14.86</cell></row><row><cell>3D-DL [18] CPU 10-core</cell><cell>57.034</cell><cell>7.161</cell><cell>548.23</cell></row><row><cell>SADNN 2 [1] GPU GTX 1080</cell><cell>0.471</cell><cell>0.064</cell><cell>3.029</cell></row><row><cell>Ours CPU 8-core</cell><cell>0.061</cell><cell>0.035</cell><cell>0.155</cell></row><row><cell>Ours GPU Titan X</cell><cell>0.033</cell><cell>0.018</cell><cell>0.085</cell></row></table><note><p><p>1,2 </p>Evaluated on finest isotropic resolution of 4 mm.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We thank David Liu for his contributions to this work during his time at Medical Imaging Technologies, Siemens Healthineers, Princeton NJ.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Florin-Cristian Ghesu is a PhD student at the Pattern Recognition Lab of the Friedrich-Alexander University Erlangen-Nuremberg, Germany and is supervised by Prof. Dr.-Ing. Joachim Hornegger. His research is conducted in close collaboration with the Medical Imaging Technologies team at Siemens Healthineers, Princeton, New Jersey, where his work is overseen by Dr. Dorin Comaniciu and Dr. Bogdan Georgescu. Holder of an elite scholarship from the German Academic Exchange Service, he has won several international awards including gold and silver medals in algorithmic competitions of the Association for Computing Machinery. He is a winner of the MICCAI 2017 Young Scientist Award and the BVM 2015 Award, the latter recognizing the best graduation thesis in the field of medical image analysis in Germany. His work focuses on developing technologies for medical image understanding. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bogdan</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Marginal space deep learning: Efficient architecture for volumetric image parsing</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Krubasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1217" to="1228" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully automatic segmentation of the mitral leaflets in 3D transesophageal echocardiographic images using multi-atlas joint label fusion and deformable medial modeling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Pouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H G</forename><surname>Iii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sehgal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="118" to="129" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Four-chamber heart modeling and automatic segmentation for 3-D cardiac CT volumes using marginal space learning and steerable features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scheuering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1668" to="1681" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a deep compact image representation for visual tracking</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="809" to="817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust registration of longitudinal spine CT</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zikic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Haynor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">8673</biblScope>
			<biblScope unit="page" from="251" to="258" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust 5DOF transesophageal echo probe tracking at fluoroscopic frame rates</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Hatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prediction based collaborative trackers (PCT): A robust and accurate approach toward 3D medical object tracking</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1921" to="1932" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient simulation of blood flow past complex endovascular devices using an adaptive embedding technique</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Cebral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lohner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="468" to="476" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Trends in computed tomography utilization rates: A longitudinal practice-based study</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Stroebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Denham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Swensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Patient Safety</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global localization of 3D anatomical structures by pre-filtered Hough forests and discrete optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Donner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1304" to="1314" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic detection and segmentation of kidneys in 3D CT images using random forests</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cuingnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prevost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lesage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ardon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">7512</biblScope>
			<biblScope unit="page" from="66" to="74" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Regression forests for efficient anatomy detection and localization in CT studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Computer Vision: Recognition Techniques and Applications in Medical Imaging</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">6533</biblScope>
			<biblScope unit="page" from="106" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">From local to global random regression forests: Exploring anatomical landmark localization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Štern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-organ localization with cascaded global-to-local regression and shape prior</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gauriau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cuingnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lesage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="83" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Regressing heatmaps for multiple landmark localization using CNNs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Štern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="230" to="238" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D U-net: Learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName><forename type="first">Ö</forename></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">9901</biblScope>
			<biblScope unit="page" from="424" to="432" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D Deep learning for efficient and robust landmark detection in volumetric data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">9349</biblScope>
			<biblScope unit="page" from="565" to="572" />
			<date type="published" when="2015">2015</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scale-Space Theory in Computer Vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="101" to="122" />
		</imprint>
	</monogr>
	<note>Scale-space for N-D discrete signals</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An artificial agent for anatomical landmark detection in medical images</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Ghesu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hornegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">9902</biblScope>
			<biblScope unit="page" from="229" to="237" />
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno>abs/1312.6229</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Probabilistic boosting-tree: Learning discriminative models for classification, recognition, and clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1589" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">2D image classification for 3D anatomy localization: employing deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>De Vos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolterink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>De Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Išgum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging: Image Processing</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9784</biblScope>
			<biblScope unit="page" from="9784" to="9791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="511" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Search strategies for multiple landmark detection by submodular maximization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bernhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2831" to="2838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Active scheduling of organ detection and segmentation in whole-body medical images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">5241</biblScope>
			<biblScope unit="page" from="313" to="321" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully automatic localization and segmentation of 3D vertebral bodies from CT/MR images via a learning-based method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Belav Ý</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Armbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Felsenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decision Forests for Computer Vision and Medical Image Analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Konukoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinez M Öller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nekolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anatomy Detection and Localization in 3D Medical Images</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="193" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast multiple organ detection and localization in whole-body MR Dixon sequences</title>
		<author>
			<persName><forename type="first">O</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mateus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Öller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nekolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">6893</biblScope>
			<biblScope unit="page" from="239" to="247" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable object detection using deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2155" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic labeling of anatomical structures in MR fastView images using a statistical atlas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fenchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schilling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<biblScope unit="volume">5241</biblScope>
			<biblScope unit="page" from="576" to="584" />
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-atlas-based segmentation with local decision fusion: Application to cardiac and aortic segmentation in CT scans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Isgum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Staring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rutten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1000" to="1010" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Personalized graphical models for anatomical landmark localization in wholebody medical images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Potesil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Platsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="49" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust and accurate shape model matching using random forest regression-voting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Bromiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Ionita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1862" to="1874" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Norvig</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: A Modern Approach</title>
		<imprint>
			<publisher>Pearson Education</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep neural networks segment neuronal membranes in electron microscopy images</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="2843" to="2851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shape and arrangement of columns in cat&apos;s striate cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="559" to="568" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Explorations in the microstructure of cognition: Foundations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Introduction to Reinforcement Learning, 1st ed</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dynamic Programming, 1st ed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Q-Learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">How precise can bony landmarks be determined on a CT scan of the knee?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Doninck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Labey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Innocenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Parizel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bellemans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Knee</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="358" to="365" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Comparison of reliability in anatomical landmark identification using two-dimensional digital cephalometrics and threedimensional cone beam computed tomography in vivo</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Parks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eraso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hartsfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ofner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dentomaxillofacial Radiology</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="262" to="273" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Active object localization with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2488" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hierarchical object detection with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gir Ó I Nieto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<idno>abs/1611.03718</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Tree-structured reinforcement learning for sequential object localization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Reinforcement learning for visual object detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2894" to="2902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for visual object tracking in videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1701.08936</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to track: Online multi-object tracking by decision making</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4705" to="4713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1609.05143</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
