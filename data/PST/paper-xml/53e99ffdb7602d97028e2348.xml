<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Feature Selection for Graph Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangnan</forename><surname>Kong</surname></persName>
							<email>xkong4@uic.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>at Chicago 851 S. Morgan Street Chicago</addrLine>
									<postCode>60607-0753</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
							<email>psyu@cs.uic.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<addrLine>at Chicago 851 S. Morgan Street Chicago</addrLine>
									<postCode>60607-0753</postCode>
									<region>IL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Semi-Supervised Feature Selection for Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">53AFC25D794E020B0E33651B1B047B6F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.8 [Database Management]: Database Applications-Data Mining Algorithm</term>
					<term>Performance</term>
					<term>Experimentation Semi-Supervised Learning</term>
					<term>Feature Selection</term>
					<term>Graph Classification</term>
					<term>Data Mining</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of graph classification has attracted great interest in the last decade. Current research on graph classification assumes the existence of large amounts of labeled training graphs. However, in many applications, the labels of graph data are very expensive or difficult to obtain, while there are often copious amounts of unlabeled graph data available. In this paper, we study the problem of semi-supervised feature selection for graph classification and propose a novel solution, called gSSC, to efficiently search for optimal subgraph features with labeled and unlabeled graphs. Different from existing feature selection methods in vector spaces which assume the feature set is given, we perform semi-supervised feature selection for graph data in a progressive way together with the subgraph feature mining process. We derive a feature evaluation criterion, named gSemi, to estimate the usefulness of subgraph features based upon both labeled and unlabeled graphs. Then we propose a branch-and-bound algorithm to efficiently search for optimal subgraph features by judiciously pruning the subgraph search space. Empirical studies on several real-world tasks demonstrate that our semi-supervised feature selection approach can effectively boost graph classification performances with semi-supervised feature selection and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Graphs are ubiquitous and have become increasingly important in modeling diverse kinds of objects. In many realworld applications, instances are not represented as feature vectors, but as graphs with complex structures, e.g., chemical compounds, program flows and XML web documents. One central issue in graph mining research is graph classification, which has a wide variety of real world applications, e.g. drug activity predictions, toxicology tests and kinase inhibitions. A major difficulty in graph classification lies in the complex structure of graphs and lack of vector representations. Selecting a proper set of features for graph data is an essential and important procedure for graph classification.</p><p>The general problem of feature selection is well studied in the literature. Semi-supervised feature selection problem for graph data, however, has not been studied in this context so far. Conventional feature selection approaches on graph data assume, explicitly or implicitly, that there exists a large amount of labeled training data. However, in many real world applications, the labels of graph data are very expensive or difficult to obtain. Creating a large training dataset can be too expensive, time-consuming or even infeasible. For example, in molecular medicine, it requires time, efforts and excessive resources to test drugs' anti-cancer efficacies by pre-clinical studies and clinical trials, while there are often copious amounts of unlabeled drugs or molecules available from various sources.</p><p>Thus it is much desired that the large amounts of unlabeled graphs can be effectively utilized to select better features for graphs, and improve the graph classification performances. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, we show a dataset with two labeled graphs and four unlabeled graphs. Based only on the two labeled graphs, subgraph feature "a-b" and "a-c" are both discriminative features. Clearly, when we consider the distribution of the four unlabeled graphs, "a-b" is more likely to be useful than "a-c". This is because the unlabeled graphs are not separable based on the subgraph feature "ac".</p><p>Despite its value and significance, the semi-supervised feature selection for graph classification is a much more challenging task due to the specific characteristics of the task. The reasons are listed as follows. 1. Lack of labels. Conventional feature selection in graph classification approaches focuses on supervised settings <ref type="bibr" target="#b8">[7,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b14">13]</ref>. The mining strategy of discriminative subgraph patterns strictly follows the assumption that there exists a large amount of labeled graphs. However, many real-world graph classifications usually suffer from a lack of training graphs. It is usually laborious, or even infeasible to create a large training set of graph instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Lack of features.</head><p>Another fundamental problem in semi-supervised feature selection on graph data lies in the complex structures and lack of feature representations of graphs. Conventional feature selection approaches in vector spaces, which assume a candidate feature set is available, cannot be directly applied to graph data, because it is usually infeasible to generate all the subgraph features of a graph dataset before feature selection. The number of subgraphs is usually too large to be fully generated, since it grows exponentially with the graph size. Furthermore checking subgraph isomorphism is NP-complete.</p><p>In order to efficiently find discriminative subgraph features, conventional supervised subgraph feature mining approaches rely on the label information from a large training set to prune the subgraph search space and select useful features <ref type="bibr" target="#b15">[14]</ref>. However, when the number of labeled graphs is not large enough, the usefulness of the mined subgraph features can be weak, and the pruning of the subgraph mining process can be ineffective.</p><p>Figure <ref type="figure">2</ref> illustrates the feature selection process in conventional graph classification approaches. Obviously, when there is only a small number of labeled graphs available, supervised approaches cannot work well due to two reasons: (1) During the subgraph features mining procedure, supervised feature selection approaches for graph classification need to employ evaluation criteria to select discriminative subgraph features based on labeled graphs. However, when the labeled graphs are too few, the usefulness of the selected subgraph features can be weak, and thus detriment to the classification performances. (2) During the subgraph feature mining procedure, most supervised graph classification approaches require a branch-and-bound search to avoid exhaustive enumeration of all subgraphs in a dataset. However, when there are not enough labeled graphs, the pruning ability of the upper-bound based on labeled graphs can be poor, thus making it infeasible to find discriminative subgraph features within a reasonable amount of time.</p><p>In this paper, we introduce a novel framework to the above problems by mining subgraph features using both labeled and unlabeled graphs. Our framework is illustrated in Figure <ref type="figure">3</ref>. Different from existing supervised feature selection methods for graph classification, our approach, called gSSC, can utilize both labeled and unlabeled graphs to find optimal subgraph features for graph classification. We first derive a feature evaluation criterion, named gSemi, based upon a given graph dataset with both labeled and unlabeled graphs. Then we propose a branch-and-bound algorithm to efficiently search for optimal subgraph features by deriving an upper-bound of gSemi and pruning the subgraph search space using labeled and unlabeled graphs. In order to evaluate our model, we perform comprehensive experiments on real-world graph classification tasks. The experiments demonstrate that the proposed semi-supervised feature selection method for graph classification outperforms supervised approaches and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs.</p><p>The rest of the paper is organized as follows. We start by a brief review on related works of graph feature selection and semi-supervised feature selection in Section 2. We then introduce the preliminary concepts, give the problem analysis and present the gSemi criterion in Section 3. In Section 4, we derive an upper-bound of gSemi and propose the gSSC method. Then Section 5 reports the experiment results on real-world graph classification tasks. In Section 6, we conclude the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>To the best of our knowledge, this paper is the first work on semi-supervised feature selection problem for graph classification. Some research works have been done in related areas.</p><p>Extracting subgraph features from graph data have been investigated by many researchers. The goal of such approaches is to extract informative subgraph features from a set of graphs. Typically some filtering criteria are used. Upon whether considering the label information, there are two types of approaches: unsupervised and supervised. A typical evaluation criterion is frequency, which aims at collecting frequently appearing subgraph features. Most of the frequent subgraph feature extraction approaches are unsupervised. For example, Yan and Han develop a depth-first search algorithm: gSpan <ref type="bibr" target="#b16">[15]</ref>. This algorithm builds a lexicographic order among graphs, and maps each graph to an unique minimum DFS code as its canonical label. Based on this lexicographic order, gSpan adopts the depth-first search strategy to mine frequent connected subgraphs efficiently. Many other approaches for frequent subgraph feature extraction have also been developed, e.g. AGM <ref type="bibr" target="#b6">[5]</ref>, FSG <ref type="bibr">[8]</ref>, MoFa <ref type="bibr" target="#b2">[2]</ref>, FFSM <ref type="bibr" target="#b5">[4]</ref>, and Gaston <ref type="bibr" target="#b11">[10]</ref>. Moreover, supervised subgraph feature extraction problem has also been studied in literature, such as LEAP <ref type="bibr" target="#b15">[14]</ref> and CORK <ref type="bibr" target="#b14">[13]</ref>, which look for discriminative subgraph patterns for graph classifications.</p><p>Dimensionality reduction and feature selection in vector spaces have also been studied. Several recent works use pairwise constraints as weak supervision for dimensionality reduction, i.e. must-link constraints <ref type="bibr" target="#b1">[1]</ref> (pairs of instances with the same class) and cannot-link constraints <ref type="bibr" target="#b13">[12]</ref> (pairs of intstances with different classes). Feature selection methods in vector spaces using both labeled and unlabeled instances have also been proposed <ref type="bibr" target="#b17">[16,</ref><ref type="bibr" target="#b12">11]</ref>, which select useful features within a pre-defined feature set. These methods assume that a set of candidate features is given before the feature selection. However, conventional semi-supervised feature selection approaches cannot be directly applied to graph data, because it is usually infeasible to generate all the subgraph features of a graph dataset before feature selection. The number of subgraphs is usually too large to be fully generated, since it grows exponentially with the graph size. Instead, our proposed semi-supervised feature selection for graph data works in a progressive way: the semi-supervised feature selection is integrated to the subgraph feature generation, which can skip most of the bad subgraph features without even generating them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROBLEM FORMULATION</head><p>In this section, we formulate the semi-supervised feature selection problem for graph classification based on subgraph features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semi-Supervised Feature Selection</head><p>Before presenting the semi-supervised feature selection model for graph classification, we first introduce the notations that will be used throughout this paper. Let D = {G1, • • • , Gn} denote the entire graph dataset, which consists of n graph objects, represented as connected graphs. The data set includes both labeled and unlabeled graphs. We assume that the first l graphs within D are labeled by {y1, • • • , y l }, where yi ∈ {-1, +1} denotes the binary class label assigned to Gi. For convenience, we also denote the labeled graph dataset by D l = {G1, • • • , G l }, and the unlabeled graph dataset as</p><formula xml:id="formula_0">Du = {G l+1 , • • • , Gn}, D = D l ∪ Du. Definition 1 (Connected Graph). A graph is rep- resented as G = (V, E, L), where V is a set of vertices V = {v1, • • • , vn v }, E ⊆ V × V is a set of edges, L</formula><p>is the set of symbols for the vertices and the edges. A connected graph is a graph such that there is a path between any pair of vertices.</p><formula xml:id="formula_1">Definition 2 (Subgraph). Let G = (V , E , L ) and G = (V, E, L) be connected graphs. G is a subgraph of G(G ⊆ G) iff: (1) V ⊆ V, (2) E ⊆ E, (3) L ⊆ L. If G is a subgraph of G, then G is a supergraph of G .</formula><p>In this paper, we adopt the idea of subgraph-based graph classification approaches, which assume that each graph object Gi is represented as a feature vector</p><formula xml:id="formula_2">xi = [x 1 i , • • • , x m i ] corresponding to a set of subgraph patterns {g1, • • • , gm}. Denote x k</formula><p>i as the binary feature associated with the subgraph pattern g k .</p><formula xml:id="formula_3">x k i = 1 iff g k is a subgraph of Gi (g k ⊆ Gi), otherwise x k i = 0.</formula><p>The key issue of semi-supervised feature selection for graph classification is how to find the most informative subgraph patterns from a limited number of labeled graphs and a large number of unlabeled graphs. So, in this paper, the studied research problem can be described as follow: in order to train an effective graph classifier, how to efficiently find a set of optimal subgraph features from both labeled and unlabeled graphs?</p><p>Mining the optimal subgraph features from both labeled and unlabeled graphs is a non-trivial task due to the following problems: (P1) How to properly evaluate the usefulness of a set of subgraph features based upon both labeled and unlabeled graphs?</p><p>(P2) How to find the optimal subgraph features within a reasonable amount of time by avoiding the exhaustive enumeration? The subgraph feature space of graph objects is usually too large, because the number of subgraphs grows exponentially with the size of the graphs.</p><p>It is infeasible to completely enumerate all the subgraph features for a given graph dataset.</p><p>In the following sections, we will first introduce the optimization framework for selecting informative subgraph features from labeled and unlabeled graphs. Next we will describe our subgraph mining strategy using the evaluation criteria derived from the optimization solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimization Framework</head><p>We first address the problem (P1) discussed in Section 3.1 by defining the subgraph feature selection as an optimization problem. Our target is to find an optimal set of subgraph features from both labeled and unlabeled graphs. Formally, let us introduce the following notations:</p><p>• S = {g1, g2, • • • , gm}: the given set of all the subgraph features, which are used to predict class membership of graph instances. Usually there is only a subset of the subgraph features T ⊆ S relevant to the graph classification task.</p><p>• T * : the optimal set of subgraph features T * ⊆ S.</p><p>• J(T ): an evaluation criterion to estimate the usefulness of subgraph feature subset T .</p><p>• X: the matrix consisting binary feature vectors using S to represent the graph dataset {G1, G2,</p><formula xml:id="formula_4">• • • , Gn}. X = [x1, x2, • • • , xn] = [f1, f2, • • • , fm] ∈ {0, 1} m×n , where X = [Xij ] m×n , Xij = 1 iff gi ⊆ Gj.</formula><p>The first l graphs are labeled as y1, • • • , y l .</p><p>• C and M: C = {(i, j)|yiyj = -1} denotes the cannotlink pairwise constraint sets among labeled graphs. M = {(i, j)|yiyj = 1} denotes the must-link pairwise constraint sets among labeled graphs.</p><p>We propose the following general optimization framework to select optimal subgraph feature set:</p><formula xml:id="formula_5">T * = argmax T ⊆S J(T ) s.t. |T | ≤ t,<label>(1)</label></formula><p>where | • | denotes the size of the feature set and t is the maximum number of feature selected. The objective function in Eq. 1 has two components: the evaluation criterion J(T ) and the subgraph features of graphs S.</p><p>We assume that the optimal subgraph features set should have the following properties: (a) cannot-link : labeled graphs in different classes should be far away from each other; (b) must-link : labeled graphs in the same class should be close to each other; (c) separability: unlabeled graphs should be able to be separated from each other. Intuitively, (a) and (b) only consider the constraints from labeled graphs, and tend to select the most discriminative subgraph features based on the graph labels. They are similar to the LDA <ref type="bibr" target="#b10">[9]</ref> criterion. Note (c) incorporates the distribution of unlabeled graphs, and tends to select the subgraph features that can separate graphs far from each other. It is similar to the PCA's assumption, which is expressed as the average squared distance between unlabeled samples. An opposite example for property (c) is: The subgraph features that are too rare or too frequent in the dataset are not useful at all, because unlabeled graphs cannot be separated from each other using these subgraph features. Similar assumptions have also been used by previous works on dimensionality reduction in vector spaces <ref type="bibr" target="#b17">[16]</ref>.</p><p>Based upon the above properties, we derive an evaluation criterion J(T ) as follow:</p><formula xml:id="formula_6">J(T ) = α 2|C| y i y j =-1 (DT xi -DT xj) 2 - β 2|M| y i y j =1 (DT xi -DT xj) 2 + 1 2|Du| 2 G i ,G j ∈Du (DT xi -DT xj ) 2 (2)</formula><p>where DT = diag(d(T )) is a diagonal matrix indicating which features are selected into feature set T from S, d(T )i = I(gi ∈ T ). α, β are two parameters, which control the weights of the three types of constraints. Different settings of α and β can refer to different scenarios, and reflect different beliefs we have for the problem. A discussion on the parameter setting will be presented analytically in Section 4.4 and empirically in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>By defining a matrix</head><formula xml:id="formula_7">W = [Wij] n×n as Wij = ⎧ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎩ α |C| if yiyj = -1 -β |M| if yiyj = 1 1 |Du| 2 if Gi, Gj ∈ Du 0 o t h e r w i s e<label>(3)</label></formula><p>we can rewrite the J(T ) in Eq. 2 as follow:</p><formula xml:id="formula_8">J(T ) = 1 2 i,j (DT xi -DT xj) 2 Wij = tr(DT X (D -W ) X DT ) = tr(DT XLX DT ) = g k ∈T (f k Lf k )<label>(4)</label></formula><p>where tr(•) is the trace of a matrix, D is a diagonal matrix whose entries are column sums of W , i.e. Dii = j Wij. L = D -W is a Laplacian matrix.</p><p>By denoting function h(g k , L) = f k Lf k , the optimization in Eq. 1 can be written as max</p><formula xml:id="formula_9">T g k ∈T h(g k , L) s.t. T ⊆ S, |T | ≤ t (5)</formula><p>Definition 3 (gSemi). Let D = {G1, • • • , Gn} denote a graph dataset, with first l graphs labeled as y1, • • • , y l . Suppose W is a matrix defined as Eq. 3. L is a Laplacian matrix defined as L = D -W , where D is a diagonal matrix, Dii = j Wij. We define a quality criterion q called gSemi, for a subgraph feature g as</p><formula xml:id="formula_10">q(g) = h(g, L) = fg Lfg (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>where fg = [f</p><formula xml:id="formula_12">(1) g , • • • , f (n)</formula><p>g ] ∈ {0, 1} n is the indicator vector for subgraph feature g, f</p><formula xml:id="formula_13">(i) g = 1 iff g ⊆ Gi (i = 1, 2, • • • , n).</formula><p>Since the Laplacian matrix L is positive semi-definite, for any subgraph pattern g, q(g) ≥ 0.</p><p>The optimal solution to the problem in Eq. 5 can be found by using gSemi to make feature selection on a set of subgraphs S. Suppose the gSemi values for all subgraphs are denoted as q(g1) ≥ q(g2) ≥ • • • ≥ q(gm) in sorted order. Then the optimal solution to the optimization problem in Eq. 5 is:</p><formula xml:id="formula_14">T * = {gi|i ≤ t}. (<label>7</label></formula><formula xml:id="formula_15">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">gSSC</head><p>In this section, we address the problem (P2) discussed in Section 3.1 by proposing an efficient method to find the optimal set of subgraphs features from a dataset with both labeled and unlabeled graphs.</p><p>The straightforward method is the exhaustive enumeration: We first enumerate all subgraph patterns in the graph dataset, and then calculate the gSemi values for all subgraph patterns. This method is usually impractical, because the number of subgraphs grows exponentially with the size of the graphs. Inspired by recent graph classification approaches, e.g. <ref type="bibr" target="#b15">[14]</ref>, which put their evaluation criteria into the subgraph pattern mining process and develop constraints to prune search spaces, we take a similar approach by deriving a different constraint from both labeled and unlabeled graphs. In order to avoid the exhaustive search, we proposed a branch-and-bound algorithm, named gSSC, which is summarized as follow: a) Adopt a canonical search space where all the subgraph patterns can be enumerated. b) Search through the space, and find the optimal subgraph features by gSemi. c) Propose an upper bound of gSemi and prune the search space. Details with these three steps will be described in the next subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Subgraph Mining</head><p>In this paper, we adopted a depth first search algorithm, gSpan proposed by Yan et al <ref type="bibr" target="#b16">[15]</ref>, to enumerate all subgraphs from a graph dataset. The key idea of gSpan <ref type="bibr" target="#b16">[15]</ref> is that, instead of enumerating subgraphs and testing for isomorphism, they first build a lexicographic order of all the edges of a graph, and then map each graph to an unique minimum DFS code as its canonical label. The minimum DFS codes of two graphs are equivalent iff they are isomorphic. Details can be found in <ref type="bibr" target="#b16">[15]</ref>. Based on this lexicographic order, a depth-first search (DFS) strategy is used to efficiently search through all the subgraphs in a DFS code tree. By a depth-first search through the DFS code tree's nodes, we can enumerate all the subgraphs of a graph in their DFS codes' order. And the nodes with non-minimum DFS codes can be directly pruned in the tree, which saves us from performing an explicit isomorphic test among the subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Upper Bound of gSemi</head><p>By adopting gSpan's DFS Code Tree, we can efficiently enumerate all the subgraph patterns of a graph dataset in a canonical search space. We now derive an upper bound for the gSemi value which can be used to prune the subgraph search space. A convenient method to compute a upperbound on gSemi value is given as follow:</p><p>Theorem 1 (Upper bound of gSemi). Given any two subgraphs g, g ∈ S, g is a supergraph of g (g ⊇ g). The gSemi value of g (q(g )) is bounded by q(g) (i.e., q(g ) ≤ q(g)). q(g) is defined as follow: q(g) fg Lfg <ref type="bibr">(8)</ref> where the matrix L is defined as Lij max(0, Lij ). fg = {I(g ⊆ Gi)} n i=1 ∈ {0, 1} n is a vector indicating which graphs in a graph dataset {G1, • • • , Gn} contain the subgraph g, I(•) is the indicator function. Suppose the gSemi value of g is q(g) = fg Lfg.</p><p>Proof.</p><formula xml:id="formula_16">q g = f g Lf g = i,j:G i ,G j ∈G(g ) Lij where G(g ) {Gi|g ⊆ Gi, 1 ≤ i ≤ n}.</formula><p>Since g is the supergraph of g (g ⊇ g), according to anti-monotonic property, we have G(g ) ⊆ G(g). Also Lij max(0, Lij ), we have Lij ≥ Lij and Lij ≥ 0. So,</p><formula xml:id="formula_17">q g = i,j:G i ,G j ∈G(g ) Lij ≤ i,j:G i ,G j ∈G(g ) Lij ≤ i,j:G i ,G j ∈G(g) Lij = q (g)</formula><p>Thus, for any g ⊇ g, q(g ) ≤ q(g). else if q(g) &gt; min g ∈T q(g ), then 7 gmin = argmin g ∈T q(g ) and T = T /gmin; 8 T = T ∪ {g} and θ = q(gmin); 9 if q(g) ≥ θ and freq(g) ≥ min sup, then 10</p><p>Depth-first search the subtree rooted from node g; 11 return T ; Output:</p><p>T : Set of optimal subgraph features </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pruning Search Space</head><p>We can now utilize the upper bound to efficiently prune the DFS Code Tree with a branch-and-bound method. During the depth-first search through the DFS Code Tree, we always maintain the temporally suboptimal gSemi value (denoted by θ) among all the gSemi values calculated before. If q(g) &lt; θ, the gSemi value of any supergraph g of g (g ⊇ g) is no greater than θ. Thus, we can safely prune the subtree from g in the search space. If q(g) ≥ θ, we cannot prune this space since there might exist a supergraph g ⊇ g that q(g ) ≥ θ.</p><p>The algorithm gSSC is summarized in Figure <ref type="figure" target="#fig_3">4</ref>. We initialize a set of selected subgraphs T as an empty set. In order to speed up the mining process, we can prune the search space from gSpan by always maintaining the currently top-t best subgraphs according to q. During the course of mining, whenever we reach a subgraph g with q(g) ≤ ming i ∈T q(gi), we can prune the branches originating from g. This is because for any supergraph g ⊇ g we have q(g ) ≤ q(g), according to the bound defined in Eq. 8. As long as the resulting subgraph g can improve the gSemi value of any subgraphs gi ∈ T , it is accepted into T and the least best subgraph is dropped off from T . And then we start searching for the next subgraph in the DFS Code Tree.</p><p>We further note that in our experiments among almost all datasets gSemi provides such a bound that we can even omit the support threshold min sup and still find a set of optimal subgraphs within a reasonable time cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion</head><p>In this section we show the connection between our framework and various application scenarios of graph classification.</p><p>Parameter Setting: There are two parameters in the objective function: α and β, which represent the weights of different constraints based on both labeled and unlabeled graphs. Different settings of these parameters fit the optimization to different scenarios of graph classification:</p><p>• α = 0, β = 0. In this case, we only consider the cannot-link constraints and unlabeled graph's separability in subgraph feature selection. No must-link constraint is considered, i.e. labeled graphs within the same classes are not necessarily close together. α controls how much we assume labeled graphs within different classes should be far from each other. This setting of parameters is useful when there is a large diversity within graphs from the same class. For example, drug molecules that have the same toxicology activities on one animal can have very different structures. Furthermore, if α = +∞, we only trust the cannot-link constraints. This reduce the problem into a supervised feature selection task.</p><p>• α = 0, β = 0. In this setting of parameter, we only consider the must-link constraints and unlabeled graph's separability in subgraph feature selection. The larger β is, the more we trust the must-link constraints in feature selection. No cannot-link constraint is considered, i.e. labeled graphs in different classes are not necessarily far from each other.</p><p>• α = 0, β = 0. In this case, we don't trust label constraints. Only unlabeled graph's separability is considered in subgraph feature selection. This reduce the problem into an unsupervised feature selection task for the unlabeled graph data.</p><p>• α = 0, β = 0. In this case, we consider all constraints (must-link, cannot-link, unlabeled separability) with different weights. This setting is a typical setting for semi-supervised feature selection, where we need to consider both labeled and unlabeled graphs.</p><p>The smaller the values of α and β, the more we trust the separability constraints from unlabeled graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>In this section, we conduct extensive experiments to examine the effectiveness and efficiency of gSSC in semi-supervised feature selection for graph classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Data Collections: In order to evaluate the performances of our semi-supervised feature selection approach for graph classification, we tested our algorithm on five real-world graph classification datasets including the following tasks: (Summarized in Table <ref type="table" target="#tab_0">1</ref>) The task is to classify chemical compounds' anti-cancer activities on three types of cancers, i.e. breast, lung and ovarian. The datasets consist information on the biological activities of small molecules, containing anticancer activity records of more then 10,000 chemical compounds against the three types of cancers. Each chemical compound is represented as a graph. We collected 3 graph datasets with active and inactive labels from PubChem Website. The original datasets are unbalanced, where the active class is around 5%. We randomly sample 500 inactive compounds and 500 active compounds from each dataset for performance evaluation.</p><p>2) Toxicology prediction (PTC): The last two benchmark datasets are collected from PTC datasets<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b3">[3]</ref>. The task is to classify chemical compounds' carcinogenicity on two animal models, i.e. MM (Male Mouse) and FM (Female Mouse). The datasets consist carcinogenicity records of more than 300 chemical compounds. Each chemical compound is assigned with carcinogenicity labels for these animal models. On each animal model the carcinogenicity label is one of {CE, SE, P, E, EE, IS, NE, N}. We assume {CE, SE, P} as 'positive' labels, and {NE, N} as 'negative', which is the same setting as <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b8">7]</ref>. Each chemical compound is represented as a graph with an average of 25.7 vertices.</p><p>Comparing Methods: In order to demonstrate the effectiveness of our semi-supervised features selection approach for graph classification, we compare our methods with two baseline methods, including a supervised feature selection approach and an unsupervised approach.</p><p>The compared methods are summarized as follows:</p><p>• Semi-Supervised (gSSC): The proposed semi-supervised feature selection method for graph classification. We first use gSSC to find a set of subgraph features. The parameters in gSSC are set to α = β = 1 unless otherwise specified.</p><p>• Supervised (IG): We compare with a supervised feature selection method for graph classification. In this approach, a set of frequent subgraphs within labeled graphs are first mined. Then a supervised feature selection based upon Information Gain (IG), an entropy based measure, is used to select a subset of discriminative features from frequent subgraphs.</p><p>• Unsupervised (Top-k): We also compare with an unsupervised feature selection method. In this approach, the evaluation criterion for subgraph feature selection is based upon frequency. The top-k frequent subgraph features in labeled graphs are selected.</p><p>All experiments are conducted on machines with 4 GB RAM and Intel Xeon TM Quad-Core CPUs of 2.40 GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performances on Graph Classification</head><p>In our experiments, the labeled training graphs are randomly sampled from each datasets. All the remaining graphs In all these datasets, our semi-supervised feature selection algorithm (gSSC) outperform the supervised approach (IG). gSSC can achieve a good performances with a few labeled training graphs together with a large amount of unlabeled graphs. Although the performance of IG improves with a larger number of features, the IG cannot reach the best performance achievable by gSSC. These results support our first intuition that semi-supervised feature selection methods based on gSemi can boost the performance of graph classification with large amount of unlabeled graphs.</p><p>We further observe that gSSC's performances are better than our second baseline Top-k, i.e. unsupervised feature selection approaches without label information. These results support our second intuition that the gSemi evaluation criterion in gSSC can find better subgraph patterns for graph classification than unsupervised top-k frequent subgraph selection approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pruning Search Space</head><p>In our second experiment, we evaluated the effectiveness of the upper-bound for gSemi proposed in Section 4.2. In this section we compare the runtime performance of two versions of implementation for gSSC: 'nested gSSC' versus 'un-nested gSSC'. The 'nested gSSC' denotes the proposed method using the upper-bound proposed in Section 4.2 to prune the search space of subgraph enumerations; the 'un-nested gSSC' denotes the method without the gSemi's upper-bound pruning, which first uses gSpan to find a set of frequent subgraphs, and then selects the optimal set of subgraphs via gSemi. We run both approaches and record the average CPU time used on feature mining and selection. The result is shown in Figure <ref type="figure">8</ref>.</p><p>In all these datasets, the un-nested gSSC needs to explore increasingly larger subgraph search spaces as we decrease the min sup in the frequent subgraph mining. The size increases exponentially when decreasing min sup. In the MCF-7 dataset, when the min sup get too low ( min sup &lt; 8%), the subgraph feature enumeration step in un-nested gSSC can run out of the computer memory. However, the nested gSSC's running time does not increase as much, because the gSemi can help pruning the subgraph search space using both labeled and unlabeled graphs. As we can see, the min sup can go to very low value in all datasets for the "nested gSSC".</p><p>Figure <ref type="figure">9</ref> shows the number of subgraph feature explored in the process of subgraph pattern enumeration. In all datasets, we observe that the number of searched subgraph patterns in nested gSSC is much smaller than that of un-nested gSSC. In our experiments, we further noticed that on most datasets, nested gSSC provides such a strong bound that we may even allow nested gSSC to omit the minimum support threshold min sup and still receive an optimal set of subgraph features within a reasonable time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Parameter Settings</head><p>In our model we can take different weights on constraints from labeled graphs and unlabeled graphs. If we use different setting for the two parameters α and β, we can take the feature selection with different weights for the three types of constraints: must-link, cannot-link and unlabeled sep-arability. α represents how much we weight the cannotlink constraints, and β denotes how much we weight the must-link constraints. The larger α is, the further away the graphs with different classes are separated from each other. The larger β is, the closer the graphs with the same classes are from each other. We test α and β with values among {0.001, 0.01, • • • 10000} separately. The result in Figure <ref type="figure" target="#fig_0">10</ref> shows that the performance of our model using α with large values and β with small values is often better than other settings. The reason is that in these real-world graph classification tasks, graphs in the same class are not always similar with each other, actually graphs can be very different within a same class.</p><p>In Figure <ref type="figure" target="#fig_0">10</ref>, we find the best parameter setting for MCF-3 dataset is α = 1, β = 0.1 (accuracy = 0.526), and with our default parameter setting (α = β = 1) the accuracy is 0.523. For NCI-H23 dataset, the best parameter setting is α = 1, β = 0.1 (accuracy= 0.556), and the accuracy with default setting is 0.553. For OVCAR-8 dataset, the best parameter setting is α = 1, β = 0.1 (accuracy= 0.539), and the accuracy with default setting is 0.530. Generally, we can see that the performance of gSSC with default setting ( α = β = 1) is pretty good. If we try to optimize the selection of α and β value, the accuracy improvement relative the two base line schemes will be even bigger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we study the problem of semi-supervised feature selection for graph classification. It is significantly more challenging than the conventional setting of supervised feature selection in graph data because of the lack of labeled training graphs. To address this challenge, we propose a feature evaluation criterion, named gSemi, to evaluate subgraph features with both labeled and unlabeled graphs, and derive an upper-bound for gSemi to prune the subgraph search space. Then we propose a branch-and-bound algorithm to efficiently find a set of optimal subgraph feature which is useful for graph classification. Empirical studies on real-world tasks show that our semi-supervised feature selection approach for graph classification outperforms supervised and unsupervised approaches and is very efficient by pruning the subgraph search space using both labeled and unlabeled graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An example of semi-supervised feature selection on graph data. The subgraph feature "ab" is more useful than "a-c" based on both labeled and unlabeled graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: Supervised Feature Selection Process for Graph Classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>T</head><label></label><figDesc>= gSSC(D, y l , min sup, t) Input: D : Graph data set {G1, • • • , Gn} y l : The first l graphs' labels, where y l = [y1, • • • , y l ] min sup : Minimum support threshold t : number of subgraph feature selected Process: 1 T = ∅, θ = 0; 2 Recursively visit the DFS Code Tree in gSpan: 3 g = currently visited subgraph in DFS Code Tree 4 if |T | &lt; t, then 5 T = T ∪ {g}; 6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The gSSC algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :Figure 7 :</head><label>567</label><figDesc>Figure 5: Classification accuracy with different number of features. (#label=30)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 Figure 8 :Figure 9 :Figure 10 :</head><label>88910</label><figDesc>Figure 8: Average CPU time for nested gSSC versus un-nested gSSC with varying min sup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Summary of experimental datasets. "Pos%" denotes the average percentage of postive graphs in each dataset.</head><label>1</label><figDesc>Anti-cancer activity prediction: The first three benchmark datasets are collect from PubChem Website 1 .</figDesc><table><row><cell>Name</cell><cell cols="3">#Graph Pos% Details</cell></row><row><cell>MCF-7</cell><cell>27784</cell><cell>8.19</cell><cell>Breast Cancer</cell></row><row><cell>NCI-H23</cell><cell>40460</cell><cell>5.06</cell><cell>Lung Cancer</cell></row><row><cell>OVCAR-8</cell><cell>40626</cell><cell>5.08</cell><cell>Ovarian Cancer</cell></row><row><cell>PTC-MM</cell><cell>336</cell><cell>41.0</cell><cell>Male Mice Toxicology</cell></row><row><cell>PTC-FM</cell><cell>349</cell><cell>38.4</cell><cell>Female Mice Toxicology</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://pubchem.ncbi.nlm.nih.gov</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.predictive-toxicology.org/ptc/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGMENTS</head><p>This work is supported in part by NSF through grant IIS-0905215.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a mahalanobis metric from equivalence constraints</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bar-Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shental</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="937" to="965" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining molecular fragments: Finding relevant substructures of molecules</title>
		<author>
			<persName><forename type="first">C</forename><surname>Borgelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berthold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Data Mining</title>
		<meeting>the 2nd International Conference on Data Mining<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Maebashi City</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="211" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The predictive toxicology challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Helma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000-2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="108" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient mining of frequent subgraph in the presence of isomorphism</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Data Mining</title>
		<meeting>the 3rd International Conference on Data Mining<address><addrLine>Melbourne, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="549" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An apriori-based algorithm for mining frequent substructures from graph data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th European Conference on Principles of Data Mining and Knowledge Discovery</title>
		<meeting>the 4th European Conference on Principles of Data Mining and Knowledge Discovery<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning</title>
		<meeting>the 20th International Conference on Machine Learning<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An application of boosting to graph classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Frequent subgraph discovery</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kuramochi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Conference on Data Mining</title>
		<meeting>the 1st International Conference on Data Mining<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Mardia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Kent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bibby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Multivariate Analysis. Academic Press</publisher>
			<pubPlace>San Diego, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A quickstart in frequent structure mining can make a difference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nijssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="647" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Forword semi-supervised feature selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 12th Pacific-Asia Conference on Knowledge Discovery and Data Mining<address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="970" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise constraints-guided dimensionality reduction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining Workshop on Feature Selection for Data Mining</title>
		<meeting><address><addrLine>Bethesda, MD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Near-optimal supervised feature selection among frequent subgraphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining<address><addrLine>Sparks, Nevada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1075" to="1086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining significant graph patterns by leap search</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the ACM SIGMOD International Conference on Management of Data<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="433" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">gSpan: Graph-based substructure pattern mining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Data Mining</title>
		<meeting>the 2nd International Conference on Data Mining<address><addrLine>Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Maebashi City</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="721" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised feature selection via spectral analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIAM International Conference on Data Mining</title>
		<meeting>the SIAM International Conference on Data Mining<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="641" to="646" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
