<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Balanced Policy Evaluation and Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
							<email>kallus@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Cornell University and Cornell Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Balanced Policy Evaluation and Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B2BAC05C06865162D7C3A051338B9762</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balancebased weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Using observational data with partially observed outcomes to develop new and effective personalized decision policies has received increased attention recently <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b44">45]</ref>. The aim is to transform electronic health records to personalized treatment regimes <ref type="bibr" target="#b5">[6]</ref>, transactional records to personalized pricing strategies <ref type="bibr" target="#b4">[5]</ref>, and click-and "like"-streams to personalized advertising campaigns <ref type="bibr" target="#b7">[8]</ref> -problems of great practical significance. Many of the existing methods rely on a reduction to weighted classification via a rejection and importance sampling technique related to inverse propensity weighting and to doubly robust estimation. However, inherent in this reduction are several shortcomings that lead to reduced personalization efficacy: it involves a naïve plug-in estimation of a denominator nuisance parameter leading either to high variance or scarcely-motivated stopgaps; it necessarily rejects a significant amount of observations leading to smaller datasets in effect; and it proceeds in a two-stage approach that is unnatural to the single learning task.</p><p>In this paper, we attempt to ameliorate these by using a new approach that directly optimizes for the balance that is achieved only on average or asymptotically by the rejection and importance sampling approach. We demonstrate that this new approach provides improved performance and explain why. And, we provide extensive theory to characterize the behavior of the new methods. The proofs are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Setting, Notation, and Problem Description</head><p>The problem we consider is how to choose the best of m treatments based on an observation of covariates x 2 X ✓ R d (also known as a context). An instance is characterized by the random variables X 2 X and Y (1), . . . , Y (m) 2 R, where X denotes the covariates and Y (t) for t 2 [m] = {1, . . . , m} is the outcome that would be derived from applying treatment t. We always assume that smaller outcomes are preferable, i.e., Y (t) corresponds to costs or negative rewards.</p><p>A policy is a map ⇡ : X ! m from observations of covariates to a probability vector in the m-simplex m = {p 2 R m + :</p><p>P m t=1 p t = 1}. Given an observation of covariates x, the policy ⇡ specifies that treatment t should be applied with probability ⇡ t (x). There are two key tasks of interest: policy evaluation and policy learning. In policy evaluation, we wish to evaluate the performance of a given policy based on historical data. This is also known as off -policy evaluation, highlighting the fact that the historical data was not necessarily generated by the policy in question. In policy learning, we wish to determine a policy that has good performance.</p><p>We consider doing both tasks based on data consisting of n passive, historical observations of covariate, treatment, and outcome: S n = {(X 1 , T 1 , Y 1 ), . . . , (X n , T n , Y n )}, where the observed outcome Y i = Y i (T i ) corresponds only to the treatment T i historically applied. We use the notation X 1:n to denote the data tuple (X 1 , . . . , X n ). The data is assumed to be iid. That is, the data is generated by drawing from a stationary population of instances (X, T, Y (1), . . . , Y (m)) and observing a censored form of this draw given by (X, T, Y (T )). <ref type="foot" target="#foot_0">1</ref> From the (unknown) joint distribution of (X, T ) in the population, we define the (unknown) propensity function</p><formula xml:id="formula_0">' t (x) = P(T = t | X = x) = E[ T t | X = x],</formula><p>where st = I [s = t] is the Kronecker delta. And, from the (unknown) joint distribution of (X, Y (t)) in the population, we define the (unknown) mean-outcome function</p><formula xml:id="formula_1">µ t (x) = E[Y (t) | X = x]. We use the notation '(x) = (' 1 (x), . . . , ' m (x)) and µ(x) = (µ 1 (x), . . . , µ m (x)).</formula><p>Apart from being iid, we also assume the data satisfies unconfoundedness:</p><formula xml:id="formula_2">Assumption 1. For each t 2 [m]: Y (t) is independent of T given X, i.e., Y (t) ? ? T | X.</formula><p>This assumption is equivalent to there being a logging policy ' that generated the data by prescribing treatment t with probability ' t (X i ) to each instance i and recording the outcome</p><formula xml:id="formula_3">Y i = Y i (T i ).</formula><p>Therefore, especially in the case where the logging policy ' t is in fact known to the user, the problem is often called learning from logged bandit feedback <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>In policy evaluation, given a policy ⇡, we wish to estimate its sample-average policy effect (SAPE),</p><formula xml:id="formula_4">SAPE(⇡) = 1 n P n i=1 P m t=1 ⇡ t (X i )µ t (X i )</formula><p>, by an estimator ⌧ (⇡) = ⌧ (⇡; X 1:n , T 1:n , Y 1:n ) that depends only on the observed data and the policy ⇡. The SAPE quantifies the average outcome that a policy ⇡ would induce in the sample and hence measures its risk. SAPE is strongly consistent for the population-average policy effect (PAPE):</p><formula xml:id="formula_5">PAPE(⇡) = E[SAPE(⇡)] = E[ P m t=1 ⇡ t (X)µ t (X)] = E[Y ( T⇡(X) )], where T⇡(x) is defined as ⇡'s random draw of treatment when X = x, T⇡(x) ⇠ Multinomial(⇡(x)). Moreover, if ⇡ ⇤ is such that ⇡ ⇤ t (x) &gt; 0 () t 2 argmin s2[m] µ s (x), then b R(⇡) = SAPE(⇡) SAPE(⇡ ⇤ ) is the regret of ⇡ [10].</formula><p>The policy evaluation task is closely related to causal effect estimation <ref type="bibr" target="#b18">[19]</ref> where, for m = 2, one is interested in estimating the sample and population average treatment effects:</p><formula xml:id="formula_6">SATE = 1 n P n i=1 (µ 2 (X i ) µ 1 (X i )), PATE = E[SATE] = E[Y (2) Y (1)].</formula><p>In policy learning, we wish to find a policy ⇡ that achieves small outcomes, i.e., small SAPE and PAPE. The optimal policy ⇡ ⇤ minimizes both SAPE(⇡) and PAPE(⇡) over all functions X ! m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Existing Approaches and Related Work</head><p>The so-called "direct" approach fits regression estimates μt of µ t on each dataset {(X i , Y i ) :</p><formula xml:id="formula_7">T i = t}, t 2 [m].</formula><p>Given these estimates, it estimates SAPE in a plug-in fashion:</p><formula xml:id="formula_8">⌧ direct (⇡) = 1 n P n i=1 P m t=1 ⇡ t (X i )μ t (X i ).</formula><p>A policy is learned either by ⇡direct (x) = argmin t2[m] μt (x) or by minimizing ⌧ direct (⇡) over ⇡ 2 ⇧ <ref type="bibr" target="#b32">[33]</ref>. However, direct approaches may not generalize as well as weighting-based approaches <ref type="bibr" target="#b6">[7]</ref>.</p><p>Weighting-based approaches seek weights based on covariate and treatment data W (⇡) = W (⇡; X 1:n , T 1:n ) that make the outcome data, when reweighted, look as though it were generated by the policy being evaluated or learned, giving rise to estimators that have the form</p><formula xml:id="formula_9">⌧W = 1 n P n i=1 W i Y i .</formula><p>Bottou et al. <ref type="bibr" target="#b7">[8]</ref>, e.g., propose to use inverse propensity weighting (IPW). Noting that <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> </p><formula xml:id="formula_10">SAPE(⇡) = E[ 1 n P n i=1 Y i ⇥ ⇡ Ti (X i )/' Ti (X i ) | X 1:n ],</formula><p>one first fits a probabilistic classification model ' to {(X i , T i ) : i 2 [n]} and then estimates SAPE in an alternate but also plug-in fashion:</p><formula xml:id="formula_11">⌧ IPW (⇡) = ⌧W IPW (⇡) , W IPW i (⇡) = ⇡ Ti (X i )/ 'Ti (X i )</formula><p>For a deterministic policy, ⇡ t (x) 2 {0, 1}, this can be interpreted as a rejection and importance sampling approach <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b40">41]</ref>: reject samples where the observed treatment does not match ⇡'s recommendation and up-weight those that do by the inverse (estimated) propensity. For deterministic policies ⇡ t (x) 2 {0, 1}, we have that ⇡ T (X) = T, T⇡(X) is the complement of 0-1 loss of ⇡(X) in predicting T . By scaling and constant shifts, one can therefore reduce minimizing ⌧ IPW (⇡) over policies ⇡ 2 ⇧ to minimizing a weighted classification loss over classifiers ⇡ 2 ⇧, providing a reduction to weighted classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Given both μ(x) and '(x) estimates, Dudík et al. <ref type="bibr" target="#b12">[13]</ref> propose a weighting-based approach that combines the direct and IPW approaches by adapting the doubly robust (DR) estimator <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>:</p><formula xml:id="formula_12">⌧ DR (⇡) = 1 n P n i=1 P m t=1 ⇡ t (X i )μ t (X i ) + 1 n P n i=1 (Y i μTi (X i ))⇡ Ti (X i )/ 'Ti (X i</formula><p>). ⌧ DR (⇡) can be understood either as debiasing the direct estimator by via the reweighted residuals ✏i = Y i μTi (X i ) or as denoising the IPW estimator by subtracting the conditional mean from Y i . As its bias is multiplicative in the biases of the regression and propensity estimates, the estimator is consistent so long as one of the estimates is consistent. For policy learning, <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref> minimize this estimator via weighted classification. Athey and Wager <ref type="bibr" target="#b0">[1]</ref> provide a tight and favorable analysis of the corresponding uniform consistency (and hence regret) of the DR approach to policy learning.</p><p>Based on the fact that 1 = E[⇡ T (X)/' T (X)], a normalized IPW (NIPW) estimator is given by normalizing the weights so they sum to n, a common practice in causal effect estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref>:</p><formula xml:id="formula_13">⌧ NIPW (⇡) = ⌧W NIPW (⇡) , W NIPW i (⇡) = W IPW i (⇡)/ P n i 0 =1 W IPW i 0 (⇡).</formula><p>Any IPW approaches are subject to considerable variance because the plugged-in propensities are in the denominator so that small errors can have outsize effects on the total estimate. Another stopgap measure is to clip the propensities <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20]</ref> resulting in the clipped IPW (CIPW) estimator:</p><formula xml:id="formula_14">⌧ M -CIPW (⇡) = ⌧W M -CIPW (⇡) , W M -CIPW i (⇡) = ⇡ Ti (X i )/ max{M, 'Ti (X i )}.</formula><p>While effective in reducing variance, the practice remains ad-hoc, loses the unbiasedness of IPW (with true propensities), and requires the tuning of M . For policy learning, Swaminathan and Joachims <ref type="bibr" target="#b41">[42]</ref> propose to minimizes over ⇡ 2 ⇧ the M -CIPW estimator plus a regularization term of the sample variance of the estimator, which they term POEM. The sample variance scales with the level of overlap between ⇡ and T 1:n , i.e., the prevalence of ⇡ Ti (X i ) &gt; 0. Indeed, when the policy class ⇧ is very flexible relative to n and if outcomes are nonnegative, then the anti-logging policy ⇡ Ti (X i ) = 0 minimizes any of the above estimates. POEM avoids learning the anti-logging policy by regularizing overlap, reducing variance but limiting novelty of ⇡. A refinement, SNPOEM <ref type="bibr" target="#b42">[43]</ref> uses a normalized and clipped IPW (NCIPW) estimator (and regularizes variance):</p><formula xml:id="formula_15">⌧ M -NCIPW (⇡) = ⌧W M -NCIPW (⇡) , W M -NCIPW i (⇡) = W M -CIPW i (⇡)/ P n i 0 =1 W M -CIPW i 0 (⇡).</formula><p>Kallus and Zhou <ref type="bibr" target="#b25">[26]</ref> generalize the IPW approach to a continuum of treatments. Kallus and Zhou <ref type="bibr" target="#b24">[25]</ref> suggest a minimax approach to perturbations of the weights to account for confounding factors. Kallus <ref type="bibr" target="#b22">[23]</ref> proposes a recursive partitioning approach to policy learning, the Personalization Tree (PT) and Personalization Forest (PF), that dynamically learns both weights and policy, but still uses within-partition IPW with dynamically estimated propensities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">A Balance-Based Approach</head><p>Shortcomings in existing approaches. All of the above weighting-based approaches seek to reweight the historical data so that they look as though they were generated by the policy being evaluated or learned. Similarly, the DR approach seeks to make the historical residuals look like those that would be generated under the policy in question so to remove bias from the estimated regression model of the direct approach. However, the way these methods achieve this through various forms and versions of inverse propensity weighting, has three critical shortcomings:</p><p>(1) By taking a simple plug-in approach for a nuisance parameter (propensities) that appears in the denominator, existing weighting-based methods are either subject to very high variance or must rely on scarcely-motivated stopgap measures such as clipping (see also <ref type="bibr" target="#b26">[27]</ref>). (2) In the case of deterministic policies (such as an optimal policy), existing methods all have weights that are multiples of ⇡ Ti (X i ), which means that one necessarily throws away every data point T i that does not agree with the new policy recommendation T⇡(Xi) . This means that one is essentially only using a much smaller dataset than is available, leading again to higher variance. <ref type="foot" target="#foot_1">2</ref>(3) The existing weighting-based methods all proceed in two stages: first estimate propensities and then plug these in to a derived estimator (when the logging policy is unknown). On the one hand, this raises model specification concerns, and on the other, is unsatisfactory when the task at hand is not inherently two-staged -we wish only to evaluate or learn policies, not to learn propensities.</p><p>A new approach. We propose a balance-based approach that, like the existing weighting-based methods, also reweights the historical data to make it look as though they were generated by the policy being evaluated or learned and potentially denoises outcomes in a doubly robust fashion, but rather than doing so circuitously via a plug-in approach, we do it directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample.</p><p>In particular, we formalize balance as a discrepancy between the reweighted historical covariate distribution and that induced by the target policy and prove that it is directly related to the worst-case conditional mean square error (CMSE) of any weighting-based estimator. Given a policy ⇡, we then propose to choose (policy-dependent) weights W ⇤ (⇡) that optimize the worst-case CMSE and therefore achieve excellent balance while controlling for variance. For evaluation, we use these optimal weights to evaluate the performance of ⇡ by the estimator ⌧W ⇤ (⇡) as well as a doubly robust version. For learning, we propose a bilevel optimization problem: minimize over ⇡ 2 ⇧, the estimated risk ⌧W ⇤ (⇡) (or a doubly robust version thereof and potentially plus a regularization term), given by the weights W ⇤ (⇡) that minimize the estimation error. Our empirical results show the stark benefit of this approach while our main theoretical results (Thm. 6, Cor. 7) establish vanishing regret bounds.</p><p>2 Balanced Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CMSE and Worst-Case CMSE</head><p>We begin by presenting the approach in the context of evaluation. Given a policy ⇡, consider any weights W = W (⇡; X 1:n , T 1:n ) that are based on the covariate and treatment data. Given these weights we can consider both a simple weighted estimator as well as a W -weighted doubly robust estimator given a regression estimate μ:</p><formula xml:id="formula_16">⌧W = 1 n P n i=1 W i Y i , ⌧W,μ = 1 n P n i=1 P m t=1 ⇡ t (X i )μ t (X i ) + 1 n P n i=1 W i (Y i μTi (X i )).</formula><p>We can measure the risk of either such estimator as the conditional mean square error (CMSE), conditioned on all of the data upon which the chosen weights depend:</p><formula xml:id="formula_17">CMSE(⌧ , ⇡) = E[(⌧ SAPE(⇡)) 2 | X 1:n , T 1:n ].</formula><p>Minimal CMSE is the target of choosing weights for weighting-based policy evaluation. Basic manipulations under the unconfoundedness assumption decompose the CMSE of any weightingbased policy evaluation estimator into its conditional bias and variance:  Corollary 2. Let μ be given such that μ ? ? Y 1:n | X 1:n , T 1:n (e.g., trained on a split sample).</p><formula xml:id="formula_18">Theorem 1. Let ✏ i = Y i µ Ti (X i ) and ⌃ = diag(E[✏ 2 1 | X 1 , T 1 ], . . . , E[✏ 2 n | X n , T n ]). Define B t (W, ⇡ t ; f t ) = 1 n P n i=1 (W i Tit ⇡ t (X i ))f t (X i ) and B(W, ⇡; f ) = P m t=1 B t (W, ⇡ t ; f t ) Then we have that: ⌧W SAPE(⇡) = B(W, ⇡; µ) + 1 n P n i=1 W i ✏ i . Moreover, under Asn. 1: CMSE(⌧ W , ⇡) = B 2 (W, ⇡; µ) + 1 n 2 W T ⌃W.</formula><p>Then we have that:</p><formula xml:id="formula_19">⌧W,μ SAPE(⇡) = B(W, ⇡; µ μ) + 1 n P n i=1 W i ✏ i . Moreover, under Asn. 1: CMSE(⌧ W,μ , ⇡) = B 2 (W, ⇡; µ μ) + 1 n 2 W T ⌃W.</formula><p>In Thm. 1 and Cor. 2, B(W, ⇡; µ) and B(W, ⇡; µ μ) are precisely the conditional bias in evaluating ⇡ for ⌧W and ⌧W,μ , respectively, and 1 n 2 W T ⌃W the conditional variance for both. In particular, B t (W, ⇡ t ; µ t ) or B t (W, ⇡ t ; µ t μt ) is the conditional bias in evaluating the effect on the instances where ⇡ assigns t. Note that for any function f t , B t (W, ⇡ t ; f t ) corresponds to the discrepancy between the f t (X)-moments of the measure ⌫ t,⇡ (A) =</p><formula xml:id="formula_20">1 n P n i=1 ⇡ t (X i )I [X i 2 A] on X and the measure ⌫ t,W (A) = 1 n P n i=1 W i Tit I [X i 2 A].</formula><p>The sum B(W, ⇡; f ) corresponds to the sum of moment discrepancies over the components of f = (f 1 , . . . , f m ) between these measures. The moment discrepancy of interest is that of f = µ or f = µ μ, but neither of these are known.</p><p>Balanced policy evaluation seeks weights W to minimize a combination of imbalance, given by the worst-case value of B(W, ⇡; f ) over functions f , and variance, given by the norm of weights W T ⇤W for a specified positive semidefinite (PSD) matrix ⇤. This follows a general approach introduced by <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref> of finding optimal balancing weights that optimize a given CMSE objective directly rather than via a plug-in approach. Any choice of k • k gives rise to a worst-case CMSE objective for policy evaluation:</p><formula xml:id="formula_21">E 2 (W, ⇡; k • k, ⇤) = sup kf k1 B 2 (W, ⇡; f ) + 1 n 2 W T ⇤W.</formula><p>Here, we focus on k • k given by the direct product of reproducing kernel Hilbert spaces (RKHS): kf k p,K1:m, 1:m = ( P m t=1 kf t k p Kt / p t ) 1/p , where k • k Kt is the norm of the RKHS given by the PSD kernel K t (•, •) : X 2 ! R, i.e., the unique completion of span(K t (x, •) : x 2 X ) endowed with hK t (x, •), K t (x 0 , •)i = K t (x, x 0 ) [see 39]. We say kf k Kt = 1 if f is not in the RKHS. One example of a kernel is the Mahalanobis RBF kernel: K s (x, x 0 ) = exp( (x x 0 ) T Ŝ 1 (x x 0 )/s 2 ) where Ŝ is the sample covariance of X 1:n and s is a parameter. For such an RKHS product norm, we can decompose the worst-case objective into the discrepancies in each treatment as well as characterize it as a posterior (rather than worst-case) risk.</p><formula xml:id="formula_22">Lemma 1. Let B 2 t (W, ⇡ t ; k • k Kt ) = P n i,j=1 (W i Tit ⇡ t (X i ))(W j Tj t ⇡ t (X j ))K t (X i , X j ) and 1/p + 1/q = 1. Then E 2 (W, ⇡; k • k p,K1:m, 1:m , ⇤) = ( P m t=1 q t B q t (W, ⇡ t ; k • k Kt )) 2/q + 1 n 2 W T ⇤W.</formula><p>Moreover, if p = 2 and µ t has a Gaussian process prior <ref type="bibr" target="#b43">[44]</ref> with mean f t and covariance t K t then</p><formula xml:id="formula_23">CMSE(⌧ W,f , ⇡) = E 2 (W, ⇡; k • k p,K1:m, 1:m , ⌃),</formula><p>where the CMSE marginalizes over µ. This gives the CMSE of ⌧W for f constant or ⌧W,μ for f = μ.</p><p>The second statement in Lemma 1 suggests that, in practice, model selection of 1:m , ⇤, and kernel hyperparameters such as s or even Ŝ, can done by the marginal likelihood method [see 44, Ch. 5].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation Using Optimal Balancing Weights</head><p>Our policy evaluation estimates are given by either the estimator</p><formula xml:id="formula_24">⌧W ⇤ (⇡;k•k,⇤) or ⌧W ⇤ (⇡;k•k,⇤),μ where W ⇤ (⇡) = W ⇤ (⇡; k • k, ⇤) is the minimizer of E 2 (W, ⇡; k • k, ⇤)</formula><p>over the space of all weights W that sum to n, W = {W 2 R n + :</p><formula xml:id="formula_25">P n i=1 W i = n} = n n . Specifically, W ⇤ (⇡; k • k, ⇤) 2 argmin W 2W E 2 (W, ⇡; k • k, ⇤). When k • k = k • k p,K1:m, 1:m ,</formula><p>this problem is a quadratic program for p = 2 and a second-order cone program for p = 1, 1. Both are efficiently solvable <ref type="bibr" target="#b8">[9]</ref>. In practice, we solve these using Gurobi 7.0.</p><p>In Lemma 1, B t (W, ⇡ t ; k • k Kt ) measures the imbalance between ⌫ t,⇡ and ⌫ t,W as the worst-case discrepancy in means over functions in the unit ball of an RKHS. In fact, as a distributional distance metric, it is the maximum mean discrepancy (MMD) used, for example, for testing whether two samples come from the same distribution <ref type="bibr" target="#b15">[16]</ref>. Thus, minimizing E 2 (W, ⇡; k • k p,K1:m, 1:m , ⇤) is simply seeking the weights W that balance ⌫ t,⇡ and ⌫ t,W subject to variance regularization in W .</p><p>Example 1. We demonstrate balanced evaluation with a mixture of m = 5 Gaussians: X | T ⇠ N(X T , I 2⇥2 ), X 1 = (0, 0), X t = (Re, Im)(e i2⇡(t 2)/(m 1) ) for t = 2, . . . , m, and T ⇠ Multinomial(1/5, . . . , 1/5). Fix a draw of X 1:n , T 1:n with n = 100 shown in Fig. <ref type="figure" target="#fig_0">1a</ref> (numpy seed 0). Color denotes T i and size denotes ' Ti (X i ). The centers X t are marked by a colored number. Next, we let µ t (x) = exp(1 1/kx t k 2 ) where t = (Re, Im)(e i2⇡t/m / p 2) for t 2 [m], ✏ i ⇠ N (0, ), and = 1. Fig. <ref type="figure" target="#fig_0">1b</ref> plots µ 1 (x). Fig. <ref type="figure" target="#fig_0">1c</ref> shows the corresponding optimal policy ⇡ ⇤ .</p><p>Next we consider evaluating ⇡ ⇤ . Fixing X 1:n as in Fig. <ref type="figure" target="#fig_0">1a</ref>, we have SAPE(⇡ ⇤ ) = 0.852. With X 1:n fixed, we draw 1000 replications of T 1:n , Y 1:n from their conditional distribution. For each replication, we fit ' by estimating the (well-specified) Gaussian mixture by maximum likelihood and fit μ using m separate gradient-boosted tree models (sklearn defaults). We consider evaluating ⇡ ⇤ either using the vanilla estimator ⌧W or the doubly robust estimator ⌧W,μ for W either chosen in the 4 different standard ways laid out in Sec. 1.2, using either the true ' or the estimated ', or chosen by the balanced evaluation approach using untuned parameters (rather than fit by marginal likelihood) using the standard (s = 1) Mahalanobis RBF kernel for K t , kf k 2 = P m t=1 kf t k 2 Kt , and ⇤ = I. (Note that this misspecifies the outcome model, kµ t k Kt = 1.) We tabulate the results in Tab. 1.</p><p>We note a few observations on the standard approaches: vanilla IPW with true ' has zero bias but large SD (standard deviation) and hence RMSE (root mean square error); a DR approach improves on a vanilla IPW with ' by reducing bias; clipping and normalizing IPW reduces SD. The balanced evaluation approach achieves the best RMSE by a clear margin, with the vanilla estimator beating all standard vanilla and DR estimators and the DR estimator providing a further improvement by nearly eliminating bias (but increasing SD). The marked success of the balanced approach is unsurprising when considering the support kW k 0 = P n i=1 I [W i &gt; 0] of the weights. All standard approaches use weights that are multiples of ⇡ Ti (X i ), limiting support to the overlap between ⇡ and T 1:n , which hovers around 10-16 over replications. The balanced approach uses weights that have significantly wider support, around 88-94. In light of this, the success of the balanced approach is expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Consistent Evaluation</head><p>Next we consider the question of consistent evaluation: under what conditions can we guarantee that ⌧W ⇤ (⇡) SAPE(⇡) and ⌧W ⇤ (⇡),μ SAPE(⇡) converge to zero and at what rates.</p><p>One key requirement for consistent evaluation is a weak form of overlap between the historical data and the target policy to be evaluated using this data:  This ensures that if ⇡ can assign treatment t to X then the data will have some examples of units with similar covariates being given treatment t; otherwise, we can never say what the outcome might look like. Another key requirement is specification. If the mean-outcome function is well-specified in that it is in the RKHS product used to compute W ⇤ (⇡) then convergence at rate 1/ p n is guaranteed. Otherwise, for a doubly robust estimator, if the regression estimate is well-specified then consistency is still guaranteed. In lieu of specification, consistency is also guaranteed if the RKHS product consists of C 0 -universal kernels, defined below, such as the RBF kernel <ref type="bibr" target="#b39">[40]</ref>. Definition 1. A PSD kernel K on a Hausdorff X (e.g., R d ) is C 0 -universal if, for any continuous function g : X ! R with compact support (i.e., for some C compact, {x : g(x) 6 = 0} ✓ C) and ⌘ &gt; 0, there exists m,</p><formula xml:id="formula_26">Assumption 2 (Weak overlap). P(' t (X) &gt; 0_⇡ t (X) = 0) = 1 8t 2 [m], E[⇡ 2 T (X)/' 2 T (X)] &lt; 1.</formula><formula xml:id="formula_27">↵ 1 , x 1 , . . . , ↵ m , x m such that sup x2X | P m j=1 ↵ i K(x j , x) g(x)|  ⌘. Theorem 3. Fix ⇡ and let W ⇤ n (⇡) = W ⇤ n (⇡; kf k p,K1:m, n,1:m , ⇤ n ) with 0 I ⇤ n I, 0 &lt;  n,t  8t 2 [m] for each n. Suppose Asns. 1 and 2 hold, Var(Y | X) a.s. bounded, E[ p K t (X, X)] &lt; 1, and E[K t (X, X)⇡ 2 T (X)/' 2 T (X)] &lt; 1.</formula><p>Then the following two results hold:</p><formula xml:id="formula_28">(a) If kµ t k Kt &lt; 1 for all t 2 [m]: ⌧W ⇤ n (⇡) SAPE(⇡) = O p (1/ p n). (b) If K t is C 0 -universal for all t 2 [m]: ⌧W ⇤ n (⇡) SAPE(⇡) = o p (1).</formula><p>The key assumptions of Thm. 3 are unconfoundedness, overlap, and bounded variance. The other conditions simply guide the choice of method parameters. The two conditions on the kernel are trivial for bounded kernels like the RBF kernel. An analogous result for the DR estimator is a corollary. </p><formula xml:id="formula_29">SAPE(⇡) = ( 1 n 2 P n i=1 W ⇤ ni 2 Var(Y i | X i )) 1/2 + o p (1/ p n). (b) If kμ n (X) µ(X)k 2 = O p (r(n)), r(n) = ⌦(1/ p n): ⌧W ⇤ n (⇡),μn SAPE(⇡) = O p (r(n)). (c) If kµ t k Kt &lt; 1, kμ nt k Kt = O p (1) for all t 2 [m]: ⌧W ⇤ n (⇡),μn SAPE(⇡) = O p (1/ p n). (d) If K t is C 0 -universal for all t 2 [m]: ⌧W ⇤ n (⇡),μn SAPE(⇡) = o p (1).</formula><p>Cor. 4(a) is the case where both the balancing weights and the regression function are well-specified, in which case the multiplicative bias disappears faster than o p (1/ p n), leaving us only with the irreducible residual variance, leading to an efficient evaluation. The other cases concern the "doubly robust" nature of the balanced DR estimator: Cor. 4(b) requires only that the regression be consitent and Cor. 4(c)-(d) require only the balancing weights to be consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Balanced Learning</head><p>Next we consider a balanced approach to policy learning. Given a policy class ⇧ ⇢ [X ! m ], we let the balanced policy learner yield the policy ⇡ 2 ⇧ that minimizes the balanced policy evaluation using either a vanilla or DR estimator plus a potential regularization term in the worst-case/posterior CMSE of the evaluation. We formulate this as a bilevel optimization problem:</p><formula xml:id="formula_30">⇡bal 2 argmin ⇡ {⌧ W + E(W, ⇡; k • k, ⇤) : ⇡ 2 ⇧, W 2 argmin W 2W E 2 (W, ⇡; k • k, ⇤)} (1) ⇡bal-DR 2 argmin ⇡ {⌧ W,μ + E(W, ⇡; k • k, ⇤) : ⇡ 2 ⇧, W 2 argmin W 2W E 2 (W, ⇡; k • k, ⇤)} (2)</formula><p>The regularization term regularizes both the balance (i.e., worst-case/posterior bias) that is achievable for ⇡ and the variance in evaluating ⇡. We include this regularizer for completeness and motivated by the results of <ref type="bibr" target="#b41">[42]</ref> (which regularize variance), but find that it not necessary to include it in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Optimizing the Balanced Policy Learner</head><p>Unlike <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b44">45]</ref>, our (nonconvex) policy optimization problem does not reduce to weighted classification precisely because our weights are not multiplies of ⇡ Ti (X i ) (but therefore our weights also lead to better performance). Instead, like <ref type="bibr" target="#b41">[42]</ref>, we use gradient descent. For that, we need to be able to differentiate our bilevel optimization problem. We focus on p = 2 for brevity.</p><formula xml:id="formula_31">Theorem 5. Let k • k = k • k 2,K1:m, 1:m . Then 9W ⇤ (⇡) 2 argmin W 2W E 2 (W, ⇡; k • k, ⇤) such that r ⇡ t (X 1 ),...,⇡ t (Xn) ⌧W ⇤ (⇡) = 1 n Y T 1:n H(I (A + (I A) H) 1 (I A) H)Jt r ⇡ t (X 1 ),...,⇡ t (Xn) ⌧W ⇤ (⇡), μ = 1 n ✏T 1:n H(I (A + (I A) H) 1 (I A) H)Jt + 1 n μt(X1:n) r ⇡ t (X 1 ),...,⇡ t (Xn) E(W ⇤ (⇡), ⇡; k • k, ⇤) = Dt/E(W ⇤ (⇡), ⇡; k • k, ⇤) where H = F (F T HF ) 1 F T , F ij = ij in for i 2 [n], j 2 [n 1], A ij = ij I [W ⇤ i (⇡) &gt; 0], D ti = 2 t P n j=1 K t (X i , X j )(W j Tj t ⇡ t (X j )), H ij = 2 P m t=1<label>2</label></formula><p>t Tit Tj t K t (X i , X j ) + 2⇤, and J tij = 2 2 t Tit K t (X i , X j ). To leverage this result, we use a parameterized policy class such as ⇧ logit = {⇡ t (x; t ) / exp( t0 + T t x)} (or kernelized versions thereof), apply chain rule to differentiate objective in the parameters , and use BFGS <ref type="bibr" target="#b14">[15]</ref> with random starts. The logistic parametrization allows us to smooth the problem even while the solution ends up being deterministic (extreme ). This approach requires solving a quadratic program for each objective gradient evaluation. While this can be made faster by using the previous solution as warm start, it is still computationally intensive, especially as the bilevel problem is nonconvex and both it and each quadratic program solved in "batch" mode. This is a limitation of the current optimization algorithm that we hope to improve on in the future using specialized methods for bilevel optimization <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>.</p><p>Example 2. We return to Ex. 1 and consider policy learning. We use the fixed draw shown in Fig. <ref type="figure" target="#fig_0">1a</ref> and set to 0. We consider a variety of policy learners and plot the policies in Fig. <ref type="figure">2</ref> along with their population regret PAPE(⇡) PAPE(⇡ ⇤ ). The policy learners we consider are: minimizing standard IPW and DR evaluations over ⇧ logit with ', μ as in Ex. 1 (versions with combinations of normalized, clipped, and/or true ', not shown, all have regret 0.26-0.5), the direct method with Gaussian process regression gradient boosted trees (both sklearn defaults), weighted SVM classification using IPW and DR weights (details in supplement), SNPOEM <ref type="bibr" target="#b42">[43]</ref>, PF <ref type="bibr" target="#b22">[23]</ref>, and our balanced policy learner <ref type="bibr" target="#b0">(1)</ref> with parameters as in Ex. 1, ⇧ = ⇧ logit , = ⇤ = 0 (the DR version (2), not shown, has regret .08).</p><p>Example 3. Next, we consider two UCI multi-class classification datasets <ref type="bibr" target="#b29">[30]</ref>, Glass (n = 214, d = 9, m = 6) and Ecoli (n = 336, d = 7, m = 8), and use a supervised-to-contextual-bandit transformation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b41">42]</ref> to compare different policy learning algorithms. Given a supervised multiclass dataset, we draw T as per a multilogit model with random ±1 coefficients in the normalized covariates X. Further, we set Y to 0 if T matches the label and 1 otherwise. And we split the data 75-25 into training and test sample. Using 100 replications of this process, we evaluate the performance of learned linear policies by comparing the linear policy learners as in Ex. 2. For IPW-based approaches, we estimate ' by a multilogit regression (well-specified by construction). For DR approaches, we estimate μ using gradient boosting trees (sklearn defaults). We compare these to our balanced policy learner in both vanilla and DR forms with all parameters fit by marginal likelihood using the RBF kernel with an unspecified length scale after normalizing the data. We tabulate the results in Tab. 2. They first demonstrate that employing the various stopgap fixes to IPW-based policy learning as in SNPOEM indeed provides a critical edge. This is further improved upon by using a balanced approach to policy learning, which gives the best results. In this example, DR approaches do worse than vanilla ones, suggesting both that XGBoost provided a bad outcome model and/or that the additional variance of DR was not compensated for by sufficiently less bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Uniform Consistency and Regret Bounds</head><p>Next, we establish consistency results uniformly over policy classes. This allows us to bound the regret of the balanced policy learner. We define the sample and population regret, respectively, as </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The setting in Ex. 1</figDesc><graphic coords="5,256.50,93.07,99.00,91.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc>Figure 2: Policy learning results in Ex. 2; numbers denote regret</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Corollary 4 .</head><label>4</label><figDesc>Suppose the assumptions of Thm. 3 hold and. Then (a) If kμ nt µ t k Kt = o p (1) 8t 2 [m]: ⌧W ⇤ n (⇡),μn</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>R</head><label></label><figDesc>⇧ (⇡) = PAPE(⇡) min ⇡2⇧ PAPE(⇡), b R ⇧ (⇡) = SAPE(⇡) min ⇡2⇧ SAPE(⇡)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Policy evaluation performance in Ex. 1</figDesc><table><row><cell>Weights W</cell><cell>RMSE</cell><cell>Vanilla ⌧W Bias</cell><cell>SD</cell><cell cols="2">Doubly robust ⌧W,μ RMSE Bias SD</cell><cell>kW k0</cell></row><row><cell>IPW, ' IPW, ' .05-CIPW, ' .05-CIPW, ' NIPW, ' NIPW, ' .05-NCIPW, ' .05-NCIPW, ' Balanced eval</cell><cell>2.209 0.568 0.581 0.568 0.519 0.463 0.485 0.463 0.280</cell><cell cols="2">0.005 2.209 0.514 0.242 0.491 0.310 0.514 0.242 0.181 0.487 0.251 0.390 0.250 0.415 0.251 0.390 0.227 0.163</cell><cell>4.196 0.428 0.520 0.428 0.754 0.692 0.724 0.692 0.251</cell><cell>0.435 4.174 0.230 0.361 0.259 0.451 0.230 0.361 0.408 0.634 0.467 0.511 0.471 0.550 0.467 0.511 0.006 0.251</cell><cell>13.6 ± 2.9 13.6 ± 2.9 13.6 ± 2.9 13.6 ± 2.9 13.6 ± 2.9 13.6 ± 2.9 13.6 ± 2.9 13.6 ± 2.9 90.7 ± 3.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2: Policy learning results in Ex. 2; numbers denote regret</figDesc><table><row><cell></cell><cell>IPW</cell><cell>.50</cell><cell>Gauss Proc 0.29</cell><cell cols="2">IPW-SVM 0.34</cell><cell>SNPOEM</cell><cell>0.28</cell></row><row><cell>Balanced policy learner</cell><cell>.06</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>DR</cell><cell>.26</cell><cell>Grad Boost 0.20</cell><cell>DR-SVM</cell><cell>0.18</cell><cell>PF</cell><cell>0.23</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Thus, although the data is iid, the t-treated sample {i : Ti = t} may differ systematically from the t 0 -treated sample {i : Ti = t 0 } for t 6 = t 0 , i.e., not necessarily just by chance as in a randomized controlled trial (RCT).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This problem is unique to policy evaluation and learning -in causal effect estimation, the IPW estimator for SATE has nonzero weights on all of the data points. For policy learning with m = 2, Athey and Wager<ref type="bibr" target="#b0">[1]</ref>, Beygelzimer and Langford<ref type="bibr" target="#b6">[7]</ref> minimize estimates of the form<ref type="bibr" target="#b0">1</ref> 2 (⌧ (⇡) ⌧ (1(•) ⇡)) with ⌧ (⇡) = ⌧ IPW (⇡) or = ⌧ DR (⇡). This evaluates ⇡ relative to the uniformly random policy and the resulting total weighted sums over Yi or ✏i have nonzero weights whether ⇡T i (Xi) = 0 or not. While a useful approach for reduction to weighted classification<ref type="bibr" target="#b6">[7]</ref> or invoking semi-parametric theory<ref type="bibr" target="#b0">[1]</ref>, it only works for m = 2, has no effect on learning as the centering correction is constant in ⇡, and, for evaluation, is not an estimator for SAPE.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This material is based upon work supported by the National Science Foundation under Grant No. 1656996.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A key requirement for these to converge is that the best-in-class policy is learnable. We quantify that using Rademacher complexity <ref type="bibr" target="#b2">[3]</ref> and later extend our results to VC dimension. Let us define</p><p>and same for b R n (F). We also strengthen the overlap assumption.</p><p>for each n and ⇡ 2 ⇧. Suppose Asns. 1 and 3 hold, |✏ i |  B a.s. bounded, and</p><p>Then the following two results hold:</p><p>we have that, with probability at least 1 ⌫,</p><p>.</p><p>The proof crucially depends on simultaneously handling the functional complexities of both the policy class ⇧ and the space of functions {f : kf k &lt; 1} being balanced against. Again, the key assumptions of Thm. 6 are unconfoundedness, overlap, and bounded residuals. The other conditions simply guide the choice of method parameters. Regret bounds follow as a corollary.</p><p>Corollary 7. Suppose the assumptions of Thm. 6 hold. If ⇡bal n is as in (1) then:</p><p>. And, all the same results hold when replacing R n (⇧) with b R n (⇧) and/or replacing R ⇧ with b R ⇧ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>Considering the policy evaluation and learning problems using observational or logged data, we presented a new method that is based on finding optimal balancing weights that make the data look like the target policy and that is aimed at ameliorating the shortcomings of existing methods, which included having to deal with near-zero propensities, using too few positive weights, and using an awkward two-stage procedure. The new approach showed promising signs of fixing these issues in some numerical examples. However, the new learning method is more computationally intensive than existing approaches, solving a QP at each gradient step. Therefore, in future work, we plan to explore faster algorithms that can implement the balanced policy learner, perhaps using alternating descent, and use these to investigate comparative numerics in much larger datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02896</idno>
		<title level="m">Efficient policy learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Moving towards best practice when using inverse probability of treatment weighting (iptw) using the propensity score to estimate causal treatment effects in observational studies</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="3661" to="3679" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bilevel optimization and machine learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kunapuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE World Congress on Computational Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="25" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The power and limits of predictive approaches to observationaldata-driven optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02347</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Personalized diabetes management using electronic medical records</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diabetes care</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="217" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The offset tree for learning with partial labels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: the example of computational advertising</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Q</forename><surname>Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Snelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<title level="m">Convex Optimization</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Regret analysis of stochastic and nonstochastic multi-armed bandit problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Double machine learning for treatment and causal parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00060</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On the algorithmic implementation of multiclass kernel-based vector machines</title>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="265" to="292" />
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1103.4601</idno>
		<title level="m">Doubly robust policy evaluation and learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Model averaging methods for weight trimming</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Elliott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of official statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">517</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Practical methods of optimization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel method for the two-sample-problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of the propensity score in estimating dose-response functions</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Causal inference in statistics, social, and biomedical sciences</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Truncated importance sampling</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Ionides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="311" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the complexity of linear prediction: Risk bounds, margin bounds, and regularization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08321</idno>
		<title level="m">Generalized optimal matching methods for causal inference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursive partitioning for personalization using observational data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1789" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Optimal a priori balance in the design of controlled experiments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Confounding-robust policy improvement</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Policy evaluation and optimization with continuous treatments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="539" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Probability in Banach Spaces: isoperimetry and processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ledoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Talagrand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unbiased offline evaluation of contextual-banditbased news article recommendation algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth ACM international conference on Web search and data mining</title>
		<meeting>the fourth ACM international conference on Web search and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="297" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stratification and weighting via the propensity score in estimation of causal treatment effects: a comparative study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Lunceford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Davidian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2937" to="2960" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Techniques for gradient-based bilevel optimization with non-smooth lower level problems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="175" to="194" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Performance guarantees for individualized treatment rules</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1180</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust estimation in sequentially ignorable missing data and causal inference models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the American Statistical Association</title>
		<meeting>the American Statistical Association</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of regression coefficients when some regressors are not always observed</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rotnitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">427</biblScope>
			<biblScope unit="page" from="846" to="866" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Royden</surname></persName>
		</author>
		<title level="m">Real Analysis</title>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A first order method for solving convex bilevel optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shtern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="640" to="660" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adjusting for nonignorable drop-out using semiparametric nonresponse models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Scharfstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rotnitzky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">448</biblScope>
			<biblScope unit="page" from="1096" to="1120" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1003.0887</idno>
		<title level="m">Universality, characteristic kernels and rkhs embedding of measures</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning from logged implicit exploration data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2217" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Counterfactual risk minimization: Learning from logged bandit feedback</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="814" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The self-normalized estimator for counterfactual learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3231" to="3239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Residual weighted learning for estimating individualized treatment rules</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer-Hamblett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Kosorok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">517</biblScope>
			<biblScope unit="page" from="169" to="187" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
