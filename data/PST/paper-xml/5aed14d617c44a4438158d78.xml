<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Memory Access Patterns</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
							<email>&lt;miladh@google.com&gt;</email>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jamie</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
							<affiliation key="aff0">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Memory Access Patterns</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing.</p><p>Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The proliferation of machine learning, and more recently deep learning, in real-world applications has been made possible by an exponential increase in compute capabilities, largely driven by advancements in hardware design. To maximize the effectiveness of a given design, computer architecture often involves the use of prediction and heuristics. Prefetching is a canonical example of this, where instruc-Proceedings of the 35 th International Conference on Machine <ref type="bibr">Learning, Stockholm, Sweden, PMLR 80, 2018.</ref> Copyright 2018 by the author(s).</p><p>tions or data are brought into much faster storage well in advance of their required usage.</p><p>Prefetching addresses a critical bottleneck in von Neumann computers: computation is orders of magnitude faster than accessing memory. This problem is known as the memory wall <ref type="bibr" target="#b47">(Wulf &amp; McKee, 1995)</ref>, and modern applications can spend over 50% of all compute cycles waiting for data to arrive from memory <ref type="bibr" target="#b23">(Kozyrakis et al., 2010;</ref><ref type="bibr" target="#b6">Ferdman et al., 2012;</ref><ref type="bibr" target="#b20">Kanev et al., 2015)</ref>. To mitigate the memory wall, microprocessors use a hierarchical memory system, with small and fast memory close to the processor (i.e., caches), and large yet slower memory farther away. Prefetchers predict when to fetch what data into cache to reduce memory latency, and the key towards effective prefetching is to attack the difficult problem of predicting memory access patterns.</p><p>Predictive optimization such as prefetching is one form of speculation. Modern microprocessors leverage numerous types of predictive structures to issue speculative requests with the aim of increasing performance. Historically, most predictors in hardware are table-based. That is, future events are expected to correlate with past history tracked in lookup tables (implemented as memory arrays). These memory arrays are sized based on the working set, or amount of information that the application actively uses. However, the working sets of modern datacenter workloads are orders of magnitude larger than those of traditional workloads such as SPEC CPU2006 and continue to grow <ref type="bibr" target="#b0">(Ayers et al., 2018;</ref><ref type="bibr" target="#b6">Ferdman et al., 2012;</ref><ref type="bibr" target="#b10">Gutierrez et al., 2011;</ref><ref type="bibr" target="#b11">Hashemi et al., 2016)</ref>. This trend poses a significant challenge, as prediction accuracy drops sharply when the working set is larger than the predictive table. Scaling predictive tables with fast-growing working sets is difficult and costly for hardware implementation.</p><p>Neural networks have emerged as a powerful technique to address sequence prediction problems, such as those found in natural language processing (NLP) and text understanding <ref type="bibr" target="#b1">(Bengio et al., 2003;</ref><ref type="bibr" target="#b31">Mikolov et al., 2010;</ref><ref type="bibr">2013)</ref>. Simple perceptrons have even been deployed in hardware (e.g., SPARC T4 processor <ref type="bibr" target="#b8">(Golla &amp; Jordan, 2011)</ref>), to handle branch prediction <ref type="bibr" target="#b17">(Jim√©nez &amp; Lin, 2001</ref>). Yet, exploring the effectiveness of sequential learning algorithms in microarchitectural designs is still an open area of research.</p><p>In this paper, we explore the utility of sequence-based neural networks in microarchitectural systems. In particular, given the challenge of the memory wall, we apply sequence learning to the difficult problem of prefetching.</p><p>Prefetching is fundamentally a regression problem. The output space, however, is both vast and extremely sparse, making it a poor fit for standard regression models. We take inspiration from recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet <ref type="bibr" target="#b37">(Oord et al., 2016a;</ref><ref type="bibr">b)</ref>. Discretization makes prefetching more analogous to neural language models, and we leverage it as a starting point for building neural prefetchers. We find that we can successfully model the output space to a degree of accuracy that makes neural prefetching a very distinct possibility. On a number of benchmark datasets, we find that recurrent neural networks significantly outperform the stateof-the-art of traditional hardware prefetchers. We also find that our results are interpretable. Given a memory access trace, we show that the RNN is able to discern semantic information about the underlying application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Microarchitectural Data Prefetchers</head><p>Prefetchers are hardware structures that predict future memory accesses from past history. They can largely be separated into two categories: stride prefetchers and correlation prefetchers. Stride prefetchers are commonly implemented in modern processors and lock onto stable, repeatable deltas (differences between subsequent memory addresses) <ref type="bibr" target="#b7">(Gindele, 1977;</ref><ref type="bibr" target="#b19">Jouppi, 1990;</ref><ref type="bibr" target="#b39">Palacharla &amp; Kessler, 1994)</ref>. For example, given an access pattern that adds four to a memory address every time <ref type="bibr">(0,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">12)</ref>, a stride prefetcher will learn that delta and try to prefetch ahead of the demand stream, launching parallel accesses to potential future address targets <ref type="bibr">(16,</ref><ref type="bibr">20,</ref><ref type="bibr">24)</ref> up to a set prefetch distance.</p><p>Correlation prefetchers try to learn patterns that may repeat, but are not as consistent as a single stable delta <ref type="bibr" target="#b3">(Charney &amp; Reeves, 1995;</ref><ref type="bibr" target="#b25">Lai et al., 2001;</ref><ref type="bibr" target="#b43">Somogyi et al., 2006;</ref><ref type="bibr" target="#b42">Roth et al., 1998)</ref>. They store the past history of memory accesses in large tables and are better at predicting more irregular patterns than stride prefetchers. Examples of correlation prefetchers include Markov prefetchers <ref type="bibr" target="#b18">(Joseph &amp; Grunwald, 1997)</ref>, GHB prefetchers <ref type="bibr" target="#b36">(Nesbit &amp; Smith, 2004)</ref>, and more recent work that utilizes larger in-memory structures <ref type="bibr" target="#b16">(Jain &amp; Lin, 2013)</ref>. Correlation prefetchers require large, costly tables, and are typically not implemented in modern multi-core processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Recurrent Neural Networks</head><p>Deep learning has become the model-class of choice for many sequential prediction problems. Notably, speech recognition <ref type="bibr" target="#b13">(Hinton et al., 2012)</ref> and natural language processing <ref type="bibr" target="#b31">(Mikolov et al., 2010)</ref>. In particular, RNNs are a preferred choice for their ability to model long-range dependencies. LSTMs <ref type="bibr" target="#b14">(Hochreiter &amp; Schmidhuber, 1997)</ref> have emerged as a popular RNN variant that deals with training issues in standard RNNs, by propagating the internal state additively instead of multiplicatively. An LSTM is composed of a hidden state h and a cell state c, along with input i, forget f , and output gates o that dictate what information gets stored and propagated to the next timestep. At timestep N , input x N is presented to the LSTM, and the LSTM states are computed using the following process:</p><p>1. Compute the input, forget, and output gates</p><formula xml:id="formula_0">i N = œÉ(W i [x N , h N ‚àí1 ] + b i ) f N = œÉ(W f [x N , h N ‚àí1 ] + b f ) o N = œÉ(W o [x N , h N ‚àí1 ] + b o ) 2. Update the cell state c N = f N c N ‚àí1 + i N tanh(W c [x N , h N ‚àí1 ] + b c ) 3. Compute the LSTM hidden (output) state h N = o N tanh(c N )</formula><p>Where [x N , h N ‚àí1 ] represents the concatenation of the current input and previous hidden state, represents elementwise multiplication, and œÉ(u) = 1 1+exp(‚àíu) is the sigmoid non-linearity.</p><p>The above process forms a single LSTM layer, where W {i,f,o,c} are the weights of the layer, and b {i,f,o,c} are the biases. LSTM layers can be further stacked so that the output of one LSTM layer at time N becomes the input to another LSTM layer at time N , allowing for greater modeling flexibility with relatively few extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prefetching as a Prediction Problem</head><p>Prefetching is the process of predicting future memory accesses that will miss in the on-chip cache and access memory based on past history. Each of these memory addresses are generated by a memory instruction (a load/store). Memory instructions are a subset of all instructions that interact with the addressable memory of the computer system.</p><p>Many hardware proposals use two features to make these prefetching decisions: the sequence of caches miss addresses that have been observed so far and the sequence of instruction addresses, also known as program counters (PCs), that are associated with the instruction that generated each of the cache miss addresses.</p><p>PCs are unique tags, that is each PC is unique to a particular instruction that has been compiled from a particular function in a particular code file. PC sequences can inform the model of patterns in the control flow of higher level code, while the miss address sequence informs the model of which address to prefetch next. In modern computer systems, both of these features are represented as 64-bit integers.</p><p>Therefore, an initial model could use two input features at a given timestep N . It could use the address and PC that generated a cache miss at that timestep to predict the address of the miss at timestep N + 1.</p><p>However, one concern quickly becomes apparent: the address space of an application is extremely sparse. In our training data with O(100M) cache misses, only O(10M) unique cache block miss addresses appear on average out of the entire 2 64 physical address space. This is displayed when we plot an example trace from omnetpp (a benchmark from the standard SPEC CPU2006 benchmark suite <ref type="bibr">(Sta, 2006)</ref>) in Figure <ref type="figure" target="#fig_1">1</ref>, where the red datapoints are cache miss addresses<ref type="foot" target="#foot_0">1</ref> . The wide range and severely multi-modal nature of this space makes it a challenge for time-series regression models. For example, neural networks tend to work best with normalized inputs, however when normalizing this data, the finite precision floating-point representation results in a significant loss of information. This issue affects modeling at both the input and output levels, and we will describe several approaches to deal with both aspects.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Prefetching as Classification</head><p>Rather than treating the prefetching problem as regression, we opt to treat the address space as a large, discrete vocabulary, and perform classification. This is analogous to next-word or character prediction in natural language processing. The extreme sparsity of the space, and the fact that some addresses are much more commonly accessed than others, means that the effective vocabulary size can actually be manageable for RNN models. Additionally, the model gains flexibility by being able to produce multi-modal outputs, compared to unimodal regression techniques that assume e.g., a Gaussian likelihood.</p><p>However, there are 2 64 possible softmax targets, so a quantization scheme is necessary. Importantly, in order to be useful, a prefetch must be within a cache line to be completely accurate, usually within 64 bytes. There is a second order benefit if it is within a page, usually 4096 bytes, but even predicting at the page level would leave 2 52 possible targets. In <ref type="bibr" target="#b38">(Oord et al., 2016b)</ref>, they predict 16-bit integer values from an acoustic signal. To avoid having to apply a softmax over 2 16 values, they apply a non-linear quantization scheme to reduce the space to 256 categories. This form of quantization is inappropriate for our purposes, as it decreases the resolution of addresses towards the extremes of the address space, whereas in prefetching we need high resolution in every area where addresses are used.</p><p>Luckily, programs tend to behave in predictable ways, so only a relatively small (but still large in absolute numbers), and consistent set of addresses are ever seen. Our primary quantization scheme is to therefore create a vocabulary of common addresses during training, and to use this as the set of targets during testing. This reduces the coverage, as there may be addresses at test time that are not seen during training time, however we will show that for reasonablysized vocabularies, we capture a significant proportion of the space. The second approach we explore is to cluster the addresses using clustering on the address space. This is akin to an adaptive form of non-linear quantization.</p><p>Due to dynamic side-effects such as address space layout randomization (ASLR), different runs of the same program will lead to different raw address accesses <ref type="bibr">(PaX Team, 2003)</ref>. However, given a layout, the program will behave in a consistent manner. Therefore, one potential strategy is to predict deltas, ‚àÜ N = Addr N +1 ‚àíAddr N , instead of addresses directly. These will remain consistent across program executions, and come with the benefit that the number of uniquely occurring deltas is often orders of magnitude smaller than uniquely occurring addresses. This is shown in Table <ref type="table" target="#tab_0">1</ref>, where we show the number of unique PCs, addresses, and deltas across a suite of program trace datasets. We also show the number of unique addresses and deltas required to achieve 50% coverage. In almost all cases, this is much smaller when considering deltas. In our models, we therefore use deltas as inputs instead of raw addresses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Models</head><p>In this section we introduce two LSTM-based prefetching models. The first version is analogous to a standard language model, while the second exploits the structure of the  <ref type="table" target="#tab_0">1</ref>, the size of the vocabulary required in order to obtain at best 50% accuracy is usually O(1000) or less, well within the capabilities of standard language models. Our first model therefore restricts the output vocabulary size to 50,000 of the most frequent, unique deltas. For the input vocabulary, we include all deltas as long as they appear in the dataset at least 10 times. Expanding the vocabulary beyond this is challenging, both computationally and statistically.</p><p>We leave an exploration of approaches like the hierarchical softmax <ref type="bibr" target="#b33">(Mnih &amp; Hinton, 2009)</ref> to future work.</p><p>We refer to this model as the embedding LSTM, as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. It uses a categorical (one-hot) representation for both the input and output deltas. At timestep N , the input PC N and ‚àÜ N are individually embedded and then the embeddings are concatenated and fed as inputs to a twolayer LSTM. The LSTM then performs classification over the delta vocabulary, and the K highest-probability deltas are chosen for prefetching 2 .</p><p>In a practical implementation, a prefetcher can return several predictions. This creates a trade-off, where more predictions increases the probability of a cache hit at the next timestep, but potentially removes other useful items from the cache. We opt to prefetch the top-10 predictions of the LSTM at each timestep. Other possibilities that we do not explore here include using a beam-search to predict the next n deltas, or to learn to directly predict N to N + n steps ahead in one forward pass of the LSTM.</p><p>There are several limitations to this approach. First, a large vocabulary increases the model's computational and storage footprint. Second, truncating the vocabulary necessarily 2 Directly predicting probabilites is another advantage that classification provides over traditional hardware. puts a ceiling on the accuracy of the model. Finally, dealing with rarely occurring deltas is non-trivial. This is analogous to the rare word problem in NLP <ref type="bibr" target="#b28">(Luong et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cache</head><formula xml:id="formula_1">PC 1 Œî 1 f(Œî 1 ) g(PC 1 ) PC 2 Œî 2 PC 3 Œî 3 PC 4 Œî 4 PC 5 Œî 5 PC 6 Œî 6 PC 7 Œî 7 PC 8 Œî 8 PC 9 Œî 9 f(Œî 2 ) g(PC 2 ) f(Œî 3 ) g(PC 3 ) f(Œî 4 ) g(PC 4 ) f(Œî 5 ) g(PC 5 ) f(Œî 6 ) g(PC 6 ) f(Œî 7 ) g(PC 7 ) f(Œî 8 ) g(PC 8 ) f(Œî 9 ) g(PC 9 ) Concat LSTM Embed Œî ÃÉ10.2 Œî ÃÉ10.1 Œî ÃÉ10.3 ‚Ä¶ Œî ÃÉ10.K</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Clustering + LSTM</head><p>We hypothesize that much of the interesting interaction between addresses occurs locally in address space. As one example, data structures like structs and arrays tend to be stored in contiguous blocks, and accessed repeatedly. In this model, we exploit this idea to design a prefetcher that very carefully models local context, whereas the embedding LSTM models both local and global context.</p><p>By looking at narrower regions of the address space, we can see that there is indeed rich local context. We took the set of addresses from omnetpp and clustered them into 6 different regions using k-means. We show two of the clusters in Figure <ref type="figure" target="#fig_3">3</ref>, and the rest can be found in the appendix.</p><p>To assess the relative accuracy of modeling local addressspace regions, we first cluster the raw address space using k-means. The data is then partitioned into these clusters, and deltas are computed within each cluster. A visual example of this is shown in Figure <ref type="figure" target="#fig_5">4a</ref>. We found that one of the major advantages of this approach is that the set of deltas within a cluster is significantly smaller than the global vocabulary, alleviating some of the issues with the embedding LSTM.</p><p>To reduce the size of the model, we use a multi-task LSTM to model all of the clusters. Stated another way, we use an LSTM to model each cluster independently, but tie the weights of the LSTMs. However, we provide the cluster ID as an additional feature, which effectively gives each LSTM a different set of biases.</p><p>The partitioning of the address space into narrower regions also means that the set of addresses within each cluster will take on roughly the same order of magnitude, meaning that the resulting deltas can be effectively normalized and used as real-valued inputs to the LSTM. This allows us to further reduce the size of the model, as we do not need to keep around a large matrix of embeddings. Importantly, we still treat next-delta prediction as a classification problem, as we found that regression is still too inaccurate to be practical<ref type="foot" target="#foot_2">3</ref> .</p><p>This version of the LSTM addresses some of the issues of the embedding LSTM. The trade-offs are that it requires an additional step of pre-processing to cluster the address space, and that it only models local context. That is, it cannot model the dynamics that cause the program to access different regions of the address space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>A necessary condition for neural networks to be effective prefetchers is that they must be able to accurately predict cache misses. Our experiments measure their effectiveness in this task when compared with traditional hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data Collection</head><p>The data used in our evaluation is a dynamic trace that contains the sequence of memory addresses that an application computes. This trace is captured by using a dynamic instrumentation tool, Pin <ref type="bibr" target="#b27">(Luk et al., 2005)</ref>, that attaches to the process and emits a "PC, Virtual Address" tuple into a file every time the instrumented application accesses memory (every load or store instruction).</p><p>This raw access trace mostly contains accesses that hit in the cache (such as stack accesses, which are present in the data cache). Since we are focused on predicting cache misses, we obtain the sequence of cache misses by simulating this trace through a simple cache simulator that emulates an Intel Broadwell microprocessor (Section 3.1).</p><p>To evaluate our proposals, we use the memory intensive applications of SPEC CPU2006. This is a standard benchmark suite that is used pervasively to evaluate the performance of computer systems. However, SPEC CPU2006 also has small working sets when compared to modern datacenter workloads. Therefore in addition to SPEC benchmarks, we also include Google's websearch workload. Websearch is a unique application with complex access patterns that exemplifies enterprise-scale software development and drives industrial hardware platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Setup</head><p>We split each trace into a training and testing set, using 70% for training and 30% for evaluation, and train each LSTM on each dataset independently. The embedding LSTM was trained with ADAM <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2015)</ref> while the clustering LSTM was trained with Adagrad <ref type="bibr" target="#b5">(Duchi et al., 2011)</ref>.</p><p>We report the specific hyperparameters used in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Metrics</head><p>Precision We measure precision-at-10, which makes the assumption that each model is allowed to make 10 predictions at a time. The model predictions are deemed correct if the true delta is within the set of deltas given by the top-10 predictions. A label that is outside of the output vocabulary of the model is automatically deemed to be a failure.</p><p>Recall We measure recall-at-10. Each time the model makes predictions, we record this set of 10 deltas. At the end, we measure the recall as the cardinality of the set of predicted deltas over the entire set seen at test-time. This measures the ability of the prefetcher to make diverse predictions, and quantifies the percentage of the dataset that the prefetcher could predict.</p><p>One subtlety involving the clustering + LSTM model is how it is used at test-time. In practice, if an address generates a cache miss, then we identify the region of this miss, feed it as an input to the appropriate LSTM, and retrieve predictions.   Therefore, the bandwidth required to make a prediction is nearly identical between the two LSTM variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Model Comparison</head><p>We compare our LSTM-based prefetchers to two state-ofthe-art hardware prefetchers. The first is a standard stream prefetcher. We simulate a hardware structure that supports up to 10 simultaneous streams to maintain parity between the ML and traditional predictors. The second is a GHB PC/DC prefetcher <ref type="bibr" target="#b36">(Nesbit &amp; Smith, 2004)</ref>. This is a correlation prefetcher that uses two tables. The first table stores PCs, these PCs then serve as a pointer into the second table where delta history is recorded. On every access, the GHB prefetcher jumps through the second table in order to prefetch deltas that it has recorded in the past. This prefetcher excels at more complex memory access patterns, but has much lower recall than the stream prefetcher.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the comparison of the different prefetchers across a range of benchmark datasets. While the stream prefetcher is able to achieve a high recall due to its dynamic vocabulary, the LSTM models otherwise dominate, especially in terms of precision. This is particularly apparent on the websearch dataset, where the complex patterns are a much better fit for the LSTM prefetchers.</p><p>Comparing the embedding LSTM to the cluster + LSTM models, neither model obviously outperforms the other in terms of precision. The clustering + LSTM tends to generate much higher recall, likely the result of having multiple vocabularies. An obvious direction is to ensemble these models, which we leave for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Predictive information of ‚àÜs vs PCs</head><p>In this experiment, we remove one of the ‚àÜs or PCs from the embedding LSTM inputs, and measure the change in predictive ability. This allows us to determine the relative information content contained in each input modality. As Figure <ref type="figure" target="#fig_7">6</ref> shows, both PCs and deltas contain a good amount of predictive information. Most of the information required for high precision is contained within the delta sequence, however the PC sequence helps improve recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Interpreting Program Semantics</head><p>One of the key advantages of using a model to learn patterns that generalize (as opposed to lookup tables) is that the model can then be introspected in order to gain insights into the data. In Figure <ref type="figure" target="#fig_8">7</ref>, we show a t-SNE <ref type="bibr" target="#b29">(Maaten &amp; Hinton, 2008)</ref> visualization of the final state of the concatenated (‚àÜ, PC) embeddings on mcf, colored according to PCs.</p><p>There is clearly a lot of structure to the space. Linking PCs back to the source code in mcf, we observe one cluster that consists of repetitions of the same code statement, caused by the compiler unrolling a loop. A different cluster consists only of pointer dereferences, as the application traverses a linked list. Applications besides mcf show this learned structure as well. In omnetpp we find that inserting and removing into a data structure are mapped to the same cluster and data comparisons are mapped into a different cluster.</p><p>We show these code examples in the appendix, and leave further inspection for future work, but the model appears to be learning about the higher level structure of the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Machine Learning in Microarchitecture</head><p>Machine learning in microarchitecture and computer systems is not new, however the application of machine learning as a complete replacement for traditional systems, especially using deep learning, is a relatively new and largely uncharted area. Here we outline several threads of interaction between machine learning and microarchitecture research.</p><p>Prior work has also directly applied machine learning techniques to microarchitectural problems. Notably, the perceptron branch predictor <ref type="bibr" target="#b17">(Jim√©nez &amp; Lin, 2001</ref>) uses a linear classifier to predict whether a branch is taken or not-taken.</p><p>The perceptron learns in an online fashion by incrementing or decrementing weights based on taken/not-taken outcome. The key benefit of the perceptron is its simplicity, eschewing more complicated training algorithms such as back-propagation to meet tight latency requirements. Other applications of machine learning in microarchitecture include applying reinforcement learning for optimizing the long-term performance of memory controller scheduling algorithms <ref type="bibr" target="#b15">(Ipek et al., 2008)</ref>, tuning performance knobs <ref type="bibr" target="#b2">(Blanton et al., 2015)</ref>, and using bandits to identify patterns in hardware and software features that relate to a memory access <ref type="bibr" target="#b41">(Peled et al., 2015)</ref>.</p><p>Recent work also proposed an LSTM-based prefetcher and evaluated it with generated traces following regular expressions <ref type="bibr" target="#b49">(Zeng, October 2017)</ref>. Using the squared loss, the model caters to capturing regular, albeit non-stride, patterns. Irregular memory reference patterns, either due to workload behavior or multi-core processor reordering/interleaving, pose challenges to such regression-based approaches. Zeng (October 2017) evaluate their model on randomly generated patterns. As detailed in Section 3, we have found regression models to be a poor fit on real workloads.</p><p>Very recent work has also explored the usage of machine learning to replace conventional database index structures such as b-trees and bloom filters <ref type="bibr" target="#b24">(Kraska et al., 2017)</ref>.</p><p>Although the nature of this problem differs from cache prefetching, there are many similarities as well. Specifically, the idea of using the distribution of the data to learn specific models as opposed to deploying generic data structures. Our clustering approach is also reminiscent of the hierarchical approach that they deploy. Importantly, they find that neural network models are faster to query than conventional data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Machine Learning of Program Behavior</head><p>Memory traces can be thought of as a representation of program behavior. Specifically, they represent a bottom-up view of the dynamic interaction of a pre-specified program with a particular set of data. From a machine learning perspective, researchers have taken a top-down approach to explore whether neural networks can understand program behavior and structure.  <ref type="formula">2014</ref>) falls into the category of sequence-tosequence models <ref type="bibr" target="#b44">(Sutskever et al., 2014)</ref>. Another example in this category is the Neural Turing Machine, which augments an LSTM with an external memory and an attention mechanism to form a differentiable analog of a Turing machine <ref type="bibr" target="#b9">(Graves et al., 2014)</ref>. This is used to solve simple problems such as sorting, copying, and associative recall.</p><p>There is also work on modeling source code directly, as opposed to modeling properties of the resulting program. For example, <ref type="bibr" target="#b30">(Maddison &amp; Tarlow, 2014)</ref> creates a generative model of source code using a probabilistic context-free grammar. <ref type="bibr" target="#b12">(Hindle et al., 2012</ref>) models source code as if it were natural language using an n-gram model. This approach has been extended to neural language models for source code <ref type="bibr" target="#b46">(White et al., 2015)</ref>. Lastly, Cummins et al. use neural networks to mine online code repositories to automatically synthesize applications <ref type="bibr" target="#b4">(Cummins et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>Computer architects have long exploited the benefits of learning and predicting program behavior to unlock control and data parallelism. The conventional approach of tablebased predictors, however, is too costly to scale for dataintensive irregular workloads. The models described in this paper demonstrate significantly higher precision and recall than table-based approaches. This study also motivates a rich set of questions that this initial exploration does not solve, and we leave these for future research.</p><p>We have focused on a train-offline test-online model, using precision and recall as evaluation metrics. A prefetcher, through accurate prefetching, changes the distribution of cache misses, which could make a static RNN model less effective. There are several ways to alleviate this, such as adapting the RNN online, or training on both hits and misses. However, this changes the dataset and increases the computational and memory burden of the prefetcher.</p><p>There is a notion of timeliness that is also an important consideration. If the RNN prefetches a line too early, it risks evicting data from the cache that the processor hasn't used yet. If it prefetches too late, the performance impact of the request is minimal, as much of the latency cost of accessing main memory has already been paid. One simple heuristic is to predict several steps ahead, instead of just the next step. This would be similar to the behavior of stream prefetchers.</p><p>Finally, the effectiveness of an RNN prefetcher must eventually be measured in terms of its performance impact within a program. Ideally the RNN would be directly optimized for this. This and the previous issues motivate the use of reinforcement learning techniques <ref type="bibr" target="#b45">(Sutton &amp; Barto, 1998)</ref> as a method to train these RNNs in dynamic environments. Indeed, modern microarchitectures also employ control systems to control prefetcher aggressiveness, and this provides yet another area in which neural networks could be used.</p><p>Additionally, we have not evaluated the hardware design of our models. Correlation prefetchers are difficult to implement in hardware because of their memory size. While it is unclear if DNNs can meet the latency demands required for a hardware accelerator, neural networks also significantly compress learned representations during training, and shift the problem to a compute problem rather than a memory capacity problem. Given the recent proliferation of ML accelerators, this shift towards compute leaves us optimistic at the prospects of neural networks in this domain.</p><p>Prefetching is not the only domain where computer systems employ speculative execution. Branch prediction is the process of predicting the direction of branches that an application will take. Branch target buffers predict the address that a branch will redirect control flow to. Cache replacement algorithms predict the best line to evict from a cache when a replacement decision needs to be made. One consequence of replacing microarchitectural heuristics with learned systems is that we can introspect those systems in order to better understand their behavior. Our t-SNE experiments only scratch the surface and show an opportunity to leverage much of the recent work in understanding RNN systems <ref type="bibr" target="#b34">(Murdoch &amp; Szlam, 2017;</ref><ref type="bibr" target="#b35">Murdoch et al., 2018)</ref>.</p><p>Recent work has also identified timing based attacks as a vulnerability for speculative systems <ref type="bibr" target="#b22">(Kocher et al., 2018;</ref><ref type="bibr" target="#b26">Lipp et al., 2018)</ref>, security implications and adversarial attacks are an open problem.</p><p>The t-SNE results also indicate that an interesting view of memory access traces is that they are a reflection of program behavior. A trace representation is necessarily different from e.g., input-output pairs of functions, as in particular, traces are a representation of an entire, complex, human-written program. This view of learning dynamic behavior provides a different path towards building neural systems that learn and replicate program behavior.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Zoomed</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Cache miss addresses on the omnetpp dataset, demonstrating sparse access patterns at multiple scales.</figDesc><graphic url="image-2.png" coords="3,192.78,408.76,169.02,115.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The embedding LSTM model. f and g represent embedding functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Two of six k-means clusters from omnetpp. Memory accesses are colored according to the PC that generated them.</figDesc><graphic url="image-3.png" coords="5,55.44,67.06,234.00,179.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The clustering + LSTM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The clustering + LSTM data processing and model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Precision and recall comparison between traditional and LSTM prefetchers. Geomean is the geometric mean.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Precision and Recall of the embedding LSTM with different input modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. A t-SNE visualization of the concatenated (‚àÜ,PC) embeddings on mcf colored according to PC instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Program trace dataset statistics. M stands for million.</figDesc><table><row><cell>Dataset</cell><cell># Misses</cell><cell cols="5"># PC # Addrs # Deltas # Addrs 50% mass # Deltas 50% mass</cell></row><row><cell>gems</cell><cell>500M</cell><cell cols="2">3278 13.11M</cell><cell>2.47M</cell><cell>4.28M</cell><cell>18</cell></row><row><cell>astar</cell><cell>500M</cell><cell>211</cell><cell>0.53M</cell><cell>1.77M</cell><cell>0.06M</cell><cell>15</cell></row><row><cell>bwaves</cell><cell>491M</cell><cell cols="2">893 14.20M</cell><cell>3.67M</cell><cell>3.03M</cell><cell>2</cell></row><row><cell>lbm</cell><cell>500M</cell><cell>55</cell><cell>6.60M</cell><cell>709</cell><cell>3.06M</cell><cell>9</cell></row><row><cell>leslie3d</cell><cell>500M</cell><cell>2554</cell><cell>1.23M</cell><cell>0.03M</cell><cell>0.23M</cell><cell>15</cell></row><row><cell>libquantum</cell><cell>470M</cell><cell>46</cell><cell>0.52M</cell><cell>30</cell><cell>0.26M</cell><cell>1</cell></row><row><cell>mcf</cell><cell>500M</cell><cell cols="3">174 27.41M 30.82M</cell><cell>0.07M</cell><cell>0.09M</cell></row><row><cell>milc</cell><cell>500M</cell><cell>898</cell><cell>3.74M</cell><cell>9.68M</cell><cell>0.87M</cell><cell>46</cell></row><row><cell>omnetpp</cell><cell>449M</cell><cell>976</cell><cell>0.71M</cell><cell>5.01M</cell><cell>0.12M</cell><cell>4613</cell></row><row><cell>soplex</cell><cell>500M</cell><cell>1218</cell><cell>3.49M</cell><cell>5.27M</cell><cell>1.04M</cell><cell>10</cell></row><row><cell>sphinx</cell><cell>283M</cell><cell>693</cell><cell>0.21M</cell><cell>0.37M</cell><cell>0.03M</cell><cell>3</cell></row><row><cell>websearch</cell><cell cols="4">500M 54600 77.76M 96.41M</cell><cell>0.33M</cell><cell>5186</cell></row><row><cell cols="4">memory access space in order to reduce the vocabulary size</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and reduce the model memory footprint.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1. Embedding LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Suppose we restricted the output vocabulary size in order to</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">only model the most frequently occurring deltas. According</cell><cell></cell><cell></cell><cell></cell></row><row><cell>to Table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Cache miss addresses are from a three level simulated cache hierarchy with a 32 KB L1,</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="256" xml:id="foot_1">KB L2, and 1.25 MB Last Level Cache (LLC), similar to a single thread context from an Intel Broadwell microprocessor.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The reason for this is after de-normalization, small inaccuracies become dramatically magnified.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory hierarchy for web search</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Parthasarathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA -Industrial Session</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><surname>R√©jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Statistical learning in chip (SLIC)</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">D</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName><surname>Diana</surname></persName>
		</author>
		<author>
			<persName><surname>Marculescu</surname></persName>
		</author>
		<author>
			<persName><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><surname>Paramesh</surname></persName>
		</author>
		<author>
			<persName><surname>Jeyanandh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">E</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/ACM International Conference on Computer-Aided Design</title>
				<meeting>the IEEE/ACM International Conference on Computer-Aided Design</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generalized correlationbased hardware prefetching</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Reeves</surname></persName>
		</author>
		<idno>EE-CEG- 95-1</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>Cornell Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Synthesizing benchmarks for predictive modeling</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName><surname>Petoumenos</surname></persName>
		</author>
		<author>
			<persName><surname>Pavlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Leather</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO. IEEE</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07">Jul. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Clearing the clouds: A study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><surname>Almutaz</surname></persName>
		</author>
		<author>
			<persName><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><surname>Onur</surname></persName>
		</author>
		<author>
			<persName><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><surname>Stavros</surname></persName>
		</author>
		<author>
			<persName><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><surname>Djordje</surname></persName>
		</author>
		<author>
			<persName><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><surname>Cansu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Buffer block prefetching method</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gindele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Technical Disclosure Bulletin</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="696" to="697" />
			<date type="published" when="1977-07">July 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">T4: A highly threaded server-on-a-chip with native support for heterogeneous computing</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Golla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hot Chips 23</title>
				<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Full-system analysis and characterization of interactive smartphone applications</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">G</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><surname>Trevor</surname></persName>
		</author>
		<author>
			<persName><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Emmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Paver</surname></persName>
		</author>
		<editor>IISWC</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient execution of bursty applications</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debbie</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the naturalness of software</title>
		<author>
			<persName><forename type="first">Abram</forename><surname>Hindle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Zhendong</surname></persName>
		</author>
		<author>
			<persName><surname>Gabel</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Premkumar</forename><surname>Devanbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="837" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Abdel-Rahman</surname></persName>
		</author>
		<author>
			<persName><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Self-optimizing memory controllers: A reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Engin</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><surname>Onur</surname></persName>
		</author>
		<author>
			<persName><surname>Mart√≠nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jos√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic branch prediction with perceptrons</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim√©nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prefetching using Markov predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">Svilen</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><surname>Pablo</surname></persName>
		</author>
		<author>
			<persName><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><surname>Tipp</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Spectre attacks: Exploiting speculative execution</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><surname>Hamburg</surname></persName>
		</author>
		<author>
			<persName><surname>Mike</surname></persName>
		</author>
		<author>
			<persName><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><surname>Mangard</surname></persName>
		</author>
		<author>
			<persName><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName><surname>Prescher</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><surname>Yarom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Server engineering insights for largescale online services</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><surname>Kansal</surname></persName>
		</author>
		<author>
			<persName><surname>Aman</surname></persName>
		</author>
		<author>
			<persName><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><surname>Sriram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kushagra</forename><surname>Vaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8" to="19" />
			<date type="published" when="2010-07">July 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Polyzotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neoklis</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01208</idno>
		<title level="m">The case for learned index structures</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dead-block prediction and dead-block correlating prefetchers</title>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">-</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fide</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lipp</surname></persName>
		</author>
		<author>
			<persName><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Prescher</surname></persName>
		</author>
		<author>
			<persName><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><surname>Werner</surname></persName>
		</author>
		<author>
			<persName><surname>Mangard</surname></persName>
		</author>
		<author>
			<persName><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><surname>Genkin</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Yarom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Hamburg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-01">January 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Meltdown</note>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pin: Building customized program analysis tools with dynamic instrumentation</title>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">-</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><surname>Keung</surname></persName>
		</author>
		<author>
			<persName><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><surname>Harish</surname></persName>
		</author>
		<author>
			<persName><surname>Klauser</surname></persName>
		</author>
		<author>
			<persName><surname>Artur</surname></persName>
		</author>
		<author>
			<persName><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><surname>Geoff</surname></persName>
		</author>
		<author>
			<persName><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><surname>Steven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><surname>Janapa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Hazelwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Addressing the rare word problem in neural machine translation</title>
		<author>
			<persName><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">Nov. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured generative models of natural source code</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><surname>Karafi√°t</surname></persName>
		</author>
		<author>
			<persName><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cernock·ª≥</surname></persName>
		</author>
		<author>
			<persName><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Interspeech</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A scalable hierarchical distributed language model</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1081" to="1088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic rule extraction from long short term memory networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<title level="m">Contextual decomposition to extract interactions from LSTMs. International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><surname>Heiga</surname></persName>
		</author>
		<author>
			<persName><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><surname>Karen</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Nal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
				<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Evaluating stream buffers as a secondary cache replacement</title>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pax address space layout randomization ASLR</title>
		<ptr target="https://pax.grsecurity.net/" />
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic locality and context-based prefetching using reinforcement learning</title>
		<author>
			<persName><forename type="first">Leeor</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><surname>Shie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dependence based prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshovos</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Sohi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gurindar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Anastassia</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><surname>Babak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<ptr target="http://www.specbench.org/" />
	</analytic>
	<monogr>
		<title level="m">ISCA, 2006. SPEC CPU2006. The Standard Performance Evaluation Corporation</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Introduction to reinforcement learning</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Toward deep learning software repositories</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><surname>Vendome</surname></persName>
		</author>
		<author>
			<persName><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Linares-V√°squez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denys</forename><surname>Poshyvanyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining Software Repositories</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="334" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: Implications of the obvious</title>
		<author>
			<persName><forename type="first">Wm</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sally</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computer Architecture News</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Learning to execute. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long short term based memory hardware prefetcher</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Memory Systems</title>
				<imprint>
			<date type="published" when="2017-10">October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
