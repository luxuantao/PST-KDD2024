<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SnakeGAN: A Universal Vocoder Leveraging DDSP Prior Knowledge and Periodic Inductive Bias</title>
				<funder ref="#_YJZfPnV #_HWnHWHC">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_69H9fr4">
					<orgName type="full">Tsinghua University -Tencent Joint Laboratory</orgName>
				</funder>
				<funder ref="#_FPK7Ptg">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sipan</forename><surname>Li</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Helen</forename><surname>Meng</surname></persName>
							<email>hmmeng@se.cuhk.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">nd Songxiang Liu ? AI</orgName>
								<orgName type="laboratory">Lab Tencent Inc Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">rd Luwen Zhang Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">th Xiang Li Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">th Yanyao Bian AI Lab Tencent Inc Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">th Chao Weng AI Lab Tencent Inc Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">Department of Systems Engineering and Engineering Management</orgName>
								<orgName type="institution">The Chinese University of Hong Kong Hong Kong SAR</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SnakeGAN: A Universal Vocoder Leveraging DDSP Prior Knowledge and Periodic Inductive Bias</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>universal vocoder</term>
					<term>differentiable digital signal processing</term>
					<term>audio generation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial network (GAN)-based neural vocoders have been widely used in audio synthesis tasks due to their high generation quality, efficient inference, and small computation footprint. However, it is still challenging to train a universal vocoder which can generalize well to out-of-domain (OOD) scenarios, such as unseen speaking styles, non-speech vocalization, singing, and musical pieces. In this work, we propose SnakeGAN, a GAN-based universal vocoder, which can synthesize high-fidelity audio in various OOD scenarios. SnakeGAN takes a coarse-grained signal generated by a differentiable digital signal processing (DDSP) model as prior knowledge, aiming at recovering high-fidelity waveform from a Mel-spectrogram. We introduce periodic nonlinearities through the Snake activation function and anti-aliased representation into the generator, which further brings desired inductive bias for audio synthesis and significantly improves the extrapolation capacity for universal vocoding in unseen scenarios. To validate the effectiveness of our proposed method, we train SnakeGAN with only speech data and evaluate its performance for various OOD distributions with both subjective and objective metrics. Experimental results show that SnakeGAN significantly outperforms the compared approaches and can generate high-fidelity audio samples including unseen speakers with unseen styles, singing voices, instrumental pieces, and nonverbal vocalization.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Neural vocoders <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b4">[5]</ref> have drawn much attention as they generate the final waveform from acoustic information * Work done during the internship at Tencent AI Lab ? Corresponding authors in many applications like Text-to-Speech (TTS) <ref type="bibr" target="#b20">[21]</ref>, singing voice synthesis <ref type="bibr" target="#b1">[2]</ref>, voice conversion <ref type="bibr" target="#b9">[10]</ref>, etc. Most highfidelity neural vocoders are based on the generative adversarial network (GAN) and have shown their advantages in generating raw waveform conditioned on Mel-spectrogram with fast inference speed and lightweight networks <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b0">[1]</ref>. Existing works on GAN-based neural vocoders mainly focus on improving the discriminator architecture <ref type="bibr" target="#b25">[26]</ref> or incorporating auxiliary training losses into the adversarial training. MelGAN <ref type="bibr" target="#b15">[16]</ref> first realizes a competitive GAN network vocoder by introducing a multi-scale discriminator (MSD) that downsamples the raw waveform at multiple scales through average pooling, leading to a loss of high-frequency information. Parallel WaveGAN <ref type="bibr" target="#b24">[25]</ref> improves the training loss by extending the short-time Fourier transform (STFT) loss to be multi-resolution. Multi-period discriminator (MPD) and multi-receptive field fusion (MRF) are proposed by HiFi-GAN <ref type="bibr" target="#b13">[14]</ref>, which achieves high-fidelity performance.</p><p>In real applications, however, neural vocoders typically suffer from heavy quality degradation when directly applied to unseen data. It is of significant meaning to achieve the flexible generation of high-quality audio under various scenarios without any fine-tuning. Therefore, the universal vocoders aim to improve the ability to model the robust mapping between the condition and the target (e.g. Mel-spectrogram and waveform), especially on the out-of-domain (OOD) inference data.</p><p>Recent works on universal vocoders like Universal MelGAN <ref type="bibr" target="#b7">[8]</ref> and UnivNet <ref type="bibr" target="#b8">[9]</ref>  For the purpose of further enhancing the effectiveness and robustness of the generator, we propose a universal neural vocoder named SnakeGAN. SnakeGAN improves the waveform generator by introducing both the DDSP-based prior knowledge of waveform composition, and the periodic nonlinearities through incorporating the Snake activation function <ref type="bibr" target="#b26">[27]</ref>. Specifically, the Snake generator first obtains a coarsegrained DDSP-generated signal waveform and downsamples it for N times by Discrete Wavelet Transform (DWT) <ref type="bibr" target="#b12">[13]</ref>, which can keep the high-frequency component. Then, each of the downsampled signals is added to the corresponding upsample block, which is composed of the transposed convolutional block, followed by the anti-aliased multi-periodicity composition module with Snake activation. On the one hand, the snake activation function achieves the desired periodic inductive bias to learn a periodic function while maintaining a favorable optimization property of the ReLU-based activations. On the other hand, coupling the characteristics of the time-domain periodic and aperiodic components prior provided by DDSP strengthens the generator's robustness under unseen scenarios.</p><p>We choose speech, singing voice, instrumental pieces, and nonverbal vocalization as the target scenarios in which vocoders are mainly used. In our experiments, the proposed model generates high-quality audio in various scenarios, outperforming the state-of-the-art DDSP-based vocoder and the mainstream HiFi-GAN vocoder. The audio synthesized by the proposed universal SnakeGAN vocoder and other models is available at our demo page * .</p><p>In general, the contributions of this paper are three-fold:</p><p>? We demonstrate by experiments that DDSP-based vocoders have better robustness when given a small amount of data. ? We introduce the state-of-the-art generator and discriminator with the periodic inductive Snake activation function, which can highly eliminate aliasing artifacts and improves audio quality. ? We propose a novel and effective GAN vocoder, which can generalize well to universal scenarios by conditioned on DDSP prior even with a large amount of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WOEK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries of typical GAN vocoder</head><p>GAN-based vocoder generates waveform normally by a few transposed convolution upsampling network layers which also contain a stack of residual blocks with dilated convolutions. Typically, multiple discriminators are adopted for adversarial training to learn different frequency domain features of audio.</p><p>1) Generator: Specifically, to address the problem of generalization ability, BigVGAN <ref type="bibr" target="#b16">[17]</ref> proposes the anti-aliased multi-periodicity composition (AMP) block with Snake activation function <ref type="bibr" target="#b26">[27]</ref>. The BigVGAN's generator with AMP block is similar to the structure of StyleGAN3 <ref type="bibr" target="#b11">[12]</ref>, which has shown satisfying generalization ability in the image generation domain.</p><p>Meanwhile, the Snake activation function, defined as f (x) = x + sin 2 (x), is demonstrated in <ref type="bibr" target="#b26">[27]</ref> that can bring periodic inductive bias and can perform well for temperature and financial data prediction. Considering the audio waveform is known to exhibit high periodicity and can be represented as a composition of primitive periodic components, BigVGAN suggests that we can provide the desired inductive bias to the generator architecture based on Snake.</p><p>In addition, StyleGAN3 <ref type="bibr" target="#b11">[12]</ref> identifies that the aliasing artifacts in image synthesis are rooted in careless signal processing. StyleGAN3 applies the nonlinearity to the temporarily increased resolution (e.g. 2?) that approximates the continuous representation inspired by the Nyquist-Shannon sampling theorem. The continuous representation of nonlinearity ensures translation equivariance in the feature space, and the nonlinearity generates novel frequencies in the continuous domain, thereby eliminating the aliasing.</p><p>2) Discriminator: The state-of-the-art GAN vocoders usually comprise several types of discriminators to guide the generator to synthesize coherent waveform while minimizing perceptual artifacts which are easily detectable. We apply the Fre-GAN's setting of discriminators, including MPD and MSD, both with DWT instead of average pooling. Noteworthy, the average pooling ignores the sampling theorem, and high-frequency contents are aliased and become invalid, while DWT is an efficient but effective way of downsampling non-stationary signals into several frequency sub-bands and can preserve high-frequency components better. A few recent works propose to apply the discriminator on the time-frequency domain using the multi-resolution discriminator (MRD). MRD is also composed of several subdiscriminators that operate on multiple 2-D linear spectrograms with different STFT resolutions. We also apply MRD to improve the quality by sharpening the signal in the spectral domain with reduced pitch and periodicity artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of DDSP</head><p>The DDSP <ref type="bibr" target="#b3">[4]</ref> model ? has shown the ability to decouple and further control the characters of a time domain waveform. It can flexibly adjust the amplitude, envelope, and fundamental frequency of audio respectively, and then decode these characters into the harmonic structure and filtered noise, which can precisely meet our goal to simulate the prior knowledge of target audio from different domains.</p><p>According to HNM, audio signal s(t) can be represented as the sum of the harmonic s h (t) and noise components s n (t):</p><formula xml:id="formula_0">s(t) = s h (t) + s n (t).<label>(1)</label></formula><p>For the voiced part, the signal can be approximated by superimposing a series of harmonic components whose pitches are the integer multiples of the fundamental frequency:</p><formula xml:id="formula_1">s h (t) = L(t) k=-L(t) A k (t)e jk?0(t)t ,<label>(2)</label></formula><p>in which L(t) denotes the number of harmonic. A k (t) denotes the amplitude and ? 0 (t) denotes the fundamental frequency. ? https://github.com/acids-ircam/ddsp pytorch And for the unvoiced part, the signal can be directly represented by random noise based on the time-varying autoregressive (AR) model h(?, t):</p><formula xml:id="formula_2">s n (t) = e(t)[h(?, t) * b(t)],<label>(3)</label></formula><p>where e(t) denotes the spectral envelope of noise signal and b(t) denotes white noise signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>This section is organized as follows: Section III-A will introduce the overall pipeline of the proposed model architecture. Section III-B introduces the DDSP model and proposes a novel method that uses the Snake-based upsampling blocks to introduce periodic inductive bias and time-varying harmonicplus-noise prior knowledge, making the generator perform better in extrapolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overall pipeline</head><p>The pipeline of our proposed model architecture for the universal vocoder is represented in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>It consists of two main stages. To begin with, based on the prior knowledge from different target audio domains, including speech, singing voice, instrumental pieces, and nonverbal vocalization, we model the distributions of acoustic features corresponding to fundamental frequency f 0 , harmonic distribution D, harmonic amplitude A, and time-varying filtered noise through DDSP, a typical Harmonic-plus-Noise Model (HNM).</p><p>Next, we propose two versions of the SnakeGAN generator, the SnakeGANv1 is the dotted line while the SnakeGANv2 is the solid red line, as is shown in Figure1.</p><p>Lastly, we refer to Fre-GAN <ref type="bibr" target="#b12">[13]</ref> and UnivNet <ref type="bibr" target="#b8">[9]</ref>, MPD with DWT and MRD discriminators are adopted to improve the synthesized audio quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Introducing DDSP prior into black-box GAN</head><p>The DDSP module in our work generates a coarse-grained signal in the time domain. We note that the DDSP signal is generated combining prior knowledge from both harmonic oscillator and filtered noise, and the black-box mode GAN generator is lack of such guidance in the time domain. The natural way to think about it would be how to introduce the prior into the generator to make it more robust. The SnakeGANv1 simply adds the DDSP signal to the synthesized audio, it is a simple but effective way. Additionally, we present SnakeGANv2. The SnakeGANv2 generator aims to couple the time-domain signal of DDSP with the GAN generator more effectively and combines with the Snake activation function. we downsample the DDSP signal N times by DWT, as the up sample multiple of the generator is [8, 8, 2, 2], thus the DWT down sample multiple is <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b7">8]</ref>, correspondingly. At last, we add the down-sampled signals to each upsampling block respectively as time-domain supervision to guide the Snake generator learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>To validate the effectiveness of our proposed method, we train SnakeGAN with only speech data and evaluate its performance with both subjective and objective metrics for various OOD distributions, including singing voice, instrumental pieces, and nonverbal vocalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Corpus and data configuration</head><p>The audio sample rate is 24KHz and the 80-band log Melspectrogram is extracted with a 1024-point FFT, 256 sample frameshift, and 1024 sample frame length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Training set:</head><p>We use an internal gender-balanced multispeaker speech corpus for training. The dataset contains 291 speakers and has duration of 278 hours in total. Most sentences are in Mandarin Chinese and the remaining sentences are in English or Chinese-English code-switched.</p><p>2) Testing set: We consider the following OOD scenarios in the test set:</p><p>? Unseen speakers with OOD-expressive styles</p><p>The unseen speakers with OOD-expressive styles data contain 1024 utterances and 8 speakers, and every utterance is highly expressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Singing voice</head><p>We evaluated our method on singing voice clips extracted from the Mandarin singing corpus dataset Opencpop <ref type="bibr" target="#b23">[24]</ref>, which usually includes some skills such as trill, long tone, and leaning tone, which usually do not exist in speech.</p><p>? Instrumental pieces Audio clips were extracted from the single instrument musical pieces of URMP dataset <ref type="bibr" target="#b17">[18]</ref>. The URMP dataset is made of 44 simple multi-instrument music works, which are composed of performances recorded separately by a single track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Nonverbal vocalization</head><p>We extracted audio clips from the Nonverbal Vocalization dataset <ref type="bibr" target="#b2">[3]</ref>, which is a human nonverbal vocal sound dataset containing crying, laughing, Pitch features of each dataset are extracted from the groundtruth audio by praat-parselmouth <ref type="bibr" target="#b6">[7]</ref>. The pitch range after zscore normalization as well as the mean and standard deviation are shown in Table <ref type="table" target="#tab_2">III</ref>. Much differences can be observed among different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Investigation of the effectiveness of DDSP structure</head><p>In this section, we refer to DDSP primarily concerning the additive oscillator and the model's ability to learn timevarying amplitude envelopes. The generalization ability of the DDSP structure is investigated by implementing a DDSPbased vocoder HooliGAN <ref type="bibr" target="#b18">[19]</ref>, to validate the robustness of the DDSP architecture.</p><p>We train a modified HooliGAN on LJSpeech dataset <ref type="bibr" target="#b5">[6]</ref>, as the official open-source HiFi-GAN ? is trained on LJSpeech. Hereafter, to verify the robustness of DDSP structure, we compare the HooliGAN and HiFi-GAN with OOD scenarios, ? https://github.com/jik876/hifi-gan   including unseen speakers and musical pieces. We conduct the ABX test to demonstrate the effectiveness of the DDSP structure, as it reasonably states the sound-generating mechanism. The results are shown in Fig. <ref type="figure" target="#fig_2">2</ref>. 64.58% participants selected HooliGAN on musical pieces and 22.92% for HiFi-GAN. For speech utterances, 39.84% participants selected HiFi-GAN while 39.84% selected NP, and 20.32% selected HooliGAN. The results show that when feed with fewer data, DDSP-based HooliGAN can be more robust to some unseen scenarios, but HiFi-GAN is better when faced with speech.</p><p>It should be noted that DDSP-based vocoders are usually small and have fewer parameters. Although they can perform well with only a small amount of data and are easy to train, it is still challenging and may lead to inferior audio quality with massive data. Experiments show that when using the 278hour training set (Section 3.1.1), the generalization ability of HiFi-GAN exceeds that of HooliGAN. Therefore, we decide not to take DDSP as generator directly. Instead, we introduce the DDSP coarse-grained signal as time-domain supervision to strengthen the state-of-the-art neural generator. C. Experimental results of the candidate vocoders 1) Speech: We evaluated vocoders on several dimensions, including a subjective metric, mean opinion score (MOS) of audio quality (Table <ref type="table" target="#tab_0">I</ref>), and two objective metrics, perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b21">[22]</ref> and short-term objective intelligibility (STOI) <ref type="bibr" target="#b22">[23]</ref> (Table <ref type="table" target="#tab_1">II</ref>). For each evaluation, we selected ten clips for the MOS test and three hundred clips for the PESQ and STOI tests, and also computed the Multi-Resolution STFT Loss. A total of twenty people participated in the MOS test.</p><p>For speech with OOD-expressive styles, the proposed SnakeGANv1 and SnakeGANv2 vocoders achieved the MOS score of 4.37 and 4.39, PESQ score of 3.289 and 3.264, STOI score of 0.972, and MR-STFT Loss 0.987 and 0.985, respectively.</p><p>Overall, experimental results on speeches proved the effectiveness of the proposed approach to introduce the timedomain DDSP signals as prior knowledge guidance and the effectiveness of the Snake activation function to strengthen the ability of generalization.</p><p>2) Singing voice: Since there is a big difference between the singing voice and speech, the vocoder trained on speech may degrade when facing the singing voice during inference.</p><p>The results show that the proposed SnakeGANv2 achieved superior performance, with a 3.70 MOS score and 1.270 MR-STFT Loss.</p><p>3) Instrumental pieces &amp; nonverbal vocalization: Similar to the singing voice, instrumental pieces and nonverbal vocalization's distribution are various from speech, and without semantic information. Thus, we only refer to MR-STFT Loss as the metric, which is shown in TableIV. The proposed SnakeGANv2 performs best of all models, which achieved 1.214 MR-STFT Loss, 3.34 MOS in instrumental pieces, and 1.242 MR-STFT Loss, 3.89 MOS in nonverbal vocalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, to improve the robustness of universal neural vocoding across diverse scenarios, and especially outof-domain data, we present two versions of SnakeGAN. Specifically, we model the distributions of acoustic features under prior audio knowledge from multiple target scenarios through a DDSP module, the prior knowledge is then used as time-domain supervision to guide the GAN generator. The generalization of periodic components is explicitly modeled through the Snake activation function. In conclusion, a robust Snake generator and discriminator are applied in this work. Experimental results show that the proposed vocoder trained with a 278-hour speech corpus can be employed well and has achieved superior performance in many diverse scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Schematic diagram of SnakeGAN generator. The generator is composed of multiple transposed-convolution-based upsampling blocks, where hidden features are enhanced by anti-aliased multi-periodicity composition modules. It applies the Snake activation function for periodic inductive bias and filtered nonlinearities for anti-aliasing purposes. The DDSP oscillator generates the coarse-grained signal as time-domain prior and then applied to the generator block after N times DWT downsample at each block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Results of ABX tests comparing DDSP-HooliGAN and HiFi-GAN on speech and musical pieces respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Subjective evaluation results (MOS values). "SnakeGANv1" denotes the method that simply adds the DDSP signal to the generator. "SnakeGANv2" denotes the approach that couples the signals after DWT down-sampled with each upsample block. "CI" denotes the confidence interval. ? denotes the proposed vocoder.</figDesc><table><row><cell></cell><cell cols="2">unseen styles</cell><cell cols="2">singing</cell><cell cols="2">instrumental</cell><cell cols="2">nonverbal</cell></row><row><cell>Model</cell><cell cols="2">(OOD-expressive)</cell><cell></cell><cell>voices</cell><cell cols="2">pieces</cell><cell cols="2">vocalization</cell></row><row><cell></cell><cell>MOS?</cell><cell cols="4">95% CI MOS? 95% CI MOS?</cell><cell cols="3">95% CI MOS? 95% CI</cell></row><row><cell>Ground Truth</cell><cell>4.64</cell><cell>? 0.07</cell><cell>4.86</cell><cell>? 0.05</cell><cell>4.76</cell><cell>? 0.08</cell><cell>4.42</cell><cell>? 0.10</cell></row><row><cell>HiFi-GAN (V1)</cell><cell>4.12</cell><cell>? 0.09</cell><cell>3.52</cell><cell>? 0.09</cell><cell>3.18</cell><cell>? 0.10</cell><cell>3.66</cell><cell>? 0.12</cell></row><row><cell>HooliGAN</cell><cell>4.07</cell><cell>? 0.08</cell><cell>3.36</cell><cell>? 0.10</cell><cell>2.97</cell><cell>? 0.10</cell><cell>3.40</cell><cell>? 0.11</cell></row><row><cell>SnakeGANv1</cell><cell>4.37</cell><cell>? 0.08</cell><cell>3.44</cell><cell>? 0.09</cell><cell>3.16</cell><cell>? 0.10</cell><cell>3.78</cell><cell>? 0.12</cell></row><row><cell>SnakeGANv2  ?</cell><cell>4.39</cell><cell>? 0.08</cell><cell>3.70</cell><cell>? 0.09</cell><cell>3.34</cell><cell>? 0.10</cell><cell>3.89</cell><cell>? 0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II :</head><label>II</label><figDesc>Objective evaluation results (PESQ, STOI, and MR-STFT Loss values) of unseen styles and singing voices.</figDesc><table><row><cell>Metric</cell><cell>Model</cell><cell>unseen styles (OOD-expressive)</cell><cell>singing voices</cell></row><row><cell></cell><cell>HiFi-GAN (V1)</cell><cell>3.059</cell><cell>2.785</cell></row><row><cell>PESQ?</cell><cell>HooliGAN SnakeGANv1</cell><cell>2.916 3.289</cell><cell>2.594 2.848</cell></row><row><cell></cell><cell>SnakeGANv2  ?</cell><cell>3.264</cell><cell>2.642</cell></row><row><cell></cell><cell>HiFi-GAN (V1)</cell><cell>0.968</cell><cell>0.845</cell></row><row><cell>STOI?</cell><cell>HooliGAN SnakeGANv1</cell><cell>0.954 0.972</cell><cell>0.816 0.823</cell></row><row><cell></cell><cell>SnakeGANv2  ?</cell><cell>0.972</cell><cell>0.823</cell></row><row><cell></cell><cell>HiFi-GAN (V1)</cell><cell>1.020</cell><cell>1.329</cell></row><row><cell>MR-STFT Loss?</cell><cell>HooliGAN SnakeGANv1</cell><cell>1.074 0.987</cell><cell>1.344 1.311</cell></row><row><cell></cell><cell>SnakeGANv2  ?</cell><cell>0.985</cell><cell>1.270</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III :</head><label>III</label><figDesc>Pitch distribution of each dataset.</figDesc><table><row><cell>Dataset</cell><cell>#Utterance</cell><cell>Min</cell><cell>Max</cell><cell>Mean</cell><cell>Std</cell></row><row><cell>Training set</cell><cell>218k</cell><cell>-0.947</cell><cell>6.258</cell><cell>123.992</cell><cell>130.806</cell></row><row><cell>Test OOD styles</cell><cell>1024</cell><cell>-0.857</cell><cell>7.477</cell><cell>95.864</cell><cell>111.842</cell></row><row><cell>Singing</cell><cell>300</cell><cell>-1.554</cell><cell cols="3">2.631 277.634 178.607</cell></row><row><cell>Instrumental</cell><cell>300</cell><cell>-1.286</cell><cell>2.625</cell><cell>287.254</cell><cell>223.335</cell></row><row><cell>Nonverbal</cell><cell>300</cell><cell>-0.891</cell><cell>3.391</cell><cell cols="2">175.025 196.476</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>MR-STFT Loss values of instrumental pieces &amp; nonverbal vocalization.</figDesc><table><row><cell>Metric</cell><cell>Model</cell><cell>instrumental pieces</cell><cell>nonverbal vocalization</cell></row><row><cell></cell><cell>HiFi-GAN (v1)</cell><cell>1.224</cell><cell>1.326</cell></row><row><cell>MR-STFT Loss?</cell><cell>HooliGAN SnakeGANv1</cell><cell>1.287 1.225</cell><cell>1.425 1.250</cell></row><row><cell></cell><cell>SnakeGANv2  ?</cell><cell>1.214</cell><cell>1.242</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>* Demo page: https://github.com/thuhcsi/SnakeGAN/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62076144</rs>), <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">WDZC20220816140515001</rs>, <rs type="grantNumber">JCYJ20220818101014030</rs>), <rs type="programName">Tencent AI Lab Rhino-Bird Focused Research Program</rs> (<rs type="grantNumber">RBFR2022005</rs>) and <rs type="funder">Tsinghua University -Tencent Joint Laboratory</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_FPK7Ptg">
					<idno type="grant-number">62076144</idno>
				</org>
				<org type="funding" xml:id="_YJZfPnV">
					<idno type="grant-number">WDZC20220816140515001</idno>
				</org>
				<org type="funding" xml:id="_HWnHWHC">
					<idno type="grant-number">JCYJ20220818101014030</idno>
					<orgName type="program" subtype="full">Tencent AI Lab Rhino-Bird Focused Research Program</orgName>
				</org>
				<org type="funding" xml:id="_69H9fr4">
					<idno type="grant-number">RBFR2022005</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Avocodo: Generative adversarial network for artifact-free vocoder</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Joo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.13404</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Hifisinger: Towards high-fidelity neural singing voice synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01776</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond words: Using nonverbal communication data in research to enhance thick description and interpretation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Denham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Onwuegbuzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Qualitative Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="670" to="696" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ddsp: Differentiable digital signal processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Hantrakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1x1ma4tDr" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fastdiff: A fast conditional diffusion model for high-quality speech synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.09934</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introducing parselmouth: A python interface to praat</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jadoul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Boer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Phonetics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Universal melgan: A robust neural vocoder for high-fidelity waveform generation in multiple domains</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09631</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Univnet: A neural vocoder with multi-resolution spectrogram discriminators for highfidelity waveform generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07889</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Maskcycleganvc: Learning non-parallel voice conversion with filling in frames</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5919" to="5923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Istftnet: Fast and lightweight mel-spectrogram vocoder incorporating inverse short-time fourier transform</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6207" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fre-gan: Adversarial frequency-consistent audio synthesis</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09761</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Melgan: Generative adversarial networks for conditional waveform synthesis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>De Boissiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gestin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Z</forename><surname>Teoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>De Br?bisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bigvgan: A universal neural vocoder with large-scale training</title>
		<author>
			<persName><forename type="first">W</forename><surname>S.-G. Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><surname>Yoon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04658</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Creating a multitrack classical music performance dataset for multimodal music analysis: Challenges, insights, and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2018.2856090</idno>
		<ptr target="https://doi.org/10.1109/TMM.2018.2856090" />
	</analytic>
	<monogr>
		<title level="j">Trans. Multi</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="522" to="535" />
			<date type="published" when="2019-02">feb 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hooligan: Robust, high quality neural vocoding</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02493</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Differentiable world synthesizer-based neural vocoder with application to end-to-end audio style transfer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nercessian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.07282</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Portaspeech: Portable and high-quality generative text-to-speech</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Opencpop: A high-quality open source chinese popular song corpus for singing voice synthesis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07429</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6199" to="6203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gan vocoder: Multi-resolution discriminator is all you need</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.05236</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural networks fail to learn periodic functions and how to fix it</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ziyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hartwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ueda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1583" to="1594" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
