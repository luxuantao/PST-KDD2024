<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_EV63gCx">
					<orgName type="full">GenPro Consortium</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shlomo</forename><surname>Greenberg</surname></persName>
							<email>shlomog@bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yossi</forename><surname>Eni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yehuda</forename><surname>Ben-Shimol</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
								<address>
									<postCode>8410501</postCode>
									<settlement>Beer-Sheva</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TCSI.2021.3117490</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficient Hint-Based Event (EHE) Issue Scheduling for Hardware Multithreaded RISC-V Pipeline Yossi Eni, Shlomo Greenberg , Member, IEEE, and Yehuda Ben-Shimol , Member, IEEE Abstract-Hardware multithreading is a common approach for tolerating memory latency by utilizing idle cycles and avoiding CPU stalling. Nowadays, multithreading architectures are commonly used across many processors and various embedded edge devices to improve performance. This work suggests a new multithreading in-order pipeline microarchitecture design for RISC-V and proposes an efficient event-based issue scheduling algorithm. The proposed scheduling algorithm is based on the unique RISC-V ISA that enables decoding of the instruction type in an early stage of the pipeline. The RISC-V-based multithreading architecture is evaluated using a dedicated software simulator. Simulation results show that the proposed algorithm outperforms the classical Round Robin and the coarse grain algorithms. The proposed architecture is evaluated using the standard MiBench benchmark and other common applications, demonstrating pipeline utilization improvement of up to about 26% in terms of IPC using four threads.</p><p>Index Terms-Hardware multithreading, issue scheduling, RISC-V, in-order pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H ARDWARE multithreading (MT) is a common and efficient strategy used to exploit Instruction Level Parallelism (ILP) and better utilization of the single-thread pipeline in case of stalls and instructions dependencies. Multithreading allows multiple threads to share the functional units of a single processor in an overlapping fashion by duplicating the independent state of each thread (using a separate register file and different PC for each thread). Multithreading architectures are commonly used across many industrial processors and various embedded edge devices in seeking to improve performance <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. Hardware multithreading can significantly increase processor throughput compared to single-threaded cores by hiding idle times such as memory latencies or branch penalties. Gomes et al. <ref type="bibr" target="#b3">[4]</ref> propose to unify RTOS scheduling and hardware-based thread scheduling to cope with real-time applications. Fettes et al. <ref type="bibr" target="#b4">[5]</ref> propose to use reinforcement learning (RL) to learn relatively complex data access patterns to improve on hardware-level thread migration techniques.</p><p>Multithreading is supported in various processor architectures. Ungerer et al. <ref type="bibr" target="#b5">[6]</ref> present a survey of explicit multithreading processors that apply processes or operating system threads in their hardware thread slots. These processors optimize the throughput of multi-programming workloads rather than single-thread performance. The IBM Power9 processor <ref type="bibr" target="#b6">[7]</ref> has an enhanced core architecture optimizing for emerging workloads with superior thread performance. It supports up to eight hardware threads built on the same basic building block of the Power9 core microarchitecture. The IBM Z13 introduces high instruction execution parallelism using a Simultaneous Multithreading (SMT) architecture that supports concurrent execution of two threads <ref type="bibr" target="#b7">[8]</ref>. Das et al. <ref type="bibr" target="#b8">[9]</ref> exploit available Network on Chip (NoC) resources to reduce latency experienced by multithreaded applications running in multicore systems.</p><p>Graphics processing units (GPUs) have become one of the best platforms for exploiting the plentiful thread-level parallelism of applications <ref type="bibr" target="#b9">[10]</ref>. GPUs rely on massive multithreading and fast context switching to overcome the overhead of long latencies of pipeline or memory accesses. Do et al. <ref type="bibr" target="#b9">[10]</ref> propose a long-latency operation-based warp scheduler to improve GPU performance and suggest partitioning warps into different pools based on the characteristics of instructions that are subsequently executed. Lin et al. <ref type="bibr" target="#b10">[11]</ref> present a concurrent placement framework exploiting multithreading and GPU acceleration.</p><p>Different scheduling mechanisms are used to decide which thread should fetch the next instruction (using fetch scheduling algorithms) and select which thread is assigned to execute the next instruction (using issue scheduling algorithms). Various types of issue scheduling algorithms have been developed to ensure stall-free pipeline operation and efficient resource utilization improving the pipeline throughput by hiding CPU stalls. Ku et al. <ref type="bibr" target="#b11">[12]</ref> propose a fast data switching mechanism to support multiple multithreading DSP using tight-coupled data switches to provide closely collaborative load sharing among threads.</p><p>Among the main traditional scheduling approaches are the Fine-grain, Coarse-grain, and SMT <ref type="bibr" target="#b12">[13]</ref>. The Fine-grain, also called Interleaving Multithreading (IMT) <ref type="bibr" target="#b13">[14]</ref>, is a multithreading scheduling technique that issues an instruction from a different hardware thread at every clock cycle using a Round Robin scheduler <ref type="bibr" target="#b14">[15]</ref>. Coarse-grain scheduling suggests adapting a switch-on-event technique forcing switching to the next thread in case of a high latency operation such as a cache miss <ref type="bibr" target="#b15">[16]</ref>.</p><p>Interleaved multithreaded architectures have proven to be an advantageous solution to maximize pipeline utilization when it comes to executing parallel applications, as different threads operate different instruction processing phases in the same cycle. Cheikh et al. <ref type="bibr" target="#b16">[17]</ref> expand the target applications of IMT microarchitecture to support mathematical and convolutions operations efficiently. Zagan and Gaitan <ref type="bibr" target="#b17">[18]</ref> suggest improving CPU performance using fine-grain multithreading architecture empowered with a hardware scheduler that uses remapping techniques for the program counter, the register file, and the pipeline registers. Benchara et al. <ref type="bibr" target="#b13">[14]</ref> present an in order five-stage pipeline interleaved multithreaded processor with two hardware thread contexts for embedded systems. The interleaved multithreading core has an augmentation of 73.2% in the core area, while the average performance gain is 17% compared to the single-threaded core <ref type="bibr" target="#b13">[14]</ref>.</p><p>Switch on event multithreading (also known as coarsegrained MT and block MT) processors run multiple threads on a pipeline machine, while the pipeline switches threads on stall events (e.g., a cache miss). The thread switch penalty is determined by the number of stages in the pipeline that are flushed off. Kvatinsky et al. <ref type="bibr" target="#b18">[19]</ref> present continuous flow multithreading using a dedicated multi-state pipeline register to hold the state of multiple threads within the execution pipeline stages, where only one thread is active at a time.</p><p>The SMT approach allows simultaneous issuing of multiple instructions from multiple threads on each cycle <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[20]</ref>. SMT requires complex architectures involving dynamic scheduling hardware. Several models of SMT techniques have been presented <ref type="bibr" target="#b19">[20]</ref>. Results show that SMT has the potential to achieve four times the throughput of superscalar and double that of fine-grain multithreading. However, while SMT increases processor utilization, it adds substantial complexity to the design <ref type="bibr" target="#b19">[20]</ref>.</p><p>This work suggests a new microarchitecture design for RISC-V multithreading in-order pipeline. We also propose an efficient new issue scheduling algorithm that improves pipeline utilization. The RISC-V-based multithreading architecture was developed using a dedicated software simulator. The proposed scheduling algorithm is based on the specific RISC-V ISA that enables decoding of the instruction type in an early stage of the pipeline. Simulation results show that the proposed algorithms outperform the classical Round Robin and the coarse grain algorithms.</p><p>The proposed architecture is evaluated using the common MiBench benchmark a selection of common benchmark applications such as MiBench <ref type="bibr" target="#b20">[21]</ref>, EEMBC's CoreMark <ref type="bibr" target="#b21">[22]</ref>, and a DSP oriented Turbo Encoder (TE) legacy application <ref type="bibr" target="#b22">[23]</ref>. Simulation results demonstrate pipeline utilization improvement of up to about 26% in terms of IPC using four threads. The rest of the paper is organized as follows. Section II presents the related work. Section III describes the proposed RISC-V hardware multithreaded architecture.</p><p>Section IV introduces the proposed issue scheduling algorithm and its implementation. Section V demonstrates experiments and results, and Section VI summarizes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The RISC-V open-source instruction set architecture (ISA), developed at Berkeley University, paved the way for computer architects to design innovative, capable cores to execute complex instruction extensions <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. The RISC-V ISA is becoming an increasingly popular ecosystem for both hardware and software development. The RISC-V ISA allows adding nonstandard application-specific instructions, i.e., custom ISA extension, to improve performance and achieve flexibility for extensible processors. Over the last few years, the RISC-V ISA has emerged as a completely free, open-source, widely supported, and suitable hardware implementation <ref type="bibr" target="#b25">[26]</ref>. Asanovic et al. <ref type="bibr" target="#b26">[27]</ref> present the opensource Berkeley Rocket SoC design generator used to generate general-purpose synthesizable, parameterized processor cores that use the open RISC-V ISA and provides both an in-order (Rocket) and an out-of-order core generator (BOOM) <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Various multicore platforms with clusters of RISC cores have been proposed as an alternative model for real-time applications that consume a lot of computing power <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b30">[31]</ref>. Conti et al. <ref type="bibr" target="#b28">[29]</ref> present a heterogeneous multi-core platform equipped with four OpenRISC cores that can execute generalpurpose applications and are optimized for signal processing algorithms. Schiavone et al. <ref type="bibr" target="#b29">[30]</ref> introduce two low-power RISC-V cores targeting IoT applications, and control-oriented tasks. Conti et al. <ref type="bibr" target="#b30">[31]</ref> propose the PULP (Parallel processing Ultra-Low Power platform), an architecture built on clusters of tightly-coupled RISC ISA cores, with advanced techniques for fast performance and energy scalability.</p><p>Recently various hardware multithreaded RISC-V-based architecture have been proposed supporting multithreaded execution in a single core, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b33">[34]</ref>. Collange <ref type="bibr" target="#b31">[32]</ref> present the Simty multithreaded RISC-V-based architecture implementing a SIMT execution model purely at the microarchitecture level. Olivieri et al. <ref type="bibr" target="#b34">[35]</ref> explore microarchitecture design solutions targeting power efficiency for multithreaded RISC-V soft processor core implementations on FPGA. They suggest the usage of interleaved multithreading based on fetching a new instruction from a different thread at each clock cycle. Cheikh et al. <ref type="bibr" target="#b32">[33]</ref> present a microarchitecture of a multithreaded RISC-V compliant processing core family for IoT, implementing interleaved multithreading. Patsidis et al. <ref type="bibr" target="#b25">[26]</ref> demonstrates a synthesizable in-order RISC-V dual-issue core of the compressed ISA, eliminating unnecessary stalls due to dependencies. Pu?ka and Milik <ref type="bibr" target="#b35">[36]</ref> introduces a multithread RISC architecture based on programmable interleaved pipelining. They adapt the thread interleaving approach, but the duration of the presence of a given thread within the pipe and the frequency of its appearance change.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RISC-V MULTITHREADED PIPELINE</head><p>This section describes the hardware implementation of the in-order RISC-V ISA and the required extensions to support single-issue multithreading. The RI5CY processor core, a 4-stage in-order 32-bit integer RISC-V, has been selected as a single thread reference model <ref type="bibr" target="#b29">[30]</ref>. The reference RI5CY pipeline (which is presented in the following subsections) was extended to support multithreading and implementing the 64-bit Integer RISC-V M-mode instruction set. The proposed microarchitecture supports up to 8 threads and therefore utilizes the pipeline by hiding the memory latency and avoiding pipeline hazards and instruction dependencies. While the majority of processor logic is kept the same (such as the instruction decoder and the execution unit), the necessary register file and the fetch mechanisms, as well as the PC, are replicated for each thread. The in-order approach is used for its simplicity, avoiding a complex mechanism. The following sections describe the RISC-V instruction set, the RISC-V inorder single thread pipeline, and the proposed multithread extension support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RISC-V ISA</head><p>This section describes in detail the RISC-V ISA and its unique specifications. The RISC-V ISA supports both 32-bit and 64-bit register files. The ISA user specification <ref type="bibr" target="#b23">[24]</ref> defines various RISC-V ISA extensions such as: (a) I-Integer, (b) M-Multiplication and Division, (c) A-Atomic Memory Operations, and (d) F-D-Q supporting Single-Double and Quad precision Floating Point. Table <ref type="table" target="#tab_0">I</ref> depicts the different RISC-V instruction categories. The RISC-V privilege architecture <ref type="bibr" target="#b24">[25]</ref> includes the following ISA modules: Machine ISA (M-mode), Supervisor ISA, and Hypervisor ISA. This work suggests supporting the RV64IM, i.e., 64-bit Integer with multiplication and division extension and the privileged M mode.</p><p>The basic Integer instruction set (I-Integer) includes 47 32-bits instructions that can be classified into eight categories. The instruction opcode is 7-bit length (lowest 7-bits of the instruction), where the lowest two bits indicate the optional compressed instruction-set extension. The unique RISC-V ISA encoding enables easy and fast decoding of the instruction type and, therefore, can be used to identify events associated with some specified instructions (such as Branch or Load).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single Thread In-Order RISC-V Pipeline</head><p>Fig. <ref type="figure" target="#fig_0">1</ref> depicts the basic single-thread in-order RI5CY architecture. The RI5CY core pipeline is composed of 4-stages: Fetch, Decode, Execute, and Write Back (WB).</p><p>1) Fetch Unit: The fetch unit includes a 32 bytes Instruction Queue (IQ), Issue PC, Next Instruction Pointer (NIP), and control logic. The IQ is implemented with a 32-bytes length FIFO, and therefore it can contain up to 8 Instructions. The Issue PC reflects the address of the instruction at the top of the IQ. This PC is incremented by four while dispatching a new instruction from the IQ and is updated in case of a change of flow (i.e., taken branch or unconditional jump).</p><p>The NIP points to the next instructions to be fetched from the program memory with a 16-byte granularity. The prefetch unit initiates a new memory request to fetch the next four instructions whenever the IQ contains four instructions or less. The prefetched instructions are stored in the IQ, and an internal queue counter is updated.</p><p>2) Decoder Unit: The decode stage is composed of three main modules: the register file, the decoder, and an immediate extension module. The register file contains 32 generalpurpose registers (64-bits length each). The RISC-V ISA keeps the source (RS1 and RS2) and destination (RD) registers at the same position in all formats to simplify decoding. Two registers can be read directly from the register file, and at the same time, another register can be written (as a result of a committed instruction). The decoder is responsible for decoding the instruction opcode, the instruction type (R-I-S-U), the function fields, and the immediate sub-fields. The immediate extension module extends the immediate fields in accordance with the instruction type (I-S-U). Immediate operands are always sign-extended, and are generally packed towards the leftmost available bits in the instruction, and have been allocated to reduce hardware complexity <ref type="bibr" target="#b31">[32]</ref>.</p><p>The instruction decoding and the register file access can be made in parallel with no extra hardware cost. The decoder generates two operands and control operations for the Execution unit. The operands can be fetched from the register file or directly from the immediate extension module. The selection of the operand source is performed using two multiplexers (one for each operand), as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>3) Execution Unit: The execution unit is composed of 5 functional units: (a) Integer ALU, (b) Branch logic unit (BLU), (c) Load/Store unit, (d) Multiplication/Division, and (e) Control Status Register (CSR) unit.</p><p>The number of cycles required to complete instruction in the execution unit is variable, and it depends on the operation being executed. Table II depicts the execution duration of the various instruction types. The execution stage requires one to 32 cycles to complete.</p><p>The Integer ALU supports arithmetic (ADD and SUB) and logic (OR, AND, XOR, and Shift) instructions. All registers operations are completed in a single cycle, avoiding any instruction dependency and therefore efficiently utilizing the pipeline bandwidth. The Branch logic unit is responsible for resolving the branch condition, and its execution is terminated within one cycle. The Load/Store unit has a direct connection  to the data memory. The RISC-V is a load-store ISA, and therefore, the only instructions used to access the memory are Load and Store. Any memory access is supported by using two different stages in the pipeline, one for address generation and one for the read/write access. Therefore, a nonblocking operation is supported to continue serving a new instruction while waiting for the data to arrive for the previous memory access. The Multiplication and Division functional unit supports the RISC-V ISA M-extension. The multiplication unit is fully pipelined (non-blocking operation) with a fixed execution latency of 4 cycles, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. However, the division unit has a variable latency requiring up to 32 cycles (for RV64 ISA) to complete its execution stage, during which the pipeline is blocked. The RISC-V defines specific control and status registers. The set of dedicated CSR instructions that operate on these CSRs are implemented in the CSR unit. The proposed pipeline can be easily extended to support other ISA extensions (like floating-point instructions).</p><p>4) Write-Back and Control Unit: During the Write-Back stage, the result of the committed instruction is written into the register file. The control unit handles any data hazards arising from instruction dependencies and supports change of flow. The control unit performs data forwarding to enable grabbing an operand directly from the execution stage (rather than from the register file) in case of dependencies between instructions (i.e., eliminate the need to wait until the WB stage).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multithread Support for RISC-V Pipeline</head><p>This section describes our proposed modifications required to support multithread for the RI5CY in-order pipeline. The RI5CY 4-stage in-order 32-bit Integer RISC-V core pipeline was extended to support multithreading and the 64-bit Integer RISC-V M-mode instruction set. This multithread extension requires the replication of the processor PC, the register file, and instruction fetch mechanisms for each thread. Hardware Replication is not required for some pipeline stages, such as the decoding unit and the major parts of the execution unit.</p><p>Each thread is assigned with a unique thread ID (TID), a dedicated PC and Instruction Queue (IQ), and a specific register file to support multithreading. A prefetch scheduling algorithm is responsible for fetching instructions into the selected thread IQ. Instructions can be issued into the pipeline only from a single thread in a given cycle. This is carried out using a scheduling algorithm that decides from which thread IQ to issue the next instruction.</p><p>As depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, the fetch unit is replicated for each thread and includes a new module in charge of selecting the next thread from which the next instructions should be fetched. A dedicated new issue scheduling module is added between the fetch and the decode stages to manage the thread replacement policy. The main change in the decoding stage is the extension of the register file to save each thread context. This work adapts the basic RI5CY <ref type="bibr" target="#b29">[30]</ref> microarchitecture in which the data register file is necessarily replicated on a perthread basis. The register file is statically partitioned across the threads to keep the multithreaded pipeline's simplicity and promise a fix and fast access to the register files. The active register file is selected according to the TID received from the fetch stage. The register file is extended by adding the TID as the most significant bits in the address indexing to address the appropriate register memory space. The execution stage integrates the TID, which is required in each of the five functional units. Finally, the CSRs are replicated for each thread and selected with the TID. During the WB stage, the appropriate register file is written according to the TID. The control unit supports change of flow and handles data hazards and forwarding for each thread.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THE PROPOSED ISSUE SCHEDULING APPROACH</head><p>This section describes the implementation methodology of the proposed scheduling approach and presents the proposed issue scheduling algorithm in detail. Scheduling algorithms that issue instructions from only a single thread in a given cycle typically use two principal techniques <ref type="bibr" target="#b5">[6]</ref>. (a) Interleaved fine-grained multithreading, where an instruction of another thread is fetched and fed into the execution pipeline at each processor cycle. (b) Blocked multithreading (BMT) which is also called Coarse-grain, where the instructions of a thread are executed successively until an event that may cause latency occurs (such as miss event). The fine-grained multithreading technique issues an instruction from a different thread at every clock cycle using a Round Robin scheduler with zero context switching overhead. Coarse-grained multithreading switches threads only on costly stalls, such as level two or three cache misses <ref type="bibr" target="#b12">[13]</ref>.</p><p>We suggest using the interleaved multithread as the default approach but induce a thread context switch in case of an event. The proposed scheduling algorithm is based on the specific RISC-V ISA that enables decoding of the instruction type in an early stage of the pipeline. In contrast to common issue scheduling based on a switch-on event (or switch on a miss), we suggest giving priority to a specific thread based on the next instruction associated with that thread. In case the next instruction of that thread (as appears in its IQ) is defined as an excepted event and may cause a delay (insert bubbles into the pipeline), this thread is given priority. The issue algorithm prioritizes this thread (for which an event is detected) and also issues the next instruction from the same thread IQ (i.e., two consecutive cycles are assigned to the same thread) before switching to the next thread. The rationale behind this is avoiding unrequired long latencies caused by instruction dependencies or instructions that consume a long time (i.e., more than one cycle). Therefore, this pre-detected event affects the scheduler preference by issuing the next instruction from the associated thread. Issuing the event earlier into the execution stage may resolve pipeline hazards and dependencies. While in the common switch-on-event (or switch on a miss) scheduling algorithm, the events are detected in advanced pipeline stages (decode or execution phase). We propose a mechanism to detect the event in the earlier fetch stage by inspection of the IQ. In other words, instead of performing switch-onevent, we suggest issuing on an expected event. The main idea of the proposed scheduling approach is to address the disadvantage of the fine-grained approach, which gives priority to the system throughput while harming (decreases) the singlethread performance. On the other side, while the alternative coarse-grain approach prioritizes the performance of a single thread (switching on events that cause long latency), the total throughput is decreased especially for short-latency events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Proposed Early Hint-Based Event Algorithm</head><p>The proposed Early Hint-based Event (EHE) is principally based on the fine-grained algorithm, where the thread switching is performed in each cycle except for a special scenario. The decision to switch to the next thread depends on the next instruction of the same thread. If the next instruction may cause a pipeline delay (due to an excepted stall), the scheduler also issues the next instruction of that thread, and in contrast to the fine-grain, a switch to the next thread is not carried out in the current cycle. Therefore, two consecutive instructions are issued for the current thread before switching to the next thread. By this, we avoid postponing the issuing of this instruction (which may cause a potential event) to the next thread scheduling cycle (i.e., after issuing at least one instruction for each thread, leading to delay of N clock cycles for N threads). The advantage of using the EHE approach in combination with the fine-grain scheduling is utilizing the early detection of the event and issuing the instruction associated with that event. This enables propagating the instruction into the pipeline while concurrently issuing instructions from other threads and therefore hiding the latency. The potential improvement is 3. Different scheduling algorithms policies. more significant in cases where the latency caused by that event cannot be hidden, as in the case of a complex pipeline, including a large number of execution stages or in the case of long execution instructions (multiply and division).</p><p>Fig. <ref type="figure">3</ref> depicts the principle of the proposed EHE algorithm compared to the RR and Coarse-Graine algorithms. The figure shows the policy of the scheduling algorithm for four threads (marked with different colors), demonstrating the pipeline execution flow. The IQ for each thread is initialized with four real assembly code instructions, where one of them represents an event instruction (located in different stages in each queue).</p><p>Algorithm 1 describes the implementation of the fine-grain approach based on Round Robin scheduling. The number of threads is assigned in the initialization phase, and thread zero is set as the next thread to be issued. Lines 1-10 depict the main loop testing if the IQ of the current thread is empty (line 2-4), and checking for instructions dependency (line 5-8).</p><p>Algorithm 2 depicts the pseudo-code for the proposed EHE scheduling algorithm. The number of threads is assigned in the initialization phase, and thread zero is set as the next thread to be issued. Lines 3-7 check if that thread IQ is not empty. Lines 7-12 check if the instruction in the Top of IQ represents an Event. In lines 13-18, the potential dependency between the current and next instructions is checked. In case the IQ is empty, or no event is detected, or a dependency is detected, the algorithm switches to the next thread using the RR approach. Otherwise, the algorithm issues the next instruction from the current thread, as described in line 19.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pipeline Simulation</head><p>Fig. <ref type="figure">4</ref> depicts the pipeline simulation for both the proposed EHE and RR scheduling algorithms using a simple code section including some instructions representing an event in each of the four threads queues. Fig. <ref type="figure">4</ref>(a) shows a code example of five instructions that should be committed in order, where events are marked in red. end if 10: end for and response for each thread at each clock. Instruction that represents an event, such as branch (BR) may require flushing of the pipe. We assume a prefetch latency of 3 clocks, and that the first execution stage (EX1) executes the pipeline flush upon detecting an event (BR taken) and empties the appropriate IQ. For example, as depicted in Fig. <ref type="figure">4</ref>(c), upon detecting an event in THD3 on cycle nine, a flush (Flush T3) is performed, and Q3 is clear. This example demonstrates the superior of the proposed scheduling compared to RR. While for the EHE, ten instructions are completed in 10 clock cycles (clk6-clk15), for the RR only eight instructions are completed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>This section describes the simulations and the results for the proposed EHE scheduling algorithm for various common applications and different pipeline architectures. The proposed algorithm is compared with the Coarse-grain (switch-on event), and Fine grain (RR) common approaches. Results show that the EHE algorithm outperforms those two algorithms with an improvement of 10-15%. Simulations were conducted using a selection of common benchmark applications such as MiBench <ref type="bibr" target="#b20">[21]</ref>, the common EEMBC's CoreMark application <ref type="bibr" target="#b21">[22]</ref>, and a DSP oriented Turbo Code Encoder (TE) legacy application <ref type="bibr" target="#b22">[23]</ref>. Two typical applications were chosen from the MiBench benchmark suite: (a) SHA representing security applications, and (b) Dijkstra representing networking applications.</p><p>The effect of the pipeline length and the prefetch latency on the total throughput for the proposed EHE scheduling algorithm is evaluated for various pipeline architectures and a different number of threads (1-2-4-8). The pipeline utilization increases as the number of threads increases. The longer the pipeline, there is a better potential for the scheduling algorithm to improve the total throughput in terms of IPC. Next_Thd ? Round_Robin (Thd_Id) // RR Algo. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SimPipe Simulator</head><p>Within the frame of this work, we developed a dedicated Software Pipeline Simulator (SimPipe) to model the hardware multithread architecture based on the RI5CY single Issue inorder pipeline <ref type="bibr" target="#b36">[37]</ref>. The SimPipe is a configurable simulator composed of three mains software modules: the fetch module, the Issue and Decode Unit, and the execute module. The fetch module is responsible for filling the IQ and enables controlling the following parameters: prefetch latency, the size of the IQ, and the fetch granularity. The issue unit is a configurable module enabling the simulation of the various scheduling algorithms. The execution module implements the in-order pipeline execution stages with a different number of stages (ranging from 3 to 5 stages in the performed experiments). The instruction execution time is configurable as well, ranging from only one cycle for ALU arithmetic instructions up to the number of execution stages for long latency instructions (like Load, Store, and MUL/DIV). The tested applications have been compiled on the RISC-V compiler (Riscv64-elfgcc-8.2.0) <ref type="bibr" target="#b37">[38]</ref> and executed on the GEM5 simulator <ref type="bibr" target="#b38">[39]</ref>, generating a dedicated trace file. The GEM5 tracing files serve as an input to the SimPipe simulator and describes the executed instructions, the order of execution, and the instructions' absolute address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MT Performance Evaluation</head><p>The performance evaluation has been limited to relatively short pipeline depth, ranging from 3 to 5 execution stages, to cope with typical IoT embedded processors, for which the instruction pipeline depth is relatively short. Table <ref type="table" target="#tab_3">III</ref> depicts the performance comparison of the RR and EHE scheduling algorithms in terms of IPC for 1-2-4 threads for different pipeline architectures running CoreMark application. The proposed EHE algorithms outperform the RR for all scenarios demonstrating a speedup of up to 21% for four threads assuming a prefetch latency of 4 cycles and five execution stages (4-Fetch-5-Exe).</p><p>Table <ref type="table" target="#tab_3">III</ref> shows that the pipeline utilization (in term of IPC) increases as the number of threads increase for all pipeline configurations. For example, the resulting IPC for RR using 4-Fe-5-Ex configuration is 0.31-0.48-0.76 for 1-2-4 threads, respectively. For the same configuration, the EHE results in a better IPC of 0.581 and 0.930 for 2-4 threads, respectively. The proposed EHE increases the throughput in terms of IPC by a factor of about 3. The longer the pipeline, the potential for the scheduling algorithm to improve performance is increased. The potential for speedup improvement is more significant for a long pipeline with higher latency. While the achieved EHE speedup is 14.04% for the 2-Fe-3-Ex configuration, the speedup for the 3-Fe-4-Ex and 4-Fe-5-Ex configurations increases to 18.32% and 20.29%, respectively. The proposed EHE algorithms outperform the RR for all scenarios demonstrating a speedup of up to 21% for four threads assuming a  prefetch latency of 4 cycles and five execution stages (4-Fetch-5-Exe). This analysis had been done for various applications, and the results show a similar conclusion. The EHE scheduling algorithm contributes to the pipeline utilization compared to the Round Robin algorithm. For longer pipeline configuration, for example, 4-Fe 5-Ex pipeline, with four threads, an average speedup of 20% is achieved.</p><p>Fig. <ref type="figure" target="#fig_4">5</ref> depicts the performance in terms of IPC for the three scheduling algorithms (RR, Coarse, and EHE) for various common applications, including CoreMark, Dijkstra and SHA from the MiBench suite, and the Turbo Code Encoder (TC). The performance analysis has been performed on the 4-Fe-5-Ex configuration. For all four applications, the performance linearly increases as a function of the threads number. Increasing the number of threads contributes to improving performance significantly. The pipeline utilization substantially increases as the number of threads increases for all applications. For example, CoreMark demonstrates IPC of 0.32, 0.58, and 0.93 for 1-2-4 threads using the EHE algorithm. The proposed EHE and coarse-grain algorithms demonstrate quite similar performance while using the same predefined events. Fig. <ref type="figure" target="#fig_5">6</ref> shows the achieved speedup using the proposed EHE compared to RR and Coarse. The EHE algorithm demonstrates a significant speedup compared to RR, ranging from 9.6% (SHA) to 20.3% (CoreMark) for two threads and from 21.1% (TC) to 26.9% (SHA) for four threads. The improvement potential is limited while using eight threads, and a high IPC of about 0.98 and up is demonstrated regardless of the scheduling algorithms for the 4-Fe-5-Ex configuration.</p><p>Since the proposed EHE and coarse-grain algorithms demonstrate quite similar results, the performance in terms of IPC is compared only to the RR algorithm. It is important to emphasize that while the coarse-grain requires dedicated extra hardware to save the current pipeline states on each thread switching, the proposed EHE is characterized by very simple hardware implementation. Fig. <ref type="figure" target="#fig_6">7</ref> depicts the resulted IPC for the EHE algorithms in comparison to the RR algorithm for all five applications. The EHE outperforms the RR for both two and four threads for all the applications. For example, the speedup for CoreMark is 20.3% for two threads and 21.4% for four threads.</p><p>For all four applications, the IPC using difference pipeline configuration (such as 3-Fe 5-Ex, 2-Fe 5-Ex, and 4-Fe 4-Ex) increases as the number of threads increase. As demonstrated before, increasing the number of threads contributes to improving pipeline utilization. The proposed EHE issue scheduling algorithm outperforms the Round Robin and   shows similar (or better) results to the coarse-grain for all configurations.</p><p>Other techniques were explored in order to increase pipeline utilization. One of them is instruction forwarding. Instruction forwarding is the ability to use previous instructions results before they are being written into the register file. We have conducted an experiment that uses forwarding to enable singlecycle instructions (for example, ALU operation) to forward the results to the currently issued instruction if necessary.</p><p>Table <ref type="table" target="#tab_4">IV</ref> demonstrates the contribution of the forwarding feature to the IPC speedup for the RR and EHE approaches applying to the four typical applications. The IPC speedup ranges from 9.8% (SHA) to 25.2% (CoreMark) for a single thread implementation. A similar speedup is achieved for two threads, but the speedup significantly decreases for four threads. This can be explained due to the high pipeline utilization achieved using the proposed EHE approach and the high baseline IPC (which limits the potential for improvement). Therefore, the additional forwarding support doesn't contribute in the case of four threads, and even a negligible performance degradation (-0.2%) is shown for Turbo Encoder with four threads.</p><p>The proposed EHE algorithm (without forwarding support) outperforms the RR algorithm with forwarding support for all applications. For example, for CoreMark, the RR algorithm (with forwarding) demonstrates an IPC of 0.868 using 4-threads, while our proposed EHE achieves a higher IPC of 0.93 without forwarding support. Thus, applying the proposed EHE algorithms saves the dedicated hardware required for forwarding implementation while using more than two threads.</p><p>In general, Round Robin shows a higher potential for improvement than EHE using the forwarding feature. This can be explained due to lower utilization achieved by the Round Robin in the configuration without forwarding.</p><p>Simulation results show that the contribution of supporting speculative execution (like issuing consecutive instructions after issuing conditional BR instructions) is minor (around 1-3%) and doesn't impact the IPC performance for a multithread architecture.</p><p>The RI5CY multithreaded architecture and the proposed EHE scheduler have been implemented in hardware using Altera (Intel) -Cyclone IV F23I7 FPGA board. The RTL While, the number of dedicated logic registers is linearly increased by a factor of 1.7, 3.2, and 6.1 for 2, 4, and 8 threads, respectively, due to the duplication of the register file. As expected, the main source for the extra area while implementing the multithreaded architecture is mostly due to the cost for the revised decoding unit, including the replicated register file (which occupies about 30% of the design area). Since this paper focuses on scheduling algorithms, the extra hardware required should be compared to the case of using a multithreading approach with RR as the selected scheduling algorithm. The hardware cost of the proposed EHE is negligible compared to the common RR algorithm. The complexity of the hardware implementation of the proposed EHE scheduler is very low and requires an extra cost of only a 5-bit XOR operation (for each thread) compared to RR. Therefore, the additional area and power required for implementing the proposed EHE approach are negligible compared to RR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>This work presents a new efficient event-based issue scheduling algorithm applied to RISC-V multithreaded pipeline. The proposed scheduling approach utilizes the unique RISC-V ISA property that enables decoding of the instruction type in the early stage of the pipeline. The basic RISCY in-order single-thread has been extended to efficiently support a single-issue hardware multithreading pipeline. Simulation results show that the proposed algorithm outperforms the common Round Robin and the coarse-grain algorithms. Results show that there are some scenarios for which the Coarse-Grain and EHE scheduling algorithms demonstrate similar performance. However, while the Coarse-Grain requires dedicated extra hardware for saving the current states of the pipeline on each thread switching, the proposed EHE is characterized with a very simple hardware implementation (similar to finegrained). The proposed architecture is evaluated using the MiBench benchmark suite and the common EEMBC's Core-Mark application, demonstrating pipeline utilization improvement of up to about 26% in terms of IPC using four threads. Future research can exploit the proposed issue scheduling approach to multi-issue multithreading to improve further the throughput of IoT edge devices and RISC-V-based embedded processors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. RISC-V in-order single-thread microarchitecture.</figDesc><graphic url="image-1.png" coords="4,73.43,58.85,464.18,198.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. RISC-V in-order multithread microarchitecture.</figDesc><graphic url="image-2.png" coords="5,74.03,58.49,463.34,201.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 2 : 3 :</head><label>123</label><figDesc>Fig.4depicts the pipeline simulation for both the proposed EHE and RR scheduling algorithms using a simple code section including some instructions representing an event in each of the four threads queues. Fig.4(a) shows a code example of five instructions that should be committed in order, where events are marked in red. Fig. 4(b) shows the common initialize stage for all the four threads fetching the first four instructions. Fig 4(c) and Fig. 4(d) depicts a clock accurate pipeline simulation for the RR and the proposed EHE, correspondingly. This simple example shows the pipeline propagation for 3 phases: Issue, Decode, and Execution. The figure depicts the IQ occupancy and the memory request</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Algorithm 2</head><label>42</label><figDesc>Fig. 4. Pipeline simulation for RR vs. EHE scheduling algorithms.</figDesc><graphic url="image-3.png" coords="7,74.03,58.61,463.34,144.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. IPC performance as a function of the three scheduling algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. EHE speedup performance compared to RR and coarse scheduling algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. EHE vs. RR Performance comparison (4-Fe 5-Exe).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I RISC</head><label>I</label><figDesc></figDesc><table /><note><p>-V INSTRUCTION CATEGORIES</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II INSTRUCTION LATENCY</head><label>IILATENCY</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PIPELINE</head><label>III</label><figDesc>PERFORMANCE (IN TERMS OF IPC) FOR VARIOUS PERMUTATIONS (COREMARK)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV FORWARDING</head><label>IV</label><figDesc>CONTRIBUTION TO IPC SPEEDUP (RR VS. EHE) [%]</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V QUARTUS</head><label>V</label><figDesc>SYNTHESIS REPORTS FOR THE RI5CYCPU WITH 1, 2, 4, AND EIGHT THREADS code was synthesized using the Intel Quartus Prime design compiler. The design has been simulated and verified using Intel FPGA ModelSim. TableVshows the synthesis reports for the basic RI5CY CPU and various MT configurations. The single thread implementation consists of 7525 logic cells elements and 2119 dedicated logic registers. The number of the total required logic cells is increased by a factor of 1.5, 2.4, and 4.2 for 2, 4, and 8 threads, respectively, compared to the single thread baseline implementation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Tsinghua University. Downloaded on January 01,2024 at 05:25:57 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">GenPro Consortium</rs> within the framework of the <rs type="institution">Israel Innovation Authority</rs>'s <rs type="programName">MAGNET Program</rs>. This article was recommended by <rs type="person">Associate Editor M. Martina. Shlomo Greenberg</rs> is with the <rs type="institution">School of Electrical and Computer Engineering, Ben-Gurion University of the Negev, Beer-Sheva 8410501, Israel</rs>, and also with the <rs type="institution">Department of Electrical Engineering, Sami Shamoon College of Engineering</rs>, <rs type="person">Beer-Sheva 8410802</rs>, Israel</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EV63gCx">
					<orgName type="program" subtype="full">MAGNET Program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Chrysos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Intel Xeon Phi Coprocessor Architecture</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>Intel</publisher>
			<pubPlace>Mountain View, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">IBM POWER9 processor core</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A 40 nm 16-core 128-thread CMT sparc SOC processor</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IEEE Int. Solid-State Circuits Conf. (ISSCC) Dig. Tech. Papers</title>
		<imprint>
			<biblScope unit="page" from="98" to="99" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bringing hardware multithreading to the real-time domain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tavares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Embedded Syst. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="5" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hardwarelevel thread migration to reduce on-chip data movement via reinforcement learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Fettes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karanth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bunescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Louri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shiflett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput.-Aided Design Integr. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3638" to="3649" />
			<date type="published" when="2020-11">Nov. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of processors with explicit multithreading</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ungerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>?ilc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="63" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">IBM Power9 processor architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sadasivam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Thompto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Starke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2017-03">Mar. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The IBM 13 multithreaded microprocessor</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM J. Res. Develop</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data criticality in multithreaded applications: An insight for many-core systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Very Large Scale Integr. (VLSI) Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1675" to="1679" />
			<date type="published" when="2021-09">Sep. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A novel warp scheduling scheme considering long-latency operations for highperformance GPUs</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Supercomput</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="3043" to="3062" />
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ABCDPlace: Accelerated batch-based concurrent detailed placement on multithreaded CPUs and GPUs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput.-Aided Design Integr. Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5083" to="5096" />
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VisoMT: A collaborative multithreading multicore processor for multimedia applications with a fast data switching mechanism</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Ku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1633" to="1645" />
			<date type="published" when="2009-11">Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Elsevier</publisher>
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A small footprint interleaved multithreaded processor for embedded systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bechara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Int. Conf. Electron., Circuits, Syst</title>
		<meeting>18th IEEE Int. Conf. Electron., Circuits, Syst</meeting>
		<imprint>
			<date type="published" when="2011-12">Dec. 2011</date>
			<biblScope unit="page" from="685" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design of the tera MTA integrated circuits</title>
		<author>
			<persName><forename type="first">M</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Gallium Arsenide Integr. Circuit Symp</title>
		<meeting>IEEE Gallium Arsenide Integr. Circuit Symp</meeting>
		<imprint>
			<date type="published" when="1997-12">Dec. 1997</date>
			<biblScope unit="page" from="14" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">APRIL: A processor architecture for multiprocessing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kubiatowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Annu</title>
		<meeting>17th Annu</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="104" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient mathematical accelerator design coupled with an interleaved multi-threading RISC-V microprocessor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sordillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mastrandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menichelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olivieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Appl. Electron. Pervading Ind</title>
		<meeting>Int. Conf. Appl. Electron. Pervading Ind<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Schedulability analysis of nMPRA processor based on multithreaded execution</title>
		<author>
			<persName><forename type="first">I</forename><surname>Zagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Gaitan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Develop. Appl. Syst. (DAS)</title>
		<meeting>Int. Conf. Develop. Appl. Syst. (DAS)</meeting>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="130" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memristor-based multithreading</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kvatinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Etsion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolodny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Archit. Lett</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="44" />
			<date type="published" when="2014-01">Jan. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous multithreading: Maximizing on-chip parallelism</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Eggers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 22nd Annu. Int. Symp. Comput. Archit</title>
		<meeting>22nd Annu. Int. Symp. Comput. Archit</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="392" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">MiBench: A free, commercially representative embedded benchmark suite</title>
		<author>
			<persName><forename type="first">M</forename><surname>Guthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ringenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Annu. IEEE Int. Workshop Workload Characterization</title>
		<meeting>4th Annu. IEEE Int. Workshop Workload Characterization</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A benchmark characterization of the EEMBC benchmark suite</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Poovey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Conte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gal-On</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="18" to="29" />
			<date type="published" when="2009-10">Sep./Oct. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Near Shannon limit error-correcting coding and decoding: Turbo-codes. 1</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Glavieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thitimajshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Commun. (ICC)</title>
		<meeting>IEEE Int. Conf. Commun. (ICC)</meeting>
		<imprint>
			<date type="published" when="1993-05">May 1993</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1064" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The RISC-V instruction set manual</title>
		<author>
			<persName><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovi?</surname></persName>
		</author>
		<idno>UCB/EECS-2016-118</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. EECS, Univ. California</title>
		<imprint>
			<biblScope unit="volume">I</biblScope>
			<date type="published" when="2016">2016</date>
			<pubPlace>Berkeley, Berkeley, CA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
	<note>User-level ISA, version 2.1</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Waterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Avizienis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<idno>Rep. UCB/EECS-2016-129</idno>
		<title level="m">Privileged architecture version 1.9</title>
		<meeting><address><addrLine>Berkeley, Berkeley, CA, USA, Tech</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">II</biblScope>
		</imprint>
		<respStmt>
			<orgName>EECS Dept., Univ. California</orgName>
		</respStmt>
	</monogr>
	<note>The RISC-V instruction set manual</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A low-cost synthesizable RISC-V dual-issue processor core leveraging the compressed instruction set extension</title>
		<author>
			<persName><forename type="first">K</forename><surname>Patsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Konstantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nicopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dimitrakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Microprocess. Microsyst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2018-09">Sep. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The rocket chip generator</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<idno>UCB/EECS-2016- 17</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<pubPlace>Berkeley, Berkeley, CA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>EECS Dept., Univ. California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The Berkeley out-of-order machine (boom): An industry-competitive, synthesizable, parameterized RISC-V processor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Celio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<pubPlace>Berkeley, CA, USA, Tech</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. California Berkeley</orgName>
		</respStmt>
	</monogr>
	<note>Rep. AD-1003146</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An IoT endpoint system-on-chip for secure and energyefficient near-sensor analytics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Conti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. I, Reg. Papers</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2481" to="2494" />
			<date type="published" when="2017-09">Sep. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Slow and steady wins the race? A comparison of ultra-low-power RISC-V cores for Internet-of-Things applications</title>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Schiavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 27th Int. Symp. Power Timing Modeling, Optim. Simulation (PATMOS)</title>
		<meeting>27th Int. Symp. Power Timing Modeling, Optim. Simulation (PATMOS)</meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Energy-efficient vision on the PULP platform for ultra-low power parallel computing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Conti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pullini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Loi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Signal Process. Syst. (SiPS)</title>
		<meeting>IEEE Workshop Signal ess. Syst. (SiPS)</meeting>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">SIMTY: Generalized SIMT execution on RISC-V</title>
		<author>
			<persName><forename type="first">S</forename><surname>Collange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Hal-Imria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">France</forename></persName>
		</author>
		<idno>HAL-01622208</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The microarchitecture of a multi-threaded RISC-V compliant processing core family for IoT end-nodes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mastrandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menichelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Olivieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Appl. Electron</title>
		<meeting>Int. Conf. Appl. Electron<address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="89" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A multithreading RISC-V implementation for Lagarto architecture</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Escobar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<pubPlace>Barcelona, Spain</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Dept. Comput. Archit., Univ. Polit?cnica de Catalunya</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">M.S. thesis</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Investigation on the optimal pipeline organization in RISC-V multithreaded soft processor cores</title>
		<author>
			<persName><forename type="first">M</forename><surname>Olivieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cerutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mastrandrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Menichelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. New Gener. CAS (NGCAS)</title>
		<meeting>New Gener. CAS (NGCAS)</meeting>
		<imprint>
			<date type="published" when="2017-09">Sep. 2017</date>
			<biblScope unit="page" from="45" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multithread RISC architecture based on programmable interleaved pipelining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pulka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th IEEE Int. Conf. Electron., Circuits Syst. (ICECS)</title>
		<meeting>16th IEEE Int. Conf. Electron., Circuits Syst. (ICECS)</meeting>
		<imprint>
			<date type="published" when="2009-12">Dec. 2009</date>
			<biblScope unit="page" from="647" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Eni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leon</surname></persName>
		</author>
		<ptr target="https://github.com/leondavi/simpipe" />
		<title level="m">Simpipe-A Flexible CPU Pipeline Simulator</title>
		<imprint>
			<date type="published" when="2021-02">Sep. 2, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Risc-V Gnu Compiler</forename><surname>Toolchain</surname></persName>
		</author>
		<ptr target="https://github.com/riscv/riscv-gnu-toolchain" />
		<imprint>
			<date type="published" when="2021-02">Sep. 2, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The gem5 simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
