<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Expression Recognition by De-expression Residue Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Huiyuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Binghamton</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Umur</forename><surname>Ciftci</surname></persName>
							<email>uciftci@binghamton.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Binghamton</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lijun</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">State University of New York at Binghamton</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Expression Recognition by De-expression Residue Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A facial expression is a combination of an expressive component and a neutral component of a person. In this paper, we propose to recognize facial expressions by extracting information of the expressive component through a de-expression learning procedure, called De-expression  Residue Learning (DeRL). First, a generative model is trained by cGAN. This model generates the corresponding neutral face image for any input face image . We call this procedure de-expression because the expressive information is filtered out by the generative model; however, the expressive information is still recorded in the intermediate layers. Given the neutral face image, unlike previous works using pixel-level or feature-level difference for facial expression classification, our new method learns the deposition (or residue) that remains in the intermediate layers of the generative model. Such a residue is essential as it contains the expressive component deposited in the generative model from any input facial expression images. Seven public facial expression databases are employed in our experiments. With two databases  for pre-training, the DeRL method has been evaluated on five databases, CK+, Oulu-CASIA, MMI, BU-3DFE, and BP4D+. The experimental results demonstrate the superior performance of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Research on facial expression recognition (FER) has been conducted on both posed and spontaneous facial expressions under various imaging conditions, including various head poses, illumination conditions, resolutions, and occlusions <ref type="bibr" target="#b34">[34]</ref> [14] <ref type="bibr" target="#b16">[17]</ref> [18] <ref type="bibr" target="#b11">[12]</ref>. Although significant progress has been made towards improving the expression classification, the current main challenge comes from the large variations of individuals in attributes such as: age, gender, ethnic background and personality. Facial expressions may appear different (w.r.t. expressiveness or subtlety, etc.) for people with different personalities and ex- pressive styles. Only recently have works <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b12">[13]</ref> started to take aspects of subject's identity attributes i.e., age, gender, and personal characteristics, into consideration for facial expression analysis.</p><p>Research shows that people are capable of recognizing facial expressions by comparing a subject's expression with a reference expression (i.e., neutral expression) of the same subject <ref type="bibr" target="#b3">[4]</ref> [5] <ref type="bibr" target="#b9">[10]</ref>. In other words, a facial expression can be decomposed to an expressive component and neutral component <ref type="bibr" target="#b24">[25]</ref>. Up until now, several existing works utilized the image-difference or feature-difference of the query image and neutral face image <ref type="bibr" target="#b1">[2]</ref> [30] <ref type="bibr" target="#b14">[15]</ref> [13] to recognize facial expressions. However, their assumption is that the neutral expression must be obtainable. As a matter of fact, the neutral expression may not be always available for a given subject. In order to alleviate the problem, it is on demand to develop a neutral expression generator based on the given expressive input. The Generative Adversarial Model  (GAN) <ref type="bibr" target="#b7">[8]</ref> is able to serve this purpose. To train a generative model (generator), a GAN framework utilizes another deep model (discriminator) to play an adversarial game with the generative model, rather than defining a regular cost function for the generator. The discriminator is designed to distinguish between samples from the generator and samples from the training data; while the generator learns to output samples that can maximally confuse the discriminator. An extension of the basic GAN is the conditional Generative Adversarial Networks (cGAN) <ref type="bibr" target="#b21">[22]</ref>, which is capable of learning different contextual information through the extra conditional variations. There are existing work that combine CNN and cGANs for many applications, including face generation <ref type="bibr" target="#b6">[7]</ref>, object reconstruction from edge maps <ref type="bibr" target="#b10">[11]</ref>, and object attributes manipulation <ref type="bibr" target="#b23">[24]</ref>.</p><p>In this paper, we propose a new approach called Deexprssion Residue learning (DeRL) to learn facial expressions by extracting the expressive component through a deexpression procedure. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, given a facial image with arbitrary expressions, its corresponding neutral expression is generated by the trained generative model. Through the procedure, the identity information of a subject remains unchanged while the expressive component is removed. We called this de-expression. Although the input image with an expression is "normalized" to the neutral expression as output, the expressive component that has been filtered out is still "deposited" in the generative model. In other words, the expression information is recorded in the generator during the de-expression procedure. Such a deposition is the residue of the de-expression, which is exactly the expressive component that we target to exploit for expression classification.</p><p>In contrast to the previous methods <ref type="bibr" target="#b14">[15]</ref> [2] [30] <ref type="bibr" target="#b12">[13]</ref>, which used pixel-level difference or feature-level difference of expression images and neutral images, our proposed DeRL framework learns the de-expression residue remained in the generative model, with an attempt to mitigate the influence of individual identity and improve the performance of facial expression recognition. The contribution of this work lies in two-fold:</p><p>1. We propose a novel method to learn expressions by deexpression. We first train a generative model to generate the corresponding neutral face image for the query image; and then we learn the residue (i.e., expressive component) of the generative model, thus alleviating the identity-related variation issue.</p><p>2. Our proposed method is capable of handling cases of both spontaneous expressions and posed expressions, with various styles and ethnic backgrounds. It successfully improves performance on individual datasets, as well as for cross-dataset validation, with better results compared to the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous works suggest that facial expression recognition could benefit from using a neutral face image <ref type="bibr" target="#b29">[30]</ref>  <ref type="bibr" target="#b12">[13]</ref>. Subtracting a neutral face image from the corresponding facial expression image in either pixel-level or feature-level can emphasize the facial expression while reducing the intra-class variation.</p><p>Bazzo et al. <ref type="bibr" target="#b1">[2]</ref> achieved good recognition rate by applying Gabor wavelets to recognize facial expression images subtracted from an averaged neutral face. Zafeiriou et al. <ref type="bibr" target="#b29">[30]</ref> applied sparse facial expression representations for the difference images, which were derived from the subtraction of the neutral image from the expressive one, and demonstrated that the use of neutral images tends to emphasize the moved facial parts. Lee at al. <ref type="bibr" target="#b14">[15]</ref> generated several intra-class variation images (including neutral) from a training set, which were then subtracted by the query face image to obtain difference images. The difference images were used to emphasize the facial expressions in a query face image. Kim et al. <ref type="bibr" target="#b12">[13]</ref> employed a contrastive representation in the networks to extract the feature level difference between a query face image and a neutral face image.</p><p>However, these previous works made the assumption that the neutral expression is always available given any expression of the same subject, which is not realistic. It is on demand to generate a neutral face from any expression input. The recent utility of GAN shows success in such an application. Gauthier <ref type="bibr" target="#b6">[7]</ref> tried to use cGANs to generate faces with specific attributes. Radford et al. <ref type="bibr" target="#b23">[24]</ref> attempted to scale up GANs using CNNs to model images and introduced the structure of deep convolutional generative adversarial networks (DCGANs). This work showed the capability to manipulate the generated face samples by vector arithmetic. Isola et al. <ref type="bibr" target="#b10">[11]</ref> utilized conditional adversarial networks for image-to-image translation and showed many interesting applications, i.e., generating aerial photograph from map, reconstructing objects from edge maps, and colorizing images. Also, Zhou et al. <ref type="bibr" target="#b36">[36]</ref> applied cGANs to synthesize facial expression images from the neutral faces.</p><p>So far, there has been use of image or feature difference of query images and the generated neutral images, but there is no exploration for any implicit expressive information recorded in the generative model. We propose to explore the expressive information, which is embedded in the generator, and extract the expression component from the intermediate layers directly. In fact, such information is "filtered out" by the generator during the de-expression process while its representation (or residue) is still deposited in the generative model, thus becoming the key information to represent the expressive component. Rather than using both query image and generated neutral face image to train a deep model with a contrastive loss function (e.g., <ref type="bibr" target="#b12">[13]</ref>), our proposed method focuses on learning the residue of the generative model, leading to effectively capturing the expressive component and being more robust to individual variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method -DeRL</head><p>The architecture of our proposed method -Deexpression Residue Learning (DeRL), illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>, contains two learning processes: the first is learning the neutral face generation by cGANs, and the second is learning from the intermediate layers of the generator. The input image pairs, e.g. &lt; I input , I target &gt; , are used to train the cGANs. I input is a face image showing any expression, and I target is a neutral face image of the same subject.</p><p>After training, the generator reconstructs the corresponding neutral face image for any input while keeping the identity information unchanged. From an expressive facial image to a neutral face image, the expression-related information is recorded as expressive component in the intermediate layers. For the second learning process, the parameters of the generator are fixed, and the output of the intermediate layers are combined and input into deep models for facial expression classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Neutral Face Regeneration</head><p>cGAN <ref type="bibr" target="#b21">[22]</ref> is exploited to generate a neutral face representation from a given expressive face image . A GAN framework usually contains two different players: a generator (G) and discriminator (D). The generator is trained to recover the distribution of the training data by playing a so-called minmax game with the discriminator. Image pairs &lt; I input , I target &gt; are provided for training the cGANs. I input is first input into the generator to reconstruct I output , and then &lt; I input , I target , yes &gt; and &lt; I input , I output , no &gt; are given to the discriminator. The discriminator tries to distinguish the &lt; I input , I target &gt; from the &lt; I input , I output &gt;, while the generator tries to not only maximally confuse the discriminator but also generate an image as close to the target image as possible.</p><p>The objective for the discriminator is expressed as:</p><formula xml:id="formula_0">L cGAN (D) = 1 N N i=1 logD I input , I target + log 1 − D I input , G(I input )<label>(1)</label></formula><p>, where N is the total number of training image pairs. The objective for the generator is described below:</p><formula xml:id="formula_1">L cGAN (G) = − 1 N N i=1 log D I input , G(I input ) + θ 1 • ||I target − G(I input )|| 1<label>(2)</label></formula><p>Here, we use L1 loss for the image similarity rather than L2, because L2 loss is prone to over-blurring the output image <ref type="bibr" target="#b10">[11]</ref>. The final objective is:</p><formula xml:id="formula_2">G * = arg min G max D L cGAN (D) + θ 2 • L cGAN (G)<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning Facial Expressive Component</head><p>After the neutral face regeneration, the expression information can be analyzed by comparing the neutral face and the query expression face at pixel level or feature level. However, the pixel level difference is unreliable due to the variation between images i.e., rotation, translation and lighting condition changes. This can cause a large pixel-level difference even without expression changes. Also, the feature level difference is unstable, as the expression information may vary according to the identity information. Since the difference of the query image and the neutral image is recorded in the intermediate layers, we exploit the expressive component from the intermediate layers directly to alleviate the above problem, .</p><p>We denote an image with a facial expression as: I id exp . After it is input into the generative model, a neutral expression image is generated:</p><formula xml:id="formula_3">I id=A exp=neutral = G I id=A exp=E (4)</formula><p>where,G is the generator, E belongs to any of the six basic prototypic facial expressions. From the Equation (4), we can see that an image with subject (A) and expression (E) becomes a neutral face of the same subject (A). It is reasonable to conclude that the unique expression information </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed DeRL approach is evaluated on five public facial expression databases, including CK+ <ref type="bibr" target="#b19">[20]</ref>, Oulu-CASIA <ref type="bibr" target="#b33">[33]</ref>, MMI <ref type="bibr" target="#b22">[23]</ref> and BU-3DFE <ref type="bibr" target="#b28">[29]</ref>, and spontaneous expression database BP4D+ <ref type="bibr" target="#b32">[32]</ref>. Additionally, two other databases with posed expressions BU-4DFE <ref type="bibr" target="#b27">[28]</ref> and spontaneous expressions BP4D <ref type="bibr" target="#b31">[31]</ref> are used for pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Three landmark points (the centers of eyes, and the chin) are used to align the face region. For the databases where landmarks are not provided, we apply the TSM <ref type="bibr" target="#b37">[37]</ref> for face detection and landmark localization. The aligned face region is then cropped and resized to the size of 70 × 70. To avoid over-fitting, we apply a data augmentation method to generate more training data. First, five 64 × 64 patches are cropped-out from five locations of each image (center and four corners, respectively). Each image patch is rotated by</p><formula xml:id="formula_4">[−15 o , −12 o , −9 o , −6 o , −3 o , 0 o , 3 o , 6 o , 9 o , 12 o , 15 o ]</formula><p>respectively. Horizontally flipping is also applied. The result is an augmented dataset, which is 110 times larger than the original one. The data augmentation method is only applied to the training data.</p><p>The generative model is first pre-trained on the BU-4DFE <ref type="bibr" target="#b27">[28]</ref>, which contains 60,600 images from 101 subjects. Each subject has six sequences, and each sequence shows one of the six basic facial expressions from neutral to peak expression, ending with neutral again. To construct the training dataset, the first frame of each sequence is used as target image, and the rest of the sequence is used as input images. The pre-trained model is further finetuned on CK+, Oulu-CASIA, MMI, BU-3DFE, and BP4D+ databases.</p><p>We use the Adam optimizer with a batch size of 150, momentum of 0.9, and dropout of 0.5 for the fully connected layers during training. We use 200 epochs to train the generative models, and 50 epochs for the facial expression classification models. We set λ 1 = 0.7, λ 2 = 0.5, λ 3 = 0.3, λ 4 = 0.2 and λ 5 = 1.0 for the loss function respectively. The proposed method is implemented using tensorflow <ref type="bibr" target="#b0">[1]</ref> on the GeForce GTX 1080 platform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Visualization of Expressive Component and Regenerated Neutral Faces</head><p>Fig. <ref type="figure" target="#fig_4">4</ref> illustrates several samples of the generated neutral face image on CK+, Oulu-CASIA, BU-4DFE, and BP4D+ databases, respectively. The first column is the input image, the third column is the ground-truth neutral face image of the same subject, and the middle is the output of the generative model. As shown, the expressive component is filtered out by the generative model while the subject-related information is preserved. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the samples of the de-expression residue from the CK+ dataset, which are the expressive components for anger, disgust, fear, happiness, sadness, and surprise, respectively. The corresponding histogram of each expressive component is also displayed. As we can see, both expressive components and corresponding histograms are distinguishable among the six expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Expression Recognition Results</head><p>The Extended Cohn-Kanade database (CK+) <ref type="bibr" target="#b19">[20]</ref> is widely used for evaluating facial expression recognition. It contains 593 video sequences collected from 123 subjects. Among them, 327 video sequences with 118 subjects are labeled as one of seven expressions, i.e. anger, contempt, disgust, fear, happiness, sadness and surprise, from neutral to peak expression. Only the last frame of each sequence is labeled, as a general procedure, we use the last three frames of each sequence with the provided label, which results in 981 images. The images are further split into 10 folds based on the identity in an ascending order, thus the subjects in any two subsets are mutually exclusive.</p><p>The results are reported as the average of the 10 runs. As shown in Table <ref type="table" target="#tab_1">1</ref>, our proposed DeRL method achieves over 97% recognition rate, outperforming the compared state-of-the-art methods. Note that all these methods except the IACNN <ref type="bibr" target="#b20">[21]</ref>, CNN baseline and our DeRL method exploited temporal information extracted from image sequences. DTAGN <ref type="bibr" target="#b11">[12]</ref> also utilized the landmark features. In contrast to all other methods, our proposed DeRL method performs well on recognizing facial expressions on static images and achieves around 2% improvement compared to IACNN <ref type="bibr" target="#b20">[21]</ref>. Fig. <ref type="figure" target="#fig_5">5</ref> is the confusion matrix of our method, where fear expression shows the lowest recognition rate with 90%.</p><p>The Oulu-CASIA database <ref type="bibr" target="#b33">[33]</ref> contains data captured under three different illumination conditions using two types of cameras. During the experiment, only the data captured under strong illumination condition with the VIS camera is used. The Oulu-CASIA VIS has 480 video sequences taken from 80 subjects, and each video sequence  is labeled as one of the six basic expressions. Similar to the CK+ database, each video sequence starts from a neutral face and ends with a peak facial expression. To include more data, the last three frames of each sequence are selected. Similar to the experimental setting in CK+, a 10-fold subject-independent cross validation is performed.</p><p>The average accuracy on recognizing six expressions on Oulu-CASIA over 10 runs is shown in Table <ref type="table">.</ref>2. Our DeRL method achieves the highest accuracy compared to those of state-of-the-art methods, including CNN-based methods (DTAGN-Joint <ref type="bibr" target="#b11">[12]</ref>, FN2EN <ref type="bibr" target="#b5">[6]</ref>, and PPDN <ref type="bibr" target="#b35">[35]</ref>) and handcrafted features based methods (LBP-TOP <ref type="bibr" target="#b34">[34]</ref>, HOG 3D <ref type="bibr" target="#b13">[14]</ref> and STM-Explet <ref type="bibr" target="#b17">[18]</ref>). Note that only FN2EN <ref type="bibr" target="#b5">[6]</ref>, PPDN <ref type="bibr" target="#b35">[35]</ref> and our DeRL method use the static image for facial expression recognition, while others exploited the temporal information of video sequences. The confusion matrix is shown in Fig. <ref type="figure">6</ref>, our method performs very well in recognizing expressions happiness and surprise, while anger shows the relatively low recognition rate, which is mostly confused with disgust.</p><p>The MMI database <ref type="bibr" target="#b22">[23]</ref> consists of 236 image se- quences from 31 subjects. Each sequence is labeled as one of the six basic facial expressions. We selected 208 sequences captured in frontal view. Each sequence starts with a neutral expression, reaches peak expression near the middle of the sequence, and ends with a neutral expression. Since the label is only given for the whole sequence, we selected three frames in the middle of each sequence as peak frames and associated them with the provided labels. This results a dataset with 624 images. Similar to the CK+ database setting, a 10-fold subject-independent cross validation is performed. Table <ref type="table">.</ref> 3 reports the average accuracy of 10 runs on the MMI database for recognizing six expressions. Although STM-Explet <ref type="bibr" target="#b17">[18]</ref> shows the highest accuracy of 75.12%, it employs the temporal information extracted from the video sequence. Our DeRL method, showing a close result of 73.23%, recognizes facial expressions based only on static images, which is more suitable for some applications where video sequences are not available. Compared to the IACNN <ref type="bibr" target="#b20">[21]</ref>, which is also an image-based method, our DeRL  method shows improvement of 1.7%. As shown from the confusion matrix in Fig. <ref type="figure" target="#fig_6">7</ref>, fear is relatively hard for recognition, mostly confused with surprise and disgust. On the other hand, happiness is relatively easy to classify. The BU-3DFE database <ref type="bibr" target="#b28">[29]</ref> contains 2,500 pairs of static 3D face models and texture images from 100 subjects with a variety of ages and races. For each subject, six basic expressions with four levels of intensity and a neutral expression, was captured. During the experiment, only the texture images and the high intensity expressions ( i.e. the last two levels) are used. A 10-fold cross-validation is performed, and the split is subject independent.</p><p>Table <ref type="table" target="#tab_4">4</ref> summarizes the average result of 10 runs on the BU-3DFE database. From the data we can find that our proposed method outperforms the other two imagebased methods significantly. Notice that the multi-modality based approach (2D+3D) <ref type="bibr" target="#b15">[16]</ref> performs better than the single modality approaches. However, our single modality (image-based) DeRL method achieves close performance compared to the multi-modal fusion approach. As we can see from the confusion matrix Fig. <ref type="figure">8</ref> on BU-3DFE database, surprise is relatively easy to recognize, showing a 96% recognition rate, while fear has a relatively low recognition rate, and mostly confused with disgust and happiness.</p><p>The BP4D+ <ref type="bibr" target="#b32">[32]</ref> is a multimodal spontaneous emotion corpus (MMSE), including synchronized 3D, 2D, thermal, physiological data sequences (e.g., heart rate, blood pressure, skin conductance (EDA), and respiration rate) from 140 subjects (58 males and 82 females) with ages ranging from 18 to 66 years old. Although the database provides FACS codes, there is no facial expression label available for each frame. In order to evaluate the spontaneous expression database BP4D+, we semi-automatically select 2468 frames from 72 subjects (45 female and 27 male) on four tasks based on the FACS codes. In the experiment, we only use the 2D texture images and four kind of expressions (i.e., happiness, surprise, pain, and neutral). A 10-fold crossvalidation is performed, and the split is subject independent. As we can see in Table.5, our proposed method outperforms the CNN baseline when both training and testing are done on BP4D+.</p><p>To further validate our DeRL approach, we have also conducted a cross-database validation on recognizing four expressions (i.e., happiness, surprise, pain, and neutral). We choose the spontaneous expression database BP4D <ref type="bibr" target="#b31">[31]</ref> for  training, and BP4D+ for testing. Similar to the labeling process on BP4D+, we also labeled 1262 frames from all 41 subjects in the BP4D database based on the AU codes in a semi-automatic way. The experiment result shows that the performance on cross-database validation is lower than the same-database validation, nevertheless, it is still comparable with the CNN baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Classification from Internal Layers</head><p>Internal layers of the generative model have different contributions to the recognition rate. As we can see in Fig. <ref type="figure" target="#fig_8">10</ref>, the local CNN-1 and CNN-2 models show much higher recognition rates than the CNN-3 and CNN-4 . Thus it justifies the λ 1 and λ 2 have bigger weights than λ 3 and λ 4 for combination of total loss in Equation <ref type="bibr" target="#b4">(5)</ref>. Such a combination achieves the highest recognition rate on the individual dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a novel approach for facial expression recognition, which is based on the de-expression residue learning (a.k.a. DeRL). First, a generative model is trained by cGANs to regenerate the neutral face image for any query image. Second, a learning procedure is per- formed on the internal layers of the generative model. The learning procedure is able to capture the expressive component of facial expressions that have been recorded in the generative model.</p><p>Our proposed method was evaluated on both posed and spontaneous expressions datasets. Without exploiting the temporal information, the DeRL method outperforms the baseline CNN methods, the state-of-the-art image-based methods, and even most of the state-of-the-art sequencebased methods that utilize the spatio-temporal information. A cross-database validation is also performed to show the effectiveness of extracting the expressive component from the intermediate layers of the generative model. Such expressive components are extendible for facial action units detection. Our future work will incorporate the expressive components with temporal information for addressing the issues of AU detection, as well as 3D face analysis with a variety of head poses and subtle facial changes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of our proposed method -De-expression Residue Learning (DeRL). A facial expression is the combination of a neutral face image and the expressive component. Our proposed method recognizes facial expression by learning the residual expressive component in the generative model.</figDesc><graphic url="image-1.png" coords="1,308.86,227.72,236.25,178.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Framework of our proposed De-expression Residue Learning (DeRL) method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of De-expression Residue, which are the expressive components for anger, disgust, fear, happiness, sadness, and surprise, from left and right, respectively. The corresponding expressive component histogram is also shown in the second row.</figDesc><graphic url="image-2.png" coords="4,50.11,72.00,495.03,131.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a.k.a. expressive component) of each individual must be recorded in the intermediate layers of the generator. Therefore, we propose the second learning strategy, which is to learn expressions from the intermediate layers of the generator directly. This unique information is also named as De-Expression Residue (see Fig. 3 for examples). As shown in Fig. 2, in order to learn the de-expression residue from the intermediate layers of the generator, all the filters of those layers are fixed, and all the layers that have the same size are concatenated and input into a local CNN model for facial expression classification. For each local CNN model, the cost function is noted as loss i , i ∈ [1, 2, 3, 4]. The last fully connected layers of each local CNN model are further concatenated and combined with the last encode layer for facial expression classification. Consequently, the total loss function is defined as:total loss = λ 1 loss 1 + λ 2 loss 2 + λ 3 loss 3 + λ 4 loss 4 + λ 5 loss 5<ref type="bibr" target="#b4">(5)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Illustration of neutral faces generated by the generative model for the input query images from CK+, Oulu-CASIA, BU-4DFE and BP4D+ databases, respectively.</figDesc><graphic url="image-3.png" coords="5,320.68,72.00,212.62,289.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Confusion matrix on CK+</figDesc><graphic url="image-4.png" coords="6,84.54,256.48,145.54,145.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Confusion matrix on MMI</figDesc><graphic url="image-8.png" coords="7,84.54,238.15,145.54,145.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Confusion matrix of recognizing four expressions on BP4D+</figDesc><graphic url="image-12.png" coords="8,84.54,207.21,145.54,145.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Recognition rates based on individual input parts and their combination.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average accuracy on the CK+ database for seven expressions classification.</figDesc><table><row><cell>Method</cell><cell>Setting</cell><cell>Accuracy</cell></row><row><cell>LBP-TOP [34]</cell><cell>sequence-based</cell><cell>88.99</cell></row><row><cell>HOG 3D [14]</cell><cell>sequence-based</cell><cell>91.44</cell></row><row><cell>3DCNN [17]</cell><cell>sequence-based</cell><cell>85.9</cell></row><row><cell cols="2">STM-Explet [18] sequence-based</cell><cell>94.19</cell></row><row><cell>IACNN [21]</cell><cell>image-based</cell><cell>95.37</cell></row><row><cell>DTAGN [12]</cell><cell>sequence-based</cell><cell>97.25</cell></row><row><cell>CNN (baseline)</cell><cell>image-based</cell><cell>89.50</cell></row><row><cell>DeRF (Ours)</cell><cell>image-based</cell><cell>97.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Average accuracy on the Oulu-CASIA database for six expressions classification.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="2">Setting</cell><cell></cell><cell>Accuracy</cell></row><row><cell cols="3">LBP-TOP [34]</cell><cell></cell><cell cols="3">sequence-based</cell><cell>68.13</cell></row><row><cell cols="3">HOG 3D [14]</cell><cell></cell><cell cols="3">sequence-based</cell><cell>70.63</cell></row><row><cell cols="4">STM-Explet [18]</cell><cell cols="3">sequence-based</cell><cell>74.59</cell></row><row><cell></cell><cell cols="2">Atlases [9]</cell><cell></cell><cell cols="3">sequence-based</cell><cell>75.52</cell></row><row><cell cols="7">DTAGN-Joint [12] sequence-based</cell><cell>81.46</cell></row><row><cell></cell><cell cols="2">FN2EN [6]</cell><cell></cell><cell cols="3">image-based</cell><cell>87.71</cell></row><row><cell></cell><cell cols="2">PPDN [35]</cell><cell></cell><cell cols="3">image-based</cell><cell>84.59</cell></row><row><cell cols="3">CNN (baseline)</cell><cell></cell><cell cols="3">image-based</cell><cell>72.92</cell></row><row><cell cols="3">DeRL (Ours)</cell><cell></cell><cell cols="3">image-based</cell><cell>88.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell></row><row><cell>AN</cell><cell>0.74</cell><cell>0.18</cell><cell>0.04</cell><cell>0.00</cell><cell>0.04</cell><cell>0.00</cell><cell></cell></row><row><cell>DI</cell><cell>0.07</cell><cell>0.84</cell><cell>0.02</cell><cell>0.00</cell><cell>0.06</cell><cell>0.01</cell><cell>0.8</cell></row><row><cell>FE</cell><cell>0.01</cell><cell>0.00</cell><cell>0.82</cell><cell>0.07</cell><cell>0.03</cell><cell>0.07</cell><cell>0.6</cell></row><row><cell>HA</cell><cell>0.00</cell><cell>0.00</cell><cell>0.04</cell><cell>0.96</cell><cell>0.00</cell><cell>0.00</cell><cell></cell></row><row><cell>SA</cell><cell>0.05</cell><cell>0.03</cell><cell>0.01</cell><cell>0.00</cell><cell>0.91</cell><cell>0.00</cell><cell>0.2</cell></row><row><cell>SU</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>1.00</cell><cell></cell></row><row><cell></cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell><cell>0.0</cell></row></table><note>Figure 6. Confusion matrix on Oulu-CASIA</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Average accuracy on the MMI database for six expressions classification.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="2">Setting</cell><cell></cell><cell>Accuracy</cell></row><row><cell cols="3">LBP-TOP [34]</cell><cell></cell><cell cols="3">sequence-based</cell><cell>59.51</cell></row><row><cell cols="3">HOG 3D [14]</cell><cell></cell><cell cols="3">sequence-based</cell><cell>60.89</cell></row><row><cell cols="4">STM-Explet [18]</cell><cell cols="3">sequence-based</cell><cell>75.12</cell></row><row><cell cols="7">DTAGN-Joint [12] sequence-based</cell><cell>70.24</cell></row><row><cell></cell><cell cols="2">IACNN [21]</cell><cell></cell><cell cols="3">image-based</cell><cell>71.55</cell></row><row><cell cols="3">CNN (baseline)</cell><cell></cell><cell cols="3">image-based</cell><cell>57.00</cell></row><row><cell cols="3">DeRL (Ours)</cell><cell></cell><cell cols="3">image-based</cell><cell>73.23</cell></row><row><cell>AN</cell><cell>0.70</cell><cell>0.14</cell><cell>0.00</cell><cell>0.06</cell><cell>0.08</cell><cell>0.02</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell>DI</cell><cell>0.11</cell><cell>0.73</cell><cell>0.01</cell><cell>0.04</cell><cell>0.07</cell><cell>0.05</cell><cell></cell></row><row><cell>FE</cell><cell>0.00</cell><cell>0.11</cell><cell>0.53</cell><cell>0.15</cell><cell>0.06</cell><cell>0.16</cell><cell>0.6</cell></row><row><cell>HA</cell><cell>0.02</cell><cell>0.01</cell><cell>0.01</cell><cell>0.96</cell><cell>0.00</cell><cell>0.01</cell><cell>0.4</cell></row><row><cell>SA</cell><cell>0.09</cell><cell>0.15</cell><cell>0.05</cell><cell>0.02</cell><cell>0.68</cell><cell>0.00</cell><cell>0.2</cell></row><row><cell>SU</cell><cell>0.00</cell><cell>0.04</cell><cell>0.19</cell><cell>0.00</cell><cell>0.02</cell><cell>0.76</cell><cell></cell></row><row><cell></cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell><cell>0.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Average accuracy on the BU-3DFE database for six expressions classification.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell cols="2">Setting</cell><cell></cell><cell>Accuracy</cell></row><row><cell cols="3">Wang et al.[26]</cell><cell></cell><cell>3D</cell><cell></cell><cell></cell><cell>61.79</cell></row><row><cell cols="3">Berretti et al.[3]</cell><cell></cell><cell>3D</cell><cell></cell><cell></cell><cell>77.54</cell></row><row><cell cols="3">Yang et al.[27]</cell><cell></cell><cell>3D</cell><cell></cell><cell></cell><cell>84.80</cell></row><row><cell></cell><cell cols="2">Lo et al.[16]</cell><cell cols="4">2D image + 3D</cell><cell>86.32</cell></row><row><cell></cell><cell cols="2">Lopes [19]</cell><cell></cell><cell cols="2">image-based</cell><cell></cell><cell>72.89</cell></row><row><cell cols="3">CNN (baseline)</cell><cell></cell><cell cols="2">image-based</cell><cell></cell><cell>73.2</cell></row><row><cell cols="3">DeRL (Ours)</cell><cell></cell><cell cols="2">image-based</cell><cell></cell><cell>84.17</cell></row><row><cell>AN</cell><cell>0.83</cell><cell>0.03</cell><cell>0.02</cell><cell>0.01</cell><cell>0.11</cell><cell>0.01</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell>DI</cell><cell>0.06</cell><cell>0.83</cell><cell>0.04</cell><cell>0.03</cell><cell>0.01</cell><cell>0.02</cell><cell></cell></row><row><cell>FE</cell><cell>0.03</cell><cell>0.08</cell><cell>0.72</cell><cell>0.08</cell><cell>0.06</cell><cell>0.03</cell><cell>0.6</cell></row><row><cell>HA</cell><cell>0.02</cell><cell>0.02</cell><cell>0.06</cell><cell>0.89</cell><cell>0.00</cell><cell>0.01</cell><cell></cell></row><row><cell>SA</cell><cell>0.13</cell><cell>0.03</cell><cell>0.02</cell><cell>0.00</cell><cell>0.81</cell><cell>0.01</cell><cell>0.2</cell></row><row><cell>SU</cell><cell>0.00</cell><cell>0.02</cell><cell>0.02</cell><cell>0.01</cell><cell>0.01</cell><cell>0.96</cell><cell></cell></row><row><cell></cell><cell>AN</cell><cell>DI</cell><cell>FE</cell><cell>HA</cell><cell>SA</cell><cell>SU</cell><cell>0.0</cell></row></table><note>Figure 8. Confusion matrix on BU-3DFE</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Average accuracy on the BP4D+ database for four expressions classification.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>Setting</cell><cell>Accuracy</cell></row><row><cell cols="3">CNN (baseline)</cell><cell cols="2">image-based</cell><cell>76.5</cell></row><row><cell></cell><cell cols="2">DeRL (ours)</cell><cell cols="2">image-based</cell><cell>74.41</cell></row><row><cell cols="3">train on BP4D, test on BP4D+</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">DeRL (ours)</cell><cell cols="2">image-based</cell><cell>81.39</cell></row><row><cell cols="3">train and test on BP4D+</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell></row><row><cell>Neutral</cell><cell>0.80</cell><cell>0.05</cell><cell>0.04</cell><cell>0.12</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell></row><row><cell>Happiness</cell><cell>0.00</cell><cell>0.85</cell><cell>0.08</cell><cell>0.06</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell></row><row><cell>Pain</cell><cell>0.07</cell><cell>0.07</cell><cell>0.79</cell><cell>0.07</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell></row><row><cell>Surprise</cell><cell>0.08</cell><cell>0.07</cell><cell>0.05</cell><cell>0.80</cell><cell>0.1</cell></row><row><cell></cell><cell>Neutral</cell><cell>Happiness</cell><cell>Pain</cell><cell>Surprise</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>The material is based upon the work supported by the National Science Foundation under grants CNS-1629898 and CNS-1205664.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing facial actions using gabor wavelets with neutral face average difference</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Lamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="505" to="510" />
		</imprint>
	</monogr>
	<note>Sixth IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A set of selected sift features for 3d facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Berretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bimbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Amor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Daoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="4125" to="4128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding face recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British journal of psychology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="305" to="327" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the recognition of facial identity and facial expression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Neuroscience</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="641" to="651" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Facenet2expnet: Regularizing a deep face recognition net for expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Class Project for Stanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic facial expression recognition using longitudinal facial expression atlases</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="631" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The distributed human neural system for face perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Haxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Gobbini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="223" to="233" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep generative-contrastive networks for facial expression recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07140</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2008-19th British Machine Vision Conference</title>
				<imprint>
			<publisher>British Machine Vision Association</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Intraclass variation reduction using training expression images for sparse representation based facial expression recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N K</forename><surname>Plataniotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Ro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="351" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">An efficient multimodal 2d+ 3d feature-based approach to automatic facial expression recognition. Computer Vision and Image Understanding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morvan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeply learning deformable facial action parts model for dynamic expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="143" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Facial expression recognition with convolutional neural networks: Coping with few data and the training sample order</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De Aguiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F De</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Oliveira-Santos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="610" to="628" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Identity-aware convolutional neural network for facial expression recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017. 2017</date>
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
	<note>Automatic Face &amp; Gesture Recognition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Webbased database for facial expression analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo, 2005. ICME 2005. IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Facial expression decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Ninth IEEE International Conference on</title>
				<meeting>Ninth IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003. 2003</date>
			<biblScope unit="page" from="958" to="965" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3d facial expression recognition based on primitive surface feature distribution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1399" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic 3d facial expression recognition using geometric scattering representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition (FG), 2015 11th IEEE International Conference and Workshops on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A highresolution 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition, 2008. FG&apos;08. 8th IEEE International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic face and gesture recognition, 2006. FGR 2006. 7th international conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse representations for facial expressions recognition via l 1 optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">IEEE Computer Society Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="32" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="692" to="706" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal spontaneous emotion corpus for human behavior analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3438" to="3446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Facial expression recognition from near-infrared videos. Image and Vision Computing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="607" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="915" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Peak-piloted deep network for facial expression recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Photorealistic facial expression synthesis by the conditional difference adversarial autoencoder</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09126</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Face detection, pose estimation, and landmark localization in the wild</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2879" to="2886" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
