<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-stabilizing Iterative Solvers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Piyush</forename><surname>Sao</surname></persName>
							<email>piyush3@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computational Science and Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><surname>Vuduc</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computational Science and Engineering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-stabilizing Iterative Solvers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F21E2D88DD190C069F9D67BBDA87D07</idno>
					<idno type="DOI">10.1145/2530268.2530272</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>fault-tolerance</term>
					<term>iterative linear solvers</term>
					<term>self-stabilization</term>
					<term>transient soft faults</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show how to use the idea of self-stabilization, which originates in the context of distributed control, to make faulttolerant iterative solvers. Generally, a self-stabilizing system is one that, starting from an arbitrary state (valid or invalid), reaches a valid state within a finite number of steps. This property imbues the system with a natural means of tolerating transient faults. We give two proof-of-concept examples of self-stabilizing iterative linear solvers: one for steepest descent (SD) and one for conjugate gradients (CG). Our self-stabilized versions of SD and CG require small amounts of fault-detection, e.g., we may check only for NaNs and infinities. We test our approach experimentally by analyzing its convergence and overhead for different types and rates of faults. Beyond the specific findings of this paper, we believe self-stabilization has promise to become a useful tool for constructing resilient solvers more generally.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Informally, a system is self-stabilizing if it reaches a valid state within a finite number of steps, regardless of its initial state (valid or not). The self-stabilizing property implies the possibility of fault-tolerance: if a transient fault causes the system to enter an invalid state, then the self-stabilizing property ensures the system can eventually return to a valid state. The formal design of self-stabilizing systems, primarily for combinatorial algorithms in protocol design, dates back at least to the seminal work of <ref type="bibr">Dijkstra (1973)</ref>  <ref type="bibr" target="#b1">[1]</ref>. It now appears in many settings, including distributed systems, network routing protocol design, and control theory, to name just a few <ref type="bibr" target="#b2">[2]</ref>. Given the current interest in resilient numerical computing on extreme-scale machines, we are trying to apply self-stabilization to the design of iterative solver algorithms. Whether this idea can work is unknown, but this paper provides an initial "point result" suggesting promise.</p><p>Our basic recipe for applying self-stabilization to numerical algorithm design is roughly as follows. First, we regard the execution of a given solver algorithm as the "system." Its state consists of some or all of the variables that would be needed to continue its execution. Then, we identify a condition that must hold at each iteration for the solver to converge. When the condition holds, the solver is in a "valid" state; otherwise, it is in an invalid state. Finally, we augment the solver algorithm with an explicit mechanism that will bring it from an invalid state to a valid one within a guaranteed finite number of iterations. Under specific assumptions on the type and rates of transient faults ( § 2), we will show our modified solvers are self-stabilizing.</p><p>Why would self-stabilization-if it can be achieved at allbe an attractive paradigm, relative to more established approaches? For instance, we might instead choose traditional checkpointing and algorithm-specific checksumming techniques. (See also § 6.) The basic paradigm in these other methods is to save the state of the system periodically, to detect faults, and when faults occur to restore to a previously correct state. Checksums provide a way to detect when a fault occurs. Critically, these methods rely on accurate fault detection. If a fault goes undetected, it can propagate and lead to incorrect results, or in the worst case, terminate but report the result as correct <ref type="bibr" target="#b3">[3]</ref>. Moreover, the cost of such methods can be prohibitively expensive at high fault rates, given the often high cost of checkpoint, restart, and perhaps also fault detection operations. And in the extreme case that there is some error at every iteration, these classical approaches will most likely fail to make any progress since there is never any correct state to save.</p><p>By contrast, self-stabilization can obviate the need for full state saving and fault detection. When a fault occurs, a selfstabilizing algorithm will by design reach a correct state in a finite number of steps. The valid state might not be identical to some state of a fully fault-free execution; however, the self-stabilizing property ensures convergence, in the absence of additional faults. In the extreme case of some error at every iteration, we show that a self-stabilized iterative solver can still make progress, albeit at a slower rate.</p><p>Due to space limits, this paper omits several proofs and derivations, which our longer technical report contains <ref type="bibr" target="#b28">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TERMS AND MODEL OF RELIABILITY</head><p>In this paper, a fault includes any instance in which the underlying hardware deviates from its expected behavior.</p><p>We will use the taxonomy of faults outlined by Hoemmen and Heroux <ref type="bibr" target="#b7">[7]</ref>, among others, as summarized below.</p><p>We distinguish hard faults and soft faults. A hard fault interrupts the program, causing it to immediately crash or terminate. This occurs when, for instance, a node or network link fails. By contrast, a soft fault does not cause immediate interruption of the program. L1 cache bit flips are a common type of soft fault.</p><p>Soft faults can be further divided into transient or nontransient faults. A transient fault is temporary. Examples include temporarily incorrect output from the floating point unit or a momentary bit flip while reading data from memory. By contrast, non-transient faults are permanent. For instance, suppose the input data stored on disk is corrupted. In this case, reading the data may succeed but the data is wrong on every read.</p><p>This paper concerns only transient soft faults. Thus, in the consequent, a "fault" is a transient soft fault unless otherwise noted. When faults cause the output to fall outside acceptable limits, we say the algorithm has failed.</p><p>For an algorithm to terminate successfully, at least some amount of computation must be done reliably. However, in general we do not have control over which operations are done correctly and which operations are done incorrectly. In this paper, we will attempt to distinguish algorithmic operations that must be performed reliably from those that may be performed unreliably. We refer to these different modes of computation as reliable mode (or reliable computation) from unreliable mode (or computation), without saying precisely how to implement these modes. The prior work of others similarly assumes "selective reliability" <ref type="bibr" target="#b7">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SELF-STABILIZING ITERATIONS</head><p>This section describes and characterizes self-stabilizing iterative solver algorithms and some of their properties.</p><p>We regard the state of the (iterative solver) algorithm at a given point during its execution as a subset of its variable values that is sufficient to restore and resume its execution at that point. The choice of state variables may be a matter of convenience; for instance, a particular choice of variables may have some redundancy in order to reduce the amount of computation needed to restore execution.</p><p>We say the algorithm is in a valid state if, under a faultfree execution of the algorithm from that state, the algorithm still produces a correct result. In the case of an iterative solver, correctness is usually relative to some convergence criterion. As such, the algorithm is in an invalid state if in its subsequent execution from that state it either 1. does not converge or otherwise aborts on an error condition; or 2. converges to an incorrect value.</p><p>In general, these are non-trivial properties to check.</p><p>By its design, an algorithm that starts from some valid state will, in a fault-free execution, remain in a valid state. A fault may cause a transition into an invalid state. A fault for which the algorithm either remains in a valid state or reaches a valid state after a finite number of iterations is a tolerable fault; otherwise, it is intolerable. The solution may, in the presence of tolerable faults, differ from a purely fault-free execution; and an intolerable fault will cause the algorithm to fail. Importantly, the concept of self-stabilization of an algorithm should be described only with respect to class of faults, which motivates the following definition. Definition 1. Self-stabilizing iterative algorithm. An iterative solver algorithm A is self-stabilizing with respect to class of faults F if, when a fault f ∈ F occurs, the algorithm either remains in a valid state or comes to a valid state in after finite number of iterations.</p><p>Under certain conditions, the self-stabilizing property of defn. 1 is equivalent to fault tolerance. When f occurs, a self-stabilizing iterative solver algorithm will return to a valid state after finite number of iterations, and thereby eventually converges. Note that self-stabilization is a weak form of fault-tolerance: it only says the algorithm can reach a valid state, but does not guarantee convergence. This is an inherent limitation of the approach. However, many algorithms have smoothing properties that, when combined with the self-stabilization, may result in robust fault tolerant algorithms.</p><p>Some of the algorithms are inherently self stabilizing with respect to transient soft faults including stationary iterations (Jacobi, Gauss-Seidel, Richardson) and some nonlinear iterative methods (fixed-point iteration, Newton iteration).</p><p>Periodic correction schemes. If the algorithm is not naturally self-stabilizing, the interesting research question is whether we can modify it to be so.</p><p>From the perspective of traditional algorithm-based faulttolerance (ABFT) schemes, we can make an algorithm selfstabilizing by checkpointing a valid state, detecting a fault, and restarting at the most recent valid state when a fault is detected. If it is not possible to do efficient fault detection and checkpoint/restart, we may seek a different approach.</p><p>A natural idea is to introduce a periodic correction step. For a given algorithm, we seek a correction step such that when it executes, either (a) if the algorithm is in a valid state, it remains in a valid state; or (b) if the algorithm is in an invalid state, the correction moves it into a valid one. By applying the correction step after a fixed (bounded) number of iterations, we may effectively bound the number of iterations to reach a valid state. Thus, such a periodically corrected algorithm meets the self-stabilizing criteria of defn. 1.</p><p>There are at least two more important issues in designing a correction step. The first is its cost. Since it is extra computation, we seek corrections that are cheap or may otherwise be applied at infrequent (but still fixed, per the finite steps requirement) intervals. The second is the reliability mode when executing the step. In general, we will assume the selective reliability model in which the correction step executes in guaranteed reliable mode. However, it is possible that for infrequent faults, this restriction may be relaxed.</p><p>To show corrections are possible, below we present two case studies in the context of solving Ax = b. They show how to transform the standard formulations of steepest descent ( § 3.1) and conjugate gradients ( § 3.2) methods for solving Ax = b into self-stabilizing forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-stabilizing Steepest Descent</head><p>We show the standard form of steepest descent in alg. 1. In this method, the solution of the N -dimensional system, Ax = b is the solution to an optimization problem. Specifically, it seeks to minimize the N -dimensional paraboloid,</p><formula xml:id="formula_0">F (x) = 1 2 x T Ax -x T b.</formula><p>In each step, the steepest descent Algorithm 1 Steepest descent for solving the symmetric positive definite system, Ax = b</p><p>Require: A, b, x 0 (Initial guess), and (threshold)</p><formula xml:id="formula_1">1: i=0 ; r 0 = b -Ax 0 ; x i = x 0 ; 2: r i 2 = r 0 2 = r T i r i ;</formula><p>3: while ||r i || &gt; 1 × r 0 do 4:</p><formula xml:id="formula_2">q i = Ar i ;</formula><p>5:</p><formula xml:id="formula_3">α i = r i 2 /(r T i q i ) ;</formula><p>6:</p><formula xml:id="formula_4">x i+1 = x i + α i r i ;</formula><p>7:</p><formula xml:id="formula_5">r i+1 = r i -α i q i ;</formula><p>8:</p><formula xml:id="formula_6">r i+1 2 = r T i+1 r i+1 ;</formula><p>9:</p><formula xml:id="formula_7">i = i + 1;</formula><p>10: end while 11: return x i algorithm finds the direction of maximum descent along F . When A is symmetric positive definite (SPD), this direction at iteration k is the residual, r k = b -Ax k , where x k is the current approximate solution; the algorithm calculates a step length α in the direction of r k so as to minimize</p><formula xml:id="formula_8">F (x k + αr k ) and computes a new estimate x k+1 ← x k + αr k .</formula><p>We may represent the state of alg. 1 at iteration k by (x k , r k ). In a fault-free execution, the state variables satisfy</p><formula xml:id="formula_9">r k = b -Ax k .<label>(1)</label></formula><p>One can show alg. 1 only converges to x when eq. ( <ref type="formula" target="#formula_9">1</ref>) holds. If eq. ( <ref type="formula" target="#formula_9">1</ref>) does not hold, then perturbation theory tells us that the algorithm solves a different problem, Ax = b, where b</p><formula xml:id="formula_10">= r k + Ax k . Hence, (x k , r k ) is a valid state if r k -b + Ax k 2 &lt; 1 r0 2<label>(2)</label></formula><p>where 1 determines our acceptable convergence criterion, as given in line 3 of alg. 1. Algorithm 1 is not self-stabilizing. In particular, suppose a fault occurs at line 4. Then, eq. ( <ref type="formula" target="#formula_9">1</ref>) no longer holds. If the fault is such that eq. ( <ref type="formula" target="#formula_10">2</ref>) is still satisfied, the algorithm remains in a valid state. However, it is also possible the algorithm will instead reach an invalid state and will continue to remain in an invalid state. Thus, the algorithm will fail.</p><p>To avoid reaching an invalid state, we need to restore the relation of eq. ( <ref type="formula" target="#formula_9">1</ref>). This can be done by forcing r k to be equal to b -Ax k , as shown by the correction step on line 5 of the modified algorithm, alg. 2.</p><p>This correction costs one extra matrix-vector product in reliable mode. Assuming line 4 of alg. 1 is the most expensive, the correction can cost much more than double the cost if reliable mode is costs much more than unreliable mode. As such, we guard the correction step as line 4 of alg. 2 indicates. This test ensures that we execute the correction only once every F iterations, for some fixed F . This frequency is a tuning parameter that depends on the largest possible error of the fault and the condition number of A. 1  Since line 5 of alg. 2 enforces the validity condition of eq. ( <ref type="formula" target="#formula_9">1</ref>) and since the F parameter of line 4 provides a way to bound the number of iterations to move from an invalid to valid state, we may conclude the algorithm is self-stabilizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Self-stabilizing Conjugate Gradient</head><p>1 If the error of the fault can be bounded, we may be able to estimate F . For instance, rounding errors alone suggest correcting every O( √ N ) iterations <ref type="bibr" target="#b10">[10]</ref>. Thus, we should expect to correct more frequently than that. Algorithm 2 Self-stabilizing steepest descent. The parameter F is the correction step frequency in iterations.</p><p>Require: A, b, and x 0 (Initial guess), F ,</p><formula xml:id="formula_11">1: i=0; r i = b -Ax 0 ; x i = x 0 2: r i 2 = r 0 2 = r T r ; 3: while ||r i || &gt; × r 0 do 4:</formula><p>if i ≡ 0 (mod F ) then Do Reliably 5:</p><formula xml:id="formula_12">r i = b -Ax i 6: end if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>q i = Ar i ;</p><p>8:</p><formula xml:id="formula_13">α i = r i 2 /(r T i q i ) ;</formula><p>9:</p><formula xml:id="formula_14">x i+1 = x i + α i r i ;</formula><p>10:</p><formula xml:id="formula_15">r i+1 = r i -α i q i ;</formula><p>11:</p><formula xml:id="formula_16">r i+1 2 = r T i+1 r i+1 ;</formula><p>12:</p><formula xml:id="formula_17">i = i + 1;</formula><p>13: end while 14: return x i Algorithm 3 The conjugate gradient (CG) algorithm for the SPD system, Ax = b</p><p>Require: A, b, x 0 (initial guess), , and M (max iterations)</p><formula xml:id="formula_18">1: r 0 = b -Ar 0 ; p 0 = r 0 ; i = 0 2: while norm(r i ) &gt; • norm(r 0 )</formula><p>and i &lt; M do 3:</p><formula xml:id="formula_19">q i = A × p i ;</formula><p>4:</p><formula xml:id="formula_20">α i = r i 2 /(p T i q);</formula><p>5:</p><formula xml:id="formula_21">x i+1 = x i + α × p i ;</formula><p>6:</p><formula xml:id="formula_22">r i+1 = r i -αq;</formula><p>7:</p><formula xml:id="formula_23">β = r i+1 2 / r i 2 ;</formula><p>8:</p><formula xml:id="formula_24">p i+1 = r i+1 + βp i ;</formula><p>9:</p><formula xml:id="formula_25">i = i + 1;</formula><p>10: end while 11: return x Conjugate gradient (CG) is another descent-type algorithm for SPD systems. In principle, CG converges faster than steepest descent; and in exact arithmetic, CG converges in a finite number of iterations. A typical formulation of CG appears in alg. <ref type="bibr" target="#b3">3</ref>.</p><p>The CG method maintains a search direction p k . Here r k is the residual and p k is the new search direction, given by</p><formula xml:id="formula_26">p k+1 = r k+1 + βp k , p0 = r0.<label>(3)</label></formula><p>The coefficient β is chosen so that p T k+1 Ap k = 0 corresponding to line number <ref type="bibr" target="#b7">(7)</ref> in alg. 3. In each step x k advances by α k p k , where α k is chosen so that it minimizes F (x k + α k p k ) corresponding to line number (4) in the Algorithm <ref type="bibr" target="#b3">(3)</ref>.</p><p>In a fault-free execution of alg. 3, three global orthogonality relations hold:</p><formula xml:id="formula_27">p T i Apj = 0 if i = j; r T i rj = 0 if i = j; and r T i pj = 0 if i &gt; j.<label>(4)</label></formula><p>To restore a particular state in which the relations of Equation (4) hold, one would have to store all previous search direction and residuals, which is not possible since storing them would require a lot of additional storage. However, in practice one can quickly lose these relations even under finite-precision arithmetic. Since we are interested in restoring the algorithm to a state which can still converge to a correct solution, we can relax some of these conditions.</p><p>In particular, we may invoke the following result from nonlinear CG <ref type="bibr" target="#b4">[4]</ref>: Theorem 1. Starting from any state (x0, r0, p0) such that Algorithm 4 Self-stabilizing conjugate gradient (SS-CG) Require: A, b, x 0 (initial guess), , M , and F (correction frequency)</p><p>1: r 0 = b -Ar 0 ; x = x 0 ; p 0 = r 0 ; i = 0 2: while norm(r i ) &gt; • norm(r 0 ) and i &lt; M do 3:</p><formula xml:id="formula_28">if i ≡ 0 (mod F ) then Do Reliably 4: [r i , q] = A × [x i , p i ]</formula><p>May read A just once 5:</p><formula xml:id="formula_29">r i = b -r i 6: α i = r T i p i /(p T i q);</formula><p>7:</p><formula xml:id="formula_30">x i+1 = x i + α × p i ;</formula><p>8:</p><formula xml:id="formula_31">r i+1 = r i -αq;</formula><p>9:</p><formula xml:id="formula_32">β = -r T i+1 q i /(p T i q i );</formula><p>10:</p><formula xml:id="formula_33">p i+1 = r i+1 + βp i ;</formula><p>11:</p><p>else 12:</p><formula xml:id="formula_34">q i = A × p i ;</formula><p>13:</p><formula xml:id="formula_35">α i = r i 2 /(p T i q i );</formula><p>14:</p><formula xml:id="formula_36">x i+1 = x i + α × p i ;</formula><p>15:</p><formula xml:id="formula_37">r i+1 = r i -αq;</formula><p>16:</p><formula xml:id="formula_38">β = r i+1 2 / r i 2 ;</formula><p>17:</p><formula xml:id="formula_39">p i+1 = r i+1 + βp i ;</formula><p>18:</p><p>end if</p><p>19:</p><formula xml:id="formula_40">i = i + 1;</formula><p>20: end while 21: return x r0 = b -Ax0 and r T 0 p0 = 0, CG converges if the following relations are maintained for k ≥ 0:</p><formula xml:id="formula_41">1. r k = b -Ax k ; 2. p T k r k / p k r k &gt; c1 for some c1 if ||r k || = 0; and 3. α k is chosen to minimize F (x k + αp k ), i.e., α k = r T k p k /p T k Ap k .</formula><p>Note that Theorem 1 is just one possible set of sufficient conditions that one can use to construct a correction step.</p><p>Suppose we choose to restore these conditions. Then, we may choose the quadruplet (x k , r k , p k , α k ) as the state variable and construct the correction step as follows.</p><p>First, restore the residual condition by calculating r k = b -Ax k explicitly and check for convergence.</p><p>Next, check the validity of condition (2) in thm. 1. If r k and p k are approximately orthogonal, as occurs when r T k p k &lt; 1 r k p k , then we set p k = r k .</p><p>Then, choose α k to minimize F (x k + α k p k ), as occurs if</p><formula xml:id="formula_42">α k = r T k p k /p T k Ap k .<label>(5)</label></formula><p>In a fault-free execution, eq. ( <ref type="formula" target="#formula_42">5</ref>) and line 4 of alg. 3 are equivalent. However, if a fault does occur they are no longer equivalent, hence, expression in line 4 of alg. 3 no longer has the optimality property of eq. ( <ref type="formula" target="#formula_42">5</ref>), thereby violating condition (3) of thm. 1.</p><p>Lastly, we have several choices for updating p k . We choose to do so by a particular choice of β that enforces A-orthogonality of p k and p k+1 :</p><formula xml:id="formula_43">p k+1 = r k+1 + βp k , β = -p T k Ar k+1 /p T k Ap k<label>(6)</label></formula><p>We choose this form for two reasons. First, it will guarantee that our overall self-stabilizing version of CG is mathematically equivalent to CG during a fault-free execution. Secondly, we want to be able to prove the self-stabilizing property holds and, moreover, maintain at least a few local orthogonality properties (see thm. 2 below). Combining these four state variable correction steps together, the new and complete algorithm is alg. 4. Note that alg. 4 requires a fused matrix-vector product in reliable mode. However, in modern architecture cost of 2 fused matvecs are same as one <ref type="bibr" target="#b26">[26]</ref>. Now we can state the following local orthogonality relations, which the correction step maintains.</p><p>Theorem 2. Let alg. 4 start in any state, (x0, r0, p0). If a correction is performed in the k th iteration and all subsequent iterations are fault-free, then the following local orthogonality conditions will be true (in exact arithmetic):</p><formula xml:id="formula_44">r l = b -Ax l if l &gt; k p T l+1 Ap l = 0 if l ≥ k r T l+1 r l = 0 if l &gt; k r T l+1 p l = 0 if l ≥ k r T l Ap l = p T l Ap l if l &gt; k r T l p l = r l 2 if l &gt; k p T l r l / p l r l &gt; λ1/λn if l &gt; k<label>(7)</label></formula><p>Due to space constraints, a full proof appears in our technical report <ref type="bibr" target="#b28">[28]</ref>.</p><p>We need to show that alg. 4 is indeed self-stabilizing. We have already proven condition (1) of thm. 1. Equation <ref type="bibr" target="#b7">(7)</ref> validates condition (2) of thm. 1. Lastly, the choice of α l = r T l r l /r T l Ap l = r T l p l /p T l Ap l retains the minization condition (3) of thm. 1. Hence, by thm. 1, alg. 4 will converge to the solution of Ax = b.</p><p>In the presence of faults, self-stabilizing CG loses the global orthogonality properties of fault-free CG, and therefore also the finite termination property of Krylov subspace methods. However, it at least maintains local orthogonality and optimality properties as argued above. Note that these local relations hold in case of inexact Krylov subspace based methods <ref type="bibr" target="#b5">[5]</ref>. While the finite termination property is in theory highly desirable, our experiments will show that good rates of convergence are nevertheless possible without it, even when fault rates are relatively high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NUMERICAL EXPERIMENTS</head><p>We performed a series of numerical experiments in Matlab to test the robustness of the self-stabilizing algorithms in § 3. We focus on CG and omit results for steepest descent due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Fault Injection Methodology</head><p>Since all steps of CG depend on the matrix-vector product, we emulate transient soft faults by injecting bit flips into this operation. In particular, at the beginning of each iteration of CG, we start with the uncorrupted matrix A, flip some of its bits in an independent and uniformly random way, perform the (possibly corrupted) matrix-vector product, and then restore the uncorrupted A (since we assume transient errors). Since a fault in the matrix-vector product propagates to most of CG's other variables, considering errors here is sufficient to corrupt the entire algorithm's results.</p><p>Our bit flipping procedure assumes independence between bits and iterations. Specifically, we model bit flips as Bernoulli trials, with each bit of the numerical values of A flipping independently with a uniform probability P . Depending on the value of P compared to the total number of bits in the matrix values, a given matrix-vector product may contain no bit flips in a given iteration when P is small or multiple bit flips in every iteration if P is large.</p><p>In each step we check for not-a-number (NaN) and infinity (Inf) values in the result of the matrix-vector product Table <ref type="table">1</ref>: Different problems used for experimentation and in the residual ri and replace those values with random numbers. This check is the only form of fault-detection that we use in our self-stabilizing algorithm.</p><p>Lastly, we assume reliable mode for the matrix-vector product of the correction step, as assumed by others ( § 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solvers</head><p>Our evaluation compares several solver methods. Fully reliable CG ( Error Free CG). This method is standard CG (alg. 3) where all computation is done in reliable mode. That is, it assumes no faults; as such, it will generally require the fewest iterations to converge relative to other CG-type methods but has no built-in fault-tolerance.</p><p>Restarted CG ( CG-Res). This method is a known variant of CG and might be a natural choice for fault-tolerance. The basic idea in CG-Res is simply to reset the search direction p k periodically to be the steepest descent direction. The intuitive aim of doing so is to compensate for any error that may have accumulated in p k . This approach also makes CG-Res a type of self-stabilizing algorithm, if we assume the restart step is done in reliable mode. However, CG-Res is not mathematically equivalent to standard CG. Restarting CG usually slows the rate of convergence. One may also view CG-Res as a type of checkpoint/restart method in which only x k is checkpointed. (It is not possible to know that p k is a correct direction.) As such, it is a useful baseline for comparison against our method.</p><p>Self-stabilizing CG ( CG-Ss). This is our algorithm of alg. 4. Recall that only the correction step (every F = 10 iterations) runs in reliable mode.</p><p>Fault-tolerant GMRES ( Ft-GMRES). This method is based on the inner-outer iteration paradigm <ref type="bibr" target="#b7">[7]</ref>. Outer iterations use GMRES and run in reliable mode. Inner iterations run unreliably and may use any solver; we tried both standard CG and GMRES; we report results on each test problem using whichever yielded the fewest total iterations.</p><p>Regarding our use of Ft-GMRES, we make two qualifying remarks. First, for SPD systems, GMRES and CG generally exhibit similar convergence rates in a fault-free execution. However, a practitioner will often prefer CG over GMRES because GMRES-type methods carry a significantly higher storage overhead. Secondly, Ft-GMRES is really a preconditioned method, whereas we are comparing to CG methods (including our CG-Ss) without preconditioning. Thus, when comparing convergence and iteration rates, one should use caution. Nevertheless, we include Ft-GMRES as it is the closest comparable method with built-in fault-tolerance of which we are aware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Test problems</head><p>We choose 3 different test problems, summarized in table 1.</p><p>The first test problem is K3D comes from finite difference discretization of the 3D poisson equation on a cubic domain of size 30 × 30 × 30.</p><p>The second test problem is DIAG, a diagonal matrix of size N = 10, 000 with each of its entries given by</p><formula xml:id="formula_45">D(i) = 100 + 1 + 10 k (i -1)/N ,</formula><p>where N is the size of the matrix and k is a conditioning parameter. (We use k = 16 in table 1.) To generate a righthand side, we generate a random vector and multiply by it; note that this method allows us to measure absolute error. Furthermore, a diagonal test problem is useful because it permits easy explicit control of the spectrum, and thus convergence rates, by tuning k <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b7">7]</ref>.</p><p>The third test problem is THERMAL1, which is #1402 from the University of Florida Sparse Matrix Collection <ref type="bibr">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiments</head><p>For each test problem, we ran two sets of experiments, each considering a variation on silent data corruption <ref type="bibr" target="#b3">[3]</ref>.</p><p>1. We allow bit flips to occur in any part of the floatingpoint value.</p><p>2. We allow bit flips only in the mantissa and in the sign bits, but not the exponent.</p><p>In the first case, bit flips in the exponent cause unbounded errors and will be much harder to tolerate. However, there may be efficient ways to detect these cases or to apply circuitlevel selective reliability to exponent computations.</p><p>In light of possible special measures for exponent corruption, the second case of mantissa and sign bit flips is worth considering separately. We might expect this second case when using, for example, probabilistic circuits, where we might compute the mantissa using lower-voltage powersaving circuits at the cost of increased unreliability <ref type="bibr" target="#b9">[9]</ref>. This subclass of faults is insidious in that it is harder to detect than exponent corruption; and even if detectable, it likely requires recomputation. However, a nice intellectual aspect of mantissa errors is that we may be able to bound the error analytically. We offer such an analysis in § 5.</p><p>For the first case, we choose the bit-flip probability to yield an expected value of 4 bit flips in each unreliable matrixvector product; and for the second case, 40 bit flips. These error rates are extremely high relative to estimates in the literature; however, it allows us to consider the case of high degrees of scaling, in which case problem size increases would also increase the number and frequency of bit flips.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>We compare the four solvers on each of the 3 test problems and two bit-flip scenarios. We separately assess convergence rates, measured in number of iterations to reach a given accuracy, and number of reliable matrix-vector products required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Convergence Rate</head><p>Figure <ref type="figure">1</ref> compares the observed convergence history of the four algorithms. For Ft-GMRES, we count iterations as cumulative of each inner iteration as a function of the number of unreliable matrix-vector products. Each column of plots is for a given test problem (K3D on left, THERMAL1 in the middle, and DIAG on the right), and each row considers a bit flip scenario (mantissa and sign bits only on the top, and all bits including the exponent on the bottom).</p><p>Consider first the top row of plots, in which only mantissa and sign bit flips occur. Recall the probability of bit flips is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fraction of Reliable SpMV</head><p>Error Free CG CG-SS CG-RES FT-GMRES (a) Unbounded errors (including exponent bit flips) Figure <ref type="figure">2</ref>: The fraction of matrix-vector products (with respect to matrix-vector products required by Error Free CG) must run in reliable mode (y-axis) to achieve some accuracy (x-axis).</p><p>chosen so that the expected number of bit flips per unreliable matrix-vector product is 40 flips. This rate is relatively high and serves as a stress-test for all the methods. Observe that even by doing only a small fraction (F = 10 or 10%) of matrix-vector products in reliable mode, the convergence of CG-Ss degrades relative to Error Free CG by no more than twice the number of unreliable matrix-vector products at the same accuracy level. For K3D, CG-Ss is much better than CG-Res; in the other two cases, it is comparable. Ft-GMRES outperforms CG-Ss on THERMAL1, where its extra preconditioning confers an advantage, but underperforms on DIAG.</p><p>Next consider the second row of fig. <ref type="figure">1</ref>, in which we include exponent bit flips, i.e., unbounded errors. (Note that the scale on the x-axes changes from the first row.) As expected, the number of iterations as measured by unreliable matrixvector products increases, but the qualitative observations of the left column largely hold. As such, we see CG-Ss has the potential to withstand unbounded faults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Amount of reliable computation required</head><p>For all the algorithms considered, we need to assume some degree of reliable-and presumably more expensive-computation. This section assesses the number of reliable matrix-vector products required relative to the total number of matrixvector products. Error Free CG, which computes entirely in reliable mode, provides an upper-bound.</p><p>Figure <ref type="figure">2</ref> shows the fraction of reliable computation needed to achieve the given error tolerance for unbounded errors for K3D problem. (The lower-bound on number of reliable computations is 1/F where F is the correction step frequency.) The bit flip errors are the same as in the experiments above. For unbounded errors, CG-Ss needs to run just 30% of the upper bound on reliable matvecs that Error Free CG needs; and for bounded errors, just 20% (not shown here). In both cases, the fraction seems to approach some asymptotic limit. Though not shown explicitly here, it can be inferred from fig. <ref type="figure">1b</ref> and fig. <ref type="figure">1c</ref> that CG-Ss is only very slightly above very slightly above the lower bound of 10% when F = 10. Below, we will see that this bound can be even lower as the fault rate decreases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">ANALYSIS</head><p>We may derive an analytical relationship among correction step frequency, fault rate, and properties of the linear system when errors are bounded (mantissa and sign bit flips). Our analysis is conservative as it invokes coarse bounds; however, it may still be useful in guiding the choice of parameters. The key tool in the analysis is that bounded faults allow the use of results from inexact Krylov subspace methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b6">6]</ref>. For interested readers, a full derivation appears in our technical report <ref type="bibr" target="#b28">[28]</ref>. Here, we summarize the key relation and verify it experimentally.</p><p>Let f be the largest allowable number of iteration between correction steps. If we assume linear convergence model as</p><formula xml:id="formula_46">||r i || ||r 0 || = ζ i for some ζ, then it can be shown that f = O log ζ (κ(A) √ η) . (<label>8</label></formula><formula xml:id="formula_47">)</formula><p>where η is number of bit flips in every matrix vector product and κ(A) is condition number of matrix.</p><p>We can see the key scaling relations among i, κ(A), and η: i depends on the number of bit flips, η, logarithmically. Thus, doubling the bit flip rate reduces i by just -1 2 log ζ 2. Figure <ref type="figure">3</ref> verifies this analysis on K3D. Specifically, recall that eq. ( <ref type="formula" target="#formula_46">8</ref>) predicts the minimum correction frequency needed to attain a desired convergence profile, i.e., decrease in the residual as iterations increase. Thus, we use eq. ( <ref type="formula" target="#formula_46">8</ref>) to compute i and set the correction frequency F in CG-Ss to it. <ref type="foot" target="#foot_0">2</ref> We then observe the actual convergence profiles as the fault rate increases. Each line in fig. <ref type="figure">3</ref> corresponds to a different value of η and the corresponding F predicted by eq. ( <ref type="formula" target="#formula_46">8</ref>).</p><p>These results reveal two attractive aspects of CG-Ss. First, fig. <ref type="figure">3a</ref> suggests that eq. ( <ref type="formula" target="#formula_46">8</ref>) is conservative: the convergence actually improves with a decrease in fault rate, rather than staying the same. This aspect of CG-Ss is attractive because it implies CG-Ss can in principle gracefully adapt to the fault rate. Secondly, the overhead also gracefully decreases as the fault rate decreases. Figure <ref type="figure">3b</ref> shows that as the fault rate decreases, if we adapt the correction frequency we can reduce the fraction of reliable matrix-vector products required. Taken together, these observations point to an important direction for future work, namely, how to exploit adaptivity automatically.</p><p>Lastly, observe that as the error tolerance limit decreases, CG-Ss stagnates, e.g., green squares in fig. <ref type="figure">1</ref>. This behavior depends on the problem and may or may not be acceptable. It suggests we need deeper analysis of the relationship between numerical properties and stability of CG-Ss and its ilk. We leave this possibility as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Our work is most directly inspired by the Ft-GMRES method of Hoemmen and Heroux <ref type="bibr" target="#b7">[7]</ref>, which we included in our comparisons of § 4. Other than our focus on the SPD case in this paper, we view both CG-Ss and Ft-GMRES as instances of self-stabilization. We believe the more direct connection to the lens of self-stabilization adds a new dimension to this class of methods.</p><p>Complementary to our approach, there is a large and growing literature on algorithm-based fault-tolerance (ABFT) (b) Number of reliable (sparse) matrix-vector products ("SpMV") for different fault rates Figure <ref type="figure">3</ref>: Comparison of the predicted correction step frequency to the observed behavior of CG-Ss at varying fault rates (different lines). "BF" = bit flips; "SpMV" = (sparse) matrix-vector product. For each line, the value in parentheses is the estimated critical i from eq. ( <ref type="formula" target="#formula_46">8</ref>) at the corresponding bit flip rate.</p><p>techniques, primarily by testing checksum invariants <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24]</ref>, including for preconditioned CG <ref type="bibr" target="#b18">[18]</ref>. These work well when faults are very small. Zizhong et al <ref type="bibr" target="#b19">[19]</ref> use orthogonality relations instead of checksums. However, the cost of this method grows quickly with increasing fault rates. The theory of inexact Krylov subspace methods suggests that many of these methods could further exploit other intrinsic solver properties, to reduce the amount of recomputation involved after detecting an error <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b12">12]</ref>. Since selfstabilizing does not use fault detection, future work could study hybrid ABFT and self-stabilizing methods. Oboril et al. suggest doing CG iterations on the residual if convergence slows or stops due to faults <ref type="bibr" target="#b14">[14]</ref>. Their method is essentially CG-Res, against which we compared.</p><p>Lastly, there is a long history of asynchronous iteration techniques <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b27">27]</ref>. However, these are primarily based on stationary iteration, whereas Krylov methods tend to have more favorable convergence rates and (in exact arithmetic) finite termination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION AND FUTURE WORK</head><p>Self-stabilization expands our collective repertoire of resilience techniques. We believe the abstraction of state transitions and correction steps is a useful lens for viewing the problem of how to design resilient solvers. Furthermore, by formally connecting the problem to the "orthogonal" literature of self-stabilizing algorithms may permit many new and creative approaches to the design of resilient solvers broadly <ref type="bibr" target="#b2">[2]</ref>. We sketch some directions below.</p><p>A tighter and broader model of convergence. The analysis of § 5 is useful but coarse, being limited to easily bounded errors. Exploring this problem and others, in a more deeply theoretically way, is an important direction for future work. For instance, the theory of inexact Krylov methods includes results that permit errors in each matrix-vector product to gradually increase <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b5">5]</ref>, in part because errors in later iterations have a reduced impact. This theory might permit adaptive correction step frequencies, for instance. In addition, the numerical stability and convergence rate properties of our CG-Ss method is only partly understood.</p><p>The balance between reliable and unreliable computation. One major finding of our study is the tradeoff between reliable and unreliable computation. This raises numerous questions, such as whether there is a fundamental lower bound on amount of reliable computation. Additionally, we do not yet have a good model of the difference in the time, energy, and power costs between reliable and unreliable mode, as has been suggested in numerous hardware and circuit-level analyses <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>Self-stabilizing other algorithms. Another direction for future work is to design self-stabilized solvers under preconditioning or for other solvers altogether. We expect the results of this paper extend to other CG-like methods, such as biconjugate gradients (BiCG), BiCGStab, CG-squared, and Chebyshev iterations, among others <ref type="bibr" target="#b13">[13]</ref>.</p><p>Hybridized fault-tolerance techniques. As noted in § 4.4, self-stabilization may fruitfully complement ABFT and other techniques. Such an approach may be useful for unbounded faults, as occur with exponent bit flips.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Error</head><label></label><figDesc>Tolerance versus #Reliable SpMV E rror tol e ranc e r n 2 / r 0 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :E rror tol e ranc e r n 2 / r 0 2 Fraction</head><label>122</label><figDesc>Figure 1: This figure shows the convergence history for different problems: K3D (first column), THERMAL1 (second column), and DIAG (last column). The top plots include only sign and mantissa bit flips at a rate of 40 bit flips in every unreliable matrix-vector product, while the bottom include exponent bit flips at a rate of 4 bit flips per unreliable matvec.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We analyzed the fault-free case to chose constants, e.g., ζ = 0.74.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Mr. Aftab Patel and Prof. Edmond Chow for early discussions of this work. This work was supported in part by the National Science Foundation (NSF) under NSF CAREER award number 0953100.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-stabilizing systems in spite of distributed control</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="643" to="644" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-stabilization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dolev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Soft Error Vulnerability of Iterative Linear Algebra Methods</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bronevetsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>De Supinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS 2008</title>
		<imprint>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Zoutendijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Programming, Computational Methods. Integer and nonlinear programming</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="37" to="86" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Inexact Krylov subspace methods for linear systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V D</forename><surname>Eshof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L G</forename><surname>Sleijpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="125" to="153" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Szyld Theory of inexact Krylov subspace methods and applications to scientific computing</title>
		<author>
			<persName><forename type="first">V</forename><surname>Simoncini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="454" to="477" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fault-tolerant iterative methods via selective reliability</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoemmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Heroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<meeting>the 2011 International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">University of Florida sparse matrix collection</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low power probabilistic floating point multiplier design</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Annual Symposium on VLSI (ISVLSI&apos;2011</title>
		<meeting>IEEE Computer Society Annual Symposium on VLSI (ISVLSI&apos;2011</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific and statistical computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="856" to="869" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inexact preconditioned conjugate gradient method with inner-outer iteration</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1305" to="1320" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Barrett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Numerical Defect Correction as an Algorithm-Based Fault Tolerance Technique for Iterative Solvers</title>
		<author>
			<persName><forename type="first">F</forename><surname>Oboril</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE 17th Pacific Rim International Symposium on Dependable Computing (PRDC&apos;2011</title>
		<meeting>IEEE 17th Pacific Rim International Symposium on Dependable Computing (PRDC&apos;2011</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DRAM errors in the wild: a large-scale field study</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh international joint conference on Measurement and modeling of computer systems</title>
		<meeting>the eleventh international joint conference on Measurement and modeling of computer systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Software-based replication for fault tolerance</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schiper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithm-based fault tolerance for matrix operations</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="518" to="528" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fault tolerant preconditioned conjugate gradient for sparse linear system solution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Srinivasmurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM international conference on Supercomputing</title>
		<meeting>the 26th ACM international conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online-ABFT: an online algorithm based fault tolerance scheme for soft error detection in iterative methods</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</title>
		<meeting>the 18th ACM SIGPLAN symposium on Principles and practice of parallel programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Iterative methods for linear and nonlinear equations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Kelley</surname></persName>
		</author>
		<idno>MR 96d 65002</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>SIAM, Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A linear algebraic model of algorithm-based fault tolerance</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Anfinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1599" to="1604" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Algorithm-based fault tolerance for matrix inversion with maximum pivoting</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of parallel and distributed computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="373" to="389" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An analysis of algorithm-based fault tolerance techniques</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">T</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="172" to="184" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithm-based fault tolerance for dense matrix factorizations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming</title>
		<meeting>the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Parallel iterative algorithms: from sequential to grid computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Vivier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Couturier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving the Performance of Dynamical Simulations Via Multiple Right-Hand Sides</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 26th International Parallel &amp; Distributed Processing Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A block-asynchronous relaxation method for graphics processing units</title>
		<author>
			<persName><forename type="first">H</forename><surname>Anzt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Self-stabilizing iterative solvers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology, School of Computational Science and Engineering</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
