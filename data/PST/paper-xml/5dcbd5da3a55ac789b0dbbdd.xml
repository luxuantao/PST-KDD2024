<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TENER: Adapting Transformer Encoder for Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-10">10 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University Shanghai Key</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bocao</forename><surname>Deng</surname></persName>
							<email>dengbocao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University Shanghai Key</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaonan</forename><surname>Li</surname></persName>
							<email>lixiaonan1208@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University Shanghai Key</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
							<email>xpqiu@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="laboratory">Laboratory of Intelligent Information Processing</orgName>
								<orgName type="institution" key="instit1">Fudan University Shanghai Key</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TENER: Adapting Transformer Encoder for Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-10">10 Dec 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.04474v3[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bidirectional long short-term memory networks (BiLSTMs) have been widely used as an encoder for named entity recognition (NER) task. Recently, the fully-connected self-attention architecture (aka Transformer) is broadly adopted in various natural language processing (NLP) tasks owing to its parallelism and advantage in modeling the longrange context. Nevertheless, the performance of the vanilla Transformer in NER is not as good as it is in other NLP tasks. In this paper, we propose TENER, a NER architecture adopting adapted Transformer Encoder to model the character-level features and wordlevel features. By incorporating the directionaware, distance-aware and un-scaled attention, we prove the Transformer-like encoder is just as effective for NER as other NLP tasks. Experiments on six NER datasets show that TENER achieves superior performance than the prevailing BiLSTM-based models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The named entity recognition (NER) is the task of finding the start and end of an entity in a sentence and assigning a class for this entity. NER has been widely studied in the field of natural language processing (NLP) because of its potential assistance in question generation <ref type="bibr" target="#b40">(Zhou et al., 2017)</ref>, relation extraction <ref type="bibr" target="#b24">(Miwa and Bansal, 2016)</ref>, and coreference resolution <ref type="bibr" target="#b9">(Fragkou, 2017)</ref>. Since <ref type="bibr" target="#b5">(Collobert et al., 2011)</ref>, various neural models have been introduced to avoid hand-crafted features <ref type="bibr" target="#b16">(Huang et al., 2015;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016)</ref>.</p><p>NER is usually viewed as a sequence labeling task, the neural models usually contain three components: word embedding layer, context encoder layer, and decoder layer <ref type="bibr" target="#b16">(Huang et al., 2015;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b4">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b3">Chen et al., 2019;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Gui et al., 2019b)</ref>. The difference between various NER models mainly lies in the variance in these components.</p><p>Recurrent Neural Networks (RNNs) are widely employed in NLP tasks due to its sequential characteristic, which is aligned well with language. Specifically, bidirectional long short-term memory networks (BiLSTM) <ref type="bibr" target="#b14">(Hochreiter and Schmidhuber, 1997</ref>) is one of the most widely used RNN structures. <ref type="bibr" target="#b16">(Huang et al., 2015)</ref> was the first one to apply the BiLSTM and Conditional Random Fields (CRF) <ref type="bibr" target="#b17">(Lafferty et al., 2001)</ref> to sequence labeling tasks. Owing to BiLSTM's high power to learn the contextual representation of words, it has been adopted by the majority of NER models as the encoder <ref type="bibr" target="#b22">(Ma and Hovy, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr">Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Gui et al., 2019b)</ref>.</p><p>Recently, Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017</ref>) began to prevail in various NLP tasks, like machine translation <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, language modeling <ref type="bibr" target="#b29">(Radford et al., 2018)</ref>, and pretraining models <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>. The Transformer encoder adopts a fully-connected self-attention structure to model the long-range context, which is the weakness of RNNs. Moreover, Transformer has better parallelism ability than RNNs. However, in the NER task, Transformer encoder has been reported to perform poorly <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>, our experiments also confirm this result. Therefore, it is intriguing to explore the reason why Transformer does not work well in NER task.</p><p>In this paper, we analyze the properties of Transformer and propose two specific improvements for NER.</p><p>The first is that the sinusoidal position embedding used in the vanilla Transformer is aware of distance but unaware of the directionality. In addition, this property will lose when used in the Louis Vuitton founded Louis Vuitton Inc. in 1854 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PER PER</head><p>ORG ORG ORG TIME Figure <ref type="figure">1</ref>: An example for NER. The relative direction is important in the NER task, because words before "Inc." are mostly to be an organization, words after "in" are more likely to be time or location. Besides, the distance between words is also important, since only continuous words can form an entity, the former "Louis Vuitton" can not form an entity with the "Inc.".</p><p>vanilla Transformer. However, both the direction and distance information are important in the NER task. For example in Fig 1, words after "in" are more likely to be a location or time than words before it, and words before "Inc." are mostly likely to be of the entity type "ORG". Besides, an entity is a continuous span of words. Therefore, the awareness of distance might help the word better recognizes its neighbor. To endow the Transformer with the ability of direction-and distanceawareness, we adopt the relative positional encoding <ref type="bibr" target="#b31">(Shaw et al., 2018;</ref><ref type="bibr" target="#b15">Huang et al., 2019;</ref><ref type="bibr" target="#b6">Dai et al., 2019)</ref>. instead of the absolute position encoding. We propose a revised relative positional encoding that uses fewer parameters and performs better.</p><p>The second is an empirical finding. The attention distribution of the vanilla Transformer is scaled and smooth. But for NER, a sparse attention is suitable since not all words are necessary to be attended. Given a current word, a few contextual words are enough to judge its label. The smooth attention could include some noisy information. Therefore, we abandon the scale factor of dot-production attention and use an un-scaled and sharp attention.</p><p>With the above improvements, we can greatly boost the performance of Transformer encoder for NER.</p><p>Other than only using Transformer to model the word-level context, we also tried to apply it as a character encoder to model word representation with character-level information. The previous work has proved that character encoder is necessary to capture the character-level features and alleviate the out-of-vocabulary (OOV) problem <ref type="bibr" target="#b18">(Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b4">Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b36">Xin et al., 2018)</ref>. In NER, CNN is commonly used as the character encoder. However, we argue that CNN is also not perfect for representing character-level information, be-cause the receptive field of CNN is limited, and the kernel size of the CNN character encoder is usually 3, which means it cannot correctly recognize 2-gram or 4-gram patterns. Although we can deliberately design different kernels, CNN still cannot solve patterns with discontinuous characters, such as "un..ily" in "unhappily" and "unnecessarily". Instead, the Transformer-based character encoder shall not only fully make use of the concurrence power of GPUs, but also have the potentiality to recognize different n-grams and even discontinuous patterns. Therefore, in this paper, we also try to use Transformer as the character encoder, and we compare four kinds of character encoders.</p><p>In summary, to improve the performance of the Transformer-based model in the NER task, we explicitly utilize the directional relative positional encoding, reduce the number of parameters and sharp the attention distribution. After the adaptation, the performance raises a lot, making our model even performs better than BiLSTM based models. Furthermore, in the six NER datasets, we achieve state-of-the-art performance among models without considering the pre-trained language models or designed features.</p><p>2 Related Work 2.1 Neural Architecture for NER <ref type="bibr" target="#b5">Collobert et al. (2011)</ref> utilized the Multi-Layer Perceptron (MLP) and CNN to avoid using taskspecific features to tackle different sequence labeling tasks, such as Chunking, Part-of-Speech (POS) and NER. In <ref type="bibr" target="#b16">(Huang et al., 2015)</ref>, BiLSTM-CRF was introduced to solve sequence labeling questions. Since then, the BiLSTM has been extensively used in the field of NER <ref type="bibr" target="#b4">(Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b8">Dong et al., 2016;</ref><ref type="bibr" target="#b37">Yang et al., 2018;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016)</ref>. Despite BiLSTM's great success in the NER task, it has to compute token representations one by one, which massively hinders full exploitation of GPU's parallelism. Therefore, CNN has been proposed by <ref type="bibr" target="#b33">(Strubell et al., 2017;</ref><ref type="bibr" target="#b11">Gui et al., 2019a)</ref> to encode words concurrently. In order to enlarge the receptive field of CNNs, <ref type="bibr" target="#b33">(Strubell et al., 2017)</ref> used iterative dilated CNNs (ID-CNN).</p><p>Since the word shape information, such as the capitalization and n-gram, is important in recognizing named entities, CNN and BiLSTM have been used to extract character-level informa-tion <ref type="bibr" target="#b4">(Chiu and Nichols, 2016;</ref><ref type="bibr" target="#b18">Lample et al., 2016;</ref><ref type="bibr" target="#b22">Ma and Hovy, 2016;</ref><ref type="bibr" target="#b33">Strubell et al., 2017;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref>.</p><p>Almost all neural-based NER models used pretrained word embeddings, like Word2vec and Glove <ref type="bibr" target="#b26">(Pennington et al., 2014;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013)</ref>. And when contextual word embeddings are combined, the performance of NER models will boost a lot <ref type="bibr" target="#b28">(Peters et al., 2017</ref><ref type="bibr" target="#b27">(Peters et al., , 2018;;</ref><ref type="bibr" target="#b0">Akbik et al., 2018)</ref>. ELMo introduced by <ref type="bibr" target="#b27">(Peters et al., 2018)</ref> used the CNN character encoder and BiLSTM language models to get contextualized word representations. Except for the BiLSTM based pre-trained models, BERT was based on Transformer <ref type="bibr" target="#b7">(Devlin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Transformer</head><p>Transformer was introduced by <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, which was mainly based on self-attention. It achieved great success in various NLP tasks. Since the self-attention mechanism used in the Transformer is unaware of positions, to avoid this shortage, position embeddings were used <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b7">Devlin et al., 2018)</ref>. Instead of using the sinusoidal position embedding <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> and learned absolute position embedding, <ref type="bibr" target="#b31">Shaw et al. (2018)</ref> argued that the distance between two tokens should be considered when calculating their attention score. <ref type="bibr" target="#b15">Huang et al. (2019)</ref> reduced the computation complexity of relative positional encoding from O(l 2 d) to O(ld), where l is the length of sequences and d is the hidden size. <ref type="bibr" target="#b6">Dai et al. (2019)</ref> derived a new form of relative positional encodings, so that the relative relation could be better considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Transformer Encoder Architecture</head><p>We first introduce the Transformer encoder proposed in <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. The Transformer encoder takes in an matrix H ∈ R l×d , where l is the sequence length, d is the input dimension. Then three learnable matrix W q , W k , W v are used to project H into different spaces. Usually, the matrix size of the three matrix are all R d×d k , where d k is a hyper-parameter. After that, the scaled dotproduct attention can be calculated by the following equations,</p><formula xml:id="formula_0">Q, K, V = HWq, HW k , HWv,<label>(1)</label></formula><formula xml:id="formula_1">At,j = QtK T j ,<label>(2)</label></formula><formula xml:id="formula_2">Attn(K, Q, V ) = softmax( A √ d k )V,<label>(3)</label></formula><p>where Q t is the query vector of the tth token, j is the token the tth token attends. K j is the key vector representation of the jth token. The softmax is along the last dimension. Instead of using one group of W q , W k , W v , using several groups will enhance the ability of self-attention. When several groups are used, it is called multi-head selfattention, the calculation can be formulated as follows,</p><formula xml:id="formula_3">Q (h) , K (h) , V (h) = HW (h) q , HW (h) k , HW (h) v , (4) head (h) = Attn(Q (h) , K (h) , V (h) ),<label>(5)</label></formula><formula xml:id="formula_4">MultiHead(H) = [head (1) ; ...; head (n) ]WO, (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>where n is the number of heads, the superscript h represents the head index. [head (1) ; ...; head (n) ] means concatenation in the last dimension. Usually d k × n = d, which means the output of [head (1) ; ...; head (n) ] will be of size R l×d . W o is a learnable parameter, which is of size R d×d . The output of the multi-head attention will be further processed by the position-wise feedforward networks, which can be represented as follows,</p><formula xml:id="formula_6">FFN(x) = max(0, xW1 + b1)W2 + b2,<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">W 1 , W 2 , b 1 , b 2 are learnable parameters, and W 1 ∈ R d×d f f , W 2 ∈ R d f f ×d , b 1 ∈ R d f f , b 2 ∈ R d . d f f is a hyper-parameter.</formula><p>Other components of the Transformer encoder includes layer normalization and Residual connection, we use them the same as <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Position Embedding</head><p>The self-attention is not aware of the positions of different tokens, making it unable to capture the sequential characteristic of languages. In order to solve this problem, <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> suggested to use position embeddings generated by sinusoids of varying frequency. The tth token's position embedding can be represented by the following equations</p><formula xml:id="formula_8">P Et,2i = sin(t/10000 2i/d ),<label>(8)</label></formula><formula xml:id="formula_9">P Et,2i+1 = cos(t/10000 2i/d ), (<label>9</label></formula><formula xml:id="formula_10">)</formula><p>where i is in the range of [0, d 2 ], d is the input dimension. This sinusoid based position embedding makes Transformer have an ability to model the position of a token and the distance of each two tokens. For any fixed offset k, P E t+k can be represented by a linear transformation of P E t <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>. In TENER, Transformer encoder is used not only to extract the word-level contextual information, but also to encode character-level information in a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Model</head><p>In this paper, we utilize the Transformer encoder to model the long-range and complicated interactions of sentence for NER. The structure of proposed model is shown in Fig 2 <ref type="figure">.</ref> We detail each parts in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Layer</head><p>To alleviate the problems of data sparsity and outof-vocabulary (OOV), most NER models adopted the CNN character encoder <ref type="bibr" target="#b22">(Ma and Hovy, 2016;</ref><ref type="bibr">Ye and Ling, 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2019)</ref> to represent words. Compared to BiLSTM based character encoder <ref type="bibr" target="#b18">(Lample et al., 2016;</ref><ref type="bibr" target="#b10">Ghaddar and Langlais, 2018)</ref>, CNN is more efficient. Since Transformer can also fully exploit the GPU's parallelism, it is interesting to use Transformer as the character encoder. A potential benefit of Transformer-based character encoder is to extract different n-grams and even uncontinuous character patterns, like "un..ily" in "unhappily" and "uneasily". For the model's uniformity, we use the "adapted Transformer" to represent the Transformer introduced in next subsection.</p><p>The final word embedding is the concatenation of the character features extracted by the character encoder and the pre-trained word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoding Layer with Adapted Transformer</head><p>Although Transformer encoder has potential advantage in modeling long-range context, it is not working well for NER task. In this paper, we propose an adapted Transformer for NER task with two improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Direction-and Distance-Aware Attention</head><p>Inspired by the success of BiLSTM in NER tasks, we consider what properties the Transformer lacks compared to BiLSTM-based models. One observation is that BiLSTM can discriminatively collect the context information of a token from its left and right sides. But it is not easy for the Transformer to distinguish which side the context information comes from.</p><p>Although the dot product between two sinusoidal position embeddings is able to reflect their distance, it lacks directionality and this property will be broken by the vanilla Transformer attention. To illustrate this, we first prove two properties of the sinusoidal position embeddings. Property 1. For an offset k and a position t, P E T t+k P E t only depends on k, which means the dot product of two sinusoidal position embeddings can reflect the distance between two tokens.</p><p>Proof. Based on the definitions of Eq.( <ref type="formula" target="#formula_8">8</ref>) and Eq.( <ref type="formula" target="#formula_9">9</ref>), the position embedding of t-th token is</p><formula xml:id="formula_11">P Et =        sin(c0t) cos(c0t) . . . sin(c d 2 −1 t) cos(c d 2 −1 t)        , (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where d is the dimension of the position embedding, c i is a constant decided by i, and its value is 1/10000 2i/d . Therefore,</p><formula xml:id="formula_13">P E T t P E t+k = d 2 −1 j=0 [sin(cjt) sin(cj(t + k)) + cos(cjt) cos(cj(t + k))] (11) = d 2 −1 j=0 cos(cj(t − (t + k))) (12) = d 2 −1 j=0 cos(cjk),<label>(13)</label></formula><p>where Eq.( <ref type="formula">11</ref>) to Eq.( <ref type="formula">12</ref>) is based on the equation cos(x − y) = sin(x) sin(y) + cos(x) cos(y). Figure <ref type="figure">4</ref>: The upper line is the product between P E T t P E t+k . The lower two lines are the products of P E T t W P E t+k with two random W s. Although P E T t P E t+k can reflect the distance, the P E T t W P E t+k has no clear pattern.</p><p>Property 2. For an offset k and a position t, P E T t P E t−k = P E T t P E t+k , which means the sinusoidal position embeddings is unware of directionality.</p><p>Proof. Let j = t − k, according to property 1, we have</p><formula xml:id="formula_14">P E T t P E t+k = P E T j P E j+k (14) = P E T t−k P Et. (<label>15</label></formula><formula xml:id="formula_15">)</formula><p>The relation between d, k and P E T t P E t+k is displayed in Fig 3 <ref type="figure">.</ref> The sinusoidal position embeddings are distance-aware but lacks directionality.</p><p>However, the property of distance-awareness also disappears when P E t is projected into the query and key space of self-attention. Since in vanilla Transformer the calculation between P E t and P E t+k is actually P E T t W T q W k P E t+k , where W q , W k are parameters in Eq.( <ref type="formula" target="#formula_0">1</ref>). Mathematically, it can be viewed as P E T t W P E t+k with only one parameter W . The relation between P E T t P E t+k and P E T t W P E t+k is depicted in Fig 4. Therefore, to improve the Transformer with direction-and distance-aware characteristic, we calculate the attention scores using the equations below:</p><formula xml:id="formula_16">Q, K, V = HWq, H d k , HWv,<label>(16)</label></formula><formula xml:id="formula_17">Rt−j = [. . . sin( t − j 10000 2i/d k ) cos( t − j 10000 2i/d k ) . . .] T ,<label>(17)</label></formula><formula xml:id="formula_18">A rel t,j = QtK T j + QtR T t−j + uK T j + vR T t−j ,<label>(18)</label></formula><formula xml:id="formula_19">Attn(Q, K, V ) = softmax(A rel )V, (<label>19</label></formula><formula xml:id="formula_20">)</formula><p>where t is index of the target token, j is the index of the context token, Q t , K j is the query vector and key vector of token t, j respectively,</p><formula xml:id="formula_21">W q , W v ∈ R d×d k . To get H d k ∈ R l×d k , we</formula><p>first split H into d/d k partitions in the second dimension, then for each head we use one partition. u ∈ R d k , v ∈ R d k are learnable parameters, R t−j is the relative positional encoding, and <ref type="formula" target="#formula_18">18</ref>) is the attention score between two tokens; Q T t R t−j is the tth token's bias on certain relative distance; u T K j is the bias on the jth token; v T R t−j is the bias term for certain distance and direction.</p><formula xml:id="formula_22">R t−j ∈ R d k , i in Eq.(17) is in the range [0, d k 2 ]. Q T t K j in Eq.(</formula><p>Based on Eq.( <ref type="formula" target="#formula_17">17</ref>), we have</p><formula xml:id="formula_23">Rt, R−t =        sin(c0t) cos(c0t) . . . sin(c d 2 −1 t) cos(c d 2 −1 t)        ,        − sin(c0t) cos(c0t) . . . − sin(c d 2 −1 t) cos(c d 2 −1 t)        ,<label>(20)</label></formula><p>because sin(−x) = − sin(x), cos(x) = cos(−x). This means for an offset t, the forward and backward relative positional encoding are the same with respect to the cos(c i t) terms, but is the opposite with respect to the sin(c i t) terms. Therefore, by using R t−j , the attention score can distinguish different directions and distances. The above improvement is based on the work <ref type="bibr" target="#b31">(Shaw et al., 2018;</ref><ref type="bibr" target="#b6">Dai et al., 2019)</ref>. Since the size of NER datasets is usually small, we avoid direct multiplication of two learnable parameters, because they can be represented by one learnable parameter. Therefore we do not use W k in Eq.( <ref type="formula" target="#formula_16">16</ref>). The multi-head version is the same as Eq.( <ref type="formula" target="#formula_4">6</ref>), but we discard W o since it is directly multiplied by W 1 in Eq.( <ref type="formula" target="#formula_6">7</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Un-scaled Dot-Product Attention</head><p>The vanilla Transformer use the scaled dotproduct attention to smooth the output of softmax function. In Eq.( <ref type="formula" target="#formula_2">3</ref>), the dot product of key and value matrices is divided by the scaling factor √ d k .</p><p>We empirically found that models perform better without the scaling factor √ d k . We presume this is because without the scaling factor the attention will be sharper. And the sharper attention might be beneficial in the NER task since only few words in the sentence are named entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CRF Layer</head><p>In order to take advantage of dependency between different tags, the Conditional Random Field (CRF) was used in all of our models. Given a sequence s = [s 1 , s 2 , ..., s T ], the corresponding golden label sequence is y = [y 1 , y 2 , ..., y T ], and Y(s) represents all valid label sequences. The probability of y is calculated by the following equation</p><formula xml:id="formula_24">P (y|s) = T t=1 e f (y t−1 ,y t ,s) Y(s) y T t=1 e f (y t−1 ,y t ,s) , (<label>21</label></formula><formula xml:id="formula_25">)</formula><p>where f (y t−1 , y t , s) computes the transition score from y t−1 to y t and the score for y t . The optimization target is to maximize P (y|s). When decoding, the Viterbi Algorithm is used to find the path achieves the maximum probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We evaluate our model in two English NER datasets and four Chinese NER datasets.</p><p>(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four differ-ent named entities: PERSON, LOCATION, <ref type="bibr">OR-GANIZATION, and MISC (Sang and Meulder, 2003)</ref>.</p><p>(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it <ref type="bibr" target="#b3">(Chen et al., 2019;</ref><ref type="bibr" target="#b4">Chiu and Nichols, 2016)</ref>. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.</p><p>(3) Weischedel ( <ref type="formula">2011</ref>) released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as <ref type="bibr" target="#b2">(Che et al., 2013)</ref>.</p><p>(4) The corpus of the Chinese NER dataset MSRA came from news domain <ref type="bibr" target="#b19">(Levow, 2006)</ref>.</p><p>(5) Weibo NER was built based on text in Chinese social media Sina Weibo <ref type="bibr" target="#b25">(Peng and Dredze, 2015)</ref>, and it contained 4 kinds of entities.</p><p>(6) Resume NER was annotated by <ref type="bibr" target="#b39">(Zhang and Yang, 2018)</ref>.</p><p>Their statistics are listed in Table <ref type="table" target="#tab_0">1</ref>. For all datasets, we replace all digits with "0", and use the BIOES tag schema. For English, we use the Glove 100d pre-trained embedding <ref type="bibr" target="#b26">(Pennington et al., 2014)</ref>. For the character encoder, we use 30d randomly initialized character embeddings. More details on models' hyper-parameters can be found in the supplementary material. For Chinese, we used the character embedding and bigram embedding released by <ref type="bibr" target="#b39">(Zhang and Yang, 2018)</ref>. All pretrained embeddings are finetuned during training. In order to reduce the impact of randomness, we ran all of our experiments at least three times, and its average F1 score and standard deviation are reported.</p><p>We used random-search to find the optimal hyper-parameters, hyper-parameters and their ranges are displayed in the supplemental material. We use SGD and 0.9 momentum to optimize the model. We run 100 epochs and each batch has 16 samples. During the optimization, we use the triangle learning rate <ref type="bibr" target="#b32">(Smith, 2017)</ref> where the learning rate rises to the pre-set learning rate at the first 1% steps and decreases to 0 in the left 99% steps. The model achieves the highest development performance was used to evaluate the test set. The hyper-parameter search range and other settings can be found in the supplementary material. Codes are available at https://github. com/fastnlp/TENER.  <ref type="bibr" target="#b39">(Zhang and Yang, 2018)</ref> and <ref type="bibr" target="#b11">(Gui et al., 2019a)</ref>, respectively. "w/ scale" means TENER using the scaled attention in Eq.( <ref type="formula" target="#formula_19">19</ref>). * their results are not directly comparable with ours, since they used 100d pre-trained character and bigram embeddings. Other models use the same embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Chinese NER Datasets</head><p>We first present our results in the four Chinese NER datasets. Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation.</p><p>As shown in Table <ref type="table" target="#tab_1">2</ref>, the vanilla Transformer does not perform well and is worse than the BiL-STM and CNN based models. However, when relative positional encoding combined, the performance was enhanced greatly, resulting in better results than the BiLSTM and CNN in all datasets. The number of training examples of the Weibo dataset is tiny, therefore the performance of the Transformer is abysmal, which is as expected since the Transformer is data-hungry. Nevertheless, when enhanced with the relative positional encoding and unscaled attention, it can achieve even better performance than the BiLSTM-based model. The superior performance of the adapted Transformer in four datasets ranging from small datasets to big datasets depicts that the adapted Transformer is more robust to the number of training examples than the vanilla Transformer. As the last line of Table <ref type="table" target="#tab_1">2</ref> depicts, the scaled attention will deteriorate the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on English NER datasets</head><p>The comparison between different NER models on English NER datasets is shown in Table <ref type="table" target="#tab_3">3</ref>. The poor performance of the Transformer in the NER datasets was also reported by <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>. Although performance of the Transformer is higher than <ref type="bibr" target="#b13">(Guo et al., 2019)</ref>, it still lags behind the BiLSTM-based models <ref type="bibr" target="#b22">(Ma and Hovy, 2016)</ref>. Nonetheless, the performance is massively enhanced by incorporating the relative positional encoding and unscaled attention into the Transformer. The adaptation not only makes the Transformer achieve superior performance than BiL-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Different Character Encoders</head><p>The character-level encoder has been widely used in the English NER task to alleviate the data sparsity and OOV problem in word representation. In this section, we cross different character-level encoders (BiLSTM, CNN, Transformer encoder and our adapted Transformer encoder (AdaTrans for short) ) and different word-level encoders (BiL-STM, ID-CNN and AdaTrans) to implement the NER task. Results on CoNLL2003 and OntoNotes 5.0 are presented in Table <ref type="table" target="#tab_5">5a and Table 5b</ref>, respectively.</p><p>The ID-CNN encoder is from <ref type="bibr" target="#b33">(Strubell et al., 2017)</ref>, and we re-implement their model in Py-Torch. For different combinations, we use random search to find its best hyper-parameters. Hyperparameters for character encoders were fixed. The details can be found in the supplementary material.</p><p>For the results on CoNLL2003 dataset which is depicted in Table <ref type="table" target="#tab_5">5a</ref>, the AdaTrans performs as good as the BiLSTM in different character en-coder scenario averagely. In addition, from Table 5b, we can find the pattern that the AdaTrans character encoder outpaces the BiLSTM and CNN character encoders when different word-level encoders being used. Moreover, no matter what character encoder being used or none being used, the AdaTrans word-level encoder gets the best performance. This implies that when the number of training examples increases, the AdaTrans character-level and word-level encoder can better realize their ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Convergent Speed Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose TENER, a model adopting Transformer Encoder with specific customizations for the NER task. Transformer Encoder has a powerful ability to capture the long-range context. In order to make the Transformer more suitable to the NER task, we introduce the direction-aware, distance-aware and un-scaled attention. Experiments in two English NER tasks and four Chinese NER tasks show that the performance can be massively increased. Under the same pretrained embeddings and external knowledge, our proposed modification outperforms previous models in the six datasets. Meanwhile, we also found the adapted Transformer is suitable for being used as the English character encoder, because it has the potentiality to extract intricate patterns from char-acters. Experiments in two English NER datasets show that the adapted Transformer character encoder performs better than BiLSTM and CNN character encoders.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Model structure of TENER for English NER tasks. In TENER, Transformer encoder is used not only to extract the word-level contextual information, but also to encode character-level information in a word.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Dot product between two sinusoidal position embeddings whose distance is k. It is clear that the product is symmetrical, and with the increment of |k|, it has a trend to decrease, but this decrease is not monotonous.</figDesc><graphic url="image-1.png" coords="5,79.09,62.81,204.09,136.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Convergent speed in the development dataset of OntoNotes 5.0 for four kinds of models.</figDesc><graphic url="image-3.png" coords="8,314.36,251.69,204.09,136.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of Datasets.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell>Type</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>English</cell><cell>CoNLL2003 OntoNotes 5.0</cell><cell cols="4">Sentence 14.0k 3.2k 3.5k Token 203.6k 51.4k 46.4k Sentence 59.9k 8.5k 8.3k Token 1088.5k 147.7k 152.7k</cell></row><row><cell></cell><cell>OntoNotes 4.0</cell><cell cols="4">Sentence 15.7k 4.3k 4.3k Token 491.9k 200.5k 208.1k</cell></row><row><cell>Chinese</cell><cell>MSRA Weibo</cell><cell cols="4">Sentence 46.4k 4.4k 4.4k Token 2169.9k 172.6k 172.6k Sentence 1.4k 0.3k 0.3k Token 73.5k 14.4k 14.8k</cell></row><row><cell></cell><cell>Resume</cell><cell>Sentence Token</cell><cell cols="3">3.8k 0.5k 0.5k 124.1k 13.9k 15.1k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The F1 scores on Chinese NER datasets. ♣ , ♠ are results reported in</figDesc><table><row><cell>Models</cell><cell>Weibo</cell><cell>Resume</cell><cell cols="2">OntoNotes4.0 MSRA</cell></row><row><cell>BiLSTM ♣</cell><cell>56.75</cell><cell>94.41</cell><cell>71.81</cell><cell>91.87</cell></row><row><cell>ID-CNN ♠</cell><cell>-</cell><cell>93.75</cell><cell>62.25</cell><cell>-</cell></row><row><cell cols="2">CAN-NER  *  (Zhu and Wang, 2019) 59.31</cell><cell>94.94</cell><cell>73.64</cell><cell>92.97</cell></row><row><cell>Transformer</cell><cell cols="3">46.38 ± 0.78 93.43 ± 0.26 66.49 ± 0.30</cell><cell>88.35 ± 0.60</cell></row><row><cell>TENER(Ours)</cell><cell cols="3">58.17 ± 0.22 95.00 ± 0.25 72.43 ± 0.39</cell><cell>92.74 ± 0.27</cell></row><row><cell>w/ scale</cell><cell>57.40 ± 0.3</cell><cell cols="2">94.00 ± 0.51 71.72 ± 0.08</cell><cell>91.67 ± 0.23</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The F1 scores on English NER datasets.</figDesc><table><row><cell>We</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance of models with ELMo as their embeddings in English NER datasets. "BiLSTM" is our run. In the larger OntoNotes5.0, TENER achieves much better F1 score. ± 0.32 87.30 ± 0.15 88.37 ± 0.27 BiLSTM 91.32 ± 0.13 89.99 ± 0.14 91.29 ± 0.12 CNN 91.22 ± 0.10 90.17 ± 0.02 91.45 ± 0.07 Transformer 91.12 ± 0.10 90.05 ± 0.13 91.23 ± 0.06 AdaTrans 91.38 ± 0.15 89.99 ± 0.05 91.33 ± 0.05</figDesc><table><row><cell>Char</cell><cell cols="2">Word BiLSTM</cell><cell>ID-CNN</cell><cell>AdaTrans</cell></row><row><cell cols="2">No Char</cell><cell cols="2">88.34 (a) CoNLL2003</cell></row><row><cell>Char</cell><cell cols="2">Word BiLSTM</cell><cell>ID-CNN</cell><cell>AdaTrans</cell></row></table><note>No Char 85.20 ± 0.23 84.26 ± 0.07 85.80 ± 0.10 BiLSTM 87.85 ± 0.09 87.38 ± 0.17 88.12 ± 0.16 CNN 87.79 ± 0.14 87.10 ± 0.06 88.25 ± 0.11 Transformer 88.01 ± 0.06 87.31 ± 0.10 88.20 ± 0.07 AdaTrans 88.12 ± 0.17 87.51 ± 0.11 88.43 ± 0.12 (b) OntoNotes 5.0</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>F1 scores in the CoNLL2003 and OntoNotes 5.0.</figDesc><table /><note>"Char" means character-level encoder, and "Word" means word-level encoder. "AdaTrans" means our adapted Transformer encoder.</note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>6 Supplemental Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Character Encoder</head><p>We exploit four kinds of character encoders. For all character encoders, the randomly initialized character embeddings are 30d. The hidden size of BiLSTM used in the character encoder is 50d in each direction. The kernel size of CNN used in the character encoder is 3, and we used 30 kernels with stride 1. For Transformer and adapted Transformer, the number of heads is 3, and every head is 10d, the dropout rate is 0.15, the feedforward dimension is 60. The Transformer used the sinusoid position embedding. The number of parameters for the character encoder (excluding character embedding) when using BiLSTM, CNN, Transformer and adapted Transformer are 35830, 3660, 8460 and 6600 respectively. For all experiments, the hyper-parameters of character encoders stay unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Hyper-parameters</head><p>The hyper-parameters and search ranges for different encoders are presented in Table <ref type="table">6</ref>, Table <ref type="table">7</ref>   <ref type="bibr">[4, 6, 8, 10] [8, 10, 12, 14</ref>] head dimension <ref type="bibr">[32, 48, 64, 80, 96] [64, 80, 96, 112, 128</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sequence labeling: A practical approach</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Akhundov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietrich</forename><surname>Trautmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Groh</surname></persName>
		</author>
		<idno>CoRR, abs/1808.03926</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with bilingual constraints</title>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengqiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="52" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">GRN: Gated relation network to enhance convolutional neural network for named entity recognition</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borje</forename><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">2011. Aug</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Characterbased LSTM-CRF with radical-level features for chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Chuanhai</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Hattori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="239" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applying named entity recognition and co-reference resolution for segmenting english texts</title>
		<author>
			<persName><forename type="first">Pavlina</forename><surname>Fragkou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="325" to="346" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Robust lexical features for improved neural network named-entity recognition</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Ghaddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Langlais</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03489</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cnn-based chinese NER with lexicon rethinking</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="4982" to="4988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A lexicon-based graph neural network for chinese ner</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlong</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Startransformer</title>
		<author>
			<persName><forename type="first">Qipeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1315" to="1325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C N</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
				<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
				<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The third international chinese language processing bakeoff: Word segmentation and named entity recognition</title>
		<author>
			<persName><forename type="first">Gina-Anne</forename><surname>Levow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Workshop on Chinese Language Processing</title>
				<meeting>the Fifth Workshop on Chinese Language Processing<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-07-22">2006. July 22-23, 2006</date>
			<biblScope unit="page" from="108" to="117" />
		</imprint>
	</monogr>
	<note>SIGHAN@COLING/ACL 2006</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Empower sequence labeling with task-aware neural language model</title>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">Fangzheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5253" to="5260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Contextualized non-local neural networks for sequence learning</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaichen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6762" to="6769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">End-to-end relation extraction using lstms on sequences and tree structures</title>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.00770</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Named entity recognition for chinese social media with jointly trained embeddings</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="548" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NACCL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
				<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="464" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-03-24">2017. 2017. 2017. March 24-31, 2017</date>
			<biblScope unit="page" from="464" to="472" />
			<pubPlace>Santa Rosa, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fast and accurate entity recognition with iterated dilated convolutions</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02098</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<title level="m">Ontonotes release</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning better internal structure of words for sequence labeling</title>
		<author>
			<persName><forename type="first">Yingwei</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vibhuti</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-David</forename><surname>Ruvini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2584" to="2593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Five-stroke based cnn-birnn-crf network for chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gongshen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanrong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.03838</idno>
	</analytic>
	<monogr>
		<title level="m">The hyper-parameters and hyper-parameter search ranges for BiLSTM. Zhi-Xiu Ye and Zhen-Hua Ling</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Hybrid semimarkov CRF for neural sequence labeling</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sentencestate LSTM for text representation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="317" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Chinese NER using lattice LSTM</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural question generation from text: A preliminary study</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPCC</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="662" to="671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">CAN-NER: convolutional attention network for chinese named entity recognition</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3384" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
