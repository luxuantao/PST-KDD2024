<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partial Multi-View Clustering via Consistent GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qianqian</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ISN</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<country>Xi&apos;an China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer, Information and Technology</orgName>
								<orgName type="institution">Indiana University-Purdue University Indianapolis</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanxue</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of ISN</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<country>Xi&apos;an China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Partial Multi-View Clustering via Consistent GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E687312D202DC8314F6ACAF0870D35CC</idno>
					<idno type="DOI">10.1109/ICDM.2018.00174</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>partial multi-view</term>
					<term>clustering</term>
					<term>generative adversarial network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-view clustering, as one of the most important methods to analyze multi-view data, has been widely used in many real-world applications. Most existing multiview clustering methods perform well on the assumption that each sample appears in all views. Nevertheless, in real-world application, each view may well face the problem of the missing data due to noise, or malfunction. In this paper, a new consistent generative adversarial network is proposed for partial multi-view clustering. We learn a common low-dimensional representation, which can both generate the missing view data and capture a better common structure from partial multiview data for clustering. Different from the most existing methods, we use the common representation encoded by one view to generate the missing data of the corresponding view by generative adversarial networks, then we use the encoder and clustering networks. This is intuitive and meaningful because encoding common representation and generating the missing data in our model will promote mutually. Experimental results on three different multi-view databases illustrate the superiority of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Nowadays, with the advance of hardware technology, multi-view data, which are come from different sources for one subject, are common in real-world <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. For example, one image can be represented either by visual feature or text annotation. In general, different views provide complementary information to describe the data, which lets multi-view learning achieve promising performance, and draws research efforts in many fields such as data analysis, image classification, multimedia and information retrieval <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>.</p><p>As one of the most representative methods of multi-view learning, multi-view clustering has attracted considerable attention <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. However, in practice, incomplete view data are ubiquitous due to many unforeseeable reasons <ref type="bibr" target="#b6">[7]</ref>, such as noise, or malfunction of the data-collecting equipment. traditional multi-view clustering methods cannot directly handle such data, because they learn a shared representation based on an assumption that all the views are complete. For convenience, people always put away all the relevant data, which results in a huge waste of data. *Corresponding author: Quanxue Gao (e-mail: qxgao@xidian.edu.cn).</p><p>Motivated by this problem, two kinds of approaches have been proposed <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The first kind of method is based on kernel technique. For example, Shao et al. <ref type="bibr" target="#b7">[8]</ref> completed the kernel matrices of the incomplete views according to that of the complete views and then doing clustering task. However, this method can only deal with the kernel-based multi-view clustering algorithms. To solve this problem, researchers proposed non-negative matrix factorization (NMF) based methods. For example, Li et al. <ref type="bibr" target="#b6">[7]</ref> proposed NMF based partial multi-View Clustering (PVC) approach to learn a latent subspace over two views, which achieves better clustering performance. Motivated by these methods, a lot of partial multi-view clustering approaches based on NMF have been proposed <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>. However, these methods have several limitations restricting its application. (1) It is not straightforward to employ NMF based methods for the largescale datasets, due to its heave computation caused by a large amount of inverse operations within matrix factorization. (2) NMF methods did not explicitly compensate the missing data in each view, since they exploited some regularizes to constrain on the new representation.</p><p>An intuitive way to learn better representation for partial multi-view data is synthesizing the missing data. Recently, generative adversarial networks (GANs) <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, has attracted lots of attention for its generating function. Vanilla GAN <ref type="bibr" target="#b11">[12]</ref> creates desired data from random noise, while the latest GANs <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> learn the relationship between two domains. For example, Isola et al. <ref type="bibr" target="#b15">[16]</ref> used the conditional GANs on paired training data to transfer images from one distribution to another and developed pix2pix GAN. Zhu et al. proposed Cycle GAN <ref type="bibr" target="#b13">[14]</ref> by using a cycle consistent adversarial network to train unpaired image, which achieves better performance than pix2pix GAN. <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b16">[17]</ref> use GANs on multi-view data generation. However, current research on GAN mainly focuses on data generation, while few works explore its application in data analysis, like data clustering. There have not been thoroughly studied for partial multi-view clustering based on GANs.</p><p>In this paper, we propose a novel deep generative model, termed as consistent GAN, for the partial multi-view clustering task, which consists of two encoders, two GAN networks, and one deep clustering layer. In our model, we adopt two encoders to learn the shared latent representations among multiple views, and naturally leverage the common representation given by one view to infer the missing data of the corresponding view via a GAN network. In details, the generator of GAN tries to recover the missing-view data by using the encoded codes from another view; while the discriminator pushes the fake data towards the real ones. By this means, we fully utilize the adversarial training to explore the complementary information shared by each view. Moreover, to explicitly guide the representation learning for the clustering task, we add one clustering layer to further highlight the cluster structure existing in the common representation. The main contributions of our method are summarized as follows:</p><p>1. We propose a novel partial multi-view clustering method called consistent GAN, which not only captures a better clustering structure, but also infers the missing view.</p><p>2. Compared with some GAN models which use random noises to generate data, our model fully utilize the complementary information among multi-view data. We use the common representation encoded by one view to generate the missing data. The generated missing view data promote to achieve an optimal clustering result for each view.</p><p>3. Extensive experiments have been conducted on several multi-view databases which illustrate the superiority of our method compared with several state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PARTIAL MULTI-VIEW CLUSTERING VIA CONSISTENT GAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>As aforementioned analysis, most existing partial multiview clustering are based on kernel and non-negative matrix factorization techniques to infer the missing data and learn a common clustering structure. However, these methods are not the best solution for partial multi-view clustering problem: 1) They cannot be applied to large-scale data; 2) They do not consider learning a latent space which is suitable for clustering and simultaneously infers the missing view well. From this point and motivated by GANs, we propose a novel model named partial multi-view clustering via consistent GAN. The proposed model utilizes the combination of GAN and deep embedding clustering layer to learn a shared embedding structure for partial multi-view data, which can capture a better clustering structure and infer the missing view at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Framework</head><p>Notations: To introduce easily, we use two-view data as an example. Let two views data X = X (1) , X (2) , X (v) ∈ R N ×dv (v = 1, 2) be the data matrix of each view, X (1)  Missing data of view 2</p><formula xml:id="formula_0">= {a 1 , a 2 , • • • , a N }, X (2) = {b 1 , b 2 , • • • , b N }, v</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X</head><p>Missing data of view 1 Network architecture: The model consists of seven sub-networks, two stack fully-connected encoders E 1 /E 2 , one deep embedding clustering layer, two stack fullyconnected generators G 1 /G 2 , two fully-connected discriminators D 1 /D 2 . Then we will give detailed introduction for the framework.</p><p>1. Encoder E 1 /E 2 : R dv → R m . E 1 , made up of L stacked fully connected layers, aims to learn a latent representation</p><formula xml:id="formula_1">Z 1 = z (1) 1 , z (1) 2 , • • • , z (1) N (Z 1 ∈ R N ×m ) for</formula><p>all input data X (1) . Specifically, it maps the</p><formula xml:id="formula_2">d 1 -dimensional input data a i to a low-dimensional representation z (1) i , z (1) i = f 1 (a i ; θ)</formula><p>, where f is non-linear function, and θ is the parameter of E 1 . E 2 has the same theory as</p><formula xml:id="formula_3">E 1 . It learns the latent space Z 2 = z (2) 1 , z (2) 2 , • • • z (2) N (Z 2 ∈ R N ×m )</formula><p>from X (2) and maps the</p><formula xml:id="formula_4">d 2 -dimensional input data b i to a m-dimensional representation z (2) i , z (2) i = f 2 (b i ; θ).</formula><p>In order to obtain common information from two views, there we partially share parameters of E 1 and E 2 , i.e. θ.</p><p>2. Generator G 1 /G 2 : R m → R dv , which recover the two views by latent space Z 1 /Z 2 . In our model, G 1 /G 2 have a symmetrical structure with E 1 /E 2 , and consist of L stacked fully connected layers, because G 1 /G 2 play roles of generator and decoder at the same time. For G 1 , the output ãi , is generated by the low-dimensional representation z <ref type="bibr" target="#b0">(1)</ref> i , and z</p><formula xml:id="formula_5">(2) i , i.e., ãi = G 1 (z (1) i ), and ãi = G 1 (z (2) i ). To make z (1) i be similar to z (2)</formula><p>i , we introduce a common space Z for these two view data in deep embedding clustering layer. The same for G 2 , which generates the second view data by lowdimensional representation z <ref type="bibr" target="#b0">(1)</ref> i , and z</p><formula xml:id="formula_6">(2) i , i.e., bi = G 2 (z (1) i ) and bi = G 2 (z (2) i ).</formula><p>3. Discriminator D 1 /D 2 : R dv → {0, 1}. Each discriminator consists of 3 stacked fully connected layers, and their function is to distinguish the generated samples ãi , bi and real samples {a i , b i }. For D 1 , it should distinguish that ãi is a generated sample and a i is a real instance. Then it feeds back the result to generator network and updates the parameters of generator. Until the generator can create such realistic sample, the discriminator cannot distinguish which input is real sample. Similar operation with D 2 .</p><p>4. Deep embedded clustering layer: this layer is used to change the data distribution and adopt the encoder network and generator network. We computer the current data distribution and target data distribution based on the common space Z and the cluster centroids {μ j } k j=1 . Then, we use the target data distribution to modify the current data distribution and update the parameter of encoder E 1 /E 2 and generator G 1 /G 2 , and refine the cluster centroids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Objective function</head><p>The overall objective function of our model contains three terms: Auto-encoder loss, GAN loss, and KL clustering loss.</p><formula xml:id="formula_7">L = min θ,f1,f2,G1,G2 max D1,D2 L AE +λ 1 L cycleGAN +λ 2 L KL ,</formula><p>(1) where λ 1 and λ 2 are two parameters for maintaining the impact of GAN and KL clustering loss. Auto-encoder Loss w.r.t. θ, f 1 , f 2 , G 1 , G 2 . The Autoencoder loss is minimizing the Euclidean distance of generated sample and the original sample</p><formula xml:id="formula_8">L AE = min θ,f1,f2,G1,G2 X (1) -G 1 (f 1 (X (1) ; θ)) 2 F + X (2) -G 2 (f 2 (X (2) ; θ)) 2 F .</formula><p>(2) This loss is used for the sub-networks of encoder E 1 /E 2 and generator G 1 /G 2 . It aims to ensure the encoder can catch the essential structure from two views data, and the latent representation can exactly recover the real data. The encoders take two views X (1) and X (2) as input and learn two latent representations Z 1 = f 1 (X (1) ; θ), and Z 2 = f 2 (X (2) ; θ) for these two views. The generators reconstruct the two views from the latent representation. The output are G 1 (f 1 (X (1) ; θ)) and G 2 (f 2 (X (2) ; θ)).</p><p>If all the input data are paired data, only the Auto-encoder Loss is enough. The encoder and generator networks can work well. However, our setting is partial multi-view data, the unpaired data will degrade the performance of encoder and generator networks. In order to improve the performance when database contains unpaired data, we add cycle GAN Loss in our objective function to refine the network.</p><formula xml:id="formula_9">Cycle GAN Loss w.r.t. G 1 , G 2 , D 1 , D 2 .</formula><p>In our model, we use cycle GAN, due to we have large amount of unpaired data. A cycle GAN model is composed of two GAN models and trained on unpaired data. It aims to use one distribution data to generate another. Assume that the data distribution of two views are a ∼ P (X (1) ), b ∼ P (X (2) ), and let G 1 • f 2 =G 1 (f 2 (b, θ)) denote the mapping of the second view sample to the first view data distribution, i.e., using view 2 data b to generate the corresponding view 1 data ã ∼ P (X (1) ). The same for G 2 •f 1 =G 2 (f 1 (a, θ)), which denotes the transformation of the first view sample to the second view sample. D 1 is used to discriminate between the generated data ã by G 1 and the real data a, while D 2 is used to discriminate between the generated data b by G 2 and the real data b. The cycle GAN loss is:</p><formula xml:id="formula_10">L cycleGAN (G 1 , D 1 , G 2 , D 2 ) = min G1,G2 max D1,D2 L GAN (G 1 , D 1 ) +L GAN (G 2 , D 2 ) + λ 3 L cyc (G 1 , G 2 ).<label>(3)</label></formula><p>where the loss of GAN in our model is</p><formula xml:id="formula_11">L GAN = min G1,G2 max D1,D2 E a∼P (X (1) ) [log D 1 (a)] +E b∼P (X (2) ) [log (1 -D 1 (G 1 • f 2 (b)))] +E b∼P (X (2) ) [log D 2 (b)] +E a∼P (X (1) ) [log (1 -D 2 (G 2 • f 1 (a)))] . (<label>4</label></formula><formula xml:id="formula_12">)</formula><p>The cycle consistency loss is</p><formula xml:id="formula_13">L cyc (G 1 , G 2 )=E a∼P (X (1) ) G 2 (G 1 • f 2 (b)) -b 1 +E b∼P (X (2) ) G 1 (G 2 • f 1 (a)) -a 1 .</formula><p>(5) The generator is trained to generate fake data which are similar to real data. The discriminators are trained to distinguish the fake data from the real data. They play a min-max game until convergence. However, the GANs are trained to map a same input to any random permutation of sample which are in the target data distribution. Hence, the GAN loss alone cannot ensure a desired output. To reduce the space of possible mapping functions, cycle GAN introduces cycle-consistent loss to update the learned mapping, i.e., for each image, after passing the image translation cycle, should be brought back to the itself. The cycle consistency loss can assist the generator in mapping a given sample a to a desired output b. Thus, the combination of GAN loss and cycle consistency loss ensure the generator map the input to a desired output. KL Clustering Loss w.r.t. θ, f 1 , f 2 , G 1 , G 2 , D 1 , D 2 . According the aforementioned analysis, cycle GAN loss will update the generator and discriminator networks. However, it does not modify the encoders which learn the common representation Z for the final clustering task, while unpaired data have a bad effect on the common representation learned by paired data. In order to obtain an optimal clustering structure, we add a clustering loss which is measured by Kullback-Leibler divergence (KL-divergence). We will learn two latent subspaces for the two views Z 1 = f 1 (X (1) ; θ), and Z 2 = f 2 (X (2) ; θ). Then we get a common latent representation based on these two subspaces</p><formula xml:id="formula_14">Z = h(Z 1 , Z 2 ),<label>(6)</label></formula><p>h(•) represents a function of concatation or summation. Define {μ j } k j=1 as k initial clustering centroids. According to <ref type="bibr" target="#b17">[18]</ref>, we use the Student's t-distribution as a kernel to measure the similarity between common latent representation point z i and centroid μ j . The probability of assigning sample i to cluster j can be calculated by</p><formula xml:id="formula_15">q ij = (1 + z i -μ j 2 /α) -α+1 2 j (1 + z i -μ j 2 /α) -α+1 2 . (<label>7</label></formula><formula xml:id="formula_16">)</formula><p>It is also named soft assignment. α is the degree of freedom of the Student's t-distribution. In order to improve clustering performance, and lay special stress on data points assigned with high confidence, we computer target distribution p i by first raising q i to the squared and then normalizing by frequency per cluster</p><formula xml:id="formula_17">p ij = q 2 ij f j j q 2 ij f j , (<label>8</label></formula><formula xml:id="formula_18">)</formula><p>where f j = i q ij is soft cluster frequency. The clustering loss is defined as minimizing the KL-divergence between a data distribution and a target distribution.</p><formula xml:id="formula_19">L KL = KL(P|Q) = i j p ij log p ij q ij . (<label>9</label></formula><formula xml:id="formula_20">)</formula><p>Our aim is to match the soft assignment q i to the target distribution p i . In this way, we can sharpen the data distribution and concentrate the same class data. In addition, we will get a more effective and common representation for partial multi-view clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation</head><p>Step 1: Training encoder E 1 , E 2 and generator G 1 , G 2 on paired data. We use paired data to train encoder and generator. Since generator can be seen as the decoder corresponding to encoder, we just use the AE loss to train these networks and update the parameter of θ, f 1 , f 2 , G 1 , G 2 . Specifically, we take {a i , b i } as input for encoder E 1 , E 2 and get two latent spaces Z 1 , Z 2 and common representation Z. Then we Z 1 , Z 2 as the input of generator G 1 , G 2 and get four outputs. Z 1 can generate ãi , bi , and Z 2 can also generate ãi , bi . Then we compute the AE loss and updating encoder and generator network until convergence. After step 1, we save the clustering centroids {μ j } k j=1 for the following training. These clustering centroids learned by paired data can instruct sample which has missing view to be assigned to the right group.</p><p>Step 2: Training generator G 1 , G 2 and discriminator D 1 /D 2 on all data. In this step, we train the generator and discriminator network on all data. For paired data {a i , b i }, we directly take them as the input of generators G 1 , G 2 . For unpaired data a j , in order to increase the number of unpaired data, we randomly choose one sample from all the second view b i , b j as the input of generator. The same operation for unpaired data b j . After step 2, we save the output of generator ãj , bj when inputting a j , b j , i.e., the inferred missing data. Then we use the complete database to compute the common representation Z.</p><p>Step 3: Training encoder E 1 , E 2 , generator G 1 , G 2 , discriminator D 1 /D 2 , and clustering layer on inferred data by step 2. Finally, we use the clustering centroids {μ j } k j=1 from step 1, the common representation Z, and the completed data {a i , b i } , a j , bj , ãj , b j from step 2 as input for encoders and clustering layer to train all model. In each iteration, we update the the clustering centroids, the common representation and the inferred missing data again.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTAL ANALYSIS</head><p>In this section, we evaluate our method over three kinds of databases image feature with image feature (HW), image feature with text feature (BDGP), image pixels with image pixels (MNIST). To prove the effectiveness of our method, we compare it with some state-of-the-art partial multi-view clustering methods (Incomplete Multi-Modal Visual Data Grouping (IMG) <ref type="bibr" target="#b10">[11]</ref>, Partial Multi-View Clustering using Graph Regularized NMF (GPVC) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>). Additionally, some multi-view clustering approaches: Feature Concatenation Spectral Clustering (ConSC) <ref type="bibr" target="#b18">[19]</ref>, Robust Multi-view Spectral Clustering (RMSC) <ref type="bibr" target="#b19">[20]</ref>, Auto-weighted Multiple Graph Learning (AMGL) <ref type="bibr" target="#b20">[21]</ref>, and spectral clustering for single view are also conducted in our experiments for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setting 1) Dataset:</head><p>In the experiment, we validate the clustering performance of our method on three different kinds of multiview data, each of which is briefly introduced as follows.</p><p>Image feature with image feature: Handwritten numerals (HW) <ref type="bibr" target="#b21">[22]</ref> database consists of 2,000 images for 10 classes from 0 to 9 digit. Each class contains 200 samples. Each sample has 2 kinds of features: 76 Fourier coefficients and 216 profile correlations.</p><p>Image feature with text feature: BDGP <ref type="bibr" target="#b22">[23]</ref> is a twoview database. One is visual view and the other is textual view. It contains 2,500 images about drosophila embryos belonging to 5 categories. Each image is represented by a 1,750-D visual vector and a 79-D textual feature vector. In our experiment, we use all data on BDGP database, and evaluate the performance on both visual feature and textual feature.</p><p>Image pixels with image pixels: MNIST <ref type="bibr" target="#b23">[24]</ref> is a handwritten digits image database, and the image size is 28 × 28 pixels. MNIST consists of 60,000 training examples and 10,000 testing examples. In our experiments, we use two views, the first view is the original black and white image of MNIST, and the second view is the corresponding edge image of the first view <ref type="bibr" target="#b24">[25]</ref>. On account of the comparison methods cannot be conducted on large scale database, we randomly sample 4000 images to consist a new sampled MNIST database to do the partial multi-view clustering experiment.  2) Baseline methods and evaluate metrics: To simulate the partial view setting, we test all the methods under different impartial ratio (1-partial/incomplete example ratio). Impartial ratio varying from 0.1 to 0.9 with an interval of 0.2. Considering that some multi-view clustering cannot deal with missing instances, we use the average feature vector to fill in the missing instances at first. This process is repeated 10 times in our experiments. We evaluate the clustering performance with three standard clustering evaluation metrics, i.e. Accuracy (ACC) <ref type="bibr" target="#b25">[26]</ref>, Normalized Mutual Information (NMI) <ref type="bibr" target="#b26">[27]</ref>. TABLE I lists the average clustering accuracy of all methods on the HW database, BDGP database, and sampled MNIST database. Fig. <ref type="figure">2</ref> shows the average clustering NMI vs. different impartial ratios on the three databases.</p><p>3) Implementation details: We implement our algorithm with PyTorch and run all the experiments on the platform of Ubuntu Linux 16.04 with NVIDIA Titan V Graphics Processing Units (GPUs) and 32 GB memory size. We use Adam <ref type="bibr" target="#b27">[28]</ref> optimizer with default parameter setting to train our model and fix the learning rate as 0.0001. We conduct 20 epoches for each training step. All the other methods are tested on the same environment by Matlab.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Partial Multi-view Clustering Performance</head><p>As shown by TABLE I, and Fig. <ref type="figure">2</ref>, our approach generally achieves the best clustering performance on all the cases.</p><p>Here, we summarize some observations as follows.</p><p>1. From TABLE I, and Fig. <ref type="figure">2</ref>, we observe that partial multi-view methods achieve superior results in most cases, especially when the partial ratio is large. It illustrates that the existence of missing view data will degrade the performance of multi-view clustering methods. We can see that partial multi-view methods are more effective when encountering with partial multi-view data problem. TABLE I also shows the multi-view clustering method AMGL becomes worse than single-view methods when there exist incomplete data. It further indicates that some multi-view methods are easily influenced by data missing and noise data.</p><p>2. The results of TABLE I also illustrate that our method outperforms other state-of-the-art methods, It is probably due to the fact that our method learns a consistent clustering structure for each view and uses it to infer the missing data. Then the inferred missing data puts forward to a more effective common subspace, which is crucial for the final clustering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation study</head><p>In order to show the effect of each loss in our objective function, we run ablation studies to isolate the effect of the Auto-encoder, GAN, and clustering layer. Due to our training procedure is step by step, we simply compute the clustering Step 3 uses all models under total objective function. TABLE II respectively shows the clustering accuracy for these three kinds of loss. We can see the clustering accuracy will increase with adding loss step by step. It illustrates each loss in our objective function is significant for the final performance of our method. GAN loss improves the result of AE loss, it prove the contribution that generated missing view data promote to achieve a better clustering result. Clustering loss boosts the result of the first two component. This illustrates that a well common representation help infer more realistic missing view data and they promote mutually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we propose a novel consistent GAN model for partial multi-view clustering task, which simultaneously learns an excellent clustering structure and infers the incomplete views on the common subspace structure via GAN model. In addition, the imputation of incomplete view data can further study a consistent common structure which can improve the clustering performance. Comprehensive experiments validate the clustering performance improvement of the proposed method compared with state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>be the number of views, N be the number of samples, and d v be the feature dimension of v-th view. Considering the partial multi-view setting, we separate the data to two parts: paired data {a i , b i } which has complete view, unpaired data {a j , b j } which only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Notation for partial multi-view data. has view 1 or view 2 data, as shown in Fig. 1. ãi and bi respectively denote the missing data or generated data of view 1 and view 2. Network architecture: The model consists of seven sub-networks, two stack fully-connected encoders E 1 /E 2 , one deep embedding clustering layer, two stack fullyconnected generators G 1 /G 2 , two fully-connected discriminators D 1 /D 2 . Then we will give detailed introduction for the framework. 1. Encoder E 1 /E 2 : R dv → R m . E 1 , made up of L stacked fully connected layers, aims to learn a latent representation Z 1 = z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5 NMIFigure 2 :</head><label>52</label><figDesc>Figure 2: The Average Clustering NMI of all the methods vs. different impartial ratio on the three Database: (a) HW database, (b) BDGP database, (c) MNIST database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>The average Clustering Accuracy vs. different impartial ratio on the HW Database, BDGP Database, and the sampled MNIST Database.</figDesc><table><row><cell>Methods</cell><cell>0.1</cell><cell>0.3</cell><cell>HW 0.5</cell><cell>0.7</cell><cell cols="2">0.9</cell><cell>0.1</cell><cell>0.3</cell><cell>BDGP 0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>0.1</cell><cell cols="3">sampled MNIST 0.3 0.5 0.7</cell><cell>0.9</cell></row><row><cell>SC1</cell><cell>44.58</cell><cell>50.97</cell><cell>57.42</cell><cell>64.32</cell><cell cols="2">72.12</cell><cell>32.96</cell><cell>35.39</cell><cell>38.45</cell><cell>41.03</cell><cell>44.04</cell><cell>34.83</cell><cell>39.56</cell><cell>44.29</cell><cell>47.74</cell><cell>52.77</cell></row><row><cell>SC2</cell><cell>29.81</cell><cell>32.79</cell><cell>35.27</cell><cell>37.66</cell><cell cols="2">41.73</cell><cell>47.48</cell><cell>51.69</cell><cell>56.92</cell><cell>61.39</cell><cell>67.16</cell><cell>26.45</cell><cell>29.44</cell><cell>32.07</cell><cell>35.04</cell><cell>38.87</cell></row><row><cell>AMGL</cell><cell>34.26</cell><cell>42.21</cell><cell>50.24</cell><cell>60.37</cell><cell cols="2">67.83</cell><cell>25.24</cell><cell>23.57</cell><cell>25.38</cell><cell>28.07</cell><cell>29.58</cell><cell>15.58</cell><cell>14.12</cell><cell>15.24</cell><cell>24.15</cell><cell>33.46</cell></row><row><cell>RMSC</cell><cell>40.16</cell><cell>46.25</cell><cell>55.64</cell><cell>63.30</cell><cell cols="2">69.78</cell><cell>33.95</cell><cell>36.83</cell><cell>39.07</cell><cell>42.33</cell><cell>44.99</cell><cell>34.92</cell><cell>41.50</cell><cell>45.75</cell><cell>49.60</cell><cell>51.44</cell></row><row><cell>ConSC</cell><cell>44.57</cell><cell>48.25</cell><cell>54.53</cell><cell>64.54</cell><cell cols="2">77.97</cell><cell>27.81</cell><cell>22.30</cell><cell>21.39</cell><cell>21.06</cell><cell>28.84</cell><cell>37.04</cell><cell>35.81</cell><cell>36.74</cell><cell>41.37</cell><cell>50.88</cell></row><row><cell>GPVC</cell><cell>32.38</cell><cell>30.77</cell><cell>34.19</cell><cell>42.36</cell><cell cols="2">57.30</cell><cell>50.15</cell><cell>54.24</cell><cell>62.77</cell><cell>68.33</cell><cell>75.46</cell><cell>35.25</cell><cell>38.64</cell><cell>42.38</cell><cell>44.01</cell><cell>46.44</cell></row><row><cell>IMG</cell><cell>53.50</cell><cell>54.55</cell><cell>54.57</cell><cell>55.29</cell><cell cols="2">56.33</cell><cell>43.73</cell><cell>45.08</cell><cell>48.68</cell><cell>50.55</cell><cell>51.76</cell><cell>46.55</cell><cell>46.40</cell><cell>46.13</cell><cell>45.92</cell><cell>46.22</cell></row><row><cell>Ours</cell><cell>69.82</cell><cell>83.80</cell><cell>88.06</cell><cell>90.30</cell><cell cols="2">92.34</cell><cell>52.10</cell><cell>67.11</cell><cell>86.31</cell><cell>91.54</cell><cell>94.98</cell><cell>45.17</cell><cell>48.36</cell><cell>52.80</cell><cell cols="2">52.02 53.40</cell></row><row><cell></cell><cell></cell><cell cols="2">SC1</cell><cell>SC2</cell><cell cols="2">AMGL</cell><cell cols="2">RMSC</cell><cell cols="2">ConSC</cell><cell>GPVC</cell><cell></cell><cell>IMG</cell><cell>Ours</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NMI</cell><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>The ablation study of our approach under different impartial ratios on the HW dataset, where we show the performance of our method with different loss function by clustering accuracy. , step 2 and step 3 of Algorithm 1 to evaluate the performance of each component. Step 1 only uses auto-encoder network under AE loss. Step 2 is based on the result of step 1, so the result of step 2 is obtained by autoencoder and GAN network under AE loss and GAN loss;</figDesc><table><row><cell>Loss</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell>AE</cell><cell cols="5">0.5800 0.7192 0.7734 0.7944 0.8216</cell></row><row><cell>AE+GAN</cell><cell cols="5">0.6034 0.7554 0.7817 0.8590 0.8777</cell></row><row><cell cols="6">AE+GAN+Clustering 0.6590 0.8232 0.8651 0.8855 0.9070</cell></row><row><cell cols="2">performance after step 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is supported by National Natural Science Foundation of China under Grant 61773302, and 61728103.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on multi-view learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1304.5634</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of multi-view machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="2031" to="2038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-view video representation based on fast monte carlo surface reconstruction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Casas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3342" to="3352" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Robust multi-view representation: A unified perspective from multi-view learning to domain adaption</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="5434" to="5440" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-weighted multiview clustering with multiple graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2564" to="2570" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From ensemble clustering to multi-view clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2843" to="2849" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Partial multi-view clustering</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Clustering on multiple incomplete datasets via collective kernel learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1181" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incomplete multi-modal visual data grouping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2392" to="2398" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Partial multi-view clustering using graph regularized nmf</title>
		<author>
			<persName><forename type="first">N</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Deshmukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2192" to="2197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Double constrained nmf for partial multi-view clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DICTA</title>
		<imprint>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Missing modalities imputation via cascaded residual autoencoder</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1405" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Vigan: Missing view imputation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06724</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-view image generation from a single-view</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04886</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parameter-free auto-weighted multiple graph learning: A framework for multiview clustering and semi-supervised classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1881" to="1887" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition by combined classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Van Breukelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Den</forename><surname>Hartog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Kybernetika</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="386" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint stage recognition and anatomical annotation of drosophila gene expression patterns</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="16" to="24" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Document clustering using locality preserving indexing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TKDE</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1624" to="1637" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Normalized mutual information feature selection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Estévez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tesmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="201" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
