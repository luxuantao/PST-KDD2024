<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CurrMG: A Curriculum Learning Approach for Graph Based Molecular Property Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yaowen</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution">Union Medical College</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>Peking, China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Si</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution">Union Medical College</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>Peking, China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jiao</forename><surname>Li</surname></persName>
							<email>li.jiao@imicams.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Medical Information Chinese Academy of Medical Sciences</orgName>
								<orgName type="institution">Union Medical College</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>Peking, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CurrMG: A Curriculum Learning Approach for Graph Based Molecular Property Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/BIBM52615.2021.9669478</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Molecular Property Prediction</term>
					<term>Curriculum Learning</term>
					<term>Molecule Graph Learning</term>
					<term>Training Strategy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays computational methods in bioinformatics and cheminformatics have been widely used in molecular property prediction, advancing activities such as drug discovery. Combining to expert manual annotation of molecular features, machine learning approaches have gained satisfying prediction accuracies in most molecular property prediction tasks. Recently, Graph neural networks (GNNs) have gained increasing popularity in cheminformatics, where a chemical molecule structure is represented as a graph, and have made monumental progress in molecular property prediction. However, GNNs models requires large amounts of training samples, and the diversified molecular structure information might under-utilized when the model is trained with traditional random sampling strategies, thus leading to redundancy and inefficiency. Similar to human learning procedures, training of molecule graph learning models can benefit from an easy-todifficult curriculum. In this study, we proposed a curriculum learning approach for graph based molecular property prediction, called CurrMG. A data-aware integrated difficulty measurer was proposed to distinguish easy molecules from complex ones. Without any model redesign or external data, our training strategy improves model efficiency and accuracy in numerous molecular property prediction tasks and shows potential for low data drug discovery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INTRODUCTION</head><p>Bioinformatics and cheminformatics technologies have promoted the field of drug discovery such as molecular property prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. In the early years, machine learning approaches for molecular property prediction gained great interest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>. In this way, scientists were enthusiastic in designing molecular features by biological and chemical domain knowledge, converting molecule substructures and physicochemical properties to computer-readable formats like molecule fingerprint and descriptors <ref type="bibr" target="#b4">[5]</ref>. Combining such molecular representations and a machine learning approach (e.g., logistic regression, random forest, support vector machine, extreme gradient boosting, etc.), the workflow has been widely used and gained moderate performance for molecular property prediction <ref type="bibr" target="#b5">[6]</ref>, absorption, distribution, metabolism, excretion, and toxicity (ADMET) prediction <ref type="bibr" target="#b2">[3]</ref>, drug-target interaction prediction (DTI) <ref type="bibr" target="#b6">[7]</ref>, etc. However, the information loss due to the incomplete molecular representations and the lack of capacities for machine learning models have restricted further development of molecular property prediction approaches.</p><p>In recent years, deep learning has achieved landmark improvement in computer vision (CV) <ref type="bibr" target="#b7">[8]</ref>, natural language processing (NLP) <ref type="bibr" target="#b8">[9]</ref> and gradually been applied in the field of biology and chemistry. Researches have shown an excellent performance using deep learning models, such as Conventional Neural Networks (CNNs), Transformer <ref type="bibr" target="#b9">[10]</ref>,</p><p>BERT <ref type="bibr" target="#b8">[9]</ref>, to solve protein structure prediction <ref type="bibr" target="#b10">[11]</ref>, biomarker discovery <ref type="bibr" target="#b11">[12]</ref>, drug discovery <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> and drug repositioning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. For molecular property prediction, as molecules can be transferred into a sequence format-SMILES, sequence-based models are used for molecule modeling, such as SMILES-BERT <ref type="bibr" target="#b15">[16]</ref> and MG-BERT <ref type="bibr" target="#b16">[17]</ref>. Moreover, due to consisting of atoms and bonds, molecular structures can also convert to graphs which are made up of nodes and edges, graph-based models called Graph Neural Networks (GNNs) can take molecule graphs as input to learn molecular representations for the downstream tasks. In molecular property prediction task, many GNNs (MPNN <ref type="bibr" target="#b17">[18]</ref>, AttentiveFP <ref type="bibr" target="#b18">[19]</ref>, and Pre-trained GIN <ref type="bibr" target="#b19">[20]</ref>, etc.) have been proposed and achieved excellent performance, which gradually become a paradigm for solving these prediction tasks. However, the recent improvements mainly focus on the redesign of GNNs model architecture, ignoring the reexamination of model training process and data utilization efficiency.</p><p>Curriculum learning was first proposed by Bengio in 2009 <ref type="bibr" target="#b20">[21]</ref>. Inspired by the human learning process, curriculum learning aims to release the capacity of machine learning models by designing easy-to-difficult curriculums and training models on them. The frameworks of curriculum learning always include a difficulty measurer and a training scheduler <ref type="bibr" target="#b21">[22]</ref>. The difficulty measurer is used for calculating the difficulty coefficient for each data in the train set, and the training scheduler is used to arrange them as a curriculum in training. Wide applications on CV and NLP tasks of curriculum learning have proved its ability to improve model generalization and performance <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Similar to image and sentence sequence, molecule graph can be distinguished by its structure complexity and molecule graph learning model can benefit on curriculum learning. However, the research of curriculum learning on molecular data is limited. To address these issues, we proposed a curriculum-based learning approach for molecular property prediction, called CurrMG. This data-aware optimization algorithm focuses on the training phase of molecule graph learning, rearranging training data through their difficulty coefficients calculated with domain knowledge. In this way, the molecule graph learning model is trained gradually from "data easy to learn" to "data difficult to learn" (shown as Fig I <ref type="figure">)</ref>, contributing to training a better model. The experiments show a significant improvement of our approach on 6 molecular property benchmarks and 3 QSAR benchmarks compared with those without CurrMG. Especially, CurrMG also remains improvements combining with 5 different molecule graph learning models. In addition, the model convergence results indicate the potential application of CurrMG in low data resource drug discovery. In summary, our contributions can be concluded as follows:</p><p>• We explore and demonstrate the effectiveness of curriculum learning method in the field of molecular property prediction. This is the first time that curriculum learning is applied in this field.</p><p>• We propose a simple but solid curriculum-based learning framework consisting of difficulty measurer, CDF calculator, training scheduler, and GNN trainer, called CurrMG. Incorporating cheminformatics domain knowledge, CurrMG allows the GNN model to train from easy to difficult by three pre-design difficulty coefficients.</p><p>• Comprehensive analysis of various experiments indicates a universal improvement in several benchmarks and models. We also capture the further application potential for QSAR tasks and resourceeconomizing drug discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD</head><p>In this section, we introduce the framework of our approach, named CurrMG in detail, including the four core modules, and the whole workflow of our study.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Difficulty Measurer</head><p>In CurrMG, the difficulty measurer is a method of calculating the difficulty coefficient defined by cheminformatics domain knowledge. In our approach, we defined 3 base difficulty measurers calculated by molecular complexity, which we called , and .</p><p>is a simple but effective difficulty measurer based on molecule structure. It only focuses on the sum of the number of molecular atoms and bonds so that it denotes the absolute molecular structure complexity which also corresponds to the message passing complexity in GNNs:</p><formula xml:id="formula_0">= + ! "#<label>(1)</label></formula><p>Where is the i-th molecule, and ! "# represents the number of atoms and bonds in i-th molecule, respectively.</p><p>represents the content of cyclic carbon atoms which are sp 3 hybridized. is inspired by sp 3 index (Fsp 3 ) presented by Lovering <ref type="bibr" target="#b25">[26]</ref>. Fsp 3 takes saturation as the molecular complexity, which allows a more complex structure without increasing molecular weight. In CurrMG, is represented as: is a difficulty measurer that takes into account a variety of structural properties in molecules. MCE-18 is a robust molecule descriptor for the structure evolution analysis of medicinal chemistry initially proposed by Ivanenkov <ref type="bibr" target="#b26">[27]</ref>. Due to MCE-18 has made the comprehensive consideration about multiple substructures closely related to molecular quality and complexity, we take it in as a base difficulty measurer in CurrMG framework.</p><p>is represented as:</p><formula xml:id="formula_1">= :; &lt; + ; % &lt; + ; =&gt;?&lt; @ + ; AB?&lt;C + D '( ) E &amp; FD G+* E &amp; FD H*+* E &amp; FD '( ) E &amp; I × K<label>(3)</label></formula><p>Where ; &lt; denotes whether an aromatic or heteroaromatic ring is in (0 or 1), ; % &lt; denotes whether an aliphatic or a heteroaliphatic ring is in (0 or 1), ; =&gt;?&lt; @ denotes whether a chiral center is in (0 or 1), ; AB?&lt;C denotes whether a "spiro" point is in , ; 23 ) denotes Fsp 3 (from 0 to 1), ; =54 denotes the content of cyclic carbon atoms which are sp 3 hybridized (from 0 to 1). ; 454 denotes the content of acyclic carbon atoms which are sp 3 hybridized (from 0 to 1), while K is a normalized index.</p><p>Further, after defining 3 base difficulty measurers, we established a weighted fusion method to construct an integrated difficulty measurer D:</p><formula xml:id="formula_2">L = M + M N + M<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CDF Calculator</head><p>After calculating each molecular difficulty coefficient by difficulty measurer, we proposed a Cumulative Distribution Function (CDF) calculator to normalize the difficulty coefficient so that meeting the requirement of the training scheduler. In CurrMG framework, a discrete form molecular CDF value can be calculated by dividing the number of molecules whose difficulty coefficients are smaller than the current molecular into the total number of molecules in the training set. The CDF calculator O is represented as:</p><formula xml:id="formula_3">O = P L Q ≤ L = ∑ P T U #VT U<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Training Scheduler</head><p>In curriculum learning approach, a training scheduler is necessary to guide the model on which mini-batch data to train according to their difficulty coefficient. In CurrMG, inspired by Platanios <ref type="bibr" target="#b22">[23]</ref>, we used a competence function as a monotonically increasing curve to control data sampling space. In the training phase, training scheduler starts at sampling data with a low difficulty coefficient and constructs a mini-batch dataset, gradually expanding data sampling space as the training progresses, which means that training scheduler transfers training data with a higher difficulty coefficient for the model. The training scheduler W X is represented as:</p><formula xml:id="formula_4">W X = min \1, _X 4 à b + c d e a f<label>(6)</label></formula><p>Where X and g are the number of current iterations and total iterations of the training phase, respectively. c d is the initial competence value. h is an adjustable hyperparameter, which controls the change of sample space.</p><p>Once we take X as the current model state, we can figure out the threshold for sampling data through competence function which takes X as the unique independent variable. In training scheduler, the competence value calculated by the model state can correspond to the molecular difficulty coefficient to complete the control of the current training data by the model state (e.g., when the competence value is 0.2, so training scheduler randomly samples training data whose difficulty coefficient are lower than 0.2 as a mini-batch data in this training iteration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. GNN Trainer</head><p>As the universality of CurrMG, we believe CurrMG is suitable for any deep learning model. In our study, we used several GNNs as model trainers. In general, GNN is trained on graph data, aiming to learn node representations by aggregating neighbor information. As for molecule graph learning, given a molecule graph i = j, ℰ , where j denotes the set of atoms and ℰ denotes the set of bonds, the output of GNN model is:</p><formula xml:id="formula_5">ℎ m = READOUT ℎ , ℎ N , … , ℎ<label>(7)</label></formula><p>Where ℎ m is the graph-level representation variable of i, and ℎ is the node-level representations variable of v ∈ j , which is updated layer by layer. READOUT is a function for aggregating atom representations as a molecule representation. After GNN layers output graph-level representation variables, a multilayer perceptron (MLP) is joined end-to-end as a predictor for the current task:</p><p>x m = MLP ℎ m (8)</p><formula xml:id="formula_6">F.</formula><p>Training Details Loss function. The goal of the GNN trainer is to minimize the loss function. To improve numerical stability, in classification tasks, we used a BCEWithLogitsLoss as the loss function:</p><formula xml:id="formula_7">|}~~ , x = −€ •x |}‚ƒ " + 1 − x |}‚ƒ"1 − ƒ … †<label>(9)</label></formula><p>Where and x are the output of GNN model and the true label of i-th molecule, respectively.</p><p>In regression tasks, we used a SmoothL1Loss as the loss function: Model Hyperparameters. Since model hyperparameters are not the purpose of our study. Therefore, to focus on verifying the optimization effect of our approach on molecular graph model, we set a set of fixed parameters for each GNN model based on experience and used them on all benchmark dataset. The details of model hyperparameters are shown in Tabel I. As for CurrMG, we executed hyperparameters grid searches for 4 hyperparameters (M , M N , M in {0, 0.33, 0.5, 1}, respectively. And h in {2, 3}) to find out suitable hyperparameters for each task.</p><formula xml:id="formula_8">|}~~ , x = % ‡ N − x N v;| − x | &lt; 1 | − x | − N }XℎŠ‹€v~Š<label>(10)</label></formula><p>To ensure the stability of our results, for each experiment, we set 5 random seeds for 8:1:1 (train set, validation set, and test set) data splitting and model training. The average performance was reported in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENT</head><p>In this section, we introduce comprehensive evaluation and analysis of our approach, including the datasets and models used in our study, experiment results on model performance and convergence, and ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We evaluated CurrMG in 6 molecular property benchmark datasets and 3 Quantitative Structure-Activity Relationship (QSAR) benchmark datasets as additional evaluations. The benchmarks used in our study include FreeSolv, ESOL, Lipophilicity, Blood-Brain Barrier Penetration (BBBP), BACE, HIV, Monoamine, JAK2, and HERG, The molecular property benchmark datasets were downloaded from MoleculeNet <ref type="bibr" target="#b27">[28]</ref> and the QSAR benchmark datasets were acquired from Cortés-Ciriano <ref type="bibr" target="#b28">[29]</ref>.</p><p>A brief introduction of the 9 benchmark datasets is shown in Table <ref type="table" target="#tab_2">II</ref>. FreeSolv is a database including experimental hydration free energy of small molecules; ESOL is a dataset for water solubility of molecules; Lipophilicity is a dataset about octanol/water distribution coefficient (logD at pH7.4) of compounds curated from ChEMBL database; BBBP is a dataset focusing on the barrier permeability of drugs which is a vital property for drugs targeting central nervous system; BACE is a drug-target binding affinity dataset (binary label) for a set of inhibitors of human β-secretase 1 (BACE-1); HIV, published by the Drug Therapeutics Program (DTP), contains an AIDS Antiviral Screen result for over 40,000 compounds. In MoleculeNet, the screening results are transformed to binary labels of inactive and active. Monoamine, JAK2, and HERG are bioactivity datasets from ChEMBL representing the drug-target binding affinity (pIC50 value) of Monoamine oxidase A, Tyrosine-protein kinase JAK2 and HERG, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Models</head><p>In our study, 5 GNN models (GCN, MPNN, GAT, AttentiveFP, and Pre-trained GIN) were tested to prove the model-independent of CurrMG. Each of them is partially different from the other: GCN is a classical GNN model proposed by Kipf <ref type="bibr" target="#b29">[30]</ref>. Comparing with CNN, GCN can execute the convolution process in a graph, which belongs to non-euclidean space. GCN aggregates and updates node representations by multiplying a Laplacian matrix in every layer.</p><p>MPNN is a general GNN architecture defining a message function and an update function to learn node representations originally used for quantum chemistry <ref type="bibr" target="#b17">[18]</ref>. Different from GCN, MPNN also considers edge representations when calculating message function.</p><p>GAT is a GNN model proposed for the inductive task and assigning weights for different nodes <ref type="bibr" target="#b30">[31]</ref>. GAT brings attention mechanism to GNN model, multiplying attention coefficients to neighbor nodes' representations when aggregating.</p><p>AttentiveFP is an interpretable graph attention network for molecular representation <ref type="bibr" target="#b18">[19]</ref>. AttentiveFP uses the attention mechanism to learn atom-level local features and moleculelevel global features orderly. Moreover, hidden variables from AttentiveFP also show high correlations with empirical descriptors related to certain tasks.</p><p>Pre-trained GIN is a graph isomorphism neural network pre-trained on large-scale molecule data using several selfsupervised tasks <ref type="bibr" target="#b19">[20]</ref>. Due to the well-designed tasks, the pretrained GIN significantly improves performance in molecular property prediction downstream tasks. In our study, we use the GIN model pre-trained by context prediction task in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Performance on Molecular Property Benchmarks</head><p>The performance of 5 GNN models on 6 molecular property benchmarks are shown as TABLE <ref type="table" target="#tab_3">III</ref>. The overall performance results indicate that the trained model with CurrMG achieved better performance compared with those without CurrMG (except the GAT on FreeSolv). For every GNN model, the relative improvements when taking CurrMG as the training method are vary from 6.776% (AttentiveFP) to 12.114% (GAT) on regression tasks, and from 0.727% (Pretrained GIN) to 3.216% (AttentiveFP) on classification tasks.</p><p>For every molecular property benchmark, the relative improvements when taking CurrMG as the training method are vary from 4.954% (FreeSolv) to 13.941% (ESOL) on regression tasks, and from 1.332% (BBBP) to 2.209% (HIV) on classification tasks. It is also important to note that through the experiments on 6 molecular property benchmarks, we can observe a stable improvement of our approach-CurrMG compared with the conventional training method which only randomly samples data. The results also proved the sufficient stability and robustness of CurrMG, which is applicable to different GNN models and molecular datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Convergence on Molecular Property Benchmark</head><p>Due to the efficient data utilization efficiency of curriculum learning that it can improve model prediction capacity by just controlling the training data sequences. As for our approach, we also attempted to explore how CurrMG can help with model convergence including the convergence rate and degree. We printed the train loss and validation score curve of GCN trained on molecular property benchmarks to conclude this topic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The 6 train loss curves shown in Fig 3 indicate that</head><p>CurrMG caused a rapid model convergence during the early train phase and the final train loss is commonly lower than those without CurrMG. An important explanation is, as an adequate training using CurrMG, the data sampling space is gradually expanded so that the model is trained from a local convergence state to a global convergence state smoothly. (II) although it seems that CurrMG caused the model performance improvement on validation set more slowly, combining to the characteristics of CurrMG that the control of data sampling space, the true result is that CurrMG reaches a higher data utilization efficiency. When the model trained by CurrMG reached convergence at about 60% of total iterations, the training dataset is not been "used up", which means CurrMG made a trade-off to release the model capacity with fewer data but longer training iterations. For example, as for HIV benchmark, the model used all training data, and convergence reached less than 10,000 iterations when training without CurrMG. But when trained with CurrMG, the model convergence reached about 15,000 iterations but the used training data are less (77.5% of all training data calculated by competence function). These delightful findings proved the solid improvement in model convergence with CurrMG, while also revealed a feasible prospect for efficient data utilization and low data drug discovery. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>In this section, we delved into our approach on two interesting topics: (I) Are there some part of CurrMG redundant and can be abandoned? (II) What is the best combination of hyperparameters of CurrMG in a certain benchmark? We designed a module replacement experiment and a hyperparameter combination search experiment to answer these questions.</p><p>Module Replacement Experiment. We replaced the core modules of our curriculum learning approach CurrMGdifficulty measurer, CDF calculator, and training scheduler, to conclude that each module is indispensable. For difficulty measurer, we used a random number generator as the replacement of difficulty measurer to calculate the difficulty coefficient. For the CDF calculator, we used another normalization calculator-Min Max Scaler as the replacement.</p><p>For training scheduler, we simply set c d = 1 so that training scheduler can sample data with the whole sampling space in the training phase.</p><p>We used GCN as the GNN trainer and finished Module Replacement Experiment on 6 molecular property benchmark datasets. The model performance result is shown in Table <ref type="table" target="#tab_4">IV</ref>. CurrMG reached the optimal performance on FreeSolv, ESOL, Lipophilicity, BACE, and HIV, and the Min-Max Scaler ablation method reached the optimal performance on BBBP. From our perspective, the reason for the suboptimal performance on BBBP of CurrMG is that the Min-Max Scaler is also an effective method that is widely used in normalization. In our approach, Min-Max Scaler is regarded as an alternative solution in the normalization method which has achieved suboptimal performances on HIV and Lipophilicity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results on QSAR Benchmark</head><p>For further comparison, we finished experiments on 3 QSAR benchmark datasets as additional estimations. The performance of 4 GNN models on 3 QSAR benchmarks are shown as TABLE <ref type="table" target="#tab_6">VI</ref>. Similar to the model performance comparisons on molecular property benchmarks, the results on 3 QSAR benchmarks also show an outperforming performance of CurrMG. For every GNN model, the relative improvements when taking CurrMG as the training method are 4.297% (GCN), 6.729% (MPNN), 22.058% (GAT), and 8.938% (AttentiveFP). For every molecular property benchmark, the relative improvements when taking CurrMG as the training method are 14.165% (Monoamine), 10.604% (JAK2), and 6.747% (HERG). The results show CurrMG can not only apply in molecular property prediction but is also able to expand in other molecular graph-level prediction tasks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DISCUSSION</head><p>In this paper, our proposed approach-CurrMG significantly improved the performance of the molecular graph learning model on molecular property prediction tasks. Experiments on 5 molecular graph learning models and 9 benchmark datasets proved the effectiveness of CurrMG as a model-independent data-aware curriculum-based learning method. Compared with traditional training methods without curriculum learning, CurrMG sorts molecular samples through an integrated difficulty measurer and uses a modelaware training scheduler to control the data sampling space during the training phase, so that the model can be trained from simple samples and gradually transferred to more difficult samples. A comprehensive analysis has proved that this curriculum learning method is helpful to the convergence and generalization of the model. Moreover, results on model convergence also show a feasible prospect for low data drug discovery based on CurrMG. However, the hyperparameters of CurrMG-calculation method in difficulty measurer and the power term in training scheduler are task-related. It is still necessary to simplify the number of hyperparameters as much as possible to reduce the computation time. What's more, as the model-independence of CurrMG, it may suitable for any deep learning models which are not been explored in our study. In the future, we will focus on proposing more effective taskspecific difficulty measurers through label distance, and taskaware descriptors. While the complete evaluations of curriculum learning on molecular property prediction (e.g. influence on the computing complexity and suitable models, etc.) are also in the top priorities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we have proposed a curriculum-based learning approach for molecular property prediction, called CurrMG. In particular, CurrMG achieves a universal improvement of model convergence and performance on 5 typical molecule graph learning models and 9 benchmark datasets, which also indicate the model-independence and robustness of CurrMG. In addition, benefiting from its easyto-difficult training curriculum, we observe a moderate potential of data economization for CurrMG when achieving the same performance. These results have established CurrMG as a powerful and robust training optimization algorithm in solving the challenge of molecular property prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A concept diagram of curriculum learning in molecule data 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM) | 978-1-6654-0126-5/21/$31.00 ©2021 IEEE | DOI: 10.1109/BIBM52615.2021.9669478</figDesc><graphic url="image-1.png" coords="1,306.60,560.28,243.36,173.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The architecture of CurrMG</figDesc><graphic url="image-2.png" coords="2,45.36,496.32,243.24,251.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>" and 4789 " is the number of cyclic carbon atoms which are sp 3 hybridized and the total number of carbon atoms in i-th molecule, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The train loss curve of GCN on molecular property benchmarks. Fig 3A is GCN train loss curve on FreeSolv; Fig 3B is GCN train loss curve on ESOL; Fig 3C is GCN train loss curve on Lipophilicity; Fig 3D is GCN train loss curve on BACE; Fig 3E is GCN train loss curve on BBBP; Fig 3F is GCN train loss curve on HIV.</figDesc><graphic url="image-3.png" coords="5,51.48,484.20,492.36,217.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The validation score curve of GCN on molecular property benchmarks.. Fig 4A is GCN validation score curve on FreeSolv; Fig 4B is GCN validation score curve on ESOL; Fig 4C is GCN validation score curve on Lipophilicity; Fig 4D is GCN validation score curve on BACE; Fig 4E is GCN validation score curve on BBBP; Fig 4F is GCN validation score curve on HIV</figDesc><graphic url="image-4.png" coords="6,51.48,173.64,492.36,217.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I .</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>DETAILS OF MODEL HYPERPARAMETERS</cell></row><row><cell>Model</cell><cell>Hyperparameters</cell></row><row><cell></cell><cell>dropout=0.1, gnn_hidden_feats=128,</cell></row><row><cell>GCN</cell><cell>num_gnn_layers=2, predictor_hidden_feats=64, learning rate=0.002, batch size=128,</cell></row><row><cell></cell><cell>weight decay=0.001</cell></row><row><cell></cell><cell>alpha=0.5, dropout=0.05,</cell></row><row><cell></cell><cell>gnn_hidden_feats=128, num_gnn_layers=2,</cell></row><row><cell>GAT</cell><cell>num_heads=6, predictor_hidden_feats=128,</cell></row><row><cell></cell><cell>learning rate=0.01, batch size=128,</cell></row><row><cell></cell><cell>weight decay=0.0005</cell></row><row><cell></cell><cell>edge_hidden_feats=64, node_out_feats=48,</cell></row><row><cell>MPNN</cell><cell>num_step_message_passing=2, learning rate=0.001,</cell></row><row><cell></cell><cell>batch size=128, weight decay=0.0005</cell></row><row><cell></cell><cell>dropout=0.2, graph_feat_size=32,</cell></row><row><cell>AttentiveFP</cell><cell>num_layers=2, num_timesteps=3, learning rate=0.01, batch size=128,</cell></row><row><cell></cell><cell>weight decay=0.001</cell></row><row><cell></cell><cell>jk='concat', readout='sum',</cell></row><row><cell>Pre-trained GIN</cell><cell>learning rate=0.001, batch size=128,</cell></row><row><cell></cell><cell>weight decay=0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">DESCRIPTIONS OF BENCHMARK DATASETS</cell><cell></cell><cell></cell></row><row><cell>Category</cell><cell>Dataset</cell><cell>Compounds</cell><cell>Task Type</cell><cell>Metric</cell><cell>Resource</cell></row><row><cell>Physical Chemistry</cell><cell>FreeSolv</cell><cell>642</cell><cell>Regression</cell><cell>RMSE</cell><cell>[28]</cell></row><row><cell>Physical Chemistry</cell><cell>ESOL</cell><cell>1128</cell><cell>Regression</cell><cell>RMSE</cell><cell>[28]</cell></row><row><cell>Physical Chemistry</cell><cell>Lipophilicity</cell><cell>4200</cell><cell>Regression</cell><cell>RMSE</cell><cell>[28]</cell></row><row><cell>Physiology</cell><cell>BBBP</cell><cell>2053</cell><cell>Classification</cell><cell>AUC</cell><cell>[28]</cell></row><row><cell>Biophysics</cell><cell>BACE</cell><cell>1513</cell><cell>Classification</cell><cell>AUC</cell><cell>[28]</cell></row><row><cell>Biophysics</cell><cell>HIV</cell><cell>41127</cell><cell>Classification</cell><cell>AUC</cell><cell>[28]</cell></row><row><cell>QSAR</cell><cell>Monoamine</cell><cell>1379</cell><cell>Regression</cell><cell>RMSE</cell><cell>[29]</cell></row><row><cell>QSAR</cell><cell>JAK2</cell><cell>2655</cell><cell>Regression</cell><cell>RMSE</cell><cell>[29]</cell></row><row><cell>QSAR</cell><cell>HERG</cell><cell>5207</cell><cell>Regression</cell><cell>RMSE</cell><cell>[29]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III .</head><label>III</label><figDesc>COMPARISON OF PERFORMANCE ON MOLECULAR PROPERTY BENCHMARKS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Trainer</cell><cell cols="2">FreeSolv</cell><cell cols="2">ESOL</cell><cell cols="2">Lipophlicity</cell><cell>BBBP</cell><cell></cell><cell>BACE</cell><cell></cell><cell>HIV</cell></row><row><cell></cell><cell></cell><cell cols="2">RMSE Δ% b</cell><cell>RMSE</cell><cell cols="2">Δ% RMSE</cell><cell>Δ%</cell><cell cols="2">ROC-AUC Δ%</cell><cell cols="2">ROC-AUC Δ%</cell><cell>ROC-AUC</cell><cell>Δ%</cell></row><row><cell>GCN</cell><cell cols="2">Random 0.881 CurrMG 0.864</cell><cell>2.001</cell><cell>0.789 0.675</cell><cell>13.825</cell><cell>0.699 0.632</cell><cell>9.462</cell><cell>0.912 0.920</cell><cell>0.883</cell><cell>0.902 0.907</cell><cell>0.536</cell><cell>0.752 0.772</cell><cell>2.629</cell></row><row><cell>MPNN</cell><cell cols="2">Random 1.375 CurrMG 1.205</cell><cell>12.316</cell><cell>0.826 0.667</cell><cell>19.339</cell><cell>0.689 0.658</cell><cell>4.472</cell><cell>0.852 0.880</cell><cell>3.135</cell><cell>0.824 0.845</cell><cell>2.387</cell><cell>0.705 0.722</cell><cell>2.467</cell></row><row><cell>GAT</cell><cell cols="2">Random 1.258 CurrMG 1.277</cell><cell>-1.566</cell><cell>0.975 0.847</cell><cell>13.120</cell><cell>1.162 0.874</cell><cell>24.789</cell><cell>0.870 0.880</cell><cell>1.080</cell><cell>0.845 0.859</cell><cell>1.634</cell><cell>0.640 0.645</cell><cell>0.664</cell></row><row><cell>AttentiveFP</cell><cell cols="2">Random 1.078 CurrMG 1.002</cell><cell>7.065</cell><cell>0.694 0.628</cell><cell>9.481</cell><cell>0.760 0.731</cell><cell>3.781</cell><cell>0.884 0.891</cell><cell>0.808</cell><cell>0.834 0.876</cell><cell>4.838</cell><cell>0.646 0.673</cell><cell>4.004</cell></row><row><cell>Pre-trained GIN a</cell><cell>Random CurrMG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.921 0.928</cell><cell>0.755</cell><cell>0.891 0.892</cell><cell>0.146</cell><cell>0.746 0.756</cell><cell>1.281</cell></row></table><note>a. We only report classification results for Pre-trained GIN since the original implementation<ref type="bibr" target="#b19">[20]</ref> do not admit regression task.b. For regression task, Δ% denotes 100×(Metric Random -Metric CurrMG)/ Metric Random, where for classification tasks Δ% denotes 100×(Metric CurrMG -Metric Random)/ Metric CurrMG</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV .</head><label>IV</label><figDesc></figDesc><table><row><cell>HIV</cell><cell>ROC-AUC</cell><cell>0.752</cell><cell>0.755</cell><cell></cell><cell>0.747</cell><cell>0.759</cell><cell>0.772</cell></row><row><cell cols="4">Hyperparameter Combination Search Experiment. There are 4 hyperparameters existing in CurrMG-M , M N , M and h, which represent the use of , the use of ,</cell><cell cols="4">and recorded the top3 hyperparameter combinations (shown as TABLE V). Results indicate that at least 3 hyperparameter combinations of CurrMG have achieved improvement</cell></row><row><cell cols="4">the use of increasing curve in training scheduler, respectively. Due to the and the steepness of the monotonically difference in difficulty coefficient distribution in different benchmarks leading to a slight influence on CurrMG, it's necessary to design a hyperparameter search to figure out the optimal hyperparameter combination in a current task. Represented by GCN, we executed hyperparameters grid searches (see model hypterparameter in METHOD section)</cell><cell cols="4">compared to those without CurrMG in almost all molecular property prediction tasks. Moreover, the easiest difficulty measurer got the best performance which participated in the 5 best hyperparameter combinations of 6 tasks. Meanwhile, integrating different difficulty measurer is beneficial to gain steady improvements which occupy 11 of 18 top3 performance records. As for h , related to the smoothness of sample difficulty change during training, h = 2 is the optimum value for most tasks.</cell></row><row><cell></cell><cell></cell><cell cols="5">COMPARISON OF PERFORMANCE ON MOLECULAR PROPERTY BENCHMARKS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Ablation Method</cell><cell></cell></row><row><cell>Dataset</cell><cell>Metric</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Baseline</cell><cell cols="2">Random Difficulty Measurer</cell><cell>C0=1</cell><cell>Min Max Scaler</cell><cell>CurrMG</cell></row><row><cell>FreeSolv</cell><cell>RMSE</cell><cell>0.881</cell><cell>1.045</cell><cell></cell><cell>0.928</cell><cell>0.923</cell><cell>0.864</cell></row><row><cell>ESOL</cell><cell>RMSE</cell><cell>0.784</cell><cell>0.706</cell><cell></cell><cell>0.695</cell><cell>0.723</cell><cell>0.687</cell></row><row><cell>Lipophilicity</cell><cell>RMSE</cell><cell>0.699</cell><cell>0.675</cell><cell></cell><cell>0.685</cell><cell>0.662</cell><cell>0.651</cell></row><row><cell>BBBP</cell><cell>ROC-AUC</cell><cell>0.912</cell><cell>0.916</cell><cell></cell><cell>0.917</cell><cell>0.927</cell><cell>0.920</cell></row><row><cell>BACE</cell><cell>ROC-AUC</cell><cell>0.902</cell><cell>0.904</cell><cell></cell><cell>0.900</cell><cell>0.903</cell><cell>0.907</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V .</head><label>V</label><figDesc>COMPARISON OF PERFORMANCE ON MOLECULAR PROPERTY BENCHMARKS</figDesc><table><row><cell></cell><cell>1st</cell><cell></cell><cell>2nd</cell><cell></cell><cell>3rd</cell><cell></cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Parameters</cell><cell>Metrics b</cell><cell>Parameters</cell><cell>Metrics</cell><cell>Parameters</cell><cell>Metrics</cell></row><row><cell>FreeSolv</cell><cell>λ1=1,λ2=0,λ3=0,α=2</cell><cell>0.864(+)</cell><cell>λ1=0,λ2=0,λ3=1,α=2</cell><cell>0.867(+)</cell><cell>λ1=0,λ2=0,λ3=1,α=3</cell><cell>0.916(-)</cell></row><row><cell>ESOL</cell><cell>λ1=0.33,λ2=0.33,λ3=0.33,α=2</cell><cell>0.675(+)</cell><cell>λ1=0.5,λ2=0.5,λ3=0,α=2</cell><cell>0.682(+)</cell><cell>λ1=0.5,λ2=0,λ3=0.5,α=3</cell><cell>0.687(+)</cell></row><row><cell>Lipophilicity</cell><cell>λ1=0.5,λ2=0.5,λ3=0,α=2</cell><cell>0.632(+)</cell><cell>λ1=0.5,λ2=0,λ3=0.5,α=2</cell><cell>0.651(+)</cell><cell>λ1=0,λ2=0,λ3=1,α=3</cell><cell>0.653(+)</cell></row><row><cell>BACE</cell><cell>λ1=1,λ2=0,λ3=0,α=3</cell><cell>0.907(+)</cell><cell>λ1=0,λ2=0,λ3=1,α=2</cell><cell>0.905(+)</cell><cell>λ1=0.5,λ2=0.5,λ3=0,α=2</cell><cell>0.904(+)</cell></row><row><cell>BBBP</cell><cell>λ1=0.5,λ2=0.5,λ3=0,α=3</cell><cell>0.920(+)</cell><cell>λ1=0,λ2=0.5,λ3=0.5,α=3</cell><cell>0.916(+)</cell><cell>λ1=0.5,λ2=0,λ3=0.5,α=2</cell><cell>0.916(+)</cell></row><row><cell>HIV</cell><cell>λ1=0,λ2=0,λ3=1,α=2</cell><cell>0.772(+)</cell><cell>λ1=0.5,λ2=0,λ3=0.5,α=2</cell><cell>0.764(+)</cell><cell>λ1=0.5,λ2=0,λ3=0.5,α=3</cell><cell>0.760(+)</cell></row></table><note>c. The metrics of FreeSolv, ESOL, and Lipophilicity are RMSE. In BACE, BBBP, and HIV, the metrics are ROC-AUC. (+) indicates the current result exceeds baseline result, while (-) is converse.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI .</head><label>VI</label><figDesc>COMPARISON OF PERFORMANCE ON QSAR BENCHMARKS</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dataset</cell><cell></cell></row><row><cell>Model</cell><cell>Trainer</cell><cell>Monoamine</cell><cell></cell><cell>JAK2</cell><cell></cell><cell>HERG</cell></row><row><cell></cell><cell></cell><cell>RMSE</cell><cell>Δ%</cell><cell>RMSE</cell><cell>Δ%</cell><cell>RMSE</cell><cell>Δ%</cell></row><row><cell></cell><cell>Random</cell><cell>0.727</cell><cell></cell><cell>0.811</cell><cell></cell><cell>0.720</cell></row><row><cell>GCN</cell><cell></cell><cell></cell><cell>2.733</cell><cell></cell><cell>4.885</cell><cell>5.274</cell></row><row><cell></cell><cell>CurrMG</cell><cell>0.708</cell><cell></cell><cell>0.771</cell><cell></cell><cell>0.682</cell></row><row><cell></cell><cell>Random</cell><cell>0.908</cell><cell></cell><cell>0.890</cell><cell></cell><cell>0.714</cell></row><row><cell>MPNN</cell><cell></cell><cell cols="2">11.891</cell><cell></cell><cell>4.653</cell><cell>3.642</cell></row><row><cell></cell><cell>CurrMG</cell><cell>0.800</cell><cell></cell><cell>0.849</cell><cell></cell><cell>0.688</cell></row><row><cell></cell><cell>Random</cell><cell>1.349</cell><cell></cell><cell>1.104</cell><cell></cell><cell>0.990</cell></row><row><cell>GAT</cell><cell></cell><cell cols="2">35.035</cell><cell></cell><cell>20.080</cell><cell>11.060</cell></row><row><cell></cell><cell>CurrMG</cell><cell>0.876</cell><cell></cell><cell>0.882</cell><cell></cell><cell>0.880</cell></row><row><cell></cell><cell>Random</cell><cell>0.928</cell><cell></cell><cell>0.995</cell><cell></cell><cell>0.841</cell></row><row><cell>AttentiveFP</cell><cell></cell><cell></cell><cell>7.002</cell><cell></cell><cell>12.800</cell><cell>7.013</cell></row><row><cell></cell><cell>CurrMG</cell><cell>0.863</cell><cell></cell><cell>0.868</cell><cell></cell><cell>0.782</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Authorized licensed use limited to: Tsinghua University. Downloaded on December 31,2022 at 10:06:29 UTC from IEEE Xplore. Restrictions apply.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This work is supported by The National Key Research and Development Program of China (Grant No. 2017YFC0907503 and No.2016YFC0901901) and the National Natural Science Foundation of China (Grant No. 81601573).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of current trends in computational drug repositioning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Butte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Swamidass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2" to="12" />
			<date type="published" when="2016-01">Jan 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artificial intelligence in drug design</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China. Life sciences</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1191" to="1204" />
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ADMETlab: a platform for systematic ADMET evaluation based on a comprehensively collected ADMET database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Cheminform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2018-06-26">Jun 26 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent applications of deep learning and machine intelligence on in silico drug discovery: methods, tools and databases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Rifaioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Atas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cetin-Atalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Atalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Doğan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1878" to="1912" />
			<date type="published" when="2019-09-27">Sep 27 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An overview of molecular fingerprint similarity search in virtual screening</title>
		<author>
			<persName><forename type="first">I</forename><surname>Muegge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert opinion on drug discovery</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Machine learning methods for property prediction in chemoinformatics: Quo Vadis?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Varnek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Baskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Model</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1413" to="1437" />
			<date type="published" when="2012-06-25">Jun 25 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DNN-DTIs: Improved drug-target interactions prediction using XGBoost feature selection and deep neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page">104676</biblScope>
			<date type="published" when="2021-07-29">Jul 29 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Highly accurate protein structure prediction for the human proteome</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tunyasuvunakool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<date type="published" when="2021-07-22">Jul 22 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning for prediction of colorectal cancer outcome: a discovery and validation study</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Skrede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="issue">10221</biblScope>
			<biblScope unit="page" from="350" to="360" />
			<date type="published" when="2020-02-01">Feb 1 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating Novel Compounds Targeting SARS-CoV-2 Main Protease Based on Imbalanced Dataset</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="432" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting Drugs for COVID-19/SARS-CoV-2 via Heterogeneous Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="455" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Drug Repositioning for SARS-CoV-2 Based on Graph Neural Network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="319" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SMILES-BERT: large scale unsupervised pre-training for molecular property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics</title>
				<meeting>the 10th ACM international conference on bioinformatics, computational biology and health informatics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="429" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MG-BERT: leveraging unsupervised atomic representation learning for molecular property prediction</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<date type="published" when="2021-05-05">May 5 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pushing the Boundaries of Molecular Representation for Drug Discovery with the Graph Attention Mechanism</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page" from="8749" to="8760" />
			<date type="published" when="2020-08-27">Aug 27 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
				<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Survey on Curriculum Learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Competence-based Curriculum Learning for Neural Machine Translation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Platanios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Stretcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1162" to="1172" />
			<pubPlace>Minneapolis, Minnesota</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Curriculum learning for natural language understanding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6095" to="6104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multimodal curriculum learning for semi-supervised image classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3249" to="3260" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Escape from flatland: increasing saturation as an approach to improving clinical success</title>
		<author>
			<persName><forename type="first">F</forename><surname>Lovering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bikker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Humblet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="6752" to="6756" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Are We Opening the Door to a New Era of Medicinal Chemistry or Being Collapsed to a Chemical Singularity?</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">A</forename><surname>Ivanenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Zagribelnyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Aladinskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="10026" to="10043" />
			<date type="published" when="2019-11-27">Nov 27 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MoleculeNet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018-01-14">Jan 14 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Confidence: A Computationally Efficient Framework for Calculating Reliable Prediction Errors for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cortés-Ciriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1269" to="1281" />
			<date type="published" when="2019">2019/03/25 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
