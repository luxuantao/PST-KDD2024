<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Multiple Source Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
							<email>hzhao1@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IST</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Universidade de Lisboa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
							<email>shanghaz@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IST</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Universidade de Lisboa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
							<email>guanhanw@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IST</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Universidade de Lisboa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">João</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">IST</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Universidade de Lisboa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">José</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
							<email>moura@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IST</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Universidade de Lisboa</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
							<email>ggordon@andrew.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IST</orgName>
								<orgName type="institution" key="instit1">Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Universidade de Lisboa</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Multiple Source Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">ED897230F8F8C8CE7CB44310CA3EE10B</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While domain adaptation has been actively researched, most algorithms focus on the single-source-single-target adaptation setting. In this paper we propose new generalization bounds and algorithms under both classification and regression settings for unsupervised multiple source domain adaptation. Our theoretical analysis naturally leads to an efficient learning strategy using adversarial neural networks: we show how to interpret it as learning feature representations that are invariant to the multiple domain shifts while still being discriminative for the learning task. To this end, we propose multisource domain adversarial networks (MDAN) that approach domain adaptation by optimizing task-adaptive generalization bounds. To demonstrate the effectiveness of MDAN, we conduct extensive experiments showing superior adaptation performance on both classification and regression problems: sentiment analysis, digit classification, and vehicle counting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The success of machine learning has been partially attributed to rich datasets with abundant annotations <ref type="bibr" target="#b39">[40]</ref>. Unfortunately, collecting and annotating such large-scale training data is prohibitively expensive and time-consuming. To solve these limitations, different labeled datasets can be combined to build a larger one, or synthetic training data can be generated with explicit yet inexpensive annotations <ref type="bibr" target="#b40">[41]</ref>. However, due to the possible shift between training and test samples, learning algorithms based on these cheaper datasets still suffer from high generalization error. Domain adaptation (DA) focuses on such problems by establishing knowledge transfer from a labeled source domain to an unlabeled target domain, and by exploring domain-invariant structures and representations to bridge the gap <ref type="bibr" target="#b37">[38]</ref>. Both theoretical results <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref> and algorithms <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> for DA have been proposed. Most theoretical results and algorithms with respect to DA focus on the single-source-single-target setting <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. However, in many application scenarios, the labeled data available may come from multiple domains with different distributions. As a result, naive application of the single-source-single-target DA algorithms may lead to suboptimal solutions. Such problem calls for an efficient technique for multiple source domain adaptation. Some existing multisource DA methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b50">51]</ref> cannot lead to effective deep learning based algorithms, leaving much space to be improved for their performance.</p><p>In this paper, we analyze the multiple source domain adaptation problem and propose an adversarial learning strategy based on our theoretical results. Specifically, we give new generalization bounds for both classification and regression problems under domain adaptation when there are multiple source domains with labeled instances and one target domain with unlabeled instances. Our theoretical results build on the seminal theoretical model for domain adaptation introduced by Blitzer et al. <ref type="bibr" target="#b8">[9]</ref> and Ben-David et al. <ref type="bibr" target="#b7">[8]</ref>, where a divergence measure, known as the H-divergence, was proposed to measure the distance between two distributions based on a given hypothesis space H. Our new result generalizes the bound <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">Thm. 2]</ref> to the case when there are multiple source domains, and ⇤ The first two authors contributed equally to this work.</p><p>to regression problems as well. The new bounds achieve a finite sample error rate of Õ( p 1/km), where k is the number of source domains and m is the number of labeled training instances from each domain. We provide detailed comparisons with existing work in Section 3.</p><p>Interestingly, our bounds also lead to an efficient algorithm using adversarial neural networks. This algorithm learns both domain invariant and task discriminative features under multiple domains. Specifically, we propose a novel MDAN model by using neural networks as rich function approximators to instantiate the generalization bound we derive (Fig. <ref type="figure" target="#fig_0">1</ref>). MDAN can be viewed as computationally efficient approximations to optimize the parameters of the networks in order to minimize the bounds. We introduce two versions of MDAN: The hard version optimizes directly a simple worst-case generalization bound, while the soft version leads to a more data-efficient model and optimizes an average case and task-adaptive bound. The optimization of MDAN is a minimax saddle point problem, which can be interpreted as a zero-sum game with two participants competing against each other to learn invariant features. MDAN combine feature extraction, domain classification, and task learning in one training process. We propose to use stochastic optimization with simultaneous updates to optimize the parameters in each iteration.</p><p>Contributions. Our contributions are three-fold: 1). Theoretically, we provide average case generalization bounds for both classification and regression problems under the multisource domain adaptation setting. 2). Inspired by our theoretical results, we also propose efficient algorithms that tackle multisource domain adaptation problems using adversarial learning strategy. 3). Empirically, to demonstrate the effectiveness of MDAN as well as the relevance of our theoretical results, we conduct extensive experiments on real-world datasets, including both natural language and vision tasks, classification and regression problems. We achieve consistently superior adaptation performances on all the tasks, validating the effectiveness of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>We first introduce the notations and review a theoretical model for domain adaptation when there is one source and one target <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b26">27]</ref>. The key idea is the H-divergence to measure the discrepancy between two distributions. Other theoretical models for DA exist <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35]</ref>; we choose to work with the above model because this distance measure has a particularly natural interpretation and can be well approximated using samples from both domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations</head><p>We use domain to represent a distribution D on input space X and a labeling function f : X ! [0, 1]. In the setting of one source one target domain adaptation, we use hD S , f S i and hD T , f T i to denote the source and target, respectively. A hypothesis is a function h : X ! [0, 1]. The error of a hypothesis h w.r.t. a labeling function f under distribution D S is defined as:</p><formula xml:id="formula_0">" S (h, f ) := E x⇠D S [|h(x) f (x)|].</formula><p>When f and h are binary classification functions, this definition reduces to the probability that h disagrees with f under D S :</p><formula xml:id="formula_1">E x⇠D S [|h(x) f (x)|] = E x⇠D S [I(f (x) 6 = h(x))] = Pr x⇠D S (f (x) 6 = h(x)).</formula><p>We define the risk of hypothesis h as the error of h w.r.t. a true labeling function under domain D S , i.e., " S (h) := " S (h, f S ). As common notation in computational learning theory, we use b " S (h) to denote the empirical risk of h on the source domain. Similarly, we use " T (h) and b " T (h) to mean the true risk and the empirical risk on the target domain. H-divergence is defined as follows: Definition 1. Let H be a hypothesis class for instance space X , and A H be the collection of subsets of X that are the support of some hypothesis in H, i.e., A H := {h 1 ({1}) | h 2 H}. The distance between two distributions D and D 0 based on H is:</p><formula xml:id="formula_2">d H (D, D 0 ) := 2 sup A2A H | Pr D (A) Pr D 0 (A)|.</formula><p>When the hypothesis class H contains all the possible measurable functions over X , d H (D, D 0 ) reduces to the familiar total variation. Given a hypothesis class H, we define its symmetric difference w.r.t. itself as:</p><formula xml:id="formula_3">H H = {h(x) h 0 (x) | h, h 0</formula><p>2 H}, where is the XOR operation. Let h ⇤ be the optimal hypothesis that achieves the minimum combined risk on both the source and the target domains: h ⇤ := arg min h2H " S (h) + " T (h), and use to denote the combined risk of the optimal hypothesis h ⇤ : := " S (h ⇤ ) + " T (h ⇤ ). Ben-David et al. <ref type="bibr" target="#b6">[7]</ref> and Blitzer et al. <ref type="bibr" target="#b8">[9]</ref> proved the following generalization bound on the target risk in terms of the source risk and the discrepancy between the single source domain and the target domain: </p><formula xml:id="formula_4">" T (h)  b " S (h) + 1 2 d H H ( b D S , b D T ) + + O r d log(m/d) + log(1/ ) m !<label>(1)</label></formula><p>The bound depends on , the optimal combined risk that can be achieved by hypothesis in H. The intuition is if is large, we cannot hope for a successful domain adaptation. One notable feature is that the empirical discrepancy distance between two samples can be approximated by a discriminator to distinguish instances from two domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Generalization Bound for Multiple Source Domain Adaptation</head><p>In this section we discuss two approaches to obtain generalization guarantees for multiple source domain adaptation in both classification and regression settings, one by a union bound argument and one using reduction from multiple source domains to single source domain. We conclude this section with a discussion and comparison of our bounds with existing generalization bounds for multisource domain adaptation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35]</ref>. We refer readers to appendix for proof details and we mainly focus on discussing the interpretations and implications of the theorems.</p><p>Let {D Si } k i=1 and D T be k source domains and the target domain, respectively. One idea to obtain a generalization bound for multiple source domains is to apply Thm. 1 repeatedly k times, followed by a union bound to combine them. Following this idea, we first obtain the following bound as a corollary of Thm. 1 in the setting of multiple source domains, serving as a baseline model:</p><formula xml:id="formula_5">Corollary 1 (Worst case classification bound). Let H be a hypothesis class with V Cdim(H) = d. If b D T and { b D Si } k i=1</formula><p>are the empirical distributions generated with m i.i.d. samples from each domain, then, for 0 &lt; &lt; 1, with probability at least 1 , for all h 2 H, we have:</p><formula xml:id="formula_6">" T (h)  max i2[k] ⇢ b " Si (h) + 1 2 d H H ( b D T ; b D Si ) + i + O s 1 m ✓ log k + d log m d ◆ ! (<label>2</label></formula><formula xml:id="formula_7">)</formula><p>where i is the combined risk of the optimal hypothesis on domains S i and T .</p><p>This bound is quite pessimistic, as it essentially is a worst case bound, where the generalization on the target only depends on the worst source domain. However, in many real-world scenarios, when the number of related source domains is large, a single irrelevant source domain may not hurt the generalization too much. Furthermore, in the case of multiple source domains, despite the possible discrepancy between the source domains and the target domain, effectively we have a labeled sample of size km, while the asymptotic convergence rate in Corollary. 1 is of Õ( p 1/m). Hence naturally one interesting question to ask is: is it possible to have a generalization bound of finite sample rate Õ(</p><formula xml:id="formula_8">p 1/km)?</formula><p>In what follows we present a strategy to achieve a generalization bound of rate Õ( p 1/km). The idea of this strategy is a reduction using convex combination from multiple domains to single domain by combining all the labeled instances from k domains to one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2 (Average case classification bound). Let H be a hypothesis class with</head><formula xml:id="formula_9">V Cdim(H) = d. If { b D Si } k i=1</formula><p>are the empirical distributions generated with m i.i.d. samples from each domain, and b D T is the empirical distribution on the target domain generated from mk samples without labels, then, 8↵ 2 R k + ,</p><formula xml:id="formula_10">P i2[k] ↵ i = 1</formula><p>, and for 0 &lt; &lt; 1, w.p.b. at least 1 , for all h 2 H, we have:</p><formula xml:id="formula_11">" T (h)  X i2[k] ↵ i ✓ b " Si (h) + 1 2 d H H ( b D T ; b D Si ) ◆ + ↵ + O s 1 km ✓ log 1 + d log km d ◆ ! (<label>3</label></formula><formula xml:id="formula_12">)</formula><p>where ↵ is the risk of the optimal hypothesis on the mixture source domain</p><formula xml:id="formula_13">P i2[k] ↵ i S i and T .</formula><p>Different from Corollary 1, Thm. 2 requires mk unlabeled instances from the target domain. This is a mild requirement since unlabeled data is cheap to collect. Roughly, the bound in Thm. 2 can be understood as an average case bound if we choose</p><formula xml:id="formula_14">↵ i = 1/k, 8i 2 [k].</formula><p>Note that a simple convex combination by applying Thm. 1 k times can only achieve finite sample rate of Õ( p 1/m), while the one in (3) achieves Õ( p 1/km). On the other hand, the constants max i2[k] i (in Corollary 1) and ↵ (in Thm. 2) are generally not comparable. As a final note, although the proof works for any convex combination ↵ i , in the next section we will describe a practical method so that we do not need to explicitly choose it. Thm. 2 upper bounds the generalization error for classification problems. Next we also provide generalization guarantee for regression problem, where instead of VC dimension, we use pseudo-dimension to characterize the structural complexity of the hypothesis class.</p><p>Theorem 3 (Average case regression bound). Let H be a set of real-valued functions from X to [0, 1]</p><formula xml:id="formula_15">2 with P dim(H) = d. If { b D Si } k i=1</formula><p>are the empirical distributions generated with m i.i.d. samples from each domain, and b D T is the empirical distribution on the target domain generated from mk samples without labels, then, 8↵ 2 R k + ,</p><formula xml:id="formula_16">P i2[k] ↵ i = 1</formula><p>, and for 0 &lt; &lt; 1, with probability at least 1 , for all h 2 H, we have:</p><formula xml:id="formula_17">" T (h)  X i2[k] ↵ i ✓ b " Si (h) + 1 2 d H( b D T ; b D Si ) ◆ + ↵ + O s 1 km ✓ log 1 + d log km d ◆ ! (<label>4</label></formula><formula xml:id="formula_18">)</formula><p>where ↵ is the risk of the optimal hypothesis on the mixture source domain P i2[k] ↵ i S i and T , and</p><formula xml:id="formula_19">H := {I |h(x) h 0 (x)|&gt;t : h, h 0 2 H, 0  t  1} is the set of threshold functions induced from H.</formula><p>Comparison with Existing Bounds. First, it is easy to see that, the bounds in both ( <ref type="formula" target="#formula_6">2</ref>) and ( <ref type="formula" target="#formula_11">3</ref>) reduce to the one in Thm. 1 when there is only one source domain (k = 1). Blitzer et al. <ref type="bibr" target="#b8">[9]</ref> give a generalization bound for semi-supervised classification with multiple sources where, besides labeled instances from multiple source domains, the algorithm also has access to a fraction of labeled instances from the target domain. Although in general our bound and the one in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Thm. 3</ref>] are incomparable, it is instructive to see the connections and differences between them: our bound works in the unsupervised domain adaptation setting where we do not have any labeled data from the target. As a comparison, their bound in [9, Thm. 3] is a bound for semi-supervised domain adaptation. As a result, because of the access to labeled instances from the target domain, their bound is expressed relative to the optimal error on the target, while ours is in terms of the empirical error on the source domains, hence theirs is more informative. To the best of our knowledge, our bound in Thm. 3 is the first one using the idea of H-divergence for regression problems. The proof of this theorem relies on a reduction from regression to classification. Mansour et al. <ref type="bibr" target="#b33">[34]</ref> give a generalization bound for multisource domain adaptation under the assumption that the target distribution is a mixture of the k sources and the target hypothesis can be represented as a convex combination of the source hypotheses. Also, their generalized discrepancy measure can be applied for other loss functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Multisource Domain Adaptation with Adversarial Neural Networks</head><p>Motivated by the bounds given in the last section, in this section we propose our model, multisource domain adversarial networks (MDAN), with two versions: Hard version (as a baseline) and Soft version. Suppose we are given samples drawn from k source domains {D Si }, each of which contains m instance-label pairs. Additionally, we also have access to unlabeled instances sampled from the target domain D T . Once we fix our hypothesis class H, the last two terms in the generalization bounds ( <ref type="formula" target="#formula_6">2</ref>) and (3) will be fixed; hence we can only hope to minimize the bound by minimizing the first two terms, i.e., the source training error and the discrepancy between source domains and target domain. The idea is to train a neural network to learn a representation with the following two properties: 1). indistinguishable between the k source domains and the target domain; 2). informative enough for our desired task to succeed. Note that both requirements are necessary: without the second property, a neural network can learn trivial random noise representations for all the domains, and such representations cannot be distinguished by any discriminator; without the first property, the learned representation does not necessarily generalize to the unseen target domain.</p><p>One key observation that leads to a practical approximation of d H H ( b D T ; b D Si ) from Ben-David et al. <ref type="bibr" target="#b6">[7]</ref> is that computing the discrepancy measure is closely related to learning a classifier that is able to distinguish samples from different domains. Let b " T,Si (h) be the empirical risk of hypothesis </p><formula xml:id="formula_20">Hard version: minimize max i2[k] ✓ b " Si (h) min h 0 2H H b " T,Si (h 0 ) ◆<label>(5)</label></formula><p>The two terms in (5) exactly correspond to the two criteria we just proposed: the first term asks for an informative feature representation for our desired task to succeed, while the second term captures the notion of invariant feature representations between different domains. Inspired by Ganin et al. <ref type="bibr" target="#b16">[17]</ref>, we use the gradient reversal layer to effectively implement (5) by backpropagation. The network architecture is shown in Figure . 1. As discussed in the last section, one notable drawback of the hard version is that the algorithm may spend too much computational resources in optimizing the worst source domain. Furthermore, in each iteration the algorithm only updates its parameter based on the gradient from one of the k domains. This is data inefficient and can waste our computational resources in the forward process.</p><p>To avoid both of the problems, we propose the MDAN Soft version that optimizes an upper bound of the convex combination bound given in (3). To this end, define b</p><formula xml:id="formula_21">" i (h) := b " Si (h) min h 0 2H H b</formula><p>" T,Si (h 0 ) and let &gt; 0 be a constant. We formulate the following optimization problem:</p><formula xml:id="formula_22">Soft version: minimize 1 log X i2[k] exp ✓ (b " Si (h) min h 0 2H H b " T,Si (h 0 )) ◆<label>(6)</label></formula><p>At the first glance, it may not be clear what the above objective function corresponds to. To understand this, if we define</p><formula xml:id="formula_23">↵ i = exp(b " i (h))/ P j2[k] exp(b " j (h))</formula><p>, then the following inequality holds:</p><formula xml:id="formula_24">X i2[k] ↵ i b " i (h)  log E ↵ [exp(b " i (h))] = log P i2[k] exp 2 (b " i (h)) P i2[k] exp(b " i (h)) !  log X i2[k] exp(b " i (h))</formula><p>In other words, the objective function in ( <ref type="formula" target="#formula_22">6</ref>) is in fact an upper bound of the convex combination bound given in (3), with the combination weight ↵ defined above. Compared with the one in (3), one advantage of the objective function in ( <ref type="formula" target="#formula_22">6</ref>) is that we do not need to explicitly choose the value of ↵.</p><p>Instead, it adaptively corresponds to the loss b " i (h), and the larger the loss, the heavier the weight. Alternatively, from the algorithmic perspective, during the optimization (6) naturally provides an adaptive weighting scheme for the k source domains depending on their relative error. Use ✓ to denote all the model parameters:</p><formula xml:id="formula_25">@ @✓ 1 log X i2[k] exp ✓ (b " Si (h) min h 0 2H H b " T,Si (h 0 )) ◆ = X i2[k] exp b " i (h) P i 0 2[k] exp b " i 0 (h) @b " i (h) @✓<label>(7)</label></formula><p>Compared with (5), the log-sum-exp trick not only smooths the objective, but also provides a principled and adaptive way to combine all the gradients from the k source domains. In words, <ref type="bibr" target="#b6">(7)</ref> says that the gradient of MDAN is a convex combination of the gradients from all the domains. The larger the error from one domain, the larger the combination weight in the ensemble. As we will see in Sec. 5, the optimization problem (6) often leads to better generalizations in practice, which may partly be explained by the ensemble effect of multiple sources implied by the upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We evaluate both hard and soft MDAN and compare them with state-of-the-art methods on three real-world datasets: the Amazon benchmark dataset <ref type="bibr" target="#b10">[11]</ref> for sentiment analysis, a digit classification task that includes 4 datasets: MNIST <ref type="bibr" target="#b28">[29]</ref>, MNIST-M <ref type="bibr" target="#b16">[17]</ref>, SVHN <ref type="bibr" target="#b36">[37]</ref>, and SynthDigits <ref type="bibr" target="#b16">[17]</ref>, and a public, large-scale image dataset on vehicle counting from multiple city cameras <ref type="bibr" target="#b51">[52]</ref>. Due to space limit, details about network architecture and training parameters of proposed and baseline methods, and detailed dataset description are described in appendix. We implement both the Hard-Max and Soft-Max methods, and compare them with three baselines: MLPNet, marginalized stacked denoising autoencoders (mSDA) <ref type="bibr" target="#b10">[11]</ref>, and DANN <ref type="bibr" target="#b16">[17]</ref>. DANN cannot be directly applied in multiple source domains setting. In order to make a comparison, we use two protocols. The first one is to combine all the source domains into a single one and train it using DANN, which we denote as C-DANN. The second protocol is to train multiple DANNs separately, where each one corresponds to a source-target pair. Among all the DANNs, we report the one achieving the best performance on the target domain. We denote this experiment as B-DANN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Amazon Reviews</head><p>For fair comparison, all these models are built on the same basic network structure with one input layer (5000 units) and three hidden layers (1000, 500, 100 units). Results and Analysis. We show the accuracy of different methods in Table <ref type="table" target="#tab_1">1</ref>. Clearly, Soft-Max significantly outperforms all other methods in most settings. When Kitchen is the target domain, C-DANN performs slightly better than Soft-Max, and all the methods perform close to each other. Hard-Max is typically slightly worse than Soft-Max. This is mainly due to the low data-efficiency of the Hard-Max model (Section 4, Eq. 5, Eq. 6). We observe that with more training iterations, the performance of Hard-Max can be further improved. These results verify the effectiveness of MDAN for multisource domain adaptation. To validate the statistical significance of the results, we also run a non-parametric Wilcoxon signed-ranked test for each task to compare Soft-Max with the other competitors (see more details in appendix). From the statistical test, we see that Soft-Max is convincingly better than other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Digits Datasets</head><p>Following the setting in <ref type="bibr" target="#b16">[17]</ref>, we combine four digits datasets (MNIST, MNIST-M, SVHN, SynthDigits) to build the multisource domain dataset. We take each of MNIST-M, SVHN, and MNIST as target domain in turn, and the rest as sources. Each source domain has 20, 000 labeled images and the target test set has 9, 000 examples.</p><p>Baselines. We compare Hard-Max and Soft-Max of MDAN with 10 baselines: i). B-Source. A basic network trained on each source domain (20, 000 images) without domain adaptation and tested on the target domain. Among the three models, we report the one achieves the best performance on the test set. ii). C-Source. A basic network trained on a combination of three source domains (20, 000 images for each) without domain adaptation and tested on the target domain. iii). B-DANN. We train DANNs <ref type="bibr" target="#b16">[17]</ref> on each source-target domain pair (20, 000 images for each source) and test it on target. Again, we report the best score among the three. iv). C-DANN. We train a single DANN on a combination of three source domains (20, 000 images for each). v). B-ADDA. We train ADDA <ref type="bibr" target="#b45">[46]</ref> on each source-target domain pair (20, 000 images for each source) and test it on the target domain.</p><p>We report the best accuracy among the three. vi).C-ADDA. We train ADDA on a combination of three source domains (20, 000 images for each). vii). B-MTAE. We train MTAE <ref type="bibr" target="#b18">[19]</ref> on each source-target domain pair (20, 000 images for each source) and test it on the target domain. We report the best accuracy among the three. viii). C-MTAE. We train MTAE on a combination of three source domains (20, 000 images for each). ix). MDAC. MDAC <ref type="bibr" target="#b50">[51]</ref> is a multiple source domain adaptation algorithm that explores causal models to represent the relationship between the features X and class label Y . We directly train MDAC on a combination of three source domains. x). Target. It is the basic network trained and tested on the target data. It serves as an upper bound of DA algorithms. All the MDAN and baseline methods are built on the same basic network structure to put them on a equal footing.</p><p>Results and Analysis. The classification accuracy is shown in Table <ref type="table" target="#tab_2">2</ref>. The results show that MDAN outperforms all the baselines in the first two experiments and is comparable with Best-Single-DANN in the third experiment. For the combined sources, MDAN always perform better than the source-only baseline (MDAN vs. Combine-Source). However, a naive combination of different training datasets can sometimes even decrease the performance of the baseline methods. This conclusion comes from three observations: First, directly training DANN on a combination of multiple sources leads to worse results than the source-only baseline (Combine-DANN vs. Combine-Source); Second, The performance of Combine-DANN can be even worse than the Best-Single-DANN (the first and third experiments); Third, directly training DANN on a combination of multiple sources always has lower accuracy compared with our approach (Combine-DANN vs. MDAN). We have similar observations for ADDA and MTAE. Such observations verify that the domain adaptation methods designed for single source lead to suboptimal solutions when applied to multiple sources. It also verifies the necessity and superiority of MDAN for multiple source adaptation. Furthermore, we observe that adaptation to the SVHN dataset (the third experiment) is hard. In this case, increasing the number of source domains does not help. We conjecture this is due to the large dissimilarity between the SVHN data to the others. Surprisingly, using a single domain (best-Single DANN) in this case achieves the best result. This indicates that in domain adaptation the quality of data (how close to the target data) is much more important than the quantity (how many source domains). As a conclusion, this experiment further demonstrates the effectiveness of MDAN when there are multiple source domains available, where a naive combination of multiple sources using DANN may hurt generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">WebCamT Vehicle Counting Dataset</head><p>WebCamT is a public dataset for vehicle counting from large-scale city camera videos, which has low resolution (352 ⇥ 240), low frame rate (1 frame/second), and high occlusion. It has 60, 000 frames annotated with vehicle bounding box and count, divided into training and testing sets, with 42, 200 and 17, 800 frames, respectively. Here we demonstrate the effectiveness of MDAN to count vehicles from an unlabeled target camera by adapting from multiple labeled source cameras: we select 8 cameras located in different intersections of the city with different scenes, and each has more than 2, 000 labeled images for our evaluations. Among these 8 cameras, we randomly pick two cameras and take each camera as the target camera, with the other 7 cameras as sources. We compute the proxy A-distance (PAD) <ref type="bibr" target="#b6">[7]</ref> between each source camera and the target camera to approximate the divergence between them. We then rank the source cameras by the PAD from low to high and choose the first k cameras to form the k source domains. Thus the proposed methods and baselines can be evaluated on different numbers of sources (from 2 to 7). We implement the Hard-Max and Soft-Max MDAN, based on the basic vehicle counting network FCN <ref type="bibr" target="#b51">[52]</ref>. We compare our method with two baselines: FCN <ref type="bibr" target="#b51">[52]</ref>, a basic network without domain adaptation, and DANN <ref type="bibr" target="#b16">[17]</ref>, implemented for each frame (the average number of vehicles in one frame is around 20). Fig. <ref type="figure" target="#fig_2">2</ref> shows the counting results of Soft-Max for the two target cameras under the 5 source cameras setting. We can see that the proposed method accurately counts the vehicles of each target camera for long time sequences. Does adding more source cameras always help improve the performance on the target camera? To answer this question, we analyze the counting error when we vary the number of source cameras as shown in Fig. <ref type="figure" target="#fig_4">3a</ref>, where the x-axis refers to number of source cameras and the y-axis includes both the MAE curve on the target camera as well as the PAD distance (bar chart) between the pair of source and target cameras. From the curves, we see the counting error goes down with more source cameras at the beginning, while it goes up when more sources are added at the end. This phenomenon shows that the performance on the target domain also depends on the its distance to the added source domain, i.e., it is not always beneficial to naively incorporate more source domains into training. To illustrate this better, we also show the PAD of the newly added camera in the bar chart of Fig. <ref type="figure" target="#fig_4">3a</ref>. By observing the PAD and the counting error, we see the performance on the target can degrade when the newly added source camera has large divergence from the target camera. To show that MDAN can indeed decrease the divergences between target domain and multiple source domains, in Fig. <ref type="figure" target="#fig_4">3b</ref> we plot the PAD distances between the target domains and the corresponding source domains. We can see that MDAN consistently decrease the PAD distances between all pairs of target and source domains, for both camera A and camera B. From this experiment we conclude that our proposed MDAN models are effective in multiple source domain adaptation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>A number of adaptation approaches have been studied in recent years. From the theoretical aspect, several theoretical results have been derived in the form of upper bounds on the generalization target error by learning from the source data. A keypoint of the theoretical frameworks is estimating the distribution shift between source and target. Kifer et al. <ref type="bibr" target="#b26">[27]</ref> proposed the H-divergence to  measure the similarity between two domains and derived a generalization bound on the target domain using empirical error on the source domain and the H-divergence between the source and the target. This idea has later been extended to multisource domain adaptation <ref type="bibr" target="#b8">[9]</ref> and the corresponding generalization bound has been developed as well. Ben-David et al. <ref type="bibr" target="#b7">[8]</ref> provide a generalization bound for domain adaptation on the target risk which generalizes the standard bound on the source risk. This work formalizes a natural intuition of DA: reducing the two distributions while ensuring a low error on the source domain and justifies many DA algorithms. Based on this work, Mansour et al. <ref type="bibr" target="#b32">[33]</ref> introduce a new divergence measure: discrepancy distance, whose empirical estimate is based on the Rademacher complexity <ref type="bibr" target="#b27">[28]</ref>. See <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref> for more details.</p><p>Following the theoretical developments, many DA algorithms have been proposed, such as instancebased methods <ref type="bibr" target="#b43">[44]</ref>; feature-based methods <ref type="bibr" target="#b5">[6]</ref>; and parameter-based methods <ref type="bibr" target="#b14">[15]</ref>. Recent studies have shown that deep neural networks can learn more transferable features for DA <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">50]</ref>. Bousmalis et al. <ref type="bibr" target="#b9">[10]</ref> develop domain separation networks to extract image representations that are partitioned into two subspaces: domain private component and cross-domain shared component. The partitioned representation is utilized to reconstruct the images from both domains, improving the DA performance. Ganin et al. <ref type="bibr" target="#b16">[17]</ref> propose a domain-adversarial neural network to learn the domain indiscriminate but main-task discriminative features. Adversarial training techniques that aim to build feature representations that are indistinguishable between source and target domains have been proposed in the last few years <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17]</ref>. Specifically, one of the central ideas is to use neural networks, which are powerful function approximators, to approximate a distance measure known as the H-divergence between two domains <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b26">27]</ref>. The overall algorithm can be viewed as a zero-sum two-player game: one network tries to learn feature representations that can fool the other network, whose goal is to distinguish representations generated from the source domain between those generated from the target domain. The goal of the algorithm is to find a Nash-equilibrium of the game. Ideally, at such equilibrium state, feature representations from the source domain will share the same distributions as those from the target domain. Although these works generally outperform non-deep learning based methods, they only focus on the single-source-single-target DA problem, and much work is rather empirical design without statistical guarantees. Hoffman et al. <ref type="bibr" target="#b22">[23]</ref> present a domain transform mixture model for multisource DA, which is based on non-deep architectures and is difficult to scale up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We theoretically analyze generalization bounds for DA under the setting of multiple source domains with labeled instances and one target domain with unlabeled instances. Specifically, we propose average case generalization bounds for both classification and regression problems. The new bounds have interesting interpretations and the one for classification reduces to an existing bound when there is only one source domain. Following our theoretical results, we propose two MDAN to learn feature representations that are invariant under multiple domain shifts while at the same time being discriminative for the learning task. Both hard and soft versions of MDAN are generalizations of the popular DANN to the case when multiple source domains are available. Empirically, MDAN outperforms the state-of-the-art DA methods on three real-world datasets, including a sentiment analysis task, a digit classification task, and a visual vehicle counting task, demonstrating its effectiveness in multisource domain adaptation for both classification and regression problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 1 (</head><label>1</label><figDesc><ref type="bibr" target="#b8">[9]</ref>). Let H be a hypothesis space of V C-dimension d and b D S ( b D T ) be the empirical distribution induced by sample of size m drawn from D S (D T ). Then w.p.b. at least 1 , 8h 2 H,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: MDAN Network architecture. Feature extractor, domain classifier, and task learning are combined in one training process. Hard version: the source that achieves the minimum domain classification error is backpropagated with gradient reversal; Smooth version: all the domain classification risks over k source domains are combined and backpropagated adaptively with gradient reversal. h in the domain discriminating task. Ignoring the constant terms that do not affect the upper bound, we can minimize the worst case upper bound in (2) by solving the following optimization problem:</figDesc><graphic coords="5,108.00,72.00,396.01,98.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Counting results for target camera A (first row) and B (second row). X-frames; Y-Counts.</figDesc><graphic coords="8,108.00,481.81,395.99,118.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Counting error and PAD over different source numbers. (b) PAD distance before and after training MDAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: PAD distance over different source domains along with their changes before and after training MDAN.</figDesc><graphic coords="9,309.74,72.00,190.08,76.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Domains within the dataset consist of reviews on a specific kind of product (Books, DVDs, Electronics, and Kitchen appliances). Reviews are encoded as 5000 dimensional feature vectors of unigrams and bigrams, with binary labels indicating sentiment. We conduct 4 experiments: for each of them, we pick one product as target domain and the rest as source domains. Each source domain has 2000 labeled examples, and the target test set has 3000 to 6000 examples. During training, we randomly sample the same number of unlabeled target examples as the source examples in each mini-batch.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Sentiment classification accuracy.</figDesc><table><row><cell>Train/Test</cell><cell cols="4">MLPNet mSDA B-DANN C-DANN</cell><cell cols="2">MDAN Hard-Max Soft-Max</cell></row><row><cell>D+E+K/B</cell><cell>0.7655</cell><cell>0.7698</cell><cell>0.7650</cell><cell>0.7789</cell><cell>0.7845</cell><cell>0.7863</cell></row><row><cell>B+E+K/D</cell><cell>0.7588</cell><cell>0.7861</cell><cell>0.7732</cell><cell>0.7886</cell><cell>0.7797</cell><cell>0.8065</cell></row><row><cell>B+D+K/E</cell><cell>0.8460</cell><cell>0.8198</cell><cell>0.8381</cell><cell>0.8491</cell><cell>0.8483</cell><cell>0.8534</cell></row><row><cell>B+D+E/K</cell><cell>0.8545</cell><cell>0.8426</cell><cell>0.8433</cell><cell>0.8639</cell><cell>0.8580</cell><cell>0.8626</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy on digit classification. T: MNIST; M: MNIST-M, S: SVHN, D: SynthDigits.</figDesc><table><row><cell cols="4">Method S+M+D/T T+S+D/M M+T+D/S</cell><cell cols="4">Method S+M+D/T T+S+D/M M+T+D/S</cell></row><row><cell>B-Source</cell><cell>0.964</cell><cell>0.519</cell><cell>0.814</cell><cell>C-Source</cell><cell>0.938</cell><cell>0.561</cell><cell>0.771</cell></row><row><cell>B-DANN</cell><cell>0.967</cell><cell>0.591</cell><cell>0.818</cell><cell>C-DANN</cell><cell>0.925</cell><cell>0.651</cell><cell>0.776</cell></row><row><cell>B-ADDA</cell><cell>0.968</cell><cell>0.657</cell><cell>0.800</cell><cell>C-ADDA</cell><cell>0.927</cell><cell>0.682</cell><cell>0.804</cell></row><row><cell>B-MTAE</cell><cell>0.862</cell><cell>0.534</cell><cell>0.703</cell><cell>C-MTAE</cell><cell>0.821</cell><cell>0.596</cell><cell>0.701</cell></row><row><cell>Hard-Max</cell><cell>0.976</cell><cell>0.663</cell><cell>0.802</cell><cell>Soft-Max</cell><cell>0.979</cell><cell>0.687</cell><cell>0.816</cell></row><row><cell>MDAC</cell><cell>0.755</cell><cell>0.563</cell><cell>0.604</cell><cell>Target</cell><cell>0.987</cell><cell>0.901</cell><cell>0.898</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Counting error statistics. S is the number of source cameras; T is the target camera id.Results and Analysis. The counting error of different methods is compared in Table3. The Hard-Max version achieves lower error than DANN and FCN in most settings for both target cameras. The Soft-Max approximation outperforms all the baselines and the Hard-Max in most settings, demonstrating the effectiveness of the smooth and adaptative approximation. The lowest MAE achieved by Soft-Max is 1.1942. Such MAE means that there is only around one vehicle miscount</figDesc><table><row><cell>S T</cell><cell cols="2">MDAN Hard-Max Soft-Max</cell><cell>DANN</cell><cell>FCN</cell><cell>T</cell><cell cols="2">MDAN Hard-Max Soft-Max</cell><cell>DANN</cell><cell>FCN</cell></row><row><cell>2 A</cell><cell>1.8101</cell><cell>1.7140</cell><cell cols="2">1.9490 1.9094</cell><cell>B</cell><cell>2.5059</cell><cell>2.3438</cell><cell>2.5218 2.6528</cell></row><row><cell>3 A</cell><cell>1.3276</cell><cell>1.2363</cell><cell cols="2">1.3683 1.5545</cell><cell>B</cell><cell>1.9092</cell><cell>1.8680</cell><cell>2.0122 2.4319</cell></row><row><cell>4 A</cell><cell>1.3868</cell><cell>1.1965</cell><cell cols="2">1.5520 1.5499</cell><cell>B</cell><cell>1.7375</cell><cell>1.8487</cell><cell>2.1856 2.2351</cell></row><row><cell>5 A</cell><cell>1.4021</cell><cell>1.1942</cell><cell cols="2">1.4156 1.7925</cell><cell>B</cell><cell>1.7758</cell><cell>1.6016</cell><cell>1.7228 2.0504</cell></row><row><cell>6 A</cell><cell>1.4359</cell><cell>1.2877</cell><cell cols="2">2.0298 1.7505</cell><cell>B</cell><cell>1.5912</cell><cell>1.4644</cell><cell>1.5484 2.2832</cell></row><row><cell>7 A</cell><cell>1.4381</cell><cell>1.2984</cell><cell cols="2">1.5426 1.7646</cell><cell>B</cell><cell>1.5989</cell><cell>1.5126</cell><cell>1.5397 1.7324</cell></row><row><cell cols="9">on top of the same basic network. We record mean absolute error (MAE) between true count and</cell></row><row><cell cols="2">estimated count.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This is just for the simplicity of presentation, the range can easily be generalized to any bounded set.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>HZ and GG gratefully acknowledge support from ONR, award number N000141512365. This research was supported in part by Fundac ¸ão para a Ciência e a Tecnologia (project FCT [SFRH/BD/113729/2015] and a grant from the Carnegie Mellon-Portugal program). HZ would also like to thank Remi Tachet for his valuable comments and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with a relaxed covariate shift assumption</title>
		<author>
			<persName><forename type="first">Tameem</forename><surname>Adel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1691" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Domain-adversarial neural networks</title>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Franc ¸ois Laviolette</surname></persName>
		</author>
		<author>
			<persName><surname>Marchand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4446</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural network learning: Theoretical foundations</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by domain invariant projection</title>
		<author>
			<persName><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mehrtash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">C</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="769" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-linear domain adaptation with boosting</title>
		<author>
			<persName><forename type="first">Carlos J</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="485" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="343" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Marginalized denoising autoencoders for domain adaptation</title>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4683</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain adaptation and sample bias correction theory and algorithm for regression</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">519</biblScope>
			<biblScope unit="page" from="103" to="126" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sample selection bias correction theory</title>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="38" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Regularized multi-task learning</title>
		<author>
			<persName><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the tenth ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning attributes equals multi-source domain generalization</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="87" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Franc ¸ois Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Franc ¸ois Laviolette, and Emilie Morvant. A pac-bayesian approach for domain adaptation with specialization to linear classifiers</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (3)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="738" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (1)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation across domain shifts by generating intermediate data representations</title>
		<author>
			<persName><forename type="first">Raghuraman</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruonan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2288" to="2302" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="702" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multiple-source adaptation for regression problems</title>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ningshan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05037</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robust visual domain adaptation with low-rank reconstruction</title>
		<author>
			<persName><forename type="first">I-Hong</forename><surname>Jhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2168" to="2175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting change in data streams</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth international conference on Very large data bases</title>
		<meeting>the Thirtieth international conference on Very large data bases</meeting>
		<imprint>
			<publisher>VLDB Endowment</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="180" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rademacher penalties and structural risk minimization</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Koltchinskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1902" to="1914" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00830</idno>
		<title level="m">The variational fair autoencoder</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust domain adaptation</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Schain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISAIM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.3430</idno>
		<title level="m">Domain adaptation: Learning bounds and algorithms</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptation with multiple sources</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1041" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multiple source adaptation and the rényi divergence</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenda</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Webb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07828</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hirokazu</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A two-stage weighting framework for multi-source domain adaptation</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="505" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Direct density ratio estimation for large-scale covariate shift adaptation</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Tsuboi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Hido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Processing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="138" to="155" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05464</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Adversarial discriminative domain adaptation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robustness and generalization</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="423" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3150" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose Mf</forename><surname>Moura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05868</idno>
		<title level="m">Understanding traffic density from large-scale web camera data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
