<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Application Mapping for Chip Multiprocessors *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guangyu</forename><forename type="middle">Chen</forename><surname>Microsoft</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feihui</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Son</surname></persName>
							<email>sson@cse.psu.edu</email>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
							<email>kandemir@cse.psu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Penn State University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<settlement>Anaheim</settlement>
									<region>California</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Application Mapping for Chip Multiprocessors *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DBDB9F31078142FE72069FFA93204E1D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.m [Software]: Programming Languages -Miscellaneous Languages</term>
					<term>Experimentation Compilers</term>
					<term>NoC (Network on Chip)</term>
					<term>Power Optimization</term>
					<term>Application Mapping</term>
					<term>Chip Multiprocessing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem attacked in this paper is one of automatically mapping an application onto a Network-on-Chip (NoC) based chip multiprocessor (CMP) architecture in a locality-aware fashion. The proposed compiler approach has four major steps: task scheduling, processor mapping, data mapping, and packet routing. In the first step, the application code is parallelized and the resulting parallel threads are assigned to virtual processors. The second step implements a virtual processor-to-physical processor mapping. The goal of this mapping is to ensure that the threads that are expected to communicate frequently with each other are assigned to neighboring processors as much as possible. In the third step, data elements are mapped to memories attached to CMP nodes. The main objective of this mapping is to place a given data item into a node which is close to the nodes that access it. The last step of our approach determines the paths (between memories and processors) for data to travel in an energy efficient manner. In this paper, we describe the compiler algorithms we implemented in detail and present an experimental evaluation of the framework. In our evaluation, we test our entire framework as well as the impact of omitting some of its steps. This experimental analysis clearly shows that the proposed framework reduces energy consumption of our applications significantly (27.41% on average over a pure performance oriented application mapping strategy) as a result of improved locality of data accesses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>While there already exist efforts on building robust Network-on-Chip (NoC) based computation platforms, programming these platforms is an entirely different matter. In particular, current optimizing and parallelizing compilers do not give much support for mapping applications onto NoC based parallel execution platforms (e.g., an NoC based chip multiprocessor, CMP) in an energy-aware fashion. While one may try to employ known code parallelization techniques from the high-performance computing domain <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b24">24]</ref>, such techniques are not very suitable for NoC-based CMPs mainly because (1) these techniques are mostly performance oriented and do not consider other important metrics such as power and (2) they do not take the network structure explicitly into account, and thus, the only distinction (as far as the locations of data elements are concerned) these prior techniques consider is local versus nonlocal data. In an NoC based execution environment, however, the exact location of non-local data matters a lot, in particular from the power consumption angle. This is because, fewer the number of links used to access data, the less energy consumption is incurred. Therefore, customized compiler support for NoC based chip multiprocessors is critical in our opinion.</p><p>The main contribution of this paper is a compiler framework that takes the source code of an application and maps it to a chip multiprocessor system, the processors of which are connected to each other using a mesh based NoC. The proposed application mapping approach tries to optimize the locality of data accesses, and can be beneficial from both the power and performance perspectives. It has four major steps: task scheduling, processor mapping, data mapping, and packet routing. In the first step, the application code is parallelized and the resulting parallel threads are assigned to virtual processors. The second step implements a virtual-to-physical processor mapping. The goal of this mapping is to ensure that the threads that are expected to communicate frequently are assigned to neighboring processors as much as possible. In the third step, data elements are mapped to the memories attached to the CMP nodes. The main objective of this mapping is to place a given data item into a node which is close to the nodes that access it. The last step of our approach determines the paths (between memories and processors) for data to travel in an energy efficient manner.</p><p>In this paper, we describe the compiler algorithms we implemented using the SUIF framework <ref type="bibr" target="#b1">[1]</ref> and present an experimental evaluation of the framework. In our evaluation, we test our entire framework as well as the impact of omitting some of its steps. This experimental analysis clearly shows that the proposed framework reduces energy consumption of our applications significantly (27.41% on average over a pure performance oriented application mapping scheme).</p><p>Section 2 introduces the architectural model we consider in this paper and Section 3 explains the application execution model. The details of our four-step approach are presented in Section 4. Section 5 gives an experimental evaluation of the proposed approach. Section 6 concludes the paper by summarizing our major contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ARCHITECTURAL MODEL</head><p>Our architectural model is an NoC based CMP with two dimensional mesh topology. Each node in the mesh consists of a router, a processor, and a memory component. The memory space in a node is managed by the compiler. We use η i to denote the i th node of the mesh, and each pair of adjacent nodes, η i and η j , are assumed to be connected by a pair of directed channels, one from η i to η j and the other from η j to η i . A processor accesses a nonlocal data element by sending and receiving packets over the network. Let us assume that the processor in node η i needs to read a data element that is stored in the memory of node η j . The processor in node η i first sends a data packet containing the address of the data element to node η j ; node η j responds by sending back a packet containing the value of the data element. On the other hand, if the processor in η i needs to update a data element that is stored in the memory of node η j , it sends a packet containing both the address and the new value of the data element to be updated to node η j ; upon receiving this packet, the latter updates the value of the data element accordingly. It is important to mention that this NoC based CMP architecture is very generic and represents several possible instantiations. It is also worth mentioning that, while we discuss our application mapping scheme in the context of this two-dimensional mesh NoC, our approach is applicable to other types of network topologies as well, provided that the targeted topology is exposed to the compiler. Going to a different topology can change the concept of locality we have (as there will be a change in the number of neighbors for a given node), but our algorithms can be easily adapted to work with this change. The path consisting of channels used to transfer a message from the source node to the destination is determined by the routing scheme employed by the underlying implementation of the mesh. Routing schemes <ref type="bibr" target="#b18">[18]</ref> can be classified into two categories: dynamic and static. Dynamic routing determines the communication channels used to transfer each message dynamically based on the network traffic state during the message transfer time. Static routing, on the other hand, determines the channels to be used to transfer each message statically based on the source and destination nodes of the message, irrespective of the dynamic network traffic state at runtime. The steps of the compiler-based approach proposed in this paper, except for the step described in Section 4.4, can work under both dynamic and static routing based NoCs. When there is no confusion, we use the terms "node" and "processor" interchangeably. Also, we use <ref type="bibr">[[i, j]</ref>] to denote the logical connection between nodes η i and η j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">APPLICATION EXECUTION MODEL</head><p>As noted by prior work <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b19">19]</ref>, application mapping is critical for NoC based platforms. To our knowledge, this is the first compiler-directed fully automated application mapping scheme for mesh based CMPs. In this paper, we focus on array-based, loop-intensive applications. Such an application typically consists of a set of loop nests that access a set of arrays. Array-based codes frequently appear in the embedded image/video processing domain <ref type="bibr" target="#b6">[6]</ref>, an important application segment for the NoC based systems. We further assume that a parallelizing compiler transforms each loop nest of the application into a set of parallel tasks such that each task contains a subset of the iterations of a given loop nest. The question of how these tasks are generated is orthogonal to the problem addressed in this paper. These tasks, also called threads in this paper, are then scheduled, by our approach, to be executed in parallel in our NoC based CMP system. Note that, in this paper, a task is the smallest unit for scheduling. The order in which the tasks of a given application can be executed is determined by data dependencies, which are captured in our approach by a directed graph called the task graph. Specifically, a task graph can be represented as G = (T, E), where T and E are the set of vertices and the set of edges, respectively. Each vertex in T corresponds to a task. An edge (t 1 , t 2 ) ∈ E ⊆ T × T indicates that task t 1 must be completed before the execution of task t 2 due to data/control dependences or other constraints. For a given task t, we use ρ(t) to denote the length of its execution time (in terms of cycles), under the assumption that all the data processed by t can be accessed without any network delay, i.e., all the data accessed by task t are stored in the memory that is in the same node as the CPU which executes task t.</p><p>We further assume that the data operated by the application code can be divided into a set of blocks. This is a reasonable assumption since, as mentioned earlier, our framework targets at applications that process mostly array data, which can be (logically) divided into blocks. We use D to denote the set of data blocks operated by the application. The location of a data block in the mesh based NoC is identified using a tuple (i, o), where i is the id of the node that contains the block and o is the address of the block within the memory component of this node. For a given task t and a data block d, we define functions r(t, d) and w(t, d) as the number of times that task t reads and writes, respectively, data block d during its execution. We  As an example, let us assume a code fragment, which contains four loops, as shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>. Assume further that, using a parallelizing compiler, we extract tasks t 1 , t 2 , t 3 , and t 4 from loop L1; tasks t 5 and t 6 from loop L2; t 7 and t 8 from loop L3; and tasks t 9 and t 10 from loop L4. Figure <ref type="figure" target="#fig_0">1(b)</ref> shows the corresponding task graph. For this code fragment, we have: The values of r(t, d) and w(t, d) for these tasks are listed in Table <ref type="table" target="#tab_0">1</ref>, assuming that each statement in the code has the same unit cost.</p><formula xml:id="formula_0">T = {t 1 , t 2 , t 3 , t 4 , t 5 , t 6 , t 7 , t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">OUR APPROACH</head><p>Our goal is to schedule the tasks on the available processors and map the data blocks into the memories of the CMP nodes such that the overall execution time of the application and the energy it spends during its execution are reduced. The proposed approach achieves this goal by improving the locality of data accesses, that is, by reducing the number of communication links that are visited in accessing data. Our scheme takes a task graph (such as the one shown in Figure <ref type="figure" target="#fig_0">1(b)</ref>) and a data access table (such as the one shown in Table <ref type="table" target="#tab_0">1</ref>) as input. It works in four steps as depicted in Figure <ref type="figure" target="#fig_3">2</ref>: task scheduling, processor mapping, data mapping, and packet routing. The goal of the task scheduling step is to cluster the tasks that share a large amount of data blocks among them into the same processor. In order to isolate task scheduling from processor mapping, we first schedule the tasks on a set of virtual processors, which will be explained shortly. After that, in the processor mapping step, we map the virtual processors onto physical processors to improve inter-processor data locality. That is, two virtual processors sharing a large amount of data are mapped to a pair of neighboring physical processors. In the next step, we map data blocks into the memories of the NoC nodes such that the overall memory access cost is minimized. Finally, we determine the routings for the data packets, i.e., the set of links that are exercised during data accesses. Note that the cost for accessing a data block is determined by the distance between the processor that issues the access and the node that contains the data block. Specifically, a longer distance to data (which is a measure of locality) means that the access request and the data block need to be transferred over a larger number of network links<ref type="foot" target="#foot_1">1</ref> , and thus, incurs more energy consumption and longer delay (depending on the network switching mechanism employed). Our compiler-based approach improves performance and reduces energy consumption by mapping the tasks and the data blocks into the CMP nodes such that we can reduce the distances between the processors and the data blocks that are frequently accessed by these processors. In our experimental evaluation, we also compare this locality-oriented approach to pure performance-oriented and pure-energy oriented schemes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task Scheduling</head><p>Our task scheduling algorithm takes the number of virtual processors P and the task graph of the application as input. It determines to which virtual processor each task should be assigned. Our goal in this step is to exploit the parallelism in the application code as much as possible, and maximize data reuse within each processor by scheduling the tasks that share a large number of data blocks on the same virtual processor.</p><p>Note that, instead of directly mapping tasks to physical processors in our NoC based CMP, we first schedule them on virtual processors. The advantage is that, using this concept of virtual processors, the scheduling and mapping steps of our approach can be optimized independently. This also reduces the complexity of retargeting the same source code to different CMP architectures with different number of physical processors. The number of virtual processors can be equal to or greater than the number of physical processors. In our current implementation however, the number of the virtual processors is the same as that of the physical processors.</p><p>Figure <ref type="figure" target="#fig_4">3</ref> gives our task scheduling algorithm. The complexity of this algorithm is O(|T| 2 ), where T is the set of tasks to be scheduled. In this algorithm, for each virtual processor v, we maintain two properties:</p><formula xml:id="formula_1">Q[v] and D[v]. Specifically, Q[v]</formula><p>gives the time when virtual processor v becomes available, and D[v] tracks the set of data blocks that are accessed by virtual processor v. At each iteration of the while-loop in our algorithm shown in Figure <ref type="figure" target="#fig_4">3</ref>, we schedule a task on virtual processor v at the earliest available time captured by Q[v]. Our algorithm terminates when all the tasks in the task graph have been scheduled. After we schedule a task t on virtual processor v, we update the values of Q[v] and D[v] as follows:</p><formula xml:id="formula_2">Q[v] = Q[v] + ρ(t); D[v] = D[v] ∪ B[t],</formula><p>where B[t] is the set of data blocks used by task t. When selecting a task t to be scheduled on virtual processor v, we try to maximize the value of</p><formula xml:id="formula_3">|D[v]∩ B[t]|. Note that D[v] ∩ B[t]</formula><p>gives the set of data blocks that can be reused by task t if task t is scheduled on virtual processor v. In order to guarantee that the tasks that are scheduled on different processors will be executed in the order specified by the task graph, the compiler also inserts barriers into the code when necessary. As an example, Figure <ref type="figure" target="#fig_5">4</ref> shows the result of applying our algorithm to the task graph shown in Figure <ref type="figure" target="#fig_0">1</ref>(b), assuming, for the sake of illustration, that each task takes C cycles to execute and three virtual processors are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Processor Mapping</head><p>In this step, we map the virtual processors to the physical processors considering data locality. Specifically, we try to map the virtual processors that share a large amount of data among them to neighboring physical processors. An important point to mention here is that, while the previous step does not take into account the network structure, this step considers network topology in exploiting locality. In order to capture the amount of data sharing between a pair of virtual processors, v 1 and v 2 , we define an inter-processor data sharing function χ as follows:</p><formula xml:id="formula_4">χ(v 1 , v 2 ) = X d∈D min{F (v 1 , d), F (v 2 , d)},</formula><p>where function F (v, d) gives the number of times that virtual processor v accesses data block d, which can be defined as follows:</p><formula xml:id="formula_5">F (v, d) = X t∈θ(v) [2r(t, d) + w(t, d)],<label>(1)</label></formula><p>where θ(v) gives the set of tasks that are scheduled on virtual processor v. Note that, when computing F (v, d), we give weights 2 and 1 to the read and write accesses, respectively, because each read access requires two data packets, while each write access requires only one. As an example, Figures <ref type="figure" target="#fig_8">6(a</ref>) and (b) show the values of F (v, d) and χ(v i , v j ), respectively,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Task graph G = (T, E). P : number of virtual processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>Schedule for each virtual processor.</p><formula xml:id="formula_6">for v = 1 to P { Q[v] = 0; // Q[v]</formula><p>: the time when virtual processor p becomes available.</p><formula xml:id="formula_7">D[v] = ∅; // D[v]</formula><p>: the set of data block that have been accessed // by virtual processor v. } T = T; // the set of tasks to be scheduled. while   for the task graph in Figure <ref type="figure" target="#fig_0">1</ref>(b) under the scheduling shown in Figure <ref type="figure" target="#fig_5">4</ref>.</p><formula xml:id="formula_8">(T = ∅) { select v from 1..P such that Q[v] is minimized; let R = {t | t ∈ T and pred(t) ∩ T = ∅}; // R</formula><formula xml:id="formula_9">v; T = T -{t}; Q[v] = Q[v] + ρ(t); D[v] = D[v] ∪ B[t] }</formula><p>A processor mapping can be expressed using a function π such that π(v) gives the physical processor to which virtual processor v is mapped. A constraint on π is that π(v 1 ) = π(v 2 ) if v 1 = v 2 , i.e., we cannot map two different virtual processors to the same physical processor. In order to evaluate the quality of a processor mapping, we define a function G as follows:</p><formula xml:id="formula_10">G(π) = X v 1 ,v 2 ∈V |π(v 1 ) -π(v 2 )|χ(v 1 , v 2 ),</formula><p>where V is the set of virtual processors, π is a given processor mapping function, and |p 1p 2 | is the distance between physical processors p 1 and p 2 in terms of network hops (links). For a mesh based NoC, assuming that the coordinates of physical processors p 1 and p 2 are (x 1 , y 1 ) and (x 2 , y 2 ), respectively, we have:</p><formula xml:id="formula_11">|p 1 -p 2 | = |x 1 -x 2 | + |y 1 -y 2 |.</formula><p>A smaller value of G(π) indicates that π maps the virtual processors that share a large amount of data between them to the physical processors that are close to each other in the NoC; in other words, such a mapping indicates a good locality of data accesses. Therefore, our goal is to find a mapping function π * such that the value of G(π * ) is minimized. Figure <ref type="figure" target="#fig_6">5</ref> gives a sketch of our processor mapping algorithm. In this algorithm, we first map P virtual processors onto P physical processors using a default mapping π (any one-to-one mapping can serve as the default mapping). After that, we select two virtual processors v i and v j and construct a new mapping π ′ from π as follows:</p><formula xml:id="formula_12">π ′ (v) = ( π(v), if v = v i , v j ; π(v j ), if v = v i ; π(v i ), if v = v j .</formula><p>That is, we obtain a new mapping π ′ by swapping the target physical processors of virtual processors v i and v j . We next compare the values of G(π) and G(π ′ ). If G(π) &gt; G(π ′ ), we know that π ′ is a better mapping than π; and thus, we replace π with π ′ . We repeat this swapping activity until we are no longer able to achieve a better mapping by swapping the target physical processors of any two virtual processors.</p><p>The complexity of this algorithm is O(P N N 2 ), where N is the number of physical processors, and P is the number of virtual processors. Let us further explain the operation of our algorithm using the example shown in Figure <ref type="figure" target="#fig_8">6</ref>. Recall that Figures <ref type="figure" target="#fig_8">6(a</ref>) and (b) give the values of F (v, d) and χ(v i , v j ), respectively, for the task graph in Figure <ref type="figure" target="#fig_0">1</ref>(b) under the scheduling shown in Figure <ref type="figure" target="#fig_5">4</ref>. For illustrative purposes, we assume that three physical processors, p 1 , p 2 , and p 3 , are allocated for the application at hand, as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>P -the number of virtual processors. χ[1..P, 1..P ]χ[i, j] is value of inter-processor sharing function for virtual processors vi and vj . Output:</p><formula xml:id="formula_13">X[1..P ], Y [1..P ] -(X[i], Y [i]</formula><p>) is the coordinate of the physical processor in the mesh to which virtual processor vi is mapped. depicted in Figure <ref type="figure" target="#fig_8">6</ref>(c). We can calculate the distances among these processors, in terms of the network hops, as follows:</p><formula xml:id="formula_14">k = 1; for i = 1 to N for j = 1 to N { X[k] = i; Y [k] = j; k = k + 1; } done = false; g = G(); while(not done) { done = true; for i = 1 to N for j = 1 to N { swap(X[i], X[j]); swap(Y [i], Y [j]); g ′ = G(); if(g ′ &lt; g) { g = g ′ ; done = false; } else { swap(X[i], X[j]); swap(Y [i], Y [j]); } } } function int G() { g = 0; for i = 1 to N for j = 1 to N g = g + (|X[i] -X[j]| + |Y [i] -Y [j]|)χ[i, j]; return g; }</formula><formula xml:id="formula_15">|p 1 -p 2 | = 2; |p 1 -p 3 | = 1; and |p 2 -p 3 | = 1.</formula><p>We now need to map virtual processors v 1 , v 2 , and v 3 to these physical processors. Our algorithm starts with π 1 , which maps virtual processors v 1 , v 2 , and v 3 to physical processors, p 1 , p 2 , and p 3 , respectively, as shown in Figure <ref type="figure" target="#fig_8">6</ref>(c). The cost function for this mapping, G(π 1 ), can be calculated as 900. By switching the target physical processors of a pair of virtual processors, we can obtain three different mappings, π 2 , π 3 , and π 4 from mapping π 1 . Since G(π 4 ) &lt; G(π 1 ), we continue our search from π 4 . From π 4 , we obtain π 5 , π 6 , and π 7 by switching the target physical processors of a pair of virtual processors. Since none of these mappings yields a lower data access cost (i.e., the G() function) than the current one, our algorithm terminates giving π 4 as the final result, i.e., we map virtual processors v 1 , v 2 , and v 3 to physical processors, p 1 , p 3 , and p 2 , respectively. Figure <ref type="figure" target="#fig_8">6(d)</ref> gives the solution space explored by our algorithm for this example.</p><p>Note that, in general, for a large application, the solution space that has to be explored by this mapping scheme can be large. Therefore, in our baseline implementation, we limited the number of swaps performed by the algorithm to 50 (i.e., at most 50 swaps are performed), a number which, we found, performs well in practice. However, we also made experiments with other (limit) values and with an implementation that does not adopt any limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Mapping</head><p>The data mapping step of our approach is applied after the processor mapping step. At this step, we distribute data blocks into the memories attached to the NoC nodes such that the overall memory access cost is minimized. A data mapping can be expressed using a data mapping function φ such that φ(d) gives the NoC node to which data block d is mapped. For a given processor mapping π and a data mapping ψ, the overall memory access cost can be computed as:</p><formula xml:id="formula_16">H(π, ψ) = X d∈D X v∈V |π(v) -ψ(d)|F (v, d),</formula><p>where function F is as defined in Expression (1), and |π(v) is the distance (in terms of network hops) between the physical processor of virtual processor v and the node to which data block d is mapped. Note that, in this equation,</p><formula xml:id="formula_17">X v∈V |π(v) -ψ(d)|F (v, d)</formula><p>gives the overall cost due to the accesses to data block d.</p><p>Figure <ref type="figure">7</ref> shows our data mapping algorithm. The complexity of this algorithm is O(|D|N 2 ), where N is the number of physical processors, and   D is the set of data blocks. For each data block d, this algorithm searches every node in the NoC with sufficient free memory space to hold data block d. If there are more than one node with sufficient free memory space to hold data block d, we select the one with the minimum overall cost due to the accesses to data block d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Packet Routing</head><p>As discussed earlier in Section 2, in our NoC-based chip multiprocessor architecture, CPUs access data by sending and receiving data packets over communication links. We say that an application uses logical connection [[i, j]] if one of the following two conditions is satisfied:</p><p>• A task t reads or writes data block d, t is mapped to the physical processor in node η i , and d is mapped to the memory in node η j . In this case, task t in η i sends the access request to node η j through the logical connection [[i, j]].</p><p>• A task t reads data block d, t is mapped to the physical processor in node η j , and d is mapped to the memory in node η i . In this case, the memory in η i sends the data required by task t in node η j through the logical connection <ref type="bibr">[[i, j]</ref>].</p><p>The goal of packet routing is to map each logical connection to a path of communication channels that transfers the data packets from the source node to the destination node. Two logical connections are said to interfere with each other if they might be exercised by the application simultaneously. If two logical connections interfere with each other, the routing algorithm should map them to two paths that do not share any channels between them. Otherwise, the packets transferred by these two logical connections might compete for the shared channels, which can in turn increase the communication delays. On the other hand, if two logical connections do not interfere with each other, i.e., the application never uses them simultaneously, we want to maximize the number of communication channels that are shared by these logical connections, so that we can minimize the number of communication channels exercised by the application, and thus, more communication channels can be kept in the low power mode to conserve energy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2: Default system configuration parameters.</head><p>For routing packets across the mesh in an energy-aware fashion, we employ a technique inspired by the algorithm proposed in <ref type="bibr">[7]</ref>. This algorithm reuses communication channels as much as possible to reduce energy consumption without significantly increasing communication delay. Our implementation takes a communication interference graph as input and determines the routing for each data packet set. A communication interference graph captures the interference between the pairs of logical connections. Specifically, each vertex in this graph corresponds to a logical connection used by the application, and each edge between a pair of vertices indicates that the corresponding logical connections interfere with each other. In our implementation, we built the communication interference graph from our task graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL EVALUATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>The compiler algorithms presented in Section 4 are implemented within the SUIF compiler infrastructure from Stanford University <ref type="bibr" target="#b1">[1]</ref>. The increase in compilation time due to our approach (its four steps) over the case when the codes in our experimental suite are compiled (parallelized) without our passes was about 57%, when averaged over all twelve benchmarks we have. The largest compilation time we observed with our approach was slightly over 2.5 minutes. Also, the code size increase due to our approach was less than 5% for all the applications tested. In order to quantify the benefits of our approach, we performed experiments using a simulation infrastructure, which has two components: Simics <ref type="bibr" target="#b25">[25]</ref> and Orion <ref type="bibr" target="#b26">[26]</ref>. Specifically, an enhanced version of Simics (with cycle accurate memory access models) is used for simulating parallel execution of processors that share the on-chip memory space, whereas Orion is used for modeling NoC based communication and calculating communication energy. This simulation infrastructure is fully-coupled, i.e., the network delays are accounted for calculating CPU execution timings. In addition, this simulation platform is tuned (scaled) to simulate large number of CPUs and made cycle accurate. This infrastructure is executed on a Solaris 9 machine. Our simulation environment takes, as input, an application executable and a description and generates, as output, several statistics, including the average number of links traversed by each packet, the overall execution cycles, and the energy consumed by the application (including network, memory and computation energies). The energy numbers presented below include both dynamic energy (i.e., the energy consumed due to switching activities) and leakage energy (i.e., the energy consumed as long as the circuit is powered on) components. As mentioned above, in computing the network energy, we used an enhanced version of Orion. On the other hand, for computing the CPU execution and memory access energies, we enhanced Simics with energy models similar to those used by Wattch <ref type="bibr" target="#b5">[5]</ref>. Table <ref type="table">2</ref> presents the default values of the simulation parameters used in the experiments.</p><p>The important statistics on the benchmark codes used in this study are given in Table <ref type="table" target="#tab_4">3</ref>. We included all the SpecFP2000 benchmarks except facerec and sixtrack, which could not be executed through our simulation platform. The second, third, and fourth columns show, respectively, the average number of links traversed by a packet, the total execution cycles, and energy consumption when the computation and data are mapped using the approach described in Anderson's thesis <ref type="bibr" target="#b2">[2]</ref>. Anderson formulates the data and computation mapping problem using a linear algebraic framework and solves it using a heuristic. In her formulation, each locality requirement is expressed as a constraint. The energy consumption results presented in Table <ref type="table" target="#tab_4">3</ref> include both computation and communication energies. It is important to mention at this point that the approach in <ref type="bibr" target="#b2">[2]</ref> is a pure performanceoriented one and does not take into account the network structure (though it does an excellent job, in our opinion, in reducing the number of accesses to remote data). In other words, this performance-oriented approach minimizes the number of nonlocal accesses but it does not care where the nonlocal data resides or how it is accessed (i.e., through which links). The last two columns of Table <ref type="table" target="#tab_4">3</ref> give the execution cycles and energy consumption results when the approach in <ref type="bibr" target="#b2">[2]</ref> is followed by an energy-saving scheme, which turns off the unused links and scales down the voltage/frequency on others whenever there is an opportunity to do so. The specific link shutdown and voltage scaling algorithms used are adapted from <ref type="bibr" target="#b16">[16]</ref> and <ref type="bibr" target="#b8">[8]</ref>, respectively. The link shut-down scheme <ref type="bibr" target="#b16">[16]</ref> identifies the communication links that will not be exercised by the current computation and turns them off to save energy. The voltage scaling scheme <ref type="bibr" target="#b8">[8]</ref>, on the other hand, identifies the best voltage/frequency level for each communication link such that energy consumption is reduced without affecting performance. In our implementation of these schemes, we first applied the link shut-down algorithm, and then, for the communication links that could not be shut down completely, we applied the voltage scaling algorithm to reduce their power consumptions. The important point to note from Table <ref type="table" target="#tab_4">3</ref> is that applying energy saving techniques reduces energy consumption by 13.71% on average and increases execution cycles by 3.77%. In our setting, the processors are turned off when they are waiting in synchronization points; this prevents them from consuming extra energy on synchronization points. Recall from Section 3 that our approach can work with any code parallelization (task generation) scheme. The specific one used in this work parallelizes the outermost loop from each dependence-free nest across available processors. The sequential nests on the other hand are executed by a single processor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>We now quantify the impact of our compiler-based application mapping approach on the number of communication links traversed per packet, energy consumption and execution cycles. Our first set of results are presented in Figure <ref type="figure">8</ref> and show the average number of links traversed per packet when our approach is employed. For ease of comparison, we also reproduce the results from the second column of Table <ref type="table" target="#tab_4">3</ref>. We see from these results that our approach cuts the number of links traversed significantly. Specifically, the average number of links traversed are 1.49 and 2.3 for our approach and the approach in <ref type="bibr" target="#b2">[2]</ref>, respectively. This result underlines the importance of taking the network topology into account in mapping tasks and data to the NoC nodes, and also demonstrates that our approach improves locality of data accesses on the mesh significantly.</p><p>Before continuing with the rest of our experimental evaluation, we want to point out that the average number of traversed links per message is not very large (it is 2.30), even with the scheme in <ref type="bibr" target="#b2">[2]</ref>. The reason for this is that the scheme in <ref type="bibr" target="#b2">[2]</ref> is very successful in minimizing the number of nonlocal references. We observed that if the number of non-local references is not minimized, the average number of traversed links per message can be as high as 4.5 in our default 6 × 6 mesh configuration. Our approach tries to satisfy the non-local references from nearby nodes instead of distant nodes by exploiting data locality.</p><p>Our second set of results help us study the energy consumption trends and are presented in Figure <ref type="figure">9</ref>. Recall that our energy consumption statistics include all components of computation, communication, and memory access energies. In this graph, we have three bars for each application. The last bar (marked as "all") represents the energy consumption, normalized with respect to the fourth column of Table <ref type="table" target="#tab_4">3</ref>, when our approach is used. The second bar gives the normalized energy consumption when the data mapping component of our approach is not used, and the first bar shows the result when the packet routing component of our approach is not used. In obtaining the results captured by the first two bars, all other components of our approach (except the said component) are used. The main observation one can make from these results is that both the data mapping and packet routing steps of our approach are important since the average energy savings captured by the first, second and third bars are 13.03%, 11.77% and 27.41%, respectively.</p><p>The graph in Figure <ref type="figure" target="#fig_0">10</ref> shows the energy results obtained by our approach when it is followed by the link shut-down and voltage scaling algorithms adapted from <ref type="bibr" target="#b16">[16]</ref> and <ref type="bibr" target="#b8">[8]</ref>. Each bar in this graph is normalized with respect to the energy results shown in the sixth column of Table <ref type="table" target="#tab_4">3</ref>. We see from these results that, even if the underlying network uses energy-saving mechanisms such as communication link shutdown and voltage scaling, optimizing data locality is still important and reduces energy consumption by 30.97% on average. In fact, when the NoC employs power-saving schemes such as link shutdown and voltage scaling, the energy reductions brought by our approach become slightly higher. In addition, like those in Figure <ref type="figure">9</ref>, these results in Figure <ref type="figure" target="#fig_0">10</ref> also re-iterate the importance of using all components of our approach. When one considers the results presented in Figures <ref type="figure">8,</ref><ref type="figure">9</ref>, and 10 together, it is clear that our application mapping approach is useful in both the cases, i.e., when the underlying NoC employs energy-saving mechanisms and when it does not. Another important observation to point out is that, our approach saves more energy than employing pure power-saving techniques alone. This can be seen when we consider the fourth and sixth columns of Table <ref type="table" target="#tab_4">3</ref> along with the results presented in Figure <ref type="figure">9</ref>, which indicates that, while the pure energy-saving techniques reduce energy consumption by 13.71% on average, our approach reduces it by 27.41% on average.</p><p>We next present the results that quantify the impact of our approach on execution cycles. These results are presented in Figure <ref type="figure" target="#fig_9">11</ref>. While the results captured by the first bar for each application are normalized with respect to the third column of Table <ref type="table" target="#tab_4">3</ref>, the results captured by the second bar are normalized with respect to the fifth column of the same table. These latter results are obtained when our approach is followed by link shutdown and voltage scaling. One can observe from these results that, while the savings in execution cycles are not as high as those in energy consumption (as some performance overhead due to remote data accesses can be hidden in parallel execution), we still observe an average improvement of 12.85% when the underlying network does not adopt any power saving scheme and of 12.60% when it does. It is important to note that both energy and performance improvements brought by our approach is a direct result of cutting the distances between the processors that request data and the memories of the CMP nodes that contain these requested data, i.e., optimizing for data locality in our NoC-based CMP is important from both the power and performance perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>While recent research from both academia and industry shows the feasibility of building NoC-based CMPs, compiler support for mapping applications onto these architectures has not received much attention so far. This is unfortunate since the performance and power benefits that could be extracted from these architectures depend strongly on our ability of map-ping applications to these architectures in a performance and energy efficient fashion. Motivated by this observation, in this paper, we discuss and evaluate a compiler-based application mapping algorithm, which consists of four major steps. The experimental analysis presented clearly shows that the proposed framework reduces energy consumption of our applications significantly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A code fragment and its task graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>8 , t 9 , t 10 } D = {A[0..99], A[100..199], A[200..299], A[300..399], B[0..99], B[100..199], C[0..99], C[100..199], D[0..99], D[100..199]}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: High level view of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Task scheduling algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The schedule of the tasks shown in Figure 1(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Processor mapping algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(c) Physical processors in a mesh. (d) Searching the solution space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example application of processor mapping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 8: Average number of links exercised per packet under our approach and Anderson's approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : The values of</head><label>1</label><figDesc></figDesc><table><row><cell>r(t, d) and w(t, d)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>for the tasks shown in Figure 1(b).</head><label></label><figDesc></figDesc><table><row><cell>assume that r(t, d) and w(t, d) for each task-data pair (t, d) are stored in</cell></row><row><cell>a two-dimensional data access table. Note that the values of r(t, d) and</cell></row><row><cell>w(t, d) can be determined either by static analysis of the code or through</cell></row><row><cell>profiling (our implementation can use both).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>is the set of tasks that are ready to be scheduled.select t ∈ R such that |D[v] ∩ B[t]| is maximized. // B[t] is the set of data blocks that are used by task t. // Specifically, B[t] = {d|d ∈ D and w(t, d) + r(t, d) &gt; 0}; schedule task t on virtual processor</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 : Important statistics on the applications used in our experiments.</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">W/out Pow Sav Tech</cell><cell cols="2">W/ Pow Sav Tech</cell></row><row><cell cols="3">Benchmark Number of Execution</cell><cell>Energy</cell><cell>Execution</cell><cell>Energy</cell></row><row><cell>Name</cell><cell>Links</cell><cell cols="4">Cycles Consumption Cycles Consumption</cell></row><row><cell>wupwise</cell><cell>3.41</cell><cell>1083.5</cell><cell>3.87</cell><cell>1127.4</cell><cell>3.30</cell></row><row><cell>swim</cell><cell>1.96</cell><cell>1339.2</cell><cell>4.96</cell><cell>1408.1</cell><cell>4.13</cell></row><row><cell>mgrid</cell><cell>2.27</cell><cell>1690.9</cell><cell>5.12</cell><cell>1722.3</cell><cell>4.49</cell></row><row><cell>applu</cell><cell>1.86</cell><cell>1527.2</cell><cell>5.07</cell><cell>1599.3</cell><cell>4.35</cell></row><row><cell>mesa</cell><cell>1.88</cell><cell>1298.8</cell><cell>4.74</cell><cell>1306.7</cell><cell>4.07</cell></row><row><cell>galgel</cell><cell>1.63</cell><cell>11083.9</cell><cell>14.51</cell><cell>11507.6</cell><cell>12.6</cell></row><row><cell>art</cell><cell>2.64</cell><cell>1791.7</cell><cell>4.94</cell><cell>1905.4</cell><cell>4.81</cell></row><row><cell>equake</cell><cell>3.24</cell><cell>1473.8</cell><cell>4.71</cell><cell>1488.2</cell><cell>4.29</cell></row><row><cell>ammp</cell><cell>2.52</cell><cell>1440.1</cell><cell>4.57</cell><cell>1511.7</cell><cell>3.48</cell></row><row><cell>lucas</cell><cell>1.74</cell><cell>814.6</cell><cell>2.36</cell><cell>903.3</cell><cell>2.17</cell></row><row><cell>fma3d</cell><cell>1.80</cell><cell>1323.7</cell><cell>4.82</cell><cell>1352.0</cell><cell>4.16</cell></row><row><cell>apsi</cell><cell>2.05</cell><cell>1264.9</cell><cell>3.95</cell><cell>1286.6</cell><cell>3.05</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>* This work is supported by NSF grants CNS #0720645, CCF #0702519, a grant from Microsoft Corporation, and a support from the Gigascale Systems Research Focus Center, one of the five research centers funded under SRC's Focus Center Research Program.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>We use the terms "link", "channel", and "hop" interchangeably.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>X</p><p>) is the coordinates of the physical processor to which virtual processor v is mapped. F [1..P, 1..P ] -F (v, d) gives the number of times that virtual processor v accesses data block d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output:</head><p>DX</p><p>) is the coordinate of the mesh node to which data block d is mapped. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The SUIF compiler for scalable parallel machines</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Seventh SIAM Conference on Parallel Processing for Scientific Computing</title>
		<meeting>Seventh SIAM Conference on Parallel essing for Scientific Computing</meeting>
		<imprint>
			<date type="published" when="1995-02">Feb. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic Computation and Data Decomposition for Multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997-03">March 1997</date>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. Thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A research tool for automatic data distribution in HPF</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ayguade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="95" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Powering NoCs: energy-efficient and reliable interconnect design for SoCs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISSS</title>
		<meeting>ISSS</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wattch: a framework for architectural-level power analysis and optimizations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="83" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Data Access and Storage Management for Embedded Programmable Processors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Catthoor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Kluwer Academic Publ</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Compiler-directed channel allocation for saving power in on-chip networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. POPL</title>
		<meeting>POPL<address><addrLine>Charleston, SC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-01">Jan. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing NoC energy consumption through compiler-directed channel voltage scaling</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PLDI</title>
		<meeting>PLDI<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Route packets, not wires: on-chip interconnection networks</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Towles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DAC</title>
		<meeting>DAC<address><addrLine>Las Vegas, NV</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Demonstration Automatic data partitioning techniques for parallelizing compilers on multicomputers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Banerjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<date type="published" when="1992-03">March 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Energy-and performance-aware mapping for regular NoC architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCAD</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="551" to="562" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Energy optimization techniques in cluster interconnects</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Yum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Link</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vijaykrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yousif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISLPED</title>
		<meeting>ISLPED</meeting>
		<imprint>
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Energy characterization of a tiled architecture processor with on-chip networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISLPED</title>
		<meeting>ISLPED</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Space-time scheduling of instruction-level parallelism on a RAW machine</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srikrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Babb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASPLOS</title>
		<meeting>ASPLOS</meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convergent scheduling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Puppin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Swenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO</title>
		<meeting>MICRO</meeting>
		<imprint>
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compiler-directed proactive power management for networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CASES</title>
		<meeting>CASES</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Static placement, dynamic issue (SPDI) scheduling for EDGE architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Kushwaha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PACT</title>
		<meeting>PACT</meeting>
		<imprint>
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey of wormhole routing techniques in direct networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="62" to="76" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Key research problems in NoC design: a holistic perspective</title>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">Y</forename><surname>Ogras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CODES+ISSS</title>
		<meeting>CODES+ISSS<address><addrLine>Jersey City, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-09">Sep. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Power-aware communication optimization for networks-on-chips with voltage scalable links</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CODES+ISSS</title>
		<meeting>CODES+ISSS</meeting>
		<imprint>
			<date type="published" when="2004-09">Sept. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Managing power consumption in networks on chip</title>
		<author>
			<persName><forename type="first">T</forename><surname>Simunic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. DATE</title>
		<meeting>DATE</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Software-directed power-aware interconnection networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Soteriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CASES</title>
		<meeting>CASES</meeting>
		<imprint>
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<ptr target="http://www.spec.org/cpu/" />
		<title level="m">Spec CPU2000 V 1.3</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automatic partitioning of data and computations on scalable shared memory multiprocessors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tandri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdelrahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPP</title>
		<meeting>ICPP</meeting>
		<imprint>
			<date type="published" when="1997-08">August 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Virtutech</forename><surname>Simics</surname></persName>
		</author>
		<ptr target="http://www.virtutech.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orion: A power-performance simulator for interconnection networks</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. MICRO</title>
		<meeting>MICRO</meeting>
		<imprint>
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
