<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Furhat: A Back-Projected Human-Like Robot Head for Multiparty Human-Machine Interaction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Samer</forename><surname>Al Moubayed</surname></persName>
							<email>sameram@speech.kth.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Speech, Music, and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>Lindstedtsvägen 24</addrLine>
									<postCode>10044SE</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
							<email>beskow@speech.kth.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Speech, Music, and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>Lindstedtsvägen 24</addrLine>
									<postCode>10044SE</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><surname>Skantze</surname></persName>
							<email>skantze@speech.kth.se</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Speech, Music, and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>Lindstedtsvägen 24</addrLine>
									<postCode>10044SE</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Björn</forename><surname>Granström</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Speech, Music, and Hearing</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>Lindstedtsvägen 24</addrLine>
									<postCode>10044SE</postCode>
									<settlement>Stockholm</settlement>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Furhat: A Back-Projected Human-Like Robot Head for Multiparty Human-Machine Interaction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5797CBE717D20FC53F3321C26EF90FB3</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Facial Animation</term>
					<term>Talking Heads</term>
					<term>Robot Heads</term>
					<term>Gaze</term>
					<term>Mona Lisa Effect</term>
					<term>Avatar</term>
					<term>Dialogue System</term>
					<term>Situated Interaction</term>
					<term>Back Projection</term>
					<term>Gaze Perception</term>
					<term>Furhat</term>
					<term>Multimodal Interaction</term>
					<term>Multiparty Interaction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this chapter, we first present a summary of findings from two previous studies on the limitations of using flat displays with embodied conversational agents (ECAs) in the contexts of face-to-face human-agent interaction. We then motivate the need for a three dimensional display of faces to guarantee accurate delivery of gaze and directional movements and present Furhat, a novel, simple, highly effective, and human-like back-projected robot head that utilizes computer animation to deliver facial movements, and is equipped with a pan-tilt neck. After presenting a detailed summary on why and how Furhat was built, we discuss the advantages of using optically projected animated agents for interaction. We discuss using such agents in terms of situatedness, environment, context awareness, and social, human-like face-toface interaction with robots where subtle nonverbal and social facial signals can be communicated. At the end of the chapter, we present a recent application of Furhat as a multimodal multiparty interaction system that was presented at the London Science Museum as part of a robot festival,. We conclude the paper by discussing future developments, applications and opportunities of this technology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has always been an urge in humans to give machines an anthropomorphic appearance and behavior. This urge, perhaps, comes from the human interest to understand and recreate themselves, since humans can be considered (or at least appear to be) the most intelligent and complex animations of life. This orientation of giving machines a human body and face has been clear since the beginning of works on robotics. For example, the word "robot" was introduced to the public by the Czech interwar writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920. The play begins in a factory that makes artificial people called robots, though they are closer to the modern ideas of androids, creatures that can be mistaken for humans <ref type="bibr" target="#b0">[1]</ref>.</p><p>The Holy Grail in the quest for building human-like robots, however, has been the human face. Simulating the appearance and dynamics of the human face has been shown to be an intensely complex matter. The human face, with its subtle and minute movements, carries an incredible amount of information that is designed to be read and interpreted by others. For instance, the human lips carry significant information about speech and intonation <ref type="bibr" target="#b1">[2]</ref>  <ref type="bibr" target="#b2">[3]</ref>, the eyes are a mirror to the mind, affect and attention <ref type="bibr">([4]</ref>  <ref type="bibr" target="#b4">[5]</ref>). The combination of these components provides the human with the possibility to communicate emotions as well as interests. However, it also provides information about more physical parameters such as age and gender, ( <ref type="bibr" target="#b5">[6]</ref>).</p><p>The efforts for building natural anthropomorphic faces has mainly taken two different tracks; one building of physical, mechanical heads that simulate the structure and appearance of a human face; and the other one has been focusing on building three dimensional digital animated computer models. Figure <ref type="figure" target="#fig_0">1</ref> illustrates examples for both tracks.</p><p>Building computer simulations of the human face has indeed been a challenging task, but recently making impressive progress. This is mainly due to its major applications in the gaming and moving-picture industries, those being the driving forces behind much of the progress. These models have also been intensively used as a research tool to better understand the functionality of the human face, taking advantage of the flexibility and easy manipulation of these models. An important advantage of these computer models is that they can be replicated at no cost, providing different branches of research and industry with very good accessibility.</p><p>Unfortunately, this advancement has not been paralleled in robotics in general: The easy control of computer models is not easily mapped onto control of muscular and mechatronic movements of servos implemented in robotic heads <ref type="bibr" target="#b6">[7]</ref>, introducing huge limitations in human-looking robotic faces to exhibit smooth and human-like movement, and hence introducing inconsistencies between how the robot looks and how it behaves (usually referred to as the uncanny valley <ref type="bibr" target="#b7">[8]</ref>). The other limitation of building human-like robotic faces is their expensive manufacturing and replication. At the moment, there are only a handful of human-like robots, which is making them exclusive and inaccessible to both the research community and the public. Some trials have been carried out to bridge this gap between software animation (virtual agents) and physical robots. One solution has been to use a computer screen as a robot head <ref type="bibr" target="#b8">[9]</ref>, with a virtual agent embedded into it. This approach offers a face with natural looks and dynamics while preserving a physical robot body. However, it naturally suffers several limitations and problems that come with using a flat display as an alternative to a three dimensional physical head, such as that, (aside from large aesthetic inconsistencies), flat displays are not three dimensional and suffer from lacking absolute direction of what is presented into them in relation to where the screen is placed (more detailed discussion in Section 2).</p><p>In this chapter, we are presenting a highly natural and effective hybrid solution for using animated agents for robotic heads. We are building on two previous studies that demonstrate the limitations of flat screens in delivering accurate direction of gaze, and hence limit the capabilities of animated agents to carry out situated, multiparty interaction. After that, we present Furhat, a three dimensional back-projected robot head that utilizes a computer animated face. We describe the details on how Furhat was built and what advantages it offers over, both in-screen animated agents, and mechanical robotic heads. After that we discuss possible applications of using Furhat for multimodal, multiparty human-machine interaction, and demonstrate a system for a three-party dialogue with Furhat which has recently been showcased at the London Science Museum as part of a European robot festival. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Animated Agent and Mechanical Robots</head><p>As discussed earlier, interactive agents that are made to look and act as humans can come in two instantiations. First as virtual characters (where the body and face of the agent is a computer software), or second, as physical robots.</p><p>One may think of robots as situated physical agents: At the time of interaction, the agent and the human are co-present spatially and temporally, which ultimately simulates the human-human communication setup. However, virtual agents are computer software that are, clearly, not co-present spatially with the interactive partner (the human) in the same space, but can be thought of as living in a virtual space. Many approaches have been tried to optimally bridge these two physical and virtual worlds, and bring the human and the virtual agent into the same world. Those being virtual reality interfaces (Figure <ref type="figure" target="#fig_1">2</ref> left), and holographic projections (Figure <ref type="figure" target="#fig_1">2 right</ref>).</p><p>In virtual reality, pragmatically, the human is transferred into the three dimensional virtual world, while in holographic projection, the virtual three dimensional world is transferred into our own reality, and hence, both co-exist spatially with the human interlocutor.</p><p>These two solutions are highly complex, exclusive and expensive, and are seldom used as a user interface with virtual characters. However, the predominant solution to bridging the virtual and the real worlds has been via projections onto flat displays (such as flat screens, wall projections, etc.); an example is shown in the middle of Figure <ref type="figure" target="#fig_1">2</ref>. The flat display functions as a window between the world the human interlocutor is situated in, and the virtual world of the virtual character <ref type="bibr" target="#b9">[10]</ref>. It is known that the perception of three-dimensional objects that are displayed on two-dimensional surfaces is guided by, what is commonly referred to as the Mona Lisa effect <ref type="bibr" target="#b10">[11]</ref>. This means that the orientation of the three-dimensional objects in relation to the observer will be perceived as constant, no matter where the observer is standing in the room or in relation to the display. For example, if the portrait of a face is gazing forward, mutual gaze will be established between the portrait and the observer, and this mutual gaze will hold no matter where the observer is standing. Accordingly, if the portrayed face is gazing to the right, everyone in the room will perceive the face as looking to their left. Thus, either all observers will establish mutual gaze with the portrait or none of them will. This implies that no exclusive eyecontact between the portrait and only one of the observers is possible. This principle, of course, extends to all objects viewed on 2D surfaces, such as pointing hands or arrows.</p><p>This effect can be seen as the cost of bridging the two different, virtual and real, worlds, to allow for direct visual interaction between humans and animated agents. This effect, clearly, has important implications on the design of interactive systems, such as embodied conversation agents, that are able to engage in situated interaction, as in pointing to objects in the environment of the interaction partner, or looking at one exclusive observer in a crowd.</p><p>In the following two sections we will present the results from two previous studies showing the limitations of the Mona Lisa effect on interaction, and presenting an approach on extending the use of animated faces from the flat screen onto physical three dimensional head models (and so building a physical situated robotic head). These two studies represent a proof of concept of this approach to overcome the limitations of flat displays of animated faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background Study 1: Perception of Gaze</head><p>Since the Mona Lisa gaze effect is introduced by 2D projection surfaces, we suggested an alternative to 2D projection surfaces, by which the Mona Lisa gaze effect would be avoided. Our approach in this experiment was to use a 3D physical, static model of a human head (as seen in Figure <ref type="figure">3</ref>). In order to compare this model with a traditional 2D projection surface, we designed an experimental paradigm that tests for mutual gaze as well as for gaze direction in the physical space of the viewer. The method is used to test the differences in accuracies in predicting gaze direction from a face that is presented through a 2D surface and the 3D projected surface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 3. An earlier approach for front projecting an animated face onto a physical head model using a micro laser projector</head><p>The technique of manipulating static objects with light is commonly referred to as the Shader Lamps technique <ref type="bibr">[12] [13]</ref>. This technique is used to change the physical appearance of still objects by illuminating them using projections of static or animated textures, or video streams.</p><p>In the perception experiment in <ref type="bibr" target="#b13">[14]</ref>, five subjects were simultaneously seated around an animated agent, which shifted its gaze in different directions (see Figure <ref type="figure" target="#fig_2">4</ref>). After each shift, each subject reported who the animated agent was looking at. Two different versions of the same head were used, one projected on a 2D surface, and one projected on a 3D static head-model (see Figure <ref type="figure">5</ref>). The results showed a very clear Mona Lisa effect in the 2D setting, where all subjects perceived a mutual gaze with the head at the same time for frontal and near frontal gaze angles.</p><p>While the head was not looking frontal, none of the subjects perceived mutual gaze with the head. In the 3D setting, the Mona Lisa effect was completely eliminated and the agent was able to establish mutual and exclusive gaze with any of the subjects. The subjects achieved a very high agreement rate on guessing on which subject the gaze of the agent was directed at for all the different gaze shifts. This study provides important insights and proves the principal directional properties of gaze through a 2D display surface. The study also shows that using the simple approach of optically projecting the same face model onto a 3D physical head model would eliminate that effect. However, the study does not show whether this effect will hold during interaction, or whether people are able to cognitively compensate for the effect, and correctly infer the intended direction of gaze.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Background Study 2: Interactional Effects of Gaze</head><p>In order to explore the interactional effects of gaze in a multi-party conversational setting, a similar experiment was carried out, but with spoken interaction between the head and the participants <ref type="bibr" target="#b14">[15]</ref>. Unlike the previous perception experiment, which focused on the perceived gaze, this experiment investigated how gaze may affect the turn-taking behavior of the subjects, depending on the use of 2D or 3D displays. Two sets of five subjects were asked to take part in the experiment. In each session, the five subjects were seated at fixed positions at an equal distance from each other and from an animated agent (just as in the previous experiment, see Figure <ref type="figure" target="#fig_2">4</ref>). The agent addressed the subjects by directing its gaze in their direction. Two versions of the agent were used, one projected on a 3D head model and one projected on a flat surface, as shown in Figure <ref type="figure">5</ref>. The conversational behavior of the animated agent was controlled using a Wizard-of-Oz setup. For each new question posed by the agent, the gaze was randomly shifted to a new subject. The subjects were given the task of watching a video from a camera navigating around the city of Stockholm, after which the animated agent asked them to describe the route they had just seen. After each video was finished, the animated agent started to ask the subjects about directions on how to reach the landmark the video ended with, starting from the point of view the video started with. Each set of subjects did four dialogs in both the 2D and the 3D condition (i.e. a total of eight videos).</p><p>To measure the efficiency of the gaze control, a confusion matrix was calculated between the intended gaze target and the actual turn-taker. The accuracy for targeting the intended subject in the 2D condition was 53% and 84% for the 3D condition. The mean response time was also calculated for each condition, i.e. the time between the gaze shift of the question and the time takes for one of the subjects to answer, which showed a significant difference in response time between the two conditions: 1.86 seconds for the 2D condition vs. 1.38 seconds in the 3D condition.</p><p>The results show that the use of gaze for turn-taking control on 2D displays is limited due to the Mona Lisa effect. The accuracy of 50% is probably too low in settings where many users are involved. By using a 3D projection, this problem can be avoided to a large extent. However, the accuracy for the 2D condition was higher than what was reported in the previous experiment. A likely explanation for this is that the subjects in this task may to some extent compensate for the Mona Lisa effect -even if they do not "feel" like the agent is looking at them, they may learn to associate the agent's gaze with the intended target subject. This comes at a cost, however, which is indicated by the longer mean response time. The longer response time might be due to the greater cognitive effort required making this inference, but also to the general uncertainty among the subjects about who is supposed to answer.</p><p>The subjects were also asked to fill out a questionnaire after the interactions, in which they compared the two versions of the head along three dimensions, as shown in Figure <ref type="figure" target="#fig_3">6</ref>. As the figure shows, the 3D version was clearly preferred, perceived as more natural, and judged as less confusing when it comes to knowing whose turn it was to speak. The Furhat Robot Head</p><p>As shown in the previous studies and discussions above, the paradigm of using a physical head model as a projection surface for animated computer models, would not only bring the face outside of the traditional two-dimensional screen, but will also eliminate the Mona Lisa effect and allow for multiparty interaction. From the study above in Section 4, it also appears that people perceive the projected face as significantly more natural than the face shown inside the screen. In addition to that, using the animated computer model as an alternative to a physical robot head solves major difficulties for building naturally looking and moving robot faces, since the technology behind facial animation has reached impressive advancements, and the control of these faces is highly simple and flexible. (Refer to <ref type="bibr" target="#b15">[16]</ref> for a short review on the benefits of this approach). Building on these encouraging findings, we have started building a natural and human-like robotic head that is based on the principle of optically projected computer models. A main modification was applied to the previous approach; that is to backproject the face onto the mask, so that the projector is hidden behind the mask. This means that if the mask is placed onto a robotic neck, the mask and the projector will be attached together and the projected image will not be displaced.</p><p>To build the head, several factors had to be taken into account. For example, micro projectors have a small projection angle, and hence if the projector is placed too close to the mask, the projected image will not be big enough to cover the entire projection area of the mask. Another factor was to use a material that will diffuse the light over the mask so that the light projected on the mask will be equally illuminated. One last important factor that had to be taken into account is to be able to acquire a mask model that would exactly fit the design of the projected face, so that no calibration and transformations of the model will be needed, and subtle facial areas, like the eyes, will naturally fit the area of the eyes on the mask.</p><p>Figure <ref type="figure" target="#fig_4">7</ref> shows a flow chart of the process of how the back-projected head is built. We call the head Furhat, as it got a fur hat that covers the top and the sides of the mask. Following is a detailed description on how Furhat has been built, so that it would provide more insights into the properties of the head, and comes as a guide for others to replicate it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building Furhat</head><p>In the following section we provide a chronological list of the main steps taken to build the robot head:</p><p>1-Using an animated face model: The 3D animated face model that is used for this study is detailed in <ref type="bibr" target="#b17">[17]</ref>. An animated face model is used due to several reasons: The lips of the face model can be automatically synchronized with the speech signal the system is producing; this is done by using a transcription of the speech utterances to be produced. The lip synchronization system utilized in the face model has proven to enhance speech intelligibility over listening only to the audio signal <ref type="bibr" target="#b18">[18]</ref>. This face model also offers flexible control of gestures and facial movements (gaze movements, eyebrows movements, etc.). Gestures played using this face model have also been shown to deliver the communicative functions they are designed for (eyebrows raise to signal questioning, doubt, or surprise <ref type="bibr" target="#b19">[19]</ref>); these gestures have also been shown to enhance speech intelligibility <ref type="bibr" target="#b20">[20]</ref>.</p><p>From this evidence, it is clear that this face model can deliver highly accurate and natural movements and would be suitable as a choice for Furhat's face. 2-Printing the 3D mask: The main step is to establish a translucent mask that would allow the back projected light to be clearly visible when looked at from the front. The other important factor is to establish a mask that fits in its shape,</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f)</formula><p>the face model that would be projected on top of it (mentioned in the previous point). To establish this, a 3D copy of the exact face 3D model was printed using a 3D printer, with an equal overall thickness of 1mm. After sample testing, this thickness proved optimal to allow just enough light to be visible on the mask. Figure <ref type="figure" target="#fig_4">7a</ref> shows the original 3D computer model of the face. Figure <ref type="figure" target="#fig_4">7b</ref> shows the 3D design of the mask acquired by modifying the original face model, and making it suitable for 3D printing. Figure <ref type="figure" target="#fig_4">7c</ref> shows the mask after printing. The dimensions of the printed mask were made to resemble the size of an average human head (width 16cm, height 22cm, depth 13cm). 3-Allowing the mask to equally diffuse color: A main problem of back projecting light on translucent objects is that the light-source will be visible (glowing) when looked at from the front. This was an obvious problem when the printed face was used with a micro projector. To solve this problem, a back-projection paint, which is used to create back-projection screens, was used (goo systems Global<ref type="foot" target="#foot_0">1</ref> ). This spray paint is used specifically to allow the cured surface to diffuse the light 1 equally over its surface, and hence diminish the problem of unbalanced optical illumination over the mask. Figure <ref type="figure" target="#fig_4">7e</ref> shows the back-projected face after applying the back-projection paint on the plastic mask.</p><p>Fig. <ref type="figure">8</ref>. A front and back view of the mask and the rig of Furhat 4-Rigging the mask with a micro projector: When the mask was tested and proved ready to use as the back projection mask for the head, the mask then was rigged with a micro-projector that was placed on top of the mask, the projector then projects light onto a mirror that reflects back the face onto the mask. This approach allows for more distance between the projector and the mask, which in turn, allows for the projected image to be in focus and to fit the entire mask.</p><p>Figure <ref type="figure" target="#fig_4">7d</ref> shows how the head is rigged with a projector and a mirror. Figure <ref type="figure">8</ref> shows a front and back view of the head when the mask is rigged with the projector and a mirror, showing how the projected face fits exactly the 3D plastic mask (it is important to note here that the solution of using a mirror is probably replaceable by other alternatives such as using a fish-eye lens that widens the projection area of the projector). After the mask was rigged, the head was covered using a fur hat. The fur hat covers the projector and the rig, and hence gives a stronger focus on the facial appearance of Furhat. Figure <ref type="figure" target="#fig_5">9</ref> shows Furhat with and without its head cover. Direction of attention may of course not only be achieved with the eyes, but also by moving the head, using a neck. A neck allows the robot head to use either eye movement, head pose, or both, to direct the attention, but also to do gestures such as nodding. Depending on which behaviors need to be modeled, different degrees of freedom (DOF) may be necessary. To direct the gaze in any direction (if the eyes are centered), 2 DOF are obviously necessary, but in order to perform a wider range of gestures, more DOF may be needed. An example of a very flexible robot neck is presented in <ref type="bibr" target="#b21">[21]</ref>, where 3 DOF are used: lower and upper pitch (tilting up and down), yaw (panning side to side) and rolling (tilting side to side). Lower pitch is centered where the neck meets the shoulders, and high pitch is centered where the neck is attached to the head.</p><p>For Furhat, we are currently using a pan-tilt unit. The unit has a no-load speed of 0.162 sec/60° and a holding torque of 64 kg•cm. It has 2 DOF: pitch and yaw, which allows Furhat to direct the head in any direction, but also to do simple gestures such as nodding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Example Application</head><p>The development of Furhat is part of a European project called IURO (Interactive Urban Robot) <ref type="foot" target="#foot_1">2</ref> . As part of this project, we were invited to the EUNIC RobotVille Festival at the London Science Museum, December 1 st -December 4 th , 2011. The purpose of the IURO project is to develop robots that can obtain missing information from humans through multi-party dialogue. The central test-case will be an autonomous robot that can navigate in an urban environment by asking humans for directions. For the exhibition, we wanted to explore a similar problem, but to suit the setting we instead gave Furhat the task of asking the visitors about their beliefs of the future of robots, with the possibility of talking to two visitors at the same time and shifting attention between them.</p><p>In lab setups, we have been using Microsoft Kinect<ref type="foot" target="#foot_2">3</ref> , which includes a depth camera for visual tracking of people approaching Furhat and an array microphone for speech recognition. However, due to the crowded and noisy environment in the museum, we chose to use handheld close-range microphones and ultrasound proximity. For speech recognition, the Microsoft Speech API was used. For speech synthesis, we used the CereVoice William TTS from CereProc<ref type="foot" target="#foot_3">4</ref> . CereVoice reports the timing of the phonemes in the synthesized utterance, which was used for synchronization of the lip movements in the facial animation. It also contains a number of verbal gestures that were used to give Furhat a more human-like appearance, such as grunts, laughter and yawning.</p><p>To control Furhat's behavior, we used an event-driven system implemented in Java, inspired by Harel state-charts <ref type="bibr" target="#b22">[22]</ref> and the UML modeling language. This allowed the system to react to external sensory input (speech, proximity data) as well as self-monitoring data, and produce actions such as speech, facial gestures and head movements. The layered structure of the state-chart paradigm allows the dialogue designer to define a hierarchy of dialogue states, and the sensory-action paring that is associated with these states. For the exhibition scenario, the dialogue contained two major states reflecting different initiatives: one where Furhat had the initiative and asked questions to the visitors (i.e., "when do you think robots will beat humans in football?") and one where the visitors asked questions to Furhat (i.e., "where do you come from?"). In the former case, Furhat continued the dialogue (i.e., "why do you think so?"), even though he often understood very little of the actual answers, occasionally extracting important keywords.</p><p>With nobody close to the proximity sensors, Furhat was in an "idle" mode, looking down. As soon as somebody approached a proximity sensor, he looked up and initiated a dialogue with "Could you perhaps help me?". The multi-party setting allowed us to explore the use of head-pose and gaze during the dialogue:</p><p>• With two people standing in front of him, Furhat was able to switch interlocutor using first a rapid gaze movement and then head movement. Often Furhat used this possibility to move the dialogue forward, by switching interlocutor and asking a follow-up, such as "do you agree on that?" • Furhat could either ask a specific interlocutor, or direct the head between the interlocutors and pose an open question, moving the gaze back and forth between the interlocutors. By comparing the audio-level and timing of the audio input from the two microphones, Furhat could then choose who to attend and follow-up on.</p><p>• If Furhat asked a question specifically to one of the interlocutors, and the other person answered, he quickly used gaze to turn to this person saying "could you just wait a second", then shifted the gaze back and continued the dialogue.</p><p>To exploit the possibilities of facial gestures that the back-projection technique allows, certain sensory events were mapped to gesture actions in the state chart. For example, when the speech recognizer detected a start of speech, the eyebrows were raised to signal that Furhat was paying attention.</p><p>. Fig. <ref type="figure" target="#fig_0">10</ref>. Furhat at the London Science Museum. The monitor shows the results of the visitors' answers to Furhat's questions. The two podiums with microphones and proximity sensors can also be seen.</p><p>In total, 7949 people visited the exhibition during the course of 4 days. The system proved to be very stable during the whole period. Apart from the video data, we recorded 8 hours of speech from the visitors. We also let the visitors fill out a questionnaire about their experience after the interaction. We have not yet analyzed the data, but it was apparent that many visitors liked the interaction and continued to answer Furhat's questions although he actually understood very little of their answers. The visitors also seemed to understand Furhat's attentive behavior and act accordingly. Videos from the exhibition can be seen at www.speech.kth.se/furhat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussions</head><p>One major motivation behind this work is to build a robot head that can use state-ofthe-art facial animation to communicate and interact with humans. These include natural and smooth lip movements, control of perceivable eye and gaze movements.</p><p>To make a robot head that is able to capitalize on social signals, its head should be able to generate such signals to highly perceivable accuracy. The first step towards reaching this goal was to use animated talking agents. However, since the robot is supposed to be able to engage in interactive multimodal dialogue with multiple people, the simple solution of using a computer screen as an interface with an animated agent projected onto it became disadvantageous. This is due to the fact that the 2D screen has no direction, and suffers from the Mona Lisa gaze effect (amongst other effects). This effect makes it impossible to establish, for example, exclusive eye-contact with one person out of many.</p><p>The solution to reach these goals, while avoiding the hindering effects of flat displays, is Furhat, a hybrid solution that can be thought of as bringing the animated face out of the screen and into the real-physical world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 11. Examples showing different instantiations of the colors of Furhat's facial features</head><p>Clearly, the benefits of using an animated agent as a robot head employing optical projection meets the goal of bringing the smooth and accurate animation of 3D computer models into a robot head. But there are more advantages. The flexibility of using a computer model allows for fast and online control of the face depending, for example, on context. Furhat for example can change its facial design on the fly since the colors and shape of its different facial parts is just a software animation (this manipulation is however limited by the design of the mask). Figure <ref type="figure" target="#fig_0">11</ref> shows examples of different facial colors of Furhat.</p><p>These and other parameters can be controlled depending on context and the environment, for example, Furhat can have a different facial design depending on the cultural background, or the age of the interlocutor. It can change its color contrasts depending on the surrounding light.</p><p>One expressive and environment-sensitive part of the face that can be controlled in this setup is the eyes. The pupil size for instance, can correspond to the amount of light in the surroundings <ref type="bibr" target="#b23">[23]</ref>, and can also reflect functions such as affect and interest.</p><p>Another context-aware property of the eyes is the corneal reflection. This is when the image of the environment is reflected on the cornea. This phenomenon has been shown to provide significant amount of information about the environment where the interaction is taking place <ref type="bibr" target="#b24">[24]</ref>.</p><p>These features can be easily implemented in Furhat on the software side by controlling the size and textures of the eyes and hence the projected image will more accurately reflect the situated context Furhat is interacting in.</p><p>Other benefits of Furhat to be used as a robotic head are its low weight, low maintenance demands, low noise level (only the noise coming from the neck), and its low energy consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this chapter, we have presented Furhat, an example of a paradigm for building robot heads using software based animated faces. Based on experimental evidence, this paradigm makes animated faces look more natural and human-like since it brings them out of the screen and onto a human-head-shaped three-dimensional physical object. This, not only makes animated faces look more natural in interactions, but also solves problems that arise when visualizing them onto flat displays. Such potential problems are achieving accurate multiparty interaction using gaze and head direction (since flat displays lack the enforcement of direction).</p><p>Looking at what Furhat has to offer to robotic heads, the advantages of using software design and animation instead of hardware (physical-mechanical) design and animation are numerous. Robot heads lack the ability to move their facial parts smoothly and accurately enough to simulate human facial movements (eye movements, blinking, eyebrows movement, and specially lip movements), let alone looking like human ones.</p><p>Furhat, on the other hand, uses an animated face that can move its facial parts online, in real-time, and to a large degree like humans do. In addition to movement, the design of the face is very flexible. The design of robot heads typically cannot change after manufacturing the head (the color and design of the lips and eyebrows, the color of the eyes, the size of the iris…), Furhat's colors and design, on the other hand, can easily change. This is achieved by using the animated face model it utilizes as its face, while still using the same face mask and hardware, and hence no mechanical or hardware cost is associated with this functionality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) Two examples of physical robot heads. (b) Two examples of computer animated facial models.</figDesc><graphic coords="3,48.29,145.07,345.72,108.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a)An example of a person wearing virtual reality (VR) glasses, so to be immersed in a virtual world. (b) An example of a virtual character that is presented via a flat display, offering a bridge into the physical and virtual realities. (c) An example of a holographic display of a person, to bring the virtual character into the physical space.</figDesc><graphic coords="4,46.85,55.55,345.72,125.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Schematic setup and placement of the subject and stimuli point</figDesc><graphic coords="6,129.77,55.55,181.32,141.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The subjective assessment of the 2D and 3D versions of the talking head, showing mean and standard errors</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. A chart showing the process for building Furhat, the back projected robotic head</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Snapshots of Furhat with and without the head cover (the fur hat)</figDesc><graphic coords="11,115.25,183.47,219.72,182.16" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.goosystemsglobal.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.iuro-project.eu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://kinectforwindows.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.cereproc.com/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>After we presented Furhat and how it was built in this paper, allowing for others to possibly replicate the process, we have presented a sample application that uses Furhat for multiparty interaction with human, which was presented at the London Science Museum for 4 days and received around 8000 visitors.</p><p>We would like to use Furhat not only as a natural interactive robot head, but also as a research framework which allows for studying human-human (one can think of Furhat as a tele-presence device) and human-robot interaction in single and multiparty setups and in turn-taking and dialogue management techniques using face and neck movements, to count a few.</p><p>Acknowledgments. This work has been done at the Department for Speech, Music and Hearing, and funded by the EU project IURO (Interactive Urban Robot) No. 248314. The authors would like to thank Simon Alexanderson for designing the 3D mask model for printing, and to thank Jens Edlund, Joakim Gustafson and Preben Wik for their interest and inspiring discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Dominik</surname></persName>
		</author>
		<ptr target="http://capek.misto.cz/english/robot.html" />
		<title level="m">Who did actually invent the word robotand what does it mean? The Karel Čapek website</title>
		<imprint>
			<date type="published" when="2011-12-10">retrieved December 10, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lipreading and audio-visual speech perception</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Summerfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">335</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="1273">1273. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effects of Visual Prominence Cues on Speech Intelligibility</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Auditory-Visual Speech Processing</title>
		<meeting>Auditory-Visual Speech Processing<address><addrLine>Norwich, England</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gaze and mutual gaze</title>
		<author>
			<persName><forename type="first">M</forename><surname>Argyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaze and eye contact: a research review</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Kleinke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="78" to="100" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unmasking the face: A guide to recognizing emotions from facial clues</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Malor Books</publisher>
			<biblScope unit="volume">ISBN</biblScope>
			<biblScope unit="page" from="978" to="1883536367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Differences in effect of robot and screen agent recommendations on human decision-making</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shinozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kogure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Human Computer Studies</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="279" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bukimi no tani.:The uncanny valley</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">F</forename><surname>Macdorman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Minato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans.). Energy</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="33" to="35" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
	<note>Originally in Japanese</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grace and George: Social Robots at AAAI</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gockley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busquets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Disalvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Caffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lauducci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bugajska</surname></persName>
		</author>
		<author>
			<persName><surname>Perzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2004, Mobile Robot Competition Workshop</title>
		<meeting>AAAI 2004, Mobile Robot Competition Workshop</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Mona Lisa Gaze Effect as an Objective Metric for Perceived Cospatiality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IVA 2011. LNCS (LNAI)</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Vilhjálmsson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Kopp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Marsella</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Thórisson</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6895</biblScope>
			<biblScope unit="page" from="439" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Geometrical basis of perception of gaze direction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Todorovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3549" to="3562" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shader lamps: animating real objects with image-based illumination</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 12th Eurographics Workshop on Rendering Techniques</title>
		<meeting>of the 12th Eurographics Workshop on Rendering Techniques</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="89" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Animatronic shader lamps avatars</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lincoln</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nashel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ilie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>State</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality</title>
		<meeting>of the 2009 8th IEEE International Symposium on Mixed and Augmented Reality<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Taming Mona Lisa: Communicating gaze faithfully in 2D and 3D facial projections</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Article 11, 25 pages</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Turn-taking Control Using Gaze in Multiparty Human-Computer Dialogue: Effects of 2D and 3D Displays</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Skantze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Auditory-Visual Speech Processing AVSP</title>
		<meeting>the International Conference on Auditory-Visual Speech Processing AVSP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Animated Faces for Robotic Heads: Gaze and Beyond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication and Enactment 2010</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Esposito</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Vicsi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Pelachaud</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Nijholt</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">6800</biblScope>
			<biblScope unit="page" from="19" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Talking heads -Models and applications for multimodal speech synthesis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Doctoral dissertation</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>KTH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Animation of talking agents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc of ESCA Workshop on Audio-Visual Speech Processing</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Benoit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Campbel</surname></persName>
		</editor>
		<meeting>of ESCA Workshop on Audio-Visual Speech essing<address><addrLine>Rhodes, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="149" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling and evaluating verbal and non-verbal communication in talking animated interface agents</title>
		<author>
			<persName><forename type="first">B</forename><surname>Granström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>House</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evaluation of Text and Speech Systems</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Dybkjaer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Hemsen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Minker</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="65" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Auditory-Visual Prominence: From Intelligibility to Behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Al Moubayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Granström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="299" to="311" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mechatronic Design of a Fast and Long Range 4 Degrees of Freedom Humanoid Neck</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bennik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leideman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M J R</forename><surname>Soemers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramigioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of ICRA</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="579" />
			<date type="published" when="2009">2009</date>
			<pubPlace>Kobe, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statecharts: A visual formalism for complex systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science of Computer Programming</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="231" to="274" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pupil dilation: What does it measure</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hensel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sternthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advertising Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="15" to="18" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Corneal Imaging System: Environment from Eyes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-006-6274-9</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="40" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
