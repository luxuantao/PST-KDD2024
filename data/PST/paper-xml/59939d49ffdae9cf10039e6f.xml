<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>j.wang@cs.ucl.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<email>wnzhang@sjtu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Gong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Benyou</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
							<email>pzhang@tju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Dell</forename><surname>Zhang</surname></persName>
							<email>dell.z@ieee.org</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of London</orgName>
								<address>
									<settlement>Birkbeck</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3077136.3080786</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>is paper provides a uni ed account of two schools of thinking in information retrieval modelling: the generative retrieval focusing on predicting relevant documents given a query, and the discriminative retrieval focusing on predicting relevancy given a querydocument pair. We propose a game theoretical minimax game to iteratively optimise both models. On one hand, the discriminative model, aiming to mine signals from labelled and unlabelled data, provides guidance to train the generative model towards ing the underlying relevance distribution over documents given the query. On the other hand, the generative model, acting as an a acker to the current discriminative model, generates di cult examples for the discriminative model in an adversarial way by minimising its discrimination objective. With the competition between these two models, we show that the uni ed framework takes advantage of both schools of thinking: (i) the generative model learns to t the relevance distribution over documents via the signals from the discriminative model, and (ii) the discriminative model is able to exploit the unlabelled data selected by the generative model to achieve a be er estimation for document ranking. Our experimental results have demonstrated signi cant performance gains as much as 23.96% on Precision@5 and 15.50% on MAP over strong baselines in a variety of applications including web search, item recommendation, and question answering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A typical formulation of information retrieval (IR) is to provide a (rank) list of documents given a query. It has a wide range of applications from text retrieval <ref type="bibr" target="#b0">[1]</ref> and web search <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref> to recommender systems <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34]</ref>, question answering <ref type="bibr" target="#b8">[9]</ref>, and personalised advertising <ref type="bibr" target="#b26">[27]</ref>, to name just a few. ere are, arguably, two major schools of thinking when coming to IR theory and modelling <ref type="bibr" target="#b0">[1]</ref>.</p><p>e classic school of thinking is to assume that there is an underlying stochastic generative process between documents and information needs (clued by a query) <ref type="bibr" target="#b21">[22]</ref>. In text retrieval, the classic relevance model of information retrieval is focused on describing how a (relevant) document is generated from a given information need: q → d, where q is the query (e.g., keywords, user pro les, questions, depending on the speci c IR application), d is its corresponding document (e.g., textual documents, information items, answers), and the arrow indicates the direction of generation. Notable examples include Robertson and Sparck Jones's Binary Independence Model, of which each word token is independently generated to form a relevant document <ref type="bibr" target="#b34">[35]</ref>. Statistical language models of text retrieval consider a reverse generative process from a document to a query: d → q, typically generating query terms from a document (i.e., the query likelihood function) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b47">48]</ref>. In the related work of word embedding, word tokens are generated from their context words <ref type="bibr" target="#b27">[28]</ref>. In the application of recommender systems, we also see that a recommended target item (in the original document identi er space) can be generated/selected from known context items <ref type="bibr" target="#b1">[2]</ref>.</p><p>e modern school of thinking in IR recognises the strength of machine learning and shi s to a discriminative (classi cation) solution learned from labelled relevant judgements or their proxies such as clicks or ratings. It considers documents and queries jointly as features and predicts their relevancy or rank order labels from a large amount of training data: q +d → r , where r denotes relevance and symbol + denotes the combining of features. A signi cant development in web search is learning to rank (LTR) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>, a family of machine learning techniques where the training objective is to provide the right ranking order of a list of documents (or items) for a given query (or context) <ref type="bibr" target="#b23">[24]</ref>. ree major paradigms of learning to rank are pointwise, pairwise, and listwise. Pointwise methods learn to approximate the relevance estimation of each document to the human rating <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>. Pairwise methods aim to identify the more-relevant document from any document pair <ref type="bibr" target="#b2">[3]</ref>. Listwise methods learn to optimise the (smoothed) loss function de ned over the whole ranking list for each query <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>. Besides, a recent advance in recommender systems is matrix factorisation, where the interactive pa erns of user features and item features are exploited via vector inner product to make the prediction of relevancy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>While the generative models of information retrieval are theoretically sound and very successful in modelling features (e.g., text statistics, distribution over document identi er space), they su er from the di culty in leveraging relevancy signals from other channels such as links, clicks etc., which are largely observable in Internet-based applications. While the discriminative models of information retrieval such as learning to rank are able to learn Session 5A: Retrieval Models and Ranking 3 SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan a retrieval ranking function implicitly from a large amount of labelled/unlabelled data, they currently lack a principled way of obtaining useful features or gathering helpful signals from the massive unlabelled data available, in particular, from text statistics (derived from both documents and queries) or the distribution of relevant documents in the collection.</p><p>In this paper, we consider the generative and discriminative retrieval models as two sides of the same coin. Inspired by Generative Adversarial Nets (GANs) in machine learning <ref type="bibr" target="#b12">[13]</ref>, we propose a game theoretical minimax game to combine the above mentioned two schools of thinking. Speci cally, we de ne a common retrieval function (e.g., discrimination-based objective function) for both models. On one hand, the discriminative model p ϕ (r |q, d) aims to maximise the objective function by learning from labelled data. It naturally provides alternative guidance to the generative retrieval model beyond traditional log-likelihood. On the other hand, the generative retrieval model p θ (d |q, r ) acts as a challenger who constantly pushes the discriminator to its limit. Iteratively it provides the most di cult cases for the discriminator to retrain itself by adversarially minimising the objective function. In such a way, the two types of IR models act as two players in a minimax game, and each of them strikes to improve itself to 'beat' the other one at every round of this competition. Note that our minimax game based approach is fundamentally di erent from the existing game-theoretic IR methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b46">47]</ref>, in the sense that the existing approaches generally try to model the interaction between user and system, whereas our approach aims to unify generative and discriminative IR models.</p><p>Empirically, we have realised the proposed minimax retrieval framework in three typical IR applications: web search, item recommendation, and question answering. In our experiments, we found that the minimax game arrives at di erent equilibria and thus di erent e ects of uni cation in di erent se ings. With the pointwise adversarial training, the generative retrieval model can be signi cantly boosted by the training rewards from the discriminative retrieval model. e resulting model outperforms several strong baselines by 22.56% in web search and 14.38% in item recommendation on Precesion@5. We also found that with new pairwise adversarial training, the discriminative retrieval model is largely boosted by examples selected by the generative retrieval model and outperforms the compared strong algorithms by 23.96% on Precision@5 in web search and 2.38% on Precision@1 in question answering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">IRGAN FORMULATION</head><p>In this section, we take the inspiration from GANs and build a uni ed framework for fusing generative and discriminative IR in an adversarial se ing; we call it IRGAN, and its application to concrete IR problems will be given in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">A Minimax Retrieval Framework</head><p>Without loss of generality, let us consider the following information retrieval problem. We have a set of queries {q 1 , ..., q N } and a set of documents {d 1 , ..., d M }. In a general se ing, a query is any speci c form of the user's information need such as search keywords, a user pro le, or a question, while documents could be textual documents, information items, or answers, depending on the speci c retrieval task. For a given query q n , we have a set of relevant documents labelled, the size of which is much smaller than the total number of documents M. e underlying true relevance distribution can be expressed as conditional probability p true (d |q, r ), which depicts the (user's) relevance preference distribution over the candidate documents with respect to her submi ed query. Given a set of samples from p true (d |q, r ) observed as the training data, we can try to construct two types of IR models:</p><p>Generative retrieval model p θ (d |q, r ), which tries to generate (or select) relevant documents, from the candidate pool for the given query q, as speci ed later in Eq. ( <ref type="formula" target="#formula_16">8</ref>); in other words, its goal is to approximate the true relevance distribution over documents p true (d |q, r ) as much as possible. Discriminative retrieval model f ϕ (q, d), which, in contrary, tries to discriminate well-matched query-document tuples (q, d) from ill-matched ones, where the goodness of matching given by f ϕ (q, d) depends on the relevance of d to q; in other words, its goal is to distinguish between relevant documents and nonrelevant documents for the query q as accurately as possible.</p><p>It is in fact simply a binary classi er, and we could use 1 as the class label for the query-document tuples that truly match (positive examples) while 0 as the class label for those that do not really match (negative examples).</p><p>2.1.1 Overall Objective. us, inspired by the idea of GAN, we aim to unify these two di erent types of IR models by le ing them play a minimax game: the generative retrieval model would try to generate (or select) relevant documents that look like the groundtruth relevant documents and therefore could fool the discriminative retrieval model, whereas the discriminative retrieval model would try to draw a clear distinction between the ground-truth relevant documents and the generated ones made by its opponent generative retrieval model. Formally, we have:</p><formula xml:id="formula_0">G * , D * = min θ max ϕ N n=1 E d∼p true (d |q n ,r ) [log D(d |q n )] +<label>(1)</label></formula><formula xml:id="formula_1">E d∼p θ (d |q n ,r ) [log(1 − D(d |q n ))] ,</formula><p>where the generative retrieval model G is wri en as p θ (d |q n , r ), directly and the discriminative retrieval D estimates the probability of document d being relevant to query q, which is given by the sigmoid function of the discriminator score</p><formula xml:id="formula_2">D(d |q) = σ (f ϕ (d, q)) = exp(f ϕ (d, q)) 1 + exp(f ϕ (d, q)) . (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>Let us leave the speci c parametrisation of f ϕ (d, q) to the next section when we discuss three speci c IR tasks. From Eq. (1), we can see that the optimal parameters of the generative retrieval model and the discriminative retrieval model can be learned iteratively by maximising and minimising the same objective function, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Optimising Discriminative</head><p>Retrieval. e objective for the discriminator is to maximise the log-likelihood of correctly distinguishing the true and generated relevant documents. With the observed relevant documents, and the ones sampled from the current optimal generative model p θ * (d |q, r ), one can then obtain the Session 5A: Retrieval Models and Ranking 3 SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan optimal parameters for the discriminative retrieval model:</p><formula xml:id="formula_4">ϕ * = arg max ϕ N n=1 E d∼p true (d |q n ,r ) log(σ (f ϕ (d, q n )) + E d∼p θ * (d |q n ,r ) log(1 − σ (f ϕ (d, q n ))) ,<label>(3)</label></formula><p>where if the function f ϕ is di erentiable with respect to ϕ, the above is solved typically by stochastic gradient descent.</p><p>2.1.3 Optimising Generative Retrieval. By contrast, the generative retrieval model p θ (d |q, r ) intends to minimise the objective; it ts the underlying relevance distribution over documents p true (d |q, r ) and based on that, randomly samples documents from the whole document set in order to fool the discriminative retrieval model.</p><p>It is worth mentioning that unlike GAN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref>, we design the generative model to directly generate known documents (in the document identi er space) not their features, because our work here intends to select relevant documents from a given document pool. Note that it is feasible to generate new documents (features, such as the value of BM25) by IRGAN, but to stay focused, we leave it for future investigation.</p><p>Speci cally, while keeping the discriminator f ϕ (q, d) xed after its maximisation in Eq. ( <ref type="formula" target="#formula_0">1</ref>), we learn the generative model via performing its minimisation:</p><formula xml:id="formula_5">θ * = arg min θ N n=1 E d∼p true (d |q n ,r ) log σ (f ϕ (d, q n )) + E d∼p θ (d |q n ,r ) log(1 − σ (f ϕ (d, q n ))) = arg max θ N n=1 E d∼p θ (d |q n ,r ) log(1 + exp(f ϕ (d, q n ))) denoted as G (q n ) ,<label>(4)</label></formula><p>where for each query q n we denote the objective function of the generator as G (q n )<ref type="foot" target="#foot_0">1</ref> .</p><p>As the sampling of d is discrete, it cannot be directly optimised by gradient descent as in the original GAN formulation. A common approach is to use policy gradient based reinforcement learning (REINFORCE) <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref>. Its gradient is derived as follows:</p><formula xml:id="formula_6">∇ θ G (q n ) = ∇ θ E d∼p θ (d |q n ,r ) log(1 + exp(f ϕ (d, q n ))) = M i=1 ∇ θ p θ (d i |q n , r ) log(1 + exp(f ϕ (d i , q n ))) = M i=1 p θ (d i |q n , r )∇ θ log p θ (d i |q n , r ) log(1 + exp(f ϕ (d i , q n ))) = E d∼p θ (d |q n ,r ) ∇ θ log p θ (d |q n , r ) log(1 + exp(f ϕ (d, q n ))) 1 K K k =1 ∇ θ log p θ (d k |q n , r ) log(1 + exp(f ϕ (d k , q n ))) ,<label>(5)</label></formula><p>where for g-steps do end for 12: until IRGAN converges the term log(1 + exp(f ϕ (d, q n ))) acts as the reward for the policy p θ (d |q n , r ) taking an action d in the environment q n <ref type="bibr" target="#b37">[38]</ref>.</p><p>In order to reduce variance during the REINFORCE learning, we also replace the reward term log(1+exp(f ϕ (d, q n ))) by its advantage function:</p><formula xml:id="formula_7">log(1 + exp(f ϕ (d, q n ))) − E d∼p θ (d |q n ,r ) log(1 + exp(f ϕ (d, q n ))) ,</formula><p>where the term</p><formula xml:id="formula_8">E d∼p θ (d |q n ,r ) log(1 + exp(f ϕ (d, q n ))</formula><p>) acts as the baseline function in policy gradient <ref type="bibr" target="#b37">[38]</ref>.</p><p>e overall logic of our proposed IRGAN solution is summarised in Algorithm 1. Before the adversarial training, the generator and discriminator can be initialised by their conventional models. en during the adversarial training stage, the generator and discriminator are trained alternatively via Eqs. ( <ref type="formula" target="#formula_6">5</ref>) and (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Extension to Pairwise Case</head><p>In many IR problems, it is common that the labelled training data available for learning to rank are not a set of relevant documents but a set of ordered document pairs for each query, as it is o en easier to capture users' relative preference judgements on a pair of documents than their absolute relevance judgements on individual documents (e.g., from a search engine's click-through log) <ref type="bibr" target="#b18">[19]</ref>. Furthermore, if we use graded relevance scales (indicating a varying degree of match between each document and the corresponding query) rather than binary relevance, the training data could also be represented naturally as ordered document pairs.</p><p>Here we show that our proposed IRGAN framework would also work in such a pairwise se ing for learning to rank. For each query q n , we have a set of labelled document pairs</p><formula xml:id="formula_9">R n = { d i , d j |d i d j } where d i d j means that d i is more relevant to q n than d j .</formula><p>As in Section 2.1, we let p θ (d |q, r ) and f ϕ (q, d) denote the generative retrieval model and the discriminative retrieval model respectively.</p><p>e generator G would try to generate document pairs that are similar to those in R n , i.e., with the correct ranking. e discriminator D would try to distinguish such generated document pairs from those real document pairs. e probability that a document pair d u , d being correctly ranked can be estimated by the discriminative retrieval model through a sigmoid function:</p><formula xml:id="formula_10">D( d u , d |q) = σ (f ϕ (d u , q) − f ϕ (d , q)) = exp(f ϕ (d u , q) − f ϕ (d , q)) 1 + exp(f ϕ (d u , q) − f ϕ (d , q)) = 1 1 + exp(−z) ,<label>(6)</label></formula><p>Session 5A: Retrieval Models and Ranking 3 SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan</p><formula xml:id="formula_11">where z = f ϕ (d u , q) − f ϕ (d , q). Note that − log D( d u , d |q) = log(1 + exp(−z)</formula><p>) is exactly the pairwise ranking loss function used by the learning to rank algorithm RankNet <ref type="bibr" target="#b2">[3]</ref>. In addition to the logistic function log(1 + exp(−z)), it is possible to make use of other pairwise ranking loss functions <ref type="bibr" target="#b6">[7]</ref>, such as the hinge function (1 − z) + (as used in Ranking SVM <ref type="bibr" target="#b15">[16]</ref>) and the exponential function exp(−z) (as used in RankBoost <ref type="bibr" target="#b10">[11]</ref>), to de ne the probability</p><formula xml:id="formula_12">D( d u , d |q).</formula><p>If we use the standard cross entropy cost for this binary classi er as before, we have the following minimax game:</p><formula xml:id="formula_13">G * , D * = min θ max ϕ N n=1 E o∼p true (o |q n ) [log D(o|q n )] +<label>(7)</label></formula><formula xml:id="formula_14">E o ∼p θ (o |q n ) log(1 − D(o |q n )) ,</formula><p>where o = d u , d and o = d u , d are true and generated document pairs for query q n respectively. In practice, to generate a document pair through generator G, we rst pick a document pair d i , d j from R n , take the lower ranked document d j , and then pair it with a document d k selected from the unlabelled data to make a new document pair d k , d j . e underlying rationale is that we are more interested in identifying the documents similar to higher ranked document d i as such documents are more likely to be relevant to the query q n . e selection of the document d k is based on the criterion that d k should be more relevant than d j according to the current generative model p θ (d |q, r ). In other words, we would like to select d k from the whole document set to generate a document pair d k , d j which can imitate the document pair</p><formula xml:id="formula_15">d i , d j ∈ R n .</formula><p>Suppose that the generative model p θ (d |q, r ) is given by a somax function (which is indeed used throughout Section 3, as we shall see later)</p><formula xml:id="formula_16">p θ (d k |q, r ) = exp( θ (q, d k )) d exp( θ (q, d)) ,<label>(8)</label></formula><p>where θ (q, d) is a task-speci c real-valued function re ecting the chance of d being generated from q. e probability of choosing a particular document d k could then be given by another so max function:  <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> in the same fashion as we have explained in Section 2.1.</p><formula xml:id="formula_17">G( d k , d j |q) = p θ (o |q) = exp θ (d k , q) − θ (d j , q) d exp θ (d, q) − θ (d j , q) = exp ( θ (d k , q)) d exp ( θ (d, q)) = p θ (d k |q, r ) .<label>(9</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Discussion</head><p>It can be proved that when we know the true relevance distribution exactly, the above minimax game of IRGAN, both pointwise and pairwise, has a Nash equilibrium in which the generator perfectly ts the distribution of true relevant documents (i.e., p θ (d |q, r ) = in the pairwise case, is always 1 2 ) <ref type="bibr" target="#b12">[13]</ref>. However, in practice, the true distribution of relevant documents is unknown, and in such a situation, how the generative/discriminative retrieval models converge to achieve such an equilibrium is still an open problem in the current research literature <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In our empirical study of IRGAN, we have found that depending on the speci c task, the generative and discriminative retrieval models may reach di erent levels of performance; and at least one of them would be signicantly improved in comparison to the corresponding original model without adversarial training.</p><p>How do the discriminator and the generator help each other? For the positive documents, observed or not, their relevance scores given by the discriminator f ϕ (q, d) and the conditional probabilistic density p θ (d |q, r ) are likely to be somewhat positively correlated. In each epoch of training, the generator tries to generate samples close to the discriminator's decision boundary to confuse its training next round, while the discriminator tries to score down the generated samples. Since there exists positive correlations between the positive but unobserved (i.e., the true-positive) samples and (part of) the observed positive samples, the generator should be able to learn to push upwards these positive but unobserved samples faster than other samples with the signal from the discriminator.</p><p>To understand this process further, let us draw an analogy with a knocker kicking the oating soap in the water, as illustrated in Figure <ref type="figure" target="#fig_1">1</ref>.</p><p>ere exist linking lines (i.e. positive correlations) between the unobserved positive soaps to the observed positive soaps that keep oating on the water surface (i.e. decision boundary of the discriminator) permanently. e discriminator acts as the knocker that kicks down the oating-up soaps, while the generator acts as the water that selectively oats the soaps up to the water surface. Even if the generator cannot perfectly t the conditional data distribution, there could be still a dynamic equilibrium, which is obtained when the distribution of the positive and negative unobserved soaps get stable at di erent depth of the water. Since the unobserved positive soaps are linked to those observed positive soaps staying on the water surface, overall they should be able to reach higher positions than the (unobserved) negative soaps in the end.</p><p>Just like other GANs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44]</ref>, the complexity of IRGAN training highly depends on the number of GAN iterations, each of which is of linear complexity O(N KM) with respect to the number of candidate documents M. Such a complexity can largely be reduced to O(N K log M) by applying hierarchical so max <ref type="bibr" target="#b27">[28]</ref> in the sampling process of the generator.</p><p>Session 5A: Retrieval Models and Ranking 3 SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Links to Existing Work</head><p>Let us continue our discussion on related work started in Section 1 and make comparisons with existing techniques in a greater scope. <ref type="bibr" target="#b12">[13]</ref> were originally proposed to generate continuous data such as images. Our work is di erent in the following three aspects. First, the generative retrieval process is stochastic sampling over discrete data, i.e., the candidate documents, which is di erent from the deterministic generation based on the sampled noise signal in the original GAN. Speci cally, as shown in Eq. ( <ref type="formula" target="#formula_5">4</ref>), for each query q n , the objective of the generative retrieval model is to minimise the expectation of the reward signal from the discriminative retrieval over the generated document distribution, while in the original GAN, the reward signal is solely dependent on a single generated instance. Second, our learning process of the generative retrieval model is based on the REINFORCE algorithm, a stochastic policy gradient technique in the eld of reinforcement learning <ref type="bibr" target="#b43">[44]</ref>. In IRGAN, the generative retrieval model can be regarded as an actor which takes an action of selecting a candidate document in a given environment of the query; the discriminative retrieval model can be regarded as a critic which performs a judgement whether the query-document pair is relevant enough. ird, during training, the con ict between ground-truth documents and generated documents is quite common, because documents are discrete and the candidate set is nite, which departs from the continuous (in nite) space for images or the extremely huge discrete (nearly in nite) space for text sequences <ref type="bibr" target="#b43">[44]</ref>. Fourth, we also propose a pairwise discriminative objective, which is unique for IR problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Generative Adversarial Nets. Generative Adversarial Nets</head><p>Our work is also related to conditional GAN <ref type="bibr" target="#b28">[29]</ref> as our generative and discriminative models are both conditional on the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">MLE based Retrieval Models.</head><p>For unsupervised learning problems that estimate the data p.d.f. p(x) and supervised learning problems that estimate the conditional p.d.f. p( |x), maximum likelihood estimation (MLE) plays as the standard learning solution <ref type="bibr" target="#b29">[30]</ref>. In IR, MLE is also widely used as an estimation method for many relevance features or retrieval models <ref type="bibr" target="#b0">[1]</ref>, such as Term Frequency (TF), Mixture Model (MM) <ref type="bibr" target="#b48">[49]</ref>, and Probabilistic Latent Semantic Indexing (PLSI) <ref type="bibr" target="#b16">[17]</ref>. In this paper, we provide an alternative way of training and fusing retrieval models. First, the generative process is designed to t the underlying true conditional distribution p true (d |q, r ) via minimising the Jensen-Shannon divergence (as explained in <ref type="bibr" target="#b12">[13]</ref>). us, it is natural to leverage GAN to distil a generative retrieval model to t such an unknown conditional distribution using the observed user feedback data. Second, the uni ed training scheme of two schools of IR models o ers the potential of ge ing be er retrieval models, because (i) the generative retrieval adaptively provides di erent negative samples to the discriminative retrieval training, which is strategically diverse compared with static negative sampling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b33">34]</ref> or dynamic negative sampling using the discriminative retrieval model itself <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>; and (ii) the reward signal from the discriminative retrieval model provides strategic guidance on training the generative retrieval model, which is unavailable in traditional generative retrieval model training. From the generative retrieval's perspective, IRGAN is superior to traditional maximum likelihood estimation <ref type="bibr" target="#b17">[18]</ref>. From the discriminative retrieval's perspective, IRGAN is able to exploit unlabelled data to achieve the e ect of semi-supervised learning <ref type="bibr" target="#b35">[36]</ref>. e advantages of employing two models working together have received more and more a ention in recent research; one of the variations is dual learning <ref type="bibr" target="#b42">[43]</ref> proposed for two-agent co-learning in machine translation etc.</p><p>It is also worth comparing IRGAN with pseudo relevance feedback <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref>, where the top retrieved documents are selected to re ne the ranking result. e two techniques are quite di erent as (i) in pseudo relevance feedback the top retrieved documents are regarded as positive samples to train the ranker while in IRGAN the generator-picked documents are regarded as negative samples to train the ranker; (ii) in pseudo relevance feedback there is usually no further iterations while IRGAN involves many iterations of adversarial training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.3">Noise-Contrastive Estimation.</head><p>Our work is also related to noise-contrastive estimation (NCE) that aims to correctly distinguish the true data ( , x) ∼ p data ( |x) from known noise samples ( n , x) ∼ p noise ( n |x). NCE is proved to be equivalent with MLE when noise samples are abundant <ref type="bibr" target="#b14">[15]</ref>. With nite noise samples for contrastive learning, NCE is usually leveraged as an e cient approximation to MLE when the la er is ine cient, for example when the p.d.f is built by large-scale so max modelling.</p><p>Furthermore, self-contrastive estimation (SCE) <ref type="bibr" target="#b13">[14]</ref>, a special case of NCE when the noise is directly sampled from the current (or a very recent) version of the model. It is proved that the gradient of SCE matches that of MLE with no prerequisite of in nite noise samples, which is a very a ractive property of SCE learning. Dynamic negative item sampling <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51]</ref> in top-N item recommendation with implicit feedback turns out to be a practical use case of SCE, with speci c solution of e cient sampling strategies.</p><p>e emergence of GANs <ref type="bibr" target="#b12">[13]</ref>, including our proposed IRGAN, opens a door to learning generative and discriminative retrieval models simultaneously. Compared to NCE and SCE, the GAN paradigm enables two models to learn together in an adversarial fashion, i.e. the discriminator learns to distinguish the true samples from the generated (faked) ones while the generator learns to generate high-quality samples to fool the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPLICATIONS</head><p>In this section, we apply our IRGAN framework to three speci c IR scenarios: (i) web search with learning to rank, (ii) item recommendation, and (iii) question answering.</p><p>As formulated in Section 2, the generator's conditional distribution p θ (d i |q, r ) = exp( θ (q, d i ))/ d j exp( θ (q, d j )), i.e., Eq. ( <ref type="formula" target="#formula_16">8</ref>), fully depends on the scoring function θ (q, d). In the sampling stage, the temperature parameter τ is incorporated in Eq. ( <ref type="formula" target="#formula_16">8</ref>) as</p><formula xml:id="formula_18">p θ (d |q, r ) = exp ( θ (q, d)/τ ) j ∈I exp ( θ (q, d)/τ ) ,<label>(10)</label></formula><p>where a lower temperature would make the sampling focus more on top-ranked documents. A special case is when the temperature is set to 0, which implies that the entropy of the generator is 0. In this situation, the generator simply ranks the documents in descending order and selects the top ones. More detailed study of τ will be given in Section 4. e discriminator's ranking of documents, i.e., Eq. ( <ref type="formula" target="#formula_2">2</ref>) for the pointwise se ing and Eq. ( <ref type="formula" target="#formula_10">6</ref>) for the pairwise se ing, is fully determined by the scoring function f ϕ (q, d).</p><p>e implementation of these two scoring functions, θ (q, d) and f ϕ (q, d), are task-speci c. Although there could be various implementations of f ϕ (q, d) and θ (q, d) (e.g., f ϕ (q, d) is implemented as Session 5A: Retrieval Models and Ranking 3 SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan a three-layer neural work while θ (q, d) is implemented as a factorisation machine <ref type="bibr" target="#b32">[33]</ref>), to focus more on adversarial training, in this section we choose to implement them using the same function (with di erent sets of parameters)<ref type="foot" target="#foot_1">2</ref> : θ (q, d) = s θ (q, d) and f ϕ (q, d) = s ϕ (q, d) , <ref type="bibr" target="#b10">(11)</ref> and in the following subsections we will discuss the implementation of the relevance scoring function s(q, d) for those three chosen IR scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Web Search</head><p>Generally speaking, there are three types of loss functions designed for learning to rank in web search, namely, pointwise <ref type="bibr" target="#b30">[31]</ref>, pairwise <ref type="bibr" target="#b2">[3]</ref> and listwise <ref type="bibr" target="#b5">[6]</ref>. To our knowledge, the listwise approaches with a loss de ned on document pairs and a list-aware weight added on document pairs, e.g., LambdaRank <ref type="bibr" target="#b4">[5]</ref> and LambdaMART <ref type="bibr" target="#b3">[4]</ref>, o en can achieve the best performance across various learning to rank tasks. Despite the variety of ranking loss functions, almost every learning to rank solution boils down to a scoring function s(q, d).</p><p>In the web search scenario, each query-document pair (q, d) can be represented by a vector x q,d ∈ R k , where each dimension represents some statistical value of the query-document pair or either part of it, such as BM25, PageRank, TFIDF, language model score etc. We follow the work of RankNet <ref type="bibr" target="#b2">[3]</ref> to implement a two-layer neural network for the score function:</p><formula xml:id="formula_19">s(q, d) = w 2 tanh(W 1 x q,d + b 1 ) + w 0 ,<label>(12)</label></formula><p>where W 1 ∈ R l ×k is the fully-connected matrix for the rst layer, b 1 ∈ R l is the bias vector for the hidden layer, w 2 ∈ R l and w 0 are the weights for the output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Item Recommendation</head><p>Item recommendation is a popular data mining task that can be regarded as a generalised information retrieval problem, where the query is the user pro le constructed from their past item consumption. One of the most important methodologies for recommender systems is collaborative ltering which explores underlying useruser or item-item similarity and based on which performs personalised recommendations <ref type="bibr" target="#b40">[41]</ref>. In collaborative ltering, a widely adopted model is matrix factorisation <ref type="bibr" target="#b20">[21]</ref>, following which we de ne our scoring function for the preference of user u (i.e. the query) to item i (i.e. the document) as</p><formula xml:id="formula_20">s(u, i) = b i + v u v i , (<label>13</label></formula><formula xml:id="formula_21">)</formula><p>where b i is the bias term for item i, v u , v i ∈ R k are the latent vectors of user u and item i respectively de ned in a k-dimensional continuous space. Here we omit the global bias and the user bias as they are reduced in the task of top-N item recommendation for each user<ref type="foot" target="#foot_2">3</ref> . To keep our discussion unclu ered, we have chosen a basic matrix factorisation model to implement, and it would be straightforward to replace it with more sophisticated models such as factorisation machines <ref type="bibr" target="#b32">[33]</ref> or neural networks <ref type="bibr" target="#b7">[8]</ref>, whenever needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">estion Answering</head><p>In question answering (QA) tasks <ref type="bibr" target="#b8">[9]</ref>, a question q or an answer a is represented as a sequence of words. Typical QA solutions aim to understand the natural language question rst and then select/generate one or more answers which best match the question <ref type="bibr" target="#b8">[9]</ref>. Among various QA tasks, the document-based QA task cab be regarded as a ranking process based on the matching score between two pieces of texts (for question and answer, respectively) <ref type="bibr" target="#b8">[9]</ref>. Recently, end-to-end approaches to predicting the match of short text pairs have been proposed, by utilising neural networks, such as convolutional neural network (CNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b36">37]</ref> or long short-term memory neural network (LSTM) <ref type="bibr" target="#b39">[40]</ref>.</p><p>For any question-answer pair (q, a), we can de ne a relevance score. Speci cally, one can leverage a convolutional neural networks (CNN) to learn the representation of word sequences <ref type="bibr" target="#b19">[20]</ref>, where each word is embedded as a vector in R k . By aligning the word vectors, an l-word sentence can be considered as a matrix in R l ×k . en, a representation vector of the current sentence is obtained through a max-pooling-over-time strategy a er a convolution operation over the matrix of aligned embedding vectors, yielding v q and v a ∈ R z , where z is the number of convolutional kernels. e relevance score of such a question-answer pair can be de ned as their cosine similarity, i.e.,</p><formula xml:id="formula_22">s(q, a) = cos(v q , v a ) = v q v a |v q | • |v a | . (<label>14</label></formula><formula xml:id="formula_23">)</formula><p>With the sentence representation and the scoring function dened above, the question answering problem is transformed into a query-document scoring problem in IR <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We have conducted our experiments<ref type="foot" target="#foot_3">4</ref> corresponding to the three real-world applications of our proposed IRGAN as discussed, i.e., web search, item recommendation, and question answering. As each of the three applications has its own background and baseline algorithms, this section about experiments is split into three selfcontained subsections. We rst test both the IRGAN-pointwise and IRGAN-pairwise formulations within a single task, web search; and then IRGAN-pointwise is further investigated in the item recommendation task where the rank bias is less critical, while IRGANpairwise is examined in the question answering task where the rank bias is more critical (usually only one answer is correct).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Web Search</head><p>4.1.1 Experiment Setup. Web search is an important problem in the IR eld. Here we make use of the well-known benchmark dataset LETOR (LEarning TO Rank) <ref type="bibr" target="#b24">[25]</ref> for webpage ranking to conduct our experiments.</p><p>Although standard learning to rank tasks assume explicit expert ratings for all training query-document pairs, implicit feedback from user interaction (such as the clicks information) is much more common in practical applications. is implies that we are usually faced with a relatively small amount of labelled data inferred from implicit feedback and a large amount of unlabelled data. In the unlabelled data, there could exist some hidden positive examples that have not been discovered yet. us, we choose to do experiments in a semi-supervised se ing on the MQ2008-semi (Million ery track) collection in LETOR 4.0: other than the labelled data (judged query-document pairs), this collection also contains a large amount of unlabelled data (unjudged query-document pairs), which can be e ectively exploited by our IRGAN framework. Each query-document pair in the dataset is given a relevance level (−1, 0, 1 or 2). e higher the relevance level, the more relevant the query-document pair, except that −1 means "unknown". Each query-document pair is represented by a 46-dimensional vector of features (such as BM25 and LMIR). To evaluate our proposed IRGAN in the context of implicit feedback, we consider all the query-document pairs with relevance level higher than 0 as positive examples, and all the other query-document pairs (with relevance level −1 or 0) as unlabelled examples. According to our statistics, there are 784 unique queries in this dataset; on average each query is associated with about 5 positive documents and about 1,000 unlabelled documents. To construct the training and test sets, we perform a 4:1 random spli ing. Both pointwise and pairwise IRGANs are evaluated based on this dataset.</p><p>Similar to RankNet <ref type="bibr" target="#b2">[3]</ref>, we adopt a neural network model with one hidden layer and tanh activation to learn the query-document matching score, where the size of the hidden layer equals to the size of features. Besides, both the generator and discriminator are trained from scratch.</p><p>In the experiments, we compare the generative retrieval model in our IRGAN framework with simple RankNet <ref type="bibr" target="#b2">[3]</ref>, LambdaRank <ref type="bibr" target="#b4">[5]</ref>, and the strong baseline LambdaMART <ref type="bibr" target="#b3">[4]</ref> for which we use the RankLib 5 implementation. For the evaluation of those compared algorithms, we use standard ranking performance measures <ref type="bibr" target="#b6">[7]</ref> such as Precision@N, Normalised Discounted Cumulative Gain (NDCG@N), Mean Average Precision (MAP) and Mean Reciprocal Ranking (MRR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results and Discussions</head><p>. First, we provide the overall performance of all the compared learning to rank algorithms on the MQ2008-semi dataset in Table <ref type="table" target="#tab_2">1</ref>. In our IRGAN framework, we use the generative retrieval model to predict the distribution of the user preferred documents given a query and then carry out the ranking, which is identical to performing the so max sampling with the temperature parameter set very close to 0. From the experimental results we can see clear performance improvements brought by our IRGAN approach on all the metrics. Speci cally, IRGAN-pairwise works be er than IRGAN-pointwise on the metrics of Precision@3, NDCG@3 that focus on a few webpages at the very top of the ranked list, whereas IRGAN-pointwise performs be er than IRGAN-pairwise on the metrics of Precision@10, NDCG@10 and MAP that take into account more webpages high in the ranked list. A possible explanation is that IRGANpointwise is targeted for the conditional distribution p true (d |q, r ) which only concerns whether an individual document is relevant to the query, whereas IRGAN-pairwise cares about the whole ranking of the documents given the query.</p><p>It is worth mentioning that the dataset studied in our experiments comes with implicit feedback, which is common in real life applications including web search and online advertising. Traditional learning to rank methods like LambdaMART are not particularly e ective in this type of semi-supervised se ing, which may be due to its reliance on the ∆NDCG scoring for each document pair <ref type="bibr" target="#b4">[5]</ref>. 5 h ps://sourceforge.net/p/lemur/wiki/RankLib/  Moreover, since adversarial training is widely regarded as an e ective but unstable technique, we further investigate the learning trend of our proposed approach. Figures <ref type="figure" target="#fig_3">2 and 3</ref> show the typical learning curves of the generative/discriminative retrieval models in IRGAN-pointwise and IRGAN-pairwise respectively. Here we only show the performance measured by Precision@5 and NDCG@5 for discussion; the other metrics exhibit a similar trend. We can observe that a er about 150 epoches for IRGAN-pointwise and 60 epoches for IRGAN-pairwise of adversarial training, both Precision@5 and NDCG@5 converge and the winner player consistently outperforms the best baseline LambdaRank.</p><p>Figure <ref type="figure" target="#fig_4">4</ref> shows how the ranking performance varies over the temperature parameter in Eq. ( <ref type="formula" target="#formula_18">10</ref>) used by the generative retrieval model to sample negative query-document pairs for the discriminative retrieval model. We nd the empirically optimal sampling temperature to be 0.2. e ranking performance increases when the temperature is tuned from 0 to the optimal value and then drops down a erwards, which indicates that properly increasing the aggressiveness (i.e. the tendency to focus on the top-ranked documents) of the generative retrieval model is important.</p><p>Furthermore, we study the impact of the model complexity of f ϕ (q, d) and θ (q, d) upon the interplay between them. In Figure <ref type="figure" target="#fig_5">5</ref> we have compared di erent combinations of generative and discriminative model implementations (i.e., linear model and two-layer NN) under IRGAN-pointwise and IRGAN-pairwise, respectively. We observe that (i) for IRGAN-pointwise, the NN implemented generator     <ref type="table" target="#tab_3">2</ref>. Following the experimental se ing of <ref type="bibr" target="#b50">[51]</ref>, we regard the 5-star ratings in Net ix and both 4-star and 5-star ratings in Movielens as positive feedback and treat all other entries as unknown feedback, because we mainly focus on the implicit feedbacks problem. For training and test data spli ing, we apply a 4:1 random spli ing on both datasets as in <ref type="bibr" target="#b50">[51]</ref>. e factor numbers for matrix factorisation are 5 and 16 for Movielens and Net ix respectively. Speci cally, to help train the discriminative retrieval model, the generative retrieval model is leveraged to sample negative items (in the same number of positive items) for each user via Eq. ( <ref type="formula" target="#formula_18">10</ref>) with the temperature parameter set to 0.2, which to some extent pushes the item sampling to the top ones. en the training of the discriminative retrieval model is dictated by Eq. (3). On the other side of the game, the training of the generative retrieval model is performed by REINFORCE as in Eq. ( <ref type="formula" target="#formula_6">5</ref>), which is normally implemented by the policy gradient on the sampled K items from p θ (d |q n , r ). In such a case, if the item set size is huge (e.g., more than 10 4 ) compared with K, it is more practical to leverage importance sampling to force the generative retrieval model to sample (some) positive examples d ∈ R n , so that the positive reward can be observed from REINFORCE and the generative retrieval model can be learned properly.</p><p>In the experiments, we compare IRGAN with Bayesian Personalised Ranking (BPR) <ref type="bibr" target="#b33">[34]</ref> and a state-of-the-art LambdaRank based collaborative ltering (LambdaFM) <ref type="bibr" target="#b45">[46]</ref> for top-N item recommendation tasks <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51]</ref>. Similar to the web search task, the performance measures are Precision@N, NDCG@N, MAP and MRR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results and Discussion</head><p>. First, the overall performance of the compared approaches on the two datasets is shown in Tables <ref type="table" target="#tab_5">3  and 4</ref>. From the experimental results, we can observe that IR-GAN achieves statistically signi cant improvements across all the evaluation metrics and all the datasets. Note that the generative retrieval model in IRGAN does not explicitly learn to optimise the nal ranking measures like what LambdaFM does, it still performs consistently be er than LambdaFM. Our explanation is that the adversarial training provides both models a higher learning exibility than the single-model training of LambdaFM or BPR.  We further investigate the learning trend of the proposed approach. e learning curves are shown in Figure <ref type="figure" target="#fig_6">6</ref> for Precision@5 and NDCG@5.</p><p>e experimental results demonstrate a reliable training process where IRGAN owns a consistent superiority over the baseline LambdaFM from the beginning of adversarial training. As for this case the curves are not as stable as those in web search (Figure <ref type="figure" target="#fig_3">3</ref>), one can adopt the early stopping strategy based on a validation set.</p><p>In addition, as shown in Figure <ref type="figure" target="#fig_7">7</ref>, we also investigate how the performance varies w.r.t. the sampling temperature in Eq. <ref type="bibr" target="#b9">(10)</ref>, which is consistent with our observations in the web search task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">estion Answering</head><p>4.3.1 Experiment Setup. InsuranceQA <ref type="bibr" target="#b9">[10]</ref> is one of the most studied question-answering dataset. Its questions are submi ed from real users and the high-quality answers are composed by professionals with good domain knowledge. So the candidate answers are usually randomly sampled from the whole answers pool (whereas other QA datasets may have a small-size xed candidate answers for each single question). InsuranceQA is suitable for testing our sampling/generating strategy. ere are a training set, a development set, and two test sets (test-1 and test-2) in the published corpus. 12,887 questions are included in the training set with correct answers, while the development set have 1,000 unseen question-answer pairs and the two test sets consist of 1,800 pairs. e system is expected to nd the one and only real answer from 500 candidate answers under the Precision@1 metric. As we have found from the web search task that IRGAN-pairwise works be er for top-ranked documents, we concentrate on the former in the QA task experiments.</p><p>To focus on evaluating the e ectiveness of IRGAN, we use a simple convolutional layer on the basic embedding matrix of a question sentence or an answer sentence. A representation vector of the current sentence is distilled from a max-pooling strategy  a er convolution <ref type="bibr" target="#b19">[20]</ref>, yielding v q and v a in Eq. ( <ref type="formula" target="#formula_22">14</ref>). e matching probability of such a question-answer pair is given by the cosine distance, which is similar to the basic QA-CNN model <ref type="bibr" target="#b8">[9]</ref>. In detail, the embedding of each word is initialised as a 100dimension random vector. In the convolutional layer, the window size of the convolution kernel is set to <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5)</ref>. A er the convolutional layer, the max-pooling-over-time strategy is adopted <ref type="bibr" target="#b19">[20]</ref>, where each feature map will be pooled as a scalar since its convolution kernel width is the same as the embedding vector. e performance on the test set is calculated by the model in the epoch with the best performance evaluated on the development set. Our IRGAN solution would load pre-trained models as the initial parameters for both generator and discriminator. A question with multiple answers are considered as multiple questions each with a single corresponding answer, which means that for each questionanswer pair only the feeding positive answer is observed by the current discriminator but the other positive answers are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results and Discussion</head><p>. As shown in Table <ref type="table" target="#tab_6">5</ref>, IRGAN outperforms both the basic CNN model with a random sampling strategy (QA-CNN) and the enhanced CNN model with a dynamic negative sampling strategy (LambdaCNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b50">51]</ref>. e learning curves of the two models are shown in Figure <ref type="figure" target="#fig_8">8</ref>, which is evaluated on the test-1 set. e performance of the discriminative retrieval model in IRGAN-pairwise is be er than LambdaCNN while the generative retrieval model tends to perform less e ectively during the pairwise adversarial training. A reason for the worse generator could be the sparsity of the answers distribution, i.e., each question usually has only one correct answer and many more weak negative answers. Due to such a sparsity, the generator may fail to get a positive feedback from the discriminator. An inspection of the sampled answers from LambdaCNN and IRGAN has revealed that about 1/3 of their samples are di erent.</p><p>is suggests the e ectiveness of independently modelling the negative generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we have proposed the IRGAN framework that uni es two schools of information retrieval methodologies, i.e., generative models and discriminative models, via adversarial training in a minimax game. Such an adversarial training framework takes advantages from both schools of methodologies: (i) the generative retrieval model is guided by the signals obtained from the discriminative retrieval model, which makes it more favourable than the non-learning methods or the maximum likelihood estimation scheme; (ii) the discriminative retrieval model could be enhanced to rank top documents be er via strategic negative sampling from the generator. Overall, IRGAN provides a more exible and principled training environment that combines these two kinds of retrieval models. Extensive experiments were conducted on four real-world datasets in three typical IR tasks, namely web search, item recommendation, and question answering. Signi cant performance gains were observed in each set of experiments.</p><p>Despite the great empirical success of GAN <ref type="bibr" target="#b12">[13]</ref>, there are still many questions with regard to its theoretical foundation remaining to be answered by the research community. For example, it is "not entirely clear" why GAN can generate sharper realistic images than alternative techniques <ref type="bibr" target="#b11">[12]</ref>. Our exploration of adversarial training for information retrieval in the proposed IRGAN framework has suggested that di erent equilibria could be reached in the end depending on the task and se ing. In the pointwise version of IRGAN, the generative retrieval model gets improved more than the discriminative retrieval model, but we have an opposite observation in the pairwise case. is phenomenon certainly warrants further investigation.</p><p>For future work, further experiments on more real-world datasets will be conducted. We also plan to extend our framework and test it over the generation of word tokens. One possible direction is to delve into the word weighting schemes such as <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48]</ref> learned from the IRGAN generative retrieval model and then derive new ranking features on that basis. Furthermore, the language models could be re-de ned along with GAN training, where new useful word pa erns might emerge.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 : 6 : 9 :</head><label>569</label><figDesc>p θ (d |q, r ) generates K documents for each query q Update generator parameters via policy gradient Eq. Use current p θ (d |q, r ) to generate negative examples and combine with given positive examples S 10: Train discriminator f ϕ (q, d ) by Eq. (3) 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of IRGAN training.p true (d |q, r ) in the pointwise case and p θ (o |q) = p true (o|q) in the pairwise case), while the discriminator cannot distinguish generated relevant documents from the true ones (i.e., the probability of d being relevant to q, D(d |q) in the pointwise case or D(o |q) in the pairwise case, is always1  2 )<ref type="bibr" target="#b12">[13]</ref>. However, in practice, the true distribution of relevant documents is unknown, and in such a situation, how the generative/discriminative retrieval models converge to achieve such an equilibrium is still an open problem in the current research literature<ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In our empirical study of IRGAN, we have found that depending on the speci c task, the generative and discriminative retrieval models may reach di erent levels of performance; and at least one of them would be signicantly improved in comparison to the corresponding original model without adversarial training.How do the discriminator and the generator help each other? For the positive documents, observed or not, their relevance scores given by the discriminator f ϕ (q, d) and the conditional probabilistic density p θ (d |q, r ) are likely to be somewhat positively correlated. In each epoch of training, the generator tries to generate samples close to the discriminator's decision boundary to confuse its training next round, while the discriminator tries to score down the generated samples. Since there exists positive correlations between the positive but unobserved (i.e., the true-positive) samples and (part of) the observed positive samples, the generator should be able to learn to push upwards these positive but unobserved samples faster than other samples with the signal from the discriminator.To understand this process further, let us draw an analogy with a knocker kicking the oating soap in the water, as illustrated in Figure1.ere exist linking lines (i.e. positive correlations) between the unobserved positive soaps to the observed positive soaps that keep oating on the water surface (i.e. decision boundary of the discriminator) permanently. e discriminator acts as the knocker that kicks down the oating-up soaps, while the generator acts as the water that selectively oats the soaps up to the water surface. Even if the generator cannot perfectly t the conditional data distribution, there could be still a dynamic equilibrium, which is obtained when the distribution of the positive and negative unobserved soaps get stable at di erent depth of the water. Since the unobserved positive soaps are linked to those observed positive soaps staying on the water surface, overall they should be able to reach higher positions than the (unobserved) negative soaps in the end.Just like other GANs<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b43">44]</ref>, the complexity of IRGAN training highly depends on the number of GAN iterations, each of which is of linear complexity O(N KM) with respect to the number of candidate documents M. Such a complexity can largely be reduced to O(N K log M) by applying hierarchical so max<ref type="bibr" target="#b27">[28]</ref> in the sampling process of the generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learning curves of the pointwise IRGAN on the web search task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves of the pairwise IRGAN on the web search task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Ranking performance with di erent sampling temperatures of pointwise IRGAN on the web search task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ranking performance for IRGAN with di erent generator and discriminator scoring functions.Table2: Characteristics of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Learning curve of precision and NDCG of the generative retrieval model for the top-5 item recommendation task on the Movielens dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ranking performance with di erent sampling temperatures on the Movielens dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: e experimental results in QA task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we perform a sampling approximation in the last step in which d k is the k-th document sampled from the current version of generator p θ (d |q n , r ). With reinforcement learning terminology, Initialise p θ (d |q, r ), f ϕ (q, d ) with random weights θ, ϕ.2: Pre-train p θ (d |q, r ), f ϕ (q, d ) using S</figDesc><table><row><cell cols="2">Algorithm 1 Minimax Game for IR (a.k.a IRGAN)</cell></row><row><cell>Input: generator p θ (d |q, r ); discriminator f ϕ (x training dataset S = {x }</cell><cell>q i );</cell></row><row><cell>1: 3: repeat</cell><cell></cell></row><row><cell>4:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>For example, one alternative way is to sample d k only from the documents more relevant to the query than d j , and let G( d k , d j |q) be directly proportional to max(p θ (d k |q, r ) − p θ (d j |q, r ), 0). is generative model p θ (d |q, r ) could be trained by the REIN-FORCE algorithm</figDesc><table /><note>)In this special case, G( d k , d j |q) happens to be equal to p θ (d k |q, r ), which is simple and reasonable. In general, the calculation of G( d k , d j |q) probably involves both p θ (d k |q, r ) and p θ (d j |q, r ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Webpage ranking performance comparison on the MQ2008-semi dataset, where * means a signi cant improvement according to the Wilcoxon signed-rank test.</figDesc><table><row><cell></cell><cell>P@3</cell><cell>P@5</cell><cell>P@10</cell><cell>MAP</cell></row><row><cell>MLE</cell><cell>0.1556</cell><cell>0.1295</cell><cell>0.1029</cell><cell>0.1604</cell></row><row><cell>RankNet [3]</cell><cell>0.1619</cell><cell>0.1219</cell><cell>0.1010</cell><cell>0.1517</cell></row><row><cell>LambdaRank [5]</cell><cell>0.1651</cell><cell>0.1352</cell><cell>0.1076</cell><cell>0.1658</cell></row><row><cell>LambdaMART [4]</cell><cell>0.1368</cell><cell>0.1026</cell><cell>0.0846</cell><cell>0.1288</cell></row><row><cell>IRGAN-pointwise</cell><cell>0.1714</cell><cell>0.1657</cell><cell>0.1257</cell><cell>0.1915</cell></row><row><cell>IRGAN-pairwise</cell><cell>0.2000</cell><cell>0.1676</cell><cell>0.1248</cell><cell>0.1816</cell></row><row><cell>Impv-pointwise Impv-pairwise</cell><cell>3.82% 21.14%  *</cell><cell>22.56%  *  23.96%  *</cell><cell>16.82%  *  15.98%</cell><cell>15.50%  *  9.53%</cell></row><row><cell></cell><cell cols="3">NDCG@3 NDCG@5 NDCG@10</cell><cell>MRR</cell></row><row><cell>MLE</cell><cell>0.1893</cell><cell>0.1854</cell><cell>0.2054</cell><cell>0.3194</cell></row><row><cell>RankNet [3]</cell><cell>0.1801</cell><cell>0.1709</cell><cell>0.1943</cell><cell>0.3062</cell></row><row><cell>LambdaRank [5]</cell><cell>0.1926</cell><cell>0.1920</cell><cell>0.2093</cell><cell>0.3242</cell></row><row><cell>LambdaMART [4]</cell><cell>0.1573</cell><cell>0.1456</cell><cell>0.1627</cell><cell>0.2696</cell></row><row><cell>IRGAN-pointwise</cell><cell>0.2065</cell><cell>0.2225</cell><cell>0.2483</cell><cell>0.3508</cell></row><row><cell>IRGAN-pairwise</cell><cell>0.2148</cell><cell>0.2154</cell><cell>0.2380</cell><cell>0.3322</cell></row><row><cell>Impv-pointwise</cell><cell>7.22%</cell><cell>15.89%</cell><cell>18.63%</cell><cell>8.20%</cell></row><row><cell>Impv-pairwise</cell><cell>11.53%</cell><cell>12.19%</cell><cell>13.71%</cell><cell>2.47%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Characteristics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell>Users</cell><cell>Items</cell><cell>Ratings</cell></row><row><cell>Movielens</cell><cell>943</cell><cell>1,683</cell><cell>100,000</cell></row><row><cell>Net ix</cell><cell cols="3">480,189 17,770 100,480,507</cell></row><row><cell cols="4">works be er than its linear version, while the NN implemented</cell></row><row><cell cols="4">discriminator may not o er a good guidance if the generator has</cell></row><row><cell cols="4">lower model complexity (i.e. linear); (ii) for IRGAN-pairwise, the</cell></row><row><cell cols="4">NN implemented discriminator outperforms its linear version. is</cell></row><row><cell cols="4">suggests that the model used for making the prediction (the gener-</cell></row><row><cell cols="4">ator in IRGAN-pointwise or the discriminator in IRGAN-pairwise)</cell></row><row><cell cols="4">should be implemented with a capacity not lower than its opponent.</cell></row><row><cell cols="3">4.2 Item Recommendation</cell><cell></cell></row><row><cell cols="4">4.2.1 Experiment Setup. We conduct our experiments on two</cell></row><row><cell cols="4">widely used collaborative ltering datasets: Movielens (100k) and</cell></row><row><cell cols="3">Net ix. eir details are shown in Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Item recommendation results (Movielens).</figDesc><table><row><cell></cell><cell>P@3</cell><cell>P@5</cell><cell>P@10</cell><cell>MAP</cell></row><row><cell>MLE</cell><cell>0.3369</cell><cell>0.3013</cell><cell>0.2559</cell><cell>0.2005</cell></row><row><cell>BPR [34]</cell><cell>0.3289</cell><cell>0.3044</cell><cell>0.2656</cell><cell>0.2009</cell></row><row><cell>LambdaFM [46]</cell><cell>0.3845</cell><cell>0.3474</cell><cell>0.2967</cell><cell>0.2222</cell></row><row><cell>IRGAN-pointwise</cell><cell>0.4072</cell><cell>0.3750</cell><cell>0.3140</cell><cell>0.2418</cell></row><row><cell>Impv-pointwise</cell><cell>5.90%  *</cell><cell>7.94%  *</cell><cell>5.83%  *</cell><cell>8.82%  *</cell></row><row><cell></cell><cell cols="4">NDCG@3 NDCG@5 NDCG@10 MRR</cell></row><row><cell>MLE</cell><cell>0.3461</cell><cell>0.3236</cell><cell>0.3017</cell><cell>0.5264</cell></row><row><cell>BPR [34]</cell><cell>0.3410</cell><cell>0.3245</cell><cell>0.3076</cell><cell>0.5290</cell></row><row><cell>LambdaFM [46]</cell><cell>0.3986</cell><cell>0.3749</cell><cell>0.3518</cell><cell>0.5797</cell></row><row><cell>IRGAN-pointwise</cell><cell>0.4222</cell><cell>0.4009</cell><cell>0.3723</cell><cell>0.6082</cell></row><row><cell>Impv-pointwise</cell><cell>5.92%  *</cell><cell>6.94%  *</cell><cell>5.83%  *</cell><cell>4.92%  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Item recommendation results (Net ix).</figDesc><table><row><cell></cell><cell>P@3</cell><cell>P@5</cell><cell>P@10</cell><cell>MAP</cell></row><row><cell>MLE</cell><cell>0.2941</cell><cell>0.2945</cell><cell>0.2777</cell><cell>0.0957</cell></row><row><cell>BPR [34]</cell><cell>0.3040</cell><cell>0.2933</cell><cell>0.2774</cell><cell>0.0935</cell></row><row><cell>LambdaFM [46]</cell><cell>0.3901</cell><cell>0.3790</cell><cell>0.3489</cell><cell>0.1672</cell></row><row><cell>IRGAN-pointwise</cell><cell>0.4456</cell><cell>0.4335</cell><cell>0.3923</cell><cell>0.1720</cell></row><row><cell>Impv-pointwise</cell><cell>14.23%  *</cell><cell>14.38%  *</cell><cell>12.44%  *</cell><cell>2.87%  *</cell></row><row><cell></cell><cell cols="4">NDCG@3 NDCG@5 NDCG@10 MRR</cell></row><row><cell>MLE</cell><cell>0.3032</cell><cell>0.3011</cell><cell>0.2878</cell><cell>0.5085</cell></row><row><cell>BPR [34]</cell><cell>0.3077</cell><cell>0.2993</cell><cell>0.2866</cell><cell>0.5040</cell></row><row><cell>LambdaFM [46]</cell><cell>0.3942</cell><cell>0.3854</cell><cell>0.3624</cell><cell>0.5857</cell></row><row><cell>IRGAN-pointwise</cell><cell>0.4498</cell><cell>0.4404</cell><cell>0.4097</cell><cell>0.6371</cell></row><row><cell>Impv-pointwise</cell><cell>14.10%  *</cell><cell>14.27%  *</cell><cell>13.05%  *</cell><cell>8.78%  *</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>e Precision@1 of InsuranceQA.</figDesc><table><row><cell></cell><cell>test-1</cell><cell>test-2</cell></row><row><cell>QA-CNN [9]</cell><cell cols="2">0.6133 0.5689</cell></row><row><cell cols="3">LambdaCNN [9, 51] 0.6294 0.6006</cell></row><row><cell>IRGAN-pairwise</cell><cell cols="2">0.6444 0.6111</cell></row><row><cell>Impv-pairwise</cell><cell>2.38%  *</cell><cell>1.75%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Following<ref type="bibr" target="#b12">[13]</ref>, E d ∼p θ (d |qn ,r ) [log(σ (f ϕ (d, q n )))] is normally used instead for maximisation, which keeps the same xed point but provides more su cient gradient for the generative model.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We will, however, conduct a dedicated experiment on the interplay between these two players using the scoring functions of di erent model complexity, in Section 4.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">e user bias could be taken as a good baseline function for the advantage function in policy gradient (Eq. (5)) to reduce the learning volatility<ref type="bibr" target="#b37">[38]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">e experiment code is provided at: h ps://github.com/geek-ai/irgan</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4">SIGIR'17, August 7-11, 2017, Shinjuku, Tokyo, Japan</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We thank Geek.AI for hosting this collaborative project. e work done by SJTU is nancially supported by NSFC (61632017) and Shanghai Sailing Program (17YF1428200).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<title level="m">Berthier Ribeiro-Neto, and others</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>Modern information retrieval</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Item2vec: neural item embedding for collaborative ltering</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MLSP Workshop</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to Rank Using Gradient Descent</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Renshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Lazier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Deeds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hullender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">From RankNet to LambdaRank to LambdaMART: An Overview</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learning</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to Rank with Nonsmooth Cost Functions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><surname>Ragno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Viet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ranking Measures and Loss Functions in Learning to Rank</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Ming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>In RecSys</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A entive Pooling Networks</title>
		<author>
			<persName><forename type="first">Cícero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applying deep learning to answer selection: A study and an open task</title>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidan</forename><surname>Michael R Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An E cient Boosting Algorithm for Combining Preferences</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">NIPS 2016 Tutorial: Generative Adversarial Networks</title>
				<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6515</idno>
		<title level="m">On Distinguishability Criteria for Estimating Generative Models</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large Margin Rank Boundaries for Ordinal Regression</title>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Large Margin Classi ers</title>
				<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic Latent Semantic Indexing</title>
		<author>
			<persName><surname>Omas Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05101</idno>
		<title level="m">How (not) to Train your Generative Model: Scheduled Sampling, Likelihood, Adversary?</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">Joachims</forename><surname>Orsten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<title level="m">Convolutional Neural Networks for Sentence Classi cation</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic Relevance Models Based on Document and ery Generation</title>
		<author>
			<persName><forename type="first">John</forename><surname>La</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language Modeling and Information Retrieval</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">McRank: Learning to Rank Using Multiple Classi cation and Gradient Boosting</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Pla</surname></persName>
		</author>
		<author>
			<persName><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><surname>Roweis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to rank for information retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LETOR: Benchmark Dataset for Research on Learning to Rank for Information Retrieval</title>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenying</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR 2007 Workshop on Learning to Rank for Information Retrieval</title>
				<meeting>SIGIR 2007 Workshop on Learning to Rank for Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Win-win search: dual-agent stochastic game in session search</title>
		<author>
			<persName><forename type="first">Jiyun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>H Brendan Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<meeting><address><addrLine>Todd Phillips, Eugene Davydov, Daniel Golovin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>and others</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional Generative Adversarial Nets</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tutorial on maximum likelihood estimation</title>
		<author>
			<persName><forename type="first">In</forename><surname>Jae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myung</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of mathematical Psychology</title>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discriminative Models for Information Retrieval</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A language modeling approach to information retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ponte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cro</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName><forename type="first">Rendle</forename><surname>Ste En</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Ste En Rendle</surname></persName>
		</author>
		<author>
			<persName><surname>Freudenthaler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>Zeno Gantner, and Lars Schmidtieme</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Relevance weighting of search terms</title>
		<author>
			<persName><forename type="first">K Sparck</forename><surname>Stephen E Robertson</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information science</title>
		<imprint>
			<date type="published" when="1976">1976. 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning to rank short text pairs with convolutional deep neural networks</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Moschi I</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>In SIGIR</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Policy Gradient Methods for Reinforcement Learning with Function Approximation</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Richard S Su On</surname></persName>
		</author>
		<author>
			<persName><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Yishay Mansour, and others</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularized estimation of mixture models for robust pseudo-relevance feedback</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="162" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Long Short-Term Memory Model for Answer Sentence Selection in estion Answering</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unifying user-based and item-based collaborative ltering approaches by similarity fusion</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjen P De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><forename type="middle">Jt</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dual Learning for Machine Translation</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improving pseudorelevance feedback in web information retrieval using web page segmentation</title>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lambdafm: learning optimal ranking with factorization machines using lambda surrogates</title>
		<author>
			<persName><forename type="first">Fajie</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joemon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards a game-theoretic framework for text data retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A study of smoothing methods for language models applied to information retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>La Erty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Model-based Feedback in the Language Modeling Approach to Information Retrieval</title>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>La Erty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A Distribution Separation Method Using Irrelevance Feedback Data for Information Retrieval</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<publisher>ACM TIST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Optimizing top-n collaborative ltering via dynamic negative item sampling</title>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
