<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
							<email>timo.schick@sulzer.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Sulzer GmbH Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Hinrich Sch ütze Center for Information and Language Processing LMU Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Some NLP tasks can be solved in a fully unsupervised fashion by providing a pretrained language model with "task descriptions" in natural language (e.g., <ref type="bibr" target="#b10">Radford et al., 2019)</ref>. While this approach underperforms its supervised counterpart, we show in this work that the two ideas can be combined: We introduce Pattern-Exploiting Training (PET), a semi-supervised training procedure that reformulates input examples as cloze-style phrases which help the language model understand the given task. Theses phrases are then used to assign soft labels to a large set of unlabeled examples. Finally, regular supervised training is performed on the resulting training set. On several tasks, we show that PET outperforms both supervised training and unsupervised approaches in lowresource settings by a large margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning from examples is the predominant approach for many natural language processing tasks: A model is trained on a large number of labeled examples, from which it is supposed to generalize to unseen data. Unfortunately, learning from examples alone is exceedingly difficult when there are only a few such examples. For instance, assume we are given the following pieces of text:</p><p>• T 1 = This was the best pizza I've ever had.</p><p>• T 2 = Pretty bad. You can get better sushi down the road for half the price.</p><p>• T 3 = Pizza was average. Not worth what they were asking.</p><p>Furthermore, imagine we are told that the labels of T 1 and T 2 are l 1 and l 2 , respectively, and we are asked to infer the correct label for T 3 . Based only on these three examples, this is impossible because plausible explanations can be found for both l 1 and l 2 . However, if we know that the underlying task is to identify whether the text says anything about prices, we can easily assign l 2 to T 3 . As this shows, solving a task for which we only have few examples becomes much easier when we also have a description that helps us understand the task.</p><p>With the rise of pretrained language models such as GPT <ref type="bibr" target="#b9">(Radford et al., 2018)</ref>, BERT <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b4">(Liu et al., 2019)</ref>, the idea of providing task descriptions has become feasible for neural architectures: We can simply append such descriptions in natural language to an input and let the language model predict continuations that solve the task <ref type="bibr" target="#b10">(Radford et al., 2019;</ref><ref type="bibr" target="#b8">Puri and Catanzaro, 2019)</ref>. So far, however, this idea has only been considered in zero-shot scenarios where no training data is available at all.</p><p>In this work, we show that providing task descriptions can successfully be combined with regular supervised learning: We introduce Patternexploiting Training (PET), a semi-supervised training procedure that uses natural language patterns to reformulate input examples into cloze-style phrases helping a language model to identify the task to be solved. PET works in three steps: First, for each pattern a separate language model is finetuned on a small training set. The ensemble of all models is then used to annotate a large unlabeled dataset with soft labels. Finally, a regular sequence classifier is trained on the soft-labeled dataset.</p><p>On three diverse NLP tasks, we show that using PET results in large improvements over both unsupervised approaches and regular supervised training when the number of available labeled examples is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Providing hints in natural language for zero-shot learning was first proposed by <ref type="bibr" target="#b10">Radford et al. (2019</ref><ref type="bibr">), arXiv:2001.07676v1 [cs.CL]</ref> 21 Jan 2020 who used this idea for challenging tasks such as reading comprehension, machine translation and question answering. Recently, <ref type="bibr" target="#b8">Puri and Catanzaro (2019)</ref> applied the same idea to text classification, providing actual task descriptions. <ref type="bibr" target="#b5">McCann et al. (2018)</ref> also present tasks in the form of natural language sentences as part of their Natural Language Decathlon; however, their motivation is not to enable few-shot learning, but to provide a unified framework for many different types of NLP tasks.</p><p>Much recent work uses cloze-style phrases to probe the knowledge that masked language models acquire during pretraining; this is typically also done without any task-specific finetuning. Probing tasks include analyzing factual and commonsense knowledge <ref type="bibr" target="#b7">(Petroni et al., 2019)</ref>, probing the understanding of rare words <ref type="bibr" target="#b11">(Schick and Schütze, 2019)</ref>, and investigating a language model's ability to perform symbolic reasoning <ref type="bibr" target="#b12">(Talmor et al., 2019)</ref>. Similar to our work, <ref type="bibr" target="#b3">Jiang et al. (2019)</ref> consider the problem of finding the best description for a given task, which is a key challenge for zero-shot approaches to work. However, all of this previous work only considers the usage of patterns for probing language models and not for improving downstream task performance when labeled data are sparse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pattern-Exploiting Training</head><p>Let M be a masked language model with vocabulary V and mask token ∈ V , and let L be a set of labels. We define a pattern to be a function P that takes as input a sequence of phrases x = (s 1 , . . . , s k ) with s i ∈ V * and outputs a single phrase P (x) ∈ V * that contains exactly one mask token, i.e., its output can be viewed as a cloze question. Furthermore, we define a verbalizer as an injective function v : L → V that maps each label to a word from M 's vocabulary. We refer to (P, v) as a pattern-verbalizer pair (PVP).</p><p>The underlying intuition for these definitions is as follows: Given an input x, we apply P to obtain an input representation P (x), which is then processed by M to identify the y ∈ L for which v(y) is the most likely candidate at the masked position. For example, consider the task of identifying whether two sentences a and b contradict each other (label y 0 ) or agree with each other (y 1 ). For this task, we may choose a pattern</p><formula xml:id="formula_0">P (a, b) = a? , b.</formula><p>combined with a verbalizer v that maps y 0 to "Yes" and y 1 to "No". Given an example input pair</p><p>x = (Mia likes pie, Mia hates pie), the task now changes from having to assign a label without inherent meaning to answering whether the most likely choice for the masked position in P (x) = Mia likes pie? , Mia hates pie.</p><p>is "Yes" or "No".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PVP Training and Inference</head><p>Let p = (P, v) be a PVP. We assume access to a small training set T and a (typically much larger) set of unlabeled examples D. For each sequence z ∈ V * that contains exactly one mask token and w ∈ V , we denote with M (w | z) the unnormalized score that the language model assigns to w at the masked position. Given some input x, we define the unnormalized score for each label y ∈ L as</p><formula xml:id="formula_1">s p (y | x) = M (v(y) | P (x))</formula><p>and obtain the corresponding probability distribution using standard softmax: y|x)   y ∈L e sp(y |x)</p><formula xml:id="formula_2">q p (y | x) = e sp(</formula><p>We use the cross-entropy between q p (y | x) and the true (one-hot) distribution of training example (x, y) -summed over all (x, y) ∈ T -as loss for finetuning M on p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Language Modeling</head><p>As a model finetuned on some PVP is still a language model at its core, regular language modeling suggests itself as an auxiliary task to prevent catastrophic forgetting, especially when only a few training examples are available. With L CE denoting cross-entropy loss and L MLM denoting language modeling loss, we compute the final loss as</p><formula xml:id="formula_3">L = (1 − α) • L CE + α • L MLM</formula><p>This idea was recently applied by <ref type="bibr" target="#b0">Chronopoulou et al. (2019)</ref>, who employ auxiliary language modeling in a data-rich scenario. As L MLM is typically much larger than L CE , in preliminary experiments, we found a small value of α = 10 −4 to consistently give good results, so we use it in all our experiments. To obtain sentences for language modeling, we use the unlabeled set D. However, we do not train directly on each x ∈ D, but rather on P (x), where we never ask the language model to predict anything for the masked slot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combining PVPs</head><p>A key challenge for a pattern-based approach in a low-resource scenario is that in the absence of a large development set, it is hard to identify which PVPs perform well. To overcome this problem, we resort to the following strategy, which closely resembles knowledge distillation <ref type="bibr" target="#b2">(Hinton et al., 2015)</ref>. First, we define a set P of patterns that intuitively make sense for a given task. We then use these patterns to automatically create a large soft-labeled dataset T as follows:</p><p>1. We finetune a separate language model M p for each p ∈ P. As we assume T to contain only a few examples, this finetuning is cheap even for a large number of PVPs.</p><p>2. We use the ensemble {M p | p ∈ P} of finetuned models to annotate examples from D with soft labels. We first combine the unnormalized class scores for each example x ∈ D as</p><formula xml:id="formula_4">s P (y | x) = 1 Z p∈P w(p) • s p (y | x)</formula><p>where Z = p∈P w(p) and w(p) is a weighing term for each PVP. We experiment with two different realizations of this weighing term: either we simply set w(p) = 1 for all p or we set w(p) to be the accuracy obtained using p on the training set before training. We refer to these two variants as uniform and weighted, respectively. An idea similar to our weighted variant was recently proposed by <ref type="bibr" target="#b3">Jiang et al. (2019)</ref> in a zero-shot setting.</p><p>3. We transform the above scores into a probability distribution q using softmax. Following <ref type="bibr" target="#b2">Hinton et al. (2015)</ref>, we use a temperature of T = 2 to obtain a suitably soft distribution. The pair (x, q) is added to our new (softlabeled) training set T .</p><p>Finally, we finetune a pretrained language model with a regular sequence classification head on T ; this model then serves as our final classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments Setup</head><p>We evaluate PET on three NLP datasets: Yelp Reviews, AG's News <ref type="bibr" target="#b16">(Zhang et al., 2015)</ref> and MNLI <ref type="bibr" target="#b13">(Williams et al., 2018)</ref>. For all of our experiments, we use RoBERTa large <ref type="bibr" target="#b4">(Liu et al., 2019)</ref> as language model. Our implementation is based on the Transformers library <ref type="bibr">(Wolf et al., 2019)</ref> and Py-Torch <ref type="bibr" target="#b6">(Paszke et al., 2017)</ref>. We investigate the performance of both regular supervised training and PET for training set sizes of t = 10, 50, 100, 1000. For each t, we obtain the training set T by choosing t examples evenly distributed across all labels. Similarly, we construct the set D of unlabeled examples by selecting 10, 000 examples per label and removing all labels.</p><p>As we consider a few-shot setting, we assume no access to a large development set on which hyperparameters could be optimized. Our choice of hyperparameters is thus based on choices made in previous work and practical considerations. We use a learning rate of 1 • 10 −5 because we found higher learning rates to often result in unstable training with no accuracy improvements even on the training set. For regular supervised training, we use a batch size of 16, a maximum sequence length of 256 and perform training for 250 steps. For PET, we subdivide each batch into 4 labeled examples from T to compute L CE and 12 unlabeled examples from D to compute L MLM . Accordingly, we multiply the number of total training steps by 4 (i.e., 1000), so that the number of times each labeled example is seen remains constant.</p><p>For training the final PET classifier, we use the same set of hyperparameters as for the individual PVP models, but we train for 5000 steps due to the increased training set size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patterns</head><p>We now describe the patterns and verbalizers used for all tasks. We use two vertical bars ( ) to mark boundaries between text segments. 1</p><p>Yelp For the Yelp Reviews Full Star dataset <ref type="bibr" target="#b16">(Zhang et al., 2015)</ref>, the task is to estimate the rating that a customer gave to a restaurant on a 1-1 The way different segments are handled depends on the language model being used; they may e.g. be assigned different segment embeddings <ref type="bibr" target="#b1">(Devlin et al., 2019)</ref>, be separated by special tokens <ref type="bibr" target="#b4">(Liu et al., 2019;</ref><ref type="bibr" target="#b15">Yang et al., 2019)</ref> or simply be ignored. For example, a b is given to BERT as the input to 5-star scale based on their review's text. We define the following patterns for an input text a:</p><formula xml:id="formula_5">"[CLS] a [SEP] b [SEP]".</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Examples</head><formula xml:id="formula_6">P 1 (a) = It was . a P 2 (a) = a.</formula><p>All in all, it was .</p><p>P 3 (a) = Just ! a P 4 (a) = a In summary, the restaurant is .</p><p>We define the verbalizer v as</p><formula xml:id="formula_7">v(1) = terrible v(2) = bad v(3) = okay v(4) = good v(5) = great</formula><p>resulting in a total of 4 PVPs for the Yelp dataset.</p><p>AG's News AG's News is a news classification dataset, where given a headline a and text body b, news have to be classified as belonging to one of the categories World (1), Sports (2), Business (3) or Science/Tech (4). For x = (a, b), we define the following patterns:</p><formula xml:id="formula_8">P 1 (x) = : a b P 2 (x) = a ( ) b P 3 (x) = -a b P 4 (x) = a b ( ) P 5 (x) = News: a b P 6 (x) = [ Category: ] a b</formula><p>We define the verbalizer v as</p><formula xml:id="formula_9">v(1) = World v(2) = Sports v(3) = Business v(4) = Tech</formula><p>which gives a total of 6 PVPs for AG's News.</p><p>MNLI The MNLI dataset <ref type="bibr" target="#b13">(Williams et al., 2018)</ref> consists of text pairs x = (a, b). The task is to find out whether a implies b (0), a and b contradict each other (1) or neither (2). For this task, we define two simple patterns</p><formula xml:id="formula_10">P 1 (x) = " a " ? , " b " P 2 (x) = a ? , b</formula><p>and consider two different verbalizers v 1 and v 2 that are defined as follows:</p><formula xml:id="formula_11">v 1 (0) = Wrong v 1 (1) = Right v 1 (2) = Maybe v 2 (0) = No v 2 (1) = Yes v 2 (2) = Maybe</formula><p>Combining the two patterns with the two verbalizers results in a total of 4 PVPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We perform training for each task and training set size three times using different random seeds and report mean accuracy and standard deviation across the three runs; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>Combining PVPs We investigate whether PET is able to cope with situations were some of the given PVPs perform much worse than others. Table 2 compares the performance of both the best and worst performing pattern to the performance of PET (for |T | = 10). We see that even after finetuning the gap between the best and worst pattern is large, especially for Yelp. However, PET is not only able to compensate for this, but even improves accuracies over using only the best-performing pattern across all tasks. We find no clear difference between the uniform and weighted variant of PET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Language Modeling</head><p>We analyze the influence of the auxiliary language modeling task on PET's performance. Figure <ref type="figure" target="#fig_0">1</ref> shows performance improvements from adding the language modeling task for all considered training set sizes and tasks. As can be seen, the auxiliary task is extremely valuable when training on just 10 examples. With more data, it becomes less important, some- times even leading to worse performance. Only for MNLI, we find language modeling to consistently help for all training set sizes considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have shown that providing task descriptions to pretrained language models can be combined with regular supervised training. Our proposed method, PET, consists of defining pairs of cloze question patterns and verbalizers that help leverage the knowledge contained within pretrained language models for downstream tasks. We finetune models for all pattern-verbalizer pairs and use them to create large annotated datasets on which regular classifiers can be trained. When the initial amount of training data is limited, PET gives large improvements over regular supervised training.</p><p>Similar to <ref type="bibr" target="#b3">Jiang et al. (2019)</ref>, future work could look into automatic identification of suitable patterns and verbalizers. Furthermore, it would be interesting to see whether the idea of PET can also be transferred to other task types, e.g., token classification or question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Accuracy improvements from adding L MLM during training for all tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for RoBERTa (large) on Yelp Reviews, AG's News and MNLI (matched) for various training set sizes and training approaches. All scores for PET were obtained using the weighted variant.</figDesc><table><row><cell></cell><cell>Training Mode</cell><cell>Yelp</cell><cell>AG's News</cell><cell>MNLI (m)</cell></row><row><cell>|T | = 0</cell><cell>unsupervised (avg) unsupervised (max)</cell><cell>33.8 ± 9.6 40.8 ± 0.0</cell><cell>69.5 ± 7.2 79.4 ± 0.0</cell><cell>39.1 ± 4.3 43.8 ± 0.0</cell></row><row><cell>|T | = 10</cell><cell>supervised PET</cell><cell>21.1 ± 1.6 52.9 ± 0.1</cell><cell>25.0 ± 0.1 87.5 ± 0.0</cell><cell>34.2 ± 2.1 41.8 ± 0.1</cell></row><row><cell>|T | = 50</cell><cell>supervised PET</cell><cell>44.8 ± 2.7 60.0 ± 0.1</cell><cell>82.1 ± 2.5 86.3 ± 0.0</cell><cell>45.6 ± 1.8 63.9 ± 0.0</cell></row><row><cell>|T | = 100</cell><cell>supervised PET</cell><cell>53.0 ± 3.1 61.9 ± 0.0</cell><cell>86.0 ± 0.7 88.3 ± 0.1</cell><cell>47.9 ± 2.8 74.7 ± 0.3</cell></row><row><cell>|T | = 1 000</cell><cell>supervised PET</cell><cell>63.0 ± 0.5 64.8 ± 0.1</cell><cell>86.9 ± 0.4 86.9 ± 0.2</cell><cell>73.1 ± 0.2 85.3 ± 0.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>News examples even outperforms a regular supervised model trained on 1000 examples. As we increase the training set size, the performance gains of PET become smaller, but for both 50 and 100 examples, PET continues to considerably outperform regular supervised training. With 1000 training examples, PET has no or only a small advantage on Yelp and AG's News, but for MNLI, there is still a large gap between PET and supervised training.</figDesc><table><row><cell>shows results. 2 The top</cell></row><row><cell>rows show performance using all PVPs in a fully</cell></row><row><cell>unsupervised setting, where both average results</cell></row><row><cell>across all patterns (avg) and results using the best</cell></row><row><cell>pattern (max) are reported. Importantly, finding</cell></row><row><cell>the best pattern would require access to the test</cell></row><row><cell>set; accordingly, this row serves only as an upper</cell></row><row><cell>bound of fully unsupervised performance. The</cell></row><row><cell>large difference between both rows highlights the</cell></row><row><cell>importance of finding a strategy to cope with the</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Due to the limit of 2 submissions per 14 hours for the official MNLI test set, we treat the dev set as our test set.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An embarrassingly simple approach for transfer learning from pretrained language models</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chronopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Baziotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Potamianos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1213</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2089" to="2095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How can we know what language models know?</title>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-06">Jun Araki, and Graham Neubig. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08730</idno>
		<title level="m">The natural language decathlon: Multitask learning as question answering</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Zero-shot text classification with generative language models</title>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-Fourth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<title level="m">olmpics -on what language model pre-training captures</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rmi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Jamie Brew. 2019. Transformers: State-of-theart natural language processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
	</analytic>
	<monogr>
		<title level="m">Computing Research Repository</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
