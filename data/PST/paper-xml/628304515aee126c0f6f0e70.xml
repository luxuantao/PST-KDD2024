<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-15">15 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yinan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Institute for General Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xingang</forename><surname>Peng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
							<email>&lt;majianzhu@pku.edu.cn&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing Institute for General Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Peking Univer-sity</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing Institute for General Artificial Intelligence</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<address>
									<settlement>Peking Univer-sity</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Jianzhu Ma</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-15">15 May 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2205.07309v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning has achieved tremendous success in designing novel chemical compounds with desirable pharmaceutical properties. In this work, we focus on a new type of drug design problemgenerating a small "linker" to physically attach two independent molecules with their distinct functions. The main computational challenges include: 1) the generation of linkers is conditional on the two given molecules, in contrast to generating full molecules from scratch in previous works; 2) linkers heavily depend on the anchor atoms of the two molecules to be connected, which are not known beforehand; 3) 3D structures and orientations of the molecules need to be considered to avoid atom clashes, for which equivariance to E(3) group are necessary. To address these problems, we propose a conditional generative model, named 3DLinker, which is able to predict anchor atoms and jointly generate linker graphs and their 3D structures based on an E(3) equivariant graph variational autoencoder. So far as we know, there are no previous models that could achieve this task. We compare our model with multiple conditional generative models modified from other molecular design tasks and find that our model has a significantly higher rate in recovering molecular graphs, and more importantly, accurately predicting the 3D coordinates of all the atoms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The biological functions of most small molecule drugs are to inhibit the activity of the target protein by binding its active sites. In drug discover, designing new molecule drugs with desired pharmacophoric properties remains challenging due to the discreteness and enormity of the search space <ref type="bibr" target="#b34">(Polishchuk et al., 2013)</ref>. To address this problem, many machine learning methods have been developed to embed molecules to a compact hidden space and hence make promising progresses in multiple downstream computational tasks such as molecular de-novo design, molecular optimization, and chemical property prediction.</p><p>Molecules are generally represented by graphs with atoms and bonds represented as nodes and edges, respectively. Graph generative models <ref type="bibr" target="#b28">(Liu et al., 2018;</ref><ref type="bibr" target="#b39">Shi et al., 2019;</ref><ref type="bibr" target="#b15">Jin et al., 2018;</ref><ref type="bibr" target="#b52">2020)</ref> are commonly applied to model the marginal probability for FDA-approved drug molecules and it is expected that the newly sampled molecules from the model have similar or better pharmacophoric properties.</p><p>However, in complex diseases such as cancer, mutations of amino acids could significantly impact the binding affinity between drugs and target proteins. The drug might fall off the drug target when a particular amino acid mutates with a certain probability due to the weak binding affinity, which makes the patient drug resistant. To solve this problem, more recently, an alternative drug mechanism named Proteolysis targeting chimera (PROTAC) is developed to inhibit the protein functions by prompting complete degradation of the target protein. PROTAC is a unique molecule composed of two fragment molecules and a linker molecule: one fragment binds the target protein, the other fragment binds another molecule that can degrade the target protein, and the linker attaches the two fragments together. Because PROTAC needs only to bind their targets with high selectivity (rather than inhibit the target protein's activity), many efforts are devoted to retool previously ineffective inhibitor molecules as PROTAC for developing the next-generation drugs. Even though PROTAC owns promising potential, it has not been broadly pushed into clinical trial stages. One of the key challenges is the design of linker, which has critical influence on the ultimate degradation of target protein. To date, linker design still relies on the expertise of structural biologists and thus is very time intensive. Therefore, there are increasing efforts to develop deep learning methods to address linker design problems <ref type="bibr" target="#b13">(Imrie et al., 2020;</ref><ref type="bibr" target="#b50">Yang et al., 2020)</ref>.  graph with 3D coordinates (left), the goal is to generate a linker graph with 3D coordinates to link these two fragments (right). The 3D coordinates of the generated linker must align with the two fragments, otherwise they cannot link.</p><p>A critical challenge for computational linker design stems from its strong 3D spatial constraints compared to classical graph generation tasks. It is known that a successful fragment linker should not disturb the spatial configurations of the two fragments <ref type="bibr" target="#b12">(Ichihara et al., 2011;</ref><ref type="bibr" target="#b20">Klon, 2015)</ref>. In addition, the anchors between fragments and linker also have correlations with their spatial poses. The linker design problem should be extended to include the 3D information, and a 3D-aware generative model is in need for the generation of realistic linkers, rather than invoke a graph generative model. In this paper, we propose a conditional generative model, named 3DLinker, that jointly models the 2D molecular graphs and 3D structures of linker for solving the 3D linker design problem.</p><p>Given the graph and spatial coordinates of two fragments, 3DLinker can jointly generate graph and spatial coordinates of the linker. Particularly, it does not rely on pre-determined anchors and can accurately predict anchors based on the two observed fragments to be connected. More importantly, 3DLinker is able to predict 3D coordinates directly and at the same time keeps equivariant to rotations, translations and reflections, which makes it insensitive to choices of the coordinate system. Finally, since the generative model is based on the variational autoencoder (VAE) framework <ref type="bibr" target="#b18">(Kingma &amp; Welling, 2013)</ref>, it can be used as an unsupervised representation learning method whose latent representations are fed into downstream tasks such as drug-likeness prediction. To the best of our knowledge, 3DLinker is the first trial that simultaneously predicts equivariant graph and 3D coordinates for the linker design problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section we introduce the definition of the 3D linker design problem and basic concepts of E(3) equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Linker Design</head><p>Linker design is to generate a small molecule that can link two given molecular fragments at certain anchors (binding atoms). Instead of modelling it as a 2D graph generation problem, it is important to take 3D information into account, since the designed fragment should satisfy spatial constraints such as not disturbing the relative poses or causing any atom clashes of the two fragments. Therefore, it requires the linker design algorithm to be able to generate both the chemical graph and 3D coordinates given the two fragment and their spatial coordinates (Figure <ref type="figure" target="#fig_1">1</ref>).</p><p>Mathematically, a molecule can be viewed as a graph with atoms as nodes and chemical bonds as edges. A 3D molecule can be represented by a graph G = (V, E, X) with 3D coordinates r = (x, y, z), where V is the set of nodes, |V| is the number of nodes, E ? V ? V are edges, X are node types and R is a matrix whose i-th row is r i ( stands for transpose). In the 3D linker design, two fragments are defined by two unlinked subgraphs G F = (G F,1 , G F,2 ) with geometry R F , and a linker is denoted by G L with geometry R L . Let G, R be the graph and geometry of the ground truth linked molecule containing both fragments and linker. A 3D linker design model is a conditional generative model that completes the ground truth molecule graph as well as its geometry given the two fragments:</p><formula xml:id="formula_0">p(G, R|G F , R F ).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">O(3), E(3) Groups and Equivariance</head><p>Group is a set of operations equipped with multiplications, associativity, the identity element and the inverse element. Group of all 3D rotations and reflections is called 3D Orthogonal Group or O(3), and group of all 3D rotations, translations and reflections is called 3D Euclidean Group or E(3). Let X be the input space, Y be output space and GL(X ) be all invertible linear transformations from X to X (similar for GL(Y)). A function ? : X ? Y is called equivariant to the group G, if for all group element g ? G and all x ? X , there exists group representations ? X : G ? GL(X ) and</p><formula xml:id="formula_1">? Y : G ? GL(Y) such that ? Y (g)?(x) = ?(? X (g)x).<label>(2)</label></formula><p>If ? Y (g) is the identity function for all g ? G, then we say ? is invariant to group G. In 3D linker design, it is known that a molecule graph G should not depend on a specific coordinate system and R should change equivariantly to transformations of the coordinate system. Therefore it raises a constraint that for any g ? E(3), the generative model</p><formula xml:id="formula_2">p(G, R|G F , R F ) should satisfy p(G, ?(g)R|G F , ?(g)R F ) = p(G, R|G F , R F ),<label>(3)</label></formula><p>where ?(g) can by any rotation, translations or reflections matrix in the 3D space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>Graph-based Molecular Generation. Variational Graph Auto-Encoders are the most popular models for molecular generation. Early approaches mainly focus on embedding the 2D chemical graphs into low dimensional space and sample new molecules by perturbing the hidden values. The representative works include GraphVAE <ref type="bibr" target="#b43">(Simonovsky &amp; Komodakis, 2018)</ref>, CGVAE <ref type="bibr" target="#b28">(Liu et al., 2018)</ref>, JT-VAE <ref type="bibr" target="#b15">(Jin et al., 2018)</ref>, GraphNVP <ref type="bibr" target="#b31">(Madhawa et al., 2019)</ref> and so on.</p><p>Although none of these methods designs molecular linkers, graph-based VAE models serve as the basic building block of our entire architecture and all of these models could be plug into our framework by transforming the generative model to conditional generative model. For the perspective of training techniques, auto-regression are also widely adopted to train graph-based deep learning models, such as GraphRNN <ref type="bibr" target="#b51">(You et al., 2018b)</ref>, DeepGMG <ref type="bibr" target="#b27">(Li et al., 2018</ref><ref type="bibr">), GraphAF (Shi et al., 2019)</ref>. Most of these model generate nodes and edges in a sequential manner.</p><p>Point-cloud-based Molecular Models. An important component of our work is to model and design the 3D structures of molecular fragments, in which maintaining the equivariant properties is the crucial computational challenge. One typical solution is to model the molecules as 3D point clouds using equivariant neural networks <ref type="bibr" target="#b38">(Sch?tt et al., 2017;</ref><ref type="bibr" target="#b19">Klicpera et al., 2020;</ref><ref type="bibr" target="#b29">Liu et al., 2021;</ref><ref type="bibr">Satorras et al., 2021b;</ref><ref type="bibr" target="#b45">Thomas et al., 2018;</ref><ref type="bibr" target="#b8">Fuchs et al., 2020;</ref><ref type="bibr" target="#b6">Deng et al., 2021;</ref><ref type="bibr" target="#b17">Jing et al., 2020)</ref>. To train such models, auto-regression is a more common solution, such as G-SchNet <ref type="bibr" target="#b9">(Gebauer et al., 2019)</ref>, G-SphereNet <ref type="bibr" target="#b0">(Anonymous, 2022)</ref>, but flow-based model ENF <ref type="bibr">(Satorras et al., 2021a)</ref> and reinforcement learning <ref type="bibr">(Simm et al., 2020a;</ref><ref type="bibr">b)</ref>.could also be applied. The main limitation of point-cloud-based model is that they cannot directly generate discrete graph structures, which make it difficult to model chemical constraints like valency (maximal number of hydrogen atoms one can combine with).</p><p>Molecular Linker Design. DeLinker <ref type="bibr" target="#b13">(Imrie et al., 2020)</ref> is the first attempt to apply deep learning methods to the linker design problem. It constructs a conditional graph generative model that generates linker given two fragments. It adapts CGAVE <ref type="bibr" target="#b28">(Liu et al., 2018)</ref>, generating edges step by step starting with fragments and two known anchor nodes as the binding sites. The spatial distance and angle between two fragments are provided to the model as side information to guide the generation. DEVELOP <ref type="bibr" target="#b14">(Imrie et al., 2021)</ref> improves DeLinker by encoding the spatial information of fragments using CNN. However, both of them only take (coarse) 3D information as input and do not have a precise atom-level description of molecule geometry, which are not sufficient to express the fragments geometry. In addition, anchor nodes are assumed to be known in advance, which is rare in real world application. Most importantly, they are only able to generate graph representations of linker without 3D coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we present our 3DLinker, a conditional VAEbased generative model that generates both invariant graphs and equivariant absolute coordinates of linkers given two 3D fragments.</p><p>Notations. Let G F , G L , G be graphs of fragments, linker and full molecule (ground truth) respectively, and similarly for coordinates R F , R L , R as in section 2.1 . As we complete the full molecule graph step by step, we use G t and R t to denote the current (existing) graph and coordinates at timestamp t, where</p><formula xml:id="formula_3">G 0 = G F , R 0 = R F .</formula><p>The encoder embeds each node i ? V with both invariant features h i ? R n h (for embedding the graph) and equivariant features v i ? R nv?3 (for embedding the coordinates), which are further used for sampling invariant latent variables</p><formula xml:id="formula_4">z h i ? R m h and equivariant latent variables z v i ? R mv?3 . Symbols h, v, z h , z v without</formula><p>subscripts refer to that variable for all nodes in a general sense. For column vectors a ? R c and b ? R c , we use a b ? R c to denote pointwise multiplication and diag{a} ? R c?c to denote a matrix whose diagonal is a and zero otherwise.</p><p>Equivariant Features For Coordinates Predictions. The equivariant nature makes it difficult to predict absolute coordinates directly. Many of existing works <ref type="bibr" target="#b9">(Gebauer et al., 2019;</ref><ref type="bibr" target="#b0">Anonymous, 2022;</ref><ref type="bibr">Xu et al., 2021b)</ref> tackle this problem by encoding coordinate information as invariant node features and predicting invariant quantities such as distances and edge angles. However, these indirect methods are either computationally intensive (need transformation to local coordinate system <ref type="bibr" target="#b0">(Anonymous, 2022)</ref>) or introducing extra error from the second nonconvex optimization (for translating distance matrices into absolute coordinates <ref type="bibr">(Xu et al., 2021b)</ref>). Instead, we propose to directly generate absolute coordinates while preserving equivariance. To generate equivariant coordinates, only leveraging invariant features is not enough: we cannot produce an equivariant quantity arbitrarily by combining invariant quantities. Therefore, in addition to invariant node features, we need to introduce extra equivariant node features that can be directly used for composing equivariant coordinates. We use notations h for invariant features and v for equivariant features.</p><p>Vector Neurons. Classical fully connected neural networks or MLPs cannot preserve equivariance and thus is not suitable for transforming equivariant features v. In this regard, vector neuron networks or VN-MLP <ref type="bibr" target="#b6">(Deng et al., 2021)</ref> propose a ReLU-like nonlinear function for equivariant features. Concretely, given an equivariant input v ? R nv?3 , Vector-ReLU learns two weight matrices W ? R n v ?nv and</p><formula xml:id="formula_5">U ? R n v ?nv to map v to output v ? R n v ?3 via q = W ? v ? R n v ?3 , k = U ? v ? R n v ?3 , (4a) v = q -diag 1 q,k &lt;0 q, k k ? k k ,<label>(4b)</label></formula><p>where q, k ? R n v is the inner product in the last axis, 1 is the indicator function, and k ? R n v is the norm of k over the last axis. It is easy to verify that v is equivariant, since both q and k are linear combinations of equivariant input v while coefficients q, k is invariant. Intuitively, Vector-ReLU projects q to the orthogonal plane of a learnable direction k if q lies in the other side of the plane, which is analogous to the cutoff in classic ReLU. This nonlinearity enhances the expressive power while preserving the equivariance. We use VN-MLP to denote a neural network stacked by multiple Vector-ReLU units.</p><p>Mixed-Features Message Passing. Now we are ready to introduce our Mixed-Features Message Passing (MF-MP) scheme. MF-MP performs message passing for invariant features h and equivariant features v simultaneously, and in each step the two types of features are properly mixed so that 1) their respective invariance and equivariance properties are preserved, and 2) one type of feature helps the update of the other type and vice versa.</p><p>In the first step, invariant features h ? R n h and equivariant features v ? R nv?3 are transformed and mixed to construct new expressive intermediate features h , h , v by</p><formula xml:id="formula_6">h j = ? 1 (h j , VN-MLP 1 (v j ) ) ? R n h ,<label>(5a)</label></formula><formula xml:id="formula_7">h j = ? 2 (h j , VN-MLP 2 (v j ) ) ? R nv ,<label>(5b)</label></formula><formula xml:id="formula_8">v j = diag{? 3 (h j )} ? VN-MLP 3 (v j ) ? R nv?3 . (5c)</formula><p>Next, point convolution <ref type="bibr" target="#b45">(Thomas et al., 2018;</ref><ref type="bibr" target="#b38">Sch?tt et al., 2017;</ref><ref type="bibr">2021)</ref> is applied to linearly transform the mixed features h , h , v into messages:</p><formula xml:id="formula_9">m h i?j = Ker 1 ( r i,j ) h j ,<label>(6a)</label></formula><formula xml:id="formula_10">m v i?j = diag {Ker 2 ( r i,j )} ? v j + Ker 3 ( r i,j ) h j ? r i,j ,<label>(6b)</label></formula><p>where r i,j = r i -r j is the relative displacement, Ker are learnable kernels such as RBFs that transform a scalar distance into a multi-dimensional output vector using different shape parameters, making the messages geometry-aware. Intuitively, it reflects discrete levels of physical interactions (short-range, long-range) in different distances. More details on Ker are given in Appendix B.</p><p>Finally, Gated Recurrent Units (GRU) <ref type="bibr" target="#b26">(Li et al., 2015)</ref> and VN-MLP are applied as powerful nonlinear transformations to update the node features with the messages:</p><formula xml:id="formula_11">hi = GRU(h i , j?N (i) m h i?j ), (7a) ?i = VN-MLP 4 (v i , j?N (i) m v i?j ).<label>(7b)</label></formula><p>Here N (i) stands for neighbors of i. Our Mixed-Features Message Passing (MF-MP) above effectively mixes invariant and equivariant features in each step to help the update of each other with powerful nonlinear functions. Proof of MF-MP's equivariance w.r.t. E(3) is included in Appendix A.</p><p>We also discuss how it is related and different from Tensor Field Networks <ref type="bibr" target="#b45">(Thomas et al., 2018)</ref> in appendix B.</p><p>Now we describe details about the encoder and decoder of 3DLinker using MF-MP as building blocks. 3DLinker is a conditional latent generative model</p><formula xml:id="formula_12">p ?,? (G, R|G F , R F ) including an encoder q ? (z h , z v |G F , G, R F , R), a decoder p ? (G, R|G F , R F , z h , z v ) and a prior p(z h , z v |G F , R F ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Encoder</head><p>The encoder q ? (z h , z v |G F , G, R F , R) computes node-level latent distributions utilizing MF-MP. Initially, invariant features h are embeddings of node types and we let equivariant features v = 0. After applying several times of MF-MP, we obtain the final node features h and ?. The latent variables are sampled by</p><formula xml:id="formula_13">z h i ?N (? h i , (? h i ) 2 I), z v i ?N (? v i , (? v i ) 2 I)</formula><p>for linker nodes only, where the means and variances are computed from the final node features:</p><formula xml:id="formula_14">?i ? V L , ? h i = ? 4 ( hi ), (? h i ) 2 = ? 5 ( hi ),<label>(8a)</label></formula><formula xml:id="formula_15">? v i = VN-MLP 5 (? i ), (? v i ) 2 = ? 6 ( hi ).<label>(8b)</label></formula><p>Note that for equivariant latent variables z v the covariance (? v ) 2 I assign the same variance to x, y, z directions, which is the simplest way to preserve equivariance. Since fragments (G F , R F ) are given during generation, there is no need to sample their latent variables. Instead, we run the same (weight-sharing) MF-MP network again on fragments only, which gives another set of final node features ?i , vi for fragment nodes. Latent variables of fragment nodes are deterministically obtained by</p><formula xml:id="formula_16">?i ? V F , z h i = ? 7 ( ?i ), z v i = VN-MLP 6 (v i ).<label>(9a)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Decoder</head><p>The decoder p ? (G, R|G F , R F , z h , z v ) constructs (G, R) from fragments (G F , R F ) in a sequential manner. In the decoding process we incorporate the valency rules of molecules by masking out impossible edges and anchor nodes. The decoding process consists of the following steps: (1) Anchor Node Prediction: predict anchor nodes a = (a 1 , a 2 ) for the two fragments. These two anchor nodes are served as the binding points for linker to connect. (2) Node Type Prediction: predict node type X for all linker nodes. (3) Edge and Coordinate Prediction: Put the two anchor nodes in a queue. Then do the following until the queue is empty: (i) Pop a node f from the queue and define it as the current focus node. (ii) Predict an edge between the focus node f and another node. The connected node i is added to the queue. If node i is a linker node and is connected to the existing graph G t for the first time, predict its coordinates r i . (iii) Repeat (ii) and (iii) until an artificial stop node is connected. Update the coordinates of all nodes in the current linker. The focus node f is then marked as closed, which cannot be added to the queue or connected anymore. Then go back to (i).</p><p>Mathematically we factorize the joint probability into:</p><formula xml:id="formula_17">p ? (G, R|G F , R F , z h , z v ) = p ? (E, X, R|E F , X F , R F , z h , z v ) = p ? (a 1 , a 2 |z h , z v ) Anchor ? p ? (X|z h ) Node Types ? T -1 t=0 p ? (E t+1 , R t+1 |E t , R t , X, a 1 , a 2 , z h , z v )</formula><p>Edges and Coordinates ,</p><p>where E T = E and R T = R. Details of each component are explained in the following.</p><p>Anchor Node Prediction. To jointly predict two anchor nodes, we further factorize the joint probability into p a1 , the probability of anchor a 1 on the first fragment G F,1 , and p a2 , the probability of anchor a 2 on the second fragment G F,2 conditioning on a 1 :</p><formula xml:id="formula_19">p ? (a 1 , a 2 |z h , z v ) = p ? (a 1 |{z h i , z v i |i ? V F,1 ) ? p ? (a 2 |z h a1 , z v a1 , {z h i , z v i |i ? V F,2 ).<label>(11)</label></formula><p>Concretely, each node on the first fragment will get a score</p><formula xml:id="formula_20">c i = ? 8 (z h i , A 1 ? z v i ),</formula><p>where A 1 ? R nv?nv is a learnable linear transformation. The scores c i are then passed to a softmax to compute the anchor probability for nodes of the first fragment: p a1 = exp(c a1 )/( i?VF,1 exp(c i )). Then the latent variables of this predicted anchor node as well as nodes of the second fragment are used to compute another group of scores</p><formula xml:id="formula_21">c i = ? 9 (z h i , A 1 ? z v i , z h a1 , A 1 ? z v a1 )</formula><p>, and the probability of the second anchor is p a2 = exp c a2 /( i?VF,2 exp(c i )).</p><p>Node Type Prediction. Node types of linker are directly predicted using their latent variables. We leverage the selfattention mechanism <ref type="bibr" target="#b46">(Vaswani et al., 2017)</ref> to obtain new node features, which are then passed to an MLP to get the logits of node types. After node types are sampled, the types' embeddings are concatenated to the corresponding latent variables z h for latter procedures.</p><p>Edge and Coordinate Prediction. The edge and coordi-  We first pick up on a node to focus. Then we sample an edge between focus node and other nodes (including an artifical stop node). If a linker node is first connected to the existing graph, its coordinates will be predicted. Each time before prediction MF-MP is applied to capture information from existing graph. We keep adding edges until stop node is selected, and then coordinates of all link nodes in the existing graph will be simultaneously updated. We then refocus on a new node and repeat. The procedure continues until all nodes in linker have been focused. nate generation path (E 0 , R 0 ) ? (E 1 , R 1 )... ? (E, R) is defined by a sequence of node focusing, edge prediction and coordinate prediction procedures. The node focusing and edge connecting order is pre-determined by a Breath-First Search to enable teacher-forcing training. When a focus node f is picked, we add new edges to it until connecting to the stop node. Concretely, at each step we first apply MF-MP (with different weights from that in the encoder) to obtain updated node features zh , zv from the initial latent variables z h , z v . Then we compute the probability for edge E i,f by</p><formula xml:id="formula_22">s i,f = ? 10 (z h i , zh f , A 2 ? zv i , A 2 ? zv f , j?V zh j , j?V z h j ). p(E i,f ) = exp(s i,f ) j?V exp(s j,f ) . (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>To predict coordinates, we define ? i (z h , zv , R t , r) that outputs equivariant coordinates ri for node i:</p><formula xml:id="formula_24">p i,j = ? 11 (z h i , zh j , A 3 ? zv i , A 4 ? zv j ),<label>(13a)</label></formula><formula xml:id="formula_25">q i,j = ? 12 (z h i , zh j , A 5 ? zv i , A 6 ? zv j ), (<label>13b</label></formula><formula xml:id="formula_26">) ri = r + j?Vt p i,j (r j -r) + VN-MLP 7 j?Vt q i,j VN-MLP 8 ( zv i , zv j ) (13c)</formula><p>for any generic coordinates r called reference point. Here VN-MLP taking two inputs means concatenation along the first axis (n v ). The idea is to compute pair-wise interactions (13a, 13b) and predict a deviation from reference point r (13c). If a linker node i is first connected to the graph, we use the mass center of the current graph rt = j?Vt r j /|V t | as the reference point and predict its absolute coordinates by r i = ? pred i (z h , zv , R t , rt ) ; once the stop node is chosen, all linker nodes i in the current graph will update their coordinates using their current coordinates as reference points, i.e., r i = ? updt i (z h , zv , R t , r i ). Note that ? pred and ? updt have distinct network weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Using ELBO</head><p>Variational autoencoder <ref type="bibr" target="#b18">(Kingma &amp; Welling, 2013)</ref> is trained by maximizing the Evidence Lower Bound (ELBO):</p><formula xml:id="formula_27">L(?, ?, ?) = E z?q ? log p ? (G, R|G F , R F , z h , z v ) -?D KL (q ? p), (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>where the prior p(z h , z v |G F , R F ) simply takes standard Gaussian for all linker nodes. The reconstruction error term</p><formula xml:id="formula_29">E z?q ? log p ? (G, R|G F , R F , z h , z v</formula><p>) is approximated by one Monte-Carlo sampling, and we apply teacher forcing <ref type="bibr" target="#b21">(Kolen &amp; Kremer, 2001)</ref> for anchor, node type and edge prediction following the pre-determined order. The loss of anchor node prediction, node type prediction and edge prediction are standard cross entropy while for loss of coordinate prediction we use log-MSE used by <ref type="bibr" target="#b52">(Yu, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generation</head><p>During generation, a maximum number of linker nodes is set and we sample this maximum number of latent variables z h , z v for linker nodes (though some nodes might never be included). Then the generation follows the same procedure as the decoder except that there is no teacher forcing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiment Setup</head><p>Dataset. To evaluate our model, we choose a subset of ZINC <ref type="bibr" target="#b44">(Sterling &amp; Irwin, 2015)</ref>. For each molecule, we perform 20 times of MMFF force field optimization using RDKit <ref type="bibr">(Landrum)</ref> and choose the one with the lowest energy as the ground truth. Following the same procedure from <ref type="bibr" target="#b11">(Hussain &amp; Rea, 2010)</ref>, the (fragments, linker) pairs are produced by enumerating all double cuts of acyclic single bonds that are not within any functional groups. In total, we obtain 365,749 (fragments, linker, coordinates) triplets and randomly split them into training (365,039), validation (351) and test (358).</p><p>Evaluation. We evaluate the generated molecules for multiple 2D (graph) and 3D (coordinates) metrics, including the standard ones such as validity, uniqueness and novelty <ref type="bibr">(Brown et al., 2019)</ref>. In addition, we also evaluate the percentage of generated molecules passing 2D property filters, including synthetic accessibility <ref type="bibr" target="#b7">(Ertl &amp; Schuffenhauer, 2009)</ref>, ring aromaticity, and pan-assay interference compounds (PAINS) <ref type="bibr" target="#b1">(Baell &amp; Holloway, 2010)</ref>. After filtering by validity and 2D property filters, recovery rate is calculated to report percentage of generated molecules that perfectly recover the ground truth molecule graphs. To evaluate quality of the 3D structures, the predicted 3D structures are compared to the ground truth using root-mean-square deviation (RMSD). Note that RMSD is only computed for generated molecules that perfectly recover the ground truth molecular graphs (including their isomorphic variants), since only recovered molecules have atom-to-atom alignment to ground truth. Following DeLinker, we compute another 3D metric, named shape-and-color similarity score (SC RDKit ). Appendix C contains more details about the evaluation standards.</p><p>Baselines. Though there are works of molecular graph generative models and molecular geometry prediction given molecular graph, they rarely focus on either fragment linking or jointly modeling of both graph and geometry. Existing molecule generative models either do not work on conditional (linker) generation, or cannot predict 3D coordinates. Therefore, we implement multiple baselines by adapting multiple generative models to conditional generative models to generate 2D linker graphs given two molecular fragments. The generated graphs are then taken by a molecular geometry prediction model, ConfVAE <ref type="bibr">(Xu et al., 2021b)</ref>, to predict the 3D coordinates of each atom. Our baselines include DeLinker+ConfVAE, GraphAF+ConfVAE and GraphVAE+ConfVAE. DeLinker is an existing baseline for conditional linker generation. GraphAF is an autoregressive flow model, and GraphVAE is a VAE-based model with graph-level encodings. The latter two are adapted to conditional graph generation. For ConfVAE, we modify its decoder flow model to conditionally predict linker coordinates. Please see appendix C for implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>We trained 3DLinker for 20 epochs using Adam optimizer with learning rate 0.006, batch size 48 and KL trade-off ? = 0.6. Training details for other baselines are included in Appendix C. Each model generates 250 samples per fragments, which leads to in total 250 ? 358 = 89500 samples. Note that since DeLinker takes anchor nodes as known ground truth, we add another comparison where anchor nodes are known to 3DLinker when generation, denoted as 3DLinker (given anchor). Results in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table" target="#tab_1">2</ref> show that 3DLinker could generate valid and similar linker graph structures with a higher recovery rate, and at the same time achieve accurate predictions of the 3D coordinates of each atom. An interesting observation is that although focusing on 3D structures, 3DLinker achieves superior recovering accuracy of the 2D molecular graphs. Our interpretation is that incorporating 3D constraints benefits to the reconstruction of 2D graphs, though it might influence the diversity (low novelty and uniqueness). This also explains why the novelty and uniqueness are relatively low because the 3D constraints significantly reduce the valid chemical compound structures. In addition, GraphAF and GraphVAE are not able to obey valency rules during training, which explains their low valid and recovery rates. In terms of 3D structure predictions, the low RMSD of 3DLinker demonstrates the effectiveness of both equivariant features and coordinate update strategies. ConfVAE performs poorly since the error of distance matrix prediction scales up quickly with the number of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Study</head><p>In the ablation study, we focus on two key components of 3DLinker: (1) equivariant features vs. invariant features alone;</p><p>(2) update of all coordinates until the generation finishes, instead of fixing them in the 3D space one by one.</p><p>Results display a significant decrease of performance after removing either equivariant features or coordinate update, especially for the RMSD and recovery rate. See Appendix D for details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Molecular Property Prediction</head><p>We show a downstream task of molecule property prediction. We use the learned latent variables from VAE models to predict the Quantitative Estimate of Drug-Likeness (QED) by a gated sum:</p><formula xml:id="formula_30">QED = ? i?G ?(? 13 (z h i , V z v i ))? 14 (z h i , U z v i ) ,<label>(15</label></formula><p>) where ? 13 and ? 14 are two separate neural networks, V, U are two linear transform matrices and ?(?) is a sigmoid function. QED is widely adopted to quantify the potential for a small molecular to be a drug while the function of molecular linkers is to connect two existing drugs. Here we adopt QED to check whether 3DLinker produces biological and biochemical meaningful molecules.</p><p>As shown in Table <ref type="table" target="#tab_2">3</ref>, 3DLinker achieves the lowest Root Mean Square Error (RMSE) among all the models, suggesting a good expressing power of its learned latent representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Visualization</head><p>In the end, we present multiple examples of linker design by visualizations. In Figure <ref type="figure" target="#fig_5">4</ref>, we show the top-5 molecules with highest SC RDKit generated by 3DLinker (first row in the middle) and DeLinker+ConfVAE (second row in the middle). It is obvious that molecules from 3DLinker are generally more similar to the ground truth 2D chemical graph, and have a better spatial alignment with the ground truth 3D structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We developed 3DLinker, a conditional variational autoencoder that is able to jointly models graph and 3D representations, predict anchor nodes and samples linkers. Experiments show that 3DLinker is able to generate linkers with both high recovery rate and precise geometry.</p><p>There are still limitations to be considered in the future. First, models should be able to sample number of linker nodes instead of setting a maximal number of nodes in advance. Second, though it is known that spatial configuration of fragments should not be disturbed, in practice slight difference exist between different linkers, which is an avenue for future works to take into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software and Data</head><p>Codes and data are included in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiment Details</head><p>Some evaluation metrics. Validity is defined by percentage of generated molecules that both obey chemical constraints (valency) and successfully links two fragments into connected graphs. Invalid molecules are discarded for the following evaluations. Uniqueness means percentage of non-duplicate generated molecules: len(Set(generated molecules)) len(generated molecules) . Novelty refers to percentage of generated molecules whose linkers are not present in training set. SC RDKit uses two RDKit built-in functions as described in <ref type="bibr" target="#b35">(Putta et al., 2005)</ref> and <ref type="bibr" target="#b24">(Landrum et al., 2006)</ref> to compute color similarity scores between two 3D molecules based on the overlap of their pharmacophoric features. And the shape similarity score is a simple volumetric comparison between the two 3D molecules. Both scores are between 0 (no match) and 1 (perfect match), which are averaged to produce a final score between 0 and 1. Scores above 0.7 indicate a good match, while scores above 0.9 suggest an almost perfect match. Following DeLinker, SC RDKit is measured only on fragments (we re-generate their coordinates together with the linker), which embodies the capability to generate linkers without disturbing fragments.</p><p>3DLinker. We train 3DLinker for 20 epochs with kl trade-off beta 0.6. Note that the anchor node prediction 11 is asymmetric to the permutation of a 1 and a 2 , and thus we apply a permutation of two fragments to enhance our model.</p><p>GraphAF. Originally GraphAF is an autogressive flow model p(G) = t p(G t+1 |G t ). To model a conditional probability p(G|G F ), all we need is to mask out loss of G F and only compute loss starting from G F to G. We trained GraphAF for 170 epochs, and other hyper-parameters are consistent with its source code (https://github.com/DeepGraphLearning/GraphAF).</p><p>GraphVAE. GraphVAE represents a graph G as node types F , adjacency A and edge types E and maps them into a graph-level representation z. Then z is decoded into F , ?, ? with an additional graph matching to compute loss. To modify it into a conditional generative models, a naive approach is to build a generative model for linker G L only, and then predict the anchor nodes that connects to fragments. Concretely, let a 1 , a 2 be anchor nodes of fragments, and b 1 , b 2 be the corresponding anchor nodes of linker, the decoder model p(G|G F , z) is:</p><formula xml:id="formula_31">p(G|G F , z) = p(G L |z F , z)p(a 1 , a 2 , b 1 , b 2 |z F , z)p(E a1,b1 , E a2,b2 |z F , z),<label>(17)</label></formula><p>where z F is a graph-level encoding of fragments and E a1,b1 , E a2,b2 are two edges that connects fragments and linker. The realization is based on code provided by https://github.com/snap-stanford/GraphRNN. In experiments, all hyper-parameters are unchanged except KL trade-off beta is 0.6. where D 0 = {d i,j = r i -r j |i, j ? G 0 } are distances we already knew while D L = {d i,j = r i -r j |i ? G -G 0 or j ? G -G 0 } are distances between nodes with at least one is in linker. After distance matrix D is predicted, we transform it into absolute coordinates by optimizing the following:</p><formula xml:id="formula_32">min {ri|i?G-G0} i,j?G (D i,j -r i -r j ) 2 . (<label>20</label></formula><formula xml:id="formula_33">)</formula><p>Note that we only need to optimize {r i |i ? G -G 0 } since R 0 = {r i |i ? G 0 } are given. Also there is no need for coordinates alignment since coordinate system is well-defined by coordinates of fragments R 0 . Code is provided by https://github.com/MinkaiXu/ConfVAE-ICML21. Note that although ConfVAE can be trained in an end-to-end manner (both equation 19 and 20), it only makes a little improvements (0.01) compared to training the flow alone (see the experiments of its orignal paper <ref type="bibr">(Xu et al., 2021b)</ref>). Thus we choose only to train the flow model alone in our experiments. For each graph, we sample one geometry which is optimized by (20) with 10 times random initialization and 300 steps gradient descent.</p><p>3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>We conduct ablation study on two aspects: removing equivariant features and coordinates update strategy. We train these three models for 20 epochs and evaluate them by the same methods in previous experiments. Results are shown in Table <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>. We can see a dramatic drop of performances after moving either equivariant features or coordiantes update. Especially, both these two modules contribute greatly to the prediction of coordinates, leading to a decrease of RMSD by 1.3 and 0.3 respectively. Also it is interesting to see that coordinates update has a significant impact on graph quality. A possible reason is that updating the coordinates results in a flexible intermediate coordinates, which may increase the expressive capacity of features. In some sense it is similar to EGNN <ref type="bibr">(Satorras et al., 2021b)</ref>, who also update intermediate coordinates in the forward pass.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: 3D linker design problem: given two fragments' graph with 3D coordinates (left), the goal is to generate a linker graph with 3D coordinates to link these two fragments (right). The 3D coordinates of the generated linker must align with the two fragments, otherwise they cannot link.</figDesc><graphic url="image-2.png" coords="2,55.42,79.44,97.10,73.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of overall encoding and decoding process. For encoding, ground truth is sent into a MF-MP encoder to get node-level representations. Those representations of nodes in fragments are discarded and replaced by representations that are computed separately on fragments graph only. For decoding, two anchor nodes are predicted as the binding sites for linker. Node Types of linker are simultaneously predicted before linking. With two anchor nodes and node types of linker, edges and coordinates are sequentially predicted, as demonstrated in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FocusFigure 3 :</head><label>3</label><figDesc>Figure3: Illustration of sequential predictions of edges and coordinates. We first pick up on a node to focus. Then we sample an edge between focus node and other nodes (including an artifical stop node). If a linker node is first connected to the existing graph, its coordinates will be predicted. Each time before prediction MF-MP is applied to capture information from existing graph. We keep adding edges until stop node is selected, and then coordinates of all link nodes in the existing graph will be simultaneously updated. We then refocus on a new node and repeat. The procedure continues until all nodes in linker have been focused.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of fragment linking. The top-5 similar by SC RDKit Fragments proposed by 3DLinker (first row) and DeLinker+ConfVAE (second row) are shown. Generations from 3DLinker are more realistic and similar to ground truth in terms of SC RDKit and 3D geometry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>ConfVAE.</head><label></label><figDesc>ConfVAE is a VAE for geometry generation given graph p(R|G). It encodes graph G and distance matrix D = {d i,j = r i -r j |i, j ? G} into latent variables z, and uses a flow model f to decode distance matrix D = f (G, z). Concretely, its flow f isD = f (G, z) = D(0) + t 0 g ? (G, D(? ), z)d?,(18)where g ? is a Message Passing Neural Networks (MPNN) and D(0) ? N (0, I) is a base distribution. To transfer p(R|G) to a conditional model p(R|G, R 0 ), we modify the flow to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance metrics for generated molecules.</figDesc><table><row><cell>Metrics</cell><cell cols="6">Valid (%) Recovered (%) Pass 2D filters (%) RMSD Unique (%) Novel (%)</cell></row><row><cell>3DLinker (given anchor)</cell><cell>99.20</cell><cell>94.69</cell><cell>90.35</cell><cell>0.079</cell><cell>29.24</cell><cell>32.21</cell></row><row><cell>3DLinker</cell><cell>98.67</cell><cell>93.58</cell><cell>90.37</cell><cell>0.079</cell><cell>29.42</cell><cell>32.48</cell></row><row><cell>DeLinker+ConfVAE</cell><cell>98.38</cell><cell>81.56</cell><cell>89.92</cell><cell>1.356</cell><cell>44.67</cell><cell>39.51</cell></row><row><cell>GraphAF+ConfVAE</cell><cell>34.24</cell><cell>20.39</cell><cell>82.01</cell><cell>1.239</cell><cell>84.11</cell><cell>78.34</cell></row><row><cell>GraphVAE+ConfVAE</cell><cell>15.07</cell><cell>0.56</cell><cell>85.88</cell><cell>1.056</cell><cell>85.52</cell><cell>61.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SC RDKit score distribution (%) and averaged score.</figDesc><table><row><cell>Metrics</cell><cell></cell><cell cols="2">SC RDKit Fragments</cell><cell></cell></row><row><cell></cell><cell cols="4">&gt; 0.7 &gt; 0.8 &gt; 0.9 Average</cell></row><row><cell cols="3">3DLinker (given anchor) 43.10 16.09</cell><cell>2.60</cell><cell>0.684</cell></row><row><cell>3DLinker</cell><cell cols="2">42.55 15.85</cell><cell>2.49</cell><cell>0.683</cell></row><row><cell>DeLinker+ConfVAE</cell><cell cols="2">39.96 13.39</cell><cell>1.93</cell><cell>0.675</cell></row><row><cell>GraphAF+ConfVAE</cell><cell>19.33</cell><cell>3.36</cell><cell>0.32</cell><cell>0.624</cell></row><row><cell>GraphVAE+ConfVAE</cell><cell>13.17</cell><cell>2.15</cell><cell>0.00</cell><cell>0.601</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Root Mean Squared Error of QED prediction.</figDesc><table><row><cell>Metrics</cell><cell>RMSE QED</cell></row><row><cell>3DLinker</cell><cell>0.0833</cell></row><row><cell>DeLinker+ConfVAE</cell><cell>0.1077</cell></row><row><cell>GraphVAE+ConfVAE</cell><cell>0.1179</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study. (eqv-) stands for removing equivariant features while (update-) means removing coordinates update strategy.</figDesc><table><row><cell>Metrics</cell><cell cols="6">Valid (%) Recovered (%) Pass 2D filters (%) RMSD Unique (%) novel (%)</cell></row><row><cell>3DLinker</cell><cell>98.67</cell><cell>93.58</cell><cell>90.37</cell><cell>0.079</cell><cell>29.42</cell><cell>32.48</cell></row><row><cell>3DLinker (eqv-)</cell><cell>99.42</cell><cell>86.59</cell><cell>92.68</cell><cell>1.352</cell><cell>34.58</cell><cell>27.02</cell></row><row><cell>3DLinker (update-)</cell><cell>98.85</cell><cell>39.94</cell><cell>62.81</cell><cell>0.399</cell><cell>55.93</cell><cell>72.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation study. (eqv-) stands for removing equivariant features while (update-) means removing coordinates update strategy.</figDesc><table><row><cell>Metrics</cell><cell></cell><cell cols="2">SC RDKit Fragments</cell><cell></cell></row><row><cell></cell><cell cols="4">&gt; 0.7 (%) &gt; 0.8 (%) &gt; 0.9 (%) Average</cell></row><row><cell>3DLinker</cell><cell>42.55</cell><cell>15.85</cell><cell>2.49</cell><cell>0.683</cell></row><row><cell>3DLinker (eqv-)</cell><cell>38.51</cell><cell>13.15</cell><cell>1.76</cell><cell>0.672</cell></row><row><cell>3DLinker (update-)</cell><cell>37.34</cell><cell>10.87</cell><cell>1.13</cell><cell>0.670</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>3DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Invariance/Equivariance</head><p>In this section we prove that 3DLinker satisfies E(3) equivariance, namely for all g ? E(3), we have p ? (G, ?(g)R|G F , ?(g)R F ) = p ? (G, R|G F , R F ). For simplicity, we denote E(3) invariance and equivariance as E(3)inv and E(3)-eqv, and O(3) invariance and equivariance as O(3)-inv and O(3)-eqv, and translational invariance as T-inv.</p><p>We first prove h is E(3)-inv while ? is O(3)-eqv/T-inv after MF-MP (5, 6, 7)</p><p>Proof. First let us show that in (5), h , h are E(3)-inv and v is O(3)-eqv/T-inv. Note that VN-MLP(v) is E(3)-eqv and T-inv due to the properties of vector neurons, and thus the norm VN-MLP(v) is O(3)-inv and T-inv (because O(3) preserves inner product), namely E(3)-inv. Therefore in (5a, 5b) h and h are both E(3)-inv. Equation (5c) is essentially an O(3)-eqv VN-MLP hv (v j ) scaled by E(3)-inv features diag{? hv (h j )}, and thus its output v j is also O(3)-eqv/T-inv.</p><p>Then in message function (6a, 6b), message</p><p>Similarly for the second term is a O(3)-eqv/T-inv relative displacement r i,j scaled by both E(3)-inv kernels and features. Therefore</p><p>Finally, in (7a) h is E(3)-inv since all its inputs are E(3)-inv. In (7b) ? is O(3)-eqv/T-inv due to vector neurons.</p><p>Theorem A.2. The generative model p ? (G, R|G F , R F ) (8, 9, 10) satisfies equivariance condition (3).</p><p>Proof. By lemma A.1, the encoded latent variables z h and z v from (8, 9) is E(3)-inv and O(3)-eqv/T-inv respectively. In the decoding process, anchor nodes, node types and edges are all predicted from z h or norm of z v . Thus the probability of graph G is E(3)-inv. The coordinates are predicted through (13). Note that p i,j , q i,j in (13a, 13b) are E(3)-inv and r in ( <ref type="formula">13c</ref>) is E(3)-eqv (mass center), which implies terms j?Vt p i,j (r j -r) and VN-MLP 7 j?Vt q i,j VN-MLP 8 ( zv i , zv j ) are all O(3)-eqv/T-inv. Finally we can conclude that ? i is E(3)-eqv because an O(3)-eqv/T-inv quantity pluses an E(3)-eqv quantity r results in an E(3)-eqv quantity). Therefore R is E(3)-eqv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point Convolution</head><p>In the message function (6), kernel Ker( r i,j ) assigns weights relying on distance r i,j . Concretely in our implementation, our kernel first applies Gaussian functions with 10 different means and perform a learnable affine transform:</p><p>where ? 1 &lt; ? 2 &lt; ... &lt; ? 10 are hyper-parameters and k is a learnable parameter. The intuition behind is to capture the intensity of interactions between nodes (atoms) with different distances. The largest mean ? 10 is basically the maximal correlation length: if two nodes are separated by distance beyond ? 10 , their interaction is nearly neglectable.</p><p>Mathematically, message functions (6) can be seen as tensor products using spherical harmonics, as described in Tensor Field Networks <ref type="bibr" target="#b45">(Thomas et al., 2018)</ref>. In group representation theory, invariant features h and equivariant features v are called type-0 and type-1 tensors respectively, and the theory <ref type="bibr" target="#b10">(Griffiths &amp; Schroeter, 2018)</ref> tells us the correct way to construct new type-0 features h or type-1 features ? using type-l spherical harmonics Y l (r) and h, v. In our case since we only have type-0 and type-1 features, Y 0 (r) ? 1 and Y 1 ? r are all we need, which explains the design of (6).</p><p>Note that there are several differences between our method and tensor products in Tensor Field Networks (TFN). First TFN is based on SO(3) equivariant, while we seek for O(3) equivariant. So terms like cross product are discarded since they violate mirror symmetry. Besides, we mix different types of features before convolution, in contrast to convolution on raw features. Finally, TFN uses simple activation functions like scaling with norm, while we leverage Vector Neuron for novel non-linearity.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An autoregressive flow model for 3d molecular geometry generation from scratch</title>
		<author>
			<persName><surname>Anonymous</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">Submitted to The Tenth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>id=C03Ajc-NS5W. under review</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">New substructure filters for removal of pan assay interference compounds (pains) from screening libraries and for their exclusion in bioassays</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Baell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Holloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2719" to="2740" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design Brown</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bickerton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Hopkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fiscato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Vaucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1096" to="1108" />
			<date type="published" when="2012">2012. 2019</date>
		</imprint>
	</monogr>
	<note>Nature chemistry</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Impact of linker length on the activity of protacs</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cyrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wehenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-B</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular BioSystems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="359" to="364" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08786</idno>
		<title level="m">Syntaxdirected variational autoencoder for structured data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convolutional neural networks on graphs with fast localized spectral filtering. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vector neurons: A general framework for so (3)-equivariant networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poulenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12229</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schuffenhauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">)-transformers: 3d roto-translation equivariant attention networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><surname>Se</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10503</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Gebauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00957</idno>
	</analytic>
	<monogr>
		<title level="j">ACS central science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="276" />
			<date type="published" when="2018">2019. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Symmetryadapted generation of 3d point sets for the targeted discovery of molecules</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Introduction to quantum mechanics</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Schroeter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computationally efficient algorithm to identify matched molecular pairs (mmps) in large data sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="339" to="348" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compound design by fragment-linking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Ichihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Whittaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecular Informatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="298" to="306" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Journal of chemical information and modeling</title>
		<author>
			<persName><forename type="first">F</forename><surname>Imrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Deane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983-1995, 2020</date>
			<biblScope unit="volume">60</biblScope>
		</imprint>
	</monogr>
	<note>Deep generative models for 3d linker design</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep generative design with 3d pharmacophoric constraints</title>
		<author>
			<persName><forename type="first">F</forename><surname>Imrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Hadfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Deane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2323" to="2332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical generation of molecular graphs using structural motifs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4839" to="4848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eismann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Suriana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Townshend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dror</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01411</idno>
		<title level="m">Learning from protein structure with geometric vector perceptrons</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Directional message passing for molecular graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gro?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03123</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fragment-Based Methods in Drug Discovery</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Klon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A field guide to dynamical recurrent networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kremer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Grammar variational autoencoder</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1945" to="1954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Landrum</surname></persName>
		</author>
		<ptr target="http://www.rdkit.org/" />
		<imprint>
			<date type="published" when="2022-01-13">2022/1/13</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Featuremap vectors: a new class of informative descriptors for computational drug discovery</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Landrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Penzotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Putta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="751" to="762" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Crafting papers on machine learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning (ICML 2000)</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</editor>
		<meeting>the 17th International Conference on Machine Learning (ICML 2000)<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1207" to="1216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="7795" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oztekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05013</idno>
		<title level="m">Spherical message passing for 3d graph networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Constrained generation of semantically valid graphs via regularizing variational autoencoders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02630</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Madhawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11600</idno>
		<title level="m">Graphnvp: An invertible flow model for generating molecular graphs</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open babel: An open chemical toolbox</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>O'boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Banck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vandermeersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Hutchison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Estimation of the size of drug-like chemical space based on gdb-17 data</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Polishchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">I</forename><surname>Madzhidov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varnek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="675" to="679" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Conformation mining: an algorithm for finding biologically relevant conformations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Putta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Landrum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Penzotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3313" to="3318" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09016</idno>
		<title level="m">E (n) equivariant normalizing flows for molecule generation in 3d</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">E (n) equivariant graph neural networks</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09844</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A continuousfilter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E S</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><surname>Schnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Equivariant message passing for the prediction of tensorial properties and molecular spectra</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">T</forename><surname>Unke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gastegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03150</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2021. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graphaf: a flow-based autoregressive model for molecular graph generation</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Reinforcement learning for molecular design guided by quantum mechanics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8959" to="8969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A generative model for molecular distance geometry</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11459</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pinsler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cs?nyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12747</idno>
		<title level="m">Symmetry-aware actor-critic for 3d molecular design</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graphvae: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="412" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning neural generative dynamics for molecular conformation generation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10240</idno>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An end-to-end framework for molecular conformation generation via bilevel programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gomez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07246</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01717</idno>
		<title level="m">Molecule3d: A benchmark for predicting 3d geometries from molecular graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Syntalinker: automatic fragment linking with deep conditional transformer neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">31</biblScope>
			<biblScope unit="page" from="8312" to="8322" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">DLinker: An E(3) Equivariant Variational Autoencoder for Molecular Linker Design You</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02473</idno>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018-03">2018a. 3. 2018b</date>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Graphrnn: Generating realistic graphs with deep autoregressive models</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A tutorial on vaes: From bayes&apos; rule to lossless compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10273</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
