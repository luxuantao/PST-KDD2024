<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Information Extraction from Wikipedia: Moving Down the Long Tail</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
							<email>raphaelh@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
							<email>weld@cs.washington.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Computer Science &amp; Engineering Department</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>Las Vegas</addrLine>
									<postCode>2008</postCode>
									<settlement>Nevada</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Information Extraction from Wikipedia: Moving Down the Long Tail</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0D201D5082087BA379AB48A43E7A8A35</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T04:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.4 [Information Systems Applications]: Miscellaneous Information Extraction</term>
					<term>Wikipedia</term>
					<term>Semantic Web</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Not only is Wikipedia a comprehensive source of quality information, it has several kinds of internal structure (e.g., relational summaries known as infoboxes), which enable self-supervised information extraction. While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles, they fail in a larger number of cases, largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data. This paper presents three novel techniques for increasing recall from Wikipedia's long tail of sparse classes:</p><p>(1) shrinkage over an automatically-learned subsumption taxonomy, (2) a retraining technique for improving the training data, and (3) supplementing results by extracting from the broader Web. Our experiments compare design variations and show that, used in concert, these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>We are motivated by a vision of self-supervised information extraction -systems which can autonomously gather and organize semantic data from a large number of Web pages. Such a system could be useful for next-generation information retrieval, question answering and much more. Autonomy is crucial, since the scale of available knowledge is vast. We share this vision with a number of other projects, such as Snowball <ref type="bibr" target="#b0">[1]</ref>, KnowItAll <ref type="bibr" target="#b9">[10]</ref> and Textrunner <ref type="bibr" target="#b2">[3]</ref>, but in contrast to systems which seek to extract from arbitrary Web text, we argue that Wikipedia is an important focus for extraction. If we can render much of Wikipedia into semantic form, then it will be much easier to expand from that base. Focusing on Wikipedia largely solves the problem of inaccurate and unreliable source data <ref type="bibr" target="#b10">[11]</ref>, but introduces new challenges. For example, many previous systems (e.g., Mulder <ref type="bibr" target="#b11">[12]</ref>, AskMSR <ref type="bibr" target="#b4">[5]</ref>, and KnowItAll <ref type="bibr" target="#b9">[10]</ref>) exploit the presence of redundant information on the Web, enabling powerful statistical techniques; however, the Wikipedia corpus has greatly reduced duplication. On the other hand, Wikipedia has several attributes that significantly facilitate extraction: 1) Infoboxes, tabular summaries of an object's key attributes, may be used as a source of training data, allowing for self-supervised learning. 2) Wikipedia gives important concepts their own unique identifier -the URI of a definitional page. The first reference to such a concept often includes a link which can be used for disambiguation. As a result, homonyms are much less of a problem than in unstructured text. 3) Wikipedia lists and categories provide valuable features for classifying pages.</p><p>In previous work, we developed Kylin -a self-supervised system for information extraction from Wikipedia <ref type="bibr" target="#b25">[26]</ref>. Kylin looks for sets of pages with similar infoboxes, determines common attributes for each class, creates training examples, learns extractors, and runs them on each page -creating new infoboxes and completing others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The Long-Tailed Challenge</head><p>Kylin works extremely well for popular infobox classes where users have previously created sufficient infoboxes to train an effective extractor model. For example, in the "U.S. County" class Kylin has 97.3% precision with 95.9% recall. Unfortunately, however, many classes (e.g., "Irish Newspapers") contain only a small number of infobox-containing articles. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, 1442 of 1756 (82%) classes have fewer than 100 instances, and 709 (40%) have 10 or fewer instances. For classes sitting on this long tail, Kylin can't get enough training data -hence its extraction performance is often unsatisfactory for these classes.</p><p>Furthermore, even when Kylin does learn an effective extractor there are numerous cases where Wikipedia has an article on a topic, but the article simply doesn't have much information to be extracted. Indeed, another long-tailed distribution governs the length of articles in Wikipedia; among the 1.8 million pages, 1 many are short articles and almost 800,000 (44.2%) are marked as stub pages, indicating that much-needed information is missing.</p><p>In order to create a comprehensive semantic knowledge base summarizing the topics in Wikipedia, we must confront both of these long-tailed challenges. We must train extractors to operate on sparsely populated infobox classes and we must resort to other information sources if a Wikipedia article is superficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions</head><p>In this paper we describe three novel approaches for improving the recall of extraction of Wikipedia infobox attribute values.</p><p>• By applying shrinkage <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref> over an automatically-learned subsumption taxonomy, we allow Kylin to substantially improve the recall of its extractors for sparse infobox classes.</p><p>• By mapping the contents of known Wikipedia infobox data to TextRunner, a state-of-the-art open information extraction system <ref type="bibr" target="#b2">[3]</ref>, we enable Kylin to clean and augment its training dataset. When applied in conjunction with shrinkage, this retraining technique improves recall by a factor of between 1.1 and 5.9, depending on class.</p><p>• When it is unable to extract necessary information from a Wikipedia page, we enable Kylin to retrieve relevant sentences from the greater Web. As long as tight filtering is applied to non-Wikipedia sources, recall can be still further improved while maintaining high precision.</p><p>Our techniques work best in concert. Together, they improve recall by a factor of 1.76 to 8.71 while maintaining or increasing precision. The area under the precision-recall curve increases by a factor of between 1.96 to 23.32, depending on class. In addition to showing the great cumulative effect of these techniques, we analyze several variations of each method, exposing important engineering tradeoffs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND: EXTRACTION IN KYLIN</head><p>We start by defining the problem under consideration: infobox completion. Recall that an infobox is a relational summary of an article: a set of attribute / value pairs describing the article's subject (see <ref type="bibr" target="#b25">[26]</ref> for an example). Not every article has an infobox and some infoboxes are only partially instantiated with values. We seek to create or complete infoboxes whenever possible. Given a Wikipedia page, we seek to identify the infobox class, thus retrieving its associated schema, and extract as many attribute values as possible from the article (or possibly from the greater Web). In this paper, we concentrate on the extraction process -specifically on increasing recall for sparse classes.</p><p>Before describing our three new methods for increasing Kylin's recall, we review the system's basic architecture <ref type="bibr" target="#b25">[26]</ref>. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, Kylin has three primary components: the preprocessor, a module which generates classifiers, and one which generates Conditional Random Fields (CRF) <ref type="bibr" target="#b12">[13]</ref> extractors. The figure shows the data flow, but the components are invoked in a pipeline in the order described above. We describe them in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preprocessor</head><p>The preprocessor selects and refines infobox schemata, choosing relevant attributes; it then generates machine-learning datasets for 1 Unless noted otherwise, all statistics are taken from the 07/16/2007 snapshot of Wikipedia's English language version. training sentence classifiers and extractors. Refinement is necessary for several reasons. For example, schema drift occurs when authors create an infobox by copying from a similar article and changing attribute values. If a new attribute is needed, they just make up a name, leading to schema and attribute duplication. For example, six different attribute names are used to describe the location of an "Actor's" death: "death location", "deathlocation", "death_place", "deathplace", "place_of_death" and "location of death".</p><p>The initial Kylin implementation used a naive approach to refinement: scanning the corpus and selecting all articles with the same infobox template name. Only the attributes used in at least 15% of the articles were selected. As we discuss in the next section, one benefit of building a taxonomy over the set of infobox classes is the ability to recognize closely related and duplicate classes.</p><p>The preprocessor constructs two types of training datasetsthose for sentence classifiers, and CRF attribute extractors. For each article with an infobox mentioning one or more target attributes, Kylin tries to find a unique sentence in the article that mentions that attribute's value. The resulting labelled sentences form positive training examples for each attribute; other sentences form negative training examples. If the attribute value is mentioned in several sentences, then one is selected heuristically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generating Classifiers</head><p>Kylin learns two types of classifiers. For each class of article being processed, a heuristic document classifier is used to recognize members of the infobox class. For each target attribute within a class a sentence classifier is trained in order to predict whether a given sentence is likely to contain the attribute's value.</p><p>Robust techniques exist for document classification (e.g., Naive Bayes, Maximum Entropy or SVM approaches), but Kylin's simple heuristic technique, which exploits Wikipedia's list and category features, worked well.</p><p>Sentence classification, i.e. predicting which attribute values (if any) are contained in a given sentence, can be seen as a multi-class, multi-label text classification problem. Kylin uses a Maximum Entropy model <ref type="bibr" target="#b17">[18]</ref> with a variety of features: bag of words, augmented with part of speech (POS) tags. To decrease the impact of the noisy and incomplete training dataset, Kylin applies bagging (instead of boosting <ref type="bibr" target="#b18">[19]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning Extractors</head><p>Extracting attribute values from a sentence is best viewed as a sequential data-labelling problem. Kylin uses the CRF model with a wide variety of features (e.g., POS tags, position in the sentence, capitalization, presence of digits or special characters, relation to anchor text, etc.). Instead of training a single master extractor to clip all attributes, Kylin trains a different CRF extractor for each attribute, ensuring simplicity and fast retraining. As mentioned previously, when trained on infobox classes with copious instances (e.g., 500 or more), Kylin learns excellent extractors. The precision ranged from a percentage in the mid-70s to high-90s and recall from low-50s to mid-90s, depending on attribute type and infobox class. Though Kylin is successful on those popular classes, its performance decreases on the long-tail of sparse classes where there is insufficient training data. The next two sections describe new techniques for solving this problem. In Section 5 we explain how we extend Kylin to handle the long tail of short articles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SHRINKAGE</head><p>Although Kylin performs well when it can find enough training data, it flounders on sparsely populated infobox classes -the majority of cases. Our first attempt to improve Kylin's performance uses shrinkage, a general statistical technique for improving estimators in the case of limited training data <ref type="bibr" target="#b23">[24]</ref>. McCallum et al. applied this technique for text classification in a hierarchy classes by smoothing parameter estimate of a data-sparse child with its parent to get more robust estimates <ref type="bibr" target="#b15">[16]</ref>.</p><p>Similarly, we use shrinkage when training an extractor of an instance-sparse infobox class by aggregating data from its parent and children classes. For example, knowing that Performer IS-A Person, and Performer.loc=Person.birth_plc, we can use values from Person.birth_plc to help train an extractor for Performer.loc. The trick is automatically generating a good subsumption hierarchy which relates attributes between parent and child classes. Thus, we first describe our method for creating an ontology relating Wikipedia infoboxes, then describe our approach to shrinkage, and end the section with an empirical exploration of our technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Kylin Ontology Generator</head><p>The Kylin Ontology Generator (KOG) is an autonomous system that builds a rich ontology by combining Wikipedia infoboxes with WordNet using statistical-relational machine learning <ref type="bibr" target="#b26">[27]</ref>. At the highest level KOG computes six different kinds of features, some metric and some Boolean: similarity measures, edit history patterns, class-name string inclusion, category tags, Hearst patterns search-engine statistics, and WordNet mappings. These features are combined using statistical-relational machine learning, specifically joint inference over Markov logic networks <ref type="bibr" target="#b20">[21]</ref>, extending <ref type="bibr" target="#b22">[23]</ref>.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows KOG's architecture. First, its schema cleaner scans the infobox system to merge duplicate classes and attributes, and infer the type signature of each attribute. Then, the subsump-tion detector identifies the subsumption relations between infobox classes, and maps the classes to WordNet nodes. Finally, the schema mapper builds attribute mappings between related classes, especially between parent-child pairs in the subsumption hierarchy. KOG's taxonomy provides an ideal base for the shrinkage technique, as described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Shrinkage Using the KOG Ontology</head><p>Given a sparse target infobox class C, Kylin's shrinkage module searchs upwards and downwards through the KOG ontology to aggregate training data from related classes. The two crucial questions are: 1) How far should one traverse the tree? 2) What should be the relative weight of examples in the related class compared to those in C? For the first question, we search to a uniform distance, l, outward from C. In answer to the second question, we evaluate several alternative weighting schemes in Section 3.3. The overall shrinkage procedure is as follows:</p><p>1. Given a class C, query KOG to collect the related class set: SC = {Ci|path(C, Ci) ≤ l}, where l is the preset threshold for path length. Currently Kylin only searches strict parent/chidren paths without considering siblings. Take the "Performer" class as an example: its parent "Person" and children "Actor" and "Comedian" could be included in SC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Shrinkage Experiments</head><p>This section addresses two questions: 1) Does shrinkage over the KOG ontology help Kylin to learn extractors for sparse classes? What if the target class is not sparse? 2) What is the best strategy for computing the training weights, wij ? To answer these questions we used the 07/16/2007 snapshot of en.wikipedia.org as a source dataset. We tested on four classes 2 , namely "Irish newspaper" (which had 20 infobox-contained instance articles), "Performer" (44), "Baseball stadium" (163), and "Writer" (2213). These classes represent various degrees of "sparsity" in order to provide better understanding of how shrinkage helps in different cases. For the "Irish newspaper" and "Performer" classes, we manually labeled all the instances to compute precision and recall values. Particularly, we count the ground-truth as the attribute values contained in the articles -meaning a 100 percent recall is getting every attribute value which is present in the article. For the "Baseball stadium" and "Writer" classes, we manually labeled 40 randomlyselected instances from each. All the following experiments use 4-fold cross validation.</p><p>After schema cleaning, KOG identified 1269 infobox classes and mapped them to the WordNet lattice (82115 synsets). We found that although the whole ontology is quite dense, the current number of Wikipedia infoboxes is relatively small and most pathes through the taxonomy cover three or fewer infobox classes, which diminishes the effect of path-length threshold l. Table <ref type="table" target="#tab_1">1</ref> shows the detailed parent/children classes for each testing case. In the following, we mainly focus on testing weighting strategies. 2 In average there are around 7 attributes per class, so we actually tested for around 4 × 7 = 28 extractors.    Even with limited parent / children classes for smoothing, all forms of shrinkage improve extraction performance. Figure <ref type="figure" target="#fig_5">4</ref> shows the precision / recall curves for our different weighting strategies; parenthetical numbers (e.g., "Performer (44)" denote the number of positive examples. We draw several conclusions: First, with shrinkage, Kylin learns better extractors, especially in terms of recall. For those very sparse classes such as "Performer" and "Irish newspapers", the recall improvement is dramatic: 55% and 457% respectively; and the area under the precision and recall curve (AUC) improves 57% and 1386% respectively.</p><p>Second, we expected precision-directed shrinkage to outperform the other methods of weighting, since it automatically adapt to dif-ferent degrees of similarity between the targent and related classes. However, the three weighting strategies turn out to perform comparatively on the infobox classes used for testing. The most likely reason is that to achieve total autonomy Kylin estimates the precision, pij, of an extractor by comparing the values which it extracts to those entered manually in existing infoboxes. It turns out that in many cases Wikipedia editors use different expressions to describe attribute values in the infoboxes than they do in the article text. Naturally, this makes the accurate estimation of pij extremely difficult. This, in turn, biases the quality of weighting. In the future, we hope to investigate more sophisticated weighting methods.</p><p>Finally, Shrinkage also helps the quality of extraction in popular classes (e.g., for "Writer"), though the improvement is quite modest. This is encouraging, since "Writer" (Figure <ref type="figure" target="#fig_0">1d</ref>) already had over two thousand training examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RETRAINING</head><p>Our experiments show that shrinkage enables Kylin to find extra data within Wikipedia to help train extractors for sparse classes. A complementary idea is the notion of harvesting additional training data even from the outside Web? Leveraging information outside Wikipedia, could dramaticaly improve Kylin's recall. To see why, we note that the wording of texts from the greater Web are more diverse than the relatively strict expressions used in many places in Wikipeidia. <ref type="foot" target="#foot_0">3</ref> Training on a wider variety of sentences would improve the robustness of Kylin's extractors, which would potentially improve the recall.</p><p>The trick here is determining how to automatically identify relevant sentences given the sea of Web data. For this purpose, Kylin utilizes TextRunner, an open information extraction system <ref type="bibr" target="#b2">[3]</ref>, which extracts relations {r|r = obj1, predicate, obj2 } from a crawl of about 100 million Web pages. Importantly for our purposes, Textrunner' crawl includes the top ten pages returned by Google when queried on the title of every Wikipedia article. In the next subsection, we explain the details of our retraining process; then we follow with an experimental evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Using TextRunner for Retraining</head><p>Recall that each Wikipedia infobox implicitly defines a set of semantic triples {t|t = subject, attribute, value } where the subject corresponds to the entity which is the article's title. These triples have the same underlying schema as the semantic relations extracted by TextRunner and this allows us to generate new training data.</p><p>The retrainer iterates through each infobox class C and again through each attribute, C.a, of that class collecting a set of triples from existing Wikipedia infoboxes: T = {t|t.attribute = C.a}. <ref type="foot" target="#foot_1">4</ref>The Since false positives can greatly impair training, the Kylin retrainer morphologically clusters the predicates which are returned by TextRunner (e.g., "is married to" and "was married to" are grouped). We discard any predicate that is returned in response to a query about more than one infobox attribute. Only the k most common remaining predicates are then used for positive training examples; in our experiments we set k = 1 to ensure high precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FILTERING NEGATIVE EXAMPLES:</head><p>As explained in <ref type="bibr" target="#b25">[26]</ref>, Kylin considers a sentence to be a negative example unless it is known to be positive or the sentence classifier labels it as potentially positive. This approach eliminates many false negatives, but some remain. A natural idea is to remove a sentence from the set of negative examples if it contains the word denoting the relation itself. Unfortunately, this technique is ineffective if based soley on Wikipedia content. To see why, consider the Person.spouse attribute which denotes the "marriage" relation -because the word "spouse" seldom appears in natural sentences, few false negatives are excluded. But by using TextRunner, we can better identify the phrases (predicates) which are harbingers of the relation in question. The most common are used to eliminate negative examples.</p><p>By adding new positive examples and excluding sentences which might be false negatives, retraining generates a greatly improved training set, as we show in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Retraining Experiments</head><p>We ask two main questions: 1) Does retraining improve Kylin's extractors? 2) Do the benfits from retraining combine synergistically with those from shrinkage? Before addressing those questions we experimented with different retraining alternatives (e.g., just adding positive examples and just filtering negatives). While both approaches improved extractor performance, the combination worked best, so the combined method was used in the subsequent study.</p><p>We evaluate retraining in two different cases. In the first case, we use nothing but the target class' infobox data to prime TextRunner for training data. In the second case, we first used uniform-weight shrinkage to create a training set which was then used to query TextRunner. Figure <ref type="figure" target="#fig_8">5</ref> shows the results of these methods on four testing classes.</p><p>We note that in most cases retraining improves the performance, in both precision and recall. When compared with shrinkage, retraining provides less benefit for sparse classes but helps more on the popular class "Writer." This makes sense because without many tuples to use for querying TextRunner, retraining has little effect. For example, for "Performer (44)" retraining added 10 positive examples and filtered 20 negative examples; for "Writer (2213)" retraining added 2204 positive and filtered 3568 negative examples. We suspect that full cotraining would be more effective on sparse classes when shrinkage was unavailable. Finally, we observe synergy between shrinkage and retraining, leading to the biggest improvement. Particularly, on the two sparsest classes "Irish newspaper" and "Performer", the combination improved recall by 585% and 72% respectively, with remarkable improvement in precision as well; and the AUC improved 1680% and 74% respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXTRACTING FROM THE WEB</head><p>While shrinkage and retraining improve the quality of Kylin's extractors, the lack of redundancy of Wikipedia's content makes it increasingly difficult to extract additional information. Facts that are stated using uncommon or ambiguous sentence structures hide from the extractors. In order to retrieve facts which can't be extracted from Wikipedia, we would like to exploit another corpus, in particular the general Web. On the surface, the idea is simple: train extractors on Wikipedia articles and then apply them to relevant Web pages. An obvious benefit of this approach is the ability to find new facts which are not contained in Wikipedia at all.</p><p>The challenge for this approach -as one might expect -is maintaining high precision. Since the extractors have been trained on a very selective corpus, they are unlikely to discriminate irrelevant information. For example, a Kylin extractor for Person.birthdate has been trained on a set of pages all of which have as their primary subject that person's life. Such extractors become inaccurate when applied to a page which compares the lives of several peopleeven if the person in question is one of those mentioned.</p><p>To ensure extraction quality, it is thus crucial to carefully select and weight content that is to be processed by Kylin's extractors. In our work, we view this as an information retrieval problem, which Kylin's web extraction module solves in the following steps: It generates a set of queries and utilizes a general Web search engine, namely Google, to identify a set of pages which are likely to contain the desired information. The top-k pages are then downloaded, and the text on each page is split into sentences, which are processed by Kylin. Each extraction is then weighted using a combination of factors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CHOOSING SEARCH ENGINE QUERIES:</head><p>The first important step is to ensure that the search engine returns a set of highly relevant pages. A simple approach is to use the article title as a query. For example, let us assume that we are interested in finding the birth date of Andrew Murray, a writer. The corresponding Wikipedia page is titled 'Andrew Murray (minister)'. The information in parentheses is used in Wikipedia to resolve ambiguities, but we remove it to increase recall. To improve result relevance, we place quotes around the remaing string, here "andrew murray".</p><p>Although such a query might retrieve many pages about Murray, it is possible that none among the top contains the person's birth date which we might be interested in. We therefore run several more restrictive queries which not only limit results to pages containing the article title, but that also include other keywords to better target the search.</p><p>One such query is the quoted article title followed by the attribute name, as in "andrew murray" birth date. While this increases the chance that a returned page contains the desired information, it also greatly reduces recall, because the terms 'birth date' might not actually appear on a relevant page. For example, consider the sentence 'Andrew Murray was born in 1828.".</p><p>Such predicates which are indicative of attributes, like 'was born in' for the birth date, we have computed already, as described in section 4. We generate an appropriate query for each predicate, which combines the quoted title as well as the predicate, as in "andrew murray" was born in. The combined results of all queries (title only, title and attribute name, as well as title and any attribute predicate) are retrieved for further processing.</p><p>WEIGHTING EXTRACTIONS: Pages which do not contain the preprocessed article title, here 'Andrew Murray', are discarded. Then, using an HTML parser, formatting commands and scripts are removed, and sentences are identified in the remaining text.</p><p>Since most sentences are still irrelevant, running Kylin's extrac-tors on these directly would result in many false positives. Recall that unlike Wikipedia's articles, web pages often compare multiple related concepts, and so we would like to capture the likeliness that a sentence or extraction is relevant to the concept in question. A variety of features may be indicative of content relevance, but we focused on two in particular:</p><p>• The number of sentences δs between the current sentence and the closest sentence containing the (preprocessed) title of the article.</p><p>• The rank of the page δr on Google's results lists returned in response to our queries.</p><p>Each retrieved sentence is then sent to Kylin for extraction, and for each extraction a combined score is computed. This score takes into account both factors δs and δr as well as the confidence δc reported by Kylin's extractors; it is obtained in the following way: First, each of the three parameters δs, δr, δc is normalized by applying a linear mapping into the intervals [αs, 1], [αr, 1], and [αc, 1] respectively, where 1 corresponds to the optimal value and αs, αr, and αc are user-defined parameters. With δ * s , δ * r , and δ * c denoting the normalized weights, the combined score is then obtained as</p><formula xml:id="formula_0">score web := δ * s * δ * r * δ * c .</formula><p>COMBINING WIKIPEDIA AND WEB EXTRACTIONS: Our final question is: how can we combine extraction results from Wikipedia and the Web? Despite our efforts in identifying relevant Web pages and weighting sentences, it is likely that extractions from Wikipedia will be more precise. After all, in Wikipedia we can be sure that a given page is highly relevant, is of high quality, and has a more consistent structure, for which Kylin's extractors have been particularly trained. Yet, Kylin may err on Wikipedia too, especially when the extractors confidence score is low.   A straight-forward combination of the extractors always returns the extraction with highest score, as measured in terms of confidence for extractions from Wikipedia and the weighted combination score web for extractions from the Web. In order to balance the weights of extractors, we adjust the score of extractions from the web to 1 -(1score web ) λ , where λ is a new parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Web Experiments</head><p>In this section we would like to answer two questions: 1) Which factors are important in scoring extractions from the Web? and 2) When combining extractions from Wikipedia and the Web, can recall be significantly improved at an acceptable precision?</p><p>In previous sections, we computed recall as the proportion of facts contained in the infoboxes that our system was able to automatically extract from the text. In this section, however, we are also interested in how many new facts Kylin can extract from the Web, and so we change our definition of recall: we assume that there exists some correct value for each attribute contained in the infobox template of an article and set recall to be the proportion of correct attribute values relative to all attributes. Note that this is a very conservative estimate, since there may not always exist an appropriate value. For example, there exists no death date for a writer who has not died yet.</p><p>For all experiments, we queried Google for the top-100 pages containing the article title, and the top-10 pages containing the article title plus attribute name (or associated predicate). Each new extraction -for which no ground truth existed in Wikipediawas manually verified for correctness by visiting the source page.</p><p>In our first series of experiments, we used Shrink-Retrainthe best extractors trained on Wikipedia -and applied different scoring functions to select the best extraction for an attribute. Figure <ref type="figure" target="#fig_10">6</ref> shows our results: The CRF extractor's reported confidence performed poorly in isolation. Giving priority to extractions from pages at a higher position in Google's returned result lists and resolving ties by confidence, yielded a substantial improvement. Similarly, we tried giving priority to extractions which were fewer sentences apart from the occurrence of the Wikipedia article title on a page, again resolving ties by extractor confidence. The large improvements in precision and recall (as highlighted in the figure <ref type="figure" target="#fig_10">6</ref>) show that much of the returned text is irrelevant, but can be reweighted using simple heuristics. Finally, we were interested if a weighted combination of these factors would lead to synergies. We set αs = .1, αr = .7, αc = .9, so that each factor was roughly weighted by our observed improvement (results were not sensitive to minor variations). On all datasets, performance was comparable or better than the best factor taken in isolation.   In our second series of experiments, we combined extractions from Wikipedia and the Web. In both cases, we applied the Shrink-Retrain extractor, but scored extractions from the Web using the weighted factor combination with λ = .4. The results, shown in Figure <ref type="figure" target="#fig_13">8</ref>, show large improvements in recall at higher precision for the "Baseball stadium" (34%) and "Writer" (63%) datasets, and at moderately improved precision for the "Irish newspaper" and "Performer" datasets. The AUC was substantially expanded in all cases, ranging from 14% to 75%. Compared to the original baseline system, the area has expanded between 96% and 2232%. Table 2 shows the detailed accumulative improvements of AUC for various scenarios. Another interesting observation is that Shrinkage tends to address more the first long-tailed challenge -sparse classes(e.g., "Irish newspaper(20)"), and resorting to the Web tends to address more the second long-tailed challenge -short articles(e.g., many "Writer" articles are short ones about noteless writers).</p><p>In the future, we would like to automatically optimize the parameters αs, αr, αc, λ based on comparing the extractions with values in existing infoboxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>In the preceding sections we have discussed how our work relates to past work on shrinkage and cotraining. In this section, we discuss the broader context of previous work on unsupervised in-formation extraction, approaches for exploiting ontologies in information extraction, and other Wikipedia-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UNSUPERVISED INFORMATION EXTRACTION:</head><p>Since the Web is large and highly heterogeneous, unsupervised and self-supervised learning is necessary for scaling. Several systems of this form have been proposed. SNOWBALL <ref type="bibr" target="#b0">[1]</ref> iteratively generates extraction patterns based on occurrences of known tuples in documents to extract new tuples from plain texts. MULDER <ref type="bibr" target="#b11">[12]</ref> and AskMSR <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref> use the Web to answer questions, exploiting the fact that most important facts are stated multiple times in different ways, which licenses the use of simple syntactic processing. Instead of utilizing redundancy, Kylin exploits Wikipedia's unique structure and the presence of user-tagged data to train machine learners. Patwardhan and Riloff proposed a decoupled information extraction system by first creating a self-trained relevant sentence classifier to identify relevant regions, and using a semantic affinity measure to automatically learn domain-relevant extraction patterns <ref type="bibr" target="#b19">[20]</ref>. Kylin uses the similar idea of decoupling when applying extractors to the general Web. Differently, Kylin uses IR-based techniques to select relevant sentences and trains a CRF model for extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ONTOLOGY-DRIVEN INFORMATION EXTRACTION:</head><p>There have been a lot of work on leveraging ontology for information extraction. The SemTag and Seeker <ref type="bibr" target="#b7">[8]</ref> systems perform automated semantic tagging of large corpora. They use the TAP knowledge base <ref type="bibr" target="#b21">[22]</ref> as the standard ontology, and match it with instances on the Web. PANKOW <ref type="bibr" target="#b5">[6]</ref> queries Google with ontology-based Hearst patterns to annotate named entities in documents. Matuszek et al. uses Cyc to specify Web searches to identify and verify common senses candidates <ref type="bibr" target="#b14">[15]</ref>. The similar idea is utilized in On-toSyphon <ref type="bibr" target="#b16">[17]</ref> where ontology combined with search engines are used to identify semantic instances and relations. In contrast, Kylin automatically constructs the Wikipedia infobox ontology and uses it to help training CRF extractors by shrinkage.</p><p>OTHER WIKIPEDIA-BASED SYSTEMS: Dakka and Cucerzan trained a classifier to label Wikipedia pages with standard named entity tags <ref type="bibr" target="#b6">[7]</ref>. Auer and Lehmann developed the DBpedia <ref type="bibr" target="#b1">[2]</ref> system which extracts information from existing infoboxes within articles and encapsulate them in a semantic form for query. In contrast, Kylin populates infoboxes with new attribute values. Suchanek et al. implement the YAGO system <ref type="bibr" target="#b24">[25]</ref> which extends WordNet using facts extracted from Wikipedia's category tags. But in contrast to Kylin, which can learn to extract values for any attribute, YAGO only extracts values for a limited number of predefined relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>Kylin has demonstrated the ability to perform self-supervised information extraction from Wikipedia <ref type="bibr" target="#b25">[26]</ref>. While Kylin achieved high precision and reasonable recall when infobox classes had a large number of instances, most classes sit on the long tail of few instances. For example, 82% classes can provide fewer than 100 training examples, and for these classes Kylin's performance is unacceptable. Furthermore, even when Kylin does learn an effective extractor there are many cases where Wikipedia's article on a topic is too short to hold much-needed information.</p><p>This paper describes three powerful methods for increasing recall w.r.t. the above to long-tailed challenges: shrinkage, retraining, and supplementing Wikipedia extractions with those from the Web. Our experiments show that each of these methods is effective individually. Particularly, shrinkage addresses more the first longtailed challenge of sparse classes, and the latter two address more the second long-tailed challenge of short articles. We evaluate design tradeoffs within each method. Most importantly, we show that in concert, these methods constitute a huge improvement to Kylin's performance (Figure <ref type="figure" target="#fig_13">8</ref>):</p><p>• Precision is modestly improved in most classes, with larger gains if sparsity is extreme (e.g., "Irish newspaper").</p><p>• Recall sees extraordinary improvement with gains from 5.8% to 50.8% (a factor of 8.8) in extremely sparse classes such as "Irish newspaper." Even though the "Writer" class is populated with over 2000 infoboxes, its recall improves from 18.1% to 32.5% (a factor of 1.8) at equivalent levels of precision.</p><p>• Calculating the area under the precision / recall curve also demonstrates substantial improvement, with an improvement factor of 23.3, 1.98, 2.02, and 1.96 for "Irish newspaper," "Performer," "Baseball stadium," and "Writer," respectively.</p><p>Despite this success, much remains to be done. We hope to devise a better weighting scheme for shrinkage by comparing the KL divergence between the target and mapped classes. We wish to extend our retraining technique to full cotraining. There are several ways to better integrate extraction of Web content with that of Wikipedia, ranging from improved Google querying policies to DIRT-style analysis of extraction patterns <ref type="bibr" target="#b13">[14]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 :</head><label>1</label><figDesc>Figure 1: The number of article instances per infobox class has a long-tailed distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Kylin performs self-supervised information extraction, using Wikipedia inforboxes for training data.</figDesc><graphic coords="2,317.21,52.97,249.42,105.06" type="vector_box" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architecture of Kylin Ontology Generactor.</figDesc><graphic coords="3,69.35,51.87,208.54,190.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 . 3 .</head><label>23</label><figDesc>For each attribute C.a (e.g., Performer.loc) of C: (a) Query KOG for the mapped attribute Ci.aj (e.g., Person.birth_plc) for each Ci. (b) Assign weight wij to the training examples from Ci.aj and add them to the training dataset for C.a. Note that wij may be a function both of the target attribute C.a, the related class Ci, and Ci's mapped attribute Ci.aj. Train the CRF extractors for C on the new training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Regardless of the weighting scheme, extractors trained with KOG-enabled shrinkage outperforms the Kylin baselineespecially on the sparse "Irish newspaper," "Performer" and "Baseball stadium" classes where recall is dramatically improved. In the two sparsest classes, precision is also markedly improved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>retrainer next iterates through T , issuing TextRunner queries to get a set of potential matches R(C.a) = {r|∃t ∈ T : r.obj1 = t.subject, r.obj2 = t.value}, together with the corresponding sentences which were used by TextRunner for extraction. The retrainer uses this mapped set R(C.a) to augment and clean the training data for C's extractors in two ways: by providing additional positive examples, and by eliminating false negative examples which were mistakenly generated by Kylin from the Wikipedia data. ADDING POSITIVE EXAMPLES: Unfortunately, TextRunner's raw mappings, R(C.a), are too noisy to be used as positive training examples. There are two causes for the noise. The most obvious cause is the imperfect precision of TextRunner's extractor. But false positive examples can also be generated when there are multiple interpretations for a query. Consider the TextRunner query r.obj1 = A, r.predicate =?, r.obj2 = B , where A is a person and B is his birthplace. Since many people die in the same place that they were born, TextRunner might return the sentence "Bob died in Seattle." -a poor training example for birthplace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Used in isolation, retraining enables a modest but marked improvement in recall. Combining retraining with shrinkage yields substantially improved extractors with improvements to precision as well as recall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: When applying Kylin to Web pages, the CRF's confidence is a poor choice for scoring extractions of the same attribute. Giving priority to extractions from pages ranked higher by Google, and resolving ties by extractor confidence, improves results considerably. 'Sentence Dis' which gives priority to extractions from sentences which are closer to the next occurrence of the Wikipedia article title on a web page, improves further, and is only outperformed by a weighted combination of the other three factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: When applying Kylin to Web pages, improvements due to shrinkage and retraining become even more apparent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Combining Kylin's extractions from Wikipedia and the Web yields a substantial improvement in recall without compromising precision. Already, shrink-retrain improved recall over the original Kylin system, here the baseline, but the combination of extractions from Wikipedia and the Web, shrink-retrain-Web, performs even better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 : Parent/children classes for shrinkage.</head><label>1</label><figDesc>We considered three strategies to determine the weights wij for aggregated data from parent / children classes: Uniform: wij = 1, which weights all training samples equally.Size Adjusted: wij = min{1, k |C|+1 },where k (10 in our experiments) is the design parameter, and |C| is the number of instance articles contained in C. The intuition is that the bigger C is, the less shrinkage should rely on other classes. Precision Directed: wij = pij , where pij is the extraction precision when applying the extractor for Ci.aj on the appropriate sentences from C-class articles and comparing them with existing infobox values.</figDesc><table><row><cell>Target class</cell><cell>Parent</cell><cell>Children</cell></row><row><cell>Irish newspaper(20)</cell><cell>Newspaper(1559)</cell><cell>-</cell></row><row><cell>Performer(44)</cell><cell>Person(1201)</cell><cell>Actor(8738)</cell></row><row><cell></cell><cell></cell><cell>Comedian(106)</cell></row><row><cell>Baseball stadium(163)</cell><cell>Stadium(1642)</cell><cell>-</cell></row><row><cell>Writer(2213)</cell><cell>Person(1201)</cell><cell>Sci-fi writer(36)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 : Accumulative AUC improvements.</head><label>2</label><figDesc></figDesc><table><row><cell cols="5">AUC improved(%) +Shrink +Retrain +Web Total</cell></row><row><cell>Irish news.(20)</cell><cell>1386</cell><cell>294</cell><cell>552</cell><cell>2232</cell></row><row><cell>Performer(44)</cell><cell>57</cell><cell>17</cell><cell>24</cell><cell>98</cell></row><row><cell>Baseball stad.(163)</cell><cell>17</cell><cell>23</cell><cell>62</cell><cell>102</cell></row><row><cell>Writer(2213)</cell><cell>7</cell><cell>9</cell><cell>80</cell><cell>96</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>It is possible that Wikipedia's inbred style stems from a pattern where one article is copied and modified to form another. A general desire for stylistic consistency is another explanation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>We note that another way of generating the set, T , would be to collect baseline Kylin extractions for C.a instead of using existing infoboxes. This would lead to a cotraining approach rather than simple retraining. One could iterate the process of getting more training date from TextRunner with improvements to the Kylin extractor<ref type="bibr" target="#b3">[4]</ref>.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGEMENTS</head><p>We thank Eytan Adar, Michelle Banko, Ivan Beschastnikh, Doug Downey, Oren Etzioni, Travis Kriplean, Cynthia Matuszek, David McDonald, Alan Ritter, Stefan Schoenmackers, Jue Wang, the UW KnowItAll and Wikipedia groups, and the anonymous reviewers for valuable conversations and suggestions. This work was supported by NSF grant IIS-0307906, ONR grant N00014-06-1-0147, SRI CALO grant 03-000225 and the WRF / TJ Cable Professorship.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Snowball: Extracting relations from large plain-text collections</title>
		<author>
			<persName><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gravano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth ACM International Conference on Digital Libraries</title>
		<meeting>the Fifth ACM International Conference on Digital Libraries</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What have Innsbruck and Leipzig in common? Extracting semantics from wiki content</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ESWC07</title>
		<meeting>ESWC07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open information extraction from the Web</title>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI07</title>
		<meeting>IJCAI07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining Labeled and Unlabeled Data with Co-Training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLT98</title>
		<meeting>COLT98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An analysis of the AskMSR question-answering system</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP02</title>
		<meeting>EMNLP02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gimme&apos; the context: Context-driven automatic semantic annotation with c-pankow</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cimiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ladwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW05</title>
		<meeting>WWW05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augmenting wikipedia with named entity tags</title>
		<author>
			<persName><forename type="first">W</forename><surname>Dakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cucerzan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCNLP 2008</title>
		<meeting>IJCNLP 2008</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semtag and Seeker: bootstrapping the Semantic Web via automated semantic annotation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Eiron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gruhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jhingran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanungo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tomlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW03</title>
		<meeting>WWW03</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Web question answering: Is more always better?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGIR02</title>
		<meeting>SIGIR02</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised named-entity extraction from the Web: An experimental study</title>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="134" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Internet encyclopaedias go head to head</title>
		<author>
			<persName><forename type="first">J</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">438</biblScope>
			<biblScope unit="page" from="900" to="901" />
			<date type="published" when="2005-12">December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling question answering to the Web</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM (TOIS)</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="242" to="262" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML01</title>
		<meeting>ICML01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">DIRT-discovery of inference rules from text</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD01</title>
		<meeting>KDD01</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for common sense: Populating Cyc from the Web</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kahlert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cabral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI05</title>
		<meeting>AAAI05</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving text classification by shrinkage in a hierarchy of classes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML98</title>
		<meeting>ICML98</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ontology-driven information extraction with ontosyphon</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cafarella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ISWC06</title>
		<meeting>ISWC06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using maximum entropy for text classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop on Machine Learning for Information Filtering</title>
		<meeting>Workshop on Machine Learning for Information Filtering</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Popular ensemble methods: An empirical study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="page" from="169" to="198" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effective information extraction with semantic affinity patterns and relevant regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Patwardhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP07</title>
		<meeting>EMNLP07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markov logic networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="107" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A corpus-based approach for building semantic lexicons</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shepherd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP97</title>
		<meeting>EMNLP97</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic taxonomy induction from heterogenous evidence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL06</title>
		<meeting>ACL06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inadmissibility of the usual estimator for the mean of a multivariate normal distribution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Berkeley Symposium on Mathematical Statistics and Probability)</title>
		<meeting>the 3rd Berkeley Symposium on Mathematical Statistics and Probability)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Yago: A core of semantic knowledge -unifying WordNet and Wikipedia</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW07</title>
		<meeting>WWW07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Autonomously semantifying Wikipedia</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings CIKM07</title>
		<meeting>CIKM07</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatically refining the wikipedia infobox ontology</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WWW08</title>
		<meeting>WWW08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
