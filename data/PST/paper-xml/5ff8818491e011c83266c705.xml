<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">I-SPY: Context-Driven Conditional Instruction Prefetching with Coalescing</title>
				<funder ref="#_dvrJCyw #_gDyf4a4">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Google</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">SRC</orgName>
				</funder>
				<funder>
					<orgName type="full">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Tanvir</surname></persName>
						</author>
						<author>
							<persName><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Akshitha</forename><surname>Sriraman</surname></persName>
							<email>akshitha@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Devietti</surname></persName>
							<email>devietti@cis.upenn.edu?gilles.a.pokam@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">University of Pennsylvania</orgName>
								<orgName type="institution" key="instit2">Intel Corporation</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gilles</forename><surname>Pokam</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Heiner</forename><surname>Litz</surname></persName>
							<email>hlitz@ucsc.edu</email>
						</author>
						<author>
							<persName><forename type="first">Baris</forename><surname>Kasikci</surname></persName>
							<email>barisk@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">I-SPY: Context-Driven Conditional Instruction Prefetching with Coalescing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/MICRO50266.2020.00024</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Prefetching</term>
					<term>frontend stalls</term>
					<term>memory systems</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern data center applications have rapidly expanding instruction footprints that lead to frequent instruction cache misses, increasing cost and degrading data center performance and energy efficiency. Mitigating instruction cache misses is challenging since existing techniques (1) require significant hardware modifications, (2) expect impractical on-chip storage, or (3) prefetch instructions based on inaccurate understanding of program miss behavior.</p><p>To overcome these limitations, we first investigate the challenges of effective instruction prefetching. We then use insights derived from our investigation to develop I-SPY, a novel profiledriven prefetching technique. I-SPY uses dynamic miss profiles to drive an offline analysis of I-cache miss behavior, which it uses to inform prefetching decisions. Two key techniques underlie I-SPY's design: (1) conditional prefetching, which only prefetches instructions if the program context is known to lead to misses, and (2) prefetch coalescing, which merges multiple prefetches of noncontiguous cache lines into a single prefetch instruction. I-SPY exposes these techniques via a family of light-weight hardware code prefetch instructions.</p><p>We study I-SPY in the context of nine data center applications and show that it provides an average of 15.5% (up to 45.9%) speedup and 95.9% (and up to 98.4%) reduction in instruction cache misses, outperforming the state-of-the-art prefetching technique by 22.5%. We show that I-SPY achieves performance improvements that are on average 90.5% of the performance of an ideal cache with no misses.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The expanding user base and feature portfolio of modern data center applications is driving a precipitous growth in their complexity <ref type="bibr" target="#b0">[1]</ref>. Data center applications are increasingly composed of deep and complex software stacks with several layers of kernel networking and storage modules, data retrieval, processing elements, and logging components <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>. As a result, code footprints are often a hundred times larger than a typical L1 instruction cache (I-cache) <ref type="bibr" target="#b4">[5]</ref>, and further increase rapidly every year <ref type="bibr" target="#b0">[1]</ref>.</p><p>I-cache misses are becoming a critical performance bottleneck due to increasing instruction footprints <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6]</ref>. Even modern out-of-order mechanisms do not hide instruction misses that show up as glaring stalls in the critical path of execution. Hence, reducing I-cache misses can significantly improve data center application performance, leading to millions of dollars in cost and energy savings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The importance of mechanisms that reduce I-cache misses (e.g., instruction prefetching) has long been recognized. Prior works have proposed next-line or history-based hardware instruction prefetchers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> and several software mechanisms have been proposed to perform code layout optimizations for improving instruction locality <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>. While these techniques are promising, they (1) demand impractical on-chip storage <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, (2) require significant hardware modifications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, or (3) face inaccuracies due to approximations used in computing a cache-optimal code layout <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>A recent profile-guided prefetching proposal, AsmDB <ref type="bibr" target="#b1">[2]</ref>, was able to reduce I-cache misses in Google workloads. However, we find that even AsmDB can fall short of an ideal prefetcher by 25.5% on average. To completely eliminate Icache misses, it is important to first understand: why do existing state-of-the-art prefetching mechanisms achieve sub-optimal performance? What are the challenges in building a prefetcher that achieves near-ideal application speedup?</p><p>To this end, we perform a comprehensive characterization of the challenges in developing an ideal instruction prefetcher. We find that an ideal instruction prefetcher must make careful decisions about <ref type="bibr" target="#b0">(1)</ref> what information is needed to efficiently predict an I-cache miss, <ref type="bibr" target="#b1">(2)</ref> when to prefetch an instruction, <ref type="bibr" target="#b2">(3)</ref> where to introduce a prefetch operation in the application code, and (4) how to sparingly prefetch instructions. Each of these design decisions introduces non-trivial trade-offs affecting performance and increasing the burden of developing an ideal prefetcher. For example, the state-of-the-art prefetcher, AsmDB, injects prefetches at link time based on application's miss profiles. However, control flow may not be predicted at link time or may diverge from the profile at run time (e.g., due to input dependencies), resulting in many prefetched cache lines that never get used and pollute the cache. Moreover, AsmDB suffers from static and dynamic code bloat due to additional prefetch instructions injected into the code.</p><p>In this work, we aim to reduce I-cache misses with I-SPY-a prefetching technique that carefully identifies I-cache misses, sparingly injects "code prefetch" instructions in suitable program locations at link time, and selectively executes injected prefetch instructions at run time. I-SPY proposes two novel mechanisms that enable on average 90.4% of ideal speedup: conditional prefetching and prefetch coalescing. Conditional prefetching. Prior techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> either prefetch excessively to hide more I-cache misses, or prefetch conservatively to prevent unnecessary prefetch operations that pollute the I-cache. To hide more I-cache misses as well as to reduce unnecessary prefetches, we propose conditional prefetching, wherein we use profiled execution context to inject code prefetch instructions that cover each miss, at link time. At run-time, we reduce unnecessary prefetches by executing an injected prefetch instruction only when the miss-inducing context is observed again.</p><p>To implement conditional prefetching with I-SPY, we propose two new hardware modifications. First, we propose simple CPU modifications that use Intel's Last Branch Record (LBR) <ref type="bibr" target="#b21">[22]</ref> to enable a server to selectively execute an injected prefetch instruction based on the likelihood of the prefetch being successful. We also propose a "code prefetch" instruction called Cprefetch that holds miss-inducing context information in its operands, to enable an I-SPY-aware CPU to conditionally execute the prefetch instruction. Prefetch coalescing. Whereas conditional prefetching facilitates eliminating more I-cache misses without prefetching unnecessarily at run time, it can still inject too many prefetch instructions that might further increase the static code footprint. Since data center applications face significant I-cache misses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7]</ref>, injecting even a single prefetch instruction for each I-cache miss can significantly increase an already-large static code footprint. To avoid a significant code footprint increase, we propose prefetch coalescing, wherein we prefetch multiple cache lines with a single instruction. We find that several applications face I-cache misses from non-contiguous cache lines, i.e., in a window of N lines after a miss, only a subset of the N lines will incur a miss. We propose a new instruction called Lprefetch to prefetch these noncontiguous cache lines using a single instruction.</p><p>We study I-SPY in the context of nine popular data center applications that face frequent I-cache misses. Across all applications, we demonstrate an average performance improvement of 15.5% (up to 45.9%) due to a mean 95.9% (up to 98.4%) L1 I-cache miss reduction. We also show that I-SPY improves application performance by 22.4% compared to the state-of-theart instruction prefetcher <ref type="bibr" target="#b1">[2]</ref>. I-SPY increases the dynamicallyexecuted instruction count by 5.1% on average and incurs an 8.2% mean static code footprint increase.</p><p>In summary, we make the following contributions: ? A detailed analysis of the challenges involved in building a prefetcher that provides close-to-ideal speedups. ? Conditional prefetching: A novel profile-guided prefetching technique that accurately identifies miss-inducing program contexts to prefetch I-cache lines only when needed. ? Prefetch coalescing: A technique that coalesces multiple non-contiguous cache line prefetches based on run-time information obtained from execution profiles. ? I-SPY: An end-to-end system that combines conditional prefetching with prefetch coalescing using a new family of instructions to achieve near-ideal speedup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. UNDERSTANDING THE CHALLENGES OF INSTRUCTION PREFETCHING</head><p>In this section, we present a detailed characterization of the challenges in developing an ideal instruction prefetching technique. We define an ideal prefetcher as one that achieves the performance of an I-cache with no misses, i.e., where every Frontend-bound Fig. <ref type="figure">1</ref>: Several widely-used data center applications spend a significant fraction of their pipeline slots on "Frontend-bound" stalls, waiting for I-cache misses to return (measured using the Top-down methodology <ref type="bibr" target="#b22">[23]</ref>).</p><p>access hits in the L1 I-cache (a theoretical upper bound). We characterize prefetching challenges by exploring four important questions: (1) What information is needed to efficiently predict an I-cache miss?, (2) When must an instruction be prefetched to avoid an I-cache miss? (3) Where should a prefetcher inject a code prefetch instruction in the program?, and (4) How can a prefetcher sparingly prefetch instructions while still eliminating most I-cache misses? We characterize challenges using nine popular real-world applications that exhibit significant I-cache misses. In Fig. <ref type="figure">1</ref>, we show the "frontend" pipeline stalls that the nine applications exhibit when waiting for I-cache misses to return. We observe that these data center applications can spend 23% -80% of their pipeline slots in waiting for I-cache misses to return. Hence, we include these applications in our study.</p><p>From Facebook's HHVM OSS-performance <ref type="bibr" target="#b23">[24]</ref> benchmark suite, we analyze (1) Drupal: a PHP content management system, <ref type="bibr" target="#b1">(2)</ref> Mediawiki: an open-source Wiki engine, and (3) Wordpress: a PHP-based content management system used by services such as Bloomberg Professional and Microsoft News. From the Java DaCapo <ref type="bibr" target="#b24">[25]</ref> benchmark suite, we analyze (a) Cassandra <ref type="bibr" target="#b25">[26]</ref>: a NoSQL database management system used by companies such as Instagram and Netflix, (b) Kafka: Apache's stream-processing software platform used by companies such as Uber and Linkedin, and (c) Tomcat <ref type="bibr" target="#b26">[27]</ref>: Apache's implementation of the Java Servlet and WebSocket. From the Java Renaissance <ref type="bibr" target="#b27">[28]</ref> benchmark suite, we analyze Finagle-Chirper and Finagle-HTTP <ref type="bibr" target="#b28">[29]</ref>: Twitter Finagle's micro-blogging service and HTTP server, respectively. We also study Verilator <ref type="bibr" target="#b29">[30]</ref>, a tool used by cloud companies to simulate custom hardware designs. We describe our complete experimental setup and simulation parameters in Sec. V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. What Information is Needed to Efficiently Predict an I-Cache Miss?</head><p>An ideal prefetcher must predict all I-cache misses before they occur, to prefetch them into the I-cache in time. To this end, prior work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> (e.g., next-in-line prefetching) has shown that an I-cache miss can be predicted using the program instructions executed before the miss. Since any arbitrary instruction (e.g., direct/indirect branches or function returns) could execute before a miss, the application's dynamic control flow must be tracked to predict a miss using the program paths that lead to it. An application's execution can be represented by a dynamic Control Flow Graph (CFG). In a dynamic CFG, the nodes represent basic blocks (sequence of instructions without a branch) and the edges represent branches. Fig. <ref type="figure" target="#fig_1">2</ref> shows a dynamic CFG, where the cache miss at basic block K can be reached via various paths. The CFG's edges are typically weighted by a branch's execution count. For brevity, we assume all the weights are equal to one in this example. Software-driven prefetchers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19</ref>] construct an application's dynamic CFG and identify miss locations that can be eliminated using a suitable prefetch instruction. For example, AsmDB <ref type="bibr" target="#b1">[2]</ref> uses DynamoRIO's <ref type="bibr" target="#b30">[31]</ref> memory trace client to capture an application's dynamic CFG for locating I-cache misses in the captured trace. Unfortunately, DynamoRIO <ref type="bibr" target="#b30">[31]</ref> incurs undue overhead <ref type="bibr" target="#b31">[32]</ref>, making it costly to deploy in production. To efficiently generate miss-annotated dynamic CFGs, we propose augmenting dynamic CFG traces from Intel's LBR <ref type="bibr" target="#b21">[22]</ref> with L1 I-cache miss profiles collected with Intel's Precise Event Based Sampling (PEBS) <ref type="bibr" target="#b32">[33]</ref> performance counters. Generating dynamic CFGs using such lightweight monitoring enables profiling applications in production.</p><p>Observation: Representing a program's execution using a dynamic CFG and augmenting it with L1 I-cache miss profiles enables determining prefetch candidates. Insight: Generating a lightweight miss-annotated dynamic CFG using Intel's LBR and PEBS incurs low run-time performance overhead and enables predicting miss locations in production systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. When To Prefetch an Instruction?</head><p>A prefetch is successful only if it is timely. In the dynamic CFG in Fig. <ref type="figure" target="#fig_1">2</ref>, a prefetch instruction injected at predecessor basic blocks H or I is too late: the prefetcher will not be able to bring the line into the I-cache in time and a miss will occur at K. In contrast, if a prefetch instruction is injected at predecessors E or F, the prefetched line may not be needed soon enough, and it may (1) either evict other lines that will be accessed sooner, or (2) itself get prematurely evicted before it is accessed. Instead, the prefetch must be injected in an appropriate prefetch window. In our example, we assume block G is a timely injection candidate in the prefetch window.</p><p>Prior work <ref type="bibr" target="#b1">[2]</ref> empirically determines an ideal prefetch window using average application-specific IPC to inject a prefetch instruction that hides a cache miss. I-SPY relies on this approach and injects prefetch instructions 27 -200 cycles before a miss, a window we determine in our evaluation.</p><p>Observation: An instruction must be prefetched in a timely manner to avoid a miss. Insight: Empirically determining the prefetch window such that a prefetch is not too early or too late, can effectively eliminate a miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Where to Inject a Prefetch?</head><p>An ideal prefetcher would eliminate all I-cache misses, achieving full miss coverage. To achieve full miss coverage, a prefetcher such as the one proposed by Luk and Mowry <ref type="bibr" target="#b20">[21]</ref>, might inject a "code prefetch" instruction into every basic block preceding an I-cache miss. However, the problem of this approach is that due to dynamic control flow changes, naively injecting a prefetch into a predecessor basic block causes a high number of inaccurate prefetches whenever the predecessor does not lead to the miss. Prefetching irrelevant lines hurts prefetch accuracy (the fraction of useful prefetches) and leads to I-cache pollution, degrading application performance.</p><p>Prefetch accuracy can be improved by assessing the usefulness of a prefetch and by restricting the injection of prefetches to those that are likely to improve performance. To determine the likelihood of a prefetch being useful, we can analyze the fan-out of the prefetch's injection site. We define fan-out as the percentage of paths that do not lead to a target miss from a given injection site. For example, in Fig. <ref type="figure" target="#fig_1">2</ref>, the candidate injection site G has a fan-out of 75% as only one out of four paths leads to the miss K.</p><p>By limiting prefetch injection to nodes whose fan-out is below a certain threshold, accuracy can be improved, however, coverage is also reduced. The fan-out threshold that decides whether to inject a prefetch represents a control knob to tradeoff coverage vs. accuracy. To determine this threshold, Fig. <ref type="figure" target="#fig_2">3</ref> analyzes the impact of fan-out on accuracy and coverage for the wordpress application. As it can be seen, for real applications with large CFGs, a high fan-out of 99% is required to achieve the best performance, although accuracy starts to drop sharply at this point. Hence, prior works (including AsmDB) that rely on static analysis for injecting prefetches fall short of achieving close to ideal performance (65% in the case of wordpress).</p><p>With I-SPY, we aim to break this trade-off by optimizing for prefetch accuracy and miss coverage simultaneously. To this end, we propose context sensitive conditional prefetching, a technique that statically injects prefetches to cover each miss (i.e., high miss coverage), but dynamically executes injected prefetches only when the prefetch is likely to be successful, minimizing unused prefetches (i.e., high prefetch accuracy). In Section III-A, we describe our conditional prefetching  technique and our approach that leverages dynamic context information to decide whether to execute a prefetch or not.</p><p>Observation: It is challenging to achieve both high miss coverage and prefetch accuracy if we determine prefetch injection candidate blocks based on a static CFG analysis alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insight: Leveraging dynamic run-time information to conditionally execute statically-injected prefetch instructions can help improve both miss coverage and prefetch accuracy. D. How to Sparingly Prefetch Instructions?</head><p>Several profile-guided prefetchers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> require at least one code prefetch instruction to mitigate an I-cache miss. For example, the state-of-the-art prefetcher, AsmDB <ref type="bibr" target="#b1">[2]</ref>, covers each miss by injecting a prefetch instruction into a high fan-out (?99%) predecessor. However, statically injecting numerous prefetch instructions and executing them at run time, increases the static and dynamic application code footprint by 13.7% and 7.3% respectively, as portrayed in Fig. <ref type="figure" target="#fig_3">4</ref>. An increase in static and dynamic code footprints can pollute the I-cache and cause unnecessary cache line evictions, further degrading application performance. Hence, it is critical to sparingly prefetch instructions to minimize code footprints. Prefetch coalescing. Our conditional prefetching proposal allows statically injecting more prefetch instructions to eliminate more I-cache misses, without having to dynamically perform inaccurate prefetches. However, a large number of statically-injected code prefetch instructions can still increase an application's static code footprint.</p><p>A na?ve approach to statically inject fewer instructions is to leverage the spatial locality of I-cache misses to prefetch multiple contiguous cache lines with a single prefetch instruction rather than a single line at a time <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. In contrast, another approach <ref type="bibr" target="#b2">[3]</ref> finds value in prefetching multiple noncontiguous cache lines together. Similarly, we posit that it is unlikely that all the contiguous cache lines in a window of n lines after a given miss will incur misses. It is more likely that a subset of the next-n lines will incur misses, whereas others will not. To validate this hypothesis, we consider a window of eight cache lines immediately following a miss to implement two prefetchers: (1) Contiguous-8, that prefetches all eight contiguous cache lines after a miss and (2) Non-contiguous-8, that prefetches only the missed cache lines in the eight cache line window.</p><p>We profile all our benchmarks to detect I-cache misses and measure the speedup achieved by both prefetchers in Fig. <ref type="figure">5</ref>. We find that Non-contiguous-8 provides an average 7.6% speedup over Contiguous-8. We conclude that prefetch coalescing of non-contiguous, but spatially nearby I-cache misses, via a single prefetch instruction can improve performance while minimizing the number of static and dynamic prefetch instructions. We note that our conclusion holds for larger windows of cache lines (e.g., <ref type="bibr">16 and 32)</ref>. We find that a window of eight lines offers a good trade-off between speedup and circuit complexity required to support a larger window size. We provide a sensitivity analysis for window sizes in ?VI-B.</p><p>Observation: Injecting too many prefetch instructions can increase static and dynamic code footprints, inducing additional cache line evictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Insight: Conditional prefetching can minimize dynamic code footprints; coalescing spatially-near non-contiguous I-cache miss lines into a single prefetch instruction can minimize both static and dynamic code footprints.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. I-SPY</head><p>I-SPY proposes two novel techniques to improve profileguided instruction prefetching. I-SPY introduces conditional prefetching to address the dichotomy between high coverage and accuracy discussed in ?II-C. Furthermore, I-SPY proposes prefetch coalescing to reduce the static code footprint increase due to injected prefetch instructions explored in ?II-D. I-SPY relies on profile-guided analysis at link-time to determine frequently missing blocks and prefetch injection sites using Intel LBR <ref type="bibr" target="#b21">[22]</ref> and PEBS <ref type="bibr" target="#b32">[33]</ref>. We provide a detailed description of I-SPY's usage model in ?IV. I-SPY also introduces minor hardware modifications to improve prefetch efficiency at run time. As a result, our proposed techniques close the gap between static and dynamic prefetching by combining the performance of dynamic hardware-based mechanisms with the low complexity of static software prefetching schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conditional Prefetching</head><p>Conditionally executing prefetches has a two-fold benefit: I-SPY can liberally inject conditional prefetch instructions to cover each miss (i.e., achieve high miss coverage) while simultaneously minimizing unused prefetches (i.e., achieve high accuracy). I-SPY uses the execution context to decide whether to conditionally execute a prefetch or not. We first discuss how I-SPY computes contexts leading to misses. We then explain how I-SPY 's conditional prefetching instruction is implemented, and finally discuss micro-architectural details. Miss context discovery. Similar to many other branch prediction schemes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>, I-SPY uses the basic block execution history to compute the execution context. Initially, we attempted to use the exact basic block sequence to predict a miss. However, we found this approach intractable since the number of block sequences (i.e., the number of execution paths) leading to a miss grows exponentially with the increase in sequence length. As a result, I-SPY only considers the presence of certain important basic blocks in the recent context history to inform its prefetching decisions. This approach is in line with prior work <ref type="bibr" target="#b35">[36]</ref> that observes that prediction accuracy is largely insensitive to the basic block order sequence.</p><p>We use the dynamic CFG in Fig. <ref type="figure" target="#fig_1">2</ref> to describe the miss context discovery process. Recall that in this example, the miss occurs in basic block K and block G is the injection site in the prefetch window. As shown in Fig. <ref type="figure" target="#fig_5">6a</ref>, there are six execution paths including the candidate injection site G and two of these paths lead to the basic block K, where the miss occurs.</p><p>I-SPY starts miss context discovery by identifying predictor basic blocks-blocks with the highest frequency of occurrence in the execution paths leading to each miss. In our example, B and E are predictor blocks. Since I-SPY only relies on the presence of blocks to identify the context (as opposed to relying on the order of blocks), it computes combinations of predictor blocks as potential contexts for a given miss. Then, I-SPY calculates the conditional probability of each context leading to a miss in a block B, i.e., P(Miss in Block "B"|context) as per the Bayes theorem. As shown in Fig. <ref type="figure" target="#fig_5">6b,</ref><ref type="figure">I</ref> I-SPY then selects the combination with the highest probability as the context for a given miss. In our example, this context, namely (B and E) will be encoded into the conditional prefetch instruction injected at G. At run time, the conditional prefetch will be executed if the run-time branch history contains the recorded context. We now detail I-SPY's conditional prefetch instruction. Conditional prefetch instruction. We propose a new prefetch instruction, Cprefetch that requires an extra operand to specify the execution context. Each basic block in the context is identified by its address, i.e., the address of the first instruction in the basic block. I-SPY computes the basic block address using the LBR data.</p><p>To reduce the code size of Cprefetch, I-SPY hashes the individual basic block addresses in the context into an n-byte immediate operand (context-hash) using hash functions, FNV-1 <ref type="bibr" target="#b36">[37]</ref> and MurmurHash3 <ref type="bibr" target="#b37">[38]</ref>. When a Cprefetch is executed at run time, the processor recomputes a hash value (runtime-hash) using the last 32 predecessor basic blocks (Intel LBR <ref type="bibr" target="#b21">[22]</ref> provides the addresses of 32 most recently executed basic blocks), and compares it against the context-hash. The prefetch operation is performed only if the set-bits in context-hash are a subset of the set-bits in the runtime-hash.</p><p>Both runtime-hash and context-hash are compressed representations of multiple basic block addresses. While compressing multiple 64 bit basic block addresses into fewer bits reduces the code bloat, it might also introduce  I-SPY extends the LBR to maintain a rolling runtime-hash of its contents. Fig. <ref type="figure" target="#fig_6">7</ref> shows the microarchitectural requirements of I-SPY's context-sensitive prefetch instruction for 32 predecessor basic blocks and a 16 bit context-hash. Since the LBR is a FIFO, we maintain the runtime-hash incrementally. Using a counting Bloom filter <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, we assign a 6-bit counter to each of the 16 bits of the runtime-hash (96 bits in total). Whenever a new entry is added into the LBR, we hash the corresponding block address and increment the corresponding counters in the runtime-hash; the counters for the hash of the evicted LBR entry are decremented. The counters never overflow and the runtime-hash precisely tracks the LBR contents since there are only ever 32 branches recorded in the runtime-hash. We also add a small amount of logic to reduce each counter to a single "is-zero" bit; in those 16 bits, we check if the context-hash bits are a subset of the runtime-hash. If they are, the prefetch fires, otherwise it is disabled.</p><p>To clarify how Bloom filters help I-SPY match runtime-hash to context-hash, let's consider the same example in Fig. <ref type="figure" target="#fig_5">6</ref>. Let's assume the 16-bit hashes of B and E are 0x2 and 0x10, respectively. Therefore, the context-hash would be 0x12, where the Least Significant Bits (LSB) 1 and 4 are set. To enable prefetching, runtime-hash must also have these bits set. At run time, if B is present in the last 32 predecessors, the bloom filter counter corresponding to LSB-1 must be greater than 0. Similarly for E, the counter corresponding to LSB-4 must be greater than 0. Hence, the Fig. <ref type="figure">8</ref>: An example of I-SPY's prefetch coalescing process result of subset comparison between context-hash and runtime-hash will be true and a prefetch will be triggered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prefetching Coalescing</head><p>Conditional prefetching enables high-accuracy prefetching. Nevertheless, it leads to static code bloat as every prefetch instruction increases the size of the application's text segment. Prefetch coalescing reduces the static code bloat as well as the number of dynamically-executed prefetch instructions by combining multiple prefetches into a single instruction. We first describe how I-SPY decides which lines should be coalesced, followed by details of I-SPY's coalesced prefetching instruction. We then detail the micro-architectural modifications required to support prefetch coalescing.</p><p>To perform coalescing, I-SPY analyzes all prefetch instructions injected into a basic block and groups them by context. As shown in Fig. <ref type="figure">8</ref>, prefetches for addresses 0xA and 0xD are grouped together since they are conditional on the same context, C0. Similarly, 0x4, 0x2, and 0x7 are grouped together since they share the same context C1.</p><p>Next, I-SPY attempts to merge a group of prefetch instructions into a single prefetch instruction. I-SPY uses an n-bit bitmap to select a subset of cache lines within a window of n consecutive cache lines. In the example shown in Fig. <ref type="figure">8</ref>, the coalesced prefetch for context C1 has two bits set in the bitmask to encode lines 0x4 and 0x7 where the base address of the prefetch is 0x2. While a larger bitmask allows coalescing more prefetches, it also increases hardware complexity. We study the effect of bitmask size in Fig. <ref type="figure" target="#fig_13">17</ref>. Coalesced prefetch instruction. Our proposed coalesced prefetch instruction, Lprefetch, requires an additional operand for specifying the coalescing bit-vector. Prefetch instructions in current hardware (e.g., prefetcht * on x86 and pli on ARM) follow the format, (prefetch, address), which takes address as an operand and prefetches the cache line corresponding to address. Lprefetch takes an extra operand, bit-vector. The prefetcht * instruction on x86 has a size of 7 bytes, hence, with the addition of an n = 8 bits bitmask, Lprefetch has a size of 8 bytes.</p><p>I-SPY combines prefetch coalescing and conditional prefetching via another instruction, CLprefetch, with the format (prefetch, address, context-hash, bit-vector) as shown in Fig. <ref type="figure">8</ref>. CLprefetch prefetches all the prefetch targets specified by bit-vector only if the current context matches with the context encoded in the context-hash. Micro-architectural modifications. Coalesced prefetch instructions require minor micro-architectural modifications that mainly consists of a series of simple incrementers. These incrementers decode the 8-bit coalescing vector and enable prefetching up to 9 cache lines (the initial prefetch target, plus up to 8 bit-vector-dependent targets). The resultant cache line addresses are then forwarded to the prefetch engine.</p><p>Replacement policy for prefetched lines. I-SPY's prefetch instructions also update the replacement policy priority of the prefetched cache line. Instead of assigning the highest priority to the prefetched cache line (as done for demand-loads), I-SPY's prefetch instructions assign the prefetched cache line a priority equal to the half of the highest priority. I-SPY's goal with this policy is to reduce the adverse effects of a potentially inaccurate prefetch operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. USAGE MODEL</head><p>We provide an overview of the high-level usage model of I-SPY in Fig. <ref type="figure" target="#fig_8">9</ref>. I-SPY profiles an application's execution at run time, and uses these profiles to perform an offline analysis of I-cache misses to suitably inject code prefetch instructions. Online profiling. I-SPY first profiles an application's execution at run time (step 1 ). It uses Intel's LBR <ref type="bibr" target="#b21">[22]</ref> to construct a dynamic CFG (such as the one shown in Fig. <ref type="figure" target="#fig_1">2</ref>), and augments the dynamic CFG with L1 I-cache miss profiles collected with Intel's PEBS <ref type="bibr" target="#b32">[33]</ref> hardware performance counters. At every Icache miss, I-SPY records the program counters of the previous 32 branches that the program executed (on x86 64, LBR has a 32-entry limit). Run-time profiling using Intel LBR's and Intel PEBS's lightweight monitoring <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41]</ref> enables profiling applications online, in production. Offline analysis. Next, I-SPY performs an offline analysis ( 2 ) of the miss-annotated dynamic CFG that it generates at run time. For each miss, I-SPY considers all predecessor basic blocks within the prefetch window. Unlike prior work <ref type="bibr" target="#b1">[2]</ref>, I-SPY does not require the per-application IPC metric to find predecessors within the prefetch window as the LBR profile already includes dynamic cycle information for each basic block. Apart from this, the algorithm to find the best prefetch injection site is similar to prior work <ref type="bibr" target="#b1">[2]</ref> and has a worst-case complexity of O(n log n).</p><p>After finding the best prefetch injection site to cover each miss, I-SPY runs two extra analyses, context discovery and prefetch coalescing. First, if the prefetch injection site has a non-zero fan-out, I-SPY analyzes the predecessors of the injection site to reduce its fan-out (Fig. <ref type="figure" target="#fig_5">6</ref>). Next, if the same injection site is selected for prefetching multiple cache lines, I-SPY applies prefetch coalescing to reduce the number of prefetch instructions (Fig. <ref type="figure">8</ref>).</p><p>Once I-SPY finishes identifying opportunities for conditional prefetching and prefetch coalescing, it injects appropriate prefetch instructions to cover all misses. Specifically, I-SPY injects four kinds of prefetch instructions ( 3 ).</p><p>If the context of a given prefetch instruction differs from the contexts of all other prefetch instructions, then this prefetch instruction cannot be coalesced with others. In that case, I-SPY injects a Cprefetch instruction.</p><p>Conditionally prefetching a line based on the execution context may not improve the prefetch accuracy. In this case, I-SPY will try to inject an Lprefetch instruction. If multiple cache lines are within a range of n lines (where n is the size of bit-vector used to perform coalescing as in ?III-B) from the nearest prefetch target, I-SPY will inject an Lprefetch. Otherwise, I-SPY will inject multiple AsmDB-style prefetch instructions that simply prefetch a single target cache line.</p><p>If conditional prefetching improves prefetching accuracy and multiple cache lines can be coalesced, I-SPY injects CLprefetch instructions.</p><p>The new binary updated with code prefetch instructions is deployed on I-SPY-aware data center servers that can conditionally execute and (or) coalesce the injected prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION METHODOLOGY</head><p>We envision an end-to-end I-SPY system that uses application profile information and our proposed family of hardware code prefetch instructions. We evaluate I-SPY using simulation since existing server-class processors do not support our proposed hardware modifications for conditional prefetching and prefetch coalescing. Additionally, simulation enables replaying memory traces to conduct limit studies and compare I-SPY's performance against an ideal prefetch mechanism. We prototype the state-of-the-art prefetcher, AsmDB <ref type="bibr" target="#b1">[2]</ref>, and compare I-SPY against it. We now describe (1) the experimental setup that we use to collect an application's execution profile, (2) our simulation infrastructure, (3) I-SPY's system parameters, and (4) the data center applications we study. Data collection. During I-SPY's offline phase, we use Intel's LBR <ref type="bibr" target="#b21">[22]</ref> and PEBS counters <ref type="bibr" target="#b41">[42]</ref> (more specifically (frontend_retired.l1i_miss)) to collect an application's execution profile and L1 I-cache miss information. We record up to 100 million instructions executed in steady-state. We combine our captured miss profiles and instruction traces to construct an application's miss-annotated dynamic CFG.</p><p>Simulation. We use the ZSim simulator <ref type="bibr" target="#b42">[43]</ref> to evaluate I-SPY. We modify ZSim <ref type="bibr" target="#b42">[43]</ref> to support conditional prefetching and prefetch coalescing. We use ZSim in a trace-driven execution mode, modeling an out-of-order processor. The detailed system parameters are summarized in Table <ref type="table" target="#tab_1">I</ref>. Additionally, we extend ZSim to support our family of hardware code prefetch instructions. Our implemented code prefetch instructions insert prefetched cache lines with a lower replacement policy priority than any demand load requests.</p><p>System parameters. Based on the sensitivity analysis (see Fig. <ref type="figure" target="#fig_14">18</ref>), we use 27 cycles as minimum prefetch distance, and 200 cycles as maximum prefetch distance. Additionally, we empirically determine that coalescing non-contiguous prefetches that occur within a cache line window of 8 cache lines yields the best performance.</p><p>Data center applications. We evaluate nine popular data center applications described in Sec. II. We allow an application's binary to be built with classic compiler code layout optimizations such as in-lining <ref type="bibr" target="#b43">[44]</ref>, hot/cold splitting <ref type="bibr" target="#b44">[45]</ref>, or profileguided code alignment <ref type="bibr" target="#b18">[19]</ref>. We study these applications with different input parameters offered to the client's load generator (e.g., number of requests per second or the number of threads).</p><p>Evaluation metrics. We use six evaluation metrics to evaluate I-SPY's effectiveness. First, we compare I-SPY's performance improvement against an ideal cache and AsmDB. Second, we study how well I-SPY reduces L1 I-cache MPKI compared to the state-of-the-art prefetcher, AsmDB <ref type="bibr" target="#b1">[2]</ref>. Third, we analyze how much performance improvement stems from conditional prefetching and prefetch coalescing, individually. Fourth, we compare I-SPY's prefetch accuracy with AsmDB. Fifth, we analyze the static and dynamic code footprint increase induced by I-SPY. Sixth, we determine whether I-SPY achieves high performance across various application inputs. Since, data center applications often run continuously, application inputs can drastically vary (e.g., diurnal load trends or load transients <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>). Hence, a profile-guided optimization for data center applications must be able to improve performance across diverse inputs. We also perform a sensitivity analysis of I-SPY's system parameters by evaluating the effect of varying the (1) number of predecessors in context-hash, (2) minimum and maximum prefetch distances, (3) coalescing size, and (4) context size used to conditionally prefetch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>In this section, we evaluate how I-SPY improves application performance compared to an ideal cache implementation and the state-of-the-art prefetcher <ref type="bibr" target="#b1">[2]</ref>, AsmDB, using the evaluation metrics defined in ?V. We then perform sensitivity studies to determine the effect of varying I-SPY's configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. I-SPY: Performance Analysis</head><p>Speedup. We first evaluate the speedup achieved by I-SPY across all applications. In Fig. <ref type="figure" target="#fig_9">10</ref>, we show I-SPY's speedup (green bars) compared against an ideal cache that faces no misses (brown bars) and AsmDB <ref type="bibr" target="#b1">[2]</ref> (blue bars).</p><p>We find that I-SPY attains a near-ideal speedup, achieving an average speedup that is 90.4% (up to 96.4%) of an ideal cache that always hits in the L1 I-cache. I-SPY falls slightly short of an ideal cache since (1) it executes more instructions due to the injected prefetch instructions and (2) a previously unobserved execution context might not trigger a prefetch, precipitating a miss. Additionally, I-SPY outperforms AsmDB by 22.4% on average (up to 41.2%), since it eliminates more I-cache misses than AsmDB as we show next. L1 I-cache MPKI reduction. We next evaluate how well I-SPY reduces L1 I-cache misses compared to AsmDB <ref type="bibr" target="#b1">[2]</ref> in Fig. <ref type="figure">11</ref>. We evaluate across all nine applications.</p><p>We observe that I-SPY achieves a high miss coverage, reducing L1 I-cache MPKI by an average of 95.8% across all applications. Furthermore, I-SPY reduces MPKI compared to AsmDB by an average of 15.7% across all applications (the greatest improvement is 28.4% for verilator). The MPKI reduction is due to conditionally executing prefetches and coalescing them, thereby eliminating more I-cache misses. In contrast, AsmDB executes a large number of unused prefetches that evict useful data from the cache.</p><p>Performance of conditional prefetching and prefetch coalescing. In Fig. <ref type="figure" target="#fig_1">12</ref>, we quantify how much I-SPY's conditional prefetching and prefetch coalescing mechanisms contribute to net application speedup. We show the performance improvement achieved by these novel mechanisms over AsmDB, across all nine applications. We make two observations. First, we note that both conditional prefetching and prefetch coalescing provide gains over AsmDB across all applications. Conditional prefetching improves performance more than coalescing for eight of our applications, since it covers more Icache misses with better accuracy. In verilator, we observe that coalescing offers a better performance since 75% of verilator's misses have a high spatial locality even within a cache line window of 8 lines.</p><p>Second, we find that the performance gains achieved by conditional prefetching and prefetch coalescing are not strictly additive. As I-SPY only coalesces prefetches that have the same condition, many prefetch instructions that depend on different conditions are not coalesced. Yet, combining both techniques offers better speedup than their individual counterparts. Prefetch accuracy. We portray the prefetch accuracy achieved by I-SPY across all nine applications in Fig. <ref type="figure" target="#fig_2">13</ref>. We also compare I-SPY's prefetch accuracy against AsmDB.</p><p>We find that I-SPY achieves an average of 80.3% prefetch accuracy. Furthermore, I-SPY's accuracy is 8.2% (average) better than AsmDB's accuracy, since I-SPY's conditional prefetching avoids trading off prefetch accuracy for miss coverage, unlike AsmDB. Static and dynamic code footprint increase. We next evaluate by how much I-SPY increases applications' static and dynamic code footprints. First, we illustrate the static code footprint increase induced by I-SPY in Fig. <ref type="figure" target="#fig_3">14</ref>. We also compare against AsmDB's static code footprint. We observe that I-SPY increases the static code footprint by 5.1% -9.5% across all applications. By coalescing multiple prefetches into a single prefetch instruction, I-SPY introduces fewer prefetch instructions into the application's binary. In contrast, we find that AsmDB increases the static code footprint much more starkly-7.6% -15.1%.</p><p>Next, we study by how much I-SPY increases the dynamic application footprint in Fig. <ref type="figure">15</ref> across all nine applications. We note that I-SPY executes 3.7% -7.2% additional dynamic instructions since it covers I-cache misses by executing injected code prefetch instructions. We observe that AsmDB has a higher dynamic instruction footprint across eight applications (ranging from 5.5% -11.6%), since it does not coalesce prefetches like I-SPY. For verilator, I-SPY's dynamic footprint is higher than AsmDB since I-SPY covers 28.4% more misses than AsmDB by executing more prefetch instructions, while also providing 35.9% performance improvement over AsmDB. Generalization across application inputs. To determine whether I-SPY achieves a performance improvement with an application input that is different from the profiled input, we characterize I-SPY's performance for five different inputs fed to three of our applications-drupal, mediawiki, wordpress (Fig. <ref type="figure" target="#fig_5">16</ref>). We choose these three applications, because they have the greatest variety of readily-available test inputs that we can run. We compare I-SPY against AsmDB in terms of ideal cache performance. We observe that I-SPY achieves a speedup that is closer to the ideal speedup than the speedup provided by AsmDB across all test inputs. I-SPY is more resilient to the input changes than AsmDB because of conditional prefetching. I-SPY achieves at least 70% (up to 86.84%) of ideal cache performance on inputs that are different from the profiled input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. I-SPY: Sensitivity Analysis</head><p>We next evaluate how I-SPY's performance varies in response to variations of the different system parameters. Number of predecessors comprising the context. In Fig. <ref type="figure" target="#fig_13">17</ref>, we observe how the I-SPY conditional prefetching's performance varies in response to a variation in the number of predecessors comprising the context condition (see Sec III-A). We vary predecessor counts from 1 to 32 (with a geometric progression of 2) and show the I-SPY conditional prefetching's average performance improvement across all nine applications.</p><p>We find that the I-SPY conditional prefetching's performance improves with an increase in the number of predecessors composing the context condition. Using more predecessors enables a more complete context description, and slightly improves performance by predicting I-cache misses more accurately. However, a large number of predecessors impose a significant context-discovery computation overhead. Specifically, the search space of possible predecessor candidates grows exponentially with the number of predecessors comprising the context condition. Consequently, the context discovery  process takes tens of minutes to complete with more than 4 predecessors, which can be a bottleneck in the build process. Since I-SPY's conditional prefetching achieves more than 85% of ideal cache performance even with four predecessors, I-SPY's design uses four predecessors to define context and keeps the computational overhead of context discovery low. Minimum and maximum prefetch distance. We next analyze how I-SPY's performance varies with an increase in the minimum and maximum prefetch distances, in Fig. <ref type="figure" target="#fig_14">18</ref>. We observe that I-SPY achieves maximum performance for a minimum prefetch distance of 20-30 cycles (which is greater than typical L2 access latency but less than L3 access latency). On the other hand, an increase in the maximum prefetch distance always improves I-SPY's performance. However, the increase is very slow after 200 cycles. Based on these results, we use 27 cycles as the minimum prefetch distance, and 200 cycles as the maximum prefetch distance for I-SPY. Coalescing size. We next study the sensitivity of I-SPY's prefetch coalescing to the coalesce bitmask size (see ?III-B) in Fig. <ref type="figure" target="#fig_15">19</ref>. We vary the coalesce bitmask size from 1 bit to 64 bits, prefetching up to 2 and 65 cache lines using a single instruction, respectively. We then measure the percentage of ideal speedup achieved by I-SPY's prefetch coalescing as an average across all applications.</p><p>We note that I-SPY's performance improves slightly with a larger bitmask, since larger bitmasks enable coalescing more cache lines, reducing spurious evictions. However, a large  bitmask will introduce hardware design complexities since the microarchitecture must now support additional in-flight prefetch operations. Similar to prior work <ref type="bibr" target="#b2">[3]</ref>, to minimize hardware complexity, we design I-SPY with an 8-bit coalescing bitmask, since it can be implemented with minor hardware modifications (as described in ?III-B).</p><p>Additionally, we examine which and how many nearby cache lines a coalesced prefetch instruction typically prefetches for all nine applications. As shown in Fig. <ref type="figure" target="#fig_16">20</ref>, the probability of coalesced prefetching reduces with an increase in cache line distance. Moreover, most coalesced prefetch instructions (82.4% averaged across nine applications) prefetch less than four cache lines. Context hash size. We next analyze how I-SPY's false positive rate varies with an increase in the context hash size, in Fig. <ref type="figure" target="#fig_17">21</ref>. We study the wordpress benchmark since its speedup is heavily impacted by prefetch accuracy (see Fig. <ref type="figure" target="#fig_2">3</ref>).</p><p>We observe that increasing the number of bits in the context hash reduces the false positive rate. However, an increase in the context hash size increases the static code footprint, as shown in Fig. <ref type="figure" target="#fig_17">21</ref>. To minimize the static code footprint while still achieving a low false positive rate, I-SPY's design uses a 16-bit context hash-13% false positive rate and 4.6% static code increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. DISCUSSION</head><p>In this section we discuss some limitations of I-SPY and offer potential solutions. Prefetching already resident cache lines. Although our process of discovering high-probability contexts that lead to cache misses is effective, we also found that many times, the target cache line of a Cprefetch is already resident in the cache. However, the overhead of such resident prefetch operations is low since they do not poison the cache by bringing in new unnecessary cache lines. To make this overhead even lower, we design our proposed prefetch instructions such that they are always inserted with a lower priority as demand loads in regards to the replacement policy. Prefetching within JITted code. Most instruction cache misses in code generated at run time are out of I-SPY's scope. While I-SPY is able to prefetch for some of these misses via Cprefetch instructions inserted into non-JITted code, there are still up to 10% of code misses in JITted code (mostly for the three HHVM applications, wordpress, drupal, and mediawiki) that are not covered. To handle these additional misses, I-SPY could be integrated with a JIT compiler since all of I-SPY's offline machinery (which leverages hardware performance monitoring mechanisms) can, in principle, be used online by the runtime instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. RELATED WORK</head><p>The performance criticality of instruction cache misses has resulted in a rich body of prior literature. We discuss three categories of related work. Software prefetching. Several software techniques <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref> improve instruction locality by relocating infrequently executed code via Profile-Guided Optimizations (PGO) at compile time <ref type="bibr" target="#b16">[17]</ref>, link time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>, or post link time <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref>. However, finding the optimal cache-conscious layout is intractable in practice <ref type="bibr" target="#b1">[2]</ref>, since it requires meandering through a vast number of control-flow combinations. Hence, existing techniques must oftentimes make inaccurate control-flow approximations. Whereas PGO-based techniques have been shown to improve data center application performance <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, they still eliminate only a small subset of all instruction cache misses <ref type="bibr" target="#b1">[2]</ref>. Hardware prefetching. Hardware instruction prefetching techniques began with next-line instruction prefetchers that exploit the common case of fetching sequential instructions <ref type="bibr" target="#b53">[54]</ref>. These next-line prefetchers soon evolved into next-N-line and instruction stream prefetchers <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref> that use trigger events and control mechanisms to prefetch by adaptively looking a few instructions ahead. Next-line and stream prefetchers have been widely deployed in industrial designs because of their implementation simplicity. However, such next-line prefetchers are often inaccurate for complex data center applications that implement frequent branching and function calls.</p><p>Branch predictor based prefetchers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref> improve prefetch accuracy in branch-and call-heavy code. Runahead execution <ref type="bibr" target="#b56">[57]</ref>, wrong path instruction prefetching <ref type="bibr" target="#b57">[58]</ref>, and speculative prefetching mechanisms <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref> can also explore ahead of the instruction fetch unit. However, such prefetchers are susceptible to interference precipitated by wrong path execution and insufficient look ahead when the branch predictor traverses loop branches <ref type="bibr" target="#b11">[12]</ref>.</p><p>TIFS <ref type="bibr" target="#b11">[12]</ref> and PIF <ref type="bibr" target="#b9">[10]</ref> record the instruction fetch miss and instruction commit sequences to overcome the limitations of branch predictor based prefetching. Whereas these mechanisms have improved accuracy and miss coverage, they require considerable on-chip storage to maintain an ordered log of instruction block addresses. Increasing on-chip storage is impractical at data center scale due to strict energy requirements.</p><p>More sophisticated hardware instruction prefetchers proposed by prior works (e.g., trace caches and special hardware replacement policies) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b60">[61]</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref> are too complex to be deployed. We conclude that hardware prefetching mechanisms either provide low accuracy and coverage or they require significant on-chip storage and are too complex to implement in real hardware.</p><p>In comparison, I-SPY covers most instruction cache misses with minor micro-architectural modifications. I-SPY requires only 96-bits of extra storage while state-of-the-art hardware prefetchers (e.g., SHIFT <ref type="bibr" target="#b12">[13]</ref>, Confluence <ref type="bibr" target="#b63">[64]</ref>, and Shotgun <ref type="bibr" target="#b2">[3]</ref>) require kilobytes to megabytes of extra storage. Hybrid hardware-software prefetching. Hybrid hardwaresoftware techniques <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref> attempt to overcome the limitations of hardware-only and software-only prefetching mechanisms. These mechanisms propose hardware code prefetch instructions <ref type="bibr" target="#b64">[65]</ref> that are similar to existing data prefetch instructions <ref type="bibr" target="#b65">[66]</ref>. They use software-based control flow analyses to inject hardware code prefetch instructions.</p><p>Although existing hybrid instruction prefetching mechanisms have been the most effective in reducing I-cache misses in datacenter applications <ref type="bibr" target="#b1">[2]</ref>, they suffer from key limitations that hurt prefetch accuracy. First, such hybrid techniques rely on a single predecessor basic block as the execution context to predict a future cache miss. However, as we show in Section II, we find that miss patterns are more complex and multiple predecessor basic blocks are needed to construct the execution context to accurately predict a future cache miss. Second, existing hybrid prefetching techniques often execute far too many dynamic prefetch instructions, further increasing application code footprints. In contrast, I-SPY achieves near-ideal prefetch accuracy via conditional prefetching, while allowing only a small increase in application footprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION</head><p>Large instruction working sets in modern data center applications have resulted in frequent I-cache misses that significantly degrade data center performance. We investigated instruction prefetching to address this problem and analyze the challenges of designing an ideal instruction prefetcher. We then used insights derived from our investigation to develop I-SPY, a novel profile-driven prefetching technique. I-SPY exposes two new instruction prefetching techniques: conditional prefetching and prefetch coalescing via a family of light-weight hardware code prefetch instructions. We evaluated I-SPY on nine widelyused data center applications to demonstrate an average of 15.5% (up to 45.9%) speedup and 95.9% (and up to 98.4%) reduction in instruction cache misses, outperforming the stateof-the-art prefetching technique by 22.5%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>c a s s a n d r a d r u p a l fi n a g le -c h ir p e r fi n a g le -h tt p k a fk a m e d ia w ik i to m c a t v e r il a to r w o r d p r e s</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2:A partial example of a miss-annotated dynamic control flow graph. Dashed edges represent execution paths that do not lead to a miss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig.3: Prefetch accuracy vs. miss coverage tradeoff in AsmDB and its relation to ideal cache performance: Miss-coverage increases with an increase in fan-out threshold, but prefetch accuracy starts to reduce. Only 65% of ideal cache performance can be reached at 99% fan-out due to low prefetch accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: AsmDB's static and dynamic code footprint increase: Injecting prefetches in high fan-out predecessors significantly increases static and dynamic code footprints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>c a s s a n d r a d r u p a l fi n a g le -c h ir p e r fi n a g le -h tt p k a fk a m e d ia w ik i to m c a t v e r il a to r w o rd p re s s A 8 Fig. 5 :</head><label>85</label><figDesc>Fig. 5: Speedup of Contiguous-8 (prefetches all 8 contiguous lines after a miss) vs. Non-contiguous-8 (prefetches only the misses in an 8-line window after a miss): Prefetching noncontiguous cache lines offers a greater speedup opportunity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: An example of I-SPY's context discovery process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Micro-architectural changes needed to execute the context-sensitive conditional prefetch instruction, Cprefetch</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Usage model of I-SPY</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>c a s s a n d r a d r u p a l fi n a g le -c h ir p e r fi n a g le -h tt p k a fk a m e d ia w ik i to m c a t v e r il a to r w o rd p re s s AFig. 10 :</head><label>10</label><figDesc>Fig.10: I-SPY's speedup compared to an ideal cache and AsmDB: I-SPY achieves an average speedup that is 90.4% of ideal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :Fig. 12 :</head><label>1112</label><figDesc>Fig. 11: I-SPY's L1 I-cache MPKI reduction compared with AsmDB: I-SPY removes 15.7% more misses than AsmDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :Fig. 14 :</head><label>1314</label><figDesc>Fig.13: I-SPY's prefetch accuracy compared with AsmDB: I-SPY achieves an average of 8.2% better accuracy than AsmDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 :Fig. 16 :</head><label>1516</label><figDesc>Fig.15: I-SPY's dynamic code footprint increase compared to AsmDB: On average, I-SPY executes 36% fewer prefetch instructions than AsmDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 17 :</head><label>17</label><figDesc>Fig.17: I-SPY's conditional prefetching achieves better performance with an increase in the number of predecessors comprising the context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 18 :</head><label>18</label><figDesc>Fig.18: I-SPY's average performance variation in response to changes in the minimum (left) and the maximum (right) prefetch distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 19 :</head><label>19</label><figDesc>Fig.19: I-SPY's average performance variation in response to increasing the coalescing size: Larger coalescing sizes achieve higher gains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 20 :</head><label>20</label><figDesc>Fig. 20: (left) The probability of coalesced prefetching reduces with an increase in cache line distance. (right) Coalesced prefetch instructions usually bring in less than 4 cache lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 21 :</head><label>21</label><figDesc>Fig. 21: (left) I-SPY's false positive rate variation in response to an increase in context size: False positives are reduced with a larger context; (right) I-SPY's static code footprint size variation in response to context size: Static code footprint increases with an increase in context size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Simulated System</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>CPU</cell><cell>Intel Xeon Haswell</cell></row><row><cell cols="2">Number of cores per socket 20</cell></row><row><cell>L1 instruction cache</cell><cell>32 KiB, 8-way</cell></row><row><cell>L1 data cache</cell><cell>32 KiB, 8-way</cell></row><row><cell>L2 unified cache</cell><cell>1 MB, 16-way</cell></row><row><cell>L3 unified cache</cell><cell>Shared 10 MiB per socket, 20-way</cell></row><row><cell>All-core turbo frequency</cell><cell>2.5 GHz</cell></row><row><cell>L1 I-cache latency</cell><cell>3 cycles</cell></row><row><cell>L1 D-cache latency</cell><cell>4 cycles</cell></row><row><cell>L2 cache latency</cell><cell>12 cycles</cell></row><row><cell>L3 cache latency</cell><cell>36 cycles</cell></row><row><cell>Memory latency</cell><cell>260 cycles</cell></row><row><cell>Memory bandwidth</cell><cell>6.25 GB/s</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: Auckland University of Technology. Downloaded on December 24,2020 at 18:26:53 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank anonymous reviewers for their insightful feedback and suggestions. This work was supported by the <rs type="funder">Intel Corporation</rs>, the <rs type="funder">NSF</rs> FoMR grants #<rs type="grantNumber">1823559</rs> and #<rs type="grantNumber">2011168</rs>, and the <rs type="institution">Applications Driving Architectures (ADA) Research Center</rs>, a JUMP Center co-sponsored by <rs type="funder">SRC</rs> and <rs type="funder">DARPA</rs>. We thank <rs type="person">Gagan Gupta</rs> and <rs type="person">Rathijit Sen</rs> from <rs type="affiliation">Microsoft Corporation</rs> for insightful suggestions on the characterization of data center applications. We thank <rs type="person">Grant Ayers</rs> from <rs type="funder">Google</rs> for excellent discussions and helpful feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dvrJCyw">
					<idno type="grant-number">1823559</idno>
				</org>
				<org type="funding" xml:id="_gDyf4a4">
					<idno type="grant-number">2011168</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Profiling a warehouse-scale computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="158" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Asmdb: understanding and mitigating front-end stalls in warehouse-scale computers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Nagendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>August</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="462" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blasting through the front-end bottleneck with shotgun</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Boomerang: A metadata-free architecture for control flow delivery</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="493" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memory hierarchy for web search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="643" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm sigplan notices</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="37" to="48" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Softsku: Optimizing server architectures for microservice diversity@ scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dhanotia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th International Symposium on Computer Architecture</title>
		<meeting>the 46th International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="513" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fetch directed instruction prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Reinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-32. Proceedings of the 32nd Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="16" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">B-fetch: Branch prediction directed prefetching for in-order processors</title>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="44" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Proactive instruction fetch</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shift: Shared history instruction fetch for lean-core server processors</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rdip: return-address-stack directed instruction prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ispike: a post-link optimizer for the intel/spl reg/itanium/spl reg/architecture</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Muth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lowney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004. 2004</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Lightweight feedback-directed cross-module optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization</title>
		<meeting>the 8th annual IEEE/ACM international symposium on Code generation and optimization</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="53" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autofdo: Automatic feedbackdirected optimization for warehouse-scale applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE/ACM International Symposium on Code Generation and Optimization</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="12" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing function placement for large-scale data-center applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ottoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bolt: a practical binary optimizer for data centers and beyond</title>
		<author>
			<persName><forename type="first">M</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Auler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ottoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The hardness of cache conscious data placement</title>
		<author>
			<persName><forename type="first">E</forename><surname>Petrank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rawitz</surname></persName>
		</author>
		<idno type="DOI">10.1145/503272.503283</idno>
		<ptr target="https://doi.org/10.1145/503272.503283" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, ser. POPL &apos;02</title>
		<meeting>the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, ser. POPL &apos;02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="101" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cooperative prefetching: Compiler and hardware support for effective instruction prefetching in modern processors</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An introduction to last branch records</title>
		<ptr target="https://lwn.net/Articles/680985/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A top-down method for performance analysis and counters architecture</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">facebookarchive/oss-performance: Scripts for benchmarking various php implementations when running open source software</title>
		<ptr target="https://github.com/facebookarchive/oss-performance" />
		<imprint>
			<date type="published" when="2019-11">2019. November-2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Khang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bentzur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Guyer</surname></persName>
		</author>
		<title level="m">Proceedings of the 21st annual ACM SIGPLAN conference on Objectoriented programming systems, languages, and applications</title>
		<meeting>the 21st annual ACM SIGPLAN conference on Objectoriented programming systems, languages, and applications</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="169" to="190" />
		</imprint>
	</monogr>
	<note>The dacapo benchmarks: Java benchmarking development and analysis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Apache cassandra</title>
		<ptr target="http://cassandra.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Apache tomcat</title>
		<ptr target="https://tomcat.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Renaissance: Benchmarking suite for parallel applications on the jvm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prokopec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ros?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Leopoldseder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Duboscq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>T?ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Studener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bulej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Villaz?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>W?rthinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Binder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Twitter finagle</title>
		<ptr target="https://twitter.github.io/finagle/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Verilator</title>
		<ptr target="https://www.veripool.org/wiki/verilator" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">An infrastructure for adaptive dynamic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bruening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploring code cache eviction granularities in dynamic optimization systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Inside 6th-generation intel core: New microarchitecture code-named skylake</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>-Y. Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mandelblat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahatekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rappoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yoaz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two level bulk preload branch prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bonanno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Collura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lipetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saporito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE 19th Interna-tional Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Phantom-btb: a virtualized branch target buffer design</title>
		<author>
			<persName><forename type="first">I</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigplan Notices</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="313" to="324" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Applying deep learning to the cache replacement problem</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="413" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=Fowler%E2" />
		<title level="m">Fowler-noll-vo hash function -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2019-04">2019. April-2020</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
	<note>%80%93Noll%E2%80%93Vo hash function&amp;oldid=931348563</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<ptr target="https://en.wikipedia.org/w/index.php?title=MurmurHash&amp;oldid=950347972" />
		<title level="m">Murmurhash -Wikipedia, the free encyclopedia</title>
		<imprint>
			<date type="published" when="2020-04">2020. April-2020</date>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Summary cache: a scalable wide-area web cache sharing protocol</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Broder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM transactions on networking</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="293" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An improved construction for counting bloom filters</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bonomi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitzenmacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varghese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="684" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Griffin: Guarding control flows using intel processor trace</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Intel (r) 64 and ia-32 architectures software developer&apos;s manual</title>
		<author>
			<persName><forename type="first">I</forename><surname>Corparation</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combined Volumes</title>
		<imprint>
			<date type="published" when="2016-12">Dec, 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Zsim: Fast and accurate microarchitectural simulation of thousand-core systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggressive inlining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schooler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gottlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cache-conscious structure definition</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN conference on Programming language design and implementation</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">utune: Auto-tuned threading for OLDI microservices</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Serverless in the wild: Characterizing and optimizing the serverless workload at a large cloud provider</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shahrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Batum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Laureano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tresness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Russinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bianchini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03423</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Steps towards cache-resident transaction processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harizopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Very large data bases</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Code layout optimizations for transaction processing workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Barroso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gharachorloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Larriba-Pey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Lowney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Buffering databse operations for enhanced instruction cache performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Management of data</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Architectural and compiler support for effective instruction prefetching: a cooperative approach</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">L</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Classifying memory access patterns for prefetching</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="513" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning memory access patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ayers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Litz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02329</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The ibm system/360 model 91: Machine philosophy and instruction-handling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sparacio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Tomasulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effective instruction prefetching in chip multiprocessors for modern commercial applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Branch history guided instruction prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High-Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Runahead execution: An effective alternative to large instruction windows</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Wrong-path instruction prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Slipstream processors: Improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Execution-based prediction using speculative slices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Trace cache: a low latency approach to high bandwidth instruction fetching</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 29th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="24" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Path-based next trace prediction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 30th Annual International Symposium on Microarchitecture</title>
		<meeting>30th Annual International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="14" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exploring predictive replacement policies for instruction cache and branch target buffer</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ajorpaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="519" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Confluence: unified instruction supply for scale</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Grot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
		<meeting>the 48th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="166" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Prfm (literal) -a64</title>
		<ptr target="OPT/xhtml/prfmreg.html" />
		<imprint>
			<date type="published" when="2020-03">March-2020</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Prefetchh: Prefetch data into caches (x86 instruction set reference)</title>
		<ptr target="https://c9x.me/x86/html/filemodulex86id252.html" />
		<imprint>
			<date type="published" when="2020-03">March-2020</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
