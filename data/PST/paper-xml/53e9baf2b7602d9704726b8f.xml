<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
							<email>ilyasu@google.com</email>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Martens</surname></persName>
							<email>jmartens@cs.toronto.edu</email>
						</author>
						<author>
							<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
							<email>gdahl@cs.toronto.edu</email>
						</author>
						<author>
							<persName><forename type="first">Geo↵rey</forename><surname>Hinton</surname></persName>
							<email>hinton@cs.toronto.edu</email>
						</author>
						<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.</p><p>Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods su ce for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep and recurrent neural networks (DNNs and RNNs, respectively) are powerful models that achieve high performance on di cult pattern recognition problems in vision, and speech <ref type="bibr" target="#b15">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b10">Hinton et al., 2012;</ref><ref type="bibr" target="#b5">Dahl et al., 2012;</ref><ref type="bibr" target="#b8">Graves, 2012)</ref>.</p><p>Although their representational power is appealing, the di culty of training DNNs has prevented their Proceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&amp;CP volume 28. Copyright 2013 by the author(s).</p><p>widepread use until fairly recently. DNNs became the subject of renewed attention following the work of <ref type="bibr">Hinton et al. (2006)</ref> who introduced the idea of greedy layerwise pre-training. This approach has since branched into a family of methods <ref type="bibr" target="#b1">(Bengio et al., 2007)</ref>, all of which train the layers of the DNN in a sequence using an auxiliary objective and then "finetune" the entire network with standard optimization methods such as stochastic gradient descent (SGD). More recently, <ref type="bibr" target="#b17">Martens (2010)</ref> attracted considerable attention by showing that a type of truncated-Newton method called Hessian-free Optimization (HF) is capable of training DNNs from certain random initializations without the use of pre-training, and can achieve lower errors for the various auto-encoding tasks considered by <ref type="bibr" target="#b9">Hinton &amp; Salakhutdinov (2006)</ref>. Recurrent neural networks (RNNs), the temporal analogue of DNNs, are highly expressive sequence models that can model complex sequence relationships. They can be viewed as very deep neural networks that have a "layer" for each time-step with parameter sharing across the layers and, for this reason, they are considered to be even harder to train than DNNs. Recently, <ref type="bibr">Martens &amp; Sutskever (2011)</ref> showed that the HF method of <ref type="bibr" target="#b17">Martens (2010)</ref> could e↵ectively train RNNs on artificial problems that exhibit very long-range dependencies <ref type="bibr" target="#b12">(Hochreiter &amp; Schmidhuber, 1997)</ref>. Without resorting to special types of memory units, these problems were considered to be impossibly di cult for first-order optimization methods due to the well known vanishing gradient problem <ref type="bibr" target="#b0">(Bengio et al., 1994)</ref>. <ref type="bibr" target="#b27">Sutskever et al. (2011)</ref> and later <ref type="bibr" target="#b20">Mikolov et al. (2012)</ref> then applied HF to train RNNs to perform character-level language modeling and achieved excellent results.</p><p>Recently, several results have appeared to challenge the commonly held belief that simpler first-order methods are incapable of learning deep models from random initializations. The work of <ref type="bibr" target="#b7">Glorot &amp; Bengio (2010)</ref>, <ref type="bibr" target="#b21">Mohamed et al. (2012)</ref>, and <ref type="bibr" target="#b15">Krizhevsky et al. (2012)</ref> reported little di culty training neural networks with depths up to 8 from certain well-chosen random initializations. Notably, <ref type="bibr" target="#b3">Chapelle &amp; Erhan (2011)</ref> used the random initialization of <ref type="bibr" target="#b7">Glorot &amp; Bengio (2010)</ref> and SGD to train the 11-layer autoencoder of <ref type="bibr" target="#b9">Hinton &amp; Salakhutdinov (2006)</ref>, and were able to surpass the results reported by <ref type="bibr" target="#b9">Hinton &amp; Salakhutdinov (2006)</ref>. While these results still fall short of those reported in <ref type="bibr" target="#b17">Martens (2010)</ref> for the same tasks, they indicate that learning deep networks is not nearly as hard as was previously believed.</p><p>The first contribution of this paper is a much more thorough investigation of the di culty of training deep and temporal networks than has been previously done.</p><p>In particular, we study the e↵ectiveness of SGD when combined with well-chosen initialization schemes and various forms of momentum-based acceleration. We show that while a definite performance gap seems to exist between plain SGD and HF on certain deep and temporal learning problems, this gap can be eliminated or nearly eliminated (depending on the problem) by careful use of classical momentum methods or Nesterov's accelerated gradient. In particular, we show how certain carefully designed schedules for the constant of momentum µ, which are inspired by various theoretical convergence-rate theorems <ref type="bibr" target="#b22">(Nesterov, 1983;</ref><ref type="bibr" target="#b23">2003)</ref>, produce results that even surpass those reported by <ref type="bibr" target="#b17">Martens (2010)</ref> on certain deep-autencoder training tasks. For the long-term dependency RNN tasks examined in <ref type="bibr">Martens &amp; Sutskever (2011)</ref>, which first appeared in <ref type="bibr" target="#b12">Hochreiter &amp; Schmidhuber (1997)</ref>, we obtain results that fall just short of those reported in that work, where a considerably more complex approach was used.</p><p>Our results are particularly surprising given that momentum and its use within neural network optimization has been studied extensively before, such as in the work of <ref type="bibr" target="#b24">Orr (1996)</ref>, and it was never found to have such an important role in deep learning. One explanation is that previous theoretical analyses and practical benchmarking focused on local convergence in the stochastic setting, which is more of an estimation problem than an optimization one <ref type="bibr" target="#b2">(Bottou &amp; LeCun, 2004)</ref>. In deep learning problems this final phase of learning is not nearly as long or important as the initial "transient phase" <ref type="bibr" target="#b6">(Darken &amp; Moody, 1993)</ref>, where a better argument can be made for the beneficial e↵ects of momentum.</p><p>In addition to the inappropriate focus on purely local convergence rates, we believe that the use of poorly designed standard random initializations, such as those in <ref type="bibr" target="#b9">Hinton &amp; Salakhutdinov (2006)</ref>, and suboptimal meta-parameter schedules (for the momentum constant in particular) has hampered the discovery of the true e↵ectiveness of first-order momentum methods in deep learning. We carefully avoid both of these pitfalls in our experiments and provide a simple to understand and easy to use framework for deep learning that is surprisingly e↵ective and can be naturally combined with techniques such as those in <ref type="bibr" target="#b26">Raiko et al. (2011)</ref>.</p><p>We will also discuss the links between classical momentum and Nesterov's accelerated gradient method (which has been the subject of much recent study in convex optimization theory), arguing that the latter can be viewed as a simple modification of the former which increases stability, and can sometimes provide a distinct improvement in performance we demonstrated in our experiments. We perform a theoretical analysis which makes clear the precise di↵erence in local behavior of these two algorithms. Additionally, we show how HF employs what can be viewed as a type of "momentum" through its use of special initializations to conjugate gradient that are computed from the update at the previous time-step. We use this property to develop a more momentum-like version of HF which combines some of the advantages of both methods to further improve on the results of Martens (2010).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Momentum and Nesterov's Accelerated Gradient</head><p>The momentum method <ref type="bibr" target="#b25">(Polyak, 1964)</ref>, which we refer to as classical momentum (CM), is a technique for accelerating gradient descent that accumulates a velocity vector in directions of persistent reduction in the objective across iterations. Given an objective function f (✓) to be minimized, classical momentum is given by:</p><formula xml:id="formula_0">v t+1 = µv t "rf (✓ t ) (1) ✓ t+1 = ✓ t + v t+1 (2)</formula><p>where " &gt; 0 is the learning rate, µ 2 [0, 1] is the momentum coe cient, and rf (✓ t ) is the gradient at ✓ t . Since directions d of low-curvature have, by definition, slower local change in their rate of reduction (i.e., d &gt; rf ), they will tend to persist across iterations and be amplified by CM. Second-order methods also amplify steps in low-curvature directions, but instead of accumulating changes they reweight the update along each eigen-direction of the curvature matrix by the inverse of the associated curvature. And just as secondorder methods enjoy improved local convergence rates, <ref type="bibr" target="#b25">Polyak (1964)</ref> showed that CM can considerably accelerate convergence to a local minimum, requiring p Rtimes fewer iterations than steepest descent to reach the same level of accuracy, where R is the condition number of the curvature at the minimum and µ is set to ( p R 1)/( p R + 1).</p><p>Nesterov <ref type="bibr">'s Accelerated Gradient (abbrv. NAG;</ref><ref type="bibr" target="#b22">Nesterov, 1983</ref>) has been the subject of much recent attention by the convex optimization community (e.g., <ref type="bibr" target="#b4">Cotter et al., 2011;</ref><ref type="bibr" target="#b16">Lan, 2010)</ref>. Like momentum, NAG is a first-order optimization method with better convergence rate guarantee than gradient descent in certain situations. In particular, for general smooth (non-strongly) convex functions and a deterministic gradient, NAG achieves a global convergence rate of O(1/T 2 ) (versus the O(1/T ) of gradient descent), with constant proportional to the Lipschitz coe cient of the derivative and the squared Euclidean distance to the solution. While NAG is not typically thought of as a type of momentum, it indeed turns out to be closely related to classical momentum, di↵ering only in the precise update of the velocity vector v, the significance of which we will discuss in the next sub-section. Specifically, as shown in the appendix, the NAG update may be rewritten as:</p><formula xml:id="formula_1">v t+1 = µv t "rf (✓ t + µv t ) (3) ✓ t+1 = ✓ t + v t+1<label>(4)</label></formula><p>While the classical convergence theories for both methods rely on noiseless gradient estimates (i.e., not stochastic), with some care in practice they are both applicable to the stochastic setting. However, the theory predicts that any advantages in terms of asymptotic local rate of convergence will be lost <ref type="bibr" target="#b24">(Orr, 1996;</ref><ref type="bibr" target="#b28">Wiegerinck et al., 1999)</ref>, a result also confirmed in experiments <ref type="bibr" target="#b16">(LeCun et al., 1998)</ref>. For these reasons, interest in momentum methods diminished after they had received substantial attention in the 90's. And because of this apparent incompatibility with stochastic optimization, some authors even discourage using momentum or downplay its potential advantages <ref type="bibr" target="#b16">(LeCun et al., 1998)</ref>.</p><p>However, while local convergence is all that matters in terms of asymptotic convergence rates (and on certain very simple/shallow neural network optimization problems it may even dominate the total learning time), in practice, the "transient phase" of convergence <ref type="bibr" target="#b6">(Darken &amp; Moody, 1993)</ref>, which occurs before fine local convergence sets in, seems to matter a lot more for optimizing deep neural networks. In this transient phase of learning, directions of reduction in the objective tend to persist across many successive gradient estimates and are not completely swamped by noise.</p><p>Although the transient phase of learning is most noticeable in training deep learning models, it is still noticeable in convex objectives. The convergence rate of stochastic gradient descent on smooth convex functions is given by O(L/T + / p T ), where is the variance in the gradient estimate and L is the Lipshits coe cient of rf . In contrast, the convergence rate of an accelerated gradient method of Lan (2010) (which is related to but di↵erent from NAG, in that it combines Nesterov style momentum with dual averaging) is O(L/T 2 + / p T ). Thus, for convex objectives, momentum-based methods will outperform SGD in the early or transient stages of the optimization where L/T is the dominant term. However, the two methods will be equally e↵ective during the final stages of the optimization where / p T is the dominant term (i.e., when the optimization problem resembles an estimation one).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Relationship between CM and NAG</head><p>From Eqs. 1-4 we see that both CM and NAG compute the new velocity by applying a gradient-based correction to the previous velocity vector (which is decayed), and then add the velocity to ✓ t . But while CM computes the gradient update from the current position ✓ t , NAG first performs a partial update to ✓ t , computing ✓ t + µv t , which is similar to ✓ t+1 , but missing the as yet unknown correction. This benign-looking difference seems to allow NAG to change v in a quicker and more responsive way, letting it behave more stably than CM in many situations, especially for higher values of µ.</p><p>Indeed, consider the situation where the addition of µv t results in an immediate undesirable increase in the objective f . The gradient correction to the velocity v</p><p>t is computed at position ✓ t + µv t and if µv t is indeed a poor update, then rf (✓ t + µv t ) will point back towards ✓ t more strongly than rf (✓ t ) does, thus providing a larger and more timely correction to v t than CM. See fig. <ref type="figure" target="#fig_0">1</ref> for a diagram which illustrates this phenomenon geometrically. While each iteration of NAG may only be slightly more e↵ective than CM at correcting a large and inappropriate velocity, this di↵erence in e↵ectiveness may compound as the algorithms iterate. To demonstrate this compounding, we applied both NAG and CM to a two-dimensional oblong quadratic objective, both with the same momentum and learning rate constants (see fig. <ref type="figure">2</ref> in the appendix). While the optimization path taken by CM exhibits large oscillations along the high-curvature vertical direction, NAG is able to avoid these oscillations almost entirely, confirming the intuition that it is much more e↵ective than CM at decelerating over the course of multiple iterations, thus making NAG more tolerant of large values of µ compared to CM.</p><p>In order to make these intuitions more rigorous and help quantify precisely the way in which CM and NAG di↵er, we analyzed the behavior of each method when applied to a positive definite quadratic objective q(x) = x &gt; Ax/2 + b &gt; x. We can think of CM and NAG as operating independently over the di↵erent eigendirections of A. NAG operates along any one of these directions equivalently to CM, except with an e↵ective value of µ that is given by µ( <ref type="formula">1</ref>"), where is the associated eigenvalue/curvature. The first step of this argument is to reparameterize q(x) in terms of the coe cients of x under the basis of eigenvectors of A. Note that since A = U &gt; DU for a diagonal D and orthonormal U (as A is symmetric), we can reparameterize q(x) by the matrix transform U and optimize y = Ux using the objective p(y) ⌘ q</p><formula xml:id="formula_2">(x) = q(U &gt; y) = y &gt; U U &gt; DU U &gt; y/2 + b &gt; U &gt; y = y &gt; Dy/2 + c &gt; y, where c = Ub. We can further rewrite p as p(y) = P n i=1 [p] i ([y] i ), where [p] i (t) = i t 2 /2+[c] i t and</formula><p>i &gt; 0 are the diagonal entries of D (and thus the eigenvalues of A) and correspond to the curvature along the associated eigenvector directions. As shown in the appendix (Proposition 6.1), both CM and NAG, being first-order methods, are "invariant" to these kinds of reparameterizations by orthonormal transformations such as U . Thus when analyzing the behavior of either algorithm applied to q(x), we can instead apply them to p(y), and transform the resulting sequence of iterates back to the default parameterization (via multiplication by</p><formula xml:id="formula_3">U 1 = U &gt; ). Theorem 2.1. Let p(y) = P n i=1 [p] i ([y] i ) such that [p] i (t) = i t 2 /2 + c i t.</formula><p>Let " be arbitrary and fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Denote by CM</head><p>x (µ, p, y, v) and CM v (µ, p, y, v) the parameter vector and the velocity vector respectively, obtained by applying one step of CM (i.e., Eq. 1 and then Eq. 2) to the function p at point y, with velocity v, momentum coe cient µ, and learning rate ". Define N AG</p><p>x and N AG v analogously. Then the following holds for z 2 {x, v}:</p><formula xml:id="formula_4">CM z (µ, p, y, v) = 2 6 4 CM z (µ, [p] 1 , [y] 1 , [v] 1 ) . . . CM z (µ, [p] n , [y] n , [v] n ) 3 7 5 N AG z (µ, p, y, v) = 2 6 4 CM z (µ(1 1 "), [p] 1 , [y] 1 , [v] 1 ) . . . CM z (µ(1 n "), [p] n , [y] n , [v] n ) 3 7 5</formula><p>Proof. See the appendix.</p><p>The theorem has several implications. First, CM and NAG become equivalent when " is small (when " ⌧ 1 for every eigenvalue of A), so NAG and CM are distinct only when " is reasonably large. When " is relatively large, NAG uses smaller e↵ective momentum for the high-curvature eigen-directions, which prevents oscillations (or divergence) and thus allows the use of a larger µ than is possible with CM for a given ".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Autoencoders</head><p>The aim of our experiments is three-fold. First, to investigate the attainable performance of stochastic momentum methods on deep autoencoders starting from well-designed random initializations; second, to explore the importance and e↵ect of the schedule for the momentum parameter µ assuming an optimal fixed choice of the learning rate "; and third, to compare the performance of NAG versus CM.</p><p>For our experiments with feed-forward nets, we focused on training the three deep autoencoder problems described in <ref type="bibr" target="#b9">Hinton &amp; Salakhutdinov (2006)</ref> (see sec. A.2 for details). The task of the neural network autoencoder is to reconstruct its own input subject to the constraint that one of its hidden layers is of low-dimension. This "bottleneck" layer acts as a low-dimensional code for the original input, similar to other dimensionality reduction techniques like Principle Component Analysis (PCA). These autoencoders are some of the deepest neural networks with published results, ranging between 7 and 11 layers, and have become a standard benchmarking problem (e.g., <ref type="bibr" target="#b17">Martens, 2010;</ref><ref type="bibr" target="#b7">Glorot &amp; Bengio, 2010;</ref><ref type="bibr" target="#b3">Chapelle &amp; Erhan, 2011;</ref><ref type="bibr" target="#b26">Raiko et al., 2011)</ref>. See the appendix for more details.</p><p>Because the focus of this study is on optimization, we only report training errors in our experiments. Test error depends strongly on the amount of overfitting in these problems, which in turn depends on the type and amount of regularization used during training. While regularization is an issue of vital importance when designing systems of practical utility, it is outside the scope of our discussion. And while it could be objected that the gains achieved using better optimization methods are only due to more exact fitting of the training set in a manner that does not generalize, this is simply not the case in these problems, where undertrained solutions are known to perform poorly on both the training and test sets (underfitting).</p><p>The networks we trained used the standard sigmoid nonlinearity and were initialized using the "sparse initialization" technique (SI) of <ref type="bibr" target="#b17">Martens (2010)</ref> that is described in sec. 3.1. Each trial consists of 750,000 parameter updates on minibatches of size 200. No regularization is used. The schedule for µ was given by the following formula:</p><formula xml:id="formula_5">µ t = min(1 2 1 log 2 (bt/250c+1) , µ max ) (5)</formula><p>where µ max was chosen from {0.999, 0.995, 0.99, 0.9, 0}. This schedule was motivated by <ref type="bibr" target="#b22">Nesterov (1983)</ref>  (see appendix), and by <ref type="bibr" target="#b23">Nesterov (2003)</ref> who advocates a constant µ t that depends on (essentially) the condition number. The constant µ t achieves exponential convergence on strongly convex functions, while the 1 3/(t + 5) schedule is appropriate when the function is not strongly convex. The schedule of Eq. 5 blends these proposals. For each choice of µ max , we report the learning rate that achieved the best training error. Given the schedule for µ, the learning rate " was chosen from {0.05, 0.01, 0.005, 0.001, 0.0005, 0.0001} in order to achieve the lowest final error training error after our fixed number of updates.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the results of these experiments. It shows that NAG achieves the lowest published results on this set of problems, including those of <ref type="bibr" target="#b17">Martens (2010)</ref>. It also shows that larger values of µ max tend to achieve better performance and that NAG usually outperforms CM, especially when µ max is 0.995 and 0.999. Most surprising and importantly, the results demonstrate that NAG can achieve results that are comparable with some of the best HF results for training deep autoencoders. Note that the previously published results on HF used L2 regularization, so they cannot be directly compared. However, the table also includes experiments we performed with an improved version of HF (see sec. 2.1) where weight decay was removed towards the end of training.</p><p>We found it beneficial to reduce µ to 0.9 (unless µ is 0, in which case it is unchanged) during the final 1000 parameter updates of the optimization without reducing the learning rate, as shown in Table <ref type="table" target="#tab_1">2</ref>. It appears that reducing the momentum coe cient allows for finer convergence to take place whereas otherwise the overly aggressive nature of CM or NAG would prevent this. This phase shift between optimization that favors fast accelerated motion along the error surface (the "transient phase") followed by more careful optimization-as-estimation phase seems consistent with the picture presented by <ref type="bibr" target="#b6">Darken &amp; Moody (1993)</ref>. However, while asymptotically it is the second phase which must eventually dominate computation time, in practice it seems that for deeper networks in particular, the first phase dominates overall computation time as long as the second phase is cut o↵ before the remaining potential gains become either insignificant or entirely dominated by overfitting (or both).</p><p>It may be tempting then to use lower values of µ from the outset, or to reduce it immediately when progress in reducing the error appears to slow down. However, in our experiments we found that doing this was detrimental in terms of the final errors we could achieve, and that despite appearing to not make much progress, or even becoming significantly non-monotonic, the optimizers were doing something apparently useful over these extended periods of time at higher values of µ.</p><p>A speculative explanation as to why we see this behavior is as follows. While a large value of µ allows the momentum methods to make useful progress along slowly-changing directions of low-curvature, this may not immediately result in a significant reduction in error, due to the failure of these methods to converge in the more turbulent high-curvature directions (which is especially hard when µ is large). Nevertheless, this progress in low-curvature directions takes the optimizers to new regions of the parameter space that are characterized by closer proximity to the optimum (in the case of a convex objective), or just higher-quality local minimia (in the case of non-convex optimization). Thus, while it is important to adopt a more careful scheme that allows fine convergence to take place along the high-curvature directions, this must be done with care. Reducing µ and moving to this fine convergence regime too early may make it di cult for the optimization to make significant progress along the low-curvature directions, since without the benefit of momentum-based acceleration, first-order methods are notoriously bad at this (which is what motivated the use of second-order methods like HF for deep learning). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Random Initializations</head><p>The results in the previous section were obtained with standard logistic sigmoid neural networks that were initialized with the sparse initialization technique (SI) described in <ref type="bibr" target="#b17">Martens (2010)</ref>. In this scheme, each random unit is connected to 15 randomly chosen units in the previous layer, whose weights are drawn from a unit Gaussian, and the biases are set to zero. The intuitive justification is that the total amount of input to each unit will not depend on the size of the previous layer and hence they will not as easily saturate.</p><p>Meanwhile, because the inputs to each unit are not all randomly weighted blends of the outputs of many 100s or 1000s of units in the previous layer, they will tend to be qualitatively more "diverse" in their response to inputs. When using tanh units, we transform the weights to simulate sigmoid units by setting the biases to 0.5 and rescaling the weights by 0.25.</p><p>We investigated the performance of the optimization as a function of the scale constant used in SI (which defaults to 1 for sigmoid units). We found that SI works reasonably well if it is rescaled by a factor of 2, but leads to noticeable (but not severe) slow down when scaled by a factor of 3. When we used the factor 1/2 or 5 we did not achieve sensible results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Recurrent Neural Networks</head><p>Echo-State Networks (ESNs) is a family of RNNs with an unusually simple training method: their hidden-tooutput connections are learned from data, but their recurrent connections are fixed to a random draw from a specific distribution and are not learned. Despite their simplicity, ESNs with many hidden units (or with units with explicit temporal integration, like the LSTM) have achieved high performance on tasks with long range dependencies (?). In this section, we investigate the e↵ectiveness of momentum-based methods with ESN-inspired initialization at training RNNs with conventional size and standard (i.e., non-integrating) neurons. We find that momentum-accelerated SGD can successfully train such RNNs on various artificial datasets exhibiting considerable long-range temporal dependencies. This is unexpected because RNNs were believed to be almost impossible to successfully train on such datasets with first-order methods, due to various di culties such as vanishing/exploding gradients <ref type="bibr" target="#b0">(Bengio et al., 1994)</ref>. While we found that the use of momentum significantly improved performance and robustness, we obtained nontrivial results even with standard SGD, provided that the learning rate was set low enough.</p><p>connection type sparsity scale in-to-hid (add,mul) dense 0.001•N (0, 1) in-to-hid (mem) dense 0.1•N (0, 1) hid-to-hid 15 fan-in spectral radius of 1.1 hid-to-out dense 0.1•N (0, 1) hid-bias dense 0 out-bias dense average of outputs Table <ref type="table">4</ref>. The RNN initialization used in the experiments.</p><p>The scale of the vis-hid connections is problem dependent.</p><p>Each task involved optimizing the parameters of a randomly initialized RNN with 100 standard tanh hidden units (the same model used by <ref type="bibr">Martens &amp; Sutskever (2011)</ref>). The tasks were designed by <ref type="bibr" target="#b12">Hochreiter &amp; Schmidhuber (1997)</ref>, and are referred to as training "problems". See sec. A.3 of the appendix for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ESN-based Initialization</head><p>As argued by <ref type="bibr" target="#b14">Jaeger &amp; Haas (2004)</ref>, the spectral radius of the hidden-to-hidden matrix has a profound e↵ect on the dynamics of the RNN's hidden state (with a tanh nonlinearity). When it is smaller than 1, the dynamics will have a tendency to quickly "forget" whatever input signal they may have been exposed to. When it is much larger than 1, the dynamics become oscillatory and chaotic, allowing it to generate responses that are varied for di↵erent input histories. While this allows information to be retained over many time steps, it can also lead to severe exploding gradients that make gradient-based learning much more di cult. However, when the spectral radius is only slightly greater than 1, the dynamics remain oscillatory and chaotic while the gradient are no longer exploding (and if they do explode, then only "slightly so"), so learning may be possible with a spectral radius of this order. This suggests that a spectral radius of around 1.1 may be e↵ective.</p><p>To achieve robust results, we also found it is essential to carefully set the initial scale of the input-to-hidden connections. When training RNNs to solve those tasks that possess many random and irrelevant distractor inputs, we found that having the scale of these connections set too high at the start led to relevant information in the hidden state being too easily "overwritten" by the many irrelevant signals, which ultimately led the optimizer to converge towards an extremely poor local minimum where useful information was never relayed over long distances. Conversely, we found that if this scale was set too low, it led to significantly slower learning. Having experimented with multiple scales we found that a Gaussian draw with a standard deviation of 0.001 achieved a good balance between these concerns. However, unlike the value of 1.1 for the spectral radius of the dynamics matrix, which worked well on all tasks, we found that good choices for initial scale of the input-to-hidden weights depended a lot on the particular characteristics of the particular task (such as its dimensionality or the input variance). Indeed, for tasks that do not have many irrelevant inputs, a larger scale of the input-to-hidden weights (namely, 0.1) worked better, because the aforementioned disadvantage of large input-to-hidden weights does not apply. See table <ref type="table">4</ref> for a summary of the initializations used in the experiments. Finally, we found centering (mean subtraction) of both the inputs and the outputs to be important to reliably solve all of the training problems. See the appendix for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>We conducted experiments to determine the ecacy of our initializations, the e↵ect of momentum, and to compare NAG with CM. Every learning trial used the aforementioned initialization, 50,000 parameter updates and on minibatches of 100 sequences, and the following schedule for the momentum coe cient µ: µ = 0.9 for the first 1000 parameter, after which µ = µ 0 , where µ 0 can take the following values {0, 0.9, 0.98, 0.995}. For each µ 0 , we use the empirically best learning rate chosen from {10 3 , 10 4 , 10 5 , 10 6 }.</p><p>The results are presented in Table <ref type="table">5</ref>, which are the average loss over 4 di↵erent random seeds. Instead of reporting the loss being minimized (which is the squared error or cross entropy), we use a more interpretable zero-one loss, as is standard practice with these problems. For the bit memorization, we report the fraction of timesteps that are computed incorrectly. And for the addition and the multiplication problems, we report the fraction of cases where the RNN the error in the final output prediction exceeded 0.04.</p><p>Our results show that despite the considerable longrange dependencies present in training data for these problems, RNNs can be successfully and robustly trained to solve them, through the use of the initialization discussed in sec. 4.1, momentum of the NAG type, a large µ 0 , and a particularly small learning rate (as compared with feedforward networks). Our results also suggest that with larger values of µ 0 achieve better results with NAG but not with CM, possibly due to NAG's tolerance of larger µ 0 's (as discussed in sec. 2). Although we were able to achieve surprisingly good training performance on these problems using a sufficiently strong momentum, the results of <ref type="bibr">Martens &amp; Sutskever (2011)</ref> appear to be moderately better and more robust. They achieved lower error rates and their initialization was chosen with less care, although the initializations are in many ways similar to ours. Notably, <ref type="bibr">Martens &amp; Sutskever (2011)</ref> were able to solve these problems without centering, while we had to use centering to solve the multiplication problem (the other problems are already centered). This suggests that the initialization proposed here, together with the method of <ref type="bibr">Martens &amp; Sutskever (2011)</ref>, could achieve even better performance. But the main achievement of these results is a demonstration of the ability of momentum methods to cope with long-range temporal dependency training tasks to a level which seems su cient for most practical purposes. Moreover, our approach seems to be more tolerant of smaller minibatches, and is considerably simpler than the particular version of HF proposed in <ref type="bibr">Martens &amp; Sutskever (2011)</ref>, which used a specialized update damping technique whose benefits seemed mostly limited to training RNNs to solve these kinds of extreme temporal dependency problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Momentum and HF</head><p>Truncated Newton methods, that include the HF method of Martens (2010) as a particular example, work by optimizing a local quadratic model of the objective via the linear conjugate gradient algorithm (CG), which is a first-order method. While HF, like all truncated-Newton methods, takes steps computed using partially converged calls to CG, it is naturally accelerated along at least some directions of lower curvature compared to the gradient. It can even be shown <ref type="bibr" target="#b19">(Martens &amp; Sutskever, 2012</ref>) that CG will tend to favor convergence to the exact solution to the quadratic sub-problem first along higher curvature directions (with a bias towards those which are more clustered together in their curvature-scalars/eigenvalues).</p><p>While CG accumulates information as it iterates which allows it to be optimal in a much stronger sense than any other first-order method (like NAG), once it is terminated, this information is lost. Thus, standard truncated Newton methods can be thought of as persisting information which accelerates convergence (of the current quadratic) only over the number of iterations CG performs. By contrast, momentum methods persist information that can inform new updates across an arbitrary number of iterations.</p><p>One key di↵erence between standard truncated Newton methods and HF is the use of "hot-started" calls to CG, which use as their initial solution the one found at the previous call to CG. While this solution was computed using old gradient and curvature information from a previous point in parameter space and possibly a di↵erent set of training data, it may be wellconverged along certain eigen-directions of the new quadratic, despite being very poorly converged along others (perhaps worse than the default initial solution of 0). However, to the extent to which the new local quadratic model resembles the old one, and in particular in the more di cult to optimize directions of lowcurvature (which will arguably be more likely to persist across nearby locations in parameter space), the previous solution will be a preferable starting point to 0, and may even allow for gradually increasing levels of convergence along certain directions which persist  <ref type="table">5</ref>. Each column reports the errors (zero-one losses; sec. 4.2) on di↵erent problems for each combination of µ0 and momentum type (NAG, CM), averaged over 4 di↵erent random seeds. The "biases" column lists the error attainable by learning the output biases and ignoring the hidden state. This is the error of an RNN that failed to "establish communication" between its inputs and targets. For each µ0, we used the fixed learning rate that gave the best performance.</p><p>in the local quadratic models across many updates.</p><p>The connection between HF and momentum methods can be made more concrete by noticing that a single step of CG is e↵ectively a gradient update taken from the current point, plus the previous update reapplied, just as with NAG, and that if CG terminated after just 1 step, HF becomes equivalent to NAG, except that it uses a special formula based on the curvature matrix for the learning rate instead of a fixed constant. The most e↵ective implementations of HF even employ a "decay" constant <ref type="bibr" target="#b19">(Martens &amp; Sutskever, 2012)</ref> which acts analogously to the momentum constant µ. Thus, in this sense, the CG initializations used by HF allow us to view it as a hybrid of NAG and an exact secondorder method, with the number of CG iterations used to compute each update e↵ectively acting as a dial between the two extremes.</p><p>Inspired by the surprising success of momentum-based methods for deep learning problems, we experimented with making HF behave even more like NAG than it already does. The resulting approach performed surprisingly well (see Table <ref type="table" target="#tab_0">1</ref>). For a more detailed account of these experiments, see sec. A.6 of the appendix.</p><p>If viewed on the basis of each CG step (instead of each update to parameters ✓), HF can be thought of as a peculiar type of first-order method which approximates the objective as a series of quadratics only so that it can make use of the powerful first-order CG method. So apart from any potential benefit to global convergence from its tendency to prefer certain directions of movement in parameter space over others, perhaps the main theoretical benefit to using HF over a first-order method like NAG is its use of CG, which, while itself a first-order method, is well known to have strongly optimal convergence properties for quadratics, and can take advantage of clustered eigenvalues to accelerate convergence (see <ref type="bibr" target="#b19">Martens &amp; Sutskever (2012)</ref> for a detailed account of this well-known phenomenon). However, it is known that in the worst case that CG, when run in batch mode, will converge asymptotically no faster than NAG (also run in batch mode) for certain specially designed quadratics with very evenly distributed eigenvalues/curvatures. Thus it is worth asking whether the quadratics which arise during the optimization of neural networks by HF are such that CG has a distinct advantage in optimizing them over NAG, or if they are closer to the aforementioned worst-case examples. To examine this question we took a quadratic generated during the middle of a typical run of HF on the curves dataset and compared the convergence rate of CG, initialized from zero, to NAG (also initialized from zero). Figure <ref type="figure">5</ref> in the appendix presents the results of this experiment. While this experiment indicates some potential advantages to HF, the closeness of the performance of NAG and HF suggests that these results might be explained by the solutions leaving the area of trust in the quadratics before any extra speed kicks in, or more subtly, that the faithfulness of approximation goes down just enough as CG iterates to o↵set the benefit of the acceleration it provides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Martens (2010) and <ref type="bibr">Martens &amp; Sutskever (2011)</ref> demonstrated the e↵ectiveness of the HF method as a tool for performing optimizations for which previous attempts to apply simpler first-order methods had failed. While some recent work <ref type="bibr" target="#b3">(Chapelle &amp; Erhan, 2011;</ref><ref type="bibr" target="#b7">Glorot &amp; Bengio, 2010)</ref> suggested that first-order methods can actually achieve some success on these kinds of problems when used in conjunction with good initializations, their results still fell short of those reported for HF. In this paper we have completed this picture and demonstrated conclusively that a large part of the remaining performance gap that is not addressed by using a well-designed random initialization is in fact addressed by careful use of momentumbased acceleration (possibly of the Nesterov type). We showed that careful attention must be paid to the momentum constant µ, as predicted by the theory for local and convex optimization.</p><p>Momentum-accelerated SGD, despite being a firstorder approach, is capable of accelerating directions of low-curvature just like an approximate Newton method such as HF. Our experiments support the idea that this is important, as we observed that the use of stronger momentum (as determined by µ) had a dramatic e↵ect on optimization performance, particularly for the RNNs. Moreover, we showed that HF can be viewed as a first-order method, and as a generalization of NAG in particular, and that it already derives some of its benefits through a momentum-like mechanism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (Top) Classical Momentum (Bottom) Nesterov Accelerated Gradient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>who advocates using what amounts to µ The table reports the squared errors on the problems for each combination of µmax and a momentum type (NAG, CM). When µmax is 0 the choice of NAG vs CM is of no consequence so the training errors are presented in a single column. For each choice of µmax , the highest-performing learning rate is used. The column SGDC lists the results of<ref type="bibr" target="#b3">Chapelle &amp; Erhan (2011)</ref> who used 1.7M SGD steps and tanh networks. The column HF † lists the results of HF without L2 regularization, as described in sec. 5; and the column HF ⇤ lists the results of<ref type="bibr" target="#b17">Martens (2010)</ref>.</figDesc><table><row><cell>task</cell><cell>0</cell><cell>(SGD)</cell><cell cols="4">0.9N 0.99N 0.995N 0.999N</cell><cell cols="4">0.9M 0.99M 0.995M 0.999M</cell><cell>SGDC</cell><cell>HF  †</cell><cell>HF ⇤</cell></row><row><cell>Curves</cell><cell></cell><cell>0.48</cell><cell>0.16</cell><cell>0.096</cell><cell>0.091</cell><cell>0.074</cell><cell>0.15</cell><cell>0.10</cell><cell>0.10</cell><cell>0.10</cell><cell>0.16</cell><cell>0.058</cell><cell>0.11</cell></row><row><cell>Mnist</cell><cell></cell><cell>2.1</cell><cell>1.0</cell><cell>0.73</cell><cell>0.75</cell><cell>0.80</cell><cell>1.0</cell><cell>0.77</cell><cell>0.84</cell><cell>0.90</cell><cell>0.9</cell><cell>0.69</cell><cell>1.40</cell></row><row><cell>Faces</cell><cell></cell><cell>36.4</cell><cell>14.2</cell><cell>8.5</cell><cell>7.8</cell><cell>7.7</cell><cell>15.3</cell><cell>8.7</cell><cell>8.3</cell><cell>9.3</cell><cell>NA</cell><cell>7.5</cell><cell>12.0</cell></row><row><cell></cell><cell></cell><cell cols="4">problem before after</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Curves</cell><cell>0.096</cell><cell>0.074</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Mnist</cell><cell>1.20</cell><cell>0.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Faces</cell><cell>10.83</cell><cell>7.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>t = 1 3/(t + 5) after some manipulation</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The e↵ect of low-momentum finetuning for NAG. The table shows the training squared errors before and after the momentum coe cient is reduced. During the primary ("transient") phase of learning we used the optimal momentum and learning rates.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The table reports the training squared error that is attained by changing the scale of the initialization.</figDesc><table><row><cell>SI scale multiplier</cell><cell cols="2">0.25 0.5</cell><cell>1</cell><cell>2</cell><cell>4</cell></row><row><cell>error</cell><cell>16</cell><cell cols="4">16 0.074 0.083 0.35</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Work was done while the author was at the University of Toronto.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is dicult</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large scale online learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 16: Proceedings of the 2003 Conference</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">217</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved Preconditioner for Hessian Free Optimization</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Better mini-batch algorithms via accelerated gradient methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sridharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.4574</idno>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contextdependent pre-trained deep neural networks for largevocabulary speech recognition. Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="42" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards faster stochastic gradient search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Darken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1009" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the di culty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS 2010</title>
				<meeting>AISTATS 2010</meeting>
		<imprint>
			<date type="published" when="2010-05">may 2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>personal communication</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">304</biblScope>
			<biblScope unit="page" from="78" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An optimal method for stochastic composite optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
				<imprint>
			<date type="published" when="1998">2010. 1998</date>
			<biblScope unit="page" from="546" to="546" />
		</imprint>
	</monogr>
	<note>Mathematical Programming</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning via Hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
				<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning recurrent neural networks with hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML)</title>
				<meeting>the 28th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training deep and recurrent networks with hessian-free optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="479" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Subword language modeling with neural networks</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Hai-Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cernocky</surname></persName>
		</author>
		<ptr target="http://www.fit.vutbr.cz/imikolov/rnnlm/char.pdf" />
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="14" to="22" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate O(1/sqr(k))</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">. Soviet Mathematics Doklady</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Introductory lectures on convex optimization: A basic course</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dynamics and algorithms for stochastic search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Orr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning made easier by linear transformations in perceptrons</title>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2011 Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<meeting><address><addrLine>Sierra Nevada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating text with recurrent neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning, ICML &apos;11</title>
				<meeting>the 28th International Conference on Machine Learning, ICML &apos;11</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="1017" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic dynamics of learning with momentum in neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wiegerinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Komoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physics A: Mathematical and General</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">4425</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
