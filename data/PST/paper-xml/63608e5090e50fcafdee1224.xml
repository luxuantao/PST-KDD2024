<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Search to Pass Messages for Temporal Knowledge Graph Completion</title>
				<funder>
					<orgName type="full">CCF-Baidu Open Fund and Tsinghua University-Foshan Institute of Advanced Manufacturing</orgName>
				</funder>
				<funder>
					<orgName type="full">XPLORER PRIZE</orgName>
				</funder>
				<funder ref="#_TUHV6gx">
					<orgName type="full">Key Technology Research and Development Program of Science and Technology-Scientific and Technological Innovation Team of Shaanxi Province</orgName>
				</funder>
				<funder ref="#_hhR3C9B">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_uv4Pk36">
					<orgName type="full">National Science Fund for Distinguished Young Scholarship of China</orgName>
				</funder>
				<funder ref="#_WsfT6bg">
					<orgName type="full">Fok Ying-Tong Education Foundationm China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-30">30 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haotong</forename><surname>Du</surname></persName>
							<email>duhaotong@mail.nwpu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>li@nwpu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Artificial Intelligence, Optics and Electronics (iOPEN)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Search to Pass Messages for Temporal Knowledge Graph Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-30">30 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.16740v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Completing missing facts is a fundamental task for temporal knowledge graphs (TKGs). Recently, graph neural network (GNN) based methods, which can simultaneously explore topological and temporal information, have become the state-of-the-art (SOTA) to complete TKGs. However, these studies are based on hand-designed architectures and fail to explore the diverse topological and temporal properties of TKG. To address this issue, we propose to use neural architecture search (NAS) to design data-specific message passing architecture for TKG completion. In particular, we develop a generalized framework to explore topological and temporal information in TKGs. Based on this framework, we design an expressive search space to fully capture various properties of different TKGs. Meanwhile, we adopt a search algorithm, which trains a supernet structure by sampling single path for efficient search with less cost. We further conduct extensive experiments on three benchmark datasets. The results show that the searched architectures by our method achieve the SOTA performances. Besides, the searched models can also implicitly reveal diverse properties in different TKGs. Our code is released in https://github.com/striderdu/ SPA.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A temporal knowledge graph (TKG) <ref type="bibr" target="#b2">(Cai et al., 2022)</ref> is a graph-structural data with many timesensitive relational facts. The facts can be formed as qudaruples (subject entity, relationship, object entity, timestamp), denoted as (s, r, o, t), e.g., <ref type="bibr">(FIFA World Cup, is held in, Qatar, 2022)</ref>. TKGs are used extensively in various applications that require the assistance of temporal knowledge such as temporal question answering <ref type="bibr" target="#b23">(Saxena et al., 2021)</ref>, recommendation systems <ref type="bibr" target="#b45">(Zhao et al., 2022)</ref> and mobility prediction <ref type="bibr">(Wang et al., 2021a)</ref>. * Corresponding author.</p><p>Notably, similar to static KG, most TKGs are inherently incompletion, which seriously hampers their applications in downstream tasks. Therefore, a great number of works focus on TKG completion (TKGC) to infer the missing facts in TKGs. Pioneer embedding-based methods <ref type="bibr" target="#b14">(Leblay and Chekol, 2018;</ref><ref type="bibr" target="#b4">Dasgupta et al., 2018;</ref><ref type="bibr" target="#b8">Goel et al., 2020;</ref><ref type="bibr" target="#b13">Lacroix et al., 2020)</ref> directly construct timeaware score functions to evaluate the plausibility of quadruple. However, embedding-based methods do not explicitly encode local graph structures in TKG, which limits their expressiveness.</p><p>Recently, based on the success of graph neural networks (GNNs), some GNN-based methods have been proposed to solve TKGC. TeMP <ref type="bibr" target="#b33">(Wu et al., 2020)</ref>, a typical GNN-based method for TKGC, discretizes a TKG into multiple static KG snapshots and generates dynamic entity representations along two dimensions: structural neighborhoods and temporal dynamics. Structural encoder extracts feature from local node neighborhoods in each snapshot through message passing and aggregation, while temporal encoder captures feature evolution over multiple time steps by sequential models. T-GAP <ref type="bibr" target="#b11">(Jung et al., 2021)</ref>, views timestamps as properties of links between entities, and proposes the temporal GNN to learn structural and temporal information on the whole graph. GNNs have been demonstrated to achieve better performance for TKGC tasks, due to their powerful expressiveness.</p><p>However, these GNN-based methods use the fixed GNN architectures to tackle different TKGs, failing to explore the diverse topological and temporal properties of TKGs, which prevents the model from fully discovering the diverse implicit patterns in different datasets. More recently, BoxTE <ref type="bibr">(Messner et al., 2022)</ref> has also pointed out this problem. Therefore, it is critical to design data-specific GNN architectures for TKGC task.</p><p>Neural architecture search (NAS) <ref type="bibr" target="#b39">(Yao et al., 2018;</ref><ref type="bibr" target="#b10">Hutter et al., 2019)</ref> has achieved great success in designing data-specific architectures, of which the performances exceed the architectures crafted by human experts in various areas, e.g., computer vision <ref type="bibr">(Zhang et al., 2022a)</ref>, natural language processing <ref type="bibr" target="#b25">(So et al., 2019)</ref>, and graph learning <ref type="bibr" target="#b42">(Zhang et al., 2021)</ref>. More recently, in static KG completion, there are some works that adopt NAS techniques for designing the score function <ref type="bibr">(Zhang et al., 2022b)</ref> or GNN architecture <ref type="bibr">(Wang et al., 2021b)</ref>. However, no one has made similar attempts on TKG. And designing data-specific architectures for TKGC task is nontrivial, because of the demand to simultaneously explore topological and temporal information.</p><p>In this work, we propose a novel method which tries to Search to PAss messages(SPA), to automatically design data-specific architectures for TKGC. Firstly, we design a generalized framework to simultaneously explore topological and temporal information in TKGs. From this, we define a novel and expressive search space, in which different combinations of operations can capture various patterns of different TKGs. To enable efficient search on top of the search space, we adopt a flexible and effective search algorithm, which trains a supernet by sampling single path uniformly, thus greatly reducing the GPU memory cost. To demonstrate the effectiveness of SPA, we conduct extensive experiments on three benchmark datasets of TKGC. Experimental results show that SPA can consistently achieve state-of-the-art performance by designing data-specific architectures. Further empirical results verify the searched models provide implicitly properties expression for different TKGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>As mentioned in the introduction, the GNN-based method for TKGC should be data-specific. Generally, TKGs contain both topological and temporal information. Thus, to design a proper model, we first define a framework which can model topological patterns and temporal contexts jointly. Then, we introduce our novel search space. Finally, we describe our search objective and search algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Generalized Framework</head><p>To search for data-specific and well-performing architectures based on GNN, we need to define a framework which has the ability to model topological and temporal information in TKG. Following some existing works <ref type="bibr" target="#b26">(Taheri et al., 2019;</ref><ref type="bibr" target="#b22">Sankar et al., 2020;</ref><ref type="bibr" target="#b19">Manessi et al., 2020;</ref><ref type="bibr" target="#b33">Wu et al., 2020;</ref><ref type="bibr" target="#b5">Gao et al., 2022;</ref><ref type="bibr" target="#b31">Wang et al., 2022)</ref>, we firstly discretize a TKG into multiple static KG snapshots along the time, and utilize GNNs and sequential models to generate dynamic entity representations.</p><p>The main advantages of this approach include simplicity as well as enabling the use of a wealth of GNN and sequential model techniques. A large number of works on temporal graphs also achieve competitive results with such this approach consisting of combinations of GNNs and recurrent architectures, whereby the former digest graph information and the latter handle dynamism.</p><p>Based on this motivation, we develop a generalized framework that mainly consists of four key modules for learning expressive dynamic entity representation, including spatial aggregation, temporal aggregation, layer connection, and layer fusion. In Figure <ref type="figure">1</ref>, we use a 2-layer architecture as an illustrative example of the generalized framework. More detailed descriptions of the four modules are as follows: 1. Spatial Aggregation at the i-th layer is conducted to aggregate information from the neighbors of s in static snapshot G t and results in the intermediate representation of entity s, as follows,</p><formula xml:id="formula_0">h 1 s,t = O SA (G t , h 0 s ),<label>(1)</label></formula><p>where h 0 s ? H is the initialized embedding of entity s, H is the representation matrix containing embeddings of entities and relations in TKG. 2. Temporal Aggregation at the i-th layer generates temporal feature z i s,t based historical feature sequences h i s,t-? , ? ? ? , h i s,t-1 behind, as follows,</p><formula xml:id="formula_1">z 1 t = O TA (h 1 s,t-? , ? ? ? , h 1 s,t-1 , h 1 s,t ), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ? is a hyper-parameter, stands for the number of input KG snapshots to the model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Layer Connection</head><formula xml:id="formula_3">z s,t = O LF (z 1 s,t , z 2 s,t ).<label>(4)</label></formula><p>Based on this generalized framework, we can search the specific form of each operation to obtain data-specific architecture. An effective search space can be naturally designed by including human-designed operations, the details of which are given in Table <ref type="table" target="#tab_2">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Search Space</head><p>Based on above framework, we design one novel search space with a set of candidate operations as shown in Table <ref type="table" target="#tab_2">1</ref>. In the following, we will describe the details of these operations. Spatial Aggregation. We choose three widely used multi-relational GNNs as alternative spatial aggregation module: RGCN <ref type="bibr" target="#b24">(Schlichtkrull et al., 2018)</ref>, RGAT <ref type="bibr" target="#b1">(Busbridge et al., 2019)</ref>, CompGCN <ref type="bibr" target="#b28">(Vashishth et al., 2020)</ref> Temporal Aggregation. For the temporal aggregation module, we consider two sequential models to learn temporal patterns: GRU <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> , Self-Attention (SA) <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>. Besides, we incorporate the operation IDENTITY, which means using the results of spatial aggregation directly, i.e., z i s,t = h i s,t , rather than learning dynamic feature between snapshots. Layer Connection. It has been well proven in many literatures <ref type="bibr" target="#b16">(Li et al., 2021)</ref> that the use of skip connections between spatial aggregation modules can help alleviate over-smoothing and the vanishing gradient issue, and improve the performance of the model. In our search space, we add three different skip connection operations to encourage various feature reuse, i.e., LC_SKIP, LC_SUM, LC_CONCAT. Layer Fusion. In static graph learning, some studies <ref type="bibr">(Xu et al., 2018a)</ref> focus on obtaining more expressive structure-aware representation by selectively fusing the intermediate representation of spatial aggregation. We borrow this idea to temporal graph learning and provide four fusion operations to integrate the representations of the intermediate temporal aggregation layers with the average, maximum, concatenation and skip, denoted as LF_MEAN, LF_MAX, LF_CONCAT and LF_SKIP, respectively. The search for various fusion operations allows the model to learn to adapt to different dynamic subgraph structures.</p><p>An example of the L-layer search space is shown in Figure <ref type="figure" target="#fig_0">2</ref>. With so many candidate architectures in the search space, SPA can use efficient search algorithm to obtain data-specific architectures beyond existing human-designed ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Search Objective</head><p>Let the training and validation set be D tra and D val , N (W ?,H ; ?) be a TKGC model (where W ?,H represents model parameters containing model weights ? and TKG embedding H, and ? is the model architecture), M be the measurement on D val and L be the loss on D tra . The problem is defined to find an architecture ? such that validation performance is maximized, i.e.,</p><formula xml:id="formula_4">? * = arg max ??A M(N (W * ?,H ; ?), D val ),<label>(5)</label></formula><formula xml:id="formula_5">s.t. W * ?,H = arg min W L(N (W ?,H ; ?), D tra ),<label>(6)</label></formula><p>which is a bi-level optimization problem and is nontrivial to solve. Because the computation cost to get the optimal parameters W * ?,H is generally high. And the search space is large. Thus, how to efficiently search the architectures is a big challenge.</p><p>To perform TKGC task, we use score function to measure the plausibility of each candidate quadruple (s, r, o, t). Since our proposed framework can generate time-aware entity embeddings, we only need static score function.</p><p>Specifically, the score function for quadruple is defined as follows in SPA:</p><formula xml:id="formula_6">?(s, r, o, t) = f (z s,t , h r , z o,t ),<label>(7)</label></formula><p>where z s,t and z o,t are time-aware representaions for subject and object entities, while z r is a learned embedding of the relation r. In this work, we use ComplEx <ref type="bibr" target="#b27">(Trouillon et al., 2016)</ref> as the score function, which is known to perform well on static KGC benchmarks.</p><p>For the loss function, following the setting of TeMP, we employ the cross-entropy loss for parameter learning. More details about loss function is in the Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Search Algorithm</head><p>Based on the proposed framework and the search space, the search algorithm is used to search operations from the corresponding operation set.</p><p>Inspired by recent advances in NAS, we propose to solve Equation ( <ref type="formula" target="#formula_4">5</ref>), (6) using one-shot NAS paradigm, which greatly improves the efficiency of performance estimation by training only one supernet.</p><p>There are two types of methods in one-shot NAS: the single-stage method and the two-stage method. The first one combines supernet training and search in a single stage. Representative methods include DARTS <ref type="bibr" target="#b18">(Liu et al., 2019)</ref>, SNAS <ref type="bibr" target="#b34">(Xie et al., 2019)</ref> Calculate the validation performance for ? in D val . 13: end while 14: return The searched architecture with the highest validation performance.</p><p>etc. The single-stage approach requires that the validation metrics be differentiable to allow supernet training and architecture search to be jointly optimized by gradient-based methods, which is inappropriate for our task as its metric(i.e. MRR) is non-differentiable. And the correlation between the validation loss and the validation metric is unclear.</p><p>Using the validation loss to update the architecture parameters may mislead the search algorithm to find a sub-optimal architecture. Moreover, the single-stage approach requires training the whole supernet, which demands tremendous GPU memory as the proposed search space contains spatial encoder and temporal encoder. Hence, we adopt the two-stage approach, which decouples supernet training and architecture search.</p><p>In this paper, we adopt SPOS <ref type="bibr" target="#b9">(Guo et al., 2020)</ref>, a typical two-stage method, as it can consume the GPU memory less and fully train each candidate operation. Algorithm 1 delineates the full procedure. Supernet Training. For the solution of Equation (6), following SPOS, we construct a supernet structure that each candidate architecture is a single path. In each step of optimization, as shown in Figure <ref type="figure" target="#fig_1">3</ref>. an architecture ? (one path, i.e., the solid part of the figure) is sampled from the search space in a uniformly distributed manner. It guarantees equal expectations of the number of times each architecture is sampled, thus all architectures (and their weights) are trained fully and equally. And then, only the weights corresponding ? are activated and updated. So the GPU memory usage is efficient. Architecture Search. After getting the trained optimal weights of supernet W * ?,H , to solve the problem in Equation ( <ref type="formula" target="#formula_4">5</ref>), we leverage random search to find well-performing architecture ?. This is simple but effective for our search space.</p><p>Finally, architecture with the highest validation performance(i.e. validation MRR) in all iterations will be returned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Settings</head><p>Datasets.</p><p>We perform evaluation on three widely used TKG completion datasets, including ICEWS14 <ref type="bibr" target="#b7">(Garc?a-Dur?n et al., 2018)</ref>, <ref type="bibr">ICEWS05-15 (Garc?a-Dur?n et al., 2018)</ref> and GDELT <ref type="bibr" target="#b15">(Leetaru and Schrodt, 2013)</ref>. ICEWS14 and ICEWS05-15 are two subsets of Integrated Crisis Early Warning System (ICEWS) database with different time spans. GDELT is a subset of Global Database of Events, Language, and Tone (GDELT), which contains facts facts from April 1, 2015 to March 31, 2016. The detailed dataset statistics is presented in the Appendix A.2. Evaluation Metrics. We follow <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref> to use the filtered ranking-based metrics, i.e., mean reciprocal ranking (MRR) and Hit@1/3/10 for evaluation. For both metrics, the larger value indicates the better performance. Baseline Methods. We compare SPA with two types of baselines: human-designed methods and NAS methods.</p><p>For human-designed methods, we take TransE <ref type="bibr" target="#b0">(Bordes et al., 2013)</ref>, Distmult <ref type="bibr" target="#b38">(Yang et al., 2015)</ref>, ComplEx <ref type="bibr" target="#b27">(Trouillon et al., 2016)</ref> and <ref type="bibr">Sim-plE (Kazemi and Poole, 2018)</ref> to represent static KG completion methods, and TTransE <ref type="bibr" target="#b14">(Leblay and Chekol, 2018)</ref>, <ref type="bibr">TA-Distmult (Garc?a-Dur?n et al., 2018</ref><ref type="bibr">), HyTE (Dasgupta et al., 2018)</ref>, DE-SimplE <ref type="bibr" target="#b8">(Goel et al., 2020)</ref>, TNTComplEx <ref type="bibr" target="#b13">(Lacroix et al., 2020)</ref>, ChronoR <ref type="bibr" target="#b21">(Sadeghian et al., 2021)</ref>, TeLM <ref type="bibr" target="#b35">(Xu et al., 2021)</ref> and <ref type="bibr">BoxTE (Messner et al., 2022)</ref> to represent state-of-the-art embeddingbased methods designed for TKGC. For the GNN-based methods, we compare with both TeMP <ref type="bibr" target="#b33">(Wu et al., 2020)</ref> and T-GAP <ref type="bibr" target="#b11">(Jung et al., 2021)</ref> here.</p><p>For NAS methods, since existing methods cannot learn the data-specific architecture for temporal graph, we further provide Random search as the baseline for comparisons based on the proposed search space in Section 2.2. Implementation and Hyperparameters. For all NAS methods (Random baseline and SPA), we derived the candidate GNNs from the search space in the search process. All the searched candidates are tuned individually with hyperparameters like learning rate, weight decay, etc. In this paper, the 3-layer framework is empirically chosen for all NAS methods on all datasets. We set the negative sampling ratio to 500, i.e. 500 negative samples per positive triple. More details about the implementation and hyperparamters are given in Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison</head><p>Table <ref type="table">2</ref> shows the overall result on three benchmarks. As can be seen, there is no clear winner among the human-designed baselines on all datasets. Besides, we can see that SPA consistently outperforms all baselines on all datasets, which demonstrates the effectiveness of SPA on searching for data-specific architectures for TKGC.</p><p>When it comes to NAS baselines, the performance gains of SPA are also significant. On one hand, the Random baselines achieve considerable performance gains on all these datasets, which Type Model ICEWS14 ICEWS05-15 GDELT MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 MRR H@1 H@3 H@10 demonstrates the effectiveness of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human</head><p>On the other hand, compared with Random, which use the designed search space of SPA, the performance gains are from the single path one-shot search algorithm on obtaining better architectures.</p><p>Figure <ref type="figure" target="#fig_2">4</ref> shows the learning curves of GNNbased methods on ICEWS14 and ICEWS05-15, including TeMP, T-GAP and the proposed SPA. As can be seen, the searched architecture not only outperform baselines, but also have comparable time as the other GNN-based methods, which demonstrates the searched architecture can better capture diverse topological and temporal properties of different TKGs. Further, we visualize the searched architectures on three benchmark datasets in Figure <ref type="figure" target="#fig_5">7</ref>, from which it is clear that different operation combinations of these four modules are obtained, i.e., data-specific architectures. We will discuss the details about the searched architectures in Section 3.5.</p><p>Therefore, these results demonstrate the need for data-specific methods for TKGC, and at the same time, the effectiveness of SPA on designing adaptive architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Understanding the Search Algorithm</head><p>In this part, we evaluate the search algorithm from the perspectives of the efficiency of search algorithm, the effectiveness of weight sharing, and the choice of validation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Efficiency of Search Algorithm</head><p>To show the efficiency of the search algorithm, we compare SPA with Random search baseline. As can be seen, random search have to take a long time to train each candidate architecture from scratch, while SPA spend most of the time on training supernet. In the stage of architecture search, SPA directly picks the corresponding weights from the trained supernet for the specific architecture evaluation, which significantly improves the efficiency compared to random search. This is mainly attributed to the weight-sharing strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Effectiveness of Weight Sharing</head><p>To demonstrate the effectiveness of weight sharing, we empirically visualize the rank correlation of the validation performance between the weight sharing strategy and the stand-alone approach, as shown in Figure <ref type="figure" target="#fig_4">6</ref>. For the stand-alone approach, we randomly sample 50 architectures C, train and evaluate them from scratch. About weight sharing, we inherit the corresponding subweights of the trained supernet for each structure in C and evaluate it. As can be seen, it is obvious that the rank of weight sharing validation MRR has near positive correlation with the rank of stand-alone validation MRR. And then, most structures that have high estimated ranks by weight sharing truly have high ranks using the setting of stand-alone. This demonstrates that the weight sharing strategy can search for good structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Choice of Validation Metric</head><p>In Section 2.4, we discuss the rationality of adopting the SPOS <ref type="bibr" target="#b9">(Guo et al., 2020)</ref> method for search algorithm. Here, to show the impact of validation metric for SPOS, we compare the following SPA variants: (i) SPA(train loss), which uses training loss rather than valid MRR for evaluating candidate architecture in the stage of architecture search; (ii) SPA(valid loss), which uses validation loss for evaluating candidate architecture. Moreover, we adopt two variants of DARTS <ref type="bibr" target="#b18">(Liu et al., 2019)</ref> as search algorithms, including SPA-D(train loss) and SPA-D(valid loss), which use gradient-based optimization to update architecture parameters by minimizing training loss and validation loss, respectively.</p><p>Table <ref type="table" target="#tab_6">3</ref> shows the testing MRRs of different variants on ICEWS14 and GDELT. As can be seen, the use of validation MRR can help to select the better sub-network. The variants associated with DARTS run out of memory on GDELT with 3 million facts due to the demand for tremendous GPU memory. Besides, when using the same validation metric, the performances of architecture searched by SPOS consistently outperform that of DARTS, which may be due to the coupling of supernet weights and architecture parameters leading to the selection of inferior architectures.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Studies on the Search Space</head><p>We conduct ablation studies to show the influences of the four modules in the search space. For simplicity, we use two datasets: ICEWS14 and GDELT, and run SPA over different variants of search space, for which the results are shown in Table <ref type="table" target="#tab_8">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Spatial Aggregation Module</head><p>To evaluate how the spatial aggregation module affects the performance, we only search for the other three modules based on fixed aggregators RGCN and RGAT, which denoted as SPA-RGCN and SPA-RGAT, respectively. As shown in Table 4, with fixed aggregators, SPA-RGCN and SPA-RGAT have a performance drop compared with SPA. This indicatess that the diverse spatial aggregation modules can capture various topological information in different TKGs and significantly improve the model performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Temporal Aggregation Module</head><p>To evaluate the importance of searching for temporal aggregation module, we learn to design architectures with fixed temporal aggregation module instead. In Table <ref type="table" target="#tab_8">4</ref>, with the two predefined temporal aggregation operations, the degree of performance degradation is inconsistent across different datasets. To be specific, the performance drop is evident on SPA-IDENTITY for GDELT. But for ICEWS14, the performance of SPA-GRU drops significantly compared to SPA. This observation shows the importance of including temporal aggregation module in the search space. Meanwhile, it shows that the temporal aggregation operations should also be data-specific for TKGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Layer Connection and Layer Fusion Module</head><p>In this section, we evaluate the proposed Layer Connection and Layer Fusion Module, which are novel compared to existing GNN-based architec-tures for TKGC. By fixing the skip-connection function as LC_SKIP, we create the variant SPA-LC_SKIP, which means that we do not search for different skip-connection functions. By fixing the layer fusion function as LF_SKIP, we only preserve the output of last temporal aggregation module as entity representation. This variant is denoted by SPA-LA_SKIP, which means the outputs of intermediate layers are not used.</p><p>From Table <ref type="table" target="#tab_8">4</ref>, we can see that ? The performance drop of SPA-LC_SKIP means that the spatial aggregation module can benefit from skip-connection, which have been shown in previous works <ref type="bibr" target="#b16">(Li et al., 2021;</ref><ref type="bibr" target="#b17">Li and King, 2020)</ref>.</p><p>? The performance drop of SPA-LA_SKIP means that the outputs of intermediate layers are important for the final representation in temporal graph learning. Thus, it demonstrates the importance of the proposed Layer Fusion Module. Taking all results in Table <ref type="table" target="#tab_8">4</ref> into consideration, we can see that it is important for TKGC to search for combinations of operations from the four essential modules by SPA, which demonstrates the contribution of the proposed framework and the designed search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Case Study</head><p>We visualize the searched architectures on three benchmark datasets in Figure <ref type="figure" target="#fig_5">7</ref>. Especially, the searched temporal aggregation modules contain more IDENTITY operations in ICEWS14, while in GDELT more SA operations are searched. This observation implies that for the ICEWS14 dataset, capturing complex temporal context may not be necessary in comparison to GDELT.</p><p>To verify above conjecture, we compare the differences in temporal properties between ICEWS14 and GDELT. From the perspective of temporal properties, as shown in Figure <ref type="figure" target="#fig_6">8</ref>, the activity frequency of entities on GDELT is much higher than ICEWS14. This means that we do not need to design architectures with complicated sequential models for ICEWS14, but it is useful for GDELT. This result confirms our conjecture and the importance of designing data-specific architectures for TKGC.</p><p>Taking into consideration these experimental results from Figure <ref type="figure" target="#fig_5">7</ref> and 8, it indicates the effectiveness of our method in finding data-specific architectures for TKGC.  In the literature, existing methods for TKGC can be roughly divided into two categories: the embedding-based method and the GNN-based method. Embedding-based methods <ref type="bibr" target="#b14">(Leblay and Chekol, 2018;</ref><ref type="bibr" target="#b4">Dasgupta et al., 2018;</ref><ref type="bibr" target="#b8">Goel et al., 2020;</ref><ref type="bibr" target="#b13">Lacroix et al., 2020;</ref><ref type="bibr">Messner et al., 2022)</ref> design time-aware score functions to measure the correctness of quadruples in TKGs. Although embedding-based methods well capture the semantic patterns in TKGs, they fail to capture the more complex topological patterns.</p><p>Recently, with the success of graph neural networks (GNNs), GNN has achieved significant progress in temporal knowledge graph completion. TeMP <ref type="bibr" target="#b33">(Wu et al., 2020)</ref>, uses structural encoder to obtain entity representations including multihop neighbor information and relies on temporal encoder to incorporate structural and temporal information into entity representation. T-GAP <ref type="bibr" target="#b11">(Jung et al., 2021)</ref> designs one temporal GNN to learn structural and temporal information on TKG, and another GNN to dynamically expand and prune the inference subgraph from the query entity e q by attention flow <ref type="bibr">(Xu et al., 2018b)</ref>. However, existing GNN-based methods use predefined structure and temporal encoder, which are difficult to adapt to various datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Neural Architecture Search</head><p>Neural architecture search (NAS) aims to automatically find suitable neural architecture for the given dataset, which has been demonstrated as a promising technique in many research fields such as computer vision and neural language processing.</p><p>More recently, some works focus on automatically designing GNNs by NAS. GraphNAS <ref type="bibr" target="#b6">(Gao et al., 2021)</ref>, AGNN <ref type="bibr" target="#b46">(Zhou et al., 2019)</ref> learn to design aggregation operation. AutoGraph <ref type="bibr" target="#b17">(Li and King, 2020)</ref> learns to select the connections in each intermediate layer. SNAG <ref type="bibr" target="#b43">(Zhao et al., 2020)</ref> and SANE <ref type="bibr" target="#b44">(Zhao et al., 2021)</ref> search to select and fuse the features of intermediate layers in the output node. AutoGEL <ref type="bibr">(Wang et al., 2021b)</ref> focuses on designing intra-layer and inter-layer message passing GNN architectures automatically. However, no work applies NAS technique to design GNN for dynamic graphs or temporal knowledge graphs. To the best of our knowledge, SPA is the first method to learn data-specific GNN architectures for TKG completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel method SPA to automatically design data-specific architectures for TKGC task. We define a novel and expressive search space, in which different combinations of operations can capture various patterns of different TKGs. To enable efficient search on top of the search space, we adopt a flexible and effective search algorithm, which trains a simplified supernet in that each architecture is a single path, thus greatly reducing the GPU memory cost. To demonstrate the effectiveness of SPA for TKGC, we conduct extensive experiments on three datasets. The experimental results show that SPA can search SOTA data-specific architectures for TKGC.</p><p>For future work, we will explore more advanced NAS approaches to further improve the search efficiency of SPA. Besides, a promising direction is to explore how to efficiently search network architectures and hyper-parameters simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>There are two limitations for SPA. (1) SPA is focused on method design rather than system design. In the future, we will co-design the algorithm and the system to further improve the efficiency. (2) At present, SPA only search for data-specific architectures, while hyper-parameters are also important for TKGC. A promising direction is to explore how to efficiently search network architectures and hyper-parameters simultaneously.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The illustration of search space of SPA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The illustration of the single path supernet. In the training stage, the weights of the solid line part (RGAT, GRU) are activated and updated, the dotted portions are masked and inactivated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison on convergence between the searched architectures (by SPA) and human-designed GNN-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison of SPA with Random search during the search process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Rank correlation between stand-alone and weight sharing approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The searched architectures on three benchmark datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure8: Difference in temporal property between two datasets. The figure represents the proportion of timestamps when the entity is active 1 to the total timestamps. As can be seen, entities in GDELT are much more active than those in ICEWS14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>, which denoted as RGCN, RGAT, COMPGCN. The operations used in our search space.</figDesc><table><row><cell>Module name</cell><cell>Operations</cell></row><row><cell>Spatial Aggregation</cell><cell>RGCN, RGAT,</cell></row><row><cell>(O SA )</cell><cell>COMPGCN</cell></row><row><cell>Temporal Aggregation</cell><cell>GRU, SA,</cell></row><row><cell>(O TA )</cell><cell>IDENTITY</cell></row><row><cell>Layer Connection</cell><cell>LC_SKIP, LC_SUM,</cell></row><row><cell>(O LC )</cell><cell>LC_CONCAT</cell></row><row><cell>Layer Fusion</cell><cell>LF_MAX, LF_CONCAT,</cell></row><row><cell>(O LF )</cell><cell>LF_SKIP, LF_MEAN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>,</head><label></label><figDesc>Algorithm 1 SPA -Search to PAss messages Require: Training dataset D tra , validation dataset D val , the epoch T 1 for train supernet, the epoch T 2 for search architecture, the search space A.</figDesc><table><row><cell cols="2">Ensure: The searched architecture.</cell></row><row><cell cols="2">1: Random initialize the parameter of supernet</cell></row><row><cell></cell><cell>W.</cell></row><row><cell cols="2">2: while t &lt; T 1 do</cell></row><row><cell>3:</cell><cell>for each minibatch B ? D train do</cell></row><row><cell>4:</cell><cell>Random sample ? from A.</cell></row><row><cell>5:</cell><cell>Calculate the training loss L tra for ?.</cell></row><row><cell>6:</cell><cell>Update weight subset W ?,H (?) with</cell></row><row><cell></cell><cell>L tra .</cell></row><row><cell>7:</cell><cell>end for</cell></row><row><cell cols="2">8: end while</cell></row><row><cell cols="2">9: while t &lt; T 2 do</cell></row><row><cell>10:</cell><cell>Random sample ? from A.</cell></row><row><cell>11:</cell><cell>Inherit weight subset W  *  ?,H (?) from W  *  ?,H .</cell></row></table><note><p>12:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance of SPA using different variants of search algorithm. "OOM" means out of memory.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performance of SPA using different search spaces. The first column represents the corresponding module we try to evaluate by fixing it with one OP in the reduced search space.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>An entity is active at a timestep if it has at least one neighboring entity in the same KG snapshot(Wu et al.,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2020).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers for their valuable comments. This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">U1803263</rs>), the <rs type="funder">National Science Fund for Distinguished Young Scholarship of China</rs> (No. <rs type="grantNumber">62025602</rs>), <rs type="funder">Fok Ying-Tong Education Foundationm China</rs> (No. <rs type="grantNumber">171105</rs>), <rs type="funder">Key Technology Research and Development Program of Science and Technology-Scientific and Technological Innovation Team of Shaanxi Province</rs> (No. <rs type="grantNumber">2020TD-013</rs>), and the <rs type="funder">XPLORER PRIZE</rs>. Q. Yao is sponsored by <rs type="funder">CCF-Baidu Open Fund and Tsinghua University-Foshan Institute of Advanced Manufacturing</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hhR3C9B">
					<idno type="grant-number">U1803263</idno>
				</org>
				<org type="funding" xml:id="_uv4Pk36">
					<idno type="grant-number">62025602</idno>
				</org>
				<org type="funding" xml:id="_WsfT6bg">
					<idno type="grant-number">171105</idno>
				</org>
				<org type="funding" xml:id="_TUHV6gx">
					<idno type="grant-number">2020TD-013</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Loss Function</head><p>To train our TKGC model using score function, the model parameters are learned using gradient-based optimization in mini-batches. Specifically. for each quadruple ? = (s, r, o, t) ? D + , we sample a negative set of entities D - ?,o = {o |(s, r, o , t) ? D + }. Then, we apply the cross-entropy loss function for object queries to train the model:</p><p>.</p><p>(8) Similarly, we can also obtain the loss for subject queries L sub . The final training loss is the sum of losses for two types of queries: L = L sub + L obj .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Statistics and Characteristics</head><p>The dataset statistics are summarized in Table <ref type="table">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Implementation Details</head><p>All the experiments are implemented in Python with the PyTorch framework <ref type="bibr" target="#b20">(Paszke et al., 2019)</ref> and run on a single NVIDIA RTX 3090 GPU with 24GB memory.</p><p>For Random, we use the Adam optimizer, set learning rate is 0.001, dropout rate = 0.1, and L2 norm to 0.0005. We randomly sample 100 architectures from the designed search space and train them from scratch. After training finished, we select one candidate with the highest validation performance.</p><p>For SPA, we set the epoch T 1 for supernet training is 800 and the epoch T 2 for architecture searching is 1000. in each minibatch sample single path to train supernet. After training process is finished, we derive the candidate architecture with the highest validation performance from the supernet by random search. Repeat 5 times with different seeds, we can get 5 candidates.</p><p>Other hyperparameters settings for NAS methods during the search process are shown in Table <ref type="table">6</ref>.</p><p>The searched candidates are finetuned individually with the hyper-parameters shown in Table <ref type="table">7</ref>. In the stage of fine-tuning, we use the ReduceL-ROnPlateau scheduler. Each method candidates 30 hyper steps. In each hyper step, a set of hyperparameters will be sampled from   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Neural Information Processing Systems</title>
		<meeting>the 26th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Dan</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dane</forename><surname>Sherburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05811</idno>
		<title level="m">Relational graph attention networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Borui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longxiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08236</idno>
		<title level="m">Temporal knowledge graph completion: A survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HyTE: Hyperplane-based temporally aware knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Swayambhu</forename><surname>Shib Sankar Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Nath Ray</surname></persName>
		</author>
		<author>
			<persName><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1225</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2001" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel representation learning for dynamic graphs based on graph convolutional networks</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2022.3159661</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1403" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning sequence encoders for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc?a-Dur?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastijan</forename><surname>Duman?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1516</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4816" to="4821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Diachronic embedding for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Rishab</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3988" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="544" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Automated machine learning: methods, systems, challenges</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Vanschoren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Springer Nature</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to walk across time for interpretable temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Jaehun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhong</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="786" to="795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor decompositions for temporal knowledge base completion</title>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deriving validity time in knowledge graph</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melisachew</forename><forename type="middle">Wudage</forename><surname>Chekol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1771" to="1776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gdelt: Global data on events, location, and tone, 1979-2012</title>
		<author>
			<persName><forename type="first">Kalev</forename><surname>Leetaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Schrodt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISA annual convention</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deepgcns: Making gcns go as deep as cnns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolina</forename><forename type="middle">Delgadillo</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulellah</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><forename type="middle">Kassem</forename><surname>Abualshour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3074057</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Autograph: Automated graph neural network</title>
		<author>
			<persName><forename type="first">Yaoman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic graph convolutional networks. Pattern Recognition, Johannes Messner, Ralph Abboud, and Ismail Ilkan Ceylan</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Manzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="7779" to="7787" />
		</imprint>
	</monogr>
	<note>Temporal knowledge graph completion using box embeddings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pytorch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chronor: Rotation based temporal knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6471" to="6479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dysat: Deep neural representation learning on dynamic graphs via self-attention networks</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Question answering over temporal knowledge graphs</title>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.520</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6663" to="6676" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to represent the evolution of dynamic graphs with recurrent models</title>
		<author>
			<persName><forename type="first">Aynaz</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanya</forename><surname>Berger-Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of The 2019 World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="301" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Th?o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2021a. Spatio-temporal urban knowledge graph enabled mobility prediction</title>
		<author>
			<persName><forename type="first">Huandong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaohong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evolutionary markov dynamics for network community detection</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianghua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyou</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2020.2997043</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1206" to="1220" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autogel: An automated graph neural network with explicit link information</title>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24509" to="24522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TeMP: Temporal message passing for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackie</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kit</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.462</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5730" to="5746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal knowledge graph completion using a linear temporal regularizer and multivector embeddings</title>
		<author>
			<persName><forename type="first">Chengjin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yung-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mojtaba</forename><surname>Nayyeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Xiaoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songpeng</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00497</idno>
		<title level="m">Modeling attention flow on graphs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Wei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13306</idno>
		<title level="m">Taking human out of learning applications: A survey on automated machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Searching a high performance feature extractor for text recognition network</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3205748</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bilinear scoring function search for knowledge graph learning</title>
		<author>
			<persName><forename type="first">Yongqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2022.3157321</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automated machine learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4704" to="4712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simplifying architecture search for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11652</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Search to aggregate neighborhood for graph neural network</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 37th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="552" to="563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Time-aware path reasoning on knowledge graph for recommendation</title>
		<author>
			<persName><forename type="first">Yuyue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyong</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>TOIS</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<title level="m">Neural architecture search of graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
