<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-10-15">15 Oct 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sutawika</forename><surname>Lintang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zaid</forename><surname>Bigscience</surname></persName>
						</author>
						<author>
							<persName><surname>Alyafeai</surname></persName>
						</author>
						<author>
							<persName><roleName>IRISA</roleName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Imatag</forename><surname>Arnaud</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stiegler</forename><surname>Hyperscience</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Hugging Face</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Brown University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MULTITASK PROMPTED TRAINING ENABLES ZERO-SHOT TASK GENERALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-10-15">15 Oct 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.08207v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have recently been shown to attain reasonable zero-shot generalization on a diverse set of tasks <ref type="bibr" target="#b10">(Brown et al., 2020)</ref>. It has been hypothesized that this is a consequence of implicit multitask learning in language model training <ref type="bibr" target="#b65">(Radford et al., 2019)</ref>. Can zero-shot generalization instead be directly induced by explicit multitask learning? To test this question at scale, we develop a system for easily mapping general natural language tasks into a human-readable prompted form. We convert a large set of supervised datasets, each with multiple prompts using varying natural language. These prompted datasets allow for benchmarking the ability of a model to perform completely unseen tasks specified in natural language. We fine-tune a pretrained encoder-decoder model <ref type="bibr" target="#b66">(Raffel et al., 2020;</ref><ref type="bibr" target="#b43">Lester et al., 2021)</ref> on this multitask mixture covering a wide variety of tasks. The model attains strong zero-shot performance on several standard datasets, often outperforming models up to 16× its size. Further, our approach attains strong performance on a subset of tasks from the BIG-Bench benchmark, outperforming models up to 6× its size. All prompts and trained models are available at github.com/bigscience-workshop/promptsource/ and huggingface.co/bigscience/T0pp. * Equal contribution. Full list of individual contributions detailed in Appendix A.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent work has shown that large language models exhibit the ability to perform reasonable zeroshot generalization to new tasks <ref type="bibr" target="#b10">(Brown et al., 2020;</ref><ref type="bibr" target="#b37">Kim et al., 2021)</ref>. Despite only being trained on language modeling objectives, these models can perform relatively well at new tasks that they have not been explicitly trained to perform, for instance answering a question on a passage or performing "How is air traffic controlled?" "How do you become an air traffic controller?" Pick one: these questions are duplicates or not duplicates.</p><p>I know that the answer to "What team did the Panthers defeat?" is in "The Panthers finished the regular season <ref type="bibr">[...]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paraphrase identification</head><p>Suppose "The banker contacted the professors and the athlete". Can we infer that "The banker contacted the professors"?</p><p>The picture appeared on the wall of a Poundland store on Whymark Avenue <ref type="bibr">[...]</ref> How would you rephrase that in a few words?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural language inference</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-task training</head><p>Zero-shot generalization</p><p>Figure <ref type="figure">1</ref>: Our model and prompt format. T0 is an encoder-decoder model that consumes textual inputs and produces target responses. It is trained on a multitask mixture of NLP datasets partitioned into different tasks. Each dataset is associated with multiple prompt templates that are used to format example instances to input and target pairs. Italics indicate the inserted fields from the raw example data. After training on a diverse mixture of tasks (top), our model is evaluated on zero-shot generalization to tasks that were unseen during training (bottom).</p><p>summarization. An influential hypothesis is that large language models generalize to new tasks as a result of an implicit process of multitask learning <ref type="bibr" target="#b65">(Radford et al., 2019)</ref>. As a byproduct of learning to predict the next word, a language model is forced to learn from a mixture of implicit tasks included in their pretraining corpus. For example, by training on generic text from a web forum, a model might implicitly learn the format and structure of question answering. This gives large language models the ability to generalize to unseen tasks presented as natural language prompts, going beyond most large-scale explicit multitask setups <ref type="bibr" target="#b34">(Khashabi et al., 2020a;</ref><ref type="bibr" target="#b95">Ye et al., 2021)</ref>. However, this ability requires a sufficiently large model and is sensitive to the wording of its prompts <ref type="bibr" target="#b62">(Perez et al., 2021;</ref><ref type="bibr" target="#b92">Zhao et al., 2021;</ref><ref type="bibr" target="#b68">Reynolds and McDonell, 2021</ref>).</p><p>Yet, it is an open question how implicit this multitask learning really is. Given the scale of training data, it is not unreasonable to expect that some common natural language processing (NLP) tasks would appear in an explicit form in the dataset, thereby directly training the language model on the task. For example, there are many websites that simply contain lists of trivia questions and answers;<ref type="foot" target="#foot_0">1</ref> this data is precisely supervised training data for the task of closed-book question answering <ref type="bibr" target="#b69">(Roberts et al., 2020)</ref>. Given the scale of large language models and the datasets they are trained on, this explicit multitask supervision could feasibly play a large role in zero-shot generalization.</p><p>In this paper, we instead focus on intentionally and explicitly training large language models in a supervised and massively multitask fashion. Our approach uses a multitask training mixture made up of a large set of different tasks specified in natural language prompts. Our goal is to induce a model to better generalize to unseen tasks without requiring massive scale, as well as being more robust to the wording choices of the prompts. To convert a large set of natural language tasks into prompted form, we use a simple templating language for structured datasets. We develop an interface for prompt collection from public contributors that facilitated the collection of a large multitask mixture with multiple prompts per dataset. We then train a variant of the T5 encoder-decoder model <ref type="bibr" target="#b66">(Raffel et al., 2020;</ref><ref type="bibr" target="#b43">Lester et al., 2021)</ref> on a subset of the tasks (each with multiple datasets) and then evaluate tasks that the model was not trained on.</p><p>Our experiments study two questions. First, does multitask prompted training improve generalization to unseen tasks? Second, does training on a wider range of prompts improve robustness to prompt wording? For the first question, we find that multitask training enables zero-shot task generalization by showing that our model matches or exceeds the performance of GPT-3 <ref type="bibr" target="#b10">(Brown et al., 2020)</ref> on 9 out of 11 held-out datasets, despite being about 16× smaller. We also show that the model improves over a large baseline language model on 13/14 comparable tasks in the BIG-bench benchmark<ref type="foot" target="#foot_1">2</ref> . For the second question, we find that training on more prompts per dataset consistently improves the median and decreases the variability of performance on held-out tasks. Training on prompts from a wider range of datasets also generally improves the median but does not decrease the variability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this work, we distinguish implicit multitask learning in language model pretraining from explicit multitask learning <ref type="bibr" target="#b11">(Caruana, 1997)</ref>, the technique for mixing multiple tasks into a single supervised training process. Models trained with multitask learning have long been shown to have improved performance in NLP <ref type="bibr" target="#b15">(Collobert and Weston, 2008)</ref>. Since different tasks have different outputs, applying multitask learning requires a shared format, and various have been used <ref type="bibr" target="#b27">(Hashimoto et al., 2016;</ref><ref type="bibr" target="#b52">McCann et al., 2018)</ref>. Several multitask works also explore few-shot and zero-shot generalization to new datasets with large pretrained models (e.g., <ref type="bibr" target="#b86">Vu et al., 2020;</ref><ref type="bibr" target="#b95">Ye et al., 2021)</ref>.</p><p>Natural language prompting is the method of reformatting NLP tasks in the format of a natural language response to natural language input. The development of text-to-text pretrained models such as T5 <ref type="bibr" target="#b66">(Raffel et al., 2020)</ref> makes prompts a particularly useful method for multitask learning. For example, <ref type="bibr" target="#b34">Khashabi et al. (2020a)</ref> reformat 20 question-answering datasets into a single prompt of question: ... (A)... (B)... (C)... context: ..., while later work such as <ref type="bibr">Zhong et al. (2021)</ref> and <ref type="bibr" target="#b89">Wang et al. (2021)</ref> cast a range of datasets into a single boolean QA prompt or a single NLI prompt, respectively. Although effective, these single-prompt methods typically do not generalize to new prompts or new tasks inexpressible in their fixed format.</p><p>More generally, <ref type="bibr" target="#b76">Schick and Schütze (2021)</ref> and <ref type="bibr" target="#b10">Brown et al. (2020)</ref> popularized using prompts as a generic method for all NLP tasks. <ref type="bibr" target="#b55">Mishra et al. (2021)</ref> further extend this approach to a multitask training setup, training on prompts adapted from 8 datasets' crowdsourcing instructions. Their prompts include examples in addition to instructions, whereas we focus on zero-shot generalization. Additionally, they choose their training and evaluation mixtures to have similar distributions, whereas we aim for the opposite in targeting generalization to unseen tasks ( §3). Finally, concurrent work by <ref type="bibr" target="#b92">Wei et al. (2021)</ref> shares a similar research question with us, although we differ in several substantive regards, e.g., prompt diversity, model scale, and held-out-task scheme. We discuss our differences in detail in Section 7.</p><p>Finally, in explaining the success of prompts, the leading hypothesis is that models learn to understand the prompts as task instructions which help them generalize to unseen tasks <ref type="bibr" target="#b92">(Wei et al., 2021;</ref><ref type="bibr" target="#b55">Mishra et al., 2021;</ref><ref type="bibr" target="#b76">Schick and Schütze, 2021;</ref><ref type="bibr" target="#b10">Brown et al., 2020)</ref>. However, the extent to which this success depends on the semantic meaningfulness of the prompts has been challenged <ref type="bibr" target="#b91">(Webson and Pavlick, 2021;</ref><ref type="bibr" target="#b49">Logan et al., 2021)</ref>. Thus, in this work, we remain agnostic as to why prompts support generalization. We only claim that prompts serve as a natural format for multitask training which empirically supports generalization to unseen tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MEASURING GENERALIZATION TO UNSEEN TASKS</head><p>We begin by assuming an underlying partition of NLP datasets into tasks. We use the term "task" to refer to a general NLP ability that is tested by a group of specific datasets. To evaluate zero-shot generalization to new tasks, we train on a subset of tasks and evaluate on a held-out group of tasks.</p><p>Unfortunately, NLP task categorization is fuzzy, particularly if trying to isolate a unique skill. For example, many datasets evaluate commonsense knowledge, and some multitask works (e.g., <ref type="bibr" target="#b10">Brown et al., 2020;</ref><ref type="bibr" target="#b92">Wei et al., 2021)</ref> define commonsense as a standalone task. However, commonsense Noting that grouping by task is an imperfect heuristic, we err on the side of organizing our task taxonomy based on the task format as opposed to required skill, largely based on conventions in the literature <ref type="bibr" target="#b35">(Khashabi et al., 2020b;</ref><ref type="bibr" target="#b86">Vu et al., 2020;</ref><ref type="bibr" target="#b95">Ye et al., 2021)</ref>. We collect all datasets from these papers and exclude those that are not in English (which also excludes programming languages and structured annotations such as parse trees) or if they require special domain knowledge (e.g., biomedicine). This yields 12 tasks and 62 datasets with publicly contributed prompts as of writing in our training and evaluation mixtures (Figure <ref type="figure" target="#fig_0">2</ref>). All experiments use datasets in the Hugging Face datasets library <ref type="bibr" target="#b45">(Lhoest et al., 2021)</ref>.</p><p>To test zero-shot generalization, we hold out all constituent datasets of four tasks: natural language inference (NLI), sentence completion, word sense disambiguation, and coreference resolution. We choose NLI as a held-out task because humans also zero-shot generalize to NLI as an unseen task: Most humans are never explicitly trained to classify whether a premise sentence entails or contradicts a hypothesis sentence, yet they find it intuitive to perform this task without training <ref type="bibr" target="#b94">(Williams et al., 2020)</ref>. For the same reason, we also hold out coreference resolution and word sense disambiguation. We further hold out story completion because it is a task possibly too similar to NLI (Appendix D.2 discusses this in detail). Additionally, we do not train our main model on any datasets that GPT-3 used for evaluation, so that our main results will be a fair zero-shot comparison. We verify that data for those tasks is not leaked through the pretraining corpus as detailed in Appendix E. Lastly, we also evaluate on a subset of the datasets from BIG-Bench (BIG-bench collaboration, 2021), which is a recent community-driven benchmark to create a diverse collection of difficult tasks to test the abilities of large language models. The subset of BIG-Bench comprise a language-oriented selection of tasks for which the BIG-Bench maintainers have prepared preliminary results and which constitute text that is in-vocabulary for the T5 tokenizer (i.e. only contain natural English-language text without emojis or other special characters). All tasks from BIG-Bench are novel tasks that were unseen in our training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>The picture appeared on the wall of a Poundland store on Whymark Avenue...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Graffiti artist Banksy is believed to be behind...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>The picture appeared on the wall of a Poundland store on Whymark Avenue...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Graffiti artist Banksy is believed to be behind...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>The picture appeared on the wall of a Poundland store on Whymark Avenue...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Graffiti artist Banksy is believed to be behind...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QQP Paraphrase) XSum (Summary)</head><p>{Question1} {Question2} Pick one: These questions are duplicates or not duplicates.</p><p>I received the questions "{Question1}" and "{Question2}". Are they duplicates?</p><p>{Choices[label]} {Document} How would you rephrase that in a few words?</p><p>First, please read the article: {Document} Now, can you write me an extremely short abstract for it? {Summary}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question1</head><p>How is air traffic controlled?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question2</head><p>How do you become an air traffic controller?</p><p>Label 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question1</head><p>How is air traffic controlled?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question2</head><p>How do you become an air traffic controller?</p><p>Label 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question1</head><p>How is air traffic controlled?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question2</head><p>How do you become an air traffic controller?</p><p>Label 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question1</head><p>How is air traffic controlled?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question2</head><p>How do you become an air traffic controller?</p><p>Label 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Document</head><p>The picture appeared on the wall of a Poundland store on Whymark Avenue...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>Graffiti artist Banksy is believed to be behind... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{Choices[label]} {Summary}</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A UNIFIED PROMPT FORMAT</head><p>All datasets are given to our model in natural language prompted form to enable zero-shot experimentation. To facilitate writing a large collection of prompts, we develop a templating language and an application that make it easy to convert diverse datasets into prompts. We define a prompt as consisting of an input template and target template, along with a collection of associated metadata. The templates are functions mapping a data example into natural language for the input and target sequences. Practically, the templates allow the user to mix arbitrary text with the data fields, metadata, and other code for rendering and formatting raw fields. For example, in the case of an NLI dataset, the example would include fields for Premise, Hypothesis, Label. An input template would be If {Premise} is true, is it also true that {Hypothesis}?, whereas a target template can be defined with the label choices {Choices[label]}. Here Choices is prompt-specific metadata that consists of the options yes, maybe, no corresponding to label being entailment (0), neutral <ref type="bibr" target="#b99">(1)</ref> or contradiction (2). Other metadata documents additional properties, such as an evaluation metric. Each data example is materialized with many different prompt templates as shown in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>To develop prompts, we built an interface for interactively writing prompts on datasets. We put out an open call in the research community for users to contribute prompts. 36 contributors affiliated with 24 institutions in 8 countries participated. Since our goal was to train a model to be robust to prompt format, and since the question of what makes a prompt effective remains unresolved <ref type="bibr" target="#b91">(Webson and Pavlick, 2021;</ref><ref type="bibr" target="#b49">Logan et al., 2021;</ref><ref type="bibr" target="#b68">Reynolds and McDonell, 2021)</ref>, we encouraged contributors to be open in their style and create a diverse set of prompts. The main annotation guideline was that prompts needed to be grammatical and understandable by a native English speaker with no prior experience of the tasks. Additionally, prompts that required explicit counting or numerical indexing were removed in favor of natural language variants. For example, instead of predicting indices of a span to extract (e.g. in extractive QA), the model was expected to copy the span's text instead. With these minimal constraints, prompt writers were encouraged to use both formal and creative prompts and various orderings of the data. Most of the prompts correspond directly to a version of the original proposed task, although we also allowed prompts that permuted the original task (for instance, generating a document from its summary) or allowed for ambiguous output (for instance, not indicating a list of available choices). Such non-original-task prompts are included in our training mixtures for improved diversity, but they are not reported in evaluation since they deviate from the metrics and baselines reported by the original datasets.</p><p>The details of the prompting language and tool are given in Appendix C, and the prompts themselves are given in Appendix G. We collected prompts for English datasets, excluding ones that included potentially harmful content or non-natural language like programming languages. We refer to this collection as the Public Pool of Prompts (P3). As of writing, P3 contains 1939 prompts for 171 datasets (11.3 prompts per dataset on average). These prompts contain on average 14.4 tokens, not including variables and other elements from the templating language. All prompts used in our experiments are sourced from P3 (except for BIG-Bench, for which the prompts are provided by its maintainers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MODEL</head><p>At a high level, we fine-tune a pretrained model on our multi-task training mixture of natural language prompted datasets. Our model uses an encoder-decoder architecture with input text fed to the encoder and target text produced by the decoder. The model is trained to autoregressively generate the target through standard maximum likelihood training. Unlike decoder-only language models such as GPT-3, it is never trained to generate the input.</p><p>All models we trained are based on T5, a Transformer-based encoder-decoder language model pretrained with a masked language modeling-style objective on 1T tokens from C4 <ref type="bibr" target="#b66">(Raffel et al., 2020)</ref>. Since T5's pretraining objective involves filling in tokens from the input text that have been removed, it is quite different from the conditional text generation format used in our prompted datasets. We therefore use the publicly available language model-adapted T5 model from <ref type="bibr" target="#b43">Lester et al. (2021)</ref> (referred to as T5+LM), which was produced by training T5 on 100B additional tokens from C4 on a standard language modeling objective. Unless specified otherwise, we use the XXL version which has 11B parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TRAINING</head><p>Our main model, which we call T0, is trained on the multitask mixture detailed in Section 3 (i.e. yellow datasets in Figure <ref type="figure" target="#fig_0">2</ref>). T0+ is the same model but trained on a mixture that adds GPT-3's evaluation datasets. For T0++, we add GPT-3's and SuperGLUE <ref type="bibr" target="#b87">(Wang et al., 2019a)</ref>'s datasets to the training mixture which includes some held-out tasks. We also consider T0 (3B), which corresponds to the smaller 3 billion-parameter "XL" variant of T5 (results in appendix).</p><p>We perform checkpoint selection by choosing the checkpoint that yields the highest score on the validation splits of our training datasets. This still satisfies true zero-shot <ref type="bibr" target="#b62">(Perez et al., 2021</ref>) setting as we do not use any examples from any of the held-out tasks to select the best checkpoint.</p><p>Step 12'200 yielded the highest validation performance for our main model (T0), so we subsequently fine-tune all models for 12'200 steps.</p><p>At a high level, we assemble our multitask training mixture simply by combining all of the examples from all training datasets and shuffling the result. This is equivalent to sampling from each dataset in proportion to the number of examples in the dataset. However, the number of examples in each of our training datasets varies by two orders of magnitude. We therefore follow the strategy used in <ref type="bibr" target="#b66">Raffel et al. (2020)</ref> and treat any dataset with over 500'000 examples as having 500'000 / num templates examples for the purposes of sampling, where num templates is the number of templates created for the dataset. We feed the model input and target sequences of 1024 and 256 tokens, respectively. Following <ref type="bibr" target="#b66">Raffel et al. (2020)</ref>, we use packing to combine multiple training examples into a single sequence to reach the maximum sequence length. We use a batch size of 1024 sequences (corresponding to 2 20 total input tokens per batch) and the Adafactor optimizer <ref type="bibr" target="#b79">(Shazeer and Stern, 2018)</ref>. Following standard practice for fine-tuning T5, we use a learning rate of 1e-3 and a dropout rate of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EVALUATION</head><p>We evaluate zero-shot generalization on 11 NLP datasets in 4 unseen tasks: natural language inference, coreference, word sense disambiguation, and sentence completion (green datasets in Figure <ref type="figure" target="#fig_0">2</ref>). Unless specified otherwise, we report numbers on the validation splits. We also evaluate on 14 datasets from BIG-Bench (BIG-bench collaboration, 2021).</p><p>For tasks that involve choosing the correct completion from several options (e.g. multiple choice), we follow <ref type="bibr" target="#b10">Brown et al. (2020)</ref> and use rank scoring to evaluate our model: we compute the log- likelihood of each of the target options under the fine-tuned model and select the option with the highest log-likelihood as the prediction. For simplicity, we do not apply any normalization strategies to the log-likelihoods and use them as they are. We report accuracy for every dataset.</p><p>We do not perform prompt selection by comparing the performance of different prompts on the validation split; <ref type="bibr" target="#b62">Perez et al. (2021)</ref> highlights how such a strategy leaks information from the evaluation splits, which makes the evaluation not "true" zero-shot. For a given dataset, we report the median performance across the prompts for this dataset (up to 15) along with their interquartile range (Q3 -Q1) to measure the sensitivity of the model to the wording of the prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">GENERALIZATION TO UNSEEN TASKS</head><p>Our first research question is whether multitask prompted training improves generalization to unseen tasks. In Figure <ref type="figure" target="#fig_2">4</ref>, we compare T0 against our T5+LM baseline on four held-out tasks: natural language inference, coreference, sentence completion, and word sense disambiguation. Our approach leads to significant gains over our baseline on all datasets, indicating the benefits of multitask training compared to only language modeling with an identical model and prompts.</p><p>We next compare T0 to large language model baselines. First, we compare our results to the zeroshot performance of various GPT-3 model variants up to 175B parameters. Note that Brown et al.</p><p>(2020) reports performance on a single prompt,<ref type="foot" target="#foot_2">3</ref> whereas we report the median and interquartile range of performance across all prompts. T0 surpasses the performance of all GPT-3 models on 8 out of 11 held-out datasets. Neither T0 nor GPT-3 were trained on natural language inference, T0 outperforms GPT-3 on all NLI datasets (even though T5+LM does not). T0 underperforms GPT-3 significantly on HellaSwag, and Winogrande, as does the T5+LM baseline. We note though that for Winogrande, GPT-3 uses a specialized task format and evaluation procedure; we have not investigated whether these techniques would improve the performance of T0 or the baselines.</p><p>To further evaluate our model on unseen tasks, we assess the zero-shot performance of T0, T0+, and T0++ on a subset of the BIG-Bench benchmark (BIG-bench collaboration, 2021). BIG-Bench datasets come with their own prompts, prepared through a different process than P3. Tasks from BIG-Bench focus on a variety of skills not covered by our training tasks, such as deducing the order of a sequence of objects, solving logic grid puzzles, and telling apart true statements from common misconceptions. We compare our model to a series of preliminary diagnostic baseline models trained by Google and evaluated by the BIG-Bench maintainers. These models are decoder-only Transformer language models trained on a standard language modeling objective with varying model size. We find that at least one of the T0 variants outperform all baseline models on all tasks except for StrategyQA. In most cases, the performance of our models improves as the number of training datasets increases (i.e. T0++ outperforms T0+ which outperforms T0).</p><formula xml:id="formula_0">LM (1.0B) LM (8.5B) LM (28B) LM (68B) T0<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">PROMPT ROBUSTNESS</head><p>Our second research question is whether training on a wider range of prompts improves robustness to the wording of the prompts. To study this question, we conduct two ablation experiments to measure the effects of the number of prompts per dataset (p) and the number of datasets (d) used during training on held-out tasks.</p><p>Effect of More Prompts per Dataset In this analysis, we fix d and compare three models where p = 1 (one randomly chosen original-task prompt per dataset), p = all available prompts (corresponding to T0, on average p = 8.03), and p = 0 (corresponding to T5+LM without any prompted training). We train all models with the same hyperparameters and the same number of steps. Figure <ref type="figure" target="#fig_4">6</ref> shows that, even with just one prompt per dataset (red), performance on unseen tasks can improve substantially over the baseline (blue), although the spread (interquartile range between Q1 and Q3) does not appreciably improve with p = 1. However, further increasing p from 1 to an average of 8.03 does yield additional improvement in both median (increases for 11/11 datasets) and spread (decreases for 7/11 datasets). This reinforces our hypothesis that training on more prompts per dataset leads to better and more robust generalization to unseen tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Prompts from More Datasets</head><p>In this experiment, we fix p = all available prompts and increase d from 39 to 49 to 55 (T0, T0+, T0++, respectively, datasets are given in Section 5) increasing the total number of prompts seen during training. Figure <ref type="figure" target="#fig_5">7</ref> shows that the median performance of all 5 held-out datasets increases as d increases from 39 to 49. However, the spread only decreases for 1 out of 5 datasets. For some datasets (e.g., ANLI), this is an artifact of the fact that some prompts always perform poorly, so that when other prompts improve, the spread is stretched larger. For other datasets (e.g., CB), however, the spread does decrease in T0+. As d increases from 49 to 55, the Adding more prompts consistently leads to higher median performance and generally reduced interquartile range for unseen tasks.</p><p>median performance of all datasets again increases, but the spread only decreases for 2 out of 5 datasets. Although further investigation is needed, it appears that increasing d does not consistently make the model more robust to the wording of prompts.</p><p>Comparing T0 and GPT-3's robustness Because <ref type="bibr" target="#b10">Brown et al. (2020)</ref> only report one prompt per dataset with no standard deviation, we evaluate GPT-3 on RTE using the 10 prompts we prepared through OpenAI's API<ref type="foot" target="#foot_3">4</ref> in order to estimate its robustness. Note that one of our templates is identical to <ref type="bibr">Brown et al. (2020, p. 59</ref>)'s reported prompt; this prompt scores 58.8% accuracy on the API "Base" series which is lower than the reported accuracy of 63.5% from <ref type="bibr" target="#b10">Brown et al. (2020)</ref>. All other 9 prompts, however, yield roughly random-guessing performance with median accuracy = 52.96% and interquartile range = 1.28%. These results suggest that T0 is more robust to prompt formulation than GPT-3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION OF SIMILAR APPROACHES</head><p>Our results demonstrate that explicit multitask prompted fine-tuning substantially improves zeroshot generalization to unseen tasks, often outperforming significantly larger language models. In this section, we discuss two other works that share a similar approach.</p><p>OpenAI has released an "instruct series" API. <ref type="foot" target="#foot_4">5</ref> No public information is available about this model or its training other than the following short statement: "The Instruct models share our base GPT-3 models' ability to understand and generate natural language, but they're better at understanding and following your instructions." As details of this model have not been published, and it is only available through a commercial API, we do not compare to it in our paper.</p><p>Concurrent work by <ref type="bibr" target="#b92">Wei et al. (2021)</ref> proposes FLAN, which largely follows the same method of enabling zero-shot generalization through multitask prompted training. They focus on fine-tuning standard autoregressive language models on datasets from a diverse collection of tasks and evaluating performance on a single held-out task at a time. Compared to FLAN, T0's zero-shot performance   <ref type="formula">2021</ref>) perform an ablation with a model of comparable size (8B parameters) to T0 (11B parameters) and find that that performance on held-out tasks decreases after multi-task training. We identify several key differences with our work that could explain this discrepancy:</p><p>• We use an encoder-decoder model that was pretrained with a different objective (masked language modeling) before being trained as a standard language model and finally finetuned on the multitask mixture. We note that masked language modeling has repeatedly been shown to be a dramatically more effective pre-training strategy <ref type="bibr" target="#b66">(Raffel et al., 2020;</ref><ref type="bibr" target="#b0">Baevski et al., 2019;</ref><ref type="bibr" target="#b19">Devlin et al., 2019</ref>). • Our prompts are qualitatively more diverse in terms of their length and creativity ( §4). For example, <ref type="bibr" target="#b92">Wei et al. (2021)</ref> requires that answer choices are always preceded by the text OPTIONS: , whereas we allow for arbitrary formatting of the answer choices list. We hypothesize that this diversity could have concrete effects. For example, it could explain why <ref type="bibr" target="#b92">Wei et al. (2021)</ref> present ablation results where increasing the number of prompts has a negligible impact on performance whereas we observe an improvement when adding more prompts ( §6.2). • We hold out multiple tasks at once, rather than only holding out a single task at a time.</p><p>We made this choice in order to evaluate a single model's ability to generalize to multiple diverse tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we demonstrate that multitask prompted training can enable strong zero-shot generalization abilities in language models. This approach provides an effective alternative to unsupervised language model pretraining, often enabling our T0 model to outperform models many times its size. We also perform ablation studies demonstrating the importance of including many diverse prompts and the impact of increasing the number of datasets in each task. To enable future work on improving zero-shot generalization, we release all models trained in this paper in addition to the collection of prompts we created and our prompt annotation tool.</p><p>generously provided TPU credits via Hugging Face. Those credits were used to train all the models from this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A CONTRIBUTIONS AND PROJECT STRUCTURE</head><p>This research was conducted under the BigScience project for open research,<ref type="foot" target="#foot_5">6</ref> a year-long initiative targeting the study of large models and datasets. The goal of the project is to research language models in a public environment outside large technology companies. The project has 600 researchers from 50 countries and more than 250 institutions. The BigScience project was initiated by Thomas Wolf at Hugging Face, and this collaboration would not have been possible without his effort. This research was the focus of the BigScience Prompt Engineering working group, which focused on the role of prompting in large language model training.</p><p>This project was led by the joint first-authors of this work. Victor Sanh co-led the prompt engineering group, managed the prompt collection procedure, implemented the prompt materialization, and ran evaluation systems. Albert Webson reviewed and selected all training and evaluation datasets, led the analysis of results, designed the ablation studies, and co-managed the writing process. Colin Raffel proposed the research direction, trained all the models, named the model, and built the main evaluation system. Stephen Bach co-led the prompt engineering group, developed the prompting tool and guidelines, and led the prompt collection effort central to the work. Additionally, Alexander Rush helped develop the prompt templating language and tool, and co-managed paper writing.</p><p>Following the goals of the BigScience project, this work is co-authored by all contributors to the working group. We define this contribution as having contributed at least 3 accepted prompted datasets to the project. Lacking a better metric, authors are sorted based on code contributions to the project. We explicitly highlight the work of: Lintang Sutawika, who helped with evaluation and writing; Urmish Thakker, Mike Tian-Jian Jiang, Shanya Sharma, Arnaud Stiegler, and Manan Dey who helped with the development of the prompting tool; M Saiful Bari, who helped for the models and dataset release; Teven Le Scao, who conducted the contamination analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BROADER IMPACTS B.1 ENVIRONMENTAL COSTS</head><p>Training large language models can incur substantial environmental costs <ref type="bibr" target="#b82">(Strubell et al., 2019;</ref><ref type="bibr" target="#b77">Schwartz et al., 2020;</ref><ref type="bibr" target="#b38">Lacoste et al., 2019;</ref><ref type="bibr" target="#b4">Bender et al., 2021)</ref>. These costs are due to the energy used to power the hardware required for training. Recently, The focus of this paper is an empirical exploration of multitask prompt training and how it improves zero-shot performance on multiple tasks. We transformed datasets by writing multiple prompts for each of the datasets, fine-tuned pretrained models on the transformed examples and observed strong zero-shot capabilities on multiple tasks. We note that the zero-shot performance of our model is still significantly behind models that are fine-tuned on the given task in a "traditional" transfer-learning setup. This highlights how much research is still needed in this area, and we believe this work and the resources developed as part of this work are central to future research.</p><p>This work is built exclusively on publicly available datasets from the Hugging Face datasets library <ref type="bibr" target="#b45">(Lhoest et al., 2021)</ref> and a publicly available model, T5+LM <ref type="bibr" target="#b43">(Lester et al., 2021)</ref>. The implications of releasing large language models have been extensively discussed in <ref type="bibr" target="#b4">Bender et al. (2021)</ref>; <ref type="bibr" target="#b9">Bommasani et al. (2021)</ref>; <ref type="bibr" target="#b81">Solaiman et al. (2019)</ref> among others. We expect replicating our work to be within the capabilities of dozens of organizations worldwide, the main barrier being financial constraints. As such, we believe that the additional potential risks and harms produced by releasing our model are limited, and not releasing it would primarily impact less funded research organizations and individuals.</p><p>Moreover, we favor openness, transparency, and reproducibility. Releasing the dataset, models and tools developed as part of this work are key elements that will facilitate the reproduction of our results and future works. As such, our resources are available at :</p><p>• Collection of prompts: http://github.com/bigscience-workshop/ promptsource</p><p>• Trained models: https://huggingface.co/bigscience/T0pp</p><p>• Materialized prompted data used to train the models: https://huggingface.co/ datasets/bigscience/P3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 BIAS AND FAIRNESS</head><p>Some of the datasets available in the Hugging Face library contain potentially harmful content. We deliberately excluded these datasets from the training mixture to avoid training the model to generate such outputs. In particular, we excluded datasets that are constructed from forums and social media content such as Sentiment140 <ref type="bibr" target="#b25">(Go et al., 2009)</ref>  Language models can reproduce undesirable social biases represented in the large corpus they are pre-trained on. We evaluate our models in two ways: first, in their ability to recognize or label gender biases and second, in the extent to which they reproduce those biases.</p><p>To measure the ability of our model to recognize gender biases, we evaluate our models using the WinoGender Schemas <ref type="bibr" target="#b72">(Rudinger et al., 2018)</ref> (also called AX-g under SuperGLUE) and CrowS-Pairs <ref type="bibr" target="#b56">(Nangia et al., 2020)</ref>. WinoGender Schemas are minimal pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias. We use the version from <ref type="bibr" target="#b64">Poliak et al. (2018)</ref> that casts WinoGender as a textual entailment task and report accuracy. CrowS-Pairs is a challenge dataset for measuring the degree to which U.S. stereotypical biases present in the masked language models using minimal pairs of sentences. We re-formulate the task by predicting which of two sentences is stereotypical (or anti-stereotypical) and report accuracy. For each dataset, we evaluate between 5 and 10 prompts. To measure the extent to which our model reproduces gender biases, we evaluate our models using the WinoBias Schemas <ref type="bibr">(Zhao et al., 2018)</ref>. WinoBias Schemas are pronoun coreference resolution tasks that have the potential to be influenced by gender bias. WinoBias Schemas has two schemas (type1 and type2) which are partitioned into pro-stereotype and anti-stereotype subsets.</p><p>A "pro-stereotype" example is one where the correct answer conforms to stereotypes, while an "anti-stereotype" example is one where it opposes stereotypes. All examples have an unambiguously correct answer, and so the difference in scores between the "pro-" and "anti-" subset measures the extent to which stereotypes can lead the model astray.We report accuracies by considering a prediction correct if the target noun is present in the model's prediction. We evaluate on 6 prompts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ANNOTATION SYSTEM -PROMPTSOURCE</head><p>In order to collect hundreds of templates for prompts, we first needed a system that enabled users to view data, provide templates in a standard format, and verify that their templates work correctly. We implemented a lightweight interface in Streamlit<ref type="foot" target="#foot_7">8</ref> that users could download, run locally in a web browser, and then upload their results to a central repository.</p><p>Testing iterations of the interface on pilot template-writing tasks, we converged on three views for the interface. First, a "helicopter" view allows users to see what datasets are available for writing templates and how many are written for each, to prioritize user attention. Second, a "sourcing" view allows users to select a dataset to prompt, browse examples from that dataset in the form of Python dictionaries provided by the Hugging Face datasets library, and enter a template for that dataset. As the user writes their template, every time they save it, the output of the template applied to the current example is displayed next to the editor. We also collect metadata like a name for the template, and a reference for any bibliographic information or rationale for the template. Third, in the "prompted dataset" view, users can select templates and browse the prompts generated by them. The original example (a Python dictionary) is viewed side-by-side with the resulting prompt, with the substituted text highlighted to distinguish from text hard-coded in the template. Users can quickly scroll through many examples, verify the behavior of their template, and return to the sourcing view if changes are needed.</p><p>A key design decision is the format for templates. We experimented with multiple formats and found that they exhibited a tradeoff between expressivity and explicit structure. On one side, a maximally expressive format such as pure Python code would let users write complex programs to manipulate the semi-structured examples into prompts. However, analyzing these programs to understand how the prompts are created becomes difficult. This difficulty limits downstream manipulation and analysis of the templates, such as automatic template augmentation. On the other side, a maximally structured format such as rule-based generation limits the kinds of templates that users can create. We found it infeasible to enumerate types of rules sufficient for the wide range of tasks and data formats for which we wanted templates.</p><p>We therefore settled on a middle ground between the two: the Jinja templating engine<ref type="foot" target="#foot_8">9</ref> originally designed for producing web markup. Users write templates as prompts with placeholders, such as If {{premise}} is true, is it also true that {{hypothesis}}? ||| {{entailed}}. The separator ||| denotes the break between the conditioning text and the desired completion. Placeholders refer to fields in the underlying example dictionary. Users also have access to Jinja's built-in functions, such as manipulating strings and structured data. For each template, prompts are created by applying the template to all examples in the corresponding dataset.</p><p>During the development of our tool (which we called promptsource), we found that a few idioms were particularly useful. First, not all templates are applicable to all examples in a dataset. Users can wrap templates in Jinja's built-in conditional statements, and any example that results in an empty prompt is simply skipped. Second, many examples can be used to make multiple training prompts, such as a question that has multiple valid answers. We therefore added a choice function that selects an element from a list in a way that can be controlled during dataset generation, such as picking a random element using a seeded random number generator or generating different prompts for each combination of elements in the template. Third, many tasks such as classification and binary question answering have a small set of possible valid completions, and it is common to make predictions for these tasks by scoring only the valid completions and returning the highest one <ref type="bibr" target="#b10">(Brown et al., 2020)</ref>. Users therefore can list the valid completions in a separate field and access them as a list in their templates. These completions are then explicitly available when evaluating predictions for these prompts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D DATASETS D.1 CATEGORIZING DATASETS INTO TASKS</head><p>Our task taxonomy (Figure <ref type="figure" target="#fig_0">2</ref>) consists of mostly uncontroversial decisions that refelect well-known tasks in the literature: sentiment analysis, topic classification, paraphrase identification, natural language inference, word sense disambiguation, coreference resolution, summarization, and structureto-text generation. The main difficulty lies in the fact that a large collection of datasets are all commonly known as "question answering", and there is no commonly accepted way of subdividing this category. CrossFit and UnifiedQA categorize them by format (multiple-choice vs. extractive vs. abstractive/generative), whereas <ref type="bibr" target="#b10">Brown et al. (2020)</ref> categorize by content (reading comprehension vs. commonsense vs. closed-book QA).</p><p>In principle, categorizing by content makes more sense than by format. Most humans would consider taking an exam in history vs. in physics as two different tasks, whereas whether the exam is multiple-choice or extractive matters less. By this logic, it is relatively uncontroversial to establish closed-book QA as a distinct task, which largely evaluates a model's memorization of world knowledge <ref type="bibr" target="#b69">(Roberts et al., 2020)</ref>. The distinction between commonsense and (mere) reading comprehension, however, is much more blurry. As mentioned in Section 3, there are vast differences in what is considered as commonsense by each dataset's authors. To oversimplify, they usually include questions that evaluate physical cognition and (US-centric) cultural norms.</p><p>For comparison, <ref type="bibr">Brown et al. (2020, p. 17</ref>) define a commonsense task as an "attempt to capture physical or scientific reasoning, as distinct from sentence completion, reading comprehension, or broad knowledge question answering." Circular definition aside, it is far from clear that scientific reasoning is commonsense. Among <ref type="bibr" target="#b10">Brown et al. (2020)</ref>'s selection, ARC exemplifies how evaluation of scientific knowledge goes far beyond commonsense. Despite being constructed from grade school science questions, authors of this paper find most of ARC difficult to answer (and, to a lesser degree, OpenBookQA too).</p><p>Finally, note that NLI and coreference datasets (especially the newer ones such as ANLI and Winogrande) all in practice require commonsense knowledge. Therefore, we find it difficult to establish commonsense as a standalone category of task, defaulting back to categorizing QAs by their format. This implies that we categorize ARC as multiple-choice QA, because other closed-book QAs require generating the answer without any provided answer options.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 HOW UNSEEN ARE THE HELD-OUT TASKS?</head><p>Because "question answering" is so broadly defined, QA datasets could have included entailment or coreference questions, rendering them not strictly held-out tasks. For example, ReCoRD is an extractive QA dataset that exclusively asks questions which amount to identifying a referent. We hold out ReCoRD as part of SuperGLUE, but it is impractical to inspect every dataset and slice out the subsets of examples which ask entailment or coreference questions.</p><p>One common concern is that paraphrasing identification is too similar to NLI and should also be held out. We disagree for two reasons. First, NLI tests for unidirectional entailment, while paraphrasing asks for bidirectional entailment. Second, an author manually reviewed ANLI and RTE and found almost no entailment examples that are also valid paraphrases.</p><p>Another tricky category that has been challenged as too similar to NLI is sentence completion, choosing the most plausible option which continues or completes a sentence or a short paragraph. SWAG was proposed as "commonsense inference" to supplement NLI, but the distinction between formal semanticists' deductive inference and natural pragmatic inference is not clearly drawn in most NLI datasets <ref type="bibr" target="#b61">(Pavlick and Kwiatkowski, 2019)</ref>. Additionally, coreference and any "continuationstyle" prompt could also be interpreted as a sentence completion task. These blurry boundaries have no clear answers. So we categorically hold out the sentence completion task.</p><p>Evaluation datasets in BIG-Bench were created with the goal of testing language models on diverse, difficult, and novel skills. Therefore, those datasets are unlikely to have high overlap with T0's training tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 LAMBADA</head><p>As described above, our task categorization is overall somewhat similar to that of <ref type="bibr" target="#b10">Brown et al. (2020)</ref>. One additional exception is the LAMBADA dataset <ref type="bibr" target="#b59">(Paperno et al., 2016)</ref>, which <ref type="bibr" target="#b10">Brown et al. (2020)</ref> classify as part of the "sentence completion" task group. LAMBADA differs significantly from the other tasks in this group since it requires open-ended next word prediction (rather than choosing among a few possible continuations). The dataset was designed in this way specifically so that its format is exactly the same as standard language modeling, thereby allowing language models to be evaluated on it without additional fine-tuning or adaptation. <ref type="bibr" target="#b10">Brown et al. (2020)</ref> deviate from standard practice on this benchmark in the following ways: First, they introduce a prompted form that converts it to a fill-in-the-blank-style task. Second, they evaluate on a non-standard format of the dataset that omits the tokenization and lowercasing of the official benchmark. 10 Third, GPT-3 was trained on the Book Corpus dataset, which is the same dataset that was used as a source of all passages in LAMBADA. <ref type="bibr" target="#b10">Brown et al. (2020)</ref> estimate that 57% of the LAMBADA test set examples appeared in GPT-3's training set.</p><p>We evaluated T5+LM on the standard LAMBADA dataset in the original unprompted next-wordprediction form and found that it achieved an accuracy of 6.2%. This is substantially below the accuracy of 72.5% achieved by the comparably-sized GPT-3-13B variant. T0 did not fare much better, achieving only 18.7%. We therefore evaluated using the same cloze-style prompted form used by GPT-3, which raised T0's accuracy to 27.8%. If we swap out the official LAMBADA dataset for the variant used by GPT-3, T0's accuracy further increases to 40.5% and T5+LM achieves 10.7%. We suspect that the additional gap between T0 and GPT-3-13B's performance is at least partially due to the fact that GPT-3 was trained on a large portion of LAMBADA's test set. Due to this discrepancy and the fact that LAMBADA is dissimilar to the other sentence completion tasks, we omitted LAMBADA from our evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 TABLE OF ALL DATASETS</head><p>See  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E CONTAMINATION ANALYSIS OF PRETRAINING CORPUS ON TEST TASKS</head><p>Zero-shot performance estimation can be confounded if the pretraining corpus for the model contains text from the test tasks because models could improve performance through memorization rather than generalization. In order to control for this effect, we searched for long common substrings between the input examples (presented in prompted form) for our zero-shot test tasks on one hand, and documents in C4 (our model's pretraining set) on the other hand.</p><p>In order to do this effectively, we use the suffix array method described and implemented in <ref type="bibr" target="#b41">Lee et al. (2021)</ref> to index C4, allowing us to run fast counts of how many times a substring appears in the corpus. To limit the number of queries, we search by partitioning sentences into groups of 16 tokens and doing an exact match query. This gives us an over-counting on how many length-32 token overlaps there are in the corpus. We flag examples that produce a match during that procedure, then manually inspect them.</p><p>For NLI datasets, we separate matches for premises and hypotheses since, the premises tend to be sourced from the internet and therefore have a high number of matches. However, if the hypothesis it is paired with is novel, memorization might not be helpful. As expected, ANLI and RTE return a high proportion of matches on the premises. However, ANLI hypotheses have negligible overlap with the pre-training set, which prevents pre-training memorization from solving the task. On the contrary, RTE hypotheses are contained in the pretraining dataset 5.2% of time. Those largely correspond to short, factual sentences ("Paris is the capital of France").</p><p>Those are examples where the pre-training dataset could help if factual knowledge helps with solving the task. HellaSwag has 9.12% matches, which could be problematic as it is a continuation task: the correct answer is also contained in the same original internet page as the input sequence, even though the multiple-choice answering format prevents the model from just generating the correct answer verbatim through memorization. Other datasets are free of contamination.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F FULL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G LIST OF ALL PROMPTS</head><p>Datasets are listed by their task categorization and the canonical dataset name in Hugging Face datasets.</p><p>For each dataset, a data example is given for context. Then each prompt template is listed with bibliographic reference, input template, and target template. For some prompts, there is a template for answer choices included as well. Additionally, we indicate prompts that do not correspond to the original task description. The city councilmen refused the demonstrators a pe...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONTENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>Prompt not from the original task.</p><p>Identify the pronoun in "{{text}}" {{pronoun}} Prompt not from the original task.</p><p>Identify the pronoun in "{{text}}" and the entity it is referring to "{{pronoun}}" which refers to the "{{options[label]}}"</p><p>Prompt not from the original task.</p><p>Who does the pronoun "{{pronoun}}" in "{{text}}" refer to?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{options[label]}}</head><p>Prompt not from the original task.</p><p>Who does the pronoun "{{pronoun}}" in "{{text}}" refer to?</p><p>The options are {{options | join(" and ")}}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{options[label]}}</head><p>Prompt not from the original task.</p><p>Identify the phrase in "{{text}}" in which the key action or context surrounding the pronoun is described {{quote}} The Vatican Apostolic Library (), more commonly ca...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>Prompt not from the original task.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: T0 datasets and task taxonomy. Color represents the level of supervision. Yellow datasets are in the training mixture. Green datasets are held out and represent tasks that were not seen during training. Zero-shot task generalization experiments are evaluated on green datasets. Hotpot QA is recast as closed-book QA due to long input length. Variant experimental models T0+ and T0++ use additional datasets. The full list is shown in Appendix D.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Prompt templates from the P3 prompt collection. Each dataset has multiple prompt templates consisting of an input and target template. Italics indicate the formatting instructions. These use the raw fields of the example as well as template metadata. For example, the paraphrase prompts use Choices, a template-level variable consisting of Not duplicates, Duplicates for the first prompt and No, Yes. These templates are materialized to produce the prompted instance shown in Figure1. The complete set of prompt templates used in T0 is given in Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Results for T0 task generalization experiments compared to GPT-3<ref type="bibr" target="#b10">(Brown et al., 2020)</ref>. The baseline T5+LM model is the same as T0 except without multitask prompted training. Bars are median accuracy, and error bars are the interquartile range across all prompts of a dataset. GPT-3 results are shown with the single prompt reported in their work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Results for a subset of BIG-Bench which has available baselines. The baseline models are Transformer-based language models provided by BIG-Bench maintainers, who also provide one prompt per dataset. T0, T0+ and T0++ are identical except for increasing the number of training datasets ( §5.2). BIG-Bench Tasks are all zero-shot for all the reported models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Effect of More Prompts per Dataset. Zero-shot performance of T0 with varying number of training prompts per dataset (p = 0, p = 1, p = all, respectively). Adding more prompts consistently leads to higher median performance and generally reduced interquartile range for unseen tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Effect of Prompts from More Datasets. Zero-shot performance of three models with varying number of datasets (T0, T0+, T0++). Adding more datasets consistently leads to higher median performance but does not always reduce interquartile range for unseen tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of the size of the pre-trained model: comparison of T0 3B against T0 11B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>question based on the hint. {% for question, answer in zip(questions[:-1], answers["input_text"][:-1Prompt not from the original task. Can you form a set of {{questions | length}} question-answer pairs about the passage below? Passage: {{story}} {{(answers.texts | last | choice).replace("CANNOTANSWER","Cannot answer") }} I read an article : {{context}} Then the following conversation occurred: {% for i in range(0, questions | length -1) %} Student: {{questions[i]}} Teacher: {{(answers.texts[i] | choice).replace("CANNOTANSWER","Cannot answer") }} {% endfor %} Use both to answer the question: {{questions | last }} {{(answers.texts | last | choice).replace("CANNOTANSWER","Cannot answer") }} Read the article: {{context}} Then answer the question: {{questions | last}} You can use this dialogue to find the answer faster: {% for i in range(0, questions | length -1)%} Student: {{questions[i]}} Teacher: {{(answers.texts[i] | choice).replace("CANNOTANSWER","Cannot answer") }} {% endfor %} {{(answers.texts | last | choice).replace("CANNOTANSWER","Cannot answer") }} A student is asking a teacher about the following article: {{context}} This is a summary of their conversation: {% for i in range(0, questions | length -1)%} Student: {{questions[i]}} Teacher: {{(answers.texts[i] | choice).replace("CANNOTANSWER","Cannot answer") }} {% endfor %} Use their conversation and the article to answer the question : {{questions | last}} {{(answers.texts | last | choice).replace("CANNOTANSWER","Cannot answer") }}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>). Adding more datasets consistently leads to higher median performance but does not always reduce interquartile range for unseen tasks.better on CB and RTE, similar on Story Cloze and COPA, and worse on Winogrande, ANLI, and HellaSwag. T0++ outperforms FLAN on CB, RTE, and COPA and matches FLAN's performance on Winogrande and ANLI. Notably, T0 and T0++ attain this performance despite being over 10× smaller than FLAN (137B vs. 11B parameters). Surprisingly,Wei et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In Advances in neural information processing systems, pages 649-657, 2015a.Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015b.</figDesc><table><row><cell>Yuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase Adversaries from Word Scram-bling. In Proc. of NAACL, 2019.</cell></row><row><cell>Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Con-</cell></row><row><cell>ference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, Volume 2 (Short Papers), pages 15-20, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2003. URL</cell></row><row><cell>https://aclanthology.org/N18-2003.</cell></row><row><cell>Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improv-ing few-shot performance of language models, 2021.</cell></row></table><note>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. CoRR, abs/2104.04670, 2021. URL https://arxiv.org/abs/2104.04670.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc><ref type="bibr" target="#b60">Patterson et al. (2021)</ref> performed a detailed analysis of the carbon emissions resulting from the training of various recent large language models. One model analyzed in that study was the largest T5 variant which was estimated to have emitted around 46.7 tCO 2 e. Since we based T0 on this T5 variant and performed training on the same hardware (Google Cloud TPUs), we can estimate the carbon emissions produced by our study by simply re-scaling the T5 estimate from<ref type="bibr" target="#b60">Patterson et al. (2021)</ref> by the amount of training we performed. Specifically, T5 was pretrained for one trillion tokens; across all of our training runs (including preliminary test experiments not described in this paper) we trained for 250 billion tokens, or about 25% as many. These training runs corresponded to about 270 total hours of training on a v3-512 Cloud TPU device. Further, T5 was trained in Google's Taiwan datacenter, whereas we trained in the europe-west4-a Cloud region. The gCO 2 eq/kWh published by Google for these datacenters are 540 and 410 respectively, 7 suggesting that our carbon emissions should further be scaled by a factor of 410/540 ≈ 75.9%. Based on the above, we estimate the total emissions for training our models to be about 46.7×25%×75.9% ≈ 8.9 tCO 2 e. As a point of reference,<ref type="bibr" target="#b60">Patterson et al. (2021)</ref> estimate that a roundtrip jet plane flight from San Francisco to New York emits around 180 tCO 2 e and<ref type="bibr" target="#b82">Strubell et al. (2019)</ref> estimate the average per-passenger emissions to be about 1 tCO 2 e. Note that our experiments incurred additional emissions due to the cost of evaluation, the XL-sized ablation, and data preprocessing, but these costs are negligible compared to the training runs for the main T0 model. Moreover, most of the evaluations and data preprocessing ran on the French Jean-Zay cluster whose electricity mostly comes from nuclear energy. Carbon emissions information for T0 and T5.</figDesc><table><row><cell>Model</cell><cell cols="2">Hardware Hours</cell><cell>Grid</cell><cell cols="2">gCO2eq/kWh Estimated tCO2e</cell></row><row><cell>T0 (single run) All experiments in this paper T5-11B (single run)</cell><cell>v3-512 v3-512 v3-1024</cell><cell>27 270 528</cell><cell>europe-west4-a europe-west4-a Taiwan</cell><cell>410 410 540</cell><cell>0.9 8.9 46.7</cell></row></table><note>B.2 RISKS IN DEVELOPING AND RELEASING LARGE LANGUAGE MODELS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>which was based on Twitter content or Hate Speech 18<ref type="bibr" target="#b18">(de Gibert et al., 2018)</ref> which was built on data from a white supremacist forum.Even if we took deliberate decisions to exclude datasets with potentially harmful content from the fine-tuning, the models trained are not bias-free. Based on a few experimentations, T0++ can generate answers that could be categorized as conspiracist or biased: A few problematic predictions from T0++.</figDesc><table><row><cell>Input</cell><cell>Prediction</cell></row><row><cell cols="2">Is the earth flat? Do vaccines cause autism? Complete this sentence: This man works as a Complete this sentence: This woman works as a Nanny yes yes Architect</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average and median accuracies on CrowS-Pairs and WinoGender reformulated as classification tasks.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell cols="2">Mean (Acc.) Median (Acc.)</cell></row><row><cell cols="2">CrowS-Pairs T0 T0+ T0++ T0 (p=1) T0 (3B)</cell><cell>59.2 57.6 62.7 57.6 56.9</cell><cell>83.8 83.8 64.4 69.5 82.6</cell></row><row><cell cols="2">WinoGender T0 T0+ T0++ T0 (p=1) T0 (3B)</cell><cell>84.2 80.1 89.2 81.6 69.7</cell><cell>84.3 80.6 90.0 84.6 69.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Accuracies on WinoBias coreference task.</figDesc><table><row><cell>Model</cell><cell>Subset</cell><cell cols="4">Mean (Acc.) Pro Anti ∆(Pro -Anti) Pro Anti ∆(Pro -Anti) Median (Acc.)</cell></row><row><cell>T0</cell><cell cols="2">Type 1 68.0 61.9 Type 2 79.3 76.4</cell><cell>6.0 2.8</cell><cell>71.7 61.9 79.3 75.0</cell><cell>9.8 4.3</cell></row><row><cell>T0+</cell><cell cols="2">Type 1 66.6 57.2 Type 2 77.7 73.4</cell><cell>9.4 4.3</cell><cell>71.5 62.6 86.1 81.3</cell><cell>8.8 4.8</cell></row><row><cell>T0++</cell><cell cols="2">Type 1 63.8 55.9 Type 2 66.8 63.0</cell><cell>7.9 3.9</cell><cell>72.7 63.4 79.3 74.0</cell><cell>9.3 5.3</cell></row><row><cell>T0 (p=1)</cell><cell cols="2">Type 1 82.3 70.1 Type 2 83.8 76.5</cell><cell>12.2 7.3</cell><cell>83.6 62.9 85.9 75.0</cell><cell>20.7 10.9</cell></row><row><cell>T0 (3B)</cell><cell cols="2">Type 1 82.3 70.1 Type 2 83.8 76.5</cell><cell>12.2 7.3</cell><cell>83.6 62.9 85.9 75</cell><cell>20.7 10.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>T0 Train T0+ Train T0++ Train Eval</cell></row><row><cell cols="2">Coreference Resolution Coreference Resolution Natural Language Inference super glue/cb super glue/wsc.fixed winogrande/winogrande xl Natural Language Inference super glue/rte Natural Language Inference aNatural Language Inference Paraphrase Identification glue/mrpc Paraphrase Identification glue/qqp Paraphrase Identification paws/labeled final Closed-Book QA ai2 arc/ARC Challenge Closed-Book QA ai2 arc/ARC Easy Closed-Book QA kilt tasks/hotpotqa Closed-Book QA trivia qa/unfiltered Closed-Book QA web questions Closed-Book QA wiki qa Extractive QA adversarial qa/dbidaf Extractive QA adversarial qa/dbert Extractive QA adversarial qa/droberta Extractive QA duorc/SelfRC Extractive QA duorc/Paraphrase IdentificationRC Extractive QA ropes Extractive QA squad v2 Extractive QA super glue/record Extractive QA quoref Extractive QA tydiqa Multiple-Choice QA cos e/v1.11 Multiple-Choice QA cosmos qa Multiple-Choice QA dream Multiple-Choice QA openbookqa/main Multiple-Choice QA qasc Multiple-Choice QA quail Multiple-Choice QA quarel Multiple-Choice QA quartz Multiple-Choice QA race/high Multiple-Choice QA race/middle Multiple-Choice QA sciq Multiple-Choice QA social i qa Multiple-Choice QA super glue/boolq Multiple-Choice QA super glue/multirc Multiple-Choice QA wiki hop/original Multiple-Choice QA wiqa Multiple-Choice QA piqa Sentiment amazon polarity Sentiment app reviews Sentiment imdb Sentiment rotten tomatoes Sentiment yelp review full Sentence Completion super glue/copa Sentence Completion story cloze/2016 Sentence Completion hellaswag Structure-to-Text common gen Structure-to-Text wiki bio Summarization cnn dailymail/3.0.0 Summarization gigaword Summarization multi news Summarization samsum Summarization xsum Topic Classification ag news Topic Classification dbpedia 14 Topic Classification trec Word Sense Disambiguation super glue/wic</cell><cell></cell></row></table><note>10 https://github.com/openai/gpt-2/issues/131</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>All training and evaluation datasets. The dataset are printed in their Hugging Face datasets identifier, where the part after / is their subset name. Hotpot QA is recast as closed-book QA due to long input length. Full citations are included in Appendix G.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Results for T5+LM and all T0 model variants on all tasks. Greyed-out text corresponds to results that are not zero-shot.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell cols="2">T5+LM Mean Med.</cell><cell cols="2">T0 (p = 1) Mean Med.</cell><cell>Mean</cell><cell>T0</cell><cell>Med.</cell><cell>Mean</cell><cell>T0+</cell><cell>Med.</cell><cell>T0++ Mean Med.</cell></row><row><cell>Coref.</cell><cell>WSC Wino. (XL)</cell><cell>54.09 50.65</cell><cell>57.69 50.71</cell><cell>52.40 58.11</cell><cell>56.25 57.22</cell><cell>61.45 59.94</cell><cell></cell><cell>64.42 60.46</cell><cell>62.24 62.54</cell><cell></cell><cell>64.42 61.72</cell><cell>70.29 66.42</cell><cell>69.71 66.54</cell></row><row><cell>NLI</cell><cell>ANLI R1 ANLI R2 ANLI R3 CB RTE</cell><cell>32.89 33.76 33.82 34.34 53.03</cell><cell>32.85 32.90 33.75 33.93 51.81</cell><cell>39.02 36.96 38.09 48.85 76.43</cell><cell>40.05 38.20 39.33 50.89 79.24</cell><cell>43.56 38.68 41.26 70.12 80.83</cell><cell></cell><cell>44.70 39.40 42.42 78.57 81.23</cell><cell>43.45 39.77 40.76 59.20 67.47</cell><cell></cell><cell>45.80 41.10 41.17 71.43 64.98</cell><cell>47.07 42.18 44.09 75.69 85.31</cell><cell>49.80 44.50 46.42 83.93 84.84</cell></row><row><cell>Story Comp.</cell><cell>COPA HellaSwag Story Cloze</cell><cell>54.88 27.00 48.16</cell><cell>55.00 27.73 48.85</cell><cell>87.66 32.79 89.57</cell><cell>87.50 33.27 93.00</cell><cell>90.02 33.55 92.40</cell><cell></cell><cell>90.79 33.58 94.71</cell><cell>92.24 86.13 96.43</cell><cell></cell><cell>93.88 85.79 97.17</cell><cell>93.71 86.11 96.49</cell><cell>93.75 85.65 97.33</cell></row><row><cell>WSD</cell><cell>WiC</cell><cell>50.30</cell><cell>50.24</cell><cell>55.03</cell><cell>54.94</cell><cell>56.58</cell><cell></cell><cell>57.21</cell><cell>55.02</cell><cell></cell><cell>55.49</cell><cell>70.02</cell><cell>69.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results for T0 model variants on a subset of BIG-Bench tasks.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Coreference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.1.1 super glue wsc.fixed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 1.1.2 winograd wsc wsc273 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 1.1.3 winogrande winogrande xl . . . . . . . . . . . . . . . . . . . . . . . . . . 34 1.1.4 winogrande winogrande debiased . . . . . . . . . . . . . . . . . . . . . . 35 1.2 Grammatical Acceptability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 1.2.1 glue cola . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 .2 glue qqp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 1.4.3 paws labeled final . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 1.5 QA Closed Book . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 1.5.1 ai2 arc ARC-Challenge . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 1.5.2 ai2 arc ARC-Easy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 1.5.3 kilt tasks nq . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 1.5.4 kilt tasks hotpotqa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 1.5.5 trivia qa rc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 1.5.6 web questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 1.5.7 wiki qa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 1.6 QA Extractive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 1.6.1 adversarial qa dbidaf . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 1.6.2 adversarial qa dbert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 1.6.3 adversarial qa droberta . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 1.6.4 coqa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65 1.6.5 duorc SelfRC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 1.6.6 duorc ParaphraseRC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6.7 ropes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6.8 squad v2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6.9 super glue record . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6.10 qa srl . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6.11 quac . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6.12 quoref . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.7 QA Generative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.7.1 drop . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8 QA Multiple Choice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.1 cos e v1.11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.2 cosmos qa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.3 dream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.4 openbookqa main . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.5 qasc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.6 quail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.7 quarel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.8 quartz . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.9 race high . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.10 race middle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.11 sciq . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.12 social i qa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.13 super glue boolq . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.14 super glue copa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.15 super glue multirc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.16 wiki hop original . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.17 wiqa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.18 circa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.19 mc taco . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.8.20 piqa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.9 Sentiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.9.1 amazon polarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.9.2 app reviews . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.9.3 imdb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.9.4 rotten tomatoes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.9.5 yelp review full . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.10 Story Completion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.10.1 hellaswag . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.11 Structure To Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.11.1 common gen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.11.2 wiki bio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.12 Summarization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.12.1 cnn dailymail 3.0.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.12.2 gigaword . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.12.3 multi news . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.12.4 samsum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.12.5 xsum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.13 Topic Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.13.1 ag news . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.13.2 dbpedia 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.13.3 trec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.14 Word Sense Disambiguation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.14.1 super glue wic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .</figDesc><table><row><cell>{{ text }}</cell><cell></cell></row><row><cell cols="2">In the passage above, the pronoun "{{ span2_text }}" refers to {{</cell></row><row><cell cols="2">span1_text }}. True or false?</cell></row><row><cell cols="2">{{ answer_choices[label] }}</cell></row><row><cell>{{ text }}</cell><cell></cell></row><row><cell cols="2">{% if span2_text.lower() == "they" or span2_text.lower() == "them" %}</cell></row><row><cell cols="2">Question: Who are "{{ span2_text.lower() }}"? {{ span1_text }}?</cell></row><row><cell cols="2">1 Prompts {% else %} Question: Who is "{{ span2_text.lower() }}"? Is it {{ span1_text }}? {% endif %} Answer: {{ answer_choices[label] }} 1.1.2 WINOGRAD WSC WSC273 Dataset from Levesque et al. (2012). Used in evaluation. 1.1 1 PROMPTS Data Example</cell><cell>31</cell></row><row><cell cols="2">1.1 COREFERENCE Key Value</cell></row><row><cell cols="2">1.1.1 SUPER GLUE WSC.FIXED Dataset from Levesque et al. (2012). Used in evaluation. Data Example Key Value label 0 options ['The city councilmen', 'The demonstrators'] pronoun they pronoun loc 63 quote they feared violence quote loc 63 source (Winograd 1972) text</cell></row><row><cell>idx label span1 index span1 text span2 index span2 text text</cell><cell>0 0 0 Mark 13 He Mark told Pete many lies about himself, which Pete...</cell></row><row><cell>Prompts</cell><cell></cell></row><row><cell cols="2">Prompt from Schick and Schütze (2021)</cell></row><row><cell cols="2">{{ text }} In the previous sentence, does the pronoun "{{</cell></row><row><cell cols="2">span2_text.lower() }}" refer to {{ span1_text }}? Yes or no?</cell></row><row><cell cols="2">{{ answer_choices[label] }}</cell></row><row><cell cols="3">{{ text }} Here, by "{{ span2_text }}" they mean "{{ span1_text }}". Yes</cell></row><row><cell>or no?</cell><cell></cell></row></table><note>1.3 NLI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 1.3.1 super glue cb . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 1.3.2 super glue rte . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 1.3.3 anli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 1.3.4 hans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 1.4 Paraphrase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 1.4.1 glue mrpc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 1.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Dataset from<ref type="bibr" target="#b80">Siva et al. (2018)</ref>. Used in evaluation. When was the Vat formally opened?', 'what is the... source wikipedia story</figDesc><table><row><cell>{{answer_choices[answer | int -1]}} {{answer_choices[answer | int -1]}} {{ answer_choices[label] }} {{premise}} Using only the above description and what you know about the world, is "{{hypothesis}}" definitely correct? Yes or no? I want to know whether the following two sentences mean the same thing. {{sentence1}} Data Example This is a correct answer to the following question about answer the following question. Note that the answer is present within the {{answer_choices[label]}} {{document_title}}. Yes or no? text.</cell></row><row><cell>{{sentence2}} Answer: {{answer}}</cell></row><row><cell>{{option1}} ||| {{option2}} {{sentence}} What does the _ in the above sentence refer to? {{ option1 }} or {{ option2 }}? Prompt not from the original task. The _ in the sentence below refers to {{option1}}. True or False? {{sentence}} {{answer_choices[answer|int -1]}} I'm copy-editing a story for publication. It has the following sentence in it: {{sentence}} {{ answer_choices[label] }} Prompt from Webson and Pavlick (2021) Do they? {{ answer_choices[label] }} Key Question: {{question}} Question: {{question}} Value id 1 label 0 sentence1 Prompt not from the original task. {{answers.text | choice}} {{answer_choices[label]}} {% endif %} {% if label == 1 %} In Paris , in October 1560 , he secretly met the E... sentence2 Paraphrase the sentence: {{sentence1}} In October 1560 , he secretly met with the English... Does this sentence make sense and is it grammatically correct? Please answer {{"yes or no"}}. {{ answer_choices[label] }} {{premise}} Are we justified in saying that "{{hypothesis}}"? Yes or no? {{ answer_choices[label] }} Does the sentence {{sentence1}} paraphrase (that is, mean the same thing as) this sentence? "{{answer}}" {{sentence2}} Determine the topic of the passage. Prompts {{sentence2}} Prompt not from the original task. {% endif %} {% if label == 1 %} 1.6.4 COQA</cell></row><row><cell>Data Example Key answer 2 option1 Ian option2 Dennis sentence Ian volunteered to eat Dennis's menudo after alrea... Prompts {{ option1 }} ||| {{ option2 }} {{ sentence }} In the previous sentence, does _ refer to {{ option1 }} or {{ option2 }}? {% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %} {{option1}} ||| {{option2}} {% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %} Prompt not from the original task. The _ in the sentence below refers to {{option1}}. True or False? {{sentence}} {{answer_choices[answer|int -1]}} {{option1}} ||| {{option2}} {{sentence}} Replace the _ in the above sentence with the correct option: -{{option1}} -{{option2}} {% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %} 1.2 GRAMMATICAL ACCEPTABILITY 1.2.1 GLUE COLA Dataset from Warstadt et al. (2018). Used in evaluation. Data Example Key Value idx 0 label 1 sentence Our friends won't buy this analysis, let alone the... Does the following sentence make sense and use correct English? Please answer {{"yes"}} or {{"no"}}. {{sentence}} {{ answer_choices[label] }} Prompts Suppose {{premise}} Can we infer that "{{hypothesis}}"? Yes, no, or maybe? Paraphrase the following sentence: {{sentence1}} 1.4.2 GLUE QQP 1.5.1 AI2 ARC ARC-CHALLENGE Context: {{context}} {% if label == 1 %} {{answer_choices[label]}} Question: {{question}} 1.5 QA CLOSED BOOK Extract the answer to the question from the following context. Prompt from Webson and Pavlick (2021) Prompt not from the original task. {% if metadata.split != "test" %} Yes or No. {{ answer_choices[label] }} {{sentence2}}? Prompts {{sentence1}} {{sentence2}} {{sentence1}} Is that a paraphrase of the following sentence? Prompts {{answer_choices[label]}} Prompts 1.3 NLI Dataset from ?. Used in evaluation. Data Example Key Value hypothesis the language was peeled down idx 0 label 0 premise Do the following two sentences mean the same thing? {{sentence1}} Question: {{sentence2}} Paraphrase or not? It was a complex language. Not written down but ha... Dataset from Dolan and Brockett (2005). Used in evaluation. Data Example Key Value idx 0 label 1 sentence1 Amrozi accused his brother , whom he called " the ... sentence2 Referring to him as only " the witness " , Amrozi ... with the sentence {{sentence2}} and have it mean the same thing? {{ answer_choices[label] }} Sentence 2: {{sentence2}} Question: Can we rewrite Sentence 1 to Sentence 2? {{answer_choices[label]}} {{answer_choices[label]}} Prompt from Brown et al. (2020) id Yale_University Slack (2003) compares three groups that conducted ... what year were the research groups compared {'text': ['2003'], 'answer_start': [7]} {'split': 'train', 'model_in_the_loop': 'BiDAF'} metadata answers question context title 821607441c173838196c4d1500c2ab21a044e6b0 Sentence 1: {{sentence1}} Question: Can we rewrite Sentence 1 to Sentence 2? Yes or No? Key Value Sentence 2: {{sentence2}} {{sentence1}} Sentence 1: {{sentence1}} Can I replace the sentence Data Example {{answer_choices[label]}} 1.3.1 SUPER GLUE CB 1.4 PARAPHRASE 1.4.1 GLUE MRPC {{sentence2}} {% endif %} {{sentence1}} {{sentence2}}? Dataset from Bartolo et al. (2020). Used in training. {{answer_choices[label]}} Is that a paraphrase of the following sentence? 1.6.1 ADVERSARIAL QA DBIDAF {{sentence1}} Question: {{sentence2}} True or False? Value {% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %} {{option1}} ||| {{option2}} Fill in the _ in the below sentence: {{sentence}} Choices: -{{ option1 }} -{{ option2 }} Answer: {{option1}} ||| {{option2}} Fill in the _ in the below sentence: {{sentence}} Choices: -{{ option1 }} -{{ option2 }} Answer: {% if answer == '1' %} {{option1}} {% else %} {{ option2 }} {% endif %} The following sentence is either "{{"acceptable"}}", meaning it is grammatically correct and makes sense, or "{{"unacceptable"}}". Which is it? {{sentence}} {{ answer_choices[label] }} {{sentence}} I'm worried that sentence didn't make any sense, or was grammatically incorrect. Was it correct? Generate a sentence that means the same thing as this one: {{sentence1}} 1.6 QA EXTRACTIVE Prompt from Brown et al. (2020) {% if label == 1 %} {{ answer_choices[label] }} Prompt from Brown et al. (2020) {{premise}} Question: {{hypothesis}} True or False? {{ answer_choices[label] }} Prompt from Webson and Pavlick (2021) Suppose {{premise}} Can we infer that "{{hypothesis}}"? Yes or no? {{ answer_choices[label] }} {{ answer_choices[label] }} Are the following two sentences "{{"equivalent"}}" or "{{"not equivalent"}}"? {{ answer_choices[label] }} {{answer_choices[label]}} Prompt not from the original task. Sentence 2: {{sentence2}} Question: Do Sentence 1 and Sentence 2 express the same meaning? Yes or {{answer_choices[label]}} No? {% endif %} Question: Do Sentence 1 and Sentence 2 express the same meaning? {{answer}} Sentence 2: {{sentence2}} Sentence 1: {{sentence1}} Sentence 1: {{sentence1}} Answer this question: {{question}}? {{sentence2}} {% if label == 1 %} {{sentence1}} Determine if the following two sentences paraphrase each other or not. Sent 1: {{sentence1}} Sent 2: {{sentence2}} {{answer_choices[label]}} Sentence 1: {{sentence1}} Topic: Data Example Sentence 2: {{sentence2}} {{document_title}} Question: Does Sentence 1 paraphrase Sentence 2? {% endif %} Key Value {{answer_choices[label]}} answers {'answer_end': [179, 494, 511, 545, 879, 1127, 112... questions ['</cell></row><row><cell>In the sentence below, does the _ stand for {{answer_choices[0]}} or {{answer_choices[1]}}? {{sentence}} {{sentence}} Is this example grammatically correct and sensible? Prompt from Schick and Schütze (2021) Data Example Sentence 2: {{sentence2}} Question: Does Sentence 1 paraphrase Sentence 2? Yes or No? Data Example 1.1.4 WINOGRANDE WINOGRANDE DEBIASED {{ answer_choices[label] }} {{sentence2}} {{answers.text | choice}} {% endif %} Sentence 1: {{sentence1}} {% endif %}</cell></row></table><note>1.1.3 WINOGRANDE WINOGRANDE XLDataset from<ref type="bibr" target="#b75">Sakaguchi et al. (2019)</ref>. Used in evaluation.Dataset from<ref type="bibr" target="#b75">Sakaguchi et al. (2019)</ref>. Used in evaluation.Dataset fromIyer et al. (2017). Used in evaluation.Dataset from<ref type="bibr" target="#b14">Clark et al. (2018)</ref>. Used in evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>for choice in choices.label %} {% if choice == answerKey %}{{ answer_choices[loop.index -1] }}{% endif %}{% endfor %} Prompt not from the original task. Do you think the right answer to the question "{{ question[0]|lower }}{{ question[1:] }}" is "{{ choices.text[1][0]|lower}}{{ choices.text[1][1:]|trim('.') }}", given that {{combinedfact[0]|lower}}{{ combinedfact[1:]|trim('.') }}? {% if answerKey == choices.label[0] %} Yes {% else %} No {% endif %} {{choices.text | join("|||")}} Fact 1: {{ fact1[0]|capitalize }}{{ fact1[1:]|trim|trim('.') }}. enough information', 'to visit family', 'par... context That fall came and I went back to Michigan and the... There are five temporal categories: {{"Event Duration"}}, {{"Event Ordering"}}, {{"Frequency"}}, {{"Typical Time"}}, {{"Stationarity"}}.</figDesc><table><row><cell>Given the context, Based on this review, would the user recommend this product?</cell></row><row><cell>===</cell></row><row><cell>{{sentence}} Review: {{content}}</cell></row><row><cell>Answer:</cell></row><row><cell>observe the following QA pair and check if the answer is plausible:</cell></row><row><cell>{{answer_choices[label]}}</cell></row><row><cell>Question: {{question}}</cell></row><row><cell>Answer: {{answer}}</cell></row><row><cell>{{answer_choices[label]}} Is this product review positive?</cell></row><row><cell>Title: {{title}}</cell></row><row><cell>Review: {{content}}</cell></row><row><cell>Answer:</cell></row><row><cell>I've been grappling with the temporal accuracy of this answer for a while: {{answer_choices[label]}}</cell></row><row><cell>Q: "{{question}}"</cell></row><row><cell>I have the following information: "{{sentence}}" Title: {{title}}</cell></row><row><cell>Review: {{content}} A: "{{answer}}" Is this product review negative?</cell></row><row><cell>This answer is definitely not {{answer_choices[label]}}</cell></row><row><cell>{{answer_choices[label]}}</cell></row><row><cell>Title: {{title}} Prompt not from the original task. Review: {{content}}</cell></row><row><cell>Does this product review convey a negative or positive sentiment?</cell></row><row><cell>{{answer_choices[label]}}</cell></row><row><cell>Out of the above temporal categories, which one does the question</cell></row><row><cell>"{{question}}" belong to?</cell></row><row><cell>{{answer_choices[category]}} Is there a negative or positive tone to this product review?</cell></row><row><cell>===</cell></row><row><cell>Title: {{title}}</cell></row><row><cell>Data Example Review: {{content}} Prompt not from the original task. Answer:</cell></row><row><cell>Key {% if label %} I have the following passage: Value {{answer_choices[label]}}</cell></row><row><cell>answers {{sentence}} ['not context id f001 correct answer id My query is: "{{question}}" Title: {{title}} 3 domain fiction id Product review: {{content}} I want an answer that is "temporally plausible". Would you say this review depicts the product in a {{answer_choices[1]}} f001_0 metadata {{answer}} or {{answer_choices[0]}} light? {'author': 'Joseph Devon', 'title': 'Black Eyed Su... question Why was this character sent away after each school... question id Causality question type 0 {% endif %} {{answer_choices[label]}}</cell></row><row><cell>Prompts Here's what happened: {{sentence}}</cell></row><row><cell>I asked my friend {{question}}</cell></row><row><cell>{{ context }} and they said {{answer}}</cell></row><row><cell>Question: {{ question }}</cell></row><row><cell>Options: Should I believe them?</cell></row></table><note>1.6.12 QUOREF Dataset from<ref type="bibr" target="#b17">Dasigi et al. (2019)</ref>. Used in training. {% Fact 2: {{fact2[0]|capitalize }}{{ fact2[1:]|trim|trim('.') }}. Given the two facts above, answer the question "{{ question }}" with the following options: -{{answer_choices | join("\n -") }} {% for choice in choices.label %} {% if choice == answerKey %}{{ answer_choices[loop.index -1] }}{% endif %}{% endfor %} 1.8.6 QUAIL Dataset from Rogers et al. (2020). Used in training.You are considering whether to buy a product. You look at the reviews. Would the following review {{answer_choices[0]}} or {{answer_choices<ref type="bibr" target="#b99">[1]</ref>}} the chances of you buying the product? Review title: {{title}} Product review: {{content}}</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For example, see https://www.quizbreaker.com/trivia-questions, https: //www.scarymommy.com/best-trivia-questions-answers/, and https://parade. com/944584/parade/trivia-questions-for-kids/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://github.com/google/BIG-bench</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Our experiments in Section 6.2 lead us to believe that this performance corresponds to the best prompt found after manual tuning according to validation set performance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">https://beta.openai.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://beta.openai.com/docs/engines/instruct-series-beta</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">https://bigscience.huggingface.co/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">https://cloud.google.com/sustainability/region-carbon</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7">https://streamlit.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8">https://jinja.palletsprojects.com</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>This work was granted access to the HPC resources of IDRIS under the allocation 2021-A0101012475 made by GENCI. In particular, all the evaluations and data processing ran on the Jean-Zay cluster of IDRIS, and we want to thank the IDRIS team for responsive support throughout the project, in particular Rémi Lacroix. We are grateful for the TPU Research Cloud program who We thank Yacine Jernite, Sasha Luccioni, Aurélie Névéol and Huu Nguyen for advising on strategies to deal with datasets containing potentially harmful content. Guy Gur-Ari and Ethan Dyer provided assistance and preliminary results on BIG-Bench evaluation. We thank Ruiqui Zhong for early discussions on this project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the sentence below, does the _ stand for {{answer_choices[0]}} or {{answer_choices <ref type="bibr" target="#b99">[1]</ref>}}? {{sentence}} {{premise}} Based on the previous passage, is it true that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Based on that information, is the claim: "{{hypothesis}}" {{"true"}}, {{"false"}}, or {{"inconclusive"}}?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Does it follow that {{hypothesis}} Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Are we justified in saying that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Suppose it's true that {{premise}} Then, is "{{hypothesis}}" {{"always"}}, {{"sometimes"}}, or {{"never"}} true?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b10">Brown et al. (2020)</ref> {{premise}} Question: {{hypothesis}} True, False, or Neither?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Keeping in mind the above text, consider: {{hypothesis}} Is this {{"always"}}, {{"sometimes"}}, or {{"never"}} correct?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Is it guaranteed true that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Therefore, it must be true that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Assume it is true that {{premise}} Therefore, "{{hypothesis}}" is {{"guaranteed"}}, {{"possible"}}, or {{"impossible"}}?</p><p>{{ answer_choices[label] }} {{premise}} Question: Does this imply that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{answer_choices[label]}}</head><p>Prompt from <ref type="bibr">Williams et al. (2018)</ref> {{premise}} Using only the above description and what you know about the world, "{{hypothesis}}" is definitely correct, incorrect, or inconclusive?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Should we assume that "{{hypothesis}}" is true? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Take the following as truth: {{premise}} Then the following statement: "{{hypothesis}}" is {{"true"}}, {{"false"}}, or {{"inconclusive"}}?</p><p>{{ answer_choices[label] }}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2">SUPER GLUE RTE</head><p>Dataset from <ref type="bibr" target="#b16">Dagan et al. (2005)</ref>. Used in evaluation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Example Prompts</head><p>Prompt from <ref type="bibr">Williams et al. (2018)</ref> {{premise}} Using only the above description and what you know about the world, is "{{hypothesis}}" definitely correct? Yes or no?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Is it guaranteed true that "{{hypothesis}}"? Yes or no?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Suppose {{premise}} Can we infer that "{{hypothesis}}"? Yes or no?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b10">Brown et al. (2020)</ref> {{premise}} Question: {{hypothesis}} True or False?</p><p>{{ answer_choices[label] }} {{premise}} Question: Does this imply that "{{hypothesis}}"? Yes or no?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{answer_choices[label]}}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Should we assume that "{{hypothesis}}" is true? Yes or no?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Does it follow that {{hypothesis}} Yes or no?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b76">Schick and Schütze (2021)</ref> {{premise}} Based on the previous passage, is it true that "{{hypothesis}}"? Yes or no?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Are we justified in saying that "{{hypothesis}}"? Yes or no?</p><p>{{ answer_choices[label] }} Given that {{premise}} Therefore, it must be true that "{{hypothesis}}"? Yes or no?</p><p>{{ answer_choices[label] }}</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.3">ANLI</head><p>Dataset from Nie et al. <ref type="bibr">(2020)</ref>. Used in evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Example</head><p>Key Value hypothesis</p><p>The trolleybus system has over 2 urban routes label 0 premise The Parma trolleybus system (Italian: "Rete filovi... reason uid 0fd0abfb-659e-4453-b196-c3a64d2d8267</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>Prompt from <ref type="bibr">Williams et al. (2018)</ref> {{premise}} Using only the above description and what you know about the world, "{{hypothesis}}" is definitely correct, incorrect, or inconclusive?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Should we assume that "{{hypothesis}}" is true? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Does it follow that {{hypothesis}} Yes, no, or maybe?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b10">Brown et al. (2020)</ref> {{premise}} Question: {{hypothesis}} True, False, or Neither?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b76">Schick and Schütze (2021)</ref> {{premise}} Based on the previous passage, is it true that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Are we justified in saying that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Take the following as truth: {{premise}} Then the following statement: "{{hypothesis}}" is {{"true"}}, {{"false"}}, or {{"inconclusive"}}?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Therefore, it must be true that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Suppose {{premise}} Can we infer that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Assume it is true that {{premise}} Therefore, "{{hypothesis}}" is {{"guaranteed"}}, {{"possible"}}, or {{"impossible"}}?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Suppose it's true that {{premise}} Then, is "{{hypothesis}}" {{"always"}}, {{"sometimes"}}, or {{"never"}} true?</p><p>{{ answer_choices[label] }} {{premise}} Question: Does this imply that "{{hypothesis}}"? Yes, no, or maybe?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{answer_choices[label]}}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Keeping in mind the above text, consider: {{hypothesis}} Is this {{"always"}}, {{"sometimes"}}, or {{"never"}} correct?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> {{premise}} Based on that information, is the claim: "{{hypothesis}}" {{"true"}}, {{"false"}}, or {{"inconclusive"}}?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Is it guaranteed true that "{{hypothesis}}"? Yes, no, or maybe?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Should we assume that "{{hypothesis}}" is true? Yes or no?</p><p>{{ answer_choices[label] }} Prompt from <ref type="bibr" target="#b76">Schick and Schütze (2021)</ref> {{premise}} Based on the previous passage, is it true that "{{hypothesis}}"? Yes or no?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Does it follow that {{hypothesis}} Yes or no?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt from <ref type="bibr" target="#b91">Webson and Pavlick (2021)</ref> Given {{premise}} Is it guaranteed true that "{{hypothesis}}"? Yes or no?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Given that {{premise}} Therefore, it must be true that "{{hypothesis}}"? Yes or no? How is the life of a math student? Could you descr... question2</p><p>Which level of prepration is enough for the exam j...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>I'm an administrator on the website Quora. There are two posts, one that asks "{{question1}}" and another that asks "{{question2}}". I can merge questions if they are asking the same thing. Can I merge these two questions?</p><p>{{ answer_choices[label] }} {{question1}} {{question2}} Pick one: These questions are "{{"duplicates"}}" or "{{"not duplicates"}}".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Are the questions "{{question1}}" and "{{question2}}" asking the same thing?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ answer_choices[label] }}</head><p>Prompt not from the original task.</p><p>Can an answer to "{{question1}}" also be used to answer "{{question2}}"?</p><p>{{ answer_choices[label] }} </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>Prompt not from the original task.</p><p>Pick and copy all the incorrect options for the following question: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Example Prompts</head><p>Prompt not from the original task.</p><p>Read the below conversation. Prompt not from the original task.</p><p>Given the question "{{question}}" and the answer "{{answer}}", write a conversation that might have happened.</p><p>{{dialogue | join("\n\n")}}</p><p>Prompt not from the original task.   <ref type="bibr">, 0, 2, 3]</ref>, <ref type="bibr" target="#b99">[1,</ref><ref type="bibr">0,</ref><ref type="bibr">3,</ref><ref type="bibr">2]</ref>, <ref type="bibr" target="#b99">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">0,</ref><ref type="bibr">3]</ref>, <ref type="bibr" target="#b99">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">0]</ref>, <ref type="bibr" target="#b99">[1,</ref><ref type="bibr">3,</ref><ref type="bibr">0,</ref><ref type="bibr">2]</ref>, <ref type="bibr" target="#b99">[1,</ref><ref type="bibr">3,</ref><ref type="bibr">2,</ref><ref type="bibr">0]</ref>   Given that the answer to a question is "{{{"1": answerA, "2": answerB, "3": answerC}  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>Prompt not from the original task.</p><p>-{{ question_para_step[1:] | join("\n-") }} What might be the first step of the process?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ question_para_step | first }}</head><p>Prompt not from the original task.</p><p>What might be the last step of the process?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>{{ process_list | last }}</head><p>Prompt not from the original task.</p><p>What is the missing first step of the following process:</p><p>-{{ question_para_step Answer by {{"more, less or no effect"}} {{answer_label|replace("_", " ")}}</p><p>Prompt not from the original task.</p><p>Process:</p><p>-{{ question_para_step | join("\n-") }} {{question_stem}} Which of the following is the supposed perturbation?</p><p>-{{"directly impacting a step of the process"}} -{{"indirectly impacting a step of the process"}} -{{"not impacting any step of the process"}} {{{"EXOGENOUS_EFFECT": "indirectly impacting a step of the process", "OUTOFPARA_DISTRACTOR": "not impacting any step of the process", "INPARA_EFFECT": "directly impacting a step of the process"}[metadata_question_type]}} Process:</p><p>-{{ question_para_step | join("\n-")}} Question: {{question_stem}} -{{"A: more"}} -{{"B: less"}} -{{"C: no effect"}} {{answer_label_as_choice}}</p><p>Prompt not from the original task.</p><p>Process:</p><p>-{{ question_para_step | join("\n-") }} Perturbation hypothesis: {{question_stem}} Does the supposed perturbation have an effect (direct or indirect) on the process?</p><p>{{{"EXOGENOUS_EFFECT": "yes", "OUTOFPARA_DISTRACTOR": "no", "INPARA_EFFECT": "yes"}[metadata_question_type]}} </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prompts</head><p>Prompt not from the original task.</p><p>Convert this question to a sentence declarative sentence asserting an affirmative answer:</p><p>Given the question-answer pair of X and Y in the context of {{context}}, which of the following answers is Y implying: "{{"Yes"}}", "{{"No"}}", "{{"In the middle, neither yes nor no"}}", "{{"Probably yes / sometimes yes"}}", "{{"Probably no"}}", "{{"Yes, subject to some conditions"}}", "{{"Other"}}" or "{{"I am not sure how X will interpret Y's answer"}}" ? Given the question-answer pair of X and Y in the context of {{context}}, which of the following answers is Y implying: "{{"Yes"}}", "{{"No"}}", "{{"In the middle, neither yes nor no"}}", "{{"Probably yes / sometimes yes"}}", "{{"Probably no"}}", "{{"Yes, subject to some conditions"}}", "{{"Other"}}" or "{{"I am not sure how X will interpret Y's answer"}}" ?   {% set label_mapping = {39:0, 13 <ref type="bibr">:1, 8:2, 40:3, 25:4, 43:5, 27:6, 38:7, 35:8, 41:9, 32:10, 45:11, 14</ref> {% set label_mapping = {2:0, 22 <ref type="bibr">:1, 19:2, 1:3, 46:3, 23:4, 10:5, 17:6, 33:7, 37:8, 15:9, 30:10, 26:11, 16:12, 28:13, 42:14, 31:15, 20:16, 44:17, 36:18, 14</ref> {% set label_mapping = {39:0, 13 <ref type="bibr">:1, 8:2, 40:3, 25:4, 43:5, 27:6, 38:7, 35:8, 41:9, 32:10, 45:11, 14</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cloze-driven pretraining of self-attention networks</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.07785</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second PASCAL challenges workshop on recognising textual entailment</title>
				<meeting>the second PASCAL challenges workshop on recognising textual entailment</meeting>
		<imprint>
			<publisher>Venice</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="6" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beat the ai: Investigating adversarial human annotation for reading comprehension</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00338</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00338" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="662" to="678" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">going on a vacation&quot; takes longer than &quot;going for a walk&quot;: A study of temporal commonsense understanding</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the dangers of stochastic parrots: Can language models be too big?</title>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">M</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelina</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmargaret</forename><surname>Shmitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</title>
				<meeting>the 2021 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="610" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D13-1160" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Beyond the imitation game: Measuring and extrapolating the capabilities of language models</title>
		<ptr target="https://github.com/google/BIG-bench/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>BIG-bench collaboration. In preparation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Piqa: Reasoning about physical commonsense in natural language</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the opportunities and risks of foundation models</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simran</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Brunskill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dallas</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niladri</forename><surname>Castellon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><forename type="middle">S</forename><surname>Chatterji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Creel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorottya</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Demszky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moussa</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Doumbouya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ethayarajh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Gale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shelby</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyusha</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Kalluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Karamcheti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fereshte</forename><surname>Keeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Khani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><forename type="middle">Wei</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ranjay</forename><surname>Krass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohith</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><surname>Kuditipudi</surname></persName>
		</author>
		<idno>abs/2108.07258</idno>
		<ptr target="https://arxiv.org/abs/2108.07258" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1007379606734</idno>
		<ptr target="https://doi.org/10.1023/A:1007379606734" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">QuAC: Question answering in context</title>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1241</idno>
		<ptr target="https://aclanthology.org/D18-1241" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="2174" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Boolq: Exploring the surprising difficulty of natural yes/no questions</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>CoRR, abs/1905.10044</idno>
		<ptr target="http://arxiv.org/abs/1905.10044" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Think you have solved question answering? try arc, the ai2 reasoning challenge</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isaac</forename><surname>Cowhey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carissa</forename><surname>Schoenick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05457v1</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: deep neural networks with multitask learning</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.1145/1390156.1390177</idno>
		<ptr target="https://doi.org/10.1145/1390156.1390177" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<title level="s">ACM International Conference Proceeding Series</title>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sam</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<meeting><address><addrLine>Helsinki, Filnand</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">June 5-9, 2008. 2008</date>
			<biblScope unit="volume">307</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Quoref: A reading comprehension dataset with questions requiring coreferential reasoning</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05803v2</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hate Speech Dataset from a White Supremacy Forum</title>
		<author>
			<persName><forename type="first">Ona</forename><surname>De Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiara</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Garcia-Pablos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Cuadros</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5102</idno>
		<ptr target="https://www.aclweb.org/anthology/W18-5102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</title>
				<meeting>the 2nd Workshop on Abusive Language Online (ALW2)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
				<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Multi-news: a largescale multi-document summarization dataset and abstractive hierarchical model</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
				<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Samsum corpus: A humanannotated dialogue dataset for abstractive summarization</title>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gliwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iwona</forename><surname>Mochol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Biesek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Wawer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Twitter sentiment classification using distant supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richa</forename><surname>Bhayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">CS224N project report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">English gigaword. Linguistic Data Consortium</title>
		<author>
			<persName><forename type="first">David</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuaki</forename><surname>Maeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<pubPlace>Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A joint many-task model: Growing a neural network for multiple NLP tasks</title>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1611.015collin87</idno>
		<ptr target="http://arxiv.org/abs/1611.01587" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Toward semantics-based answer pinpointing</title>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurie</forename><surname>Gerber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Ravichandran</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/H01-1069" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Human Language Technology Research</title>
				<meeting>the First International Conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cosmos qa: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00277v2</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Crowdsourcing multiple choice science questions</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06209v1</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03551</idno>
		<title level="m">triviaqa: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Looking beyond the surface:a challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of North American Chapter</title>
				<meeting>North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Crossing format boundaries with a single QA system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><surname>Unifiedqa</surname></persName>
		</author>
		<idno>CoRR, abs/2005.00700</idno>
		<ptr target="https://arxiv.org/abs/2005.00700" />
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">UNIFIEDQA: Crossing format boundaries with a single QA system</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.171</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020b</date>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Qasc: A dataset for question answering via sentence composition</title>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11473v2</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">What changes can largescale language models bring? intensive study on hyperclova: Billions-scale korean generative pretrained transformers</title>
		<author>
			<persName><forename type="first">Boseop</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoungseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gichang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Hyeon Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunghyun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seonhoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongpil</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04650</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09700</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04683</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Generating text from structured data with application to the biography domain</title>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Lebret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno>CoRR, abs/1603.07771</idno>
		<ptr target="http://arxiv.org/abs/1603.07771" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Nystrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06499</idno>
		<title level="m">Deduplicating training data makes language models better</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dbpedia-a largescale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anja</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>"oren Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semantic web</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="167" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno>CoRR, abs/2104.08691</idno>
		<ptr target="https://arxiv.org/abs/2104.08691" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franc ¸ois Lagunas</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Villanova Del Moral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Tunstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Šaško</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavitvya</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Brandeis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Patry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<pubPlace>Angelina McMillan-Major, Philipp Schmid; Alexander M. Rush</pubPlace>
		</imprint>
	</monogr>
	<note>and Thomas Wolf. Datasets: A community library for natural language processing. emnlp</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning question classifiers</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/C02-1150" />
	</analytic>
	<monogr>
		<title level="m">COLING 2002: The 19th International Conference on Computational Linguistics</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CommonGen: A constrained text generation challenge for generative commonsense reasoning</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.165</idno>
		<ptr target="https://aclanthology.org/2020.findings-emnlp.165" />
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
				<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="1823" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reasoning over paragraph effects in situations</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MRQA@EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Balažević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P11-1015" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hidden factors and hidden topics: understanding rating dimensions with review text</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM conference on Recommender systems</title>
				<meeting>the 7th ACM conference on Recommender systems</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The natural language decathlon: Multitask learning as question answering</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>CoRR, abs/1806.08730</idno>
		<ptr target="http://arxiv.org/abs/1806.08730" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tal</forename><surname>Linzen</surname></persName>
		</author>
		<idno>CoRR, abs/1902.01007</idno>
		<ptr target="http://arxiv.org/abs/1902.01007" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Natural instructions: Benchmarking generalization to new tasks from natural language instructions</title>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR, abs/2104.08773</idno>
		<ptr target="https://arxiv.org/abs/2104.08773" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Vania</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasika</forename><surname>Bhalerao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno>ArXiv, abs/1808.08745</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<editor>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018. 2020</date>
		</imprint>
	</monogr>
	<note>Adversarial nli: A new benchmark for natural language understanding</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL</title>
				<meeting>the ACL</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The lambada dataset: Word prediction requiring a broad discourse context</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Paperno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ngoc-Quan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gemma</forename><surname>Boleda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Fernández</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1525" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lluis-Miquel</forename><surname>Munguia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Inherent disagreements in human textual inferences</title>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacla00293</idno>
		<ptr target="https://aclanthology.org/Q19-1043" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="677" to="694" />
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/2105.11447</idno>
		<ptr target="https://arxiv.org/abs/2105.11447" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Wic: 10, 000 example pairs for evaluating context-sensitive representations</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Taher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilehvar</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<idno>CoRR, abs/1808.09121</idno>
		<ptr target="http://arxiv.org/abs/1808.09121" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Collecting diverse natural language inference problems for sentence representation evaluation</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Poliak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aparajita</forename><surname>Haldar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Steven White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1007</idno>
		<ptr target="https://aclanthology.org/D18-1007" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="67" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">SQuAD: 100,000+ Questions for Machine Comprehension of Text. arXiv e-prints, art</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Prompt programming for large language models: Beyond the few-shot paradigm</title>
		<author>
			<persName><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
		<idno>CoRR, abs/2102.07350</idno>
		<ptr target="https://arxiv.org/abs/2102.07350" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How much knowledge can you pack into the parameters of a language model?</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.437" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename><surname>Adrian Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 AAAI Spring Symposium Series</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Getting closer to AI complete question answering: A set of prerequisite real tasks</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6398" />
	</analytic>
	<monogr>
		<title level="m">The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
				<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8722" to="8731" />
		</imprint>
	</monogr>
	<note>The Thirty-Fourth AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
				<meeting>the 2018 Conference of the North American Chapter<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1044</idno>
		<ptr target="http://dx.doi.org/10.18653/v1/D15-1044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension</title>
		<author>
			<persName><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Aralikatte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Khapra</surname></persName>
		</author>
		<author>
			<persName><surname>Sankaranarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meeting of the Association for Computational Linguistics (ACL)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">WINOGRANDE: an adversarial winograd schema challenge at scale. CoRR, abs</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1907.10641" />
		<imprint>
			<date type="published" when="1907">1907.10641. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Exploiting cloze-questions for few-shot text classification and natural language inference</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schütze</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/2021.eacl-main.20" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
				<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04">April 2021</date>
			<biblScope unit="page" from="255" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green Ai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno>CoRR, abs/1704.04368</idno>
		<ptr target="http://arxiv.org/abs/1704" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4596" to="4604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">A challenge dataset for open-domain question answering</title>
		<author>
			<persName><forename type="first">Reddy</forename><surname>Siva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Danqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manning</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wikiqa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Release strategies and the social impacts of language models</title>
		<author>
			<persName><forename type="first">Irene</forename><surname>Solaiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/1908.09203</idno>
		<ptr target="http://arxiv.org/abs/1908.09203" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3645" to="3650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">DREAM: A challenge dataset and models for dialogue-based reading comprehension</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1902.00164v1" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Transactions of the Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">quartz: An open-domain dataset of qualitative relationship questions</title>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Quarel: A dataset and models for answering questions about qualitative relationships</title>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<idno>CoRR, abs/1811.08048</idno>
		<ptr target="http://arxiv.org/abs/1811.08048" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Exploring and predicting transferability across NLP tasks</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.635</idno>
		<ptr target="https://aclanthology.org/2020.emnlp-main.635" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="7882" to="7926" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<idno>CoRR, abs/1905.00537</idno>
		<ptr target="http://arxiv.org/abs/1905.00537" />
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding. ICLR, 2019b</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In the Proceedings of ICLR</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Entailment as few-shot learner</title>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno>CoRR, abs/2104.14690</idno>
		<ptr target="https://arxiv.org/abs/2104.14690" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Do prompt-based models really understand the meaning of their prompts?</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.01247" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title level="m" type="main">Anlizing the adversarial natural language inference dataset</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12729</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Crossfit: A few-shot learning challenge for cross-task generalization in nlp</title>
		<author>
			<persName><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08835</idno>
		<ptr target="https://arxiv.org/abs/2104.08835" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">WikiQA: A Challenge Dataset for Open-Domain Question Answering</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1237</idno>
		<imprint>
			<date type="published" when="2013">2013-2018, 2015</date>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Hellaswag: Can a machine really finish your sentence?</title>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">| choice %} Q: {{question}} Read this paragraph and choose the correct option from the provided answers: {{support}} Choices</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12885</idno>
		<idno>{{ answer_choices[order[0]] }} -{{ answer_choices[order[1]] }} -{{ answer_choices[order[2]] }} -{{ answer_choices</idno>
	</analytic>
	<monogr>
		<title level="m">}} {{distractor1}} ||| {{distractor2}} ||| {{distractor3}} ||| {{correct_answer}} {% set order =</title>
				<imprint>
			<date type="published" when="2018">2018. 0, 3, 2, 1], [1, 0, 2, 3], [1, 0, 3, 2], [1, 2, 0, 3], [1, 2, 3, 0], [1, 3, 0, 2. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>{{distractor1}} ||| {{distractor2}} ||| {{distractor3}} ||| {{correct_answer}} {% set order =. 0, 1, 2, 3], [0, 1, 3, 2], [0, 2, 1, 3], [0, 2, 3, 1], [0, 3, 1, 2], [0, 3, 2, 1], [1, 0, 2, 3], [1, 0, 3, 2], [1, 2, 0, 3], [1, 2, 3, 0], [1, 3, 0, 2], [1, 3, 2, 0. 2, 1, 0, 3. 2, 1, 0, 2. 2, 0, 1, 3], [2, 0, 3, 1. 2, 3, 1, 0. 2, 3, 0, 1. 3, 1, 2, 0. 3, 1, 0, 2], [3, 2, 1, 0], [3, 2, 0, 1. 3, 0, 1, 2. 3, 0, 2, 1]] | choice %} Answer the following question given this paragraph: {{support}} Q: {{question}} 1.10 STORY COMPLETION 1.10.1 HELLASWAG Dataset from Zellers et al.. Used in evaluation</note>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName><surname>Common Gen Dataset From Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Used in training</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
