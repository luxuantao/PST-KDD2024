<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Uncertain&lt; &lt; &lt;T&gt; &gt; &gt;: A First-Order Type for Uncertain Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">James</forename><surname>Bornholt</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Todd</forename><surname>Mytkowicz</surname></persName>
							<email>toddm@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
							<email>mckinley@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Uncertain&lt; &lt; &lt;T&gt; &gt; &gt;: A First-Order Type for Uncertain Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3FC26E37D3AA962907CE09E5514BC1BE</idno>
					<idno type="DOI">10.1145/2541940.2541958</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D.3.3 [Programming Languages]: Language Constructs and Features-Data types and structures Estimates</term>
					<term>Statistics</term>
					<term>Probabilistic Programming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emerging applications increasingly use estimates such as sensor data (GPS), probabilistic models, machine learning, big data, and human data. Unfortunately, representing this uncertain data with discrete types (floats, integers, and booleans) encourages developers to pretend it is not probabilistic, which causes three types of uncertainty bugs. (1) Using estimates as facts ignores random error in estimates. (2) Computation compounds that error. (3) Boolean questions on probabilistic data induce false positives and negatives.</p><p>This paper introduces Uncertain T , a new programming language abstraction for uncertain data. We implement a Bayesian network semantics for computation and conditionals that improves program correctness. The runtime uses sampling and hypothesis tests to evaluate computation and conditionals lazily and efficiently. We illustrate with sensor and machine learning applications that Uncertain T improves expressiveness and accuracy.</p><p>Whereas previous probabilistic programming languages focus on experts, Uncertain T serves a wide range of developers. Experts still identify error distributions. However, both experts and application writers compute with distributions, improve estimates with domain knowledge, and ask questions with conditionals. The Uncertain T type system and operators encourage developers to expose and reason about uncertainty explicitly, controlling false positives and false negatives. These benefits make Uncertain T a compelling programming model for modern applications facing the challenge of uncertainty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Applications that sense and reason about the complexity of the world use estimates. Mobile phone applications estimate location with GPS sensors, search estimates information needs from search terms, machine learning estimates hidden parameters from data, and approximate hardware estimates precise hardware to improve energy efficiency. The difference between an estimate and its true value is uncertainty. Every estimate has uncertainty due to random or systematic error. Random variables model uncertainty with probability distributions, which assign a probability to each possible value. For example, each flip of a biased coin may have a 90% chance of heads and 10% chance of tails. The outcome of one flip is only a sample and not a good estimate of the true value. Figure <ref type="figure">1</ref> shows a sample from a Gaussian distribution which is a poor approximation for the entire distribution.</p><p>Most programming languages force developers to reason about uncertain data with discrete types (floats, integers, and booleans). Motivated application developers reason about uncertainty in ad hoc ways, but because this task is complex, many more simply ignore uncertainty. For instance, we surveyed 100 popular smartphone applications that use GPS and find only one (Pizza Hut) reasons about the error in GPS measurements. Ignoring uncertainty creates three types of uncertainty bugs which developers need help to avoid: Using estimates as facts ignores random noise in data and introduces errors. Computation compounds errors since computations on uncertain data often degrade accuracy significantly.</p><p>Conditionals ask boolean questions of probabilistic data, leading to false positives and false negatives. While probabilistic programming <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> and domain-specific solutions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> address parts of this problem, they demand expertise far beyond what client applications require. For example in current probabilistic programming languages, domain experts create and query distributions through generative models. Current APIs for estimated data from these programs, sensors, big data, machine learning, and other sources then project the resulting distributions into discrete types. We observe that the probabilistic nature of estimated data does not stop at the API boundary. Applications using estimated data are probabilistic programs too! Existing languages do not consider the needs of applications that consume estimated data, leaving their developers to face this difficult problem unaided.</p><p>This paper introduces the uncertain type, Uncertain T , a programming language abstraction for arbitrary probability distributions. The syntax and semantics emphasize simplicity for non-experts. We describe how expert developers derive and expose probability distributions for estimated data. Similar to probabilistic programming, the uncertain type defines an algebra over random variables to propagate uncertainty through calculations. We introduce a Bayesian network semantics for computations and conditional expressions. Instead of eagerly evaluating probabilistic computations as in prior languages, we lazily evaluate evidence for the conditions. Finally, we show how the uncertain type eases the use of prior knowledge to improve estimates.</p><p>Our novel implementation strategy performs lazy evaluation by exploiting the semantics of conditionals. The Uncertain T runtime creates a Bayesian network that represents computations on distributions and then samples it at conditional expressions. A sample executes the computations in the network. The runtime exploits hypothesis tests to take only as many samples as necessary for the particular conditional, rather than eagerly and exhaustively producing unnecessary precision (as in general inference over generative models). These hypothesis tests both guarantee accuracy bounds and provide high performance.</p><p>We demonstrate these claims with three case studies. <ref type="bibr" target="#b0">(1)</ref> We show how Uncertain T improves accuracy and expressiveness of speed computations from GPS, a widely used hardware sensor. <ref type="bibr" target="#b1">(2)</ref> We show how Uncertain T exploits prior knowledge to minimize random noise in digital sensors. <ref type="bibr" target="#b2">(3)</ref> We show how Uncertain T encourages developers to explicitly reason about and improve accuracy in machine learning, using a neural network that approximates hardware <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. In concert, the syntax, semantics, and case studies illustrate that Uncertain T eases probabilistic reasoning, improves estimates, and helps domain experts and developers work with uncertain data.</p><p>Our contributions are (1) characterizing uncertainty bugs; (2) Uncertain T , an abstraction and semantics for uncertain data; <ref type="bibr" target="#b2">(3)</ref> implementation strategies that make this semantics practical; and (4) case studies that show Uncertain T 's potential to improve expressiveness and correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>Modern and emerging applications compute over uncertain data from mobile sensors, search, vision, medical trials, benchmarking, chemical simulations, and human surveys. Characterizing uncertainty in these data sources requires domain expertise, but non-expert developers (perhaps with other expertise) are increasingly consuming the results. This section uses Global Positioning System (GPS) data to motivate a correct and accessible abstraction for uncertain data.</p><p>On mobile devices, GPS sensors estimate location. APIs for GPS typically include a position and estimated error radius (a confidence interval for location). The Windows Phone (WP) API returns three fields: public double Latitude, Longitude; // location public double HorizontalAccuracy;</p><p>// error estimate This interface encourages three types of uncertainty bugs.</p><p>Interpreting Estimates as Facts Our survey of the top 100 WP and 100 Android applications finds 22% of WP and 40% of Android applications use GPS for location. Only 5% of the WP applications that use GPS read the error radius and only one application (Pizza Hut) acts on it. All others treat the GPS reading as a fact. Ignoring uncertainty this way causes errors such as walking through walls or driving on water. Current abstractions encourage this treatment by obscuring uncertainty. Consider the map applications on two different smartphone operating systems in Figure <ref type="figure" target="#fig_0">2</ref>, which depict location with a point and horizontal accuracy as a circle. Smaller circles should indicate less uncertainty, but the left larger circle is a 95% confidence interval (widely used for statistical confidence), whereas the right is a 68% confidence interval (one standard deviation of a Gaussian). The smaller circle has a higher standard deviation and is less accurate! (We reverse engineered this confidence interval detail.) A single accuracy number is insufficient to characterize the underlying error distribution or to compute on it. The horizontal accuracy abstraction obscures the true uncertainty, encouraging developers to ignore it completely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compounding Error Computation compounds uncertainty.</head><p>To illustrate, we recorded GPS locations on WP while walk-  ing and computed speed each second. Our inspection of smartphone applications shows this computation on GPS data is very common. Figure <ref type="figure" target="#fig_1">3</ref> plots speed computed by the code in Figure <ref type="figure">5</ref>(a) using the standard GPS API. Whereas Usain Bolt runs 100 m at 24 mph, the average human walks at 3 mph. This experimental data shows an average of 3.5 mph, 35 s spent above 7 mph (running speed), and absurd speeds of 30, 33, and 59 mph. These errors are significant in both magnitude and frequency. The cause is compounded error, since speed is a function of two uncertain locations. When the locations have a 95% confidence interval of 4 m (the best that smartphone GPS delivers), speed has a 95% confidence interval of 12.7 mph. Current abstractions do not capture the compounding of error because they do not represent the distribution nor propagate uncertainty through calculations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditionals</head><p>Programs eventually act on estimated data with conditionals. Consider using GPS to issue tickets for a 60 mph speed limit with the conditional Speed &gt; 60. If your actual speed is 57 mph and GPS accuracy is 4 m, this conditional gives a 32% probability of a ticket due to random noise alone. Figure <ref type="figure" target="#fig_2">4</ref> shows this probability across speeds and GPS accuracies. Boolean questions ignore the potential for random error, leading to false positives and negatives. Applications instead should ask probabilistic questions; for example, only issuing a ticket if the probability is very high that the user is speeding.</p><p>Without an appropriate abstraction for uncertain data, propagation of errors through computations, and probabilistic semantics for conditionals, correctness is out of reach for many developers.  programming approaches, which focus on statistics and machine learning experts, Uncertain T focuses on creating an accessible interface for non-expert developers, who are increasingly encountering uncertain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A First-Order Type for Uncertain Data</head><p>This section first overviews Uncertain T 's syntax and probabilistic semantics and presents an example. It then describes the syntax and semantics for specifying distributions, computing with distributions by building a Bayesian network representation of computations, and executing conditional expressions by evaluating evidence for a conclusion. We then describe how Uncertain T makes it easier for developers to improve estimates with domain knowledge. Section 4 describes our lazy evaluation and sampling implementation strategies that make these semantics efficient.</p><p>Overview An object of type Uncertain T encapsulates a random variable of a numeric type T . To represent computations, the type's overloaded operators construct Bayesian networks, directed acyclic graphs in which nodes represent random variables and edges represent conditional dependences between variables. The leaf nodes of these Bayesian networks are known distributions defined by expert developers. Inner nodes represent the sequence of operations that compute on these leaves. For example, the following code Here the distribution of c is more uncertain than a or b, as Figure <ref type="figure">6</ref> shows, since computation compounds uncertainty.</p><p>This paper describes a runtime that builds Bayesian networks dynamically and then, much like a JIT, compiles those  expression trees to executable code at conditionals. The semantics establishes hypothesis tests at conditionals. The runtime samples by repeatedly executing the compiled code until the test is satisfied.</p><p>Example Program Figure <ref type="figure">5</ref> shows a simple fitness application (GPS-Walking) written in C#, both without (left) and with (right) Uncertain T . GPS-Walking encourages users to walk faster than 4 mph. The Uncertain T version produces more accurate results in part because the type encourages the developer to reason about false positives and negatives. In particular, the developer chooses not to nag, admonishing users to SpeedUp only when it is very confident they are walking slowly. As in traditional languages, Uncertain T only executes one side of conditional branches. Some probabilistic languages execute both sides of conditional branches to create probabilistic models, but Uncertain T makes concrete decisions at conditional branches, matching the host language semantics. We demonstrate the improved accuracy of the Uncertain T version of GPS-Walking in Section 5.1.</p><p>This example serves as a good pedagogical tool because of its simplicity, but the original embodies a real-world uncertainty problem because many smartphone applications on all major platforms use the GPS API exactly this way. Uncertain T 's simple syntax and semantics result in very few changes to the program: the developer only changes the variable types and the conditional operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Syntax with Operator Overloading</head><p>Table <ref type="table">1</ref> shows the operators and methods of Uncertain T . Uncertain T defines an algebra over random variables to propagate uncertainty through computations, overloading the usual arithmetic operators from the base numeric type T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Operators</head><p>Math (+ - * /) op ::</p><formula xml:id="formula_0">U T → U T → U T Order (&lt; &gt; ≤ ≥) op :: U T → U T → U Bool Logical (∧ ∨) op :: U Bool → U Bool → U Bool Unary (¬) op :: U Bool → U Bool Point-mass Pointmass :: T → U T Conditionals Explicit Pr :: U Bool → [0, 1] → Bool Implicit Pr :: U Bool → Bool Evaluation Expected value E :: U T → T U T is shorthand for Uncertain T .</formula><p>Table <ref type="table">1</ref>. Uncertain T operators and methods.</p><p>Developers may override other types as well. Developers compute with Uncertain T as they would with T , and the Bayesian network the operators construct captures how error in an estimate flows through computations.</p><p>Uncertain T addresses two sources of random error: domain error and approximation error. Domain error motivates our work and is the difference between an estimate and its true value. Approximation error is created because Uncertain T must approximate distributions. Developers ultimately make concrete decisions on uncertain data through conditionals. Uncertain T 's conditional operators enforce statistical tests at conditionals, mitigating both sources of error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Identifying Distributions</head><p>The underlying probability distribution for uncertain data is specific to the problem domain. In many cases expert library developers already know these distributions, and sometimes they use them to produce crude error estimates such as the GPS horizontal accuracy discussed in Section 2. Uncertain T offers these expert developers an abstraction to expose these distributions while preserving the simplicity of their current API. The non-expert developers who consume this uncertain data program against the common Uncertain T API, which is very similar to the way they already program with uncertain data today, but aids them in avoiding uncertainty bugs.</p><p>The expert developer has two broad approaches for selecting the right distribution for their particular problem.</p><p>(1) Selecting a theoretical model. Many sources of uncertain data are amenable to theoretical models which library writers may adopt. For example, the error in the mean of a data set is approximately Gaussian by the Central Limit Theorem. Section 5.1 uses a theoretical approach for the GPS-Walking case study.</p><p>(2) Deriving an empirical model. Some problems do not have or are not amenable to theoretical models. For these cases, expert developers may determine an error distribution empirically by machine learning or other mechanisms. Section 5.3 uses an empirical approach for machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representing Distributions</head><p>There are a number of ways to store probability distributions. The most accurate mechanism stores the probability density function exactly. For finite domains, a simple map can assign a probability to each possible value <ref type="bibr" target="#b29">[30]</ref>. For continuous domains, one might reason about the density function algebraically <ref type="bibr" target="#b3">[4]</ref>. For example, a Gaussian random variable with mean µ and variance σ 2 has density function</p><formula xml:id="formula_1">f (x) = 1 σ √ 2π exp - (x -µ) 2 2σ 2 .</formula><p>An instance of a Gaussian random variable need only store this formula and values of µ and σ 2 to represent the variable exactly (up to floating point error). Exact representation has two major downsides. First, the algebra quickly becomes impractical under computation: even the sum of two distributions requires evaluating a convoluted convolution integral. Second, many important distributions for sensors, road maps, approximate hardware, and machine learning do not have closed-form density functions and so cannot be stored this way.</p><p>To overcome these issues, Uncertain T represents distributions through approximate sampling functions. Approximation can be arbitrarily accurate given sufficient space and time <ref type="bibr" target="#b30">[31]</ref>, leading to an efficiency-accuracy trade-off. Many possible approximation schemes might be appropriate for Uncertain T , including fixed vectors of random samples, Chebyshev polynomials <ref type="bibr" target="#b15">[16]</ref>, or sampling functions <ref type="bibr" target="#b22">[23]</ref>. We use sampling functions because they implement a principled solution for balancing accuracy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Computing with Distributions</head><p>Developers combine uncertain data by using the usual operators for arithmetic, logic, and comparison, and Uncertain T manages the resultant uncertainty. Uncertain T overloads such operators from the base type T to work over distributions. For example, the example program in Figure <ref type="figure">5</ref> Note given x of type T , the semantics coerces x to type Uncertain T with a pointmass distribution centered at x, so the denominator dt is cast to type Uncertain Double .</p><p>The same lifting occurs on other arithmetic operators of type T → T → T , as well as comparison operators (type T → T → Bool) and logical operators (type Bool → Bool → Bool).</p><p>A lifted operator may have any type. For example, we can define real division of integers as type Int → Int → Double, which Uncertain T lifts without issue. Instead of executing operations instantaneously, the lifted operators construct Bayesian network representations of the computations. A Bayesian network is a probabilistic graphical model and is a directed acyclic graph whose nodes represent random variables and whose edges represent conditional dependences between those variables <ref type="bibr" target="#b4">[5]</ref>. Figure <ref type="figure" target="#fig_5">7</ref> shows an example of this Bayesian network representation. The shaded leaf nodes are known distributions (such as Gaussians) specified by expert developers, as previously noted. The final Bayesian network defines a joint distribution over all the variables involved in the computation:</p><formula xml:id="formula_2">Pr[A, B,C, D, E] = Pr[A] Pr[B] Pr[C] Pr[D | A, B] Pr[E |C, D].</formula><p>The incoming edges to a node in the Bayesian network graph specify the other variables that the node's variable depends on. For example, A has no dependences while E depends on C and D. Since we know the distributions of the leaf nodes A, B, and C, we use the joint distribution to infer the marginal distributions of the variables D and E, even though their distributions are not explicitly specified by the program. The conditional distributions of inner nodes are specified by their associated operators. For example, the distribution of <ref type="figure" target="#fig_5">7</ref> is simply a pointmass at a/b.</p><formula xml:id="formula_3">Pr[D | A = a, B = b] in Figure</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependent Random Variables</head><p>Two random variables X and Y are independent if the value of one has no bearing on the value of the other. Uncertain T 's Bayesian network representation assumes that leaf nodes are independent. This assumption is common in probabilistic programming, but expert developers can override it by specifying the joint distribution between two variables.  Speed (mph)</p><formula xml:id="formula_4">A = Y + X B = A + X + X + B X Y A (a) Wrong network + + B X Y A (b) Correct network</formula><p>Figure <ref type="figure">9</ref>. Uncertainty in data means there is only a probability that Speed &gt; 4, not a concrete boolean value.</p><p>The Uncertain T semantics automatically addresses program-induced dependences. For example, Figure <ref type="figure" target="#fig_6">8(a)</ref> shows a simple program with a naive incorrect construction of its Bayesian network. This network implies that the operands to the addition that defines B are independent, but in fact both operands depend on the same variable X. When producing a Bayesian network, Uncertain T 's operators echo static single assignment by determining that the two X occurrences refer to the same value, and so A depends on the same X as B. Our analysis produces the correct Bayesian network in Figure <ref type="figure" target="#fig_6">8(b)</ref>. Because the Bayesian network is constructed dynamically and incrementally during program execution, the resulting graph remains acyclic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Asking the Right Questions</head><p>After computing with uncertain data, programs use it in conditional expressions to make decisions. The example program in Figure <ref type="figure">5</ref> compares the user's speed to 4 mph. Of course, since speed is computed with uncertain estimates of location, it too is uncertain. The naive conditional Speed &gt; 4 incorrectly asks a deterministic question of probabilistic data, creating uncertainty bugs described in Section 2.</p><p>Uncertain T defines the semantics of conditional expressions involving uncertain data by computing evidence for a conclusion. This semantics encourages developers to ask appropriate questions of probabilistic data. Rather than asking "is the user's speed faster than 4 mph?" Uncertain T asks "how much evidence is there that the user's speed is faster than 4 mph?" The evidence that the user's speed is faster than 4 mph is the quantity Pr[Speed &gt; 4]; in Figure <ref type="figure">9</ref>, this is the area under the distribution of the variable Speed to the right of 4 mph. Since we are considering probability distributions, the area A satisfies 0 ≤ A ≤ 1.</p><p>When the developer writes a conditional expression if (Speed &gt; 4) ...</p><p>the program applies the lifted version of the &gt; operator &gt; :: Uncertain T → Uncertain T → Uncertain Bool .</p><p>This operator creates a Bernoulli distribution with parameter p ∈ [0, 1], which by definition is the probability that Speed &gt; 4 (i.e., the shaded area under the curve). Unlike other probabilistic programming languages, Uncertain T executes only a single branch of the conditional to match the host language's semantics. The Uncertain T runtime must therefore convert this Bernoulli distribution to a concrete boolean value. This conditional uses the implicit conditional operator, which compares the parameter p of the Bernoulli distribution to 0.5, asking whether the Bernoulli is more likely than not to be true. This conditional therefore evaluates whether Pr[Speed &gt; 4] &gt; 0.5, asking whether it is more likely that the user's speed is faster than 4 mph. Using an explicit conditional operator, developers may specify a threshold to compare against. The second comparison in Figure <ref type="figure">5</ref>(b) uses this explicit operator: else if ((Speed &lt; 4).Pr(0.9)) ... This conditional evaluates whether Pr[Speed &lt; 4] &gt; 0.9. The power of this formulation is reasoning about false positives and negatives. Even if the mean of the distribution is on one side of the conditional threshold, the distribution may be very wide, so there is still a strong likelihood that the opposite conclusion is correct. Higher thresholds for the explicit operator require stronger evidence, and produce fewer false positives (extra reports when ground truth is false) but more false negatives (missed reports when ground truth is true). In this case, the developer chooses to favor some false positives for encouraging users (GoodJob), and to limit false positives when admonishing users (SpeedUp) by demanding stronger evidence that they are walking slower than 4 mph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypothesis Testing for Approximation Error</head><p>Because Uncertain T approximates distributions, we must consider approximation error in conditional expressions. We use statistical hypothesis tests, which make inferences about population statistics based on sampled data. Uncertain T establishes a hypothesis test when evaluating the implicit and explicit conditional operators. In the implicit case above, the null hypothesis is H 0 : Pr[Speed &gt; 4] ≤ 0.5 and the alternate hypothesis H A : Pr[Speed &gt; 4] &gt; 0.5. Section 4.3 describes our sampling process for evaluating these tests in detail.</p><p>Hypothesis tests introduce a ternary logic. For example, given the code sequence if (A &lt; B) ... else if (A &gt;= B) ... neither branch may be true because the runtime may not be able to reject the null hypothesis for either conditional at the required confidence level. This behavior is not new: just as programs should not compare floating point numbers for equality, neither should they compare distributions for equality. However, for problems that require a total order, such as sorting algorithms, Uncertain T provides the expected value operator E. This operator outputs an element of type T and so it preserves the base type's ordering properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Improving Estimates</head><p>Bayesian statistics represents the state of the world with degrees of belief and is a powerful mechanism for improving the quality of estimated data. Given two random variables, B representing the target variable to estimate (e.g., the user's location), and E the estimation process (e.g., the GPS sensor output), Bayes' theorem says that Uncertain T unlocks Bayesian statistics by encapsulating entire data distributions. Abstractions that capture only single point estimates are insufficient for this purpose and force developers to resort to ad-hoc heuristics to improve estimates.</p><formula xml:id="formula_5">Pr[B = b|E = e] = Pr[E = e|B = b] • Pr[B = b] Pr[E = e] .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorporating Knowledge with Priors</head><p>Bayesian inference is powerful because developers can encode their domain knowledge as prior distributions, and use that knowledge to improve the quality of estimates. For example, a developer working with GPS can provide a prior distribution that assigns high probabilities to roads and lower probabilities elsewhere. This prior distribution achieves a "road-snapping" behavior <ref type="bibr" target="#b20">[21]</ref>, fixing the user's location to nearby roads unless GPS evidence to the contrary is very strong. Figure <ref type="figure" target="#fig_8">10</ref> illustrates this example -the mean shifts from p to s, closer to the road the user is actually on.</p><p>Specifying priors, however, requires a knowledge of statistics beyond the scope of most developers. In our current implementation, applications specify domain knowledge with constraint abstractions. Expert developers add preset prior distributions to their libraries for common cases. For example, GPS libraries would include priors for driving (e.g., roads and driving speeds), walking (walking speeds), and being on land. Developers combine priors through flags that select constraints specific to their application. The library applies the selected priors, improving the quality of estimates without much burden on developers. This approach is not very satisfying because it is not compositional, so the application cannot easily mix and match priors from different sources (e.g., maps, calendars, and physics for GPS). We anticipate future work creating an accessible and compositional abstraction for prior distributions, perhaps with the Bayes operator P Q for sampled distributions by Park et al. <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>This section describes our practical and efficient C# implementation of the Uncertain T abstraction. We also implemented prototypes of Uncertain T in C++ and Python and believe most high-level languages could easily implement it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Identifying Distributions</head><p>Uncertain T approximates distributions with sampling functions, rather than storing them exactly, to achieve expressiveness and efficiency. A sampling function has no arguments and returns a new random sample, drawn from the distribution, on each invocation <ref type="bibr" target="#b22">[23]</ref>. For example, a pseudo-random number generator is a sampling function for the uniform distribution, and the Box-Mueller transform <ref type="bibr" target="#b7">[8]</ref> is a sampling function for the Gaussian distribution.</p><p>In the GPS-Walking application in Figure <ref type="figure">5</ref>(b), the variables L1 and L2 are distributions obtained from the GPS library. The expert developer who implements the GPS library derives the correct distribution and provides it to Uncertain T as a sampling function. Bornholt <ref type="bibr" target="#b6">[7]</ref> shows in detail the derivation for the error distribution of GPS data. The resulting model says that the posterior distribution for a GPS estimate is Pr</p><formula xml:id="formula_6">[Location = p | GPS = Sample]</formula><p>= Rayleigh( Samplep ; ε/ √ ln 400) where Sample is the raw GPS sample from the sensor, ε is the sensor's estimate of the 95% confidence interval for the location (i.e., the horizontal accuracy from Section 2), and the Rayleigh distribution <ref type="bibr" target="#b21">[22]</ref> is a continuous non-negative single-parameter distribution with density function <ref type="figure">11</ref> shows the posterior distribution given a particular value of ε. Most existing GPS libraries return a coordinate (the center of the distribution) and present ε as a confidence parameter most developers ignore. Figure <ref type="figure">11</ref> shows that the true location is unlikely to be in the center of the distribution and more likely to be some fixed radius from the center.</p><formula xml:id="formula_7">Rayleigh(x; ρ) = x ρ 2 exp - x 2 2ρ 2 , x ≥ 0. Figure</formula><p>We built a GPS library that captures error in its estimate with Uncertain T using this distribution. Figure <ref type="figure" target="#fig_0">12</ref> shows our library function GPS.GetLocation, which returns an instance of Uncertain GeoCoordinate by implementing a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computing with Distributions</head><p>Uncertain T propagates error through computations by overloading operators from the base type T . These lifted operators dynamically construct Bayesian network representations of the computations they represent as the program executes. However, an alternate implementation could statically build a Bayesian network and only dynamically perform hypothesis tests at conditionals. We use the Bayesian network to define the sampling function for a computed variable in terms of the sampling functions of its operands. We only evaluate the sampling function at conditionals and so the root node of a network being sampled is always a comparison operator.</p><p>The computed sampling function uses the standard ancestral sampling technique for graphical models <ref type="bibr" target="#b4">[5]</ref>. Because the Bayesian network is directed and acyclic, its nodes can be topologically ordered. We draw samples in this topological order, starting with each leaf node, for which sampling functions are explicitly defined. These samples then propagate up the Bayesian network. Each inner node is associated with an operator from the base type, and the node's children have already been sampled due to the topological order, so we simply apply the base operator to the operand samples to generate a sample from an inner node. This process continues up the network until reaching the root node of the network, generating a sample from the computed variable. The topological order guarantees that each node is visited exactly once in this process, and so the sampling process terminates.</p><p>In the GPS-Walking application in Figure <ref type="figure">5</ref> Here shaded nodes indicate leaf distributions, for which sampling functions are defined explicitly.</p><p>To draw a sample from the variable Speed, we draw a sample from each leaf: a sample d from Distance (defined by the GPS library) and t from dt (a pointmass distribution, so all samples are equal). These samples propagate up the network to the Speed node. Because this node is a division operator, the resulting sample from Speed is simply d/t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Asking Questions with Hypothesis Tests</head><p>Uncertain T 's conditional and evaluation operators address the domain error that motivates our work. These operators require concrete decisions under uncertainty. The conditional operators must select one branch target to execute. In the GPS-Walking application in Figure <ref type="figure">5</ref>(b), a conditional operator if (Speed &gt; 4)... must decide whether or not to enter this branch. Section 3.4 describes how Uncertain T executes this conditional by comparing the probability Pr[Speed &gt; 4] to a default threshold 0.5, asking whether it is more likely than not that Speed &gt; 4. To control approximation error, this comparison is performed by a hypothesis test, with null hypothesis H 0 : Pr[Speed &gt; 4] ≤ 0.5 and alternate hypothesis</p><formula xml:id="formula_8">H A : Pr[Speed &gt; 4] &gt; 0.5.</formula><p>Sampling functions in combination with this hypothesis test control the efficiency-accuracy trade-off that approximation introduces. A higher confidence level for the hypothesis test leads to fewer approximation errors but requires more samples to evaluate. We perform the hypothesis test using Wald's sequential probability ratio test (SPRT) <ref type="bibr" target="#b31">[32]</ref> to dynamically choose the right sample size for a particular condi-tional, only taking as many samples as necessary to obtain a statistically significant result.</p><p>We specify a step size, say k = 10, and start by drawing n = k samples from the Bernoulli distribution Speed &gt; 4.</p><p>We then apply the SPRT to these samples to decide if the parameter p of the distribution (i.e., the probability Pr[Speed &gt; 4]) is significantly different from 0.5. If so, we can terminate immediately and take (or not take) the branch, depending on in which direction the significance lies. If the result is not significant, we draw another batch of k samples, and repeat the process with the now n = 2k collection of samples. We repeat this process until either a significant result is achieved or a maximum sample size is reached to ensure termination. The SPRT ensures that this repeated sampling and testing process still achieves overall bounds on the probabilities of false positives (significance level) and false negatives (power).</p><p>Sampling functions and a Bayesian network representation may draw as many samples as we wish from any given variable. We may therefore exploit sequential methods, such as the SPRT, which do not fix the sample size for a hypothesis test in advance. Sequential methods are a principled solution to the efficiency-accuracy trade-off. They ensure we draw the minimum necessary number of samples for a sufficiently accurate result for each specific conditional. This goal-oriented sampling approach is a significant advance over previous random sampling approaches, which compute with a fixed pool of samples. These approaches do not efficiently control the effect of approximation error.</p><p>Wald's SPRT is optimal in terms of average sample size, but is potentially unbounded in any particular instance, so termination is not guaranteed. The artificial maximum sample size we introduce to guarantee termination has a small effect on the actual significance level and power of the test. We anticipate adapting the considerable body of work on group sequential methods <ref type="bibr" target="#b16">[17]</ref>, widely used in medical clinical trials, which provide "closed" sequential hypothesis tests with guaranteed upper bounds on the sample size.</p><p>We cannot apply the same sequential testing approach to the evaluation operator E, since there are no alternatives to compare against (i.e., no goal to achieve). Currently for this operator we simply draw a fixed number of samples and return their mean. We believe a more intelligent adaptive sampling process, sampling until the mean converges, may improve the performance of this approach. son about and improve accuracy in machine learning, using a neural network that approximates hardware <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Case Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Uncertain&lt;T&gt; on Smartphones: GPS-Walking</head><p>The modern smartphone contains a plethora of hardware sensors. Thousands of applications use the GPS sensor, and many compute distance and speed from GPS readings. Our walking speed case study serves double duty, showing how uncertainty bugs occur in practice as well as a clear pedagogical demonstration of how Uncertain T improves correctness and expressiveness. We wrap the Windows Phone (WP) GPS location services API with Uncertain T , exposing the error distribution. The GPS-Walking application estimates the user's speed by taking two samples from the GPS and computing the distance and time between them. Figure <ref type="figure">5</ref> compares the code for GPS-Walking with and without the uncertain type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Defining the Distributions Our Uncertain T GPS library provides the function</head><p>Uncertain&lt;GeoCoordinate&gt; GPS.GetLocation(); which returns a distribution over the user's possible locations. Section 4.1 overviews how to derive the GPS distribution, and Figure <ref type="figure" target="#fig_0">12</ref> shows the implementation.</p><p>Computing with Distributions GPS-Walking uses locations from the GPS library to calculate the user's speed, since Speed = ∆Distance/∆Time. Of course, since the locations are estimates, so too is speed. The developer must change the line print("Speed: " + Speed); from the original program in Figure <ref type="figure">5</ref>(a), since Speed now has type Uncertain Double . It now prints the expected value Speed.E() of the speed distribution.</p><p>We tested GPS-Walking by walking outside for 15 minutes. Figure <ref type="figure" target="#fig_12">13</ref> shows the expected values Speed.E() measured each second by the application (GPS speed). The uncertainty of the speed calculation, with extremely wide confidence intervals, explains the absurd values.</p><p>Conditionals GPS-Walking users to walk faster than 4 mph with messages triggered by conditionals. The original implementation in Figure <ref type="figure">5</ref>(a) uses naive conditionals, which are susceptible to random error. For example, on our test data, such conditionals would report the user to be walking faster than 7 mph (a running pace) for 30 seconds.</p><p>The Uncertain T version of GPS-Walking in Figure <ref type="figure">5</ref>(b) evaluates evidence to execute conditionals. The conditional</p><formula xml:id="formula_9">if (Speed &gt; 4) GoodJob();</formula><p>asks if it is more likely than not that the user is walking fast.</p><p>The second conditional else if ((Speed &lt; 4).Pr(0.9)) SpeedUp();</p><p>asks if there is at least a 90% chance the user is walking slowly. This requirement is stricter than the first conditional because we do not want to unfairly admonish the user (i.e., we want to avoid false positives). Even the simpler more likely than not conditional improves the accuracy of GPS-Walking: such a conditional would only report the user as walking faster than 7 mph for 4 seconds.</p><p>Improving GPS Estimates with Priors Because GPS-Walking uses Uncertain T , we may incorporate prior knowledge to improve the quality of its estimates. Assume for simplicity that users only invoke GPS-Walking when walking.</p><p>Since humans are incredibly unlikely to walk at 60 mph or even 10 mph, we specify a prior distribution over likely walking speeds. Figure <ref type="figure" target="#fig_12">13</ref> shows the results (Improved speed). The confidence interval for the improved data is much tighter, and the prior knowledge removes the absurd results, such as walking at 59 mph.</p><p>Summary Developers need only make minimal changes to the original GPS-Walking application and are rewarded with improved correctness. They improve accuracy by reasoning correctly about uncertainty and eliminating absurd data with domain knowledge. This complex logic is difficult to implement without the uncertain type because a developer must know the error distribution for GPS, how to propagate it through calculations, and how to incorporate domain knowledge to improve the results. The uncertain type abstraction hides this complexity, improving programming productivity and application correctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Uncertain&lt;T&gt; with Sensor Error: SensorLife</head><p>This case study emulates a binary sensor with Gaussian noise to explore accuracy when ground truth is available. For many problems using uncertain data, ground truth is difficult and costly to obtain, but it is readily available in this problem formulation. This case study shows how Uncertain T makes it easier for non-expert developers to work with noisy sensors. Furthermore, it shows how expert developers can simply and succinctly use domain knowledge (i.e., the fact that the noise is Gaussian with known variance) to improve these estimates. We use Conway's Game of Life, a cellular automaton that operates on a two-dimensional grid of cells that are each dead or alive. The game is broken up into generations. During each generation, the program updates each cell by (i) sensing the state of the cell's 8 neighbors, (ii) summing the binary value (dead or alive) of the 8 neighbors, and (iii) applying the following rules to the sum:</p><p>1. A live cell with 2 or 3 live neighbors lives. 2. A live cell with less than 2 live neighbors dies. 3. A live cell with more than 3 live neighbors dies. 4. A dead cell with exactly 3 live neighbors becomes live. These rules simulate survival, underpopulation, overcrowding, and reproduction, respectively. Despite simple rules, the Game of Life provides complex and interesting dynamics (e.g., it is Turing complete <ref type="bibr" target="#b2">[3]</ref>). We focus on the accuracy of sensing if the neighbors are dead or alive.</p><p>Defining the Distributions The original Game of Life's discrete perfect sensors define our ground truth. We view each cell as being equipped with up to eight sensors, one for each of its neighbors. Cells on corners and edges of the grid have fewer sensors. Each perfect sensor returns a binary value s ∈ {0, 1} indicating if the associated neighbor is alive.</p><p>We artificially induce zero-mean Gaussian noise N(0, σ ) on each of these sensors, where σ is the amplitude of the noise. Each sensor now returns a real number, not a binary value. We define three versions of this noisy Game of Life: NaiveLife reads a single sample from each noisy sensor and sums the results directly to count the live neighbors. SensorLife wraps each sensor with Uncertain T . The sum uses the overloaded addition operator and each sensor may be sampled multiple times in a single generation. BayesLife uses domain knowledge to improve SensorLife, as we describe below. Our construction results in some negative sensor readings, but choosing a non-negative noise distribution, such as the Beta distribution, does not appreciably change our results.</p><p>Computing with Distributions Errors in each sensor are independent, so the function that counts a cell's live neighbors is almost unchanged:</p><p>Uncertain&lt;double&gt; CountLiveNeighbors(Cell me) { Uncertain&lt;double&gt; sum = new Uncertain&lt;double&gt;(0.0); foreach (Cell neighbor in me.GetNeighbors()) sum = sum + SenseNeighbor(me, neighbor); return sum; } Because each sensor now returns a real number rather than a binary value, the count of live neighbors is now a distribution over real numbers rather than an integer. Operator overloading means that no further changes are necessary, as the addition operator will automatically propagate uncertainty into the resulting sum.</p><p>Conditionals The Game of Life applies its rules with four conditionals to the output of CountLiveNeighbors: Each comparison involving NumLive implicitly performs a hypothesis test. Evaluation We compare the noisy versions of the Game of Life to the precise version. We perform this comparison across a range of noise amplitude values σ . Each execution randomly initializes a 20 × 20 cell board and performs 25 generations, evaluating a total of 10000 cell updates. For each noise level σ , we execute each Game of Life 50 times. We report means and 95% confidence intervals.</p><p>Figure <ref type="figure" target="#fig_14">14</ref>(a) shows the rate of incorrect decisions (y-axis) made by each noisy Game of Life at various noise levels (x-axis). NaiveLife has a consistent error rate of 8%, as it takes only a small amount of noise to cross the integer thresholds in the rules of the Game of Life. SensorLife's errors scale with noise, as it considers multiple samples and so can correct smaller amounts of noise. At all noise levels, SensorLife is considerably more accurate than NaiveLife.</p><p>Mitigating noise has a cost, as Uncertain T must sample each sensor multiple times to evaluate each conditional. Figure <ref type="figure" target="#fig_14">14(b)</ref> shows the number of samples drawn for each cell update (y-axis) by each noisy Game of Life at various noise levels (x-axis). Clearly NaiveLife only draws one sample per conditional. The number of samples for SensorLife increases as the noise level increases, because noisier sensors require more computation to reach a conclusion at the conditional.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Improving Estimates SensorLife achieves better results than</head><p>NaiveLife despite not demonstrating any knowledge of the fact that the underlying true state of a sensor must be either 0 or 1. To improve SensorLife, an expert can exploit knowledge of the distribution and variance of the sensor noise. We call this improved version BayesLife.</p><p>Let v be the raw, noisy sensor reading, and s the underlying true state (either 0 or 1). Then v = s + N(0, σ ) for some σ we know. Since s is binary, we have two hypotheses for s: H 0 says s = 0 and H 1 says s = 1. The raw sensor reading v is evidence, and Bayes' theorem calculates a posterior probability</p><formula xml:id="formula_10">Pr[H 0 |v] = Pr[v|H 0 ] Pr[H 0 ]/ Pr[v]</formula><p>for H 0 given the evidence, and similarly for H 1 . To improve an estimate, we calculate which of H 0 and H 1 is more likely under the posterior probability, and fix the sensor reading to be 0 or 1 accordingly. This formulation requires (i) the prior likelihoods Pr[H 0 ] and Pr[H 1 ]; and (ii) a likelihood function to calculate Pr[v|H 0 ] and Pr[v|H 1 ]. We assume no prior knowledge, so both H 0 and H 1 are equally likely: Pr[H 0 ] = Pr[H 1 ] = 0.5. Because we know the noise is Gaussian, we know the likelihood function is just the likelihood that N(0, σ ) = sv for each of s = 0 and 1, which we calculate trivially using the Gaussian density function. We need not calculate Pr[v] since it is a common denominator of the two probabilities we compare.</p><p>To implement BayesLife we wrap each sensor with a new function SenseNeighborFixed. Since the two likelihoods Pr[v|H 0 ] and Pr[v|H 1 ] have the same variance and shape and are symmetric around 0 and 1, respectively, and the priors are equal, the hypothesis with higher posterior probability is simply the closer of 0 or 1 to v. The implementation is therefore trivial: Uncertain&lt;double&gt; SenseNeighborFixed(Cell me, Cell neighbor) { Func&lt;double&gt; samplingFunction = () =&gt; { Uncertain&lt;double&gt; Raw = SenseNeighbor(me, neighbor); double Sample = Raw.Sample(); return Sample &gt; 0.5 ? 1.0 : 0.0; }; return new Uncertain&lt;double&gt;(samplingFunction); } Evaluation of BayesLife Figure <ref type="figure" target="#fig_14">14(a)</ref> shows that BayesLife makes no mistakes at all at these noise levels. At noise levels higher than σ = 0.4, considering individual samples in isolation breaks down as the values become almost completely random. A better implementation could calculate joint likelihoods with multiple samples, since each sample is drawn from the same underlying distribution. Figure <ref type="figure" target="#fig_14">14(b)</ref> shows that BayesLife requires fewer samples than SensorLife, but of course still requires more samples than NaiveLife.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Uncertain&lt;T&gt; for Machine Learning: Parakeet</head><p>This section explores using Uncertain T for machine learning, inspired by Parrot <ref type="bibr" target="#b11">[12]</ref>, which trains neural networks for approximate hardware. We study the Sobel operator from the Parrot evaluation, which calculates the gradient of image intensity at a pixel. Parrot creates a neural network that approximates the Sobel operator.</p><p>Machine learning algorithms estimate the true value of a function. One source of uncertainty in their estimates is generalization error, where predictions are good on training data but poor on unseen data. To combat this error, Bayesian machine learning considers distributions of estimates, reflecting how different estimators would answer an unseen input. Figure <ref type="figure" target="#fig_15">15</ref> shows for one such input a distribution of neural network outputs created by the Monte Carlo method described below, the output from the single naive neural network Parrot trains, and the true output. The one Parrot value is significantly different from the correct value. Using a distribution helps mitigate generalization error by recognizing other possible predictions.</p><p>Computation amplifies generalization error. For example, edge-detection algorithms use the Sobel operator to report an edge if the gradient is large (e.g., s(p) &gt; 0.1). Though Parrot approximates the Sobel operator well, with an average root-mean-square error of 3.4%, using its output in such a conditional is a computation. This computation amplifies the error and results in a 36% false positive rate. In Figure <ref type="figure" target="#fig_15">15</ref>, by considering the entire distribution, the evidence for the condition s(p) &gt; 0.1 is only 70%. To accurately consume estimates, developers must consider the effect of uncertainty not just on direct output but on computations that consume it.</p><p>We introduce Parakeet, which approximates code using Bayesian neural networks, and encapsulates the distribution with Uncertain T . This abstraction encourages developers to consider uncertainty in the machine learning prediction. We evaluate Parakeet by approximating the Sobel operator s(p) and computing the edge detection conditional s(p) &gt; 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifying the Distribution</head><p>We seek a posterior predictive distribution (PPD), which tells us for a given input how likely each possible output is to be correct, based on the training data. Intuitively, this distribution reflects predictions from other neural networks that also explain the training data well.</p><p>Formally, a neural network is a function y(x; w) that approximates the output of a target function f (x). The weight vector w describes how each neuron in the network relates to the others. Traditional training uses example inputs and outputs to learn a single weight vector w for prediction. In a Bayesian formulation, we learn the PPD p(t|x, D) (the distribution in Figure <ref type="figure" target="#fig_15">15</ref>), a distribution of predictions of f (x) given the training data D.</p><p>We adopt the hybrid Monte Carlo algorithm to create samples from the PPD <ref type="bibr" target="#b19">[20]</ref>. Each sample describes a neural network. Intuitively, we create multiple neural networks by perturbing the search. Formally, we sample the posterior distribution p(w|D) to approximate the PPD p(t|x, D) using Monte Carlo integration, since p(t|x, D) = p(t|x, w)p(w|D) dw.</p><p>The instance of Uncertain T that Parakeet returns draws samples from this PPD. Each sample of p(w|D) from hybrid Monte Carlo is a vector w of weights for a neural network. To evaluate a sample from the PPD p(t|x, D), we execute a neural network using the weights w and input x. The resulting output is a sample of the PPD. We execute hybrid Monte Carlo offline and capture a fixed number of samples in a training phase. We use these samples at runtime as a fixed pool for the sampling function. If the sample size is sufficiently large, this approach approximates true sampling well. As with other Markov chain Monte Carlo algorithms, the next sample in hybrid Monte Carlo depends on the current sample, which improves on pure random walk behavior that scales poorly in high dimensions. To compensate for this dependence we discard most samples and only retain every M th sample for some large M.</p><p>Hybrid Monte Carlo has two downsides. First, we must execute multiple neural networks (one for each sample of the PPD). Second, it often requires hand tuning to achieve practical rejection rates. Other PPD approximations strike different trade-offs. For example, a Gaussian approximation <ref type="bibr" target="#b4">[5]</ref> to the PPD would mitigate all these downsides, but may be an inappropriate approximation in some cases. Since the Sobel operator's posterior is approximately Gaussian (Figure <ref type="figure" target="#fig_15">15</ref>), a Gaussian approximation may be appropriate.</p><p>Evaluation We approximate the Sobel operator with Parakeet, using 5000 examples for training and a separate 500 examples for evaluation. For each evaluation example we compute the ground truth s(p) &gt; 0.1 and then evaluate this conditional using Uncertain T , which asks whether Pr[s(p) &gt; 0.1] &gt; α for varying thresholds α.</p><p>Figure <ref type="figure">16</ref> shows the results of our evaluation. The x-axis plots a range of conditional thresholds α and the y-axis plots precision and recall for the evaluation data. Precision is the probability that a detected edge is actually an edge, and so describes false positives. Recall is the probability that an actual edge is detected, and so describes false negatives. Because Parrot does not consider uncertainty, it locks developers into a particular balance of precision and recall. In this example, Parrot provides 100% recall, detecting all actual edges, but only 64% precision, so 36% of reported edges are false positives. With a conditional threshold, developers select their own balance. For example, a threshold of α = 0.8 (i.e., eval- The results on Parakeet show that developers may easily control the balance between false positives and false negatives, as appropriate for the application, using Uncertain T . Of course, Uncertain T is not a panacea for machine learning.</p><p>Our formulation leverages a probabilistic interpretation of neural networks, which not all machine learning techniques have, and only addresses generalization error. These case studies highlight the need for a programming language solution for uncertainty. Many systems view error in a local scope, measuring its effect only on direct output, but as all the case studies clearly illustrate, the effect of uncertainty on programs is inevitably global. We cannot view estimation processes such as GPS and machine learning in isolation. Uncertain T encourages developers to reason about how local errors impact the entirety of their programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>This section relates our work to programming languages with statistical semantics or that support solving statistical problems. Because we use, rather than add to, the statistics literature, prior sections cite this literature as appropriate.</p><p>Probabilistic Programming Prior work has focused on the needs of experts. Probabilistic programming creates generative models of problems involving uncertainty by introducing random variables into the syntax and semantics of a programming language. Experts encode generative models which the program queries through inference techniques. The foundation of probabilistic programming is the monadic structure of earthquake = Bernoulli(0.0001) burglary = Bernoulli(0.001) alarm = earthquake or burglary if (earthquake) phoneWorking = Bernoulli(0.7) else phoneWorking = Bernoulli(0.99) observe(alarm) # If the alarm goes off... query(phoneWorking) # ...does the phone still work? Figure <ref type="figure" target="#fig_5">17</ref>. A probabilistic program. To infer the probability of phoneWorking requires exploring both branches <ref type="bibr" target="#b9">[10]</ref>.</p><p>probability distributions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref>, an elegant representation of joint probability distributions (i.e., generative models) in a functional language. Researchers have evolved this idea into a variety of languages such as BUGS <ref type="bibr" target="#b12">[13]</ref>, Church <ref type="bibr" target="#b14">[15]</ref>, Fun <ref type="bibr" target="#b5">[6]</ref>, and IBAL <ref type="bibr" target="#b23">[24]</ref>. Figure <ref type="figure" target="#fig_5">17</ref> shows an example probabilistic program that infers the likelihood that a phone is working given that an alarm goes off. The program queries the posterior distribution of the Bernoulli variable phoneWorking given the observation that alarm is true. To infer this distribution by sampling, the runtime must repeatedly evaluate both branches. This program illustrates a key shortcoming of many probabilistic programming languages: inference is very expensive for poor rejection rates. Since there is only a 0.11% chance of alarm being true, most inference techniques will have high rejection rates and so require many samples to sufficiently infer the posterior distribution. Using Church <ref type="bibr" target="#b14">[15]</ref>, we measured 20 seconds to draw 100 samples from this model <ref type="bibr" target="#b6">[7]</ref>.</p><p>Uncertain T is a probabilistic programming language, since it manipulates random variables in a host language. Unlike other probabilistic languages, Uncertain T only executes one side of a conditional branch, and only reasons about conditional distributions. For example, air temperature depends on variables such as humidity, altitude, and pressure. When programming with data from a temperature sensor, the question is not whether temperature can ever be greater than 85 • (which a joint distribution can answer), but rather whether the current measurement from the sensor is greater than 85 • . This measurement is inextricably conditioned on the current humidity, altitude, and pressure, and so the conditional distribution that Uncertain T manipulates is appropriate. For programs consuming estimated data, problems are not abstract but concrete instances and so the conditional distribution is just as useful as the full joint distribution. Uncertain T exploits this restriction to achieve efficiency and accessibility, since these conditional distributions are specified by operator overloading and evaluated with ancestral sampling.</p><p>Like the probability monad <ref type="bibr" target="#b24">[25]</ref>, Uncertain T builds and later queries a computation tree, but it adds continuous distributions and a semantics for conditional expressions that developers must implement manually using the monad. Uncertain T uses sampling functions in the same fashion as Park et al. <ref type="bibr" target="#b22">[23]</ref>, but we add accessible and principled conditional operators. Park et al. do not describe a mechanism for choosing how many samples to draw from a sampling function; instead their query operations use a fixed runtime-specified sample size. We exploit hypothesis testing in conditional expressions to dynamically select the appropriate sample size. Sankaranarayanan et al. <ref type="bibr" target="#b26">[27]</ref> provide a static analysis for checking assertions in probabilistic programs. Their estimateProbability assertion calculates the probability of a conditional expression involving random variables being true. While their approach is superficially similar to Uncertain T 's conditional expressions, estimateProbability handles only simple random variables and linear arithmetic operations so that it can operate without sampling. Uncertain T addresses arbitrary random variables and operations, and uses hypothesis testing to limit the performance impact of sampling in a principled way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain-Specific Approaches</head><p>In robotics, CES <ref type="bibr" target="#b29">[30]</ref> extends C++ to include probabilistic variables (e.g., prob&lt;int&gt;) for simple distributions. Instances of prob&lt;T&gt; store a list of pairs (x, p(x)) that map each possible value of the variable to its probability. This representation restricts CES to simple discrete distributions. Uncertain T adopts CES's idea of encapsulating random variables in a generic type, but uses a more robust representation for distributions and adds an accessible semantics for conditional expressions.</p><p>In databases, Barbara et al. incorporate uncertainty and estimation into the semantics of relational operators <ref type="bibr" target="#b0">[1]</ref>. Benjelloun et al. trace provenance back through probabilistic queries <ref type="bibr" target="#b1">[2]</ref>, and Dalvi and Suciu describe a "possible worlds" semantics for relational joins <ref type="bibr" target="#b10">[11]</ref>. Hazy <ref type="bibr" target="#b17">[18]</ref> asks developers to write Markov logic networks (sets of logic rules and confidences) to interact with databases while managing uncertainty. These probabilistic databases must build a model of the entire joint posterior distribution of a query. In contrast, Uncertain T reasons only about conditional distributions.</p><p>Interval analysis represents an uncertain value as a simple interval and propagates it through computations <ref type="bibr" target="#b18">[19]</ref>. For example, if X = <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6]</ref>, then X/2 = [2, 3]. Interval analysis is particularly suited to bounding floating point error in scientific computation. The advantage of interval analysis is its simplicity and efficiency, since many operations over real numbers are trivial to define over intervals. The downside is that intervals treat all random variables as having uniform distributions, an assumption far too limiting for many applications. By using sampling functions, Uncertain T achieves the simplicity of interval analysis when defining operations, but is more broadly applicable.</p><p>In approximate computing, EnerJ <ref type="bibr" target="#b25">[26]</ref> uses the type system to separate exact from approximate data and govern how they interact. This type system encourages developers to reason about approximate data in their program. Uncertain T builds on this idea with a semantics for the quantitative impact of uncertainty on data. Rely is a programming language that statically reasons about the quantitative error of a program executing on unreliable hardware <ref type="bibr" target="#b8">[9]</ref>. Developers use Rely to express assertions about the accuracy of their code's output, and the Rely analyzer statically verifies whether these assertions could be breached based on a specification of unreliable hardware failures. In contrast to Uncertain T , Rely does not address applications that compute with random variables and does not reason dynamically about the error in a particular instance of uncertain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Emerging applications solve increasingly ambitious and ambiguous problems on data from tiny smartphone sensors, huge distributed databases, the web, and simulations. Although practitioners in other sciences have principled ways to make decisions under uncertainty, only recently have programming languages begun to assist developers with this task. Because prior solutions are either too simple to aid correctness or too complex for most developers to use, many developers create bugs by ignoring uncertainty completely.</p><p>This paper presents a new abstraction and shows how it helps developers to correctly operate on and reason with uncertain data. We describe its syntax and probabilistic semantics, emphasizing simplicity for non-experts while encouraging developers to consider the effects of random error. Our semantics for conditional expressions helps developers to understand and control false positives and false negatives. We present implementation strategies that use sampling and hypothesis testing to realize our goals efficiently. Compared to probabilistic programming, Uncertain T gains substantial efficiency through goal-specific sampling. Our three case studies show that Uncertain T improves application correctness in practice without burdening developers.</p><p>Future directions for Uncertain T include runtime and compiler optimizations that exploit statistical knowledge; exploring accuracy, efficiency, and expressiveness for more substantial applications; and improving correctness with models of common phenomena, such as physics, calendar, and history in uncertain data libraries.</p><p>Uncertainty is not a vice to abstract away, but many applications and libraries try to avoid its complexity. Uncertain T embraces uncertainty while recognizing that most developers are not statistics experts. Our research experience with Uncertain T suggests that it has the potential to change how developers think about and solve the growing variety of problems that involve uncertainty.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. GPS samples at the same location on two smartphone platforms. Although smaller circles appear more accurate, the WP sample in (a) is actually more accurate.</figDesc><graphic coords="2,319.14,568.78,109.32,131.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Speed computation on GPS data produces absurd walking speeds (59 mph, and 7 mph for 35 s, a running pace).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Probability of issuing a speeding ticket at a 60 mph speed limit. With a true speed of 57 mph and GPS accuracy of 4 m, there is a 32% chance of issuing a ticket.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>nodes (shaded) and one inner node (white) representing the computation c = a + b. Uncertain T evaluates this Bayesian network when it needs the distribution of c, which depends on the distributions of a and b.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. A simple fitness application (GPS-Walking), encouraging users to walk faster than 4 mph, implemented with and without the uncertain type. The type GeoCoordinate is a pair of doubles (latitude and longitude) and so is numeric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Bayesian network for a simple program with independent leaves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Bayesian networks for a simple program with dependent leaves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Domain knowledge as a prior distribution improves the quality of GPS estimates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Bayes' theorem combines evidence from estimation processes (the value e of E) with hypotheses about the true value b of the target variable B. We call Pr[B = b] the prior distribution, our belief about B before observing any evidence, and Pr[B = b|E = e] the posterior distribution, our belief about B after observing evidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. The posterior distribution for GPS is a distribution over the Earth's surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(b), the user's speed is calculated by the line Uncertain&lt;double&gt; Speed = Distance / dt;The lifted division operator constructs a Bayesian network for this computation:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Data from the GPS-Walking application. Developers improve accuracy with priors and remove absurd values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>bool</head><label></label><figDesc>IsAlive = IsCellAlive(me); bool WillBeAlive = IsAlive; Uncertain&lt;double&gt; NumLive = CountLiveNeighbors(me); if (IsAlive &amp;&amp; NumLive &lt; 2) WillBeAlive = false; else if (IsAlive &amp;&amp; 2 &lt;= NumLive &amp;&amp; NumLive &lt;= 3) WillBeAlive = true; else if (IsAlive &amp;&amp; NumLive &gt; 3) WillBeAlive = false; else if (!IsAlive &amp;&amp; NumLive == 3) WillBeAlive = true;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. SensorLife uses Uncertain T to significantly decrease the rate of incorrect decisions compared to a naive version, but requires more samples to make decisions. BayesLife incorporates domain knowledge into SensorLife for even better results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Error distribution for Sobel approximated by neural networks. Parrot experiences a false positive on this test, which Parakeet eliminates by evaluating the evidence that s(p) &gt; 0.1 (the shaded area).</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Steve Blackburn, Adrian Sampson, Darko Stefanovic and Ben Zorn for comments on earlier versions of this work and the anonymous reviewers for their helpful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The management of probabilistic data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="487" to="502" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ULDBs: Databases with uncertainty and lineage</title>
		<author>
			<persName><forename type="first">O</forename><surname>Benjelloun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Very Large Data Bases (VLDB)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="953" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Berlekamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Conway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Guy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winning Ways for Your Mathematical Plays</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A type theory for probability density functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vuduc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Programming Languages (POPL)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="545" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Measure transformer semantics for Bayesian machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borgström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Margetson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Symposium on Programming (ESOP)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="77" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Abstractions and techniques for programming with uncertain data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bornholt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Australian National University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Honors thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A note on the generation of random normal deviates</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E P</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="610" to="611" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Verifying quantitative reliability of programs that execute on unreliable hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Misailovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Object-Oriented Programming Systems, Languages, and Applications (OOPSLA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="33" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Efficiently sampling probabilistic programs via program analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Chaganty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Rajamani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on Artificial Intelligence and Statistics</title>
		<meeting>the 16th international conference on Artificial Intelligence and Statistics<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-04-29">2013. April 29 -May 1, 2013. 2013</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
	<note>JMLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Management of probabilistic data: Foundations and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Database Systems (PODS)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural acceleration for general-purpose approximate programs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="449" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A language and program for complex Bayesian modelling</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Gilks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="177" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A categorical approach to probability theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Giry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Categorical Aspects of Topology and Analysis</title>
		<title level="s">Lecture Notes in Mathematics</title>
		<imprint>
			<date type="published" when="1982">1982</date>
			<biblScope unit="volume">915</biblScope>
			<biblScope unit="page" from="68" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Church: A language for generative models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Mansinghka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference in Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Arithmetic operations on independent random variables: A numerical approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jaroszewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Korzeń</surname></persName>
		</author>
		<idno>1241-A1265</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Group sequential methods with applications to clinical trials</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jennison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Turnbull</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Chapman &amp; Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hazy: Making it Easier to Build and Maintain Big-data Analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Queue</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="30" to="46" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<title level="m">Interval analysis</title>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hidden Markov map matching through noise and sparseness</title>
		<author>
			<persName><forename type="first">P</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krumm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Advances in Geographic Information Systems (GIS)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="336" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Probability, random variables, and stochastic processes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Papoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Pillai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A probabilistic language based on sampling functions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pfenning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Programming Languages (POPL)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">IBAL: a probabilistic rational programming language</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic lambda calculus and monads of probability distributions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pfeffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Programming Languages (POPL)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="154" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">EnerJ: Approximate data types for safe and general low-power computation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dietl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fortuna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gnanapragasam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grossman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Programming Language Design and Implementation (PLDI)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="164" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Static analysis for probabilistic programs: inferring whole program properties from finitely many paths</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chakarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Programming Language Design and Implementation (PLDI)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="447" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Monte Carlo methods for managing interactive state, action and feedback under uncertainty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mankoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on User Interface Software and Technology (UIST)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="235" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Global positioning system: the mathematics of GPS receivers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics Magazine</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards programming tools for robots that integrate probabilistic computation and learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="306" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the Glivenko-Cantelli theorem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Topsøe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift fr Wahrscheinlichkeitstheorie und Verwandte Gebiete</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="239" to="250" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Wald</surname></persName>
		</author>
		<title level="m">Sequential Tests of Statistical Hypotheses. The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="1945">1945</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="117" to="186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
