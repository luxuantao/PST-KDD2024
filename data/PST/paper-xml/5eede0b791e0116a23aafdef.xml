<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-06-22">22 Jun 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabian</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
							<email>fabian@robots.ox.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
							<email>d.e.worrall@uva.nl</email>
						</author>
						<author>
							<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
							<email>volker.fischer@de.bosch.com</email>
						</author>
						<author>
							<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
							<email>m.welling@uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Bosch Center for Artificial Intelligence A2I Lab</orgName>
								<orgName type="institution">Oxford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Amsterdam Machine Learning Lab</orgName>
								<orgName type="institution">Philips Lab University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Bosch Center for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Amsterdam Machine Learning Lab University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-06-22">22 Jun 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2006.10503v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce the SE(3)-Transformer, a variant of the self-attention module for 3D point clouds, which is equivariant under continuous 3D roto-translations. Equivariance is important to ensure stable and predictable performance in the presence of nuisance transformations of the data input. A positive corollary of equivariance is increased weight-tying within the model, leading to fewer trainable parameters and thus decreased sample complexity (i.e. we need less training data). The SE(3)-Transformer leverages the benefits of self-attention to operate on large point clouds with varying number of points, while guaranteeing SE(3)-equivariance for robustness. We evaluate our model on a toy N -body particle simulation dataset, showcasing the robustness of the predictions under rotations of the input. We further achieve competitive performance on two real-world datasets, ScanObjectNN and QM9. In all cases, our model outperforms a strong, non-equivariant attention baseline and an equivariant model without attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Self-attention mechanisms <ref type="bibr" target="#b27">[28]</ref> have enjoyed a sharp rise in popularity in the last few years. Their relative implementational simplicity coupled with high efficacy on a wide range of tasks such as language modeling <ref type="bibr" target="#b27">[28]</ref>, image recognition <ref type="bibr" target="#b15">[16]</ref>, or graph-based problems <ref type="bibr" target="#b28">[29]</ref>, make them an attractive component to use. However, their generality of application means that for specific tasks, knowledge of existing underlying structure is unused. In this paper, we propose the SE(3)-Transformer shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a self-attention mechanism specifically for 3D point cloud data, which adheres to equivariance constraints, improving robustness to nuisance transformations and general performance.</p><p>Point cloud data is ubiquitous across many fields, presenting itself in diverse forms such as 3D object scans <ref type="bibr" target="#b25">[26]</ref>, 3D molecular structures <ref type="bibr" target="#b18">[19]</ref>, or N -body particle simulations <ref type="bibr" target="#b11">[12]</ref>. Finding neural structures which can adapt to the varying number of points in an input, while respecting the irregular sampling of point positions, is challenging. Furthermore, an important property is that these structures should be invariant to global changes in overall input pose; that is, 3D translations and rotations of the input point cloud should not affect the output. In this paper, we find that the explicit imposition of equivariance constraints on the self-attention mechanism addresses these challenges. The SE(3)- For classification, this is followed by an invariant pooling layer and an MLP. B) In each layer, for each node, attention is performed. Here, the red node attends to its neighbours. Attention weights (indicated by line thickness) are invariant w.r.t. rotation of the input.</p><p>Transformer uses the self-attention mechanism as a data-dependent filter particularly suited for sparse, non-voxelised point cloud data, while respecting and leveraging the symmetries of the task at hand.</p><p>Self-attention itself is a pseudo-linear map between sets of points. It can be seen to consist of two components: input-dependent attention weights and an embedding of the input, called a value embedding. In Fig. <ref type="figure" target="#fig_0">1</ref>, we show an example of a molecular graph, where attached to every atom we see a value embedding vector and where the attention weights are represented as edges, with width corresponding to the attention weight magnitude. In the SE(3)-Transformer, we explicitly design the attention weights to be invariant to global pose. Furthermore, we design the value embedding to be equivariant to global pose. Equivariance generalises the translational weight-tying of convolutions. It ensures that transformations of a layer's input manifest as equivalent transformations of the output. SE(3)-equivariance in particular is the generalisation of translational weight-tying in 2D known from conventional convolutions to roto-translations in 3D. This restricts the space of learnable functions to a subspace which adheres to the symmetries of the task and thus reduces the number of learnable parameters. Meanwhile, it provides us with a richer form of invariance, since relative positional information between features in the input is preserved.</p><p>Our contributions are the following:</p><p>• We introduce a novel self-attention mechanism, guaranteeably invariant to global rotations and translations of its input. It is also equivariant to permutations of the input point labels.</p><p>• We show that the SE(3)-Transformer resolves an issue with concurrent SE(3)-equivariant neural networks, which suffer from angularly constrained filters.</p><p>• We introduce a Pytorch implementation of spherical harmonics, which is 10x faster than Scipy on CPU and 100 − 1000× faster on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background And Related Work</head><p>In this section we introduce the relevant background materials on self-attention, graph neural networks, and equivariance. We are concerned with point cloud based machine learning tasks, such as object classification or segmentation. In such a task, we are given a point cloud as input, represented as a collection of n coordinate vectors x i ∈ R 3 with optional per-point features f i ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Attention Mechanism</head><p>The standard attention mechanism <ref type="bibr" target="#b27">[28]</ref> can be thought of as consisting of three terms: a set of query vectors q i ∈ R p for i = 1, ..., m, a set of key vectors k j ∈ R p for j = 1, ..., n, and a set of value vectors v j ∈ R r for j = 1, ..., n, where r and p are the dimensions of the low dimensional embeddings. We commonly interpret the key k j and the value v j as being 'attached' to the same point j. For a given query q i , the attention mechanism can be written as</p><formula xml:id="formula_0">Attn (q i , {k j }, {v j }) = n j=1 α ij v j , α ij = exp(q i k j ) n j =1 exp(q i k j )<label>(1)</label></formula><p>where we used a softmax as a nonlinearity acting on the weights. In general, the number of query vectors does not have to equal the number of input points <ref type="bibr" target="#b13">[14]</ref>. In the case of self-attention the query, key, and value vectors are embeddings of the input features, so</p><formula xml:id="formula_1">q = h Q (f), k = h K (f), v = h V (f),<label>(2)</label></formula><p>where {h Q , h K , h V } are, in the most general case, neural networks <ref type="bibr" target="#b26">[27]</ref>. For us, query q i is associated with a point i in the input, which has a geometric location x i . Thus if we have n points, we have n possible queries. For query q i , we say that node i attends to all other nodes j = i.</p><p>Motivated by a successes across a wide range of tasks in deep learning such as language modeling <ref type="bibr" target="#b27">[28]</ref>, image recognition <ref type="bibr" target="#b15">[16]</ref>, graph-based problems <ref type="bibr" target="#b28">[29]</ref>, and relational reasoning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b6">7]</ref>, a recent stream of work has applied forms of self-attention algorithms to point cloud data <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b13">14]</ref>. One such example is the Set Transformer <ref type="bibr" target="#b13">[14]</ref>. When applied to object classification on ModelNet40 <ref type="bibr" target="#b35">[36]</ref>, the input to the Set Transformer are the cartesian coordinates of the points. Each layer embeds this positional information further while dynamically querying information from other points. The final per-point embeddings are downsampled and used for object classification.</p><p>Permutation equivariance A key property of self-attention is permutation equivariance. Permutations of point labels 1, ..., n lead to permutations of the self-attention output. This guarantees the attention output does not depend arbitrarily on input point ordering. Wagstaff et al. <ref type="bibr" target="#b29">[30]</ref> recently showed that this mechanism can theoretically approximate all permutation equivariant functions. The SE(3)-transformer is a special case of this attention mechanism, inheriting permutation equivariance. However, it limits the space of learnable functions to rotation and translation equivariant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Neural Networks</head><p>Attention scales quadratically with point cloud size, so it is useful to introduce neighbourhoods: instead of each point attending to all other points, it only attends to its nearest neighbours. Sets with neighbourhoods are naturally represented as graphs. Attention has previously been introduced on graphs under the names of intra-, self-, vertex-, or graph-attention <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23]</ref>. These methods were unified by Wang et al. <ref type="bibr" target="#b30">[31]</ref> with the non-local neural network. This has the simple form</p><formula xml:id="formula_2">y i = 1 C({f j ∈ N i }) j∈Ni w(f i , f j )h(f j )<label>(3)</label></formula><p>where w and h are neural networks and C normalises the sum as a function of all features in the neighbourhood N i . This has a similar structure to attention, and indeed we can see it as performing attention per neighbourhood. While non-local modules do not explicitly incorporate edge-features, it is possible to add them, as done in Veličković et al. <ref type="bibr" target="#b28">[29]</ref> and Hoshen <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Equivariance</head><p>Given a set of transformations T g : V → V for g ∈ G, where G is an abstract group, a function φ : V → Y is called equivariant if for every g there exists a transformation S g : Y → Y such that</p><formula xml:id="formula_3">S g [φ(v)] = φ(T g [v]) for all g ∈ G, v ∈ V.<label>(4)</label></formula><p>The indices g can be considered as parameters describing the transformation. Given a pair (T g , S g ), we can solve for the family of equivariant functions φ satisfying Equation <ref type="formula" target="#formula_3">4</ref>. Furthermore, if (T g , S g ) are linear and the map φ is also linear, then a very rich and developed theory already exists for finding φ <ref type="bibr" target="#b4">[5]</ref>. In the equivariance literature, deep networks are built from interleaved linear maps φ and equivariant nonlinearities. In the case of 3D roto-translations it has already been shown that a suitable structure for φ is a tensor field network <ref type="bibr" target="#b24">[25]</ref>, explained below. Note that Romero et al. <ref type="bibr" target="#b20">[21]</ref> recently introduced a 2D roto-translationally equivariant attention module for pixel-based image data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Representations</head><p>In general, the transformations (T g , S g ) are called group representations.</p><p>Formally, a group representation ρ : G → GL(N ) is a map from a group G to the set of N × N invertible matrices GL(N ). Critically ρ is a group homomorphism; that is, it satisfies the following property ρ(g 1 g 2 ) = ρ(g 1 )ρ(g 2 ) for all g 1 , g 2 ∈ G. Specifically for 3D rotations G = SO(3), we have a few interesting properties: 1) its representations are orthogonal matrices, 2) all representations can be decomposed as</p><formula xml:id="formula_4">ρ(g) = Q D (g) Q, (<label>5</label></formula><formula xml:id="formula_5">)</formula><p>where Q is an orthogonal, N × N , change-of-basis matrix <ref type="bibr" target="#b3">[4]</ref>; each D for = 0, 1, 2, ... is a (2 + 1) × (2 + 1) matrix known as a Wigner-D matrix <ref type="foot" target="#foot_0">3</ref> ; and the is the direct sum or concatenation of matrices along the diagonal. The Wigner-D matrices are irreducible representations of SO(3)think of them as the 'smallest' representations possible. Vectors transforming according to D (i.e. we set Q = I, i = ), are called typevectors. Type-0 vectors are invariant under rotations and type-1 vectors rotate according to 3D rotation matrices. Note, type-vectors have length 2 + 1. They can be stacked, forming a feature vector f transforming according to Eq. <ref type="bibr" target="#b4">(5)</ref>.</p><p>Tensor Field Networks Tensor field networks (TFN) <ref type="bibr" target="#b24">[25]</ref> are neural networks, which map point clouds to point clouds under the constraint of SE(3)-equivariance, the group of 3D rotations and translations. For point clouds, the input is a vector field f : R 3 → R d of the form</p><formula xml:id="formula_6">f(x) = N j=1 f j δ(x − x j ), (<label>6</label></formula><formula xml:id="formula_7">)</formula><p>where δ is the Dirac delta function, {x j } are the 3D point coordinates and {f j } are point features, representing such quantities as atomic number or point identity. For equivariance to be satisfied, the features of a TFN transform under Eq. ( <ref type="formula" target="#formula_4">5</ref>), where Q = I. Each f j is a concatenation of vectors of different types, where a subvector of type-is written f j . A TFN layer computes the convolution of a continuous-in-space, learnable weight kernel W k : R 3 → R (2 +1)×(2k+1) from type-k features to type-features. The type-output of the TFN layer at position x i is</p><formula xml:id="formula_8">f out,i = k≥0 W k (x − x i )f k in (x ) dx k→ convolution = k≥0 n j=1 W k (x j − x i )f k in,j , node j → node i message<label>(7)</label></formula><p>We can also include a sum over input channels, but we omit it here. Weiler et al. <ref type="bibr" target="#b32">[33]</ref>, Thomas et al. <ref type="bibr" target="#b24">[25]</ref> and Kondor <ref type="bibr" target="#b12">[13]</ref> showed that the kernel W k lies in the span of an equivariant basis</p><formula xml:id="formula_9">{W k J } k+ J=|k− | .</formula><p>The kernel is a linear combination of these basis kernels, where the J th coefficient is a learnable function ϕ k J : R ≥0 → R of the radius x . Mathematically this is</p><formula xml:id="formula_10">W k (x) = k+ J=|k− | ϕ k J ( x )W k J (x), where W k J (x) = J m=−J Y Jm (x/ x )Q k Jm .<label>(8)</label></formula><p>Each basis kernel W k J : R 3 → R (2 +1)×(2k+1) is formed by taking a linear combination of Clebsch-Gordan matrices Q k Jm of shape (2 + 1) × (2k + 1), where the J, m th linear combination coefficient is the m th dimension of the J th spherical harmonic Y J : R 3 → R 2J+1 . Each basis kernel W k J completely constrains the form of the learned kernel in the angular direction, leaving the only learnable degree of freedom in the radial direction. Note that W k J (0) = 0 only when k = and J = 0, which reduces the kernel to a scalar w multiplied by the identity, W = w I, referred to as self-interaction <ref type="bibr" target="#b24">[25]</ref>. As such we can rewrite the TFN layer as</p><formula xml:id="formula_11">f out,i = w f in,i self-interaction + k≥0 n j =i W k (x j − x i )f k in,j ,<label>(9)</label></formula><p>Eq. ( <ref type="formula" target="#formula_8">7</ref>) and Eq. ( <ref type="formula" target="#formula_11">9</ref>) present the convolution in message-passing form, where messages are aggregated from all nodes and feature types. They are also a form of nonlocal graph operation as in Eq. ( <ref type="formula" target="#formula_2">3</ref>), where the weights are functions on edges and the features {f i } are node features. We will later see how our proposed attention layer unifies aspects of convolutions and graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Here, we present the SE(3)-Transformer. The layer can be broken down into a procedure of steps as shown in Fig. <ref type="figure" target="#fig_2">2</ref>, which we describe in the following section. These are the construction of a graph from a point cloud, the construction of equivariant edge functions on the graph, how to propagate SE(3)-equivariant messages on the graph, and how to aggregate them. We also introduce an alternative for the self-interaction layer, which we call attentive self-interaction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neighbourhoods</head><p>Given a point cloud {(x i , f i )}, we first introduce a collection of neighbourhoods N i ⊆ {1, ..., N }, one centered on each point i. These neighbourhoods are computed either via the nearest-neighbours methods or may already be defined. For instance, molecular structures have neighbourhoods defined by their bonding structure. Neighbourhoods reduce the computational complexity of the attention mechanism from quadratic in the number of points to linear. The introduction of neighbourhoods converts our point cloud into a graph. This step is shown as Step 1 of Fig. <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The SE(3)-Transformer</head><p>The SE(3)-Transformer itself consists of three components. These are 1) edge-wise attention weights α ij , constructed to be SE(3)-invariant on each edge ij, 2) edge-wise SE(3)-equivariant value messages, propagating information between nodes, as found in the TFN convolution of Eq. ( <ref type="formula" target="#formula_8">7</ref>), and 3) a linear/attentive self-interaction layer. Attention is performed on a per-neighbourhood basis as follows:</p><formula xml:id="formula_12">f out,i = W V f in,i 3 self-interaction + k≥0 j∈Ni\i α ij 1 attention W k V (x j − x i )f k in,j 2 value message . (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>These components are visualised in Fig. <ref type="figure" target="#fig_2">2</ref>. If we remove the attention weights then we have a tensor field convolution, and if we instead remove the dependence of W V on (x j − x i ), we have a conventional attention mechanism. Provided that the attention weights α ij are invariant, Eq. ( <ref type="formula" target="#formula_12">10</ref>) is equivariant to SE(3)-transformations. This is because it is just a linear combination of equivariant value messages. Invariant attention weights can be achieved with a dot-product attention structure shown in Eq. ( <ref type="formula" target="#formula_14">11</ref>). This mechanism consists of a normalised inner product between a query vector q i at node i and a set of key vectors {k ij } j∈Ni along each edge ij in the neighbourhood N i where</p><formula xml:id="formula_14">α ij = exp(q i k ij ) j ∈Ni\i exp(q i k ij ) , q i = ≥0 k≥0 W k Q f k in,i , k ij = ≥0 k≥0 W k K (x j − x i )f k in,j .<label>(11)</label></formula><p>is the direct sum, i.e. vector concatenation in this instance. The linear embedding matrices W k Q and W k K (x j − x i ) are of TFN type (c.f. Eq. ( <ref type="formula" target="#formula_10">8</ref>)). The attention weights α ij are invariant for the following reason. If the input features {f in,j } are SO(3)-equivariant, then the query q i and key vectors {k ij } are also SE(3)-equivariant, since the linear embedding matrices are of TFN type. The inner product of SO(3)-equivariant vectors, transforming under the same representation S g is invariant, since if q → S g q and k → S g k, then q S g S g k = q k, because of the orthonormality of representations of SO(3), mentioned in the background section. We follow the common practice from the self-attention literature <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b13">14]</ref>, and chosen a softmax nonlinearity to normalise the attention weights to unity, but in general any nonlinear function could be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aside: Angular Modulation</head><p>The attention weights add extra degrees of freedom to the TFN kernel in the angular direction. This is seen when Eq. ( <ref type="formula" target="#formula_12">10</ref>) is viewed as a convolution with a datadependent kernel α ij W k V (x). In the literature, SO(3) equivariant kernels are decomposed as a sum of products of learnable radial functions ϕ k J ( x ) and non-learnable angular kernels W k J (x/ x ) (c.f. Eq. ( <ref type="formula" target="#formula_10">8</ref>)). The fixed angular dependence of W k J (x/ x ) is a strange artifact of the equivariance condition in noncommutative algebras and while necessary to guarantee equivariance, it is seen as overconstraining the expressiveness of the kernels. Interestingly, the attention weights α ij introduce a means to modulate the angular profile of W k J (x/ x ), while maintaining equivariance. Channels, Self-interaction Layers, and Non-Linearities Analogous to conventional neural networks, the SE(3)-Transformer can straightforwardly be extended to multiple channels per representation degree , so far omitted for brevity. This sets the stage for self-interaction layers. The attention layer (c.f. Fig. <ref type="figure" target="#fig_2">2</ref> and circles 1 and 2 of Eq. ( <ref type="formula" target="#formula_12">10</ref>)) aggregates information over nodes and input representation degrees k. In contrast, the self-interaction layer (c.f. circle 3 of Eq. ( <ref type="formula" target="#formula_12">10</ref>)) exchanges information solely between features of the same degree and within one node-much akin to 1x1 convolutions in CNNs. Self-interaction is an elegant form of learnable skip connection, transporting information from query point i in layer L to query point i in layer L + 1. This is crucial since, in the SE(3)-Transformer, points do not attend to themselves. In our experiments, we use two different types of self-interaction layer: (1) linear and (2) attentive, both of the form</p><formula xml:id="formula_15">f out,i,c = c w i,c c f in,i,c .<label>(12)</label></formula><p>Linear: Following Schütt et al. <ref type="bibr" target="#b21">[22]</ref>, output channels are a learned linear combination of input channels using one set of weights w i,c c = w c c per representation degree, shared across all points.</p><p>As proposed in Thomas et al. <ref type="bibr" target="#b24">[25]</ref>, this is followed by a norm-based non-linearity.</p><p>Attentive: We propose an extension of linear self-interaction, attentive self-interaction, combining self-interaction and nonlinearity. We replace the learned scalar weights w c c with attention weights output from an MLP, shown in Eq. ( <ref type="formula" target="#formula_16">13</ref>) ( means concatenation.). These weights are SE(3)-invariant due to the invariance of inner products of features, transforming under the same representation.</p><formula xml:id="formula_16">w i,c c = MLP   c,c f in,i,c f in,i,c  <label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Node and Edge Features</head><p>Point cloud data often has information attached to points (node-features) and connections between points (edge-features), which we would both like to pass as inputs into the first layer of the network. Node information can directly be incorporated via the tensors f j in Eqs. ( <ref type="formula" target="#formula_6">6</ref>) and <ref type="bibr" target="#b9">(10)</ref>. For incorporating edge information, note that f j is part of multiple neighbourhoods. One can replace f j with f ij in Eq. <ref type="bibr" target="#b9">(10)</ref>. Now, f ij can carry different information depending on which neighbourhood N i we are currently performing attention over. In other words, f ij can carry information both about node j but also about edge ij. Alternatively, if the edge information is scalar, it can be incorporated into the weight matrices W V and W K as an input to the radial network (see step 2 in Fig. <ref type="figure" target="#fig_2">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We test the efficacy of the SE(3)-Transformer on three datasets, each testing different aspects of the model. The N-body problem is an equivariant task: rotation of the input should result in rotated predictions of locations and velocities of the particles. Next, we evaluate on a real-world object Table <ref type="table">1</ref>: Predicting future locations and velocities in an electron-proton simulation.</p><p>Linear DeepSet <ref type="bibr" target="#b39">[40]</ref> Tensor Field <ref type="bibr" target="#b24">[25]</ref> Set Transformer <ref type="bibr" target="#b13">[14]</ref>  classification task. Here, the network is confronted with large point clouds of noisy data with symmetry only around the gravitational axis. Finally, we test the SE(3)-Transformer on a molecular property regression task, which shines light on its ability to incorporate rich graph structures. We compare to publicly available, state-of-the-art results as well as a set of our own baselines. Specifically, we compare to the Set-Transformer <ref type="bibr" target="#b13">[14]</ref>, a non-equivariant attention model, and Tensor Field Networks <ref type="bibr" target="#b24">[25]</ref>, which is similar to SE(3)-Transformer but does not leverage attention.</p><p>Similar to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b33">34]</ref>, we measure the exactness of equivariance by applying uniformly sampled SO(3)transformations to input and output. The distance between the two, averaged over samples, yields the equivariance error. Note that, unlike in Sosnovik et al. <ref type="bibr" target="#b23">[24]</ref>, the error is not squared:</p><formula xml:id="formula_17">∆ EQ = L s Φ(f ) − ΦL s (f ) 2 / L s Φ(f ) 2<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">N-Body Simulations</head><p>In this experiment, we use an adaptation of the dataset from Kipf et al. <ref type="bibr" target="#b11">[12]</ref>. Five particles each carry either a positive or a negative charge and exert repulsive or attractive forces on each other. The input to the network is the position of a particle in a specific time step, its velocity, and its charge. The task of the algorithm is then to predict the relative location and velocity 500 time steps into the future. We deliberately formulated this as a regression problem to avoid the need to predict multiple time steps iteratively. Even though it certainly is an interesting direction for future research to combine equivariant attention with, e.g., an LSTM, our goal here was to test our core contribution and compare it to related models. This task sets itself apart from the other two experiments by not being invariant but equivariant: When the input is rotated or translated, the output changes respectively (see Fig. <ref type="figure">3</ref>).</p><p>We trained an SE(3)-Transformer with 4 equivariant layers, each followed by an attentive selfinteraction layer (details are provided in the Appendix). Table <ref type="table">1</ref> shows quantitative results. Our model outperforms both an attention-based, but not rotation-equivariant approach (Set Transformer) and a equivariant approach which does not levarage attention (Tensor Field). The equivariance error shows that our approach is indeed fully rotation equivariant up to the precision of the computations.   equivariant layers with linear self-interaction followed by max-pooling and an MLP. Interestingly, the task is not fully rotation invariant, in a statistical sense, as the objects are aligned with respect to the gravity axis. This results in a performance loss when deploying a fully SO(3) invariant model (see Fig. <ref type="figure" target="#fig_4">4a</ref>). In other words: when looking at a new object, it helps to know where 'up' is. We create an SO(2) invariant version of our algorithm by additionally feeding the z-component as an type-0 field and the x, y position as an additional type-1 field (see Appendix). We dub this model SE(3)-Transformer +z. This way, the model can 'learn' which symmetries to adhere to by suppressing and promoting different inputs (compare Fig. <ref type="figure" target="#fig_4">4a</ref> and Fig. <ref type="figure" target="#fig_4">4b</ref>). In Table <ref type="table" target="#tab_1">2</ref>, we compare our model to the current state-of-the-art in object classification <ref type="foot" target="#foot_1">4</ref> . Despite the dataset not playing to the strengths of our model (full SE(3)-invariance) and a much lower number of input points, the performance is competitive with models specifically designed for object classification.  <ref type="formula" target="#formula_2">3</ref>), which may explain its success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">QM9</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented an attention-based neural architecture designed specifically for point cloud data. This architecture is guaranteed to be robust to rotations and translations of the input, obviating the need for training time data augmentation and ensuring stability to arbitrary choices of coordinate frame. The use of self-attention allows for anisotropic, data-adaptive filters, while the use of neighbourhoods enables scalability to large point clouds. We have also introduced the interpretation of the attention mechanism as a data-dependent nonlinearity, adding to the list of equivariant nonlinearties which we can use in equivariant networks. Furthermore, we provide pseudocode in the Appendix for a speed up of spherical harmonics computation of up to 3 orders of magnitudes. This speed-up allowed us to train significantly larger versions of both the SE(3)-Transformer and the Tensor Field network <ref type="bibr" target="#b24">[25]</ref> and to apply these models to real-world datasets.</p><p>Our experiments showed that adding attention to a roto-translation-equivariant model consistently led to higher accuracy and increased training stability. Specifically for large neighbourhoods, attention proved essential for model convergence. On the other hand, compared to convential attention, adding the equivariance constraints also increases performance in all of our experiments while at the same time providing a mathematical guarantee for robustness with respect to rotations of the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The main contribution of the paper is a mathematically motivated attention mechanism which can be used for deep learning on point cloud based problems. We do not see a direct potential of negative impact to the society. However, we would like to stress that this type of algorithm is inherently suited for classification and regression problems on molecules. The SE(3)-Transformer therefore lends itself to application in drug research. One concrete application we are currently investigating is to use the algorithm for early-stage suitability classification of molecules for inhibiting the reproductive cycle of the coronavirus. While research of this sort always requires intensive testing in wet labs, computer algorithms can be and are being used to filter out particularly promising compounds from large databases of millions of molecules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Group Theory and Tensor Field Networks</head><p>Groups A group is an abstract mathematical concept. Formally a group (G, •) consists of a set G and a binary composition operator • : G × G → G (typically we just use the symbol G to refer to the group). All groups must adhere to the following 4 axioms</p><formula xml:id="formula_18">• Closure: g • h ∈ G for all g, h ∈ G • Associativity: f • (g • h) = (f • g) • h = f • g • h for all f, g, h ∈ G</formula><p>• Identity: There exists an element e ∈ G such that e • g = g • e = g for all g ∈ G</p><p>• Inverses: For each g ∈ G there exists a g</p><formula xml:id="formula_19">−1 ∈ G such that g −1 • g = g • g −1 = e</formula><p>In practice, we omit writing the binary composition operator •, so would write gh instead of g • h.</p><p>Groups can be finite or infinite, countable or uncountable, compact or non-compact. Note that they are not necessarily commutative; that is, gh = hg in general.</p><p>Actions/Transformations Groups are useful concepts, because they allow us to describe the structure of transformations, also sometimes called actions. A transformation (operator) T g : X → X is an injective map from a space into itself. It is parameterised by an element g of a group G.</p><p>Transformations obey two laws:</p><p>• Closure: T g • T h is a valid transformation for all g, h ∈ G</p><p>• Identity: There exists at least one element e ∈ G such that T e [x] = x for all x ∈ X , where • denotes composition of transformations. For the expression T g [x], we say that T g acts on x. It can also be shown that transformations are associative under composition. To codify the structure of a transformation, we note that due to closure we can always write</p><formula xml:id="formula_20">T g • T h = T gh ,<label>(15)</label></formula><p>If for any x, y ∈ X we can always find a group element g, such that T g [x] = y, then we call X a homogeneous space. Homogeneous spaces are important concepts, because to each pair of points x, y we can always associate at least one group element.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Equivariance and Intertwiners</head><p>As written in the main body of the text, equivariance is a property of functions f : X → Y. Just to recap, given a set of transformations T g : X → X for g ∈ G, where G is an abstract group, a function f : X → Y is called equivariant if for every g there exists a transformation S g : Y → Y such that</p><formula xml:id="formula_21">S g [f (x)] = f (T g [x]) for all g ∈ G, x ∈ X . (<label>16</label></formula><formula xml:id="formula_22">)</formula><p>If f is linear and equivariant, then it is called an intertwiner. Two important questions arise: 1) How do we choose S g ? 2) once we have (T g , S g ), how do we solve for f ? To answer these questions, we need to understand what kinds of S g are possible. For this, we review representations.</p><p>Representations A group representation ρ : G → GL(N ) is a map from a group G to the set of N × N invertible matrices GL(N ). Critically ρ is a group homomorphism; that is, it satisfies the following property ρ(g 1 g 2 ) = ρ(g 1 )ρ(g 2 ) for all g 1 , g 2 ∈ G. Representations can be used as transformation operators, acting on N -dimensional vectors x ∈ R N . For instance, for the group of 3D rotations, known as SO(3), we have that 3D rotation matrices, ρ(g) = R g act on (i.e., rotate) 3D vectors, as</p><formula xml:id="formula_23">T g [x] = ρ(g)x = R g x, for all x ∈ X , g ∈ G. (<label>17</label></formula><formula xml:id="formula_24">)</formula><p>However, there are many more representations of SO(3) than just the 3D rotation matrices. Among representations, two representations ρ and ρ of the same dimensionality are said to be equivalent if they can be connected by a similarity transformation</p><formula xml:id="formula_25">ρ (g) = Q −1 ρ(g)Q, for all g ∈ G. (<label>18</label></formula><formula xml:id="formula_26">)</formula><p>We also say that a representation is reducible if is can be written as</p><formula xml:id="formula_27">ρ(g) = Q −1 (ρ 1 (g) ⊕ ρ 2 (g))Q = Q −1 ρ 1 (g) ρ 2 (g) Q, for all g ∈ G. (<label>19</label></formula><formula xml:id="formula_28">)</formula><p>If the representations ρ 1 and ρ 2 are not reducible, then they are called irreducible representations of G, or irreps. In a sense, they are the atoms among representations, out of which all other representations can be constructed. Note that each irrep acts on a separate subspace, mapping vectors from that space back into it. We say that subspace</p><formula xml:id="formula_29">X ∈ X is invariant under irrep ρ , if {ρ (g)x | x ∈ X , g ∈ G} ⊆ X .</formula><p>Representation theory of SO(3) As it turns out, all linear representations of compact groups <ref type="foot" target="#foot_2">5</ref>(such as SO( <ref type="formula" target="#formula_2">3</ref>)) can be decomposed into a direct sum of irreps, as</p><formula xml:id="formula_30">ρ(g) = Q J D J (g) Q, (<label>20</label></formula><formula xml:id="formula_31">)</formula><p>where Q is an orthogonal, N × N , change-of-basis matrix <ref type="bibr" target="#b3">[4]</ref>; and each D J for J = 0, 1, 2, ... is a (2J + 1) × (2J + 1) matrix known as a Wigner-D matrix. The Wigner-D matrices are the irreducible representations of SO <ref type="bibr" target="#b2">(3)</ref>. We also mentioned that vectors transforming according to D J (i.e. we set Q = I), are called type-J vectors. Type-0 vectors are invariant under rotations and type-1 vectors rotate according to 3D rotation matrices. Note, type-J vectors have length 2J + 1. In the previous paragraph we mentioned that irreps act on orthogonal subspaces X 0 , X 1 , .... The orthogonal subspaces corresponding to the Wigner-D matrices are the space of spherical harmonics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Spherical Harmonics</head><p>The spherical harmonics Y J : S 2 → C 2J+1 for J ≥ 0 are squareintegrable complex-valued functions on the sphere S 2 . They have the satisfying property that they are rotated directly by the Wigner-D matrices as</p><formula xml:id="formula_32">Y J (R −1 g x) = D * J (g)Y J (x), x ∈ S 2 , g ∈ G,<label>(21)</label></formula><p>where D J is the J th Wigner-D matrix and D * J is its complex conjugate. They form an orthonormal basis for (the Hilbert space of) square-integrable functions on the sphere L 2 (S 2 ), with inner product given as</p><formula xml:id="formula_33">f, h S 2 = S 2 f (x)h * (x) dx.<label>(22)</label></formula><p>So Y Jm , Y J m S 2 = δ JJ δ mm , where Y Jm is the m th element of Y J . We can express any function in L 2 (S 2 ) as a linear combination of spherical harmonics, where</p><formula xml:id="formula_34">f (x) = J≥0 f J Y J (x), x ∈ S 2 , (<label>23</label></formula><formula xml:id="formula_35">)</formula><p>where each f J is a vector of coefficients of length 2J + 1. And in the opposite direction, we can retrieve the coefficients as</p><formula xml:id="formula_36">f J = S 2 f (x)Y * J (x) dx<label>(24)</label></formula><p>following from the orthonormality of the spherical harmonics. This is in fact a Fourier transform on the sphere and the the vectors f J can be considered Fourier coefficients. Critically, we can represent rotated functions as</p><formula xml:id="formula_37">f (R −1 g x) = J≥0 f J D * J (g)Y J (x), x ∈ S 2 , g ∈ G. (<label>25</label></formula><formula xml:id="formula_38">)</formula><p>The Clebsch-Gordan Decomposition In the main text we introduced the Clebsch-Gordan coefficients. These are used in the construction of the equivariant kernels. They arise in the situation where we have a tensor product of Wigner-D matrices, which as we will see is part of the equivariance constraint on the form of the equivariant kernels. In representation theory a tensor product of representations is also a representation, but since it is not an easy object to work with, we seek to decompose it into a direct sum of irreps, which are easier. This decomposition is of the form of Eq. ( <ref type="formula" target="#formula_30">20</ref>), written</p><formula xml:id="formula_39">D k (g) ⊗ D (g) = Q k   k+ J=|k− | D J (g)   Q k .<label>(26)</label></formula><p>In this specific instance, the change of basis matrices Q k are given the special name of the Clebsch-Gordan coefficients. These can be found in many mathematical physics libraries.</p><p>Tensor Field Layers In Tensor Field Networks <ref type="bibr" target="#b24">[25]</ref> and 3D Steerable CNNs <ref type="bibr" target="#b32">[33]</ref>, the authors solve for the intertwiners between SO(3) equivariant point clouds. Here we run through the derivation again in our own notation.</p><p>We begin with a point cloud f (x) = N j=1 f j δ(x − x j ), where f j is an equivariant point feature. Let's say that f j is a type-k feature, which we write as f k j to remind ourselves of the fact. Now say we perform a convolution * with kernel W k : R 3 → R (2 +1)×(2k+1) , which maps from type-k features to type-features. Then</p><formula xml:id="formula_40">f out,i = [W k * f k in ](x)<label>(27)</label></formula><formula xml:id="formula_41">= R 3 W k (x − x i )f k in (x ) dx (28) = R 3 W k (x − x i ) N j=1 f k in,j δ(x − x j ) dx<label>(29)</label></formula><formula xml:id="formula_42">= N j=1 R 3 W k (x − x i )f k in,j δ(x − x j ) dx change of variables x = x − x j<label>(30)</label></formula><formula xml:id="formula_43">= N j=1 R 3 W k (x + x j − x i )f k in,j δ(x ) dx sifting theorem (31) = N j=1 W k (x j − x i )f k in,j .<label>(32)</label></formula><p>Now let's apply the equivariance condition to this expression, then</p><formula xml:id="formula_44">D (g)f out,i = N j=1 W k (R −1 g (x j − x i ))D k (g)f k in,j<label>(33)</label></formula><formula xml:id="formula_45">=⇒ f out,i = N j=1 D (g) −1 W k (R −1 g (x j − x i ))D k (g)f k in,j<label>(34)</label></formula><p>Now we notice that this expression should also be equal to Eq. <ref type="bibr" target="#b31">(32)</ref>, which is the convolution with an unrotated point cloud. Thus we end up at</p><formula xml:id="formula_46">W k (R −1 g x) = D (g)W k (x)D k (g) −1 ,<label>(35)</label></formula><p>which is sometimes refered to as the kernel constraint. To solve the kernel constraint, we notice that it is a linear equation and that we can rearrange it as</p><formula xml:id="formula_47">vec(W k (R −1 g x)) = (D k (g) ⊗ D (g))vec(W k (x))<label>(36)</label></formula><p>where we used the identity vec(AXB) = (B ⊗ A)vec(X) and the fact that the Wigner-D matrices are orthogonal. Using the Clebsch-Gordan decomposition we rewrite this as</p><formula xml:id="formula_48">vec(W k (R −1 g x)) = Q k   k+ J=|k− | D J (g)   Q k vec(W k (R −1 g x)).<label>(37)</label></formula><p>Lastly, we can left multiply both sides by Q k and denote η k (x) Q k vec(W k (x)), noting the the Clebsch-Gordan matrices are orthogonal. At the same time we</p><formula xml:id="formula_49">η k (R −1 g x) =   k+ J=|k− | D J (g)   η k (x).<label>(38)</label></formula><p>Thus we have that η</p><formula xml:id="formula_50">k J (R −1 g x) the J th subvector of η k (R −1 g x) is subject to the constraint η k J (R −1 g x) = D J (g)η k J (x),<label>(39)</label></formula><p>which is exactly the transformation law for the spherical harmonics from Eq. ( <ref type="formula" target="#formula_32">21</ref>)! Thus one way how W k (x) can be constructed is</p><formula xml:id="formula_51">vec W k (x) = Q k k+ J=|k− | Y J (x).<label>(40)</label></formula><p>B Recipe for Building an Equivariant Weight Matrix</p><p>One of the core operations in the SE(3)-Transformer is multiplying a feature vector f, which transforms according to SO(3), with a matrix W while preserving equivariance:</p><formula xml:id="formula_52">S g [W * f](x) = [W * T g [f]](x),<label>(41)</label></formula><p>where</p><formula xml:id="formula_53">T g [f](x) = ρ in (g)f(R −1 g x) and S g [f](x) = ρ out (g)f(R −1 g x).</formula><p>Here, as in the previous section we showed how such a matrix W could be constructed when mapping between features of type-k and type-, where ρ in (g) is a block diagonal matrix of type-k Wigner-D matrices and similarly ρ in (g) is made of type-Wigner-D matrices. W is dependent on the relative position x and underlies the linear equivariance constraints, but is also has learnable components, which we did not show in the previous section. In this section, we show how such a matrix is constructed in practice.</p><p>Previously we showed that</p><formula xml:id="formula_54">vec W k (x) = Q k k+ J=|k− | Y J (x),<label>(42)</label></formula><p>which is an equivariant mapping between vectors of types k and . In practice, we have multiple input vectors {f k c } c of type-k and multiple output vectors of type-. For simplicity, however, we ignore this and pretend we only have a single input and single output. Note that W k has no learnable components. Note that the kernel constraint only acts in the angular direction, but not in the radial direction, so we can introduce scalar radial functions ϕ k J : R ≥0 → R (one for each J), such that</p><formula xml:id="formula_55">vec W k (x) = Q k k+ J=|k− | ϕ k J ( x )Y J (x),<label>(43)</label></formula><p>There radial functions ϕ k J ( x ) act as an independent, learnable scalar factor for each degree J. The vectorised matrix has dimensionality (2 + 1)(2k + 1). We can unvectorise the above yielding</p><formula xml:id="formula_56">W k (x) = unvec   Q k k+ J=|k− | ϕ k J ( x )Y J (x)   (44) = k+ J=|k− | ϕ k J ( x )unvec Q k J Y J (x)<label>(45)</label></formula><p>where Q k J is a (2 + 1)(2k + 1) × (2J + 1) slice from Q k , corresponding to spherical harmonic Y J . As we showed in the main text, we can also rewrite the unvectorised Clebsch-Gordan-spherical harmonic matrix-vector product as</p><formula xml:id="formula_57">unvec Q k J Y J (x) = J m=−J Q k Jm Y Jm (x).<label>(46)</label></formula><p>In contrast to Weiler et al. <ref type="bibr" target="#b32">[33]</ref>, we do not voxelise space and therefore x will be different for each pair of points in each point cloud. However, the same Y J (x) will be used multiple times in the network and even multiple times in the same layer. Hence, precomputing them at the beginning of each forward pass for the entire network can significantly speed up the computation. The Clebsch-Gordan coefficients do not depend on the relative positions and can therefore be precomputed once and stored on disk. Multiple libraries exist which approximate those coefficients numerically. Transformer to Tensor Field Networks) significantly increased the stability. As a result, whenever we swapped out the attention mechanism for a convolution to retrieve the Tensor Field network baseline, we had to decrease the model size to obtain stable training. However, we would like to stress that all the Tensor Field networks we trained were significantly bigger than in the original paper <ref type="bibr" target="#b24">[25]</ref>, mostly enabled by the faster computation of the spherical harmonics.</p><p>For the ablation study in Fig. <ref type="figure" target="#fig_4">4</ref>, we trained networks with 4 hidden equivariant layers with 5 channels each, and up to representation degree 2. This results in a hidden feature size per point of</p><formula xml:id="formula_58">5 • 2 =0 (2 + 1) = 45<label>(51)</label></formula><p>We used 200 points of the point cloud and neighbourhood size 40. For the Tensor Field network baseline, in order to achieve stable training, we used a smaller model with 3 instead of 5 channels, 100 input points and neighbourhood size 10, but with representation degrees up to 3.</p><p>We used 1 head per attention mechanism yielding one attention weight for each pair of points but across all channels and degrees (for an implementation of multi-head attention, see Appendix D.3).</p><p>For the query embedding, we used the identity matrix. For the key embedding, we used a quadratic equivariant matrix preserving the number of degrees and channels per degree.</p><p>For the quantitative comparison to the start-of-the-art in Table <ref type="table" target="#tab_1">2</ref>, we used 128 input points and neighbourhood size 10 for both the Tensor Field network baseline and the SE(3)-Transformer. We used farthest point sampling with a random starting point to retrieve the 128 points from the overall point cloud. We used degrees up to 3 and 5 channels per degree, which we again had to reduce to 3 channels for the Tensor Field network to obtain stable training. We used a norm based non-linearity for the Tensor Field network (as in <ref type="bibr" target="#b24">[25]</ref>) and no extra non-linearity (beyond the softmax in the self-attention algorithm) for the SE(3) Transformer.</p><p>For all experiments, the final layer of the equivariant encoder maps to 64 channels of degree 0 representations. This yields a 64-dimensional SE(3) invariant representation per point. Next, we pool over the point dimension followed by an MLP with one hidden layer of dimension 64, a ReLU and a 15 dimensional output with a cross entropy loss. We trained for 60000 steps with batch size 10. We used the Adam optimizer <ref type="bibr" target="#b10">[11]</ref> with a start learning of 1e-2 and a reduction of the learning rate by 70% every 15000 steps. Training took up to 2 days on a system with 4 CPU cores, 30 GB of RAM, and an NVIDIA GeForce GTX 1080 Ti GPU.</p><p>The input to the Tensorfield network and the Se(3) Transformer are relative x-y-z positions of each point w.r.t. their neighbours. To guarantee equivariance, these inputs are provided as fields of degree 1.</p><p>For the '+z' versions, however, we deliberately break the SE(3) equivariance by providing additional and relative z-position as two additional scalar fields (i.e. degree 0), as well as relative x-y positions as a degree 1 field (where the z-component is set to 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DeepSet Baseline</head><p>We originally replicated the implementation proposed in <ref type="bibr" target="#b39">[40]</ref> for their object classification experiment on ModelNet40 <ref type="bibr" target="#b35">[36]</ref>. However, most likely due to the relatively small number of objects in the ScanObjectNN dataset, we found that reducing the model size helped the performance significantly. The reported model had 128 units per hidden layer (instead of 256) and no dropout but the same number of layers and type of non-linearity as in <ref type="bibr" target="#b39">[40]</ref>.</p><p>Set Transformer Baseline We used the same architecture as <ref type="bibr" target="#b13">[14]</ref> in their object classification experiment on ModelNet40 <ref type="bibr" target="#b35">[36]</ref> with an ISAB (induced set attention block)-based encoder followed by PMA (pooling by multihead attention) and an MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Relational Inference</head><p>Following Kipf et al. <ref type="bibr" target="#b11">[12]</ref>, we simulated trajectories for 5 charged, interacting particles. Instead of a 2d simulation setup, we considered a 3d setup. Positive and negative charges were drawn as Bernoulli trials (p = 0.5). We used the provided code base https://github.com/ethanfetaya/nri with the following modifications: While we randomly sampled initial positions inside a [−5, 5] 3 box, we removed the bounding-boxes during the simulation. We generated 5k simulation samples for training and 1k for testing. Instead of phrasing it as a time-series task, we posed it as a regression task: The input data is positions and velocities at a random time step as well as the signs of the charges. The labels (which the algorithm is learning to predict) are the positions and velocities 500 simulation time steps into the future.</p><p>Training Details We trained each model for 100,000 steps with batch size 128 using an Adam optimizer <ref type="bibr" target="#b10">[11]</ref>. We used a fixed learning rate throughout training and conducted a separate hyper parameter search for each model to find a suitable learning rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SE(3)-Transformer Architecture</head><p>We trained an SE(3)-Transformer with 4 equivariant layers, where the hidden layers had representation degrees {0, 1, 2, 3} and 3 channels per degree. The input is handled as two type-1 fields (for positions and velocities) and one type-0 field (for charges). The learning rate was set to 3e-3. Each layer included attentive self-interaction.</p><p>We used 1 head per attention mechanism yielding one attention weight for each pair of points but across all channels and degrees (for an implementation of multi-head attention, see Appendix D.3).</p><p>For the query embedding, we used the identity matrix. For the key embedding, we used a quadratic equivariant matrix preserving the number of degrees and channels per degree.</p><p>Baseline Architectures All our baselines fulfill permutation invariance (ordering of input points), but only the Tensor Field network and the linear baseline are SE(3) equivariant. For the Tensor Field Network <ref type="bibr" target="#b24">[25]</ref> baseline, we used the same hyper parameters as for the SE(3) Transformer but with a linear self-interaction and an additional norm-based nonlinearity in each layer as in Thomas et al. <ref type="bibr" target="#b24">[25]</ref>. For the DeepSet <ref type="bibr" target="#b39">[40]</ref> baseline, we used 3 fully connected layers, a pooling layer, and two more fully connected layers with 64 units each. All fully connected layers act pointwise. The pooling layer uses max pooling to aggregate information from all points, but concatenates this with a skip connection for each point. Each hidden layer was followed by a LeakyReLU. The learning rate was set to 1e-3. For the Set Transformer <ref type="bibr" target="#b13">[14]</ref>, we used 4 self-attention blocks with 64 hidden units and 4 heads each. For each point this was then followed by a fully connected layer (64 units), a LeakyReLU and another fully connected layer. The learning rate was set to 3e-4.</p><p>For the linear baseline, we simply propagated the particles linearly according to the simulation hyperparamaters. The linear baseline can be seen as removing the interactions between particles from the prediction. Any performance improvement beyond the linear baseline can therefore be interpreted as an indication for relational reasoning being performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 QM9</head><p>The QM9 regression dataset <ref type="bibr" target="#b18">[19]</ref> is a publicly available chemical property prediction task consisting of 134k small drug-like organic molecules with up to 29 atoms per molecule. There are 5 atomic species (Hydrogen, Carbon, Oxygen, Nitrogen, and Flourine) in a molecular graph connected by chemical bonds of 4 types (single, double, triple, and aromatic bonds). 'Positions' of each atom, measured in ångtröms, are provided. We used the exact same train/validation/test splits as Anderson et al. <ref type="bibr" target="#b0">[1]</ref> of sizes 100k/18k/13k.</p><p>The architecture we used is shown in Table <ref type="table" target="#tab_5">4</ref>. It consists of 7 multihead attention layers interspersed with norm nonlinearities, followed by a TFN layer, max pooling, and two linear layers separated by a ReLU. For each attention layer, shown in Fig. <ref type="figure">6</ref>, we embed the input to half the number of feature channels before applying multiheaded attention <ref type="bibr" target="#b27">[28]</ref>. Multiheaded attention is a variation of attention, where we partition the queries, keys, and values into H attention heads. So if our embeddings have dimensionality (4, 16) (denoting 4 feature types with 16 channels each) and we use H = 8 attention heads, then we partition the embeddings to shape (4, 2). We then combine each of the 8 sets of shape (4, 2) queries, keys, and values individually and then concatenate the results into a single vector of the original shape <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b15">16)</ref>. The keys and queries are edge embeddings, and thus the embedding matrices are of TFN type (c.f. Eq. ( <ref type="formula" target="#formula_10">8</ref>)). For TFN type layers, the radial functions are learnable maps. For these we used neural networks with architecture shown in Table <ref type="table" target="#tab_6">5</ref>.</p><p>For the norm nonlinearities <ref type="bibr" target="#b34">[35]</ref>, we use</p><formula xml:id="formula_59">Norm ReLU(f ) = ReLU(LN f ) • f f , where f = m=− (f m ) 2 , (<label>52</label></formula><formula xml:id="formula_60">)</formula><p>where LN is layer norm <ref type="bibr" target="#b1">[2]</ref> applied across all features within a feature type. For the TFN baseline, we used the exact same architecture but we replaced each of the multiheaded attention blocks with a TFN layer with the same output shape.</p><p>The input to the network is a sparse molecular graph, with edges represented by the molecular bonds.</p><p>The node embeedings are a 6 dimensional vector composed of a 5 dimensional one-hot embedding of the 5 atomic species and a 1 dimension integer node embedding for number of protons per atom. The edges embeddings are a 5 dimensional vector consisting of a 4 dimensional one-hot embedding of bond type and a positive scalar for the Euclidean distance between the two atoms at the ends of the bond. For each regression target, we normalised the values by mean and dividing by the standard deviation of the training set.</p><p>We trained for 50 epochs using Adam <ref type="bibr" target="#b10">[11]</ref> at initial learning rate 1e-3 and a single-cycle cosine rate-decay to learning rate 1e-4. The batch size was 32, but for the TFN baseline we used batch size 16, to fit the model in memory. We show results on the 6 regression tasks not requiring thermochemical energy subtraction in Table <ref type="table" target="#tab_2">3</ref>. As is common practice, we optimised architectures and hyperparameters on ε HOMO and retrained each network on the other tasks. Training took about 2.5 days on an NVIDIA GeForce GTX 1080 Ti GPU with 4 CPU cores and 15 GB of RAM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 General Remark</head><p>Across experiments on different datasets with the SE(3)-Transformer, we made the observation that the number of representation degrees have a significant but saturating impact on performance. A big improvement was observed when switching from degrees {0, 1} to {0, 1, 2}. Adding type-3 latent representations gave small improvements, further representation degrees did not change the performance of the model. However, higher representation degrees have a significant impact on memory usage and computation time. We therefore recommend representation degrees up to 2, when computation time and memory usage is a concern, and 3 otherwise. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: A) Each layer of the SE(3)-Transformer maps from a point cloud to a point cloud while guaranteeing euqivariance. For classification, this is followed by an invariant pooling layer and an MLP. B) In each layer, for each node, attention is performed. Here, the red node attends to its neighbours. Attention weights (indicated by line thickness) are invariant w.r.t. rotation of the input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Step 4 : 3 : 2 : 1 :</head><label>4321</label><figDesc>Compute attention and aggregateStep Propagate queries, keys, and values to edgesStep Get SO(3)-equivariant weight matricesStep Get nearest neighbours and relative positions Spherical Harmonics Matrix W consists of blocks mapping between</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Updating the node features using our equivariant attention mechanism in four steps. A more detailed description, especially of step 2, is provided in the Appendix. Steps 3 and 4 visualise a graph network perspective: features are passed from nodes to edges to compute keys, queries and values, which depend both on features and relative positions in a rotation-equivariant manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4. 2 Figure 3 :</head><label>23</label><figDesc>Figure3: A model based on conventional self-attention (left) and our rotation-equivariant version (right) predict future locations and velocities in a 5-body problem. The respective left-hand plots show input locations at time step t = 0, ground truth locations at t = 500, and the respective predictions. The right-hand plots show predicted locations and velocities for rotations of the input in steps of 10 degrees. The dashed curves show the predicted locations of a perfectly equivariant model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: ScanObjectNN: x-axis shows data augmentation on the test set. The x-value corresponds to the maximum rotation around a random axis in the x-y-plane. If both training and test set are not rotated (x = 0 in a), breaking the symmetry of the SE(3)-Transformer by providing the z-component of the coordinates as an additional, scalar input improves the performance significantly. Interestingly, the model learns to ignore the additional, symmetry-breaking input when the training set presents a rotation-invariant problem (strongly overlapping dark red circles and dark purple triangles in b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification accuracy on the 'object only' category of the ScanObjectNN dataset4 . The performance of the SE(3)-Transformer is averaged over 5 runs (standard deviation 0.7%).</figDesc><table><row><cell></cell><cell>D e e p S e t</cell><cell>3 D m F V</cell><cell cols="2">S e t T r a n s f o r m e r P o i n t N e t</cell><cell cols="5">S p i d e r C N N T e n s o r F i e l d + z P o i n t N e t + + S E ( 3 ) -T r a n s f . + z P o i n t C N N</cell><cell>D G C N N</cell><cell>P o i n t G L R</cell></row><row><cell>No. Points</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>128</cell><cell>1024</cell><cell>128</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>Accuracy</cell><cell cols="11">71.4% 73.8% 74.1% 79.2% 79.5% 81.0% 84.3% 85.0% 85.5% 86.2% 87.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>QM9 Mean Absolute Error. Top: Nonequivariant models. Bottom: Equivariant models</figDesc><table><row><cell>TASK</cell><cell>α</cell><cell cols="3">∆ε ε HOMO ε LUMO</cell><cell>µ</cell><cell>C ν</cell></row><row><cell>UNITS</cell><cell cols="2">bohr 3 meV</cell><cell>meV</cell><cell>meV</cell><cell cols="2">D cal/mol K</cell></row><row><cell>WaveScatt [9]</cell><cell>.160</cell><cell>118</cell><cell>85</cell><cell cols="2">76 .340</cell><cell>.049</cell></row><row><cell>NMP [8]</cell><cell>.092</cell><cell>69</cell><cell>43</cell><cell cols="2">38 .030</cell><cell>.040</cell></row><row><cell>SchNet [22]</cell><cell>.235</cell><cell>63</cell><cell>41</cell><cell cols="2">34 .033</cell><cell>.033</cell></row><row><cell>Cormorant [1]</cell><cell>.085</cell><cell>61</cell><cell>34</cell><cell cols="2">38 .038</cell><cell>.026</cell></row><row><cell>LieConv(T3) [6]</cell><cell>.084</cell><cell>49</cell><cell>30</cell><cell cols="2">25 .032</cell><cell>.038</cell></row><row><cell>TFN [25]</cell><cell>.223</cell><cell>58</cell><cell>40</cell><cell cols="2">38 .064</cell><cell>.101</cell></row><row><cell>Us</cell><cell>.148</cell><cell>53</cell><cell>36</cell><cell cols="2">33 .053</cell><cell>.057</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>-of-the-art, we offer competitive performance, especially against Cormorant and TFN, which transform under irreducible representations of SE(3) (like us), unlike LieConv(T3), using a left-regular representation of SE(</figDesc><table /><note>. Lower is better. The table is split into non-equivariant (top) and equivariant models (bottom). Our nearest models are Cormorant and TFN (own implementation). We see that while not state</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Transformer and Tensor Field Network A particularity of object classification from point clouds is the large number of points the algorithm needs to handle. We use up to 200 points out of the available 2024 points per sample and create neighbourhoods with up to 40 nearest neighbours. It is worth pointing out that especially in this setting, adding self-attention (i.e. when comparing the SE<ref type="bibr" target="#b2">(3)</ref> </figDesc><table><row><cell>D Experimental Details</cell></row><row><cell>D.1 ScanObjectNN</cell></row><row><cell>SE(3)-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>QM9 Network architecture: d out is the number of feature types of degrees 0, 1, ..., d out − 1 at the output of the corresponding layer and C is the number of multiplicities/channels per feature type. For the norm nonlinearity we use ReLUs, preceded by equivariant layer norm<ref type="bibr" target="#b32">[33]</ref> with learnable</figDesc><table><row><cell>affine transform.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">NO. REPEATS LAYER TYPE</cell><cell cols="2">d out C</cell></row><row><cell>1x</cell><cell>Input</cell><cell>1</cell><cell>6</cell></row><row><cell>1x</cell><cell cols="2">Attention: 8 heads 4 Norm Nonlinearity 4</cell><cell>16 16</cell></row><row><cell>6x</cell><cell cols="2">Attention: 8 heads 4 Norm Nonlinearity 4</cell><cell>16 16</cell></row><row><cell>1x</cell><cell>TFN layer</cell><cell>1</cell><cell>128</cell></row><row><cell>1x</cell><cell>Max pool</cell><cell>1</cell><cell>128</cell></row><row><cell>1x</cell><cell>Linear</cell><cell>1</cell><cell>128</cell></row><row><cell>1x</cell><cell>ReLU</cell><cell>1</cell><cell>128</cell></row><row><cell>1x</cell><cell>Linear</cell><cell>1</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>QM9 Radial Function Architecture. C is the number of output channels at each layer. Layer norm<ref type="bibr" target="#b1">[2]</ref> is computed per pair of input and output feature types, which have C in and C out channels each.LAYER TYPE C Attention block for the QM9 dataset. Each component is listed with a tuple of numbers representing the output feature types and multiplicities, so (4, 32) means feature types 0, 1, 2, 3 (with dimensionalities 1, 3, 5, 7), with 32 channels per type.</figDesc><table><row><cell>Input</cell><cell>6</cell></row><row><cell>Linear</cell><cell>32</cell></row><row><cell>Layer Norm</cell><cell>32</cell></row><row><cell>ReLU</cell><cell>32</cell></row><row><cell>Linear</cell><cell>32</cell></row><row><cell>Layer Norm</cell><cell>32</cell></row><row><cell>ReLU</cell><cell>32</cell></row><row><cell>Linear</cell><cell>d out  *  C in  *  C out</cell></row><row><cell>Attention Block</cell><cell></cell></row><row><cell cols="2">Input: (d in ,C in )</cell></row><row><cell cols="2">V: (d out ,C/2) K: (d in ,C/2) Q: (d in ,C/2)</cell></row><row><cell cols="2">Attention: (d out ,C/2), H heads</cell></row><row><cell cols="2">Linear Projection: (d out ,C)</cell></row><row><cell>Figure 6:</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">The 'D' stands for Darstellung, German for representation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">PointGLR is a recently published preprint<ref type="bibr" target="#b19">[20]</ref>. The performance of the following models was taken from the official benchmark of the dataset as of June 4th, 2020 (https://hkust-vgd.github.io/benchmark/): 3DmFV<ref type="bibr" target="#b2">[3]</ref>, PointNet<ref type="bibr" target="#b16">[17]</ref>, SpiderCNN<ref type="bibr" target="#b37">[38]</ref>, PointNet++<ref type="bibr" target="#b17">[18]</ref>, DGCN<ref type="bibr" target="#b31">[32]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">Over a field of characteristic zero.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to express our gratitude to the Bosch Center for Artificial Intelligence and Konincklijke Philips N.V. for their support and contribution to open research in publishing our paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3 (x), we compute P 1 3 (x), for which we need P 1 2 (x) and P 1 1 (x). We store each intermediate computation, speeding up average computation time by a factor of ∼ 10 on CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Accelerated Computation of Spherical Harmonics</head><p>We wrote our own spherical harmonics library in Pytorch, which can generate spherical harmonics on the GPU. We found this critical to being able to run the SE(3)-Transformer and Tensor Field network baselines in a reasonable time. This library is accurate to within machine precision against the scipy counterpart scipy.special.sph_harm and is 10x faster on CPU and 100-1000x on GPU. Here we outline our method to generate them.</p><p>The tesseral/real spherical harmonics are given as We make use of the following recursion relations in the computation of the ALPs:</p><p>where the semifactorial is defined as x!! = x(x − 2)(x − 4) • • • , and I is the indicator function. These relations are helpful because they define a recursion.</p><p>To understand how we recurse, we consider an example. Fig. <ref type="figure">5</ref> shows the space of J and m. The black vertices represent a particular ALP, for instance, we have highlighted P −1 3 (x). When m &lt; 0, we can use Eq. (49) to compute P −1 3 (x) from P 1 3 (x). We can then use Eq. (50) to compute P 1 3 (x) from P 1 2 (x) and P 1 1 (x). P 1 2 (x) can also be computed from Eq. (50) and the boundary value P 1 1 (x) can be computed directly using Eq. (48). Crucially, all intermediate ALPs are stored for reuse. Say we wanted to compute P −1 4 (x), then we could use Eq. ( <ref type="formula">49</ref>) to find it from P −1 4 (x), which can be recursed from the stored values P 1 3 (x) and P 1 2 (x), without needing to recurse down to the boundary. We intend to make the code for the spherical harmonics computation publicly available.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cormorant: Covariant molecular neural networks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Truong</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>Hy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three-dimensional point cloud classification in realtime using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Engineering applications of noncommutative harmonic analysis: with emphasis on rotation and motion groups</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">B</forename><surname>Gregory S Chirikjian</surname></persName>
		</author>
		<author>
			<persName><surname>Kyatkin</surname></persName>
		</author>
		<author>
			<persName><surname>Buckingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Mech. Rev</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="B97" to="B98" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<title level="m">Steerable cnns. International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalizing convolutional neural networks for equivariance to lie groups on arbitrary continuous data</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
				<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End-to-end recurrent multi-object tracking and prediction with relational reasoning</title>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oiwi</forename><forename type="middle">Parker</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
				<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wavelet scattering regression of quantum chemical energies</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Hirn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Poilvert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Model. Simul</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="827" to="863" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vain: Attentional multi-agent predictive modeling</title>
		<author>
			<persName><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural relational inference for interacting systems</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
				<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">N-body networks: a covariant hierarchical neural network architecture for learning atomic potentials</title>
		<author>
			<persName><forename type="first">Risi</forename><surname>Kondor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Set transformer: A framework for attention-based permutation-invariant neural networks</title>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoonho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungtaek</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
				<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing System (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlo</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatole</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lilienfeld</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Attentive group equivariant convolutional networks</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><surname>Hoogendoorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Schnet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela1</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michał</forename><surname>Szmaja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Scale-equivariant steerable networks. International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point clouds</title>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tess</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">M</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lusann</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv Preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
		<author>
			<persName><forename type="first">Mikaela</forename><surname>Angelina Uy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quang-Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binh-Son</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duc</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai-Kit</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Edward</forename><surname>Wagstaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Osborne</surname></persName>
		</author>
		<title level="m">On the limitations of representing functions on sets. International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">3d steerable cnns: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wouter</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName><surname>Brostow</surname></persName>
		</author>
		<title level="m">Harmonic networks: Deep translation and rotation equivariance</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attentional shapecontextnet for point cloud recognition</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling point clouds with self-attention and gumbel subset sampling</title>
		<author>
			<persName><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbhakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
