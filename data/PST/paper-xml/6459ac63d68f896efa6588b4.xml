<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-09">9 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zeqi</forename><surname>Tan</surname></persName>
							<email>zqtan@zju.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shen</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zixia</forename><surname>Jia</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">ShanghaiTech University Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiong</forename><surname>Cai</surname></persName>
							<email>caijiong@shanghaitech.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">ShanghaiTech University Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yinghui</forename><surname>Li</surname></persName>
							<email>liyinghu20@mails.tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu</email>
							<affiliation key="aff0">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Kewei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Information Science and Technology</orgName>
								<orgName type="department" key="dep2">ShanghaiTech University Tsinghua Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
							<email>yongjiang.jy@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Damo</forename><surname>Academy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alibaba</forename><surname>Group</surname></persName>
						</author>
						<title level="a" type="main">DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-09">9 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.03688v2[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The MultiCoNER II shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER I task. To cope with these problems, the previous top systems in the MultiCoNER I either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team DAMO-NLP proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entitycentric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextual scope of the model. Also, we explore various search strategies and refine the quality of retrieval knowledge. Our system 1 wins 9 out of 13 tracks in the MultiCoNER II shared task. Additionally, we compared our system with ChatGPT, one of the large language models which have unlocked strong capabilities on many tasks. The results show that there is still much room for improvement for ChatGPT on the extraction task. * : project lead. ? : equal contributions. This work was done during Zeqi Tan, Zixia Jia, Jiong Cai, and Yinghui Li's internship at DAMO Academy, Alibaba Group. 1 We will release the dataset, code, and scripts of our system at https://github.com/modelscope/ AdaSeq/tree/master/examples/U-RaNER.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The MultiCoNER series shared task <ref type="bibr">(Malmasi et al., 2022b;</ref><ref type="bibr">Fetahu et al., 2023b)</ref> aims to iden-Figure <ref type="figure">1</ref>: An example of wrong prediction in RaNER <ref type="bibr">(Wang et al., 2022b)</ref>(one of the top systems in the Mul-tiCoNER I task <ref type="bibr">(Malmasi et al., 2022b)</ref>) . This case illustrates that the knowledge covered is not sufficient for fine-grained complex NER. tify complex named entities (NE), such as titles of creative works, which do not possess the traditional characteristics of named entities, such as persons, locations, etc. It is challenging to identify these ambiguous complex entities based on short contexts <ref type="bibr" target="#b1">(Ashwini and Choi, 2014;</ref><ref type="bibr" target="#b22">Meng et al., 2021;</ref><ref type="bibr" target="#b11">Fetahu et al., 2022)</ref>. The MultiCoNER I task <ref type="bibr">(Malmasi et al., 2022b)</ref> focuses on the problem of semantic ambiguity and low context in multilingual named entity recognition (NER). In addition, the MultiCoNER II task <ref type="bibr">(Fetahu et al., 2023b)</ref> this year poses two major new challenges: (1) a finegrained entity taxonomy with 6 coarse-grained categories (Location, Creative Work, Group, Person, Product and Medical) and 33 finegrained categories, and (2) simulated errors added to the test set to make the task more realistic and difficult, like the presence of spelling mistakes.</p><p>The previous top systems <ref type="bibr">(Wang et al., 2022b;</ref><ref type="bibr" target="#b5">Chen et al., 2022)</ref> of the MultiCoNER I task incorporate additional knowledge in pre-trained language models, either a knowledge base or a gazetteer. RaNER <ref type="bibr">(Wang et al., 2022b)</ref> builds a multilingual knowledge base based on Wikipedia and the original input sentences are then aug-mented with retrieved contexts from the knowledge base, allowing the model to access more relevant knowledge. GAIN <ref type="bibr" target="#b5">(Chen et al., 2022)</ref> proposes a gazetteer-adapted integration network with a gazetteer built from Wikidata to improve the performance of language models. Although these systems achieve impressive results, they still have some drawbacks. First, insufficient knowledge is a common problem. As shown in Figure <ref type="figure">1</ref>, the knowledge used in RaNER can help the model to identify Erving Goffman as a person, but cannot further determine the fine-grained category Artist. Second, these methods mostly suffer from the limited context length. <ref type="bibr">Wang et al. (2022b)</ref> discards stitched text that is longer than 512 after tokenizing, which means that plenty of retrieved context is not visible to the model, leading to resource waste. Third, these systems have a single retrieval strategy. <ref type="bibr">Wang et al. (2022b)</ref> acquires knowledge by text retrieval, while <ref type="bibr" target="#b5">Chen et al. (2022)</ref> accesses knowledge by dictionary matching. This single way of knowledge acquisition will result in the underutilization of knowledge.</p><p>To tackle these problems, we propose a unified retrieval-augmented system (U-RaNER) for finegrained multilingual NER. We use both Wikipedia and Wikidata knowledge bases to build our retrieval module so that more diverse knowledge can be considered. As shown in Figure <ref type="figure">1</ref>, if we locate the entry for Erving Goffman in Wikidata, we can make use of fine-grained entity category information to facilitate predictions. Also, we discover that the retrieval context dropped by the model may also contain useful knowledge. Thus, we explore the infusion approach to make more context visible to the model. In addition, we use multiple retrieval strategies to obtain the most relevant knowledge from two knowledge bases, further improving the model performance.</p><p>Our main contributions are as follows:</p><p>1. We propose a unified retrieval-augmented system for fine-grained multilingual NER. Our system incorporates more diverse knowledge bases and significantly improves the system performance compared to baseline systems (Section ? 4, ? 5) 2. We initiated our investigation by identifying the primary bottleneck of the previous topperforming system, which we determined to be insufficient knowledge. Consequently, we focused on exploring both data and model en-hancements to improve system performance.</p><p>(Section ? 3) 3. We employ multiple retrieval strategies to obtain entity information from Wikidata, in order to complement the missing entity knowledge.</p><p>(Section ? 4.1) 4. Additionally, we utilize the infusion approach to provide a more extensive contextual view to the model, thus enabling better utilization of the retrieved context (Section ? 4.2). 5. Extensive experimental analysis demonstrates the effectiveness of diverse knowledge sources and broader contextual scopes for improving model performance. (Section ? 5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Named Entity Recognition (NER) <ref type="bibr" target="#b29">(Sundheim, 1995)</ref> is a fundamental task in Natural Language Processing. Because of the long-term attention and the rapid development of pre-trained language models, various models <ref type="bibr" target="#b0">(Akbik et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019;</ref><ref type="bibr" target="#b40">Yamada et al., 2020;</ref><ref type="bibr" target="#b37">Wang et al., 2020</ref><ref type="bibr">Wang et al., , 2021a) )</ref>   <ref type="bibr" target="#b10">(Fetahu et al., 2021)</ref>. Therefore, in addition to monolingual subsets, MultiCoNER also distinctively contains a multilingual subset and a code-mixed one, which makes it more challenging.</p><p>Note that in the dataset version of SemEval-2023, this challenge and setting do not exist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Progress of MultiCoNER Methods</head><p>With the MultiCoNER dataset as the core, the SemEval-2022 Task 11 attracts 236 participants, and 55 teams successfully submit their system <ref type="bibr">(Malmasi et al., 2022b)</ref>. Among them, there are many successful and excellent works worthy of discussion. DAMO-NLP <ref type="bibr">(Wang et al., 2022b)</ref> proposes a knowledge-based method that gets multilingual knowledge from Wikipedia to provide informative context for the NER model. And they achieve the previous best overall performance on the Multi-CoNER dataset. USTC-NELSLIP <ref type="bibr" target="#b5">(Chen et al., 2022)</ref> proposes a gazetteer-adapted integration network to improve the model performance for recognizing complex entities. QTrade AI <ref type="bibr" target="#b13">(Gan et al., 2022)</ref> designs kinds of data augmentation strategies for the low-resource mixed-code NER task. Previous efforts and studies on the MultiCoNER dataset have shown that external data and beneficial knowledge are essential to improve the performance of NER models on it.</p><p>Retrieval-augmented NLP Methods Retrievalaugmented techniques have proven to be highly effective in various natural language processing (NLP) tasks, as evidenced by the exceptional performance achieved in prior studies <ref type="bibr" target="#b17">(Lewis et al., 2020;</ref><ref type="bibr" target="#b15">Khandelwal et al., 2019;</ref><ref type="bibr" target="#b2">Borgeaud et al., 2022)</ref>. These approaches usually contain two parts: an information retrieval module and a task-specific module. Specifically, in the context of named entity recognition (NER), <ref type="bibr">Wang et al. (2021b)</ref> proposes leveraging off-the-shelf search engines like Google to retrieve external information and enhance the contextual representations of tokens in the input text, resulting in improved performance. Furthermore, subsequent research has focused on developing task-specific retrieval systems for domainspecific NER and multi-modal NER tasks, respectively <ref type="bibr">(Zhang et al., 2022c;</ref><ref type="bibr">Wang et al., 2022a)</ref>.</p><p>Drawing upon these insights, our proposed system is designed and optimized with guidance from these previous works.</p><p>Large Language Models On IE Recent advances in NLP scale the parametric number of language models to hundreds of billions and have achieved phenomenal performance, such as GPT3 <ref type="bibr">(Brown et al., 2020)</ref>, OPT-175B <ref type="bibr">(Zhang et al., 2022a)</ref>, <ref type="bibr">Flan-PaLM (Chung et al., 2022)</ref>, LLaMA <ref type="bibr" target="#b31">(Touvron et al., 2023)</ref> and ChatGPT<ref type="foot" target="#foot_0">2</ref> . In the field of information extraction (IE), ChatIE <ref type="bibr" target="#b39">(Wei et al., 2023)</ref> first uses ChatGPT for extraction, and the results show that there is still much room for improvement. Recent work experiments with instruction fine-tuning <ref type="bibr">(Wang et al., 2023b)</ref> and simplifying training objectives <ref type="bibr">(Wang et al., 2023a)</ref> to adapt large language models to extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data</head><p>The MultiCoNER II corpus <ref type="bibr">(Fetahu et al., 2023a)</ref> aims to recognize the complex named entities and pose new challenges for current NER systems. To meet these challenges, we first reproduce the results of the top system <ref type="bibr">(Wang et al., 2022b)</ref> and perform error analysis on validation sets. We observe that the performance bottleneck of the system lies in the lack of knowledge. Then, we investigate to break this bottleneck from data and model perspectives and improve model robustness.</p><p>Following <ref type="bibr">Wang et al. (2022b)</ref>, we build a multilingual KB based on Wikipedia of the 12 languages to search for the related documents. We download the latest (2022.10.21) version of the Wikipedia dump from Wikimedia<ref type="foot" target="#foot_1">3</ref> and convert it to plain texts. We execute the official system on MultiCoNER II corpus and categorize the results according to whether the annotated entity appears in the retrieval context or not. As shown in  While <ref type="bibr" target="#b5">Chen et al. (2022)</ref> uses Wikidata to build their gazetteer, we explore to enhance our retrieval system with Wikidata. Wikidata is a free and entitycentric knowledge base. Every entity of Wikidata has a page consisting of a label, several aliases, descriptions, and one or more entity types. As shown in Figure <ref type="figure" target="#fig_0">2</ref>, Base indicates that only the Wikipedia knowledge base is used, and More Database indicates that we use both Wikipedia and Wikidata knowledge bases. The entity coverage improves on all 4 languages and achieves the maximum gain of 12.6% on ZH. In addition, as More Context shows, expanding the length of the retrieval context also brings more entity knowledge. Thus, we use the infusion approach to make more retrieval context visible to model. More details are described in Section ? 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>Overview As depicted in Fig. <ref type="figure" target="#fig_2">3</ref>, U-RaNER is comprised of two parts: a retrieval augmentation module and a NER module. The retrieval augmentation module utilizes multiple retrieval strategies and the NER module adopts a modified transformer structure to utilize the retrieved knowledge. Given an input sentence, U-RaNER retrieves similar texts and entities as external knowledge, which are then utilized in the form of text and vectors to help the NER module obtain improved predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Retrieval Augmentation Module</head><p>In the retrieval augmentation module, we design three different retrieval strategies, namely TEXT2TEXT, TEXT2ENT, and ENT2ENT, which aim to obtain a variety of useful information from different sources to enhance our NER model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TEXT2TEXT</head><p>The TEXT2TEXT retrieval strategy is to obtain texts related to input sentences from Wikipedia by the way of sparse retrieval <ref type="bibr">(Mc-Donell, 1977;</ref><ref type="bibr" target="#b25">Robertson and Zaragoza, 2009)</ref>. Through this form of retrieval, the goal is to obtain additional and useful relevant information as much as possible to alleviate the low-context problem of MultiCoNER. Specifically, we first parse the latest Wikipedia dumps and use ElasticSearch<ref type="foot" target="#foot_2">4</ref> to index them. And finally, we use each sentence in the dataset as the query and use the BM25 retrieval algorithm that comes with ElasticSearch to search in the built index database to obtain the Top-K documents related to the input sentence from Wikipedia, as shown in the first example of Table <ref type="table" target="#tab_3">2</ref>. Note that the TEXT2TEXT strategy is used by <ref type="bibr">Wang et al. (2022b)</ref> to win 10 out of 13 tracks when competing in the SemEval-2022 Task 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TEXT2ENT</head><p>The TEXT2ENT retrieval strategy aims to retrieve candidate entities that may be mentioned in input sentences, as illustrated in the second example of Table <ref type="table" target="#tab_3">2</ref>. We believe that if the candidate entities that may be mentioned in the sentence can be retrieved in advance, the related knowledge might be helpful to build a stronger entity recognition model. The TEXT2ENT strategy is inspired by the related technologies of dictionary disambiguation <ref type="bibr" target="#b14">(Harige and Buitelaar, 2016)</ref> and entity linking <ref type="bibr" target="#b4">(Cao et al., 2021)</ref>. But dictionary disambiguation can only perform hard matching, and there is no detailed annotation information for entity linking (that is, the corresponding information between span and entity), so these two traditional   methods cannot be directly applied to our scene. Therefore, in this part of the specific practice, we tried two different retrieval methods, namely sparse retrieval and dense retrieval. The details of these two retrieval methods are in the Appendix A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ENT2ENT</head><p>The ENT2ENT retrieval strategy aims to retrieve some entities and their corresponding information from Wikidata. Wikidata integrates billions of structural information between millions of entities, such as the alias of entities and the relationships of entity pairs. And intuitively, such information is beneficial to our NER model.</p><p>In the process of ENT2ENT retrieval, we want to find out external entity types which maybe inspire the entity labeling of the input sentence. Concretely, for each given entity, we first retrieve Wikidata to get its relevant Wikidata entities. Next, we gather and utilize the properties of the Wikidata entities from their corresponding Wikidata pages. In particular, we take the "instance of" and "sub-class of" properties as the entity types. For example, as shown in Table <ref type="table" target="#tab_3">2</ref>, with entity "deal hudson" as the query, ENT2ENT strategy will retrieve its type (i.e., "human") and description text. Finally, all relevant Wikidata entities and their types are as the retrieved augmented data. The detailed procedure for ENT2ENT is in the Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Entity Recognition Module</head><p>BERT-CRF We use xlm-roberta-large (XLM-R) <ref type="bibr" target="#b7">(Conneau et al., 2020)</ref> as the PLMs for all the tracks. Given an input sentence x = x 1 , x 2 , . . . , x n , transformer-based standard finetuning for NER first feeds the input sentence x into the PLMs to get the token representations h. The token representations h are fed into a CRF layer to get the conditional probability p ? (y | h), and the model is trained by maximizing the conditional probability and minimizing the cross entropy loss: L = -log p ? (y | h).</p><p>RaNER Given the retrieval context x, we define a neural network parameterized by ? that learns from a concatenated input [x; x]. We feed the input and retrieve the representation [h; h]:</p><formula xml:id="formula_0">[h; h] = [h (1) , . . . h (n) , h(1) , . . . h(n) ] = embed([x; x]) (1)</formula><p>We then feed h into the CRF layer and train by minimizing the conditional probability p ? (y | h) as mentioned above.</p><p>U-RaNER To exploit more retrieval contexts, we first slice x by model-limited input length as x = x0 , x1 , . . . , xm . Then, we keep x0 as the text for concatenation, and feed the rest context list into PLM as [(x; x1 ), . . . , (x; xm )], which is used in <ref type="bibr" target="#b17">Lewis et al. (2020)</ref> for better information interaction, and get the token vector list [(h 1 ; h1 ), . . . , (h m ; hm )]. Afterwards, we consider two infusion (Pre-Infusion and Post-Infusion) approaches using the representation [ h1 , . . . , hm ] and [h 1 , . . . , h m ], respectively.</p><p>For Pre-Infusion, we fetch the token vectors of the corresponding positions of the anchors from the vector list [ h1 , . . . , hm ]. Then, we perform the mean operation to obtain the set of anchor vectors V ? R p?d , p is the number of anchors, and d is the hidden size. Considering that the word embedding layer in XLM-R has two input modes, including vocabulary index input as well as word embedding input, we first perform the former for [x; x0 ] to obtain the input text embedding E, and later concatenate E and the anchor vectors V to form the word embedding input. Finally, we get the representation [h; h0 ; hv ]. We only use h to pass the CRF layer.</p><p>For Post-Infusion, we first feed [x; x0 ] to XLM-R and get the token representation [h; h0 ]. For input representation list [h; h 1 , . . . , h m ], we perform the max operation on the token dimension to obtain the final representation h max . Then, we use h max for calculation as in BERT-CRF. Notably, we find that the post-infusion method is superior to the pre-infusion method in our preliminary experiments, and the default infusion method in the experimental section is post-infusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ensemble Module</head><p>Given predictions { ?? 1 , ? ? ? , ??m } from m models with different random seeds, we use majority voting to generate the final prediction ?. Following <ref type="bibr" target="#b40">Yamada et al. (2020)</ref>; <ref type="bibr">Wang et al. (2022b)</ref>, the module ranks all spans in the predictions by the number of votes in descending order and selects the spans with more than 50% votes into the final prediction. The spans with more votes are kept if the selected spans have overlaps and the longer spans are kept if the spans have the same votes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>We use the official MultiCoNER II dataset <ref type="bibr">(Fetahu et al., 2023a)</ref> in all tracks to train our models. The detailed data statistics is in the Appendix A.1 and A.3. The results on the leaderboard are evaluated with the entity-level macro F1 scores, which treat all the labels equally<ref type="foot" target="#foot_3">5</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Baselines</head><p>In this paper, we compare the proposed U-RaNER with the following baseline models:</p><p>? BERT-CRF, as introduced in 4. Table <ref type="table">3</ref>: Part of the official results on the leaderboard. BERT-CRF is the post-evaluation results of our baseline system (BERT-CRF) on the released test set.</p><p>tasks. We use xlm-roberta-large (XLM-R) <ref type="bibr" target="#b7">(Conneau et al., 2020)</ref> as the pretrained backbone for all the tracks.</p><p>? RaNER, as introduced in 4.2, improves BERT-CRF by incorporating retrieval contexts as input for better performance. Retrieval augmented methods have proven to be highly effective in the NER task <ref type="bibr">(Wang et al., 2021b;</ref><ref type="bibr">Zhang et al., 2022c;</ref><ref type="bibr">Wang et al., 2022a)</ref>.</p><p>? RaNER-MSF <ref type="bibr">(Wang et al., 2022b)</ref> achieves the previous best overall performance on the Multi-CoNER I dataset, which exploits multistage fine-tuning to leverage the annotations from all tracks and thus improve performance and accelerate training of RaNER.</p><p>? ChatGPT, also known as gpt-3.5-turbo, is the most capable GPT-3.5 <ref type="bibr">(Ouyang et</ref>  6 Results and Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main Results</head><p>There are 45 teams that participated in the Multi-CoNER II shared task. Due to limited space, we only compare our system with the systems from teams NLPeople, USTC-NELSLIP, IXA/Cogcomp, CAIR-NLP, PAI and NetEase.AI<ref type="foot" target="#foot_4">6</ref> . As NetEase.AI solely took part in the Chinese track, which means we only have access to their results for this specific track. In the post-evaluation phase, we evaluate the baseline system without the use of additional knowledge bases to further show the effectiveness of our retrieval-augmented system. The official results and the results of our baseline system are shown in Table <ref type="table">3</ref>. Our system performs the best on 9 out of 13 tracks with the average result exceeding the second-place system by the absolute F1-measure of 7.0%. Moreover, our system outperforms our baseline by the 19.18% F1-measure on average, which demonstrates that the retrievalaugmented system based on multiple knowledge bases is extremely helpful in identifying complex entities, leading to significant improvement on model performance.</p><p>In addition, we use three prompting strategies to evaluate ChatGPT. Due to the overwhelming number of test sets (millions of levels), the expense of invoking the OpenAI interface is unaffordable. We experiment on the validation set and the results are in Table <ref type="table" target="#tab_6">4</ref>. We observe that ChatGPT's performance on the multilingual NER dataset is quite poor, with an average F1-score of only 14.78% by the best strategy. Even on the coarse-grained level the result is merely 29.70% (Table <ref type="table" target="#tab_8">5</ref>), which is comparable to the result measured on MultiCoNER I <ref type="bibr">(Malmasi et al., 2022b)</ref> by <ref type="bibr" target="#b16">Lai et al. (2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>In this section, we perform extensive ablation experiments to show the effectiveness of various settings in our retrieval-augmented system. Following <ref type="bibr">Wang et al. (2022b)</ref> For the different knowledge sources, the use of Wikipedia data achieves the gain of 12.61% (RaNER-MSF vs. BERT-CRF), the use of wikidata data achieves the gain of 13.16% (ENT2ENT vs. BERT-CRF), and using both together achieves the maximum gain of 15.46% (ENT2ENT vs. BERT-CRF). This shows that knowledge is highly useful for system performance and illustrates the complementarity of the two knowledge bases.</p><p>For the different knowledge acquisition methods, the ENT2ENT approach is superior to the TEXT2ENT approach (90.47% vs. 86.22%). In addition, we use the infusion approach to further improve the model performance (RaNER-MSF vs. TEXT2TEXT), which suggests that guaranteeing knowledge to be visible to model is also important. The default infusion method in our experiments is post-infusion. We also analyze the impact of the two different infusion methods on performance in the Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Coarse-and-fine Category Analysis</head><p>To illustrate the advantages of U-RaNER on finegrained NER, we transform the model predictions to the coarse-grained level according to the official topology of fine-grained categories. We use the models of RaNER-MSF and U-RaNER w/ ENT2ENT in Table <ref type="table" target="#tab_6">4</ref>   in the Table <ref type="table" target="#tab_8">5</ref>, the improvements in coarse-grained metrics are significantly lower than those of finegrained metrics, differing by 1.38% (3.91% on the ZH track). It suggests that the proposed U-RaNER is better at coping with complex scenarios of finegrained classification. Besides, the average F1 for ChatGPT at different granularity is significant distinct (29.70% vs. 14.43%), which shows the difficulty in identifying fine-grained complex entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Query Relevance</head><p>We define a relevance metric to compute the relevance between the query and retrieval result. The metric calculates the Intersection-over-Union (IoU) between the characters 7 of the query and those of the retrieved result. We plot the results on the training set of 6 tracks in Figure <ref type="figure">4</ref>. It can be observed that the IoU values of TEXT2TEXT strategy form a larger cluster than those of TEXT2ENT and ENT2ENT, which indicates that TEXT2TEXT re- 7 We take repeat characters as different characters. trieval would focus more on the context instead of merely the entities in the query text. Additionally, we observe that the distributions of ENT2ENT have larger medians than those of TEXT2ENT. This might due to ENT2ENT would retrieve more relevant entities from the Wikidata than TEXT2ENT.</p><p>By employing diverse retrieval techniques, we can leverage data with distinct attributes to improve the effectiveness of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Context Length Analysis</head><p>In this section, we focus on analyzing the impact of different context length on model performance. We conduct a series of experiments on EN, ES, PT and MULTI datasets with the context length ranging from 128 to 2048. We can observe from Figure <ref type="figure">5</ref> that the model performance increases as the context length grows. However, when the context list length exceeds 1024, the trend of performance improvement on all four datasets slows down. This indicates that the knowledge capacity in the contexts saturates as the length of the context increases. For better performance, we need to find complementary and highly relevant contextual pieces as additional knowledge sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Error Analysis</head><p>We divided the NER task into two stages: mention detection to locate entity spans, and entity typing to classify the spans with pre-defined labels. To further analyze the limitations of our proposed model, we present the experimental results on 12 languages in Table <ref type="table" target="#tab_9">6</ref>. The experimental results reveal that the average F1 score for mention detection is 97.21, whereas the accuracy for entity typing is 90.35. These results provide evidence that the bottleneck in fine-grained NER is typing. More detailed discussion, including the different retrieval methods and case study, is in the Appendix B.3 and B.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we propose a unified retrievalaugmented system (U-RaNER) for the Multi-CoNER II shared task, which wins 9 out of 13 tracks in the shared task. We expose that the bottleneck of the previous top system is the lack of knowledge. Accordingly, we use both Wikipedia and Wikidata knowledge bases with three retrieval approaches so that more diverse knowledge can be considered. Also, we explore the infusion approach to make more context visible to the model so as to make the best use of the resources. And the error analysis indicates that the entity typing sub-task is the bottleneck in the current system. In the future, we plan to exploit the knowledge in the large language model such as ChatGPT or LLaMA by self-verification or fine-tuning some adapters, in order to achieve robust generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MultiCoNER II Corpus</head><p>The multilingual NER II corpus (MultiCoNER II<ref type="foot" target="#foot_5">8</ref> ) aims to recognize the complex named entities, like the titles of creative works which are not simple nouns, and pose challenges for current NER systems. With the same set of tags, the 12 multilingual datasets specifically include: BN-Bangla, DE-German, EN-English, ES-Spanish, FA-Farsi, FR-French, HI-Hindi, IT-Italian, PT-Portuguese, SV-Swedish, UK-Ukrainian and ZH-Chinese. Table <ref type="table" target="#tab_11">7</ref> shows the detailed dataset statistics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 System Setup</head><p>For fair comparison with prior systems, we use xlm-roberta-large <ref type="bibr" target="#b7">(Conneau et al., 2020)</ref> as our initial checkpoint. We use the AdamW <ref type="bibr" target="#b18">(Loshchilov and Hutter, 2017)</ref> optimizer with a linear warmupdecay learning schedule and a dropout <ref type="bibr" target="#b28">(Srivastava et al., 2014)</ref> of 0.1. We set the batch size and learning rate to 16 and 2e-5, and train models over 4 random seeds. According to the dataset sizes, we train the models for 5 epochs and 20 epochs for multilingual and monolingual models respectively. And all our experiments are conducted on a single NVIDIA A100 80GB GPU. For the ensemble module, we train about 4 models for each track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Fine-grained Taxonomy</head><p>The tagset of MultiCoNER II is a fine-grained tagset including 6 coarse-grained categories and 33 fine-grained categories. The coarse-to-fine mapping of the tags are as follows:</p><p>? Location (LOC): Facility, OtherLOC, Hu-manSettlement, Station; ? Creative Work (CW): VisualWork, Musical-Work, WrittenWork, ArtWork, Software;</p><p>? Group (GRP): MusicalGRP, PublicCORP, Pri-vateCORP, AerospaceManufacturer, Sports-GRP, CarManufacturer, ORG;</p><p>? Person (PER): Scientist, Artist, Athlete, Politician, Cleric, SportsManager, OtherPER;</p><p>? Product (PROD): Clothing, Vehicle, Food, Drink, OtherPROD;</p><p>? Medical (MED): Medication/Vaccine, Med-icalProcedure, AnatomicalStructure, Symptom, Disease.</p><p>The Figure <ref type="figure" target="#fig_5">6</ref> shows the fine-grained taxonomy.</p><p>A.4 Detailed Procedure for TEXT2ENT</p><p>For sparse retrieval, we find the relevant entities from Wikidata which contains millions of entities.</p><p>As in the TEXT2TEXT strategy, we utilize the description and alias information in the Wikidata and index them with ElasticSearch. We use each sentence in the dataset as the query and retrieve the candidate entity with the BM25 algorithm. In order to find candidate entities as much as possible, we apply an iterative retrieval procedure in which we construct a new query by masking the retrieved entities in the query text from the previous retrieval.</p><p>For dense retrieval, we utilize the title information and paragraph information 9 from Wikipedia 9 Considering the memory limit of dense retrieval model training, we truncate the paragraph information in wikipedia, and reserve the first 128 tokens for the construction of the knowledge base.</p><p>to construct the knowledge base for dense entity retrieval, then use the input sentence as the query to retrieve its related Top-K entities in the knowledge base. The dense retrieval model we use is the widely used Bi-Encoder architecture <ref type="bibr">(Zhang et al., 2022b)</ref>. Different from sparse retrieval, the dense retrieval model is trainable to better perceive the semantic characteristics of the MultiCoNER dataset. Therefore, in practice, we first preprocess the train/dev sets of MultiCoNER into the data format for dense retrieval model training. Specifically, because the train/dev sets provide the golden entity annotation of the sentence, we can fuzzy match the span in the sentence with the entity title in our knowledge base to link each span to a specific entity id. Then we use reconstructed training data to train a dense entity retrieval model with reliable performance, which will be finally applied to the test set to obtain candidate entities for the sentences in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Detailed Procedure for ENT2ENT</head><p>Suppose that we have already retrieved the boundaries of possible or relative entities of a sentence, we want to encode more knowledge about these entities to benefit the prediction of target entities and their types. A good choice is leveraging Wikidata which integrates billions of structural information between millions of entities, such as the alias of entities and the relationships of entity pairs. Therefore, we adopt the following steps to acquire ENT2ENT knowledge to augment the data so as to enhance the entity recognition ability of our model.</p><p>1. We preprocess Wikidata to construct two dictionaries of each language in this task. One takes each entity name and each alias string of each entity in Wikidata as keys and the index (called "Qid") of each entity as values.</p><p>The other takes Qid of each entity as keys and two attributes (called "subclass of" and "sub-instance of") content of each entity as values. It is worth mentioning that the values of the two attributes associated with each entity in Wikidata are themselves entities. Therefore, this method is referred to as ENT2ENT retrieval. For the following description, we call the first dictionary String-to-Qid and the second dictionary Qid-to-Types.</p><p>2. For each language, we retrieve argumentation data according to pre-retrieved entities and  the knowledge dictionaries from Step1. Concretely, for each retrieved entity, we first extract the corresponding Qid if it can match one key from the String-to-Qid dictionary. Next, if the first operation succeeds, we leverage the Qid to query the Qid-to-Types dictionary to get the values of "subclass of" and "subinstance of" as types of the retrieved entity. It is possible that the values of some Qid in the Qid-to-Types dictionary of a specific language are NULL. In this situation, we try to get entity types from the Qid-to-Types dictionary of English except for processing English itself.</p><p>3. If we get the language-specific types or English types of some pre-retrieved entities from Step2, we sequentially splice these preretrieved entities and their retrieved types after the original sentence. For those pre-retrieved entities without retrieved types, we only splice the pre-retrieved entities.  <ref type="figure" target="#fig_6">7</ref>, the task description part is used to explain the task and list the entity categories, the note part indicates the annotation scheme and output format, and finally we add the input text. In our experiment, {...} is filled by the content in the Appendix A.3. Multi-turn first performs the task in 6 coarse-grained categories, and later performs finergrained NER. In our experiment, {...} is filled by the response of ChatGPT and the content from in the Appendix A.3.</p><p>Multi-ICL constructs demonstrations spliced after the note part by randomly selecting examples from the training set. xxx is replaced with the selected example. The corresponding prompts can be found in Figure <ref type="figure" target="#fig_7">8</ref>.</p><p>Task Description: You are working as a named entity recognition expert and your task is to label a given text with named entity labels. Your task is to identify and label any named entities present in the text. The named entity labels that you will be using are 33 categories, as shown below {...}. Note: Please use BIO annotation schema to complete this task. Please make sure to label each word of the entity with the appropriate prefix ("B" for the first word of the entity, "I" for any non-initial word of the entity). For words which are not part of any named entity, you should return "O". Your output format should be a list of tuples, where each tuple consists of a word from the input text and its corresponding named entity label. Input: ["from", "1995", "to", "2011", "deal", "hudson", "was", "the", "magazine's", "publisher", "."] Output: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Multi-stage Fine-tuning</head><p>We observe that inconsistent training set sizes on different language tracks will lead to degradation Task Description: You are working as a named entity recognition expert and your task is to label a given text with named entity labels. Your task is to identify and label any named entities present in the text. The named entity labels that you will be using are PER (person), LOC (location), CW (creative work), GRP (group of people), PROD (product), and MED (medical). Note: Please use BIO annotation schema to complete this task. Please make sure to label each word of the entity with the appropriate prefix ("B" for the first word of the entity, "I" for any non-initial word of the entity). For words which are not part of any named entity, you should return "O". Demonstrations: Optional. [Input: xxx, Output: xxx]. Input: ["from", "1995", "to", "2011", "deal", "hudson", "was", "the", "magazine's", "publisher", "."] Output: {...}. Input: Please complete the above task at a finer granularity based on the fine-grained taxonomy below {...}. Output: of model performance from 86.47% to 85.07%. We use increasing batch size and scaling up strategy to address this issue. From the Table <ref type="table" target="#tab_12">8</ref>, increasing batch size from 4 to 128 can improve the model performance from 85.07% to 86.82%. Furthermore, scaling up the training data size on BN, DE, HI and ZH can also result in a gain of +1.09%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Two Infusion Approaches</head><p>In the section ? 4.2, we propose two infusion methods (Pre-Infusion and Post-Infusion) to make more context visible to the model. Here, we make a quantitative comparison of their effects on model performance. As shown in the Table <ref type="table" target="#tab_13">9</ref>, we observe that the post-infusion method is superior to the pre-infusion method in all language track. We attribute this to the fact that the pre-infusion method only considers the anchor information and ignores other contextual information, while the post-infusion method uses more contextual knowledge and achieves better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Different Retrieval Methods</head><p>To deeply analyze the effectiveness of the two TEXT2ENT retrieval strategies we design, we compare their retrieval performance (i.e., Recall@50) and the enhanced NER performance (i.e., F1) based on their respective retrieval results. From Table <ref type="table" target="#tab_16">11</ref>, we find that the retrieval performance of sparse retrieval does not seem to be worse than dense, and its recall is higher than dense retrieval for both PT and SV languages. In addition, for the BN and DE languages, although their recall results of sparse retrieval are lower than those of dense retrieval, their final performance of NER is higher than that of dense retrieval. We think this is mainly due to the different retrieval sources of the two retrieval strategies. Our sparse strategy is retrieved from Wikidata, while the dense strategy is retrieved from Wikipedia. The retrieval quality of Wikipedia is easily disturbed by the existence of entity alias. In addition, because the dense retrieval requires us to train the model, we actually truncate the paragraph information in Wikipedia for model training and retrieval, so the information that can be used for dense retrieval is also limited. However, from the ZH language, we know that the robustness of the dense retrieval strategy for different languages is better than the sparse retrieval strategy. Therefore, when dealing with retrieval in different languages, we can flexibly choose different strategies based on the quality of the retrieval resources in the corresponding language to obtain better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Case Study</head><p>Table <ref type="table" target="#tab_15">10</ref> provides a closer examination of the predicted results of BERT-CRF, RaNER, and U-RaNER respectively. We selected three cases from the English language dev data to analyze in detail.</p><p>In the first case, fine-grained NER necessitates comprehensive information to accurately classify long-tail entity spans. By utilizing knowledge from multiple sources, U-RaNER successfully predicts "pudendal nerve entrapment" in the first case.</p><p>In the second case, RaNER's typical ambiguity problem is evident, where the context retrieved from merely Wikipedia source lacks pertinent information about the target entity "gloucestershire" which could refer to either a county or a sports club.</p><p>However, in the third case, the retrieval-based systems wrongly predict "theles leites" and "jesse taylor" as "Athlete" due to retrieved knowledge indicating that they are both mixed martial arts fighters. This demonstrates that the use of retrieved information can sometimes be misleading and even harmful.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Entity coverage of the retrieval context for the annotated entities within the query sentence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1995 Hudson became publisher of the conservative Roman Catholic magazine, Crisis. 2. Hudson is the former publisher and editor of 3. Hudson also hosts the radio show Church and Culture on is the former publisher and editor of Crisis Magazine and InsideCatholic.com.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall architecture of U-RaNER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Strategy NER Model Training Our final NER models are trained on the combined dataset including both the training and development sets on each track to fully utilize the labeled data. For models trained on the combined dataset, we use the final model checkpoint after training. The detailed system configurations is in the Appendix A.2Multi-stage Fine-tuning Multi-stage finetuning (MSF) aims at transferring the parameters of fine-tuned embeddings in a model at an early stage into other models in the next stage<ref type="bibr" target="#b27">Shi and Lee (2021)</ref>. The approach stores the checkpoint of fine-tuned XLM-R embeddings at the early stage and uses it as the initialization of XLM-R embeddings for model training at the next stage.Wang et al. (2022b)  experimentally demonstrates that MSF can leverage the annotations from all tracks and thus improve performance and accelerate training. In addition, we observe that inconsistent training set sizes on different language tracks can also lead to degradation of model performance. We use increasing batch size and upsampling strategy to address this issue. The details are shown in the Appendix B.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: The distribution of the character-level IoU between query and its retrieval result. Each subplot is the histograms of different retrieval strategies on the corresponding dataset, where the x-axis indicates the IoU values ranging from 0 to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The taxonomy of fine-grained categories on MultiCoNER II from the official webpage.</figDesc><graphic url="image-4.png" coords="13,87.24,70.87,185.52,183.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Input prompt for Single-turn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Input prompt for Multi-turn and Multi-ICL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>The performance and ratio for different types of data on BN, DE and ZH.</figDesc><table><row><cell cols="10">insight, we consider data and model dimensions to</cell></row><row><cell cols="9">compensate for this lack of knowledge.</cell></row><row><cell>73.4</cell><cell>78.0</cell><cell>77.1</cell><cell>69.3</cell><cell>72.3</cell><cell>77.9</cell><cell>81.6</cell><cell>82.4</cell><cell cols="2">Base More Context More Database</cell></row><row><cell>Entity Coverage</cell><cell></cell><cell>64.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>47.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>35.3</cell><cell>37.0</cell></row><row><cell></cell><cell>BN</cell><cell></cell><cell>DE</cell><cell></cell><cell></cell><cell>HI</cell><cell></cell><cell></cell><cell>ZH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note><p>Examples of different retrieval strategies related to the input sentence: "from 1995 to 2011 deal hudson was the magazine's publisher." with its corresponding entity "deal hudson".</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>, we employ the multi-stage The top bar shows ChatGPT's performance (micro-F1 scores) using three prompting strategies, the former two being zero-shot learning and Multi-ICL being few-shot learning. Following the comparison between the top system(Wang et al., 2022b)  in the MultiCoNER I and the three variants of our method on the validation set. indicates that we merely use the Wikidata knowledge base. means we scale the model horizon with the infusion approach. ? and ? indicate the use of the Wikipedia or Wikidata knowledge base.fine-tuning (MSF) training strategy. As shown inTable 4, the model performance improves from 87.95% to 89.92%, which illustrates the effectiveness of the multi-stage training. Note that the following five rows in Table 4 all use the MSF training strategy.</figDesc><table><row><cell>Method</cell><cell>?  ?</cell><cell>BN</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FA</cell><cell>FR</cell><cell>HI</cell><cell>IT</cell><cell>PT</cell><cell>SV</cell><cell>UK</cell><cell>ZH</cell><cell>AVG.</cell></row><row><cell>ChatGPT w/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single-turn</cell><cell></cell><cell>7.24</cell><cell cols="5">10.06 13.36 12.44 10.94 11.05</cell><cell>9.04</cell><cell cols="4">16.32 17.27 18.03 10.88</cell><cell>5.02</cell><cell>11.80</cell></row><row><cell>Multi-turn</cell><cell></cell><cell>8.12</cell><cell cols="5">14.57 15.38 15.52 12.75 13.60</cell><cell>9.17</cell><cell cols="4">17.81 17.70 20.38 14.25</cell><cell>5.60</cell><cell>13.74</cell></row><row><cell>Multi-ICL</cell><cell></cell><cell>9.76</cell><cell cols="10">14.84 17.65 16.28 14.11 13.95 10.48 18.63 18.84 20.94 15.57</cell><cell>6.34</cell><cell>14.78</cell></row><row><cell>BERT-CRF</cell><cell></cell><cell cols="13">86.98 76.08 72.61 75.66 69.37 74.44 85.46 80.70 76.54 78.48 76.30 75.11 77.31</cell></row><row><cell>RaNER</cell><cell></cell><cell cols="13">92.30 84.29 84.32 88.81 87.85 86.77 91.75 91.08 88.45 89.74 88.46 81.55 87.95</cell></row><row><cell>RaNER-MSF</cell><cell></cell><cell cols="13">93.11 86.81 86.82 90.90 89.52 88.99 93.97 92.42 90.75 91.93 90.93 82.83 89.92</cell></row><row><cell>U-RaNER w/</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TEXT2ENT</cell><cell></cell><cell cols="13">89.87 85.83 87.54 88.03 86.44 83.86 86.82 91.19 78.92 86.20 84.26 85.62 86.22</cell></row><row><cell>ENT2ENT</cell><cell></cell><cell cols="13">94.45 88.85 88.11 91.34 89.70 89.96 94.68 91.53 90.15 91.68 88.21 87.02 90.47</cell></row><row><cell>TEXT2TEXT</cell><cell></cell><cell cols="13">94.36 87.79 88.07 92.57 90.91 91.80 94.25 93.60 91.94 93.02 91.40 84.11 91.15</cell></row><row><cell>TEXT2ENT</cell><cell></cell><cell cols="13">94.77 89.48 89.88 93.46 90.80 90.83 94.57 93.83 92.12 93.20 91.12 89.41 91.96</cell></row><row><cell>ENT2ENT</cell><cell></cell><cell cols="13">94.96 90.36 90.62 93.51 91.85 92.88 95.12 94.60 92.90 94.45 91.57 90.38 92.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>for the analysis. As shown</figDesc><table><row><cell></cell><cell>Method</cell><cell>BN</cell><cell>ES</cell><cell>PT</cell><cell>SV</cell><cell>ZH AVG.</cell></row><row><cell></cell><cell cols="6">ChatGPT 21.26 33.86 35.27 40.11 18.01 29.70</cell></row><row><cell>Coarse</cell><cell cols="6">RaNER U-RaNER 97.48 98.30 98.33 98.49 95.55 97.63 95.92 96.17 96.79 97.55 91.94 95.67</cell></row><row><cell></cell><cell>?</cell><cell cols="5">+1.56 +2.13 +1.54 +0.94 +3.61 +1.96</cell></row><row><cell></cell><cell>ChatGPT</cell><cell cols="5">9.76 16.28 18.84 20.94 6.34 14.43</cell></row><row><cell>Fine</cell><cell cols="6">RaNER U-RaNER 94.96 93.51 92.90 94.45 90.38 93.24 93.11 90.90 90.75 91.93 82.83 89.90</cell></row><row><cell></cell><cell>?</cell><cell cols="5">+1.85 +2.61 +2.15 +2.52 +7.55 +3.34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The performance comparison between RaNER and U-RaNER at coarse-and-fine grained categories.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Model performance of mention-detection and entity-typing on the 12 multilingual datasets.</figDesc><table><row><cell cols="4">Language F1-entity F1-mention Acc-typing</cell></row><row><cell>BN</cell><cell>92.30</cell><cell>97.33</cell><cell>94.83</cell></row><row><cell>DE</cell><cell>84.29</cell><cell>95.00</cell><cell>88.73</cell></row><row><cell>EN</cell><cell>84.32</cell><cell>98.15</cell><cell>85.91</cell></row><row><cell>ES</cell><cell>88.81</cell><cell>98.13</cell><cell>90.50</cell></row><row><cell>FA</cell><cell>87.85</cell><cell>97.21</cell><cell>90.37</cell></row><row><cell>FR</cell><cell>86.77</cell><cell>97.34</cell><cell>89.14</cell></row><row><cell>HI</cell><cell>91.75</cell><cell>97.15</cell><cell>94.44</cell></row><row><cell>IT</cell><cell>91.08</cell><cell>98.53</cell><cell>92.44</cell></row><row><cell>PT</cell><cell>88.45</cell><cell>98.45</cell><cell>89.84</cell></row><row><cell>SV</cell><cell>89.74</cell><cell>98.60</cell><cell>91.01</cell></row><row><cell>UK</cell><cell>88.46</cell><cell>98.33</cell><cell>89.96</cell></row><row><cell>ZH</cell><cell>81.55</cell><cell>92.25</cell><cell>87.00</cell></row><row><cell>AVG.</cell><cell>87.84</cell><cell>97.21</cell><cell>90.35</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Dataset statistics on MultiCoNER II.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>The model performance with different training strategies.</figDesc><table><row><cell>Method</cell><cell>BN</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FA</cell><cell>FR</cell><cell>HI</cell><cell>IT</cell><cell>PT</cell><cell>SV</cell><cell>UK</cell><cell>ZH</cell><cell>AVG.</cell></row><row><cell cols="14">RaNER w/ one stage 91.79 82.41 84.32 87.49 85.69 85.48 90.68 89.51 87.46 88.54 87.82 76.45 86.47</cell></row><row><cell>RaNER w/ bs 4</cell><cell cols="13">82.02 80.82 85.60 88.46 85.27 87.53 86.56 89.80 87.26 89.77 89.17 68.59 85.07</cell></row><row><cell>RaNER w/ bs 128</cell><cell cols="13">88.09 83.23 85.87 89.40 85.59 88.18 89.57 91.84 88.97 90.01 88.97 72.11 86.82</cell></row><row><cell>RaNER w/ scale up</cell><cell cols="13">90.82 86.27 85.86 89.88 86.15 88.70 90.99 91.50 89.24 90.85 88.95 75.71 87.91</cell></row><row><cell>Method</cell><cell>BN</cell><cell>DE</cell><cell>EN</cell><cell>ES</cell><cell>FA</cell><cell>FR</cell><cell>HI</cell><cell>IT</cell><cell>PT</cell><cell>SV</cell><cell>UK</cell><cell>ZH</cell><cell>AVG.</cell></row><row><cell>RaNER</cell><cell cols="12">89.81 80.55 79.98 82.99 81.17 81.73 90.57 87.48 83.61 84.43 83.69 77.30</cell><cell>83.61</cell></row><row><cell cols="14">U-RaNER w/ Pre-infusion 91.35 82.80 83.71 86.73 86.63 85.88 91.07 89.08 87.18 89.16 88.69 80.41 86.89</cell></row><row><cell cols="14">U-RaNER w/ Post-infusion 91.82 83.24 84.50 86.85 87.64 87.21 91.23 90.36 87.98 90.47 90.02 81.15 87.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>The model performance with different infusion approaches.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Examples of three NER systems. The entity type HS refers to HumanSettlement.</figDesc><table><row><cell>Sentence</cell><cell>Span</cell><cell>Gold Tag</cell><cell>BERT-CRF</cell><cell>RaNER</cell><cell>U-RaNER</cell></row><row><cell>pudendal nerve entrapment can occur when the ...</cell><cell>pudendal nerve entrapment</cell><cell>Disease</cell><cell>-</cell><cell>Symptom</cell><cell>Disease</cell></row><row><cell>he debuted for gloucestershire in 1887 at the age of ...</cell><cell>gloucestershire</cell><cell>SportsGRP</cell><cell>SportsGRP</cell><cell>HS</cell><cell>SportsGRP</cell></row><row><cell>the main event featured</cell><cell>thales leites</cell><cell>OtherPER</cell><cell>Athlete</cell><cell>Athlete</cell><cell>Athlete</cell></row><row><cell>thales leites taking on jesse taylor</cell><cell>jesse taylor</cell><cell>OtherPER</cell><cell>OtherPER</cell><cell>Athlete</cell><cell>Athlete</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Recall-Sparse 79.32 71.09 98.22 98.20 37.76 76.92 Recall-Dense 93.26 85.18 87.84 89.19 79.80 85.25 Comparison of retrieval performance and impact on NER between the sparse and dense TEXT2ENT strategies on the dev set.</figDesc><table><row><cell>Metric</cell><cell>BN</cell><cell>DE</cell><cell>PT</cell><cell>SV</cell><cell>ZH AVG.</cell></row><row><cell>F1-Baseline</cell><cell cols="5">86.98 85.46 76.54 78.48 75.11 80.51</cell></row><row><cell>F1-Sparse</cell><cell cols="5">89.81 90.57 83.61 84.43 77.30 85.14</cell></row><row><cell>F1-Dense</cell><cell cols="5">88.45 89.83 77.23 80.54 78.00 82.81</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://openai.com/blog/chatgpt/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://dumps.wikimedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/elastic/ elasticsearch</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>In comparison, most of the publicly available NER datasets (e.g.,CoNLL 2002CoNLL  , 2003 datasets)  datasets)  are evaluated with the entity-level micro F1 scores, which emphasize common labels<ref type="bibr" target="#b0">(Akbik et al., 2018;</ref><ref type="bibr" target="#b8">Devlin et al., 2019;</ref><ref type="bibr" target="#b40">Yamada et al., 2020;</ref> Wang et al., 2022b). Except for the results in Table3, the following results are entity-level micro F1 scores if not otherwise specified.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Please refer to https://multiconer.github. io/results for more details about the results.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>https://multiconer.github.io/dataset</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contextual string embeddings for sequence labeling</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Targetable named entity recognition in social media</title>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Ashwini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinho</forename><forename type="middle">D</forename><surname>Choi</surname></persName>
		</author>
		<idno>CoRR, abs/1408.0782</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Bm</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2206" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mc-Candlish</surname></persName>
		</author>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autoregressive entity retrieval</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2021-05-03">2021. May 3-7, 2021</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">USTC-NELSLIP at SemEval-2022 task 11: Gazetteer-adapted integration network for multilingual complex named entity recognition</title>
		<author>
			<persName><forename type="first">Beiduo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.semeval-1.223</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</title>
		<meeting>the 16th International Workshop on Semantic Evaluation (SemEval-2022)<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
	<note>United States. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Scaling instructionfinetuned language models</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayne</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Brahma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Shixiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirac</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Suzgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aakanksha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie</forename><surname>Castro-Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Pellat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dasha</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Valter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Slav</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denny</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multi-CoNER v2: a Large Multilingual dataset for Finegrained and Noisy Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gazetteer Enhanced Named Entity Recognition for Code-Mixed Web Queries</title>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1677" to="1681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamic gazetteer integration in multilingual models for cross-lingual and cross-domain named entity recognition</title>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2777" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Oleg Rokhlenko, and Shervin Malmasi. 2023b. SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)</title>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023)</title>
		<meeting>the 17th International Workshop on Semantic Evaluation (SemEval-2023)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Qtrade AI at SemEval-2022 task 11: An unified framework for multilingual NER task</title>
		<author>
			<persName><forename type="first">Weichao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.semeval-1.228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</title>
		<meeting>the 16th International Workshop on Semantic Evaluation (SemEval-2022)<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1654" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generating a large-scale entity linking dictionary from Wikipedia link structure and article text</title>
		<author>
			<persName><forename type="first">Ravindra</forename><surname>Harige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)</title>
		<meeting>the Tenth International Conference on Language Resources and Evaluation (LREC&apos;16)<address><addrLine>Portoro?, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA)</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2431" to="2434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00172</idno>
		<title level="m">Generalization through memorization: Nearest neighbor language models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning</title>
		<author>
			<persName><forename type="first">Dac</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nghia Trung</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Pouran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Veyseh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9459" to="9474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>CoRR, abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">MultiCoNER: a Large-scale Multilingual dataset for Complex Named Entity Recognition</title>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">2022b. SemEval-2022 Task 11: Multilingual Complex Named Entity Recognition (MultiCoNER)</title>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besnik</forename><surname>Fetahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Semantic Evaluation</title>
		<meeting>the 16th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
	<note>SemEval-2022</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An inverted index implementation</title>
		<author>
			<persName><forename type="first">Ken</forename><forename type="middle">J</forename><surname>Mcdonell</surname></persName>
		</author>
		<idno type="DOI">10.1093/comjnl/20.2.116</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="116" to="123" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GEMNET: Effective gated gazetteer representations for recognizing complex entities in low-context input</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Rokhlenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1499" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fraser</forename><surname>Kelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maddie</forename><surname>Simens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Lowe</surname></persName>
		</author>
		<title level="m">Training language models to follow instructions with human feedback</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Towards robust linguistic analysis using OntoNotes</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Moschitti</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tou</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Bj?rkelund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000019</idno>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="333" to="389" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the conll-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Conference on Natural Language Learning, CoNLL 2002, Held in cooperation with COLING 2002</title>
		<meeting>the 6th Conference on Natural Language Learning, CoNLL 2002, Held in cooperation with COLING 2002<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">TGIF: Tree-graph integrated-format parser for enhanced UD with twostage generic-to individual-language finetuning</title>
		<author>
			<persName><forename type="first">Tianze</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.iwpt-1.23</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</title>
		<meeting>the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Named entity task definition, version 2.1</title>
		<author>
			<persName><forename type="first">Beth</forename><forename type="middle">M</forename><surname>Sundheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Message Understanding Conference</title>
		<meeting>the Sixth Message Understanding Conference</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="319" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timoth?e</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozi?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelien</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<title level="m">Llama: Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">2023a. Gpt-ner: Named entity recognition via large language models</title>
		<author>
			<persName><forename type="first">Shuhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongbin</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Instructuie: Multi-task instruction tuning for unified information extraction</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Zu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianze</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuansen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihua</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunsai</forename><surname>Du</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2022a. Named entity and relation extraction with multi-modal retrieval</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="5925" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2021a. Automated Concatenation of Embeddings for Structured Prediction</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2021b. Improving named entity recognition by external context retrieving and cooperative learning</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1800" to="1812" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">More embeddings, better sequence labelers?</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Zhongqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">2022b. DAMO-NLP at SemEval-2022 task 11: A knowledge-based system for multilingual named entity recognition</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.semeval-1.200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</title>
		<meeting>the 16th International Workshop on Semantic Evaluation (SemEval-2022)<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="1457" to="1468" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Zero-shot information extraction via chatting with chatgpt</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjuan</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Susan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punit</forename><surname>Singh Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Opt: Open pretrained transformer language models</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">2022b. Entqa: Entity linking as question answering</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyue</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stratos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2022">April 25-29, 2022</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">2022c. Domain-specific NER via retrieving correlated samples</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueheng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meishan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="2398" to="2404" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
