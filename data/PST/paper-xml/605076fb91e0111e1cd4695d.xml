<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">S2AND: A Benchmark and Evaluation System for Author Name Disambiguation</title>
				<funder ref="#_eAYpNaj">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-12">12 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shivashankar</forename><surname>Subramanian</surname></persName>
							<email>shivashankars@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Melbourne</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>King</surname></persName>
							<email>daniel@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
							<email>dougd@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Northwestern University</orgName>
								<address>
									<settlement>Evanston</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
							<email>sergey@allenai.org</email>
							<affiliation key="aff0">
								<orgName type="institution">Allen Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">S2AND: A Benchmark and Evaluation System for Author Name Disambiguation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-03-12">12 Mar 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2103.07534v1[cs.DL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Digital libraries</term>
					<term>Author name disambiguation</term>
					<term>Out-of-domain evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Name Disambiguation (AND) is the task of resolving which author mentions in a bibliographic database refer to the same real-world person, and is a critical ingredient of digital library applications such as search and citation analysis. While many AND algorithms have been proposed, comparing them is difficult because they often employ distinct features and are evaluated on different datasets.</p><p>In response to this challenge, we present S2AND, a unified benchmark dataset for AND on scholarly papers, as well as an open-source reference model implementation. Our dataset harmonizes eight disparate AND datasets into a uniform format, with a single rich feature set drawn from the Semantic Scholar (S2) database. Our evaluation suite for S2AND reports performance split by facets like publication year and number of papers, allowing researchers to track both global performance and measures of fairness across facet values.</p><p>Our experiments show that because previous datasets tend to cover idiosyncratic and biased slices of the literature, algorithms trained to perform well on one on them may generalize poorly to others. By contrast, we show how training on a union of datasets in S2AND results in more robust models that perform well even on datasets unseen in training. The resulting AND model also substantially improves over the production algorithm in S2, reducing error by over 50% in terms of B 3 F1. We release our unified dataset, model code, trained models, and evaluation suite to the research community. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>A central challenge in curating large bibliographic databases is determining which author mentions in the database refer to the same real-world person. The problem is challenging because distinct authors often share the same name, and likewise the same author may appear under multiple distinct names. Major bibliographic databases struggle to attribute papers to the correct authors, and authors often resort to manually correcting the databases as a labor-intensive workaround. Automatically determining which author records, i.e. author name strings assigned to a given paper, refer to the same real-world person is a task known as Author Name Disambiguation (AND). Accurate AND is critical for features such as searching or browsing publications by author, reporting up-to-date author profiles with accurate bibliometrics, and other capabilities.</p><p>A variety of AND algorithms have been introduced and evaluated in previous work <ref type="bibr" target="#b0">[1]</ref>. In general, these algorithms operate in three steps: first, they heuristically group candidate duplicate records into blocks for tractability, based on names; then, they score the similarity of each pair of records within a block based on features such as affiliations, co-authors, and paper content; and finally, they cluster the records based on the pairwise scores. Despite considerable progress, it is difficult to accurately assess the effectiveness of today's AND techniques from the existing literature. Comparing the algorithms to one another is difficult because they often have different sets of features, and are evaluated on different datasets (see Table <ref type="table" target="#tab_0">I</ref>). Further, existing AND datasets cover biased portions of the literature <ref type="bibr" target="#b1">[2]</ref>, e.g. the popular Aminer dataset <ref type="bibr" target="#b2">[3]</ref> consists of only Chinese names, whereas SCAD-zbMATH <ref type="bibr" target="#b3">[4]</ref> contains only mathematics papers. Thus, it is unclear whether a method that performs best on one AND dataset will provide strong performance on other datasets or the diverse data found in bibliographic databases in practice.</p><p>In this paper, we present S2AND, a new author disambiguation dataset that combines eight previous datasets into a single resource with a uniform format and a consistent, rich feature set. S2AND's feature values are obtained by aligning its author records to the Semantic Scholar (S2) bibliographic knowledge graph. In experiments with a new AND model that is representative of the state-of-the-art, we show how training the model on the union of datasets in S2AND improves AND accuracy. Training on S2AND provides advantages in both the in-domain setting, in which the training set for the target test set is included in training, and the out-of-domain setting, where it is not. In the out-of-domain setting, training on the union of datasets in S2AND achieves or equals the top clustering accuracy in 4/7 experiments, compared to at most 1/7 achieved when training on any of the original datasets alone. Further, we find that our S2AND-trained system provides dramatic improvements over the author disambiguation algorithm in production on S2 today, reducing error by more than 50% in terms of B 3 accuracy.</p><p>Thus, in addition to serving as a benchmark for comparing AND algorithms, S2AND provides training data that may improve the accuracy and generalization of AND systems in practice.</p><p>To summarize, our contributions are: 1) S2AND, a new training dataset and evaluation benchmark for author name disambiguation that unifies and extends previous resources,</p><p>2 <ref type="bibr">)</ref> A new open-sourced reference AND algorithm implementation which is comparable in performance to stateof-the-art, 3) Experiments showing that training on S2AND improves generalization compared to the single-dataset approach taken in previous work, and 4) A comparison against the existing Semantic Scholar production system, showing that the S2AND-trained system performs better across all the datasets, including across all ranges for all facets examined, such as the number of papers by the author or the publication date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>Author Name Disambiguation has been studied for some years, which has led to the introduction of a variety of datasets differing along important dimensions, such as scientific domain, author ethnicity, feature availability, method of curation, and degree of ambiguity. Early datasets included Arnetminer <ref type="bibr" target="#b9">[10]</ref>, a small manually-curated set of highly ambiguous author names from papers in a variety of domains, and KISTI <ref type="bibr" target="#b6">[7]</ref>, a larger set of much less ambiguous names from computer science papers mined from DBLP <ref type="bibr" target="#b8">[9]</ref>. Later datasets focused on different individual domains, such as PubMed in the medical domain <ref type="bibr" target="#b7">[8]</ref>, INSPIRE in high-energy physics <ref type="bibr" target="#b5">[6]</ref> and SCAD-zbMATH in mathematics <ref type="bibr" target="#b3">[4]</ref>. Different datasets also differ with respect to author ethnicity-for example, Aminer <ref type="bibr" target="#b2">[3]</ref> contains predominantly Chinese names, whereas SCAD-zbMATH and INSPIRE <ref type="bibr" target="#b5">[6]</ref> contain mostly Western names. Different datasets also include different features (author name, affiliation, email, paper title, journal details, keywords, coauthor lists, domain-specific features like MeSH indicators, etc.) with varying levels of availability (see Table <ref type="table" target="#tab_1">II</ref>). Vishnyakova et al. <ref type="bibr" target="#b1">[2]</ref> noted the bias of existing AND datasets, and argued that the datasets were not representative of the challenge faced by scholarly databases, which often lack the datasets' most valuable features (e.g., author affiliation in Pubmed). That work developed the Medline dataset from the medical domain, and showed that models trained on PubMed failed to generalize to Medline, highlighting the practical significance of the dataset bias. Our goal is to unify this previous body of work and present a single AND resource that, because it is comprised of the union of disparate datasets, covers substantially more of the true diversity of the AND task faced in practice.</p><p>Several classification models have been used for learning the pairwise similarity function, including Naive Bayes <ref type="bibr" target="#b10">[11]</ref>, Logistic Regression <ref type="bibr" target="#b11">[12]</ref>, Support Vector Machines <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13]</ref>, Decision Trees (C4.5) <ref type="bibr" target="#b1">[2]</ref>, Random Forests (RF) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, Deep Neural Networks (DNN) <ref type="bibr" target="#b15">[16]</ref> and Gradient Boosted Trees (GBT) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. Tran et al. <ref type="bibr" target="#b15">[16]</ref> use DNNs with manually-crafted features, whereas Atarashi et al. <ref type="bibr" target="#b18">[19]</ref> leveraged a DNN to learn feature representations from bag-of-words vectors. Zhang et al. <ref type="bibr" target="#b2">[3]</ref> used a DNN to first learn a representation for each record, and refine it using a graph autoencoder, where the graph is constructed based on the similarity between records. Similarly, Kim et al. <ref type="bibr" target="#b17">[18]</ref> used a DNN to learn a representation for each record to provide a similarity feature, and showed that incorporating the feature among others within GBTs outperformed other ways of using neural representations from earlier work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref>. We follow a similar approach in our experiments, using GBT as our classifier over a union of features from previous datasets plus paper representations output by a state-of-the-art deep neural network <ref type="bibr" target="#b19">[20]</ref>. Our results show that this approach is representative of the state-of-the-art in AND.</p><p>Similarly, a variety of clustering techniques have been used for grouping records based on pairwise distances, including spectral clustering <ref type="bibr" target="#b20">[21]</ref>, affinity propagation <ref type="bibr" target="#b21">[22]</ref>, densitybased spatial clustering of applications with noise (DBSCAN) <ref type="bibr" target="#b12">[13]</ref>, and hierarchical agglomerative clustering (HAC) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. We evaluate the popular choices <ref type="bibr" target="#b0">[1]</ref> of DBSCAN and HAC, and find that HAC performs best. Exploration of additional clustering methods is an item of future work.</p><p>The increasing use of AI systems in society has led to an increased emphasis on the fairness of the systems, in addition to their global predictive accuracy <ref type="bibr" target="#b23">[24]</ref>. If AND systems have different error rates for different groups, this can be harmful because the publication records in scholarly databases are used as an input to decisions regarding hiring, promotion, conference responsibilities, and more. Because AND mistakes can cause representational and allocational harm, and no system will ever be perfect, it is critical that any live AND service easily allows authors to correct mistakes made by the system. Other than using self-reported demographic attributes <ref type="bibr" target="#b24">[25]</ref>, research has focused on using inferred gender from names for studying gender disparities in authorship and citation trends <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. Similarly, Bertrand and Mullainathan <ref type="bibr" target="#b28">[29]</ref> used inferred gender and race for studying disparities in hiring. In our work, we use the inferred ethnicity of authors, predicted using Ethnea <ref type="bibr" target="#b29">[30]</ref>, in addition to their prolificity (based on count of papers published), to evaluate the disparity of AND systems' performance across groups. To our knowledge, ours is the first study to begin to analyze the fairness of modern AND systems.</p><p>In our analysis of performance by ethnicity, we will refer to estimated name geographic origin. This value is predicted using Ethnea, as mentioned above, and is a highly imperfect measure of actual author attributes. Estimated name geographic origin is predicted by looking up the likely country for a particular name, and then probabilistically mapping countries to ethnicities. The resulting classes from the Ethnea tool are clearly not comprehensive or representative, as 'African' is a single category. Despite these significant shortcomings, we would like to know if our system has performance disparities across the predicted groups of the Ethnea tool. Having no disparity across these groups would not indicate a perfectly fair model, but having significant disparity across these groups would strongly suggest that real performance disparities exist, which deserve further investigation. Additionally, while our analysis is performed by mapping individual names to estimated name geographic origin, we do not release the individual values, only the aggregated performance statistics, due to  <ref type="bibr" target="#b6">[7]</ref> Medline <ref type="bibr" target="#b1">[2]</ref> PubMed <ref type="bibr" target="#b7">[8]</ref> QIAN (DBLP) <ref type="bibr" target="#b8">[9]</ref> SCAD-zbMATH <ref type="bibr" target="#b3">[4]</ref> the possible harm of misidentifying attributes of individuals <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. The gold standard would be to ask authors to selfidentify <ref type="bibr" target="#b30">[31]</ref>, but this is unfortunately not possible at scale, and suffers from other issues, including reporting bias <ref type="bibr" target="#b31">[32]</ref>. We hope that future work continues to address representational harm in bibliographic databases and its impact on people and their careers. 2   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. S2AND</head><p>We now detail how we constructed the S2AND benchmark dataset by aligning existing resources with Semantic Scholar.</p><p>S2AND is a union of eight existing AND datasets, detailed below. We analyzed a few more datasets from the literature, and left out datasets such as Han-DBLP <ref type="bibr" target="#b20">[21]</ref> due to erroneous cluster assignments <ref type="bibr" target="#b3">[4]</ref>, and REXA <ref type="bibr" target="#b32">[33]</ref>, a small dataset containing 13 blocks, due to typographical errors in paper details which made it prohibitively hard to align with S2. The very recent Author-ity2009 data set <ref type="bibr" target="#b33">[34]</ref> appeared after our investigation was complete, so it was not considered for inclusion in S2AND. In the following, a record refers to an author name appearing on a particular paper (aka signature), which can be characterized by a number of associated features (like the author's listed affiliation and email) as discussed in later sections. A block refers to a subset of records identified in the dataset as potentially the same author; different datasets use different blocking functions, but a typical choice is to put all records with the same last name and first initial into the same block. Except in the case of Medline, all datasets include a ground-truth partition of the records into disjoint clusters that represent the same author.</p><p>The datasets that comprise S2AND are: Aminer [3]: manually-disambiguated records from Aminer.cn. ArnetMiner <ref type="bibr" target="#b4">[5]</ref>: Ambiguous names gathered from Arnet-Miner (a predecessor of Aminer), and manually disambiguated by comparing affiliations, email addresses, and information on personal webpages. INSPIRE <ref type="bibr" target="#b5">[6]</ref>: Paper records that have been claimed by their original authors on INSPIRE, an online bibligraphic database. Approximately 13% of the publication records have been claimed by their original authors and are verified automatically 2 We did not observe any difference across stereotypically estimated name gender (also from the Ethnea tool), but refrain from including detailed results because the estimated gender is not of sufficient quality, not self-reported, and binary.</p><p>based on persistent identifiers or manually by professional curators. KISTI <ref type="bibr" target="#b6">[7]</ref>: Mentions from the DBLP bibliographic database manually disambiguated using Web search queries. Medline <ref type="bibr" target="#b1">[2]</ref>: Randomly sampled Medline publications. This dataset differs from the others in that it is only a pairwise classification dataset, and does not include full ground-truth clusters.</p><p>PubMed <ref type="bibr" target="#b7">[8]</ref>: First-author records from PubMed papers, annotated by identifying each author's publication list via Web search. QIAN <ref type="bibr" target="#b8">[9]</ref>: A union of other AND datasets, including Han-DBLP <ref type="bibr" target="#b20">[21]</ref> and ArnetMiner <ref type="bibr" target="#b4">[5]</ref>, manually de-duplicated and corrected for errors. SCAD-zbMATH <ref type="bibr" target="#b3">[4]</ref>: Community-curated mathematics papers from the zbMATH bibliographic database.</p><p>For unifying the above AND datasets to S2, we use a semi-automated process. Given a paper, we first perform a text search for its title on Semantic Scholar, and choose the top 10 results. <ref type="foot" target="#foot_1">3</ref> We manually verified this approach using QIAN, and found that the top 10 results provided nearly 99% recall. We then re-rank the top 10 results based on similarity of other metadata such as number of authors, author names, venue/journal details and year of publication. In manual inspection, the top-ranked result was a correct match for about 98% of the query records. We manually inspect any cases in which the top-ranked result fails to match the author name of the query record, and discard cases in which the names are actually different. Finally, our different datasets have some overlap, and when two datasets disagree on whether to cluster a pair of records, we exclude the pair from S2AND (manual inspection revealed that no dataset was fully correct).</p><p>Not all the papers in the original datasets had a match in the S2 database (similar to the outcome in <ref type="bibr" target="#b34">[35]</ref>, where datasets were aligned to DBLP). On average 82% of the records were aligned, with nearly complete alignment for PubMed and Medline, and less than 50% for INSPIRE and SCAD-zbMATH which contain Physics and Mathematics publications respectively (see Table <ref type="table" target="#tab_1">II</ref>). In spite of this, the datasets can support a holistic evaluation of a feature-based AND system, and assess the performance of S2.  The details of the individual datasets after alignment, including the prevalence of different features obtained from S2 for each dataset's records, are given in Table <ref type="table" target="#tab_1">II</ref>. Due to low coverage of affiliations data in S2 at the time of data collection, we supplement with affiliations from the Microsoft Academic Graph (MAG) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> when S2 affiliations are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. REFERENCE AUTHOR NAME DISAMBIGUATION SYSTEM</head><p>To report baseline results on our benchmark, and evaluate the utility of training on S2AND, we developed a reference AND implementation that is representative of the state-of-theart. We release the implementation as part of our benchmark. It follows the typical three-stage AND approach: (A) blocking, (B) pairwise similarity estimation, and (C) clustering. We describe each step below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Blocking</head><p>For tractability, records first are partitioned into disjoint, potentially-coreferent blocks. We put records in the same block iff they match on the author's first initial and last name.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pairwise Similarity Estimation</head><p>We estimate the similarity of each pair of records in a block using a classifier trained to predict if two records were written by the same author. We use a gradient-boosted trees (GBT) classifier over the feature set listed in Table <ref type="table" target="#tab_1">III</ref>, following previous work as discussed in Section II. We restrict to generic features that can be constructed using the information in bibliographic databases, and avoid domainspecific features such as MeSH indicators or those requiring Web search. A more novel feature in our system is name popularity estimated as counts from S2, which we expect to be correlated with the number of distinct referents of a name. Another is motivated by the success of neural representations to compute similarity features (see Section II). Specifically, we compute SPECTER embeddings <ref type="bibr" target="#b19">[20]</ref> for each record using the paper's title and abstract (if available). SPECTER is a recent document-embedding approach, trained on the citation graph to produce paper embeddings applicable across multiple tasks.</p><p>We use LightGBM <ref type="bibr" target="#b37">[38]</ref> as our pairwise classifier, and the hyperopt package <ref type="bibr" target="#b38">[39]</ref> to tune 11 hyperparameters<ref type="foot" target="#foot_2">4</ref> using a held-out set. We impose feature-wise monotonicity constraints on some features to both (a) regularize the model and (b) constrain the model to behave sensibly even when faced with data from outside of the training distribution. For example, we constrain the model to (all other features being unchanged) decrease the output probability of two records being by the same author if the year difference between the two papers increases.</p><p>The final pairwise classifier is an ensemble of two models: the classifier discussed above, and another version of it that we call 'nameless', which is identical in every respect but does not have any features that are related to the surface forms of the author names (co-author names are still included). The reason for this was practical -we observed that the pairwise model over-relied on name features even in the presence of other metadata. Ensembling with the 'nameless' model decreased pairwise AUROC performance but improved the B 3 clustering metric substantially (see Table <ref type="table" target="#tab_7">VII</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Agglomerative Clustering</head><p>Using the trained pairwise classifier from the previous step we construct a distance matrix D where D ij is the probability that two records i and j are not by the same author. We then partition each block into clusters with hierarchical agglomerative clustering <ref type="bibr" target="#b39">[40]</ref> over the matrix D. The clustering depends on a linkage function that estimates the dissimilarity between two clusters, in terms of the pairwise distances between the individual elements of each cluster. Our experiments evaluate several alternatives, and we find that a straightforward average of all the pairwise distances performs best.</p><p>We tune only the eps hyperparameter for agglomerative clustering, defined as the distance threshold above which clusters will not be merged. This is tuned on a held-out set of blocks using hyperopt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In this section, we first validate that our AND reference implementation is comparable to the state-of-the-art when evaluated on pre-existing AND datasets. Then, we perform our primary evaluation of training on S2AND, in both the in-domain and out-of-domain setting, showing that training on a union of datasets in S2AND results in more robust performance compared to the standard approach of training on a single dataset. Finally, we analyze performance in ablation studies and across different facet values, and compare a S2AND-trained system against the AND system used in Semantic Scholar (S2). As there can be variations in names for various reasons, S2's blocks are not always same as the blocks provided in the original datasets. To ensure a fair comparison, we use the blocks from the original datasets for state-ofthe-art evaluation (Section V-A), and S2's blocks for other experiments (Section V-B and Section V-C).</p><p>Our evaluation uses two standard metrics. To evaluate the pairwise classifier's performance in isolation, we use area under the ROC curve (AUROC). To evaluate the final end-task clustering performance, we use the B 3 F1 metric <ref type="bibr" target="#b40">[41]</ref>, which is the average of F1 scores for each of the individual records. For state-of-the-art comparisons, we use other metrics in order to compare directly with existing literature. These metrics include average precision for pairwise similarity ranking, and pairwise F1 for the final clustering phase <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to state-of-art performance</head><p>The goal of our work is not to propose a new state-of-the-art AND algorithm. However, to ensure that our evaluation of the S2AND dataset is representative of the current state-of-the-art, we start by showing that our reference AND implementation achieves comparable performance to existing state-of-the-art algorithms when evaluated in the same setting considered in previous work, where we train on records from a single dataset and test on held-out data from the same distribution. We evaluate on the datasets aligned to S2 (Table <ref type="table" target="#tab_1">II</ref>), which  <ref type="bibr" target="#b41">[42]</ref> 0.917 <ref type="bibr" target="#b34">[35]</ref> retains a majority of the original records, and contains features from S2. We use the evaluation settings from the state-ofthe-art approaches, and compare against their reported performance. For Aminer, the dataset provides the training and test block splits used by Zhang et al. <ref type="bibr" target="#b2">[3]</ref> (note that there is a discrepancy in the statistics of the data mentioned in the paper compared to the data publicly released -70,258 vs. 203,078 publications, respectively) and Kim et al. <ref type="bibr" target="#b17">[18]</ref>. The Inspire dataset provides records split into training and test sets <ref type="bibr" target="#b5">[6]</ref>, and Medline similarly has pre-determined train and test pairwise classification records <ref type="bibr" target="#b1">[2]</ref>. For PubMed and KISTI, we use the same cross-validation settings mentioned in Vishnyakova et al.</p><p>[2] and Santana et al. <ref type="bibr" target="#b41">[42]</ref>, respectively. <ref type="foot" target="#foot_3">5</ref> For KISTI's stateof-the-art performance, we also include DBLP's performance as evaluated by Kim <ref type="bibr" target="#b34">[35]</ref>. The results are shown in Table <ref type="table" target="#tab_3">IV</ref>. While there is some variance on the Aminer dataset, on average S2AND's performance is comparable to the published state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. S2AND vs. single-data-set training</head><p>We now turn to our primary evaluation of how training on S2AND impacts AND system performance, compared to the previous approach of training on a single dataset. We evaluate in both the in-domain and out-of-domain setting. In these experiments, we first split each dataset into train, validation, and test splits based on blocks derived from S2 names (80/10/10 split). For training and tuning the pairwise model, we then sample pairs from train and validation respectively, and from test for AUROC evaluation. For fitting and evaluating the clusterer we use the validation and test splits respectively. Each dataset contributes at most 100,000 pairwise examples to training, and at most 10,000 to each of validation and test. We evaluate on disjoint test sets from each of the seven (eight for pairwise evaluation) original datasets. When using a single dataset for training, testing on test data from the same dataset evaluates in the in-domain setting, whereas testing on the other datasets evaluates in the out-of-domain setting. To test the union approach in the out-of-domain setting, we also evaluate the union with the training examples from the target test dataset excluded (S2AND (-target)). We perform five runs  <ref type="table" target="#tab_5">VI</ref>. As discussed earlier, each of the existing AND datasets tends to focus on only a subset of the scholarly literature, and how representative any of the datasets is of the real-world AND task faced by digital libraries is unclear. Thus, in order to better gauge how systems are likely to perform on the real-world AND task faced in practice, our primary evaluation focuses on the out-of-domain setting, in which the test data distribution may differ from the training data distribution. To measure outof-domain performance, we evaluate in the setting where the training set for the target test set is not included in the training data (listed as "S2AND (-target)" in the table). Compared to each out-of-domain single dataset (the not italicized values in Table <ref type="table" target="#tab_4">V</ref>), "S2AND (-target)" outperforms or matches all others on 4 out of 7 datasets, and is never further than 0.016 B 3 from the best. This indicates that training on the union of all datasets (excluding target) is an effective way to transfer to out-of-domain data, relative to training on a single one of the existing datasets. As mentioned in Section III, there is some overlap of records between datasets, and it is less than 1% on average (relative to the size of each target dataset). Moreover, we also found an insignificant positive correlation between overlap proportions and transfer results in Table <ref type="table" target="#tab_4">V</ref>. Both of these results suggest that the positive transfer in the out-ofdomain setting is not due to simple overlap of the records in the data sets.</p><p>Regarding the in-domain setting, compared to each indomain single dataset (the italicized values in Table <ref type="table" target="#tab_4">V</ref>), "S2AND (+target)" outperforms on 3 out 7 datasets, and is never more than 0.015 B 3 lower. This indicates that training on the union of all datasets is an effective way to train a model that is robust across all the existing datasets.</p><p>For pairwise classification performance, we see similar trends, with the exception that Inspire is also best on 3 out of 8 datasets. S2AND (-target) is also best on 3 out of 8 datasets, and is never further than 0.026 AUROC from the best. In the in-domain setting, compared to each single dataset, S2AND (+target) outperforms or matches on 4 out 8 datasets, and is never further than 0.016 AUROC off.</p><p>1) Feature Importance: We study the importance of individual features using SHapley Additive exPlanations (SHAP) <ref type="bibr" target="#b42">[43]</ref> of the S2AND (-target) union model. SHAP values provide individualized, per-sample and per-feature linear additive explanations. Intuitively, a SHAP value s ij for sample i and feature j is a measure of the effect of this feature's presence (vs absence) on the model output for sample j, which in our case is the probability that two records are written by the same author. Upon manual examination of the SHAP plots for each of the seven S2AND (-target) models we observe that some features are nearly always the most important (last name counts and SPECTER similarities are the most notable examples, the latter of which is never lower than 4th most important), while other features have a more variable importance. For example (a) Jaro-Winkler, Levenshtein, and prefix first name similarities, (b) middle initials overlap, and (c) venue similarity are only near the top of the SHAP ordering for some of the datasets.</p><p>2) Ablation study: We now analyze the design choices and feature set of S2AND through ablation studies. In these experiments, for tractability we evaluate for only a single random seed (which results in the baseline S2AND performance differing from that reported in the previous section). For simplicity, we report just B 3 F1 performance of each variant averaged over the seven test sets, in the S2AND (target) setting.</p><p>Table <ref type="table" target="#tab_7">VII</ref> shows the results of removing single features from S2AND, or by altering design choices in the classifier or clusterer. The results show that many of the choices in S2AND are important for performance, in particular the use of averagelinkage in the cluster, the use of GBTs rather than a linear model, and the incorporation of the nameless classifier. The three most valuable features in this study are affiliation, coauthors, and SPECTER embedding distance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with Semantic Scholar</head><p>As we mentioned in the introduction, one of the major goals of this work is to improve the name disambiguation system in a real-world bibliographic database, in this case Semantic Scholar. Here we compare the performance of our proposed system against the current S2 clusters on the production website. Even though S2AND's feature values are computed from S2's data, S2's AND system is very different from our reference trained on S2AND. S2's algorithm relies on hand-crafted rules and was tuned using a single dataset based on ORCID. By contrast, our system differs in that it uses a learned model, and the model is trained on the diverse datasets in S2AND.</p><p>In addition to the overall performance, we also compare performance broken out across facets to measure whether overall accuracy and S2AND's improvements differ across different author or paper characteristics. For fair comparison with Semantic Scholar, which was not trained on any of the datasets we consider here, we evaluate S2AND in the "union excluding target" setting.</p><p>Overall, S2AND reduces error over S2 by more than 50%, as shown in Table <ref type="table" target="#tab_4">V</ref>, where S2AND's average B 3 F1 is 90% compared to S2 at 78.4%. The B 3 F1 performance broken out by estimated name geographic origin is shown in Figure <ref type="figure" target="#fig_0">1</ref>. As discussed above, the proxy we use as facet values for this analysis is a highly imperfect indicator of the actual attributes of the author. Nonetheless, the fact that AND performance of both systems varies significantly across the groups suggests areas for improvement in AND systems, and it is perhaps encouraging that S2AND tends to improve performance relatively more for groups for which S2's original performance was lower.</p><p>We present additional analyses in Figures <ref type="figure" target="#fig_1">2</ref><ref type="figure" target="#fig_2">3</ref><ref type="figure" target="#fig_3">4</ref>, isolating performance for the number of papers published by the author, along with attributes of the paper including the year of publication and the number of authors. Finally, we also consider attributes of the block, including its size and two ratios that characterize its difficulty: homonymity, defined as the fraction of records in the block with the same names that are in different clusters, and its synonymity, the fraction of records in the same cluster that have different names. In general, the results show that S2AND consistently improves performance across the different facet values. Also, S2AND tends to show a larger improvement in cases where the original S2 performance was lower. Among these findings, one is that authors on newer papers can be more difficult for the systems to disambiguate-we attribute this in large part to the fact that the most recent papers in the challenging Aminer dataset are more often by junior authors with smaller clusters, and thus have higher error rates. Further, one important area for improvement, highlighted in Figure <ref type="figure" target="#fig_1">2</ref>, involves disambiguating larger blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PRACTICAL CONSIDERATIONS FOR AND</head><p>We also investigated building a production AND system out of the S2AND-trained reference model evaluated in the previous section. As is common when applying machine learning to a real-world task, optimizing a metric on supervised datasets does not fully capture all the performance requirements of an actual AND system. Here we briefly comment on a handful of tactics we used for building out our production system. First, real data contains many more "paper" records that are not actually papers, and many more papers that are not in English. Our features, particularly SPECTER embeddings, are not intended for use on such records. Second, the distribution of missing metadata in a real corpus does not perfectly match that of the datasets in this paper, and if one piece of metadata being missing is strongly correlated with a particular training dataset, the model can learn a spurious interaction and make counter-intuitive predictions. Third, not all errors are created equal. Errors that are obvious to a human are both easier to identify, and more likely to incur user complaints. To partially address these issues, we:    ? Added simple rules that prevent incompatible names from clustering together (e.g. John cannot cluster with James). These rules are not perfect, as authors can change their names, and names can be mis-extracted from papers, but they help us prevent the model from making many obvious errors which break user trust. ? Assessed not only with B 3 on held-out data but also via qualitative evaluations on author profile corrections made by users to our existing AND model. We also extensively studied the outputs of many model iterations and adjusted the feature computations based on our observations. There are also practical difficulties that we did not fully address: (a) recovering from name blocking errors, (b) lossy transliteration of, for example, Chinese names, (c) successfully clustering English and non-English papers together, and (d) the hierarchical agglomerative clustering pipeline does not allow for the similarity of one pair of records to influence the similarity of another pair of records.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>We have presented S2AND, a new dataset and benchmark for author disambiguation that unifies eight previous AND datasets into a uniform format. In experiments with a reference AND implementation that we introduce, we show that training on the union of datasets in S2AND improves generalization to datasets not seen in training. Our benchmark algorithm also improves over the production Semantic Scholar system, reducing error by more than 50%. We hope that S2AND helps further new innovation in the important task of author disambiguation in digital libraries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Performance across estimated name geographic origin groups. The performance of both systems varies significantly across different groups, and S2AND improves performance significantly over S2 for almost all groups, and the improvement is relatively larger for groups with lower S2 performance.</figDesc><graphic url="image-1.png" coords="8,147.38,50.54,313.74,147.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Performance across (a) block sizes and (b) cluster sizes (number of papers by author). Larger blocks are more difficult for both algorithms, and S2AND improves performance across all block sizes. S2AND improves performance across all cluster sizes, with a larger gain for more prolific authors.</figDesc><graphic url="image-2.png" coords="8,48.96,225.38,248.79,131.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance across (a) number of authors in a paper and (b) publication years. S2AND improves performance over S2 for any number of authors, with 5-to 10-author papers showing the lowest performance for both algorithms. S2AND improves performance for all years, and newer papers are more challenging for both algorithms in our data.</figDesc><graphic url="image-4.png" coords="8,50.00,383.69,252.75,131.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance across varying author name (a) homonymity (higher values indicate more ambiguous names within a particular block) and (b) synonymity (higher values indicate more distinct name variants for a single author within a particular block). S2AND improves performance in all cases.</figDesc><graphic url="image-6.png" coords="8,50.10,552.96,250.67,130.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I FEATURES</head><label>I</label><figDesc>PROVIDED BY PREVIOUS AND DATASETS (REF=REFERENCES, ABS=ABSTRACT, AFL=AFFILIATION). THE PUBMED AND MEDLINE DATASETS PROVIDE ONLY RECORDS (AUTHOR NAME AND PMID OF PAPER) AND THEIR GROUPING. DATASETS AFTER ALIGNMENT TO S2 HAVE ALL FEATURES AVAILABLE IN S2.</figDesc><table><row><cell>Dataset</cell><cell>Co-author Ref Title Abs Position Year Afl Email Venue</cell></row><row><cell>Aminer [3]</cell><cell></cell></row><row><cell>Arnetminer [5]</cell><cell></cell></row><row><cell>INSPIRE [6]</cell><cell></cell></row><row><cell>KISTI (DBLP)</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II STATISTICS</head><label>II</label><figDesc>ON THE S2AND DATASET. FOR EACH SOURCE DATASET, THE FIRST FOUR COLUMNS LIST THE NUMBER OF DATA OBJECTS FROM THAT DATASET IN S2AND (WITH THE CORRESPONDING COUNT FROM THE ORIGINAL DATASET IN PARENTHESES). THE REMAINING COLUMNS LIST THE PREVALENCE OF EACH FEATURE IN S2AND. FIRST NAME, AFFILIATION AND EMAIL COVERAGE ARE EXPRESSED AS PROPORTIONS OF RECORDS, AND THE OTHER COVERAGES ARE PROPORTIONS OF PAPERS. FIRST NAME DENOTES THE PROPORTION OF RECORDS CONTAINING A FULL FIRST NAME.</figDesc><table><row><cell>Dataset</cell><cell>#Blocks</cell><cell>#Records</cell><cell>#Papers</cell><cell cols="5">#Clusters Abstract References First name Affiliation Email Venue</cell></row><row><cell>Aminer</cell><cell cols="4">600 (600) 157448 (208827) 153147 (203078) 31848 (39781) 0.9599</cell><cell>0.4344</cell><cell>0.9346</cell><cell>0.7375</cell><cell>0.0199 0.7009</cell></row><row><cell>Arnetminer</cell><cell>103 (110)</cell><cell>7144 (7528)</cell><cell>7067 (7447)</cell><cell>1512 (1726) 0.9420</cell><cell>0.7164</cell><cell>0.9730</cell><cell>0.7424</cell><cell>0.0000 0.9716</cell></row><row><cell>INSPIRE</cell><cell cols="4">12458 (12978) 536564 (1201763) 265497 (360066) 14996 (15388) 0.9137</cell><cell>0.3239</cell><cell>0.4997</cell><cell>0.5576</cell><cell>0.0009 0.2835</cell></row><row><cell>KISTI</cell><cell>881 (924)</cell><cell>40383 (41673)</cell><cell>36447 (37613)</cell><cell>6856 (6921) 0.9580</cell><cell>0.7840</cell><cell>0.9731</cell><cell>0.7890</cell><cell>0.0000 0.9972</cell></row><row><cell>Medline</cell><cell>-</cell><cell>3738 (3750)</cell><cell>3707 (3744)</cell><cell>-0.8778</cell><cell>0.4715</cell><cell>0.6027</cell><cell>0.6169</cell><cell>0.0594 1.0000</cell></row><row><cell>PubMed</cell><cell>41 (41)</cell><cell>2871 (2875)</cell><cell>2871 (2875)</cell><cell>385 (385) 0.9516</cell><cell>0.4932</cell><cell>0.7886</cell><cell>0.9436</cell><cell>0.4720 1.0000</cell></row><row><cell>QIAN</cell><cell>580 (592)</cell><cell>6542 (6717)</cell><cell>6542 (6717)</cell><cell>1188 (1201) 0.9590</cell><cell>0.7733</cell><cell>0.9934</cell><cell>0.7634</cell><cell>0.0000 0.9954</cell></row><row><cell>SCAD-zbMATH</cell><cell>2136 (2919)</cell><cell>15181 (33810)</cell><cell>12289 (28321)</cell><cell>2334 (2946) 0.5109</cell><cell>0.1501</cell><cell>0.6717</cell><cell>0.3415</cell><cell>0.0000 0.3728</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF OUR REFERENCE IMPLEMENTATION VS. PREVIOUSLY PUBLISHED RESULTS. OUR ALGORITHM PERFORMS COMPARABLY TO THE PREVIOUS STATE-OF-THE-ART.</figDesc><table><row><cell>Dataset</cell><cell>Metric</cell><cell>Task</cell><cell>Ours</cell><cell>SoTA</cell></row><row><cell>Aminer</cell><cell>Average Precision</cell><cell cols="3">Classification 0.758 0.691 [18]</cell></row><row><cell>Aminer</cell><cell cols="2">Pairwise Macro F1 Clustering</cell><cell cols="2">0.613 0.678 [3]</cell></row><row><cell cols="2">INSPIRE B 3 F1</cell><cell>Clustering</cell><cell cols="2">0.974 0.987 [6]</cell></row><row><cell cols="3">INSPIRE Pairwise Macro F1 Clustering</cell><cell cols="2">0.980 0.989 [6]</cell></row><row><cell>PubMed</cell><cell>F1</cell><cell cols="3">Classification 0.926 0.897 [2]</cell></row><row><cell>Medline</cell><cell>F1</cell><cell cols="3">Classification 0.901 0.872 [2]</cell></row><row><cell>KISTI</cell><cell cols="2">Pairwise Macro F1 Clustering</cell><cell>0.918</cell><cell>0.816</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V B</head><label>V</label><figDesc>3 F1 CLUSTERING PERFORMANCE OF TRAINING ON VARIOUS DATASETS, EVALUATED ON DIFFERENT TARGET TEST SETS. S2 DENOTES THE PERFORMANCE OF THE PRODUCTION SEMANTIC SCHOLAR SYSTEM. ITALICIZED GRAY ENTRIES ARE FOR THE IN-DOMAIN SETTING, AND UN-ITALICIZED ENTRIES ARE FOR THE OUT-OF-DOMAIN SETTING. BOLD INDICATES THE BEST OUT-OF-DOMAIN RESULT IN EACH COLUMN.</figDesc><table><row><cell cols="4">Train ? / Test ? Aminer Arnetminer Inspire Kisti Pubmed Qian Zbmath</cell></row><row><cell>Aminer</cell><cell>0.774</cell><cell>0.838</cell><cell>0.871 0.935 0.922 0.905 0.875</cell></row><row><cell>Arnetminer</cell><cell>0.688</cell><cell>0.872</cell><cell>0.946 0.939 0.892 0.936 0.926</cell></row><row><cell>Inspire</cell><cell>0.557</cell><cell>0.771</cell><cell>0.961 0.946 0.869 0.875 0.956</cell></row><row><cell>Kisti</cell><cell>0.611</cell><cell>0.859</cell><cell>0.952 0.954 0.902 0.928 0.955</cell></row><row><cell>Pubmed</cell><cell>0.619</cell><cell>0.773</cell><cell>0.878 0.906 0.910 0.887 0.892</cell></row><row><cell>Qian</cell><cell>0.684</cell><cell>0.871</cell><cell>0.945 0.934 0.920 0.944 0.942</cell></row><row><cell>Zbmath</cell><cell>0.487</cell><cell>0.747</cell><cell>0.945 0.897 0.692 0.825 0.953</cell></row><row><cell>S2</cell><cell>0.533</cell><cell>0.729</cell><cell>0.743 0.934 0.816 0.937 0.795</cell></row><row><cell cols="2">S2AND (-target) 0.710</cell><cell>0.866</cell><cell>0.936 0.943 0.944 0.937 0.963</cell></row><row><cell cols="2">S2AND (+target) 0.764</cell><cell>0.864</cell><cell>0.962 0.948 0.928 0.929 0.961</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI AUROC</head><label>VI</label><figDesc>CLASSIFICATION PERFORMANCE FOR DIFFERENT TRAINING SETS, EVALUATED ON DIFFERENT TARGET TEST SETS. ITALICIZED GRAY ENTRIES ARE FOR THE IN-DOMAIN SETTING, AND UN-ITALICIZED ENTRIES ARE FOR THE OUT-OF-DOMAIN SETTING. BOLD INDICATES BEST OUT-OF-DOMAIN RESULT IN EACH COLUMN. seeds and report the average results in Table V and Table VI. The results for clustering are shown in Table V, and the pairwise classification results are shown in Table</figDesc><table><row><cell cols="4">Train ? / Test ? Aminer Arnetminer Inspire Kisti Pubmed Qian Zbmath Medline</cell></row><row><cell>Aminer</cell><cell>0.933</cell><cell>0.901</cell><cell>0.916 0.980 0.974 0.946 0.802 0.951</cell></row><row><cell>Arnetminer</cell><cell>0.883</cell><cell>0.917</cell><cell>0.889 0.975 0.966 0.950 0.799 0.953</cell></row><row><cell>Inspire</cell><cell>0.927</cell><cell>0.903</cell><cell>0.959 0.983 0.980 0.938 0.868 0.952</cell></row><row><cell>Kisti</cell><cell>0.930</cell><cell>0.934</cell><cell>0.910 0.984 0.979 0.956 0.824 0.948</cell></row><row><cell>Pubmed</cell><cell>0.927</cell><cell>0.899</cell><cell>0.893 0.982 0.985 0.937 0.733 0.950</cell></row><row><cell>Qian</cell><cell>0.899</cell><cell>0.910</cell><cell>0.864 0.975 0.964 0.952 0.789 0.953</cell></row><row><cell>Zbmath</cell><cell>0.909</cell><cell>0.847</cell><cell>0.902 0.970 0.962 0.903 0.875 0.955</cell></row><row><cell>Medline</cell><cell>0.878</cell><cell>0.859</cell><cell>0.892 0.961 0.953 0.922 0.756 0.964</cell></row><row><cell cols="2">S2AND (-target) 0.904</cell><cell>0.921</cell><cell>0.929 0.981 0.970 0.957 0.852 0.971</cell></row><row><cell cols="2">S2AND (+target) 0.917</cell><cell>0.929</cell><cell>0.954 0.981 0.980 0.958 0.882 0.972</cell></row><row><cell>with different random</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table VIII shows the effect of training size (of each dataset in the union) of the pairwise models, which shows that performance increases only marginally between 1k examples and the full 100k examples used in S2AND.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII ABLATION</head><label>VII</label><figDesc>EXPERIMENTS STUDYING THE EFFECTS OF REMOVING SINGLE FEATURES; SUBSTITUTING DIFFERENT CLUSTERING LINKAGE FUNCTIONS (VS. THE AVERAGE LINKAGE IN S2AND), CLUSTERING METHODS (VS.</figDesc><table><row><cell cols="3">HIERARCHICAL AGGLOMERATIVE CLUSTERING), OR PAIRWISE</cell></row><row><cell cols="3">CLASSIFIERS (VS. GRADIENT-BOOSTED TREES); OR REMOVING THE</cell></row><row><cell cols="3">MONOTONICITY CONSTRAINTS OR NAMELESS CLASSIFIER. MOST OF</cell></row><row><cell cols="3">THESE DESIGN ALTERNATIVES HURT PERFORMANCE.</cell></row><row><cell>Ablation experiment</cell><cell cols="2">Average B 3 ? from baseline</cell></row><row><cell cols="2">Baseline (our final model) 0.926</cell><cell>-</cell></row><row><cell>Ward Linkage</cell><cell>0.843</cell><cell>0.083</cell></row><row><cell>Complete Linkage</cell><cell>0.876</cell><cell>0.05</cell></row><row><cell>DBSCAN clustering</cell><cell>0.880</cell><cell>0.046</cell></row><row><cell>Single Linkage</cell><cell>0.880</cell><cell>0.046</cell></row><row><cell>Linear pairwise classifier</cell><cell>0.882</cell><cell>0.044</cell></row><row><cell>No affiliation</cell><cell>0.896</cell><cell>0.03</cell></row><row><cell>No specter</cell><cell>0.898</cell><cell>0.028</cell></row><row><cell>No coauthor</cell><cell>0.898</cell><cell>0.028</cell></row><row><cell>No nameless classifier</cell><cell>0.903</cell><cell>0.023</cell></row><row><cell>No name counts</cell><cell>0.91</cell><cell>0.016</cell></row><row><cell>No references</cell><cell>0.918</cell><cell>0.008</cell></row><row><cell>No email</cell><cell>0.929</cell><cell>0.006</cell></row><row><cell>No advanced names</cell><cell>0.921</cell><cell>0.005</cell></row><row><cell>No year</cell><cell>0.922</cell><cell>0.004</cell></row><row><cell>No venue, journal</cell><cell>0.923</cell><cell>0.003</cell></row><row><cell>No monotonicity</cell><cell>0.924</cell><cell>0.002</cell></row><row><cell>No title</cell><cell>0.925</cell><cell>0.001</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/allenai/S2AND/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In the case of PubMed and Medline, we instead align using the given PubMed IDs and https://api.semanticscholar.org/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>See the code for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We do not have access to the precise CV splits used in those works.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>? Trained with an additional dataset, created by taking the union of all the other datasets and randomly knocking out features (e.g. randomly remove affiliation information). This helps harden the model against inappropriate predictions with missing metadata.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>VIII. ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Zejiang Shen</rs>, <rs type="person">Sonia Murthy</rs>, <rs type="person">Kyle Lo</rs>, and <rs type="person">Dan Weld</rs> for helpful feedback; <rs type="person">Bailey Kuehl</rs> and <rs type="person">Rodney Kinney</rs> for their extensive evaluation; <rs type="person">Regan Huff</rs>, <rs type="person">Jason Dunkelberger</rs>, <rs type="person">Joanna Power</rs>, <rs type="person">Angele Zamarron</rs>, and <rs type="person">Brandon Stilson</rs> for their engineering work that turned our Python code into something that scales to 200 million papers; and the entire Semantic Scholar team for creating the underlying data. This work was supported in part by <rs type="funder">NSF</rs> grant <rs type="grantNumber">OIA-2033558</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_eAYpNaj">
					<idno type="grant-number">OIA-2033558</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A brief survey of automatic methods for author name disambiguation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gonc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Laender</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Sigmod Record</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A new approach and gold standard toward author disambiguation in medline</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vishnyakova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rodriguez-Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rinaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1037" to="1045" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Name disambiguation in aminer: Clustering, maintenance, and human in the loop</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1002" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data sets for author name disambiguation: an empirical analysis and a new resource</title>
		<author>
			<persName><forename type="first">M.-C</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Reitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1467" to="1500" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified probabilistic framework for name disambiguation in digital library</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="975" to="987" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">in international conference on knowledge engineering and the semantic web</title>
		<author>
			<persName><forename type="first">G</forename><surname>Louppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Al-Natsheh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Susik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Maguire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="272" to="287" />
		</imprint>
	</monogr>
	<note>Ethnicity sensitive author disambiguation using semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Construction of a large-scale test set for author disambiguation</title>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="452" to="465" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring author name disambiguation on PubMed-scale</title>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of informetrics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="924" to="941" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic author name disambiguation for growing digital libraries</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="379" to="412" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adana: Active name disambiguation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 11th international conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="794" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Two supervised learning approaches for name disambiguation in author citations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsioutsiouliklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Joint ACM/IEEE Conference on Digital Libraries</title>
		<meeting>the 2004 Joint ACM/IEEE Conference on Digital Libraries</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="296" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Citation-based bootstrapping for large-scale author disambiguation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krawczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1030" to="1047" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient name disambiguation for large-scale databases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ertekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on principles of data mining and knowledge discovery</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="536" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disambiguating authors in academic publications using random forests</title>
		<author>
			<persName><forename type="first">P</forename><surname>Treeratpituk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th ACM/IEEE-CS joint conference on Digital libraries</title>
		<meeting>the 9th ACM/IEEE-CS joint conference on Digital libraries</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Author name disambiguation in PubMed using ensemble-based classification algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jhawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Bhowmick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020</title>
		<meeting>the ACM/IEEE Joint Conference on Digital Libraries in 2020</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="469" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Author name disambiguation by using deep neural network</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">N</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Intelligent Information and Database Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="123" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A web service for author name disambiguation in scholarly databases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sefid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Web Services (ICWS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="265" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hybrid deep pairwise classification for author name disambiguation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2369" to="2372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep neural network for pairwise classification: Enabling feature conjunctions and ensuring symmetry</title>
		<author>
			<persName><forename type="first">K</forename><surname>Atarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Furudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="83" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SPECTER: Document-level representation learning using citation-informed transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2270" to="2282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Name disambiguation in author citations using a k-way spectral clustering method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL&apos;05)</title>
		<meeting>the 5th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On graph-based name disambiguation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Data and Information Quality (JDIQ)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Author name disambiguation for PubMed</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Comeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yeganova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Wilbur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="781" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Language (technology) is power: A critical survey of&quot; bias&quot; in NLP</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daum?</surname><genName>III</genName></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14050</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Awareness in practice: tensions in access to sensitive attribute data for antidiscrimination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bogen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="492" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gender gap in natural language processing research: Disparities in authorship and citations</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00962</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">He said, she said: Gender in the acl anthology</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries</title>
		<meeting>the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="33" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The effect of gender in the publication patterns in mathematics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mihaljevi?-Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Santamar?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tullney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Are Emily and Greg more employable than Lakisha and Jamal? a field experiment on labor market discrimination</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mullainathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="991" to="1013" />
		</imprint>
	</monogr>
	<note>American economic review</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ethnea-an instance-based ethnicity classifier based on geo-coded author names in a large-scale bibliographic database</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Torvik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Science of Science</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gender recognition or gender reductionism?: The social implications of embedded gender recognition systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hamidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Scheuerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Branham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2018 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reflections on gender analyses of bibliographic corpora</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mihaljevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tullney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Santamar?a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Steinfeldt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Author disambiguation using error-driven machine learning with a ranking loss function</title>
		<author>
			<persName><forename type="first">A</forename><surname>Culotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kanani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Workshop on Information Integration on the Web</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>IIWeb-07</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ORCID-linked labeled data for evaluating author name disambiguation at scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Owen-Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<date type="published" when="2021-02">Feb 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating author name disambiguation for digital libraries: a case of dblp</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientometrics</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1867" to="1886" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An overview of Microsoft academic service (mas) and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on world wide web</title>
		<meeting>the 24th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="243" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A review of Microsoft academic services for science of science studies</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kanakia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rogahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Big Data</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="2019-12">December 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LightGBM: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">fastcluster: Fast hierarchical, agglomerative clustering routines for R and Python</title>
		<author>
			<persName><forename type="first">D</forename><surname>M?llner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the combination of domain-specific heuristics for author name disambiguation: the nearest cluster method</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Gonc ?alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Laender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ferreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Digital Libraries</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="246" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-I</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
