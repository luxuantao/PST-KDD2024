<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-21">21 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qingyu</forename><surname>Tan</surname></persName>
							<email>qingyu.tan@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruidan</forename><surname>He</surname></persName>
							<email>ruidan.he@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lidong</forename><surname>Bing</surname></persName>
							<email>l.bing@alibaba-inc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Damo</forename><surname>Academy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alibaba</forename><surname>Group</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Document-Level Relation Extraction with Adaptive Focal Loss and Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-21">21 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.10900v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level Relation Extraction (DocRE) is a more challenging task compared to its sentence-level counterpart. It aims to extract relations from multiple sentences at once. In this paper, we propose a semi-supervised framework for DocRE with three novel components. Firstly, we use an axial attention module for learning the interdependency among entity-pairs, which improves the performance on two-hop relations. Secondly, we propose an adaptive focal loss to tackle the class imbalance problem of DocRE. Lastly, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. We conducted experiments on two DocRE datasets. Our model consistently outperforms strong baselines and its performance exceeds the previous SOTA by 1.36 F1 and 1.46 Ign_F1 score on the DocRED leaderboard. 1 * † Qingyu Tan is under the Joint PhD Program between Alibaba and National University of Singapore.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of document-level relation extraction<ref type="foot" target="#foot_0">2</ref> (DocRE) is highly important for information extraction and NLP research. The DocRE task aims to extract relations among multiple entities within a document. The DocRE task is more challenging than its sentence-level counterpart in the following aspects: (1) The complexity of DocRE increases quadratically with the number of entities. If a document contains n entities, classification decisions must be made on n(n − 1) entity pairs and most of them do not contain any relation. (2) Aside from the imbalance of positive and negative examples, the distribution of relation types for the positive entity pairs is also highly imbalanced. Considering the DocRED <ref type="bibr" target="#b23">(Yao et al., 2019)</ref> dataset as an example, there are 96 relation types in total, where the top 10 relations take up 59.4% of all the relation labels. This imbalance significantly increases the difficulty of the document-level RE task.</p><p>Most existing approaches of DocRE leverage dependency information to construct a documentlevel graph <ref type="bibr" target="#b25">(Zeng et al., 2021;</ref><ref type="bibr" target="#b26">Zeng et al., 2020)</ref>, and then use graph neural networks for reasoning. Another popular strand of this field uses transformer-only <ref type="bibr" target="#b16">(Vaswani et al., 2017)</ref> architecture <ref type="bibr" target="#b30">(Zhou et al., 2021;</ref><ref type="bibr" target="#b22">Xu et al., 2021;</ref><ref type="bibr" target="#b27">Zhang et al., 2021)</ref>. Such models are able to achieve state-of-theart performance without explicit graph reasoning, showing that pre-trained language models (PrLMs) are able to implicitly capture long-distance relationships. However, there are three limitations of the existing DocRE methods. Firstly, existing methods mainly focus on the syntactic features from PrLMs while neglecting the interactions between entity pairs. <ref type="bibr" target="#b27">Zhang et al. (2021)</ref> and <ref type="bibr" target="#b5">Li et al. (2021)</ref> have used CNN structure to encode the interaction between entity pairs, but CNN structure cannot capture all the elements within the two-hop reasoning paths. Secondly, there is no prior work that explicitly tackles the class-imbalance problem for DocRE. Existing works <ref type="bibr" target="#b30">(Zhou et al., 2021;</ref><ref type="bibr" target="#b27">Zhang et al., 2021;</ref><ref type="bibr" target="#b26">Zeng et al., 2020)</ref> only focus on threshold learning for balancing the positive and negative examples, but the class-imbalance problem within positive examples is not addressed. Lastly, there are very few works discussing the method of adapting distantly supervised data for the DocRE task. <ref type="bibr" target="#b22">Xu et al. (2021)</ref> has shown that distantly supervised data is able to improve the performance of document-level relation extraction. However, it only uses the distantly supervised data to pre-train the RE model in a naive manner.</p><p>To overcome the limitations of existing works, we propose a semi-supervised learning framework for document-level relation extraction. Firstly, to improve the reasoning for two-hop relations, we propose to use an axial attention module as feature extractor. This module enables us to attend to elements that are within two-hop logical paths and capture the interdependency among the relation triplets. Secondly, we propose Adaptive Focal Loss to address the imbalanced label distribution problem. The proposed loss function encourages the long-tail classes to contribute more to the overall loss. Lastly, we use knowledge distillation to overcome the differences between the annotated data and the distantly supervised data. Specifically, we first train a teacher model with a small amount of human annotated data. The teacher model will then be used to generate predictions on a large amount of distantly supervised data. The generated predictions are used as soft labels for pre-training our student model. Finally, the pre-trained student model is further fine-tuned on the human annotated data.</p><p>We conducted experiments on two datasets -the DocRED <ref type="bibr" target="#b23">(Yao et al., 2019)</ref> dataset and the Ha-cRED <ref type="bibr" target="#b1">(Cheng et al., 2021)</ref> dataset. Experimental results show that our model consistently outperforms competitive baselines. Moreover, our model significantly outperforms the existing state-of-theart SSAN-Adapt <ref type="bibr" target="#b22">(Xu et al., 2021)</ref> on the DocRED leaderboard by 1.36 in F1 score and 1.46 in Ign_F1 score. <ref type="foot" target="#foot_1">3</ref> Besides, we provide a thorough ablation study and error analysis to identify the bottleneck of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head><p>In this section, we describe the task formulation of document-level relation classification. Given a document D that contains a set of entities {e i } n i=1 , the document-level relation extraction task is to predict the relation types between entity pairs (e s , e o ) s,o∈{1...n},s =o , where the subscripts of e s and e o refer to subject and object. The set of relations is defined as R ∪ {NR}, where NR stands for no relation. An entity may occur multiple times in a document, thus for each entity e i , there can be multiple mentions {m i j } Ne i j=1 . If no relation exists between the entities in the pair (e s , e o ), it will be labeled as NR. During test time, the relation labels for all entity pairs (e s , e o ) s,o∈{1...n},s =o will be predicted. Essentially, this is a multi-label classification problem, as there can be multiple relations between e s and e o .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model Architecture</head><p>As shown in Figure <ref type="figure">1</ref>, our semi-supervised learning framework mainly consists of three parts: (1) representation learning; (2) adaptive focal loss; and</p><p>(3) knowledge distillation for distant supervision pretraining. For representation learning, we first extract the contextual representation for each entitypair by a pre-trained language model. The entity pair representations will be further enhanced by the axial attention module, which will encode the inter-dependent information between entity pairs. We then use a feedforward neural network (FFN) classifier to obtain the logits and compute their losses. We use our proposed adaptive focal loss to better learn from long-tail classes. Finally, we use knowledge distillation to overcome the differences between human annotated data and distantly supervised data. Specifically, we train a teacher model with the annotated data and use its output as soft labels. We then pre-train a student model based on the soft labels and the distant labels. The pretrained student model will be fine-tuned again with the annotated data. We will describe the details for each part in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Representation Learning Entity Representation</head><p>We use a pretrained language model as the encoder. For a document D of length l, we have D = [x t ] l t=1 , where x t is the word at location t. Following prior works for relation classification, we use special token markers to represent entities. The entity mentions will be marked by a special token "*" at the start and end position. We then use a pre-trained language model (PrLM) to obtain the contextualized embeddings H of this document.</p><formula xml:id="formula_0">H = P rLM ([x 1 , ..., x l ]) = [h 1 , ..., h l ]) (1)</formula><p>where H ∈ R l×d and d is the hidden dimension of the PrLM. If the document length exceeds the maximum position of the PrLM, the document will be encoded as multiple overlapping chunks, and the contextualized embeddings of the overlapping chunks will be averaged. We take the embedding of the special token "*" at the start of the mention as its embedding, which is denoted as h m j . Then, for each entity e i with mentions {m i j }</p><p>Ne i j=1 , where N e i is the number of mentions for entity e i , its global representation is obtained by logsumexp pooling:</p><formula xml:id="formula_1">h e i = log Ne i j=1 exp(h m j )<label>(2)</label></formula><p>where h e i ∈ R d is the aggregated feature of e i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-enhanced Entity Representation</head><p>As prior works <ref type="bibr" target="#b22">(Xu et al., 2021;</ref><ref type="bibr" target="#b10">Peng et al., 2020)</ref> have shown that contextual information is crucial for the relation classification task, our model also adapts contextual pooling method from <ref type="bibr" target="#b30">Zhou et al. (2021)</ref>. For each entity e i , we first aggregate the attention output for its mentions by mean pooling</p><formula xml:id="formula_2">A e i = Ne i j=1 (a m j )</formula><p>, where a m j ∈ R H×l is the the self-attention weight at the position of mention m j , H is the number of attention heads, and l is the document length. Then the context query is calculated as:</p><formula xml:id="formula_3">q (s,o) = H i=1 (A i es • A i eo ) (3) c (s,o) = H q (s,o)<label>(4)</label></formula><p>where A es ∈ R H×l is the aggregated attention output for entity e s , likewise for e o . q (s,o) ∈ R l is the mean-pooled attention weight for entity pair (e s , e o ) and H ∈ R l×d is the contextual embedding of the whole document. Then the context vector c (s,o) ∈ R d is fused with the entity representations.</p><formula xml:id="formula_4">z s = tanh(W s h es + W c c (s,o) )<label>(5)</label></formula><p>where z s ∈ R d is the context-enhanced representation of subject s for entity pair (e s , e o ). We obtain the object representation z o in the same manner.</p><p>Entity Pair Representation Following Zhou et al. ( <ref type="formula">2021</ref>), we use a grouped bilinear function for feature combination. The entity embedding z s will first will be split into k equal-sized groups, such that</p><formula xml:id="formula_5">z s = [z 1 s , z 2 s , ..., z k s ].</formula><p>We perform the same splitting for z o . The value g (s,o) i at each dimension of our entity pair representation is obtained by:</p><formula xml:id="formula_6">g (s,o) i = k j=1 (z j s W j g i z j o ) + b i g (s,o) = [g (s,o) 1 , g (s,o) 2 , ..., g (s,o) d ]<label>(6)</label></formula><p>where W j g i ∈ R d/k×d/k , for i = 1, ..., d, j = 1, ..., k, is the weight matrix for dimension i. b i is a scalar bias of dimension i. g (s,o) ∈ R d is our final entity pair representation.</p><p>For a given document D with n entities, we need to classify n(n − 1) number of entity pair permutations. To help us encode all the entity pairs and their positions, we used an R n×n×d matrix G to represent all the entity pairs of document D, and the diagonal of the n × n index is neglected during training and inference.</p><p>Axial Attention-Enhanced Entity Pair Representation Instead of using only head and tail embedding for relation classification, we propose to use two-hop attention to encode the axial neighboring information of each entity pair (e s , e o ) representation. Although there are prior works that use Convolution Neural Networks (CNNs) to encode the neighbor information for relation classification <ref type="bibr" target="#b27">(Zhang et al., 2021)</ref>, we believe that attending to the axial elements is more effective and intuitive. Given an n × n entity table, for entity pair (e s , e o ), attending to its axial elements corresponds to attending to elements that are either (e s , e i ) or (e i , e o ). That is, if a two-hop relation (e s , e o ) can be dissected into a path (e s , e i ) and (e i , e o ), then the most informative neighbors for classifying (e s , e o ) are the one-hop candidates that share e s or e o with this entity pair. The axial attention is simply computed by self-attention along the height axis and the width axis, and each computation along the axes is followed by a residual connection. For the cell (e s , e o ), we have:</p><formula xml:id="formula_7">r (s,o) w = r (s,o) h + p∈1..n sof tmaxp(q T (s,o) k (s,p) )v (s,p) r (s,o) h = g (s,o) + p∈1...n sof tmaxp(q T (s,o) k (p,o) )v (p,o)<label>(7)</label></formula><p>where we denote query q (i,j) = W Q g (i,j) , key j) , and value v (i,j) = W V g (i,j) , which are all linear projections of the entity pair representation g at position (i, j). W Q ∈ R d×d , W K ∈ R d×d , and W V ∈ R d×d are all learnable weight matrices. The output of the axial attention module is r</p><formula xml:id="formula_8">k (i,j) = W K g (i,</formula><formula xml:id="formula_9">(s,o) w ∈ R d .</formula><p>The sof tmax p function denotes a softmax function that applies to all possible p = (i, j) positions. The formulation of this mechanism resembles <ref type="bibr" target="#b20">Wang et al. (2020)</ref>. However, our motivation is different, as <ref type="bibr" target="#b20">Wang et al. (2020)</ref> aim to use this mechanism to reduce the computational complexity of semantic segmentation, whereas our motivation is to attend to the one-hop neighbors for the two-hop relation triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Adaptive Focal Loss</head><p>Finally, we have a linear layer for predicting relations:</p><formula xml:id="formula_10">l (s,o) = W l r (s,o) w + b l (8)</formula><p>where l (s,o) ∈ R c denotes the output logits for all relations, W l ∈ R d×c is the weight matrix that maps the relation embedding to the logit of each class and c is the number of classes.</p><p>Our relation extraction problem is essentially a multi-label classification problem. Traditionally, binary cross-entropy (BCE) loss is used to tackle this problem. However, this method relies on a global probability threshold for inference. Recently Adaptive Thresholding Loss (ATL, <ref type="bibr" target="#b30">Zhou et al., 2021)</ref> has been proposed for multi-label classification. Instead of using a global probability threshold for all examples, ATL introduced a special class T H as the adaptive threshold value for each example. For each entity pair (e s , e o ), the classes whose logits are larger than the T H class logit will be predicted as positive classes, and the rest will be predicted as negative classes.</p><p>We propose Adaptive Focal Loss (AFL) as an enhancement to ATL for long-tail classes. Our loss consists of two parts, the first part is for positive classes and the second part is for negative classes.</p><p>During training, the label space is divided into two subsets: positive class subset P T and negative class subset N T . The positive class subset P T contains the relations that exist in entity pair (e s , e o ), and if there is no relation between (e s , e o ), P T is empty (P T = ∅). The negative subset N T , on the other hand, contains the relation classes that do not belong to the positive classes, N T = R \ P T . The probability of each positive class is computed as:</p><formula xml:id="formula_11">P (r i |e s , e o ) = exp(l (s,o) r i ) exp(l (s,o) r i ) + exp(l (s,o) T H ) (9)</formula><p>where the logit of r i is ranked with the logit of threshold class T H individually. This is different from the original ATL, where all positive logits are ranked together with a softmax function. For simplicity, P (r i |e s , e o ) is denoted as P (r i ) in this section, because we are only discussing (e s , e o ). For the negative classes, we use their logits to compute the probability of the T H class:</p><formula xml:id="formula_12">P (rT H |es, eo) = exp(l (s,o) r T H ) r j ∈N T ∪{T H} exp(l (s,o) r j )<label>(10)</label></formula><p>Similarly, P (r T H |e s , e o ) is referred to as P (r T H ) in the remainder of this section. Since the distribution of the positive labels is highly imbalanced, we leverage the idea of focal loss <ref type="bibr" target="#b6">(Lin et al., 2017)</ref> for balancing the logits of the positive classes. We have our loss function as:</p><formula xml:id="formula_13">LRE = r i ∈P T (1−P (ri)) γ log(P (ri))+log(P (rT H )) (11)</formula><p>where γ is a hyper-parameter. Our loss is designed to focus more on the low-confidence classes. If P (r i ) is low, the loss contribution from the relevant class will be higher, which enables a better optimization for long-tail classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Knowledge Distillation for Distant Supervison</head><p>In this section, we describe how we utilize the distantly supervised data in a more effective manner.</p><p>The distantly supervised data included in the Do-cRed dataset <ref type="bibr" target="#b23">(Yao et al., 2019)</ref> was obtained by performing entity linking on the Wikidata Knowledge Base <ref type="bibr" target="#b18">(Vrandečić and Krötzsch, 2014)</ref> and the Wikipedia data dump. It is shown that pre-training from the distantly supervised data is beneficial for document-level relation extraction <ref type="bibr" target="#b22">(Xu et al., 2021)</ref>. However, prior work only adapts the distantly supervised data in a naive manner. The key challenge for the distant supervision adaptation is to overcome the differences between probability distributions of the distantly supervised data and the human annotated data. We compare two strategies for adapting the distantly supervised data.</p><p>Naive Adaptation Adopting from <ref type="bibr" target="#b22">(Xu et al., 2021)</ref>, this method first pretrains the model with the distantly supervised data with the relation extraction loss L RE (Eqn. 11), and then the model is fine-tuned on the human-annotated data with the same objective. We denote this method as Naive Adaptation (NA).</p><p>Knowledge Distillation To further utilize the annotated data, we use a relation classification model trained on the human-annotated data (#Train in Table 1) as the teacher model. The teacher model is used to generate soft labels on the distantly supervised data. Specifically, the distantly supervised data is fed into the teacher model and the predicted logits will be the soft labels used for training the student model. The student model has the same configuration as the teacher model, but is trained with two signals simultaneously. The first signal is the supervision from the hard labels of the distantly supervised data and the second is from the predicted soft labels. We denote the loss computed on the hard labels as L RE and the knowledge distillation loss computed on the soft labels as L KD . We use mean squared error (MSE) as the knowledge distillation loss function:</p><formula xml:id="formula_14">L KD = M SE(l (s,o) S , l (s,o) T )<label>(12)</label></formula><p>where l</p><formula xml:id="formula_15">(s,o) S</formula><p>denotes the predicted logits of the student model and l (s,o) T is the prediction of the teacher model. The student model is further fine-tuned with human-annotated data (#Train in Table <ref type="table">1</ref>) after it has been pre-trained on the distantly supervised data. The overall loss of pre-training with distantly supervised data is computed as: L = L KD + L RE (13) We denote this method as KD in our main experimental results section. Besides the MSE loss, we also compare different adaptation methods, such as KL-Divergence, in section 3.6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Implementation Details</head><p>We implemented our model with the PyTorch version of the Huggingface Transformers <ref type="bibr" target="#b21">(Wolf et al., 2020)</ref>. For experiments on DocRED, we experimented with Roberta-large <ref type="bibr" target="#b7">(Liu et al., 2019)</ref> and Bert-base <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> as our document encoder respectively. For experiments on HacRED, we use XLM-R base <ref type="bibr" target="#b2">(Conneau et al., 2020)</ref> as the document encoder. AdamW <ref type="bibr" target="#b8">(Loshchilov and Hutter, 2019</ref>) is used as the optimizer. At the knowledge distillation stage, we trained the model with the learning rate set to 1e-5 for 2 epochs. Warmup is applied on the initial 6% steps. The dropout rates between transformer layers are set to 0.1 and the maximum gradient norm is clipped at 1.0. During the fine-tuning stage, the learning rate is set to 1e-6 and we train the model for 10 epochs. We performed grid search for γ ∈ [0, 0.5, 1.0, 1.5, 2.0] and set it to 0.5. NVIDIA V100 GPU with 32 GB memory. The main evaluation metrics are Ign_F1 and F1 score following <ref type="bibr" target="#b23">Yao et al. (2019)</ref>, where Ign_F1 refers to the F1 score that ignores the triples that appear in the annotated training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compared Methods</head><p>We The GAIN <ref type="bibr" target="#b26">(Zeng et al., 2020)</ref> model adds a graph neural network on top of a pre-trained language model, constructs a document-level graph for each example, and uses the graphical structure to extract relations. SIRE <ref type="bibr" target="#b25">(Zeng et al., 2021)</ref> uses two encoders for different types of relation -a sentencelevel encoder to extract intra-sentence relations and a document encoder to extract inter-sentence relations. ATLOP <ref type="bibr" target="#b30">(Zhou et al., 2021)</ref> is purely based on the transformer architecture and a novel adaptive thresholding loss to deal with the multi-label problem for DocRE. Besides, it also fuses the contextual information with the aggregated attention weights for each entity. The DocuNet <ref type="bibr" target="#b27">(Zhang et al., 2021)</ref> model treats the relation extraction task in a similar way as semantic segmentation in computer vision. We also conducted an experiment that pretrained the ATLOP-Rb-l model with distantly supervised data, as this model is the best model by our reproduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Main Results</head><p>Our main results for the DocRED dataset are shown in Table <ref type="table" target="#tab_2">2</ref>. Knowledge distillation is able to significantly improve the performance of our model.  The experiment results for the HacRED dataset are shown in Table <ref type="table" target="#tab_4">3</ref>. The main difference of our method with the ATLOP baseline is the Adaptive Focal Loss and the Axial Attention Module. Our proposed method is able to exceed the ATLOP baseline by 1.12 F1. Besides the performance of the models, it is worth noting that for each method, the absolute performance of HacRED is significantly higher than its performance on DocRED. This is counter-intuitive as HacRED focuses on the hard relations whereas DocRED is more general. This can be caused by the following: 1) The human annotated training instances of the HacRED dataset are significantly more than DocRED, leading to better generalization performance. 2) Even though HacRED claims it focuses on the hard cases for relation extraction, it only has 27 classes, and the relation type distribution within the HacRED dataset is more balanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>We first separate our label space into two subsets. The first subset consists of the 10 most frequent labels, accounting for 59.4% of the positive relations in the training data. The second subset is denoted as the long-tail labels, which includes the rest of the 86 relations (the total label space is 97 and there is one T H class). Since our Adaptive Focal loss function is mainly designed for improving the performance on the less frequent classes, we show the ablation study by frequent and long-tail classes in Table <ref type="table">4</ref>. When we change the AFL loss to con- We also provide an ablation study on the multihop relations in Table <ref type="table" target="#tab_5">5</ref>. We use the same evaluation method for multi-hop relations as <ref type="bibr" target="#b26">Zeng et al. (2020)</ref>. This evaluation method ignores all the onehop relation triples. Our axial attention module effectively improves Infer-F1 by 1.67, while its improvement for overall performance is only 0.63.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Comparison of Adaptation Methods</head><p>In this section, we directly compare the knowledge adaptation methods on the development set of DocRED (Table <ref type="table" target="#tab_6">6</ref>). We mainly compare three methods for adaptation: 1) Naive Adaptation (NA), 2) KD KL knowledge distillation with the KL divergence loss and 3) KD M SE with mean squared error loss. The adaptation performance on the development set is positively correlated with the per- formance of downstream fine-tuning. In the distant adaptation setting, our best method KD M SE is able to outperform NA by 3.07 F1 and KD KL by 0.77 F1. Similar performance differences are observed in the continue-trained setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Error Analysis</head><p>Even though our final model significantly outperforms the previous state of the art on the Do-cRED leaderboard, the absolute performance of our model still does not match human performance.</p><p>In this section, we provide a detailed error analysis of our model on the development set of DocRED.  We first construct the union of our model's predictions and the ground truth triples (without NR label). Then, we categorize the union into four categories: (1) Correct (C), where prediction triples are in the ground truth. ( <ref type="formula" target="#formula_1">2</ref>) Wrong (W), where the predicted head entity and tail entity are in the ground truth but the predicted relation is wrong.</p><p>(3) Missed (MS), where the model predicts no relation for a pair of head entity and tail entity with some relation in the ground truth. (4) More (MR), where the model predicts an extraneous relation for a pair of head entity and tail entity not related in the ground truth. From Table <ref type="table" target="#tab_8">7</ref>, we observe that the error percentage of the W category is very small. This indicates that for a pair of head entity and tail entity with some relation in the ground truth, and when our model predicts that there is a relation between these two entities, it is able to predict the correct relation rather accurately. However, we observe that most of our errors are under the MR and MS categories, and their counts are about the same. To better understand the performance bottleneck of the document-level RE task, we evaluate our model on a simplified subtask (Table <ref type="table" target="#tab_9">8</ref>). This subtask is binary classification, i.e., to determine whether two entities are related or not, and it is denoted as Binary Labels. In this subtask, we only care about predicting correctly that there is some relation between a head entity and a tail entity, but not what the exact relation is among the 97 relation classes. The performance on this simplified task is 68.64 F1 score, which is only marginally higher than the original F1 score of 67.12. This may be due to incomplete annotation of the two document-level relation extraction datasets, and we will illustrate this hypothesis in Figure <ref type="figure" target="#fig_2">2</ref>   In Figure <ref type="figure" target="#fig_2">2</ref>, we show an example document from the dev set of DocRED and its predictions. We observe that many triples in the MR category are factually correct. That is, some of the pairs of entities are truly related but are labeled as NR throughout the dataset. For instance, from the ground truth, we can see that Labour Party and Hol are all entities from country Norway. Similarly, Nordland and Vestvågøy are all in Norway, but their relations with Norway are not present in the ground truth triples. Therefore, when our model predicts these triples, its performance would be unfairly penal-ized during evaluation. This observation indicates that there are some incomplete annotations in the DocRED dataset. However, this is not the focus of this paper and we would like to leave this as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Early works on relation extraction mainly focused on sentence-level RE <ref type="bibr" target="#b29">(Zhang et al., 2017;</ref><ref type="bibr" target="#b0">Baldini Soares et al., 2019;</ref><ref type="bibr" target="#b10">Peng et al., 2020)</ref>. However, prior works have shown that a large number of relations can only be extracted from multiple sentences <ref type="bibr" target="#b17">(Verga et al., 2018;</ref><ref type="bibr" target="#b23">Yao et al., 2019;</ref><ref type="bibr" target="#b1">Cheng et al., 2021)</ref>. Various methods have been proposed to tackle document-level relation extraction (DocRE). Graph neural networks (GNNs; <ref type="bibr" target="#b14">Scarselli et al., 2008)</ref> have been widely used for the DocRE task. <ref type="bibr" target="#b13">Quirk and Poon (2017)</ref> used words as nodes and dependency information as edges to construct document-level graphs. This graph will be used to extract features for each entity pair. Later works extended this idea by applying different GNN architectures <ref type="bibr" target="#b11">(Peng et al., 2017;</ref><ref type="bibr" target="#b17">Verga et al., 2018;</ref><ref type="bibr">Christopoulou et al., 2019;</ref><ref type="bibr" target="#b9">Nan et al., 2020;</ref><ref type="bibr" target="#b28">Zhang et al., 2018;</ref><ref type="bibr" target="#b26">Zeng et al., 2020)</ref>. In particular, <ref type="bibr" target="#b9">Nan et al. (2020)</ref> proposed the latent stucture refinement (LSR) model, which used structured attention to induce the document-level graph. <ref type="bibr" target="#b26">Zeng et al. (2020)</ref> constructed the document-level graph by entity-mention nodes and sentence edges. Besides the graph-based methods, transformer-only architectures have also proven to be highly effective for the DocRE task <ref type="bibr" target="#b15">(Tang et al., 2020;</ref><ref type="bibr" target="#b30">Zhou et al., 2021)</ref>. Specifically, <ref type="bibr" target="#b30">Zhou et al. (2021)</ref> proposed adaptive thresholding loss to tackle the multi-label classification problem in DocRE.</p><p>On the other hand, learning from distant supervision is another important problem for relation extraction. <ref type="bibr" target="#b12">Qin et al. (2018)</ref> used generative adversarial training for selecting informative examples and <ref type="bibr" target="#b4">Feng et al. (2018)</ref> used reinforcement learning to achieve the same goal. However, there are no existing works that jointly learn from annotated data and distant data. To this end, this paper is the first to overcome the differences between the human annotated and distantly supervised data. Moreover, this paper also tackles the under-explored class imbalance problem and the two-hop logical reasoning problem with novel solutions to the shortcomings of existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we have proposed a novel framework for document-level relation extraction, based on knowledge distillation, axial attention, and adaptive focal loss. Our proposed method is able to significantly outperform the previous state of the art on the DocRED leaderboard. Besides, we also conducted a thorough ablation study and error analysis to identify the bottleneck of the document-level relation extraction task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>denote Bert-base and Bert-large encoders as Bb and B-l. The Roberta-large model is denoted as Rb-l. We compare our model with the state-of-theart systems on the DocRED leaderboard as well as strong baselines by our own implementation. They are the following models: Wang et al. (2019) has proposed to fine-tune BERT for document-level RE with a two-step process (Two-stage-B-b). The Bert model needs to classify whether the two entities have relation and then classify their relation if the first step is positive. The Coref-Rb-l (Ye et al., 2020) uses a co-reference module to aggregate the mention representations of the same entity. The SSAN (Xu et al., 2021) model utilizes cooccurrence information between entity mentions, leverages distantly supervised data for pretraining, and achieves the state of the art on the DocRED leaderboard. Since their best model SSAN-Adapt is equivalent to naive adaptation in our work, we denote it as SSAN-NA-Rb-l in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>"</head><label></label><figDesc>Eivind Bolle ( 13 October 1923 -10 June 2012 ) was a Norwegian politician for the Labour Party. He was born in Hol. He was elected to the Norwegian Parliament from Nordland in 1973. ... On the local level he was a member of Hol municipality council from 1959 to 1963 , and later in Hol 's successor municipality Vestvågøy. He served as mayor from 1971 to 1973 , during which term he was also a member of Nordland county council ..." More: (Nordland, country, Norwegian), (Vestvågøy, country, Norwegian),... Correct: (Labour Party, country, Norwegian), (Hol, country, Norwegian),...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example output of our model on the DocRED dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Model architecture of our DocRE system. We show the axial attention region for the entity pair (e 3 , e 6 ).</figDesc><table><row><cell>[bs=1, seq_len]</cell><cell>-</cell><cell cols="3">-Pairwise embeds: Mention loss -Restrict to opinion-target pairs</cell><cell></cell><cell></cell><cell></cell><cell>Φ triple dfd dfd</cell><cell>Invalid C</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>The</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>are tasty and</cell></row><row><cell cols="3">Input Document Entity-pair Representation</cell><cell></cell><cell cols="2">Axial Attention</cell><cell></cell><cell cols="2">Loss Functions</cell></row><row><cell></cell><cell></cell><cell cols="3">Representation Learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>e 1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>e 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AFL</cell></row><row><cell>BERT</cell><cell></cell><cell></cell><cell>e 3 e 4 e 5 e 6</cell><cell></cell><cell></cell><cell></cell><cell>FFN Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell>e 7 e 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>KD Loss</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">e 1 e 2 e 3 e 4 e 5 e 6 e 7 e 8</cell><cell></cell></row><row><cell>Entity Embedding</cell><cell cols="3">Context Embedding</cell><cell>Axial Attention</cell><cell></cell><cell>Related Entity Pair</cell><cell cols="2">Student Logits</cell><cell>Teacher Logits</cell></row><row><cell>Figure 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>&lt;Null&gt;</cell><cell cols="3">Korean dishes, tasty</cell><cell></cell><cell></cell><cell cols="2">Korean dishes, tasty</cell></row><row><cell>&lt;Target&gt;</cell><cell></cell><cell>The</cell><cell cols="2">Korean dishes Korean dishes, costly</cell><cell></cell><cell>tasty</cell><cell></cell><cell>costly</cell><cell>Korean dishes, costly</cell></row><row><cell>&lt;Opinion&gt;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FFNN</cell><cell></cell></row><row><cell>The</cell><cell cols="3">The Korean dishes</cell><cell cols="2">Korean dishes</cell><cell>tasty</cell><cell cols="2">are tasty but costly</cell><cell>costly</cell><cell>.</cell></row><row><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell></cell><cell>+</cell><cell>+</cell><cell>+</cell></row><row><cell></cell><cell>The</cell><cell></cell><cell>Korean</cell><cell>dishes</cell><cell>are</cell><cell>tasty</cell><cell>but</cell><cell>costly</cell><cell>.</cell></row></table><note>[1] Dirty Diana " is a song by American artist Michael Jackson . It is the ninth track on Jackson's seventh studio album , Bad . [2] The song was released by Epic Records on April 18 , 1988 as the fifth single from the album . [3] It presents a harder rock sound similar to " Beat It " from Thriller ( 1982 ) and a guitar solo played by Steve Stevens . [4] " Dirty Diana " was written and co -produced by Jackson , and produced by Quincy Jones. …</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results for the DocRED dataset. The reported metrics are F1 score and Ign_F1. We report the average of five random runs for the development set and the best checkpoint is used for the leaderboard submission for the test results. Results with</figDesc><table><row><cell>Dev</cell><cell>Test</cell></row></table><note>Our model is trained on a single * are obtained by our reproduction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Experimental results on HacRED dev set. Results with * are implemented by us. All experiments used XLM-R-base as the encoder.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study for the Infer-F1 relation triples on the development set of DocRED.</figDesc><table><row><cell></cell><cell cols="4">Frequent Long-tail Overall</cell></row><row><cell></cell><cell>F1</cell><cell></cell><cell>F1</cell><cell>F1</cell></row><row><cell>ATLOP-Rb-l</cell><cell>70.93</cell><cell></cell><cell>50.01</cell><cell>63.12</cell></row><row><cell>Ours-Rb-l</cell><cell>71.26</cell><cell></cell><cell>51.97</cell><cell>64.19</cell></row><row><cell>w/o Axial</cell><cell>70.86</cell><cell></cell><cell>50.77</cell><cell>63.56</cell></row><row><cell>w/o AFL w ATL</cell><cell>70.94</cell><cell></cell><cell>50.86</cell><cell>63.67</cell></row><row><cell cols="2">With Distant Supervision</cell><cell></cell><cell></cell></row><row><cell>ATLOP-NA-Rb-l</cell><cell>73.26</cell><cell></cell><cell>52.39</cell><cell>65.33</cell></row><row><cell>Ours-KD-Rb-l</cell><cell>74.15</cell><cell></cell><cell>56.51</cell><cell>67.12</cell></row><row><cell>w/o Axial</cell><cell>73.52</cell><cell></cell><cell>54.96</cell><cell>66.36</cell></row><row><cell>w/o AFL w ATL</cell><cell>73.50</cell><cell></cell><cell>54.73</cell><cell>66.23</cell></row><row><cell cols="5">Table 4: Experiment results for frequent and long-tail</cell></row><row><cell cols="5">type relations. Frequent types refer to the most popular</cell></row><row><cell cols="5">10 relation types, and long-tail relations refer to the rest</cell></row><row><cell>of the 86 relations.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ventional Adaptive Thresholding Loss (Zhou et al.,</cell></row><row><cell cols="5">2021), the overall performance with KD drops by</cell></row><row><cell cols="5">0.89 F1, and the F1 score for the frequent labels</cell></row><row><cell cols="5">only drops by 0.65. Meanwhile, the long-tail labels'</cell></row><row><cell cols="5">F1 drops by 1.78, which is significantly higher than</cell></row><row><cell cols="5">the drop in overall performance and frequent perfor-</cell></row><row><cell cols="5">mance. This indicates that our Adaptive Focal Loss</cell></row><row><cell cols="5">is able to balance the weight of the frequent classes</cell></row><row><cell cols="5">and infrequent classes. The axial attention module</cell></row><row><cell cols="5">is also more beneficial for the long-tail classes than</cell></row><row><cell cols="5">the frequent classes, which shows that our model's</cell></row><row><cell cols="5">performance on the frequent classes is saturated.</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell cols="2">Infer-F1</cell></row><row><cell cols="3">GAIN-B-b 38.71 59.45</cell><cell>46.89</cell></row><row><cell cols="3">Ours-Rb-l 42.15 61.56</cell><cell>50.04</cell></row><row><cell>w/o Axial</cell><cell cols="2">40.26 60.60</cell><cell>48.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Development set performance of different knowledge adaptation methods for DocRED.</figDesc><table><row><cell cols="2">Distant Adaptation Ign_F1 F1</cell></row><row><cell>NA</cell><cell>52.29 54.67</cell></row><row><cell>KD KL</cell><cell>53.89 56.97</cell></row><row><cell>KD M SE</cell><cell>55.28 57.74</cell></row><row><cell>Continue-trained</cell><cell>Ign_F1 F1</cell></row><row><cell>NA</cell><cell>63.38 65.64</cell></row><row><cell>KD KL</cell><cell>64.42 66.24</cell></row><row><cell>KD M SE</cell><cell>65.27 67.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Statistics of our error distribution. The final evaluation score is evaluated on r ∈ R triples, hence the correct predictions of NR are ignored when calculating the final scores.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>. Performance breakdown on the DocRED dev set.</figDesc><table><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Binary Labels</cell><cell cols="3">68.51 68.78 68.64</cell></row><row><cell cols="4">Original Labels 67.10 67.13 67.12</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In this work, the task of relation extraction presumes that entities are given.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Refer to https://competitions.codalab. org/competitions/20717, where our model is named KD-Roberta.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>We would like to thank the anonymous reviewers for their insightful feedback and comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">HacRED: A largescale relation extraction dataset toward hard cases in practical applications</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL. Fenia Christopoulou, Makoto Miwa, and Sophia Ananiadou</title>
				<imprint>
			<date type="published" when="2019">2021. 2019</date>
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mrn: A locally and globally mention-based reasoning network for documentlevel relation extraction</title>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from context or names? An empirical study on neural relation extraction</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DSGAN: Generative adversarial training for distant supervision relation extraction</title>
		<author>
			<persName><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
				<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HIN: hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD</title>
				<meeting>KDD</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
				<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
				<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">Denny</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Krötzsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Communications of the ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fine-tune BERT for DocRED with two-step process</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Axial-deeplab: Stand-alone axial-attention for panoptic segmentation</title>
		<author>
			<persName><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
				<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP: System Demonstrations</title>
				<meeting>EMNLP: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DocRED: a large-scale document-level relation extraction dataset</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
				<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Coreferential reasoning learning for language representation</title>
		<author>
			<persName><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SIRE: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/551</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
				<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
				<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
				<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
