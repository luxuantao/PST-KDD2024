<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tiny but Mighty: Designing and Realizing Scalable Latency Tolerance for Manycore SoCs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marcelo</forename><surname>Orenes-Vera</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Aragón</surname></persName>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Margaret</forename><forename type="middle">2022</forename><surname>Martonosi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><surname>Balkind</surname></persName>
						</author>
						<author>
							<persName><surname>Tiny</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Princeton University Jonathan Balkind UC Santa Barbara</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Murcia</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Princeton University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">ISCA &apos;22</orgName>
								<address>
									<addrLine>June 18-22</addrLine>
									<postCode>2022</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tiny but Mighty: Designing and Realizing Scalable Latency Tolerance for Manycore SoCs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3470496.3527400</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>memory</term>
					<term>latency tolerance</term>
					<term>decoupling</term>
					<term>modular RTL</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern computing systems employ significant heterogeneity and specialization to meet performance targets at manageable power. However, memory latency bottlenecks remain problematic, particularly for sparse neural network and graph analytic applications where indirect memory accesses (IMAs) challenge the memory hierarchy.</p><p>Decades of prior art have proposed hardware and software mechanisms to mitigate IMA latency, but they fail to analyze real-chip considerations, especially when used in SoCs and manycores. In this paper, we revisit many of these techniques while taking into account manycore integration and verification.</p><p>We present the first system implementation of latency tolerance hardware that provides significant speedups without requiring any memory hierarchy or processor tile modifications. This is achieved through a Memory Access Parallel-Load Engine (MAPLE), integrated through the Network-on-Chip (NoC) in a scalable manner. Our hardware-software co-design allows programs to perform longlatency memory accesses asynchronously from the core, avoiding pipeline stalls, and enabling greater memory parallelism (MLP).</p><p>In April 2021 we taped out a manycore chip that includes tens of MAPLE instances for efficient data supply. MAPLE demonstrates a full RTL implementation of out-of-core latency-mitigation hardware, with virtual memory support and automated compilation targetting it. This paper evaluates MAPLE integrated with a dualcore FPGA prototype running applications with full SMP Linux, and demonstrates geomean speedups of 2.35× and 2.27× over softwarebased prefetching and decoupling, respectively. Compared to stateof-the-art hardware, it provides geomean speedups of 1.82× and 1.72× over prefetching and decoupling techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Computer systems organization → Multicore architectures; Reconfigurable computing; Heterogeneous (hybrid) systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The "memory wall" becomes even more challenging in acceleratorrich systems. From the perspective of Amdahl's Law, as specialized accelerators speed up computation, memory operations that supply data represent a bigger portion of the runtime <ref type="bibr" target="#b52">[53]</ref>. Workloads with cache-unfriendly irregular memory access patterns are particularly bottlenecked, such as those in the domains of graph analytics and sparse linear algebra. Their irregularity arises from Indirect Memory Accesses (IMAs) that require many off-chip, long-latency accesses to DRAM. Software optimizations to reduce memory latency often require increased code complexity and reduced portability, and can incur overheads that limit performance gains <ref type="bibr" target="#b30">[31]</ref>. Thus, hardware innovations are necessary.</p><p>Table <ref type="table" target="#tab_1">1</ref> shows much of the 40 years of prior work in latency mitigation of IMAs. One might think that these hardware innovations are easy to include in real chips, but that is often not the case due to complex core or cache modifications <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b55">56]</ref>, the need for new ISA instructions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref>, or excessive area overheads per core <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b61">62]</ref>. Moreover, deep microarchitecture changes are hard to make in practice because of the verification burden. For SoC integration, it is often easier to incorporate off-the-shelf cores.</p><p>These observations are not abstract for us; our exploration into the prior work in latency tolerance started with the goal of fabricating a chip to efficiently process sparse algebra and graph analytic workloads. Prior work has leveraged SMT and beefy OoO to hoist accesses and thus mitigate the latency of IMAs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref>. However, we choose to use many slim in-order cores instead of a few out-of-order (OoO) cores, because latter are generally not effective for irregular memory accesses without additional specialization. Inorder cores with specialized hardware to handle irregular accesses offer better performance density for our application domain <ref type="bibr" target="#b55">[56]</ref>.</p><p>Previously proposed latency tolerance techniques fall short analyzing trade-offs that arise from manycore integration or real-chip Figure <ref type="figure">1</ref>: MAPLE is an area-efficient alternative to fetch irregular memory patterns. Each MAPLE can supply data for up to 8 cores in parallel. For clarity we depict the two scenarios of cores using MAPLE separately. The arrows are MAPLE API operations (off-the-shelf cores can target MAPLE using Memory-Mapped loads and stores). In decoupling mode, Core 1 runs ahead of time producing pointers to irregular memory locations for MAPLE to fetch and store in one of its scratchpad hardware queues; Core 2 is consuming already fetch data from MAPLE queue; For prefetching, Core 3 is using MAPLE as a prefetching engine, scheduling in advance a series of indirect accesses of the shape A[B[i]]; Core 3 can thus fetch cache-averse patterns using MAPLE, and fetch regular patterns using the memory hierarchy. MAPLE operations can fetch directly from DRAM or from the LLC, as desired.</p><p>implementations, such as precise per-core area overheads or engineering effort needed to verify such core modifications.</p><p>Our Approach: This paper introduces a Memory Access Parallel-Load Engine (MAPLE), the world's first taped-out NoC-connected hardware that mitigates memory latency and improves performance without requiring processor tile or memory hierarchy modifications. In this paper, we implement, verify, and evaluate MAPLE's RTL, integrated into an open-source manycore framework through the NoC in a scalable, tiled, manner. Figure <ref type="figure">1</ref> highlights two scenarios that leverage MAPLE's specialization for memory latency tolerance and timely supply of data to processing units. MAPLE supports decoupling and prefetching techniques through its API. These are not custom ISA instructions but regular load and store instructions from user mode to read and write to a MAPLE instance through simple Memory-Mapped IO (MMIO) (see <ref type="bibr">Section 3)</ref>.</p><p>MAPLE offers a flexible programming model that extends far beyond scheduling a task to an engine that subsequently raises an interrupt upon completion (e.g. DMA engines). Utilizing MAPLE's hardware queues enables decoupling of data-produce and compute operations for latency tolerance. Previously, this fine-grained supply capability has only been supported through new ISA instructions and deep microarchitectural changes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49]</ref>, which made it difficult to adopt in practice.</p><p>Off-the-shelf cores can produce (store) data into MAPLE, and consume (load) from it as if they were interacting with a software queue, but with the advantage that MAPLE can transform the data in between. Figure <ref type="figure">1</ref> shows how MAPLE can be invoked to fetch irregular or IMAs and place the data into its FIFO queues for cores or accelerators to consume from them and perform dense computation.</p><p>MAPLE's scratchpad offers hardware queues implemented as circular FIFOs. MAPLE performs hundreds of long-latency IMAs in parallel, utilizing many slots in the FIFO queues, whose indices are used to reorder memory responses. This provides memory level parallelism (MLP) without the area overhead of IMA-dedicated hardware on every core.</p><p>Our main innovation is the exploitation of the software optimizations of decoupling and prefetching, while leveraging specialized memory-access hardware, without modifying the core, ISA, or memory hierarchy, demonstrated with a real implementation. This work enables MLP in systems with area-efficient cores (e.g. with small instruction windows or in-order execution), where software-only approaches are ineffective. MAPLE provides hardware assistance through an API and does not require modifications of processor cores. Our API and hardware-software co-design are compliant with Virtual Memory (VM) and SMP Linux, and support scaling the number of MAPLE instances, as done in our chip tapeout. Each MAPLE is individually protected through the core-level, standard, virtual memory protection.</p><p>MAPLE's API can be targeted by automated compiler passes in which the original program is transformed through LLVM <ref type="bibr" target="#b29">[30]</ref> passes to offload IMAs to MAPLE. Alternatively, the API could be targeted from the backend of a Domain-Specific Language (DSL), e.g. GraphIt <ref type="bibr" target="#b66">[67]</ref> or TACO <ref type="bibr" target="#b27">[28]</ref>, to overlap memory latency with computation.</p><p>Unlike much of the prior work shown in Table <ref type="table" target="#tab_1">1</ref>, MAPLE can be adopted in practice with little engineering effort. We demonstrate this with its integration into an open-source manycore SoC (OpenPiton+Ariane) <ref type="bibr" target="#b5">[6]</ref> on its own tile. We measured MAPLE's effectiveness by evaluating prominent latency-bound workloads on an FPGA prototype.</p><p>Our main technical contributions are:</p><p>• A HW-SW co-design that mitigates long-latency memory accesses through scalable specialized units that are integrated into an SoC without core or memory hierarchy modifications.  </p><formula xml:id="formula_0">✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ DeSC/MTDCAE[22, 55] ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ SW Pre-execution [35] ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ Triggered inst.[43] ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ Slipstream [52, 54] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ HW Prefetching[9] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ Graph Pref, IMP.[1, 62] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ Programmable Pref. [3] ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ DSWP [45] ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ Outrider [15] ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ Clairvoyance [58] ✓ ✓ ✓ ✓ ✓ ✓ ✗ ✗ ✗ ✗ ✗ ✗ SWOOP [59] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ MAD [24] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ Pipette [41] ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓ ✓ Prodigy [56] ✗ ✗ ✗ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ MAPLE ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>Over the last 30 years, many works have proposed techniques for memory latency tolerance. With the increasing importance of graph analytic and sparse neural network (SNN) applications, recent techniques have focused on mitigating the latency of IMAs. These can be coarsely divided into prefetching-based <ref type="bibr">[1-3, 26, 62]</ref>, streaming multi-core / multi thread <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57]</ref>, and decouple access-execute (DAE) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref>. We accommodate these techniques in software, with the transparent usage of our out-of-the-core specialized hardware to increase performance. This paper attempts to leverage the range of techniques proposed in this field, both in hardware and software. It identifies four key limitations to be overcome to democratize the access to their benefits in modern heterogeneous systems: (1) Prior hardware techniques modify the core microarchitecture, sometimes even reducing its generality. Adopting such techniques also increases the verification burden of already overloaded hardware designers <ref type="bibr" target="#b19">[20]</ref>. This is exacerbated in the context of SoC generator frameworks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">60]</ref>, where modifications to third-party cores' RTL can be very challenging and limit their reusability. (2) Some modern software techniques assume special capabilities from the core, like OoO or SMT. This limits their usage, e.g. area-constrained environments tend to use simple in-order cores. (3) Techniques that rely on ISA extensions <ref type="bibr" target="#b22">[23]</ref> or ISA-specific instructions have limited applicability and portability problems, especially in the context of heterogeneous-ISA architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>. (4) Hardware-only techniques like Slipstream <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref> or hardware prefetching <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref> often require costly structures for book-keeping, detection, and prediction. Data movement is decisive to meet performance goals, and often these patterns are known in software <ref type="bibr" target="#b9">[10]</ref>. Leveraging program knowledge, either extracted by a compiler pass or explicitly written (in the backend of a DSL) is key to delivering high performance at a low area and complexity cost.</p><p>Table <ref type="table" target="#tab_1">1</ref> classifies prior software and hardware approaches in the extensive literature on latency tolerance of IMAs, based on the four identified features for a technique to achieve ease of adoption, software programmability, and performance in low-power, areaefficient systems.</p><p>Decoupled Access/Execute (DAE): The DAE <ref type="bibr" target="#b48">[49]</ref> paradigm was introduced decades ago to overlap memory accesses and computation without relying either on out-of-order execution or on prefetching unpredictable access patterns. DAE slices a program into two parallel threads, the Access thread handles memory access and address computation, and the Execute thread does computations. Ideally, the Access runs ahead of the Execute by issuing memory requests and enqueuing their data. In the meantime, the Execute consumes the data from the communication queue to perform value computations. If the access slice can run ahead of the execute slice and produce all of the data required for computation, it can act as a non-speculative perfect prefetcher. Among other decoupling proposals <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b54">55]</ref>, DeSC <ref type="bibr" target="#b21">[22]</ref> builds upon DAE, introducing compiler and hardware optimizations, so that loads with no further dependencies on the Access side (whose values are used exclusively by Execute) can be loaded in a side structure of the Access core without stalling the pipeline. The biggest drawback of DAE, DeSC, and all the prior-art in hardware decoupling is that it requires specific hardware changes for Access and Execute cores, limiting their usage to those roles.</p><p>Our hardware mechanism is amenable to the decoupling programming model. MAPLE provides a software API for decoupling without modifying the core altogether.</p><p>In our case, any number of cores can be programmed to behave as Access or Execute at runtime. MAPLE, as a memory access engine, can handle many data loads in parallel by utilizing hardware queues to track data for completed requests. This prevents stalls in the Access thread if it is not capable of hiding latency (e.g. short instruction window).</p><p>An important and substantial accomplishment of MAPLE is its ability to support DeSC-style decoupling, but with offthe-shelf cores. This increases the programmability of the system to allow other latency-tolerance techniques while providing comparable performance (see Section 5).</p><p>Figure <ref type="figure">2</ref> showcases how our hardware-software co-design provides the API programmability of software decoupling while being assisted by specialized hardware, to achieve MLP even with simple cores. Since the Access thread is running on a core with a small instruction window, shared-memory decoupling loses runahead due to long-latency stalls of IMAs, and so the Execute thread stalls waiting for the data to be produced. With MAPLE, the Access thread only produces IMA pointers, which MAPLE will load asynchronously to the core-in a highly parallel manner-and supply data to the Execute in time. The performance gain of MAPLE for decoupling is demonstrated in Section 5.1 against software and hardware decoupling approaches.</p><p>Prefetching IMAs: This paradigm includes changes in hardware and/or software. Recent hardware proposals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b61">62]</ref> have the drawback of core-microarchitecture modifications, and the often large and per-core area overhead of the structures needed to predict access. Software techniques are a tempting proposition since they keep the core untouched and can leverage compiler knowledge. However, software prefetching incurs overheads due to code-bloating-up to 8.5× the instruction count in innerloops <ref type="bibr" target="#b1">[2]</ref>. Prefetching might thrash the L1 too with large blocks or untimely data. To overcome these limitations, we use MAPLE as a programmable prefetcher too. State-of-the-art software prefetching techniques can use our API to issue prefetch commands to MAPLE, which can also decide the granularity and where to place the loaded data (into MAPLE queues or LLC). Moreover, MAPLE has specialized logic for IMAs which occur in loops, avoiding the extra instructions needed for address calculation of prefetches. This mechanism provides the advantages of software techniques, with the enhanced performance brought by specialized hardware for memory accesses.</p><formula xml:id="formula_1">L D B [i + X ] Time (cy)</formula><p>Core Models: Prior art has already characterized that OoO without specialized hardware for memory accesses is not effective for our domain <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56]</ref>. Because MAPLE units are effective for irregular, long-latency data (through decoupling or prefetching), this allows the use of simpler and more area-efficient cores. Since cores either consume from MAPLE or load regular-pattern data from the cache, these cores do not require expensive reordering units (in OoO) nor core-coupled IMA prefetchers. This pairing of slim cores + MAPLE saves per-core area, which allows for larger core counts and higher parallelism. We have seen the trend of manycores made out of slim cores recently in academia <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b64">65]</ref> and industry, e.g. Cerebras Systems <ref type="bibr" target="#b32">[33]</ref> and Esperanto <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE HARDWARE-SOFTWARE APPROACH</head><p>Our co-design offers a software interface to leverage MAPLE hardware specialization. Its communication mechanism is amenable for software pipelining and its programming model is easily extensible to incorporate domain-specific access patterns or more memory operations, e.g. data structure reshaping or Read-Modify-Write atomic operations. In the scope of this paper, we showcase how our co-design accommodates both software prefetching and decoupling optimizations, with enhanced performance due to specialized hardware assistance.</p><p>MAPLE can achieve speedups similar to that of latency tolerant DAE architectures, without requiring modifications to cores to designate them as Access or Execute. Instead, the DAE programming model is supported via the API. When using decoupling API operations, MAPLE provides the data communication queue, where data for memory requests from the Access thread are enqueued in order to serve the Execute thread. This allows us to offer the latency tolerance of DAE through hardware that is outside the core, unlike many prior hardware DAE approaches, which significantly modify the cores to support decoupling.</p><p>Additionally, MAPLE's connection to the interconnection network eases its scalable integration, where possibly hundreds of units could be connected to the SoC, each one supporting several queues. The concept of queues in the API is a software abstraction detached from the hardware queues. Internally, the API implementation can map hardware queues across multiple MAPLE instances, if needed. A thread can communicate with any MAPLE instance from user mode by having the OS map MAPLE's associated page (address range) into virtual memory, through Memory-Mapped IO (MMIO). This provides access protection and transparent allocation.</p><p>Sections 3.1-3.3 provide examples of how the API can be used for different memory optimizations. Section 3.4 explains the details of MAPLE's implementation and how its hardware-software mechanism is fully compliant with virtual memory and requires no ISA-dependent instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Using MAPLE for Decoupled Programs</head><p>The following list presents the API operations that emulate DAE techniques. PRODUCE_PTR utilizes MAPLE to load data and thus reduce the Access thread burden, especially on accesses with poor cache locality.</p><p>• INIT(queues): Initializes the queues for a program.</p><p>• OPEN/CLOSE(id): Opens exclusive communication with a queue, or closes such a connection. • PRODUCE (id, data): Pushes data into a queue.</p><p>• CONSUME (id): Pops data from a queue.</p><p>• PRODUCE_PTR (id, pointer): pushes (stores) a pointer into MAPLE, which will fetch its data from memory and write the response into a queue in program order.</p><p>Besides these main operations, the API also contains functions to collect performance counters and debugging. Figure <ref type="figure">2</ref> shows an example of a decoupled code targeting MAPLE, and the depiction of its runtime memory transactions. Figure <ref type="figure" target="#fig_0">3</ref> now shows the hardware components of MAPLE that are involved in this program and their interactions with the rest of the system. MAPLE is connected to the Network-on-Chip (NoC) through protocol decoders and encoders, and thus it can receive/send operations from/to the cores and make requests to DRAM and/or to the LLC. MAPLE also manages hardware queues for data communication between threads, implemented as circular FIFOs, using its scratchpad.  The Produce path works as follows:</p><p>(1) It starts by doing a store instruction where the stored data is the pointer to fetch. This store is targeted to an address composed of MAPLE's instance base address, queue ID, and operation code; <ref type="bibr" target="#b1">(2)</ref> The decoder identifies the operation as a pointer-produce, and routes it to the produce pipeline where it will reserve an entry in the corresponding queue; (3) The pointer (virtual address) is first translated into a physical address in MAPLE's MMU, and the data associated with that address is requested from DRAM, using as the transaction ID the index of the allocated entry in the queue; (4) The initial store request is acknowledged to the Access thread which considers the produce as finished and retires the store instruction; (5) The memory request reaches DRAM which responds to MAPLE; <ref type="bibr" target="#b5">(6)</ref> The response is decoded and stored in the corresponding queue entry.</p><p>Consumes occur later in time than the data production provided that the Access thread has enough runahead.This should be the norm when using MAPLE, since the Access is not stalled and the hardware queues are big enough to hold the data fetched in advance.</p><p>The Consume path works as follows: (A) The execute thread generates a consume operation-implemented in the API as a load request to MAPLE. Once the load reaches MAPLE, it is decoded and routed to the consume pipeline; (B) If the queue specified on the request parameters is not empty, it would pop the entry in the head of the queue and return it as a response to the load instruction; (C) The response reaches the core that is running the Execute thread and the consume operation finishes.</p><p>Using MAPLE for decoupling brings software flexibility over the original hardware DAE approach or state-of-the-art DeSC architecture. In our approach, Access or Execute are conceptual "roles" taken by software threads rather than a hardwired core type, and they can be determined at runtime. This enables dynamic reconfigurability for applications with different data supply and computation demands. Some might benefit from having multiple Execute threads being supplied from the same Access thread, generating </p><formula xml:id="formula_2">for ( i = 0; i &lt; N; i++ ){ // D</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Using MAPLE for Prefetching</head><p>MAPLE's API can also be used for software prefetching. Nonspeculative prefetching can leverage the aforementioned queue management functions and PRODUCE_PTR to place all the prefetched data into a queue within MAPLE. This is especially desirable in the context of IMAs like A[B <ref type="bibr">[i]</ref>]. Placing the data of irregular, cache-averse accesses into MAPLE has a two-fold advantage over placing it in the memory hierarchy: it prevents data from being replaced if fetched too early with respect to its usage, and it avoids thrashing the L1 cache with low-reuse data. Additionally, MAPLE can prefetch into the shared LLC to support speculative prefetching (PREFETCH(ptr)).</p><p>Software prefetching of IMAs within inner-loops incurs an instruction overhead to calculate the address of the target prefetch and other book-keeping <ref type="bibr" target="#b1">[2]</ref>. To remove that overhead, MAPLE can prefetch Loops of Indirect Memory Accesses (LIMA). This is targeted through API operations:</p><p>•  <ref type="figure">4</ref> shows a code example of injecting LIMA speculative prefetching. A single software operation provides prefetches for a whole loop of accesses (details in Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Targeting MAPLE Automatically</head><p>Although one could use our API directly, we nevertheless believe programmers need not directly code data movement. Instead, compiler passes or domain-specific languages such as TACO <ref type="bibr" target="#b27">[28]</ref> (sparse algebra) or GraphIt <ref type="bibr" target="#b66">[67]</ref> (graph analytics) could use the API, as they have knowledge about data structures and coherence. Recent automatic compiler techniques already target software prefetching <ref type="bibr" target="#b1">[2]</ref> for (i=0; i&lt;N; i++) res and slice decoupled programs <ref type="bibr" target="#b21">[22]</ref>. Therefore, they could be adapted to target API operations instead of ISA-specific instructions.</p><formula xml:id="formula_3">[i] = A[B[i]]*C[i] A[B[i]] * Loop start C[i] ST res[i]</formula><p>Figure <ref type="figure" target="#fig_2">5</ref> shows our adaptation of the compilation flow of DeSC <ref type="bibr" target="#b21">[22]</ref>. This flow slices the program into Access and Execute threads; loads are transformed into PRODUCE and CONSUME operations. After the program slicing, some loads no longer have dependencies on the Access code (only on the Execute), and so the Access can produce the pointer for MAPLE to load, PRODUCE_PTR. We evaluate using this LLVM-based <ref type="bibr" target="#b29">[30]</ref> automatic compiler pass in Section 5.2, showing that by simply utilizing established compiler techniques, MAPLE can be leveraged to yield significant performance improvements.</p><p>Automatic compiler techniques for prefetching could potentially target the LIMA operation, thus reducing the instruction overhead of software prefetching IMAs in tight-inner loops, but this is out of the scope of this paper. The Configuration pipeline is used to create logical queues and bind them to software threads at runtime. These queues are implemented as circular FIFOs using a local scratchpad. Depending on the program's needs, one can configure to have fewer, larger, queues, or many but smaller. There is an upper limit on the number of queues per MAPLE unit, which is set as an RTL parameter at tape-out, along with the scratchpad size. This pipeline receives read operations when the configuration requires a response (e.g. queue binding), and write operations when the configuration needs to specify a payload (e.g. for the LIMA unit). This pipeline is nonblocking as it needs to be available for software configuration of the MMU and debug operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">MAPLE Hardware Implementation</head><p>The Consume pipeline is solely used for cores to read data from the queues.  The Produce pipeline receives store instruction from the cores where the payload contains either data or a pointer to fetch. Produces are processed in several stages: First, the transaction is buffered. In the case of a pointer, the virtual address is translated in the MMU; second, a slot in the queue is reserved; third, either the data is written into the reserved slot (data-produce) or the memory request is issued to DRAM (pointer-produce) using as transaction ID the queue slot index. Memory responses come in any order. We ensure data is written in program order with the transaction ID.</p><p>The reason to have separate pipelines is to avoid deadlocks. When a specific queue is full, the produce operation is buffered (no overflow) in the first stage until an entry is consumed. Meanwhile, operations to other queues can proceed without stalls. Consumes work similarly, a load into an empty queue is buffered (no polling) until new data is available to be returned to the core. Each pipeline has a final stage to respond to the issuing core. This design was verified with industry-level formal tools (Section 3.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIMA operations fetch A[B[i]</head><p>] for a given range of i. Once the base pointers for arrays A and B are configured (virtual addresses), LIMA performs TLB translation and fetches array B in chunks of 64B that are stored in the scratchpad. As soon as the chunks start arriving, LIMA iterates over them word by word utilizing an offset into array A to calculate the final address. Finally, depending on whether the prefetch is speculative or non-speculative, it inserts into the Produce pipe the equivalence of a pointer-produce or a prefetch operation. Because MAPLE is ISA-agnostic, the prefetch operations are not using ISA prefetch instructions. Instead, it sends a network request to the shared cache, similar to how a private cache would do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Virtual Memory Support</head><p>When a core requests a queue through the API, the OS maps a free MAPLE instance into a page (MMIO). Thus, the core performs address translation to load or store into the MAPLE address-space, accessing that MAPLE context (control registers) in a protected manner. Since this is a single page, the translation hits in the TLB with no overhead. Because the data that is delivered to MAPLE can be a pointer, i.e. a virtual address, it needs translation. MAPLE fully supports virtual memory through its local MMU and TLB, to be able to access any regularly allocated memory. MAPLE's TLB is fully associative and has 16 entries, the same as the cores' TLB. Because IMAs are irregular and often span different pages, TLB misses add latency to IMAs. The total latency is mitigated by MAPLE with runahead execution and memory parallelism.</p><p>Upon a TLB miss, MAPLE's hardware Page Table Walker (PTW) fetches the corresponding entry from the memory hierarchy. If the PTW encounters a page fault (e.g. if the page is invalid), an interrupt is raised, and the kernel invokes the MAPLE driver. This driver reads the virtual address that caused the page fault (using the Configuration pipeline) and maps it into the page table if valid access. The device driver implements Linux's callback function for shootdowns, which are communicated to the MAPLE-MMU to prevent stale entries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Communicating with MAPLE units</head><p>Portable: General-purpose cores can communicate with MAPLE from user mode through MMIO. This allows operations like Produce and Consume to, under the hood, use existing store and load instructions, respectively. These are synchronous (not polling) with MAPLE, i.e. memory instructions return once MAPLE acknowledges them. The round-trip path is depicted and latencycharacterized in Figure <ref type="figure" target="#fig_12">14</ref>.</p><p>Scalable: Since many MAPLE instances can co-exist in an SoC (e.g. a tiled architecture), each one is accessed via a different physical page. We leverage virtual memory translation to provide processexclusive access to MAPLE's hardware resources and provide data protection. This also allows a process to decide at runtime which MAPLE unit to target. As we introduced before, previous approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b48">49]</ref> do not offer this software programmability of decouplinghardware resources, as these are tightly connected to specific cores.</p><p>Extensible: The fact that each unit's control registers are mapped to a page allows MAPLE to re-purpose the index of a word within the page to distinguish operation codes based on bits 3 to 8. This gives the API up to 128 operation codes, i.e. 64 for loads and 64 for stores so that more operations can be included.</p><p>Core-Agnostic: The only capability MAPLE needs from a core is having load and store instructions. Thus, it can communicate with any off-the-shelf core and is not limited to non-speculative in-order cores. Although not included in our chip tapeout, MAPLE could handle speculative queue/dequeues from cores using transaction IDs encoded in the lower address bits. A chip for different workloads, where OoO cores are desired, could also integrate MAPLE units to speed up irregular accesses.</p><p>Efficient: MAPLE has full access to the memory hierarchy, thus, it can do cache-coherent loads from the LLC or non-coherent loads MAPLE only needs to be connected to the NoC through its parameterizable encoders and decoders.</p><p>directly to main memory (determined by the decoded operationcode). There are advantages and limitations inherent to the idea of offloading memory operations into a specialized unit. MAPLE behaves effectively as a Private Local Memory (PLM) so that the data inside the scratchpad has no coherence guarantees after it is fetched. The compiler technique or DSL using the API should make sure that the arrays loaded by MAPLE have no further writes to them. This condition holds for the irregularly accessed array of most of the graph algorithms studied, since updates often occur only after an epoch barrier. Leveraging conditions known at the software level allows MAPLE to use highly parallel and efficient hardware.</p><p>We envision MAPLE to be an easy-to-adopt and scalable resource to include in an SoC. MAPLE speeds up workloads that do not leverage traditional cache locality and benefit from a programmable unit accessible from the memory hierarchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">MAPLE Integration via NoC</head><p>A key feature of MAPLE is that it can be adopted by a system without modifying existing hardware. It can simply be accessed via the on-chip interconnection network (NoC). This procedure has been followed for the 2D P-Mesh protocol of OpenPiton <ref type="bibr" target="#b7">[8]</ref>, which is an open-source, tile-based SoC framework. Figure <ref type="figure" target="#fig_5">7</ref> depicts this integration of MAPLE on its own tile via the NoC routers. This integration has been evaluated on FPGA (Section 4.2) and the results are reported in Section 5.1.</p><p>Ease of adoption: The integration of MAPLE with OpenPiton took around a hundred Verilog RTL lines of code (LoC), which demonstrates that it is easy to adopt. This contrasts with the 5K LoC of MAPLE itself. This demonstrates the advantage of integrating it as a reusable IP block versus building it from scratch. Moreover, the integration does not require details about the underlying system aside from the communication protocol. It is agnostic to ISA and CPU internals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Reusing MAPLE in SoCs</head><p>Deep microarchitecture changes are hard to take into practice because of the verification burden. Hardware designers are spending about half their time doing verification <ref type="bibr" target="#b19">[20]</ref>, and trends <ref type="bibr" target="#b46">[47]</ref> indicate that the number and diversity of IP blocks per SoC can exacerbate this burden. Several frameworks have emerged to make SoC development agile and multicore <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b59">60]</ref>, by connecting highly parameterized IP blocks to form a complete SoC design. Reusing IP alleviates the verification burden so that engineers can focus on system-level requirements <ref type="bibr" target="#b3">[4]</ref>. However, SoC generator frameworks do not have a reusable hardware solution to the memory latency bottleneck yet. Because MAPLE is agnostic to the ISA and core model, it could even be included in SoC frameworks with heterogeneous cores <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8]</ref> and hybrid ISAs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Formal Verification of MAPLE</head><p>MAPLE saves verification effort over the prior work in latency tolerance techniques. It is so because the verification burden is shifted from the integration process to the decoupled unit. We invested significant time to verify MAPLE's correctness at the unit-level to remain agnostic of the rest of the system and ease integration. This makes MAPLE reusable without the verification burden of a tightly-coupled integration.</p><p>The verification was conducted using industry-standard Sys-temVerilog Assertions (SVA) <ref type="bibr" target="#b24">[25]</ref> and JasperGold <ref type="bibr" target="#b11">[12]</ref> and assisting tools <ref type="bibr" target="#b41">[42]</ref>. We followed a verification-first approach to save latestage debugging time and increase the confidence in creating a verifiably correct design. We exhaustively tested the pipelines and MMU interactions.</p><p>As a result of this lengthy process, we deliver a verified RTL design for functional correctness and liveness. The quality metrics provided by JasperGold give confidence in the goodness of the assertions-they cover more than 99% of the MAPLE's RTL.</p><p>After integrating MAPLE with the final system, we included the SVA properties on the system-level simulation testbench. MAPLE held correct, but our effort uncovered two bugs in the open-source core interacting with MAPLE, which were communicated to the maintainers. The formal testbench will also be included in the opensourcing of this project</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION METHODOLOGY</head><p>This section first describes four widely-used data-analytic benchmarks that exhibit memory latency bottlenecks due to IMAs. Second, it provides details of the SoC prototype emulated on FPGA. At the end, it describes the methodology employed for the evaluation over the prior techniques and sensitivity studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Applications for Data-analytics</head><p>Memory latency bottlenecks of Graph and Sparse Algebra applications have been characterized several times in the last couple of years <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56]</ref> with over 60-70% of the runtime dedicated to memory stalls.</p><p>Sparse matrices often contain few non-zero elements and therefore are stored in compact representations. Two of the most popular representations are Compressed Sparse Row (CSR) and Compressed Sparse Column (CSC). They both efficiently represent a sparse matrix using three one-dimensional arrays to store the number of non-zero elements of a row (or column), indices of non-zero matrix elements within that row (or column), and the non-zero matrix elements. Meanwhile, dense matrices are simply stored as onedimensional arrays, similar to the data arrays in the CSR and CSC formats. Because they are dense, the indices of the elements can be determined by knowing the number of rows and columns and do not require information about where non-zero elements are located.</p><p>Sparse Dense Hadamard Product (SDHP): Performs an elementwise operation, e.g. multiplication, between a sparse and a dense matrix. Because the operation is performed elementwise, the dense matrix is sparsely sampled based on the locations of non-zero elements in the sparse matrix. This results in irregular accesses to the dense matrix, as they are not predictable and therefore not amenable to the cache locality. By decoupling the kernel so that the Access can fetch the irregular memory accesses before the Execute core needs their data, this performance bottleneck can be alleviated.</p><p>Sparse Matrix-Matrix Multiplication (SPMM): Performs a matrix multiplication between two sparse matrices A and B in a layer-wise fashion <ref type="bibr" target="#b38">[39]</ref> to train a sparse deep neural network. This kernel is parallelized in the columns of B, while intermediate results are stored in a dense, temporary matrix.</p><p>Sparse Matrix-Vector Multiplication (SPMV): Performs matrix multiplication between a sparse matrix and a dense vector. Similar to SDHP, the dense vector is sparsely sampled according to the non-zero elements of the sparse matrix, providing an improvement opportunity for decoupling.</p><p>Breadth First Search (BFS): Determines the distance (number of hops) from a given root vertex in a graph to all other vertices. The traversal starts at the root and in each iteration, examines all vertices in a layer-wise fashion to find neighbors that have not been visited and require an update. Accessing neighbor data requires IMAs.</p><p>Datasets: We evaluate these kernels using real-world networks and synthetic datasets. SDHP uses matrices from SuiteSparse <ref type="bibr" target="#b17">[18]</ref> and a Kronecker network <ref type="bibr" target="#b31">[32]</ref>, BFS operates on Wikipedia, YouTube, and LiveJournal graphs, while SPMM and SPMV use synthetic matrices from riscv-tests <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">FPGA Emulated SoC System</head><p>As described in Section 3.7, we have integrated MAPLE's RTL within the OpenPiton framework <ref type="bibr" target="#b5">[6]</ref>, to characterize its advantages in a real system. OpenPiton is an open-source tile-based manycore architecture, which supports multiple ISAs. We use RISC-V Ariane <ref type="bibr" target="#b62">[63]</ref> cores to demonstrate how latency tolerance can be achieved even in simple, non-speculative, in-order cores which are commonly used in area-and power-constrained environments.</p><p>Table <ref type="table" target="#tab_6">2</ref> presents the SoC details as well as the parameters used for MAPLE and the FPGA used for the prototype.</p><p>This evaluation demonstrates that our RTL implementation works on a potential SoC, emulated on FPGA, running applications on top of SMP Linux (version v5.6-rc4). We evaluate the applications and datasets described above, running single and multithreaded versions with OpenMP <ref type="bibr" target="#b15">[16]</ref> parallelization. The FPGA evaluation highlights the performance speedups obtained by doing prefetching and decoupling through MAPLE, over the baseline of do-all parallelism.</p><p>The evaluation compares the same decoupled program with the API, using (a) a shared-memory implementation of decoupling; and (b) an implementation targeting MAPLE to characterize the benefits of our hardware-software co-design. Then, the evaluation compares the latest prefetching techniques over using LIMA to fetch loops of IMAs. For a fair comparison, prefetches are inserted in the code at the best location known to the programmer. The related work has not provided RTL implementations. Since implementing in RTL the related work that we wanted to compare <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> would take months, even for people with years of industry experience, we compared against these in a simulator and compared against software techniques on FPGA emulation. The FPGA could only fit a MAPLE instance and two cores, so we ran with scaling threads on the simulator as well (Figure <ref type="figure" target="#fig_11">13</ref>). The simulator-based evaluation of MAPLE over prior decoupling leverages the automatic compiler program-slicing seen in Section 3.3. However, this slicing was done manually for the FPGA runs, since this was not yet incorporated into our FPGA flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Against Prior Work</head><p>In addition to our real-system evaluations, which demonstrate a significant improvement over the baseline, we evaluate MAPLE over the latest latency-mitigation approaches, including DeSC decoupling <ref type="bibr" target="#b21">[22]</ref> and DROPLET hardware prefetching <ref type="bibr" target="#b8">[9]</ref>, via system simulation. To do this we leverage MosaicSim <ref type="bibr" target="#b37">[38]</ref>, a simulator for heterogeneous architectures and hardware-software co-designs, and model the communication queues used in MAPLE.</p><p>Table <ref type="table" target="#tab_7">3</ref> shows the core model and memory hierarchy parameters of the simulated system. We tried to match the simulator model with the SoC configuration to prove the same premise, that MAPLE can provide latency tolerance even for single-issue in-order cores. This evaluation leverages DEC++ <ref type="bibr" target="#b50">[51]</ref> compiler flow for automatic code transformation of MAPLE-decoupling and DeSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sensitivity Parameters to Characterize</head><p>MAPLE has many interesting parameters worth evaluating, like the size of the queue connecting a pair of threads (determined at runtime). For decoupling, this queue must be big enough to allow the Access thread to run ahead and hide the memory latency so that the Execute thread does not stall waiting for data. However, the smaller the size, the more logical queues can share MAPLE scratchpad memory. This size is closely related to the round-trip latency between MAPLE and any given core (sets the throughput to/from the queue) along with the DRAM latency, since it determines the runahead that is necessary. To evaluate the impact of different queue sizes on the runahead between Access and Execute, we study the performance counters provided by MAPLE through debug operations when running on the FPGA.</p><p>It is important to characterize the round-trip latency between cores and MAPLE, since it determines the cost of a data consume. This latency depends on the memory hierarchy, the network, and the placement of MAPLE unit(s). We first break down and characterize this round-trip latency in the OpenPiton framework by analyzing waveforms of an RTL simulation. Then, we study this latency's impact on performance by varying it as a parameter in simulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>This section first presents the performance results obtained from the FPGA evaluation of our SoC prototype: It compares the speedups obtained by decoupled programs (with and without using MAPLE) over the parallel version of the original program; and the performance of a program enhanced with prefetching over no prefetching. The prefetching version is both evaluated using MAPLE's API and prefetching instructions. Second, there is a comparison against prior hardware techniques, both for decoupling and hardware prefetching. Finally, it presents the conclusions from the sensitivity studies and the area analysis of MAPLE's RTL implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FPGA Emulation of the SoC Prototype</head><p>Figure <ref type="figure" target="#fig_7">8</ref> compares the speedups achieved by decoupling Access and Execute threads using MAPLE's API and a shared-memory implementation, over traditional doall parallelism on 2-threads. The rightmost comparison shows the geomean speedup obtained across all applications. Using MAPLE achieves 1.51× speedup over doall and 2.27× over software-only decoupling. This demonstrates that decoupling is not performant by itself in area-constrained systems without MAPLE hardware support.</p><p>We also compare MAPLE against software prefetching with a baseline of no prefetching for single-thread execution. We evaluate MAPLE's LIMA_PRODUCE operation, which places the prefetched data into its hardware queues, from which the core consumes. Since IMAs have poor cache locality, it is better to consume them from MAPLE (as non-cacheable), and reserve the caches for regular data accesses that exploit locality.</p><p>Figure <ref type="figure">9</ref> compares the speedups of prefetching IMAs in hardware with MAPLE (using the LIMA operation), and conventional software prefetching. The geomean speedup is 1.73× over no prefetching and up to 2.4× for SPMV. In addition, using MAPLE achieves a   geomean speedup of 2.35× over software prefetching, showing the advantage of not bringing highly irregular data into the L1 cache. Moreover, prefetching using MAPLE reduces the instruction overhead of software prefetching since IMAs in a whole tight innerloop can be offloaded into MAPLE with a single LIMA operation.</p><p>Figure <ref type="figure">10</ref> presents the normalized overhead of load instructions due to prefetching relative to the baseline with no prefetching. Software prefetching doubles the number of loads, whereas MAPLE slightly reduces the total number of loads. The reduction occurs because the sparse IMAs are gathered inside MAPLE queues, and if the data type is a 32-bit word (as it happens in SPMV), the core loads two words at a time.</p><p>This evaluation has also collected hardware performance counters to measure the average latency of load instructions. Figure <ref type="figure" target="#fig_9">11</ref> shows that using MAPLE's LIMA operation for prefetching significantly decreases the average load latency to nearly half (1.85× geomean reduction), thus demonstrating its effectiveness to hide memory latency of cache-averse accesses. It is significantly more effective than doing software prefetching into the L1 cache, which suffers from cache thrashing due to the low spatial and temporal locality of IMAs. Moreover, consuming data from MAPLE queues avoids the premature replacement of prefetched data in caches. These advantages are shown clearly for SPMV. Although it is not characterized here, LIMA operations can complement regular prefetching instructions, where MAPLE is targeted for IMAs while regular access patterns are prefetched natively. Since the compiler can automatically detect which accesses are irregular <ref type="bibr" target="#b49">[50]</ref>, it could insert adequate prefetches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison against Prior Work</head><p>Figure <ref type="figure" target="#fig_10">12</ref> compares the runtime performance of MAPLE decoupling, DeSC <ref type="bibr" target="#b21">[22]</ref> decoupling, and DROPLET <ref type="bibr" target="#b8">[9]</ref> hardware prefetching, as well as that of traditional doall parallelism, for 2 threads. The speedup result for each application is the geomean of the speedups obtained across the datasets evaluated.</p><p>DeSC slicing is more restrictive than MAPLE decoupling, since DeSC's Execute (Compute) core does not have visibility into the memory hierarchy, and all data is passed to the Access (Supply) to be stored. This results in a loss of runahead for BFS, and thus DeSC performs poorly compared to MAPLE. Decoupling in general is not effective for the selected SPMM implementation, since the IMAs are Read-Modify-Writes and cannot be decoupled. Unlike DeSC, we do not propose a DAE architecture, but MAPLE supports decoupling; if the compiler pass for program slicing cannot find an IMA, it falls back to doall parallelism. In contrast, SPMV and SDHP kernelswell suited for decoupling-achieve high performance with DeSC. We pay a threshold latency for cores to communicate with MAPLE, which is slightly higher than the architecturally visible, tightlycoupled, DeSC queues. MAPLE supports a flexible alternative to DeSC for decoupling, which does not constrain the architecture. Despite no core modifications, MAPLE achieves at least 76% of DeSC 's performance for decoupling-friendly applications, and it presents overall better performance. It achieves a geomean speedup of 1.72× over DeSC and 1.82× over DROPLET hardware prefetching, and up to 3× (geomean 1.96×) over doall for BFS.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Conclusions about the Sensitivity Studies</head><p>Figure <ref type="figure" target="#fig_11">13</ref> shows that our co-design scales well with an increasing number of threads, maintaining the speedup achieved over doall parallelism when scaling to 4 and 8 threads sharing the same MAPLE unit for decoupling. More units can be employed for larger thread counts in a tiled manner, conforming to a scalable system, only limited ultimately by the chip IO. When fetching data into MAPLE queues (decoupling or nonspeculative prefetching), the long latency of IMAs is reduced to the consume round trip between the core and MAPLE. Figure <ref type="figure" target="#fig_12">14</ref> shows the characterization of that latency for the OpenPiton SoC. This latency is similar to the L2 access, 25 cycles plus a cycle per hop, and an order of magnitude smaller than DRAM.</p><p>In a manycore mesh scenario, MAPLE instances are often scattered across the X and Y tile axes so that MAPLE are near cores. As explained in Section 3.6, MAPLE instances are mapped into virtual memory, and a process could leverage the OS to minimize the distance between the running core and any available MAPLE instance, to minimize round-trip latency.</p><p>Besides evaluating the particular latency of the OpenPiton network, we characterize in Figure <ref type="figure" target="#fig_13">15</ref> how the performance changes with smaller and larger communication latency values. The number next to MAPLE represents the average round-trip latency between cores and MAPLE, in cycles. This demonstrates that speedups are greater with a lower NoC delay.</p><p>We studied the performance impact of different queue sizes and observed that performance remains stable while the queues can hold enough data to hide latency. Although it is not shown here, a queue of 32 entries-4 bytes each-was sufficient to provide runahead without losing performance, while 16 entries caused a 5-10% decrease. With 32 entries per queue, MAPLE can supply data for up to 8 cores with just 1KB of storage (256 entries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Area analysis of the RTL Implementation</head><p>In our design, MAPLE synthesis including 8 circular queues sharing a 1KB scratchpad represents 1.1% of the area of the single-issue in-order Ariane <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref> cores it can supply, which are already very area-efficient. Thus, the overhead of MAPLE compared to more beefy cores would be negligible. MAPLE need not be per-core, and thus its area can be amortized over multiple cores that use it. In contrast, tightly-integrated prefetchers can increase logic delay, core area, and cycle latency Some prefetcher designs claim a low storage overhead (&lt;1KB), but their designs also contain FSMs, muxes, and combinational logic whose area is not accounted for in their bitcount-based (storage) estimates. MAPLE is the only technique implemented in RTL and taped out, to our knowledge. While area overheads for IMP <ref type="bibr" target="#b61">[62]</ref>, Prodigy <ref type="bibr" target="#b55">[56]</ref> and other related works only count storage, MAPLE overhead is calculated from the 12nm synthesis of our chip tapeout.</p><p>Since Decoupled Access-Execute (DAE) was originally proposed by Smith <ref type="bibr" target="#b47">[48]</ref>, several hardware implementations have been proposed, where data communication occurs through architectural queues <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b60">61]</ref>. In these papers, DAE aims to hide memory latency as a simpler alternative to superscalar processors. Later work analyzes the problems that arise from work imbalance between Access and Execute <ref type="bibr" target="#b26">[27]</ref> and loss of decoupling (LoD) due to control dependencies <ref type="bibr" target="#b10">[11]</ref>. Other work has envisioned Access and Execute cores having multiple physical threads <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b54">55]</ref>, or even having both Access and Execute as physical threads in the same core <ref type="bibr" target="#b14">[15]</ref>. DeSC <ref type="bibr" target="#b21">[22]</ref> introduces compiler and hardware techniques to avoid LoD and large instruction windows, and a special buffer in the Access core to host early committed load instructions. Memory Access Dataflow (MAD) <ref type="bibr" target="#b23">[24]</ref> introduces an engine optimized for dataflow computation that is integrated with cores or accelerators to execute memory-intensive portions of programs.</p><p>A limitation of these approaches is they require special ISA instructions to configure and use the communication queues. We overcome this problem by placing the queues within the MAPLE tilesaddressable through MMIO-allowing them to be shared among several cores in a manycore architecture. MAPLE is capable of providing fine-grain data supply with the same programming model as a native decoupling architecture.</p><p>Hardware prefetching has long been proposed to avoid cache misses in regular access patterns <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b39">40]</ref>, but traditionally does not work for IMAs. Recent proposals <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b61">62]</ref> achieve better performance in applications dominated by IMAs, e.g. graph analytics. Notably, Prodigy <ref type="bibr" target="#b55">[56]</ref> introduces novel compiler techniques to further assist the hardware. However, these approaches still require modification of the core microarchitecture, which is a considerable engineering effort both in design and verification. Thus, software techniques for latency tolerance are a tempting proposition in terms of ease of adoption.</p><p>Other hardware techniques like Slipstream <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b53">54]</ref> and Triggered Instructions <ref type="bibr" target="#b42">[43]</ref>, strive to separate data access and usage. Pipette <ref type="bibr" target="#b40">[41]</ref> is a hardware-software co-design that aims to generalize decoupling to a stream of stages that a program can go through. However, the deep microarchitecture modifications of these techniques limit their adoption in practice.</p><p>Software latency tolerance often uses compiler knowledge to improve performance. DSWP <ref type="bibr" target="#b44">[45]</ref> does automatic software pipelining without speculation by utilizing a hardware-aided inter-thread communication mechanism; Clairvoyance <ref type="bibr" target="#b57">[58]</ref> proposes compiler code separation into Access-Execute phases, to leverage the wide execution engines present in OoO cores. However, these software techniques rely on expensive hardware structures (RoB and LSQ) to maintain large instruction windows. As an alternative to this, SWOOP <ref type="bibr" target="#b58">[59]</ref> introduces compiler techniques, with the hardware assistance for context remapping-a novel form of register renamingto enable dynamic separation of Access and Execute phases in the code. However, SWOOP requires microarchitectural changes of the core, while MAPLE works with off-the-shelf cores. Our design does not need the core to support large instruction windows since it can achieve memory-level-parallelism (MLP) in an area-efficient manner through MAPLE.</p><p>Helper threads avoid large instruction windows by using a secondary thread of execution to improve the performance of the main thread <ref type="bibr" target="#b34">[35]</ref>. This thread is either programmer <ref type="bibr" target="#b13">[14]</ref> or compiler generated <ref type="bibr" target="#b65">[66]</ref>. Software prefetching has been shown effective for pointer indirection <ref type="bibr" target="#b1">[2]</ref>, aided by compiler techniques to automatically insert prefetches in the code. Helper threads and prefetching are sensitive to timeliness and can cause cache thrashing if not properly controlled, along with other problems like code-bloating already described in Section 2.</p><p>Many of the latency tolerance techniques mentioned here can co-exist or combine with MAPLE. For example, we envision leveraging existing compiler techniques to target its API <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>. This paper combines the advantages of software techniques, i.e. leveraging program knowledge, and hardware specialization while remaining ISA-agnostic so it can be widely adopted and extended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This paper has introduced a hardware-software co-design for latency tolerance that offers the best of both worlds: its flexible software interface enables MAPLE to be automatically targeted by compiler techniques for both prefetching and decoupling, and its specialized hardware does not need ISA extensions nor microarchitectural changes to the cores, which is key in today's open-source hardware renaissance.</p><p>We have demonstrated MAPLE gains on FPGA emulation by running sparse linear algebra and graph analytic kernels on SMP Linux. MAPLE provides significant performance improvements, 2.35× and 2.27×, over software-only techniques, and 1.82× and 1.72× geomean, over hardware prefetching and decoupling respectively. Moreover, MAPLE provides increased programmability and reusability over hardware-only approaches. The key for performance/area efficiency is to benefit both from compiler-extracted program knowledge and hardware specialization, while the key for usability is to provide a generic, extensible software interface and easy-to-adopt hardware.</p><p>We envision MAPLE's VM-capable hardware and programming model to be reused and extended by both the hardware and the software community. For example, to do pipelining, where each program stage is executed in a different off-the-shelf core or accelerator. We are open sourcing MAPLE with the publication of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A high-level overview of MAPLE components including the scratchpad (SP) storage that queues are sharing. Numbers represent the steps of a pointer-produce operation and the letters the steps of data-consume.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure3depicts the software-hardware timeline of a pointerproduce (steps 1-6) and consume operation (steps A-C).The Produce path works as follows:(1) It starts by doing a store instruction where the stored data is the pointer to fetch. This store is targeted to an address composed of MAPLE's instance base address, queue ID, and operation code;<ref type="bibr" target="#b1">(2)</ref> The decoder identifies the operation as a pointer-produce, and routes it to the produce pipeline where it will reserve an entry in the corresponding queue; (3) The pointer (virtual address) is first translated into a physical address in MAPLE's MMU, and the data associated with that address is requested from DRAM, using as the transaction ID the index of the allocated entry in the queue; (4) The initial store request is acknowledged to the Access thread which considers the produce as finished and retires the store instruction; (5) The memory request reaches DRAM which responds to MAPLE;<ref type="bibr" target="#b5">(6)</ref> The response is decoded and stored in the corresponding queue entry.Consumes occur later in time than the data production provided that the Access thread has enough runahead.This should be the norm when using MAPLE, since the Access is not stalled and the hardware queues are big enough to hold the data fetched in advance.The Consume path works as follows: (A) The execute thread generates a consume operation-implemented in the API as a load request to MAPLE. Once the load reaches MAPLE, it is decoded and routed to the consume pipeline; (B) If the queue specified on the request parameters is not empty, it would pop the entry in the head of the queue and return it as a response to the load instruction; (C) The response reaches the core that is running the Execute thread and the consume operation finishes.Using MAPLE for decoupling brings software flexibility over the original hardware DAE approach or state-of-the-art DeSC architecture. In our approach, Access or Execute are conceptual "roles" taken by software threads rather than a hardwired core type, and they can be determined at runtime. This enables dynamic reconfigurability for applications with different data supply and computation demands. Some might benefit from having multiple Execute threads being supplied from the same Access thread, generating</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Simplified compiler flow for decoupling. First, the program is sliced into Access and Execute, then a LLVM pass converts the IMA (in red) into PRODUCE_PTR and CONSUME API operations targeting MAPLE. Finally, both slices are compiled down to assembly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6</head><label>6</label><figDesc>Figure 6  presents the microarchitecture of MAPLE. There we can observe the breakdown of the aforementioned engine of MAPLE into three pipelines and a queue controller.The Configuration pipeline is used to create logical queues and bind them to software threads at runtime. These queues are implemented as circular FIFOs using a local scratchpad. Depending on the program's needs, one can configure to have fewer, larger, queues, or many but smaller. There is an upper limit on the number of queues per MAPLE unit, which is set as an RTL parameter at tape-out, along with the scratchpad size. This pipeline receives read operations when the configuration requires a response (e.g. queue binding), and write operations when the configuration needs to specify a payload (e.g. for the LIMA unit). This pipeline is nonblocking as it needs to be available for software configuration of the MMU and debug operations.The Consume pipeline is solely used for cores to read data from the queues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Microarchitecture of MAPLE: designed to maximize MLP and area efficiency. The pipelines allow several concurrent operations, one per pipeline stage. The design has separate pipelines to avoid deadlock situations (formally verified). The LIMA logic loads chunks of adjacent data (B[i]) and performs pointer indirection for each word by internally feeding pointers (A[B[i]]) into the Produce path.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Integration of MAPLE into an OpenPiton tile. MAPLE only needs to be connected to the NoC through its parameterizable encoders and decoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Speedups obtained with decoupling (1 Access and 1 Execute thread), normalized to 2-thread doall parallelism. It showcases that decoupling only in software is not effective on the in-order baseline without hardware support.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>Figure9: Speedups obtained for a single-thread doing nonspeculative prefetching with MAPLE (using the LIMA operation) and conventional software prefetching, normalized to no prefetching. It shows that placing the IMA prefetches into MAPLE queues is desirable over prefetching into the L1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Average clock cycles of load instructions. It compares software prefetching and LIMA operation. It shows that MAPLE prefetches are timely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Speedup (y-axis) achieved with MAPLE, DeSC, and DROPLET over the baseline. Decoupling with MAPLE and DeSC use 1-Access and 1-Execute threads, while DROPLET and the baseline perform 2-thread doall.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Speedup (y-axis) achieved with decoupling (threads are sharing a single MAPLE unit) over do-all parallelism, with scaling threads: 2, 4, and 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Step by step breakdown of the round trip latency of Core-to-MAPLE communication at the OpenPiton framework. Latency could be lower if L1 requests would not pass through the L1.5 cache. A lower communication latency would incur greater performance benefits (studied in Figure 15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Speedup (y-axis) achieved with different core-to-MAPLE latency values, to study the impact of communication latency on performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Classification of the hardware-assisted prior work on IMA latency mitigation, based on the key features that make the adoption of a hardware technique practical for SoCs.</figDesc><table><row><cell>Amenable for /</cell><cell cols="3">Unmodif. Unmodif. Simple HW-SW</cell></row><row><cell>Proposed Technique</cell><cell>Cores</cell><cell>ISA</cell><cell>Cores Co-design</cell></row><row><cell>HW DAE [21, 36, 49]</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Access thread Execute thread ITERATION i ITERATION i ITERATION i+1</head><label></label><figDesc></figDesc><table><row><cell>for (i=0; i&lt;N; i++) produce(&amp;A[B[i]])</cell><cell></cell><cell cols="2">C O N S U M E (i ) p o in te r (i + x ) P R O D U C E</cell><cell cols="3">Access thread runs X loop iterations ahead of the Execute thread, since MAPLE loads the data</cell></row><row><cell>for (i=0; i&lt;N; i++)</cell><cell></cell><cell cols="2">data</cell><cell></cell><cell></cell></row><row><cell>data = consume()</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>res[i] = data * 42</cell><cell>0</cell><cell>50</cell><cell cols="2">100</cell><cell>150</cell><cell>200</cell></row><row><cell>res[i] = data * 42 for (i=0; i&lt;N; i++) data = A[B[i]] Original program</cell><cell cols="2">L D B [i ] L D A [B [i ]] C O N S U M E (i )</cell><cell cols="3">DRAM latency Lost of runahead due to stalled Access thread</cell><cell>data C O N S U M E (i ) d a ta (i ) P R O D U C E data</cell></row><row><cell cols="7">Figure 2: Memory transactions timeline of a decoupled pro-</cell></row><row><cell cols="7">gram running on a thin core baseline. The original program</cell></row><row><cell cols="7">(in red) has been sliced into Access (green) and Execute (blue)</cell></row><row><cell cols="7">threads, using a software API for decoupling. The figure</cell></row><row><cell cols="7">shows two executions targeting our decoupling API: using</cell></row><row><cell cols="7">MAPLE's implementation (above the timeline), and using a</cell></row><row><cell cols="5">shared-memory implementation (below).</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure 4: Code example using MAPLE for prefetches of tight Loops of IMAs (LIMA). This avoids the code bloat problem of software prefetching. MAPLE can issue prefetches that place the data in the LLC (speculative, shown here) or into MAPLE queues (non-speculative). The IMA is marked in red and the cache-friendly access is marked in green.an asymmetric decoupling relation. This is possible with MAPLE (see Section 3.6), unlike with previous architectures for DAE, which only scale in pairs of Access-Execute cores<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref>.</figDesc><table><row><cell>is distance in number of iterations</cell></row><row><cell>LIMA (A, B, ptr[i+D], ptr[i+1+D]);</cell></row><row><cell>for ( j = ptr[i]; j &lt; ptr[i+1]; j++ ){</cell></row><row><cell>res[j] = C[j] * A[B[j]];</cell></row><row><cell>}</cell></row><row><cell>}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>SoC configuration for the full-system evaluation booting Linux v5.6-rc4, including MAPLE (top); FPGA board specification and prototype utilization (bottom).</figDesc><table><row><cell>SoC configuration</cell><cell>OpenPiton + MAPLE</cell></row><row><cell>MAPLE Instances / Scratchpad Size</cell><cell>1 / 1KB</cell></row><row><cell>Core Count / Threads per core</cell><cell>2 / 1</cell></row><row><cell>Core Type</cell><cell>RISCV64 Ariane 6-Stage In-O</cell></row><row><cell>L1D+L1I per core / Latency</cell><cell>8KB+16KB 4-way / 2-cycle</cell></row><row><cell>L2-size per tile (shared) / Latency</cell><cell>64KB 8-way / 30-cycle</cell></row><row><cell>FPGA board</cell><cell>Virtex 7</cell></row><row><cell>Model</cell><cell>XC7VX485T-2FFG1761C</cell></row><row><cell>Board</cell><cell>Xilinx VC707</cell></row><row><cell>Core Frequency</cell><cell>60MHz</cell></row><row><cell>CLB LUTs Utilized</cell><cell>216831(69.9%)</cell></row><row><cell>DRAM Device / Size / Latency</cell><cell>DDR3 / 1GB / 300-cycle</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Core and memory parameters of the simulated system, to compare MAPLE over the prior work.</figDesc><table><row><cell>System Model Parameter</cell><cell>Values</cell></row><row><cell>Core Count / Threads per core</cell><cell>2 / 1</cell></row><row><cell>Instruction Window / ROB Size</cell><cell>1 / 1, In-Order</cell></row><row><cell>L1D (per core) / Latency</cell><cell>8KB / 4-way / 2-cycle</cell></row><row><cell>L2-size (shared) / Latency</cell><cell>64KB / 8-way / 30-cycle</cell></row><row><cell cols="2">DRAM Size / Bandwidth / Latency 4GB / 68GB/s / 300-cycle</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of AFRL, DARPA, NSF, or the U.S. Government.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Tyler Sorensen and Esin Tureci for helping to craft MAPLE's software API. We thank the rest of the DECADES team for their helpful feedback. This material is based on research sponsored by the Air Force Research Laboratory (AFRL) and Defense Advanced Research Projects Agency (DARPA) under agreement No. FA8650-18-2-7862 and the National Science Foundation (NSF) under Grant No. CNS-1823222. <ref type="bibr" target="#b0">1</ref> Prof. Aragón was also supported by Fundación Séneca-Agencia de Ciencia y Tecnología, Región de Murcia, Programa Jiménez de la Espada (21508/EE/21).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Graph Prefetching Using Data Structure Knowledge</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1145/2925426.2926254</idno>
		<ptr target="https://doi.org/10.1145/2925426.2926254" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Supercomputing</title>
				<meeting>the 2016 International Conference on Supercomputing<address><addrLine>Istanbul, Turkey; New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">39</biblScope>
		</imprint>
	</monogr>
	<note>ICS &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Software prefetching for indirect memory accesses</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="305" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An event-triggered programmable prefetcher for irregular workloads</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="578" to="592" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chipyard: Integrated Design, Simulation, and Implementation Framework for Custom SoCs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grubb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karandikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Magyar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pemberton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rigge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nikolić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Rocket chip generator</title>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimas</forename><surname>Avizienis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Biancolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Celio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Dabbelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Izraelevitz</surname></persName>
		</author>
		<idno>UCB/EECS-2016-17</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
		<respStmt>
			<orgName>EECS Department, University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">OpenPiton+Ariane: The First Open-Source, SMP Linux-booting RISC-V System Scaling From One to Many Cores</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Balkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinzheng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schaffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Zaruba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Computer Architecture Research with RISC-V, CARRV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BYOC: A &quot;Bring Your Own Core&quot; Framework for Heterogeneous-ISA Research</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Balkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schaffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Chirkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Lavrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaosheng</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Zaruba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Benini</surname></persName>
		</author>
		<author>
			<persName><surname>Wentzlaff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3373376.3378479</idno>
		<ptr target="https://doi.org/10.1145/3373376.3378479" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems<address><addrLine>Lausanne, Switzerland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="699" to="714" />
		</imprint>
	</monogr>
	<note>ASPLOS &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OpenPiton: An Open Source Manycore Research Framework</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Balkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaosheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Lavrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shahrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adi</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Matl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="217" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis and Optimization of the Memory Hierarchy for Graph Processing Workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="373" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Case for a Programmable Memory Hierarchy</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<ptr target="https://www.sigarch.org/the-case-for-a-programmable-memory-hierarchy/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The effectiveness of decoupling</title>
		<author>
			<persName><forename type="first">Alasdair</forename><surname>Peter L Bird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><forename type="middle">P</forename><surname>Rawsthorne</surname></persName>
		</author>
		<author>
			<persName><surname>Topham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international conference on Supercomputing</title>
				<meeting>the 7th international conference on Supercomputing</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cadence Design Systems</title>
	</analytic>
	<monogr>
		<title level="m">JasperGold Apps User&apos;s Guide</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Case for Embedded Scalable Platforms</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luca</surname></persName>
		</author>
		<author>
			<persName><surname>Carloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Design Automation Conference (DAC)</title>
				<meeting>the 53rd Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speculative precomputation: Longrange prefetching of delinquent loads</title>
		<author>
			<persName><forename type="first">Hong</forename><surname>Jamison D Collins</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dean M Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Fong</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Lavery</surname></persName>
		</author>
		<author>
			<persName><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
				<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">OUTRIDER: Efficient memory latency tolerance with decoupled strands</title>
		<author>
			<persName><forename type="first">Neal</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Crago</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay Jeram</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th annual international symposium on Computer architecture</title>
				<meeting>the 38th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">OpenMP: An Industry-Standard API for Shared-Memory Programming</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Dagum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Menon</surname></persName>
		</author>
		<idno type="DOI">10.1109/99.660313</idno>
		<ptr target="https://doi.org/10.1109/99.660313" />
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="1998-01">1998. Jan. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Celerity Open-Source 511-Core RISC-V Tiered Accelerator Fabric: Fast Architectures and Design Methodologies for Fast Chips</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaolin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Torng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Al-Hawai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><surname>Rovinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tutu</forename><surname>Ajayi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aporva</forename><surname>Amarnath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bandhav</forename><surname>Veluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Dreslinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Batten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename></persName>
		</author>
		<idno type="DOI">10.1109/MM.2018.022071133</idno>
		<ptr target="https://doi.org/10.1109/MM.2018.022071133" />
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="30" to="41" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
		<ptr target="http://faculty.cse.tamu.edu/davis/suitesparse.html" />
		<title level="m">SuiteSparse: A suite of sparse matrix software</title>
				<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Esperanto&apos;s ET-Minion on-chip RISC-V cores</title>
		<author>
			<persName><forename type="first">Esperanto</forename><surname>Technologies</surname></persName>
		</author>
		<ptr target="https://www.esperanto.ai/technology/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trends in Functional Verification: A 2014 Industry Study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Harry</surname></persName>
		</author>
		<author>
			<persName><surname>Foster</surname></persName>
		</author>
		<idno type="DOI">10.1145/2744769.2744921</idno>
		<ptr target="https://doi.org/10.1145/2744769.2744921" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Design Automation Conference</title>
				<meeting>the 52nd Annual Design Automation Conference<address><addrLine>San Francisco, California; New York, NY, USA, Article</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
	<note>DAC &apos;15)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PIPE: a VLSI decoupled architecture</title>
		<author>
			<persName><forename type="first">Jian-Tu</forename><surname>James R Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koujuch</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Liou</surname></persName>
		</author>
		<author>
			<persName><surname>Andrew R Pleszkun</surname></persName>
		</author>
		<author>
			<persName><surname>Schechter</surname></persName>
		</author>
		<author>
			<persName><surname>Honesty</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">DeSC: Decoupled Supply-compute Communication Management for Heterogeneous Architectures</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Aragón</surname></persName>
		</author>
		<author>
			<persName><surname>Martonosi</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Data Supply for Parallel Heterogeneous Architectures</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Aragón</surname></persName>
		</author>
		<author>
			<persName><surname>Martonosi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3310332</idno>
		<ptr target="https://doi.org/10.1145/3310332" />
	</analytic>
	<monogr>
		<title level="j">ACM TACO</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient execution of memory access phases using dataflow specialization</title>
		<author>
			<persName><forename type="first">Chen-Han</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Sankaralingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual International Symposium on Computer Architecture</title>
				<meeting>the 42nd Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="118" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Standard for SystemVerilog-Unified Hardware Design, Specification, and Verification Language</title>
		<idno type="DOI">10.1109/IEEESTD.2013.6469140</idno>
		<ptr target="https://doi.org/10.1109/IEEESTD.2013.6469140" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="1315" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Access map pattern matching for high performance data cache prefetch</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011. 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Program balance and its impact on high performance RISC architectures</title>
		<author>
			<persName><forename type="first">Lizy</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">T</forename><surname>Hulina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><forename type="middle">D</forename><surname>Coraor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 1995 1st IEEE Symposium on High Performance Computer Architecture</title>
				<meeting>1995 1st IEEE Symposium on High Performance Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The tensor algebra compiler</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Kjolstad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lugato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>OOPSLA</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Orchestrating the execution of stream programs on multicore platforms</title>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="114" to="124" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">LLVM: A Compilation Framework for Lifelong Program Analysis &amp; Transformation</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CGO</title>
				<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">When Prefetching Works, When It Doesn&apos;t, and Why</title>
		<author>
			<persName><forename type="first">Jaekyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vuduc</surname></persName>
		</author>
		<idno type="DOI">10.1145/2133382.2133384</idno>
		<ptr target="https://doi.org/10.1145/2133382.2133384" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012-03">2012. March 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kronecker Graphs: An Approach to Modeling Networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Reseach (JMLR)</title>
		<imprint>
			<biblScope unit="page" from="985" to="1042" />
			<date type="published" when="2010-03">2010. March 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Million Core, Multi-Wafer AI Cluster</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Lie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Hot Chips 33 Symposium (HCS)</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">JuxtaPiton: Enabling Heterogeneous-ISA Research with RISC-V and SPARC FPGA Soft-Cores</title>
		<author>
			<persName><forename type="first">Katie</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Balkind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289602.3293958</idno>
		<ptr target="https://doi.org/10.1145/3289602.3293958" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays<address><addrLine>Seaside, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">184</biblScope>
		</imprint>
	</monogr>
	<note>FPGA &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tolerating memory latency through software-controlled pre-execution in simultaneous multithreading processors</title>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28th Annual International Symposium on Computer Architecture</title>
				<meeting>28th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The effects of memory latency and fine-grain parallelism on astronautics ZS-1 performance</title>
		<author>
			<persName><forename type="first">William</forename><surname>Mangione-Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Third Annual Hawaii International Conference on System Sciences</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="288" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GraphAttack: Optimizing Data Supply for Graph Applications on</title>
		<author>
			<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Tureci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Opeoluwa</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Aragón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Order Multicore Architectures. ACM Transactions on Architecture and Code Optimization (TACO)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MosaicSim: A Lightweight, Modular Simulator for Heterogeneous Systems</title>
		<author>
			<persName><forename type="first">Opeoluwa</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Giri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Orenes-Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Tureci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tae Jun</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><forename type="middle">P</forename><surname>Juan L Aragón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Carloni</surname></persName>
		</author>
		<author>
			<persName><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="136" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multithreaded Layer-wise Training of Sparse Deep Neural Networks using Compressed Sparse Column</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hasanzadeh Mofrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Melhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousuf</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Hammoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE High Performance Extreme Computing Conference (HPEC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data Cache Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Symposium on High Performance Computer Architecture (HPCA&apos;04</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="96" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pipette: Improving Core Utilization on Irregular Applications through Intra-Core Pipeline Parallelism</title>
		<author>
			<persName><forename type="first">M</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="596" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AutoSVA: Democratizing Formal Verification of RTL Module Interactions</title>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Orenes-Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Design Automation Conference (DAC)</title>
				<meeting>the 58th Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Triggered Instructions: A Control Paradigm for Spatially-Programmed Architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Crago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gambhir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Allmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rayess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">The synergy of multithreading and access/execute decoupling</title>
		<author>
			<persName><forename type="first">J-M</forename><surname>Parcerisa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Fifth International Symposium on High-Performance Computer Architecture</title>
				<meeting>Fifth International Symposium on High-Performance Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="59" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Decoupled Software Pipelining with the Synchronization Array</title>
		<author>
			<persName><forename type="first">Ram</forename><surname>Rangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">I</forename></persName>
		</author>
		<idno type="DOI">10.1109/PACT.2004.14</idno>
		<ptr target="http://dx.doi.org/10.1109/PACT.2004.14" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
				<meeting>the 13th International Conference on Parallel Architectures and Compilation Techniques (PACT)</meeting>
		<imprint>
			<date type="published" when="2004-08">August. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><surname>Risc-V Foundation</surname></persName>
		</author>
		<ptr target="https://github.com/riscv/riscv-tests." />
		<title level="m">Riscv-tests</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Karl</forename><surname>Rupp</surname></persName>
		</author>
		<ptr target="https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/" />
		<title level="m">42 Years of Microprocessor Trend Data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Decoupled Access/Execute Computer Architectures</title>
		<author>
			<persName><forename type="first">James</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=800048.801719" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Annual Symposium on Computer Architecture</title>
				<meeting>the 9th Annual Symposium on Computer Architecture<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
	<note>ISCA). 8 pages</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Decoupled access/execute computer architectures</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="1982">1982</date>
			<publisher>IEEE Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A simulator and compiler framework for agile hardware-software co-design evaluation and exploration</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Tureci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Orenes-Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Aragón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A Simulator and Compiler Framework for Agile Hardware-Software Co-design Evaluation and Exploration</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aninda</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Tureci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Orenes-Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">L</forename><surname>Aragón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Slipstream Processors Revisited: Exploiting Branch Sets</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="105" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Reevaluating Amdahl&apos;s law in the multicore era</title>
		<author>
			<persName><forename type="first">Xian-He</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and distributed Computing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="183" to="188" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Slipstream processors: Improving both performance and fault tolerance</title>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sundaramoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Purser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rotenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="257" to="268" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Multithreading decoupled architectures for complexity-effective general purpose computing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Krashinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krste</forename><surname>Asanović</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Prodigy: Improving the Memory Latency of Data-Indirect Irregular Workloads Using Hardware-Software Co-Design</title>
		<author>
			<persName><forename type="first">Nishil</forename><surname>Talati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Behroozi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuba</forename><surname>Kaszyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Vasiladiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarunesh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="654" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The raw microprocessor: A computational fabric for software circuits and general-purpose programs</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bedford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wentzlaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fae</forename><surname>Ghodrat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Greenwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Wook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE micro</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="25" to="35" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Clairvoyance: look-ahead compile-time scheduling</title>
		<author>
			<persName><forename type="first">Kim-Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Koukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Själander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasileios</forename><surname>Spiliopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Jimborean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="171" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">SWOOP: software-hardware co-design for non-speculative, execute-ahead, in-order cores</title>
		<author>
			<persName><forename type="first">Kim-Anh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Jimborean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><forename type="middle">E</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Koukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Själander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
				<meeting>the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="328" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">CHIPKIT: An Agile, Reusable Open-Source Framework for Rapid Test Chip Development</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Donato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="32" to="40" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Evaluation of the WM Architecture</title>
		<author>
			<persName><forename type="first">Wm</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th annual international symposium on Computer architecture</title>
				<meeting>the 19th annual international symposium on Computer architecture</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="382" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">IMP: Indirect memory prefetcher</title>
		<author>
			<persName><forename type="first">Xiangyao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadathur</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Devadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th International Symposium on Microarchitecture</title>
				<meeting>the 48th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="178" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Ariane: An Open-Source 64-bit RISC-V Application Class Processor and latest Improvements</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Zaruba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
		<ptr target="https://www.youtube.com/watch?v=8HpvRNh0ux4" />
	</analytic>
	<monogr>
		<title level="m">Technical talk at the RISC-V Workshop</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The Cost of Application-Class Processing: Energy and Performance Analysis of a Linux-Ready 1.7-GHz 64-Bit RISC-V Core in 22-nm FDSOI Technology</title>
		<author>
			<persName><forename type="first">F</forename><surname>Zaruba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVLSI.2019.2926114</idno>
		<ptr target="https://doi.org/10.1109/TVLSI.2019.2926114https://github.com/openhwgroup/cva6" />
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Very Large Scale Integration (VLSI) Systems</title>
				<imprint>
			<date type="published" when="2019-11">2019. Nov 2019</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2629" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Manticore: A 4096-Core RISC-V Chiplet Architecture for Ultraefficient Floating-Point Computing</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Zaruba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Schuiki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="36" to="42" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Accelerating and adapting precomputation threads for efficient prefetching</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE 13th International Symposium on High Performance Computer Architecture</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="85" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Graphit: A high-performance graph dsl</title>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riyadh</forename><surname>Baghdadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoaib</forename><surname>Kamil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>OOPSLA</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
