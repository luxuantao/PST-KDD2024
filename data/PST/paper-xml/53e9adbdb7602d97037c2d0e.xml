<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast multiscale clustering and manifold identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Dan</forename><surname>Kushnir</surname></persName>
							<email>dan.kushnir@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meirav</forename><surname>Galun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Achi</forename><surname>Brandt</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics</orgName>
								<orgName type="institution">The Weizmann Institute of Science</orgName>
								<address>
									<postCode>76100</postCode>
									<settlement>Rehovot</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fast multiscale clustering and manifold identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF40F74CDB33924726FDFC4CFF05765B</idno>
					<idno type="DOI">10.1016/j.patcog.2006.04.007</idno>
					<note type="submission">Received 9 February 2006; accepted 6 April 2006</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Algebraic multigrid (AMG)</term>
					<term>Aggregation</term>
					<term>Graph partitioning</term>
					<term>Similarity-based clustering</term>
					<term>Manifold</term>
					<term>Data analysis</term>
					<term>Astrophysical models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel multiscale clustering algorithm inspired by algebraic multigrid techniques. Our method begins with assembling data points according to local similarities. It uses an aggregation process to obtain reliable scale-dependent global properties, which arise from the local similarities. As the aggregation process proceeds, these global properties affect the formation of coherent clusters. The global features that can be utilized are for example density, shape, intrinsic dimensionality and orientation. The last three features are a part of the manifold identification process which is performed in parallel to the clustering process. The algorithm detects clusters that are distinguished by their multiscale nature, separates between clusters with different densities, and identifies and resolves intersections between clusters. The algorithm is tested on synthetic and real data sets, its running time complexity is linear in the size of the data set.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering algorithms are useful in many fields, from image analysis through astronomy to biology. Generally, clustering is applied to a data set, which is a collection of N d-dimensional vectors (data points) representing d measured features per sample. Given a data set, clustering algorithms seek a partition of the data to coherent groups, in a sense that data points in the same group share similar properties. Many approaches try to solve the clustering problem by optimizing a global cost function, expressed in terms of the local similarities between data points.</p><p>Typical data sets contain clusters that differ from each other in density, and may also contain elongated clusters that could intersect. Moreover, in many cases clusters of interest include points that represent noisy samples from some underlying manifold structures. Also, many data sets are multiscale in nature, containing a nested structure of small clusters within larger clusters. In the scope of this work, we attempt to separate between clusters with different 0031-3203/$30.00 ᭧ 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2006.04.007 densities, to identify intersections between clusters, to support the creation of clusters that preserve smooth continuation, and in particular to distinguish between the different clusters that cross an intersection. To realize these objectives and to discriminate between the different clusters at all scales in the presence of noise, scale-dependent global properties should affect the formation of coherent clusters. The main global features that are utilized in our present study are density, shape, intrinsic dimensionality and orientation. The last three features are part of the manifold identification process, which is performed in parallel to the clustering process.</p><p>The importance of integrating these global features into the clustering process is exemplified in Fig. <ref type="figure" target="#fig_0">1</ref>. It should be emphasized that a variety of additional global features, also called multiscale similarity features or aggregative properties, can be integrated into the process. See for example <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, where in the problem of image segmentation there enter aggregative properties such as average color, color variations at all intermediate scales, boundary match, shape properties of salient sub-aggregates, etc.</p><p>In this paper, we present a novel clustering algorithm inspired by algebraic multigrid techniques (AMG) <ref type="bibr" target="#b2">[3]</ref>. At the basis of our methodology is the normalized-cut cost minimization <ref type="bibr" target="#b3">[4]</ref>, in the sense that salient clusters in the data set can be characterized by low normalized-cut costs. The minimization problem can be formulated as a generalized eigenvector problem. Many other approaches that attempt to solve the clustering problem (including spectral clustering methods e.g. <ref type="bibr" target="#b4">[5]</ref>) formulate the problem as a generalized eigenvector problem, and usually solve it by using an eigensolver with quadratic or cubic complexity. An efficient way in most practical cases to compute eigenvectors at just a linear complexity is offered by an AMG eigensolver <ref type="bibr" target="#b5">[6]</ref>. It is important to realize, however, that the AMG solver contains in itself a hierarchical aggregation procedure which already yields a (hierarchical) clustering of the data set, and that it is much better to directly use this procedure for clustering than actually computing the eigenvectors, or using other clustering procedures. This is because:</p><p>(1) If desired, this procedure can yield the same clustering as computed from the eigenvectors, at a smaller cost. (2) This procedure will actually yield a hierarchical clustering, breaking the clusters into sub-clusters, sub-subclusters, etc. (3) The produced clusters can be fuzzy, with some data points remaining undecided, belonging with different probability weights to different clusters. (4) Most important, the hierarchical aggregation procedure can be modified to account for similarities in global properties of aggregates that emerge at various intermediate levels (scales). To our best knowledge such use of multiscale similarity features cannot be considered by any uniscale procedure, or for that matter, by any formulation of the problem as a functional minimization problem. (5) Top-down procedures can easily be iteratively incorporated at all levels, to affect finer-level aggregation criteria by properties found important at coarser levels.</p><p>Our AMG-like approach discovers the desired aggregation of the data set by following the similarities between the data points at different resolutions, using (similarly to <ref type="bibr" target="#b1">[2]</ref>) a bottom-up weighted aggregation coarsening procedure that preserves the low normalized-cut costs. Moreover, to achieve coherent clusters at all scales, our approach allows combining multiscale similarity features, based on properties of aggregates that emerge at intermediate levels. The combined approach of bottom-up weighted aggregation and multiscale similarity features constructs a hierarchical pyramid of aggregates such that a salient cluster is guaranteed to emerge at a certain appropriate level with low normalized-cut cost. The cost of the algorithm is linear in the data set size, and is independent of the number of clusters.</p><p>Clustering and manifold identification are known to be related. In manifold learning one is interested in finding the intrinsic dimensionality and low dimensional structure of the data. In this work, the clustering and manifold identification processes influence each other, so that the cluster partition is used to approximate the manifolds, and the manifold structure is used to improve the cluster partition. The identification of manifolds created by aggregates is dealt within the bottom-up process by using a scale-dependent local principal component analysis (PCA). An aggregate manifold is represented as a composition of spatially ordered sub-manifolds, each of which is approximately convex and well approximated by a set of principal axes. The aggregate manifold is identified even in the cases in which the manifold is nonconvex and noisy.</p><p>In addition to the bottom-up aggregation process, a top-down process is applied in the present work to resolve intersections between clusters and to separate dense clusters from background noise. Relying on the AMG strength, the algorithm can be applied to data sets of any dimensionality, although the junction resolving, which relies on smooth continuation of orientations, is currently developed only for the cases of clusters with intrinsic dimensionality of 2D and 3D. The complexity of the algorithm is not dependent on the data dimensionality.</p><p>The paper is divided as follows. In Section 2 we describe work related to clustering algorithms and manifold learning. In Section 3 an overall description of the clustering algorithm is given. In Section 4 we demonstrate the use of aggregative properties. In Section 5 the algorithm complexity analysis is presented. In Section 6 clustering results of real astrophysical data in 3D are demonstrated. Section 7 compares our 2D and 3D results with results obtained by other algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>There are numerous approaches for data clustering and manifold learning. In this survey we mostly refer to the algorithms that are related to our approach. For an extensive overview see text books such as Refs. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Spectral clustering methods for graph-based clustering and image segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> use the eigenvectors of the Laplacian matrix to embed the data in a lower subspace where they are expected to be well separated. Spectral clustering uses explicitly eigenvector solvers to find clusters whose graph cut is minimal. The Nyström approximation <ref type="bibr" target="#b8">[9]</ref> is used to decompose the similarity matrix W efficiently by choosing a random sample of the data, so that the complexity of decomposing W to its eigenvectors is of O(nN ) where n is the size of the sample, and N is the size of the data set. Path-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> discover elongated structures and overcome noise. Their complexity is at least O(N 3 ). In the super-paramagnetic clustering (SPC) <ref type="bibr" target="#b11">[12]</ref> method, also known as the granular magnet method, the data points are modelled as a collection of magnets. The scale of the temperatures used in a Monte-Carlo simulation of this collection determines the resolution at which the magnets align to form clusters, creating hierarchical clustering similar to ours. A related work <ref type="bibr" target="#b12">[13]</ref> approaches the clustering problem as a minimal cut problem and produces a stochastic set of cuts by hard contractions of the original graph. The complexity of Ref. <ref type="bibr" target="#b12">[13]</ref> is O(N log 2 (N )). In tensor-voting <ref type="bibr" target="#b13">[14]</ref> additional properties of location and orientation of data points in 2D and 3D are used to cluster data points and characterize their manifold. The method also detects cases of junctions and copes well with noise. Moreover, a criterion to measure smooth continuation between oriented structures, which is based on proximity and curvature, has some similarities with our criterion of completion probability (Section 4.2). Tensor voting divides the data into voxels, each voxel aggregates some data points and geometric features but only on the scale induced by the partition into voxels. The algorithm complexity is O(n 3 k) where n is the number of voxels at the side length of the data set volume, and k is the number of the additional input properties. Some of the early manifold learning methods are the PCA <ref type="bibr" target="#b14">[15]</ref> and multi-dimensional scaling (MDS) <ref type="bibr" target="#b15">[16]</ref>. In extensions of PCA such as the principal curve method (and the principal surface method) <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> one estimates a manifold by computing a smooth curve that passes through the "middle" of a d-dimensional data cloud. In projection pursuit <ref type="bibr" target="#b19">[20]</ref> different optimization strategies are sought to find a basis for local data projections that optimize certain criteria. Our use of scale-dependent local PCA in different scales reveals the shape of the clusters with respect to their scatter. The local-linear embedding (LLE) algorithm <ref type="bibr" target="#b20">[21]</ref> learns the manifold structure by finding a global coordinate system on the manifold. LLE attempts to compute a low dimensional embedding such that nearby points in the high dimensional space remain nearby and similarly co-located in the low dimensional space. A similar approach is the Laplacian eigenmaps (LEM) method <ref type="bibr" target="#b21">[22]</ref>, where the graph Laplacian matrix is used for dimensionality reduction that preserves local proximity. In Isomap <ref type="bibr" target="#b22">[23]</ref> the embedding is optimized with the constraint of preserving geodesic distances. The complexity of LLE, LEM and Isomap is at least O(N 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The clustering algorithm</head><p>The clustering problem can first be formulated as seeking for a minimal normalized-cut in a weighted graph. Given a data set with N data points and a distance matrix D of the dissimilarities between the data points, a weighted graph G = (V , W ) is constructed as follows. Each data point i is represented by a graph node i ∈ V where V ={1, 2, . . . , N}. For every two nodes i and j the following similarity weight is assigned</p><formula xml:id="formula_0">w ij = exp(-c dist • d ij ),<label>(1)</label></formula><p>where c dist is a pre-defined parameter that is determined with experience, and d ij is usually the Euclidean distance between data points i and j; w ii is set to 0. This constructs the similarity matrix W = {w ij }. To evaluate clusters we define a saliency measure as follows. Every cluster S ⊆ V is associated with a state vector u = (u 1 , . . . , u N ) representing the assignments of data points to a cluster S</p><formula xml:id="formula_1">u i = 1, i ∈ S, 0, i / ∈ S.<label>(2)</label></formula><p>The saliency associated with S is defined by the normalizedcut cost</p><formula xml:id="formula_2">(S) def = i&gt;j w ij • (u i -u j ) 2 i&gt;j w ij • u i • u j ,<label>(3)</label></formula><p>which sums the weights along the boundaries of S divided by the internal weights. Clusters with small values of (S) are considered salient. In matrix notation can be written as</p><formula xml:id="formula_3">(S) = u T Lu 1 2 u T W u , (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where L is the Laplacian matrix <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1]</ref> whose elements are</p><formula xml:id="formula_5">l ij = k (k =i) w ik , i = j, -w ij , i = j. (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>If we allow arbitrary real assignments to u, then the minimum of can be obtained by the minimal generalized eigenvector u of Lu = W u ( &gt; 0). Our objective is to find those partitions characterized by a small value of .</p><p>Although an eigensolver, in particular an AMG eigensolver <ref type="bibr" target="#b5">[6]</ref>, can be applied to explicitly solve the generalized eigenvalue problem, we solve the clustering problem by an AMG-like approach (see <ref type="bibr" target="#b2">[3]</ref>) without explicit computation of the eigenvectors. Our AMG-like procedure seeks salient clusters by following the similarity of the data points at different resolutions, from fine scales to coarser ones. Moreover, to further separate clusters at all scales, our AMG-like approach calculates and incorporates multiscale similarity features (e.g. density, shape, intrinsic dimensionality and orientation), which are called aggregative properties. As a result a hierarchical pyramid of graphs is constructed. Each node, at a certain scale, represents an aggregate, which is a weighted collection of the original data points. Each cluster S, which is a salient aggregate (i.e., (S) is low) emerges as a single node at a certain scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Multiscale graph coarsening: weighted aggregation</head><p>Starting from the given graph G [0] = G, we recursively coarsen the minimization problem, creating the sequence of graphs G [1] , . . . , G [k] of decreasing size. At each scale we seek for nodes with low . The salient aggregates, or clusters, represented by low-nodes, are considered as approximate solutions to the minimization problem. As in the general AMG setting, the choice of the coarse variables ("Cpoints"), the design of the fine-to-coarse aggregation (or coarse-to-fine interpolation), and the derivation of the coarse problem are determined automatically, as described below.</p><p>Although the AMG approach can handle the full graph G [0] = G as defined above, the complexity of the algorithm is lowered by applying a dilution procedure which sets to 0 every w ij that is relatively small. We first apply to G [0]  the k-nearest neighbors algorithm (KNN) <ref type="bibr" target="#b23">[24]</ref> (typically 10 k 50). In addition to KNN we apply the following edge dilution procedure <ref type="bibr" target="#b24">[25]</ref>: for each pair of neighboring nodes i and j we set w ij to 0 in case w ij / k: i,k w ik &lt; and w ij / k: j,k w jk &lt; (or w ij /max k: i,k {w ik } &lt; and w ij /max k: j,k {w jk } &lt; ), in our experiments is set to 0.1.</p><p>The edge dilution procedure can be applied at each pyramid level.</p><p>The construction of a coarse graph from a given one is divided into three stages:</p><p>(1) A subset of the fine nodes is chosen to serve as the seeds of the aggregates (the later being the nodes of the coarse graph). ( <ref type="formula" target="#formula_1">2</ref>) The rules for interpolation are determined, thereby establishing the fraction of each non-seed node belonging to each aggregate. (3) The weight of the edges between the coarse nodes is calculated.</p><p>Coarse nodes. The construction of the set of seeds C ("Cpoints") and its complement denoted by F, is guided by the principle that each F-node should be "strongly coupled" to C. To achieve this objective we start with an empty set C, hence F = V , and sequentially (according to decreasing aggregate size defined in Section 3.2) transfer nodes from F to C until all the remaining i ∈ F satisfy j ∈C w ij j ∈V w ij , where is a parameter; in most experiments = 0.2.</p><p>The coarse problem. Each node in the chosen set C becomes the seed of an aggregate that will constitute one coarse scale node. We define for each node i ∈ F a coarse neighborhood N i = {j ∈ C, w ij &gt; 0}. Let I (j) be the index in the coarse graph of the node that represents the aggregate around a seed whose index at the fine scale is j. The classical AMG interpolation matrix P (of size N × n, where n = |C|) is defined by</p><formula xml:id="formula_7">P iI (j ) = w ij / k∈N i w ik for i ∈ F, j ∈ N i , 1 for i ∈ C, j = i, 0 otherwise. (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>It satisfies u ≈ P U, where U = (U 1 , U 2 , . . . , U n ) is the coarse level state vector. P iI represents the likelihood of i to belong to the I th aggregate. Following the weighted aggregation scheme <ref type="bibr" target="#b1">[2]</ref>, the edge connecting two coarse aggregates p and q is assigned with the weight</p><formula xml:id="formula_9">w coarse pq = k =l P kp w kl P lq . (<label>7</label></formula><formula xml:id="formula_10">)</formula><p>w coarse pq is also called the coupling weight between aggregates p and q. Intuitively, the coupling weight between a pair of coarse aggregates (left hand side of ( <ref type="formula" target="#formula_9">7</ref>)) is the weighted sum of the coupling weights between their sub-aggregates (right hand side of ( <ref type="formula" target="#formula_9">7</ref>)). Using the interpolation matrix P, the saliency measure (4) can be written as</p><formula xml:id="formula_11">= u T Lu 1 2 u T W u ≈ U T P T LP U 1 2 U T P T W P U . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>The right hand side of Eq. ( <ref type="formula" target="#formula_11">8</ref>) determines a coarser graph with n nodes whose similarity matrix is W coarse = P T W P .</p><p>Exploiting the sparseness of P, the elements of P T W P are inexpensive to calculate. L coarse = P T LP is approximated by a relation to W as in Eq. ( <ref type="formula" target="#formula_5">5</ref>) <ref type="bibr" target="#b0">[1]</ref>. This coarsening procedure is performed recursively. We denote a coarse scale by s, and its predecessor finer scale by (s -1). The scale index is attached to the graph notation, i.e. a graph at scale s is denoted by G [s] = (V [s] , W [s] ), the ap- propriate interpolation matrix between scale s and (s -1) is denoted by P [s-1][s] or P [s-1] , and |V [s] | is denoted by N [s] . A summary of the coarsening procedure is given in Fig. <ref type="figure" target="#fig_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Aggregative properties</head><p>Consider a specific clustering problem in which small clusters are nested within larger clusters, as exemplified in Fig. <ref type="figure">3</ref>. Multiscale use of the densities may reveal the nested structure and eventually cluster the data appropriately. In Fig. <ref type="figure">3</ref> all the dense patches have similar average density, yet in region A the patches are distributed sparsely, and in region B they are more tightly packed. Hence, at a small scale, the dense patches should be grouped together, yet on a larger scale regions A and B should be partitioned into different clusters as indeed achieved by our algorithm and shown in Fig <ref type="figure">3</ref>.</p><p>In the case of astrophysical flow simulations, where at a given moment each star has a defined location as well as velocity, an important example of aggregative property is the average velocity of an aggregate. While the velocities of individual stars may be quite chaotic, their averages are significant and intermediate-level aggregates with similar averages (and other matching moments) should be grouped together to give a coherent view of the flow. Since our weighted aggregation framework allows to aggregate a variety of multiscale properties, we call these properties aggregative properties. In this framework, for each aggregate i emerging at a certain scale s, we calculate a set of aggregative properties. An aggregative property can be expressed as a weighted average over the aggregate i of a property that has first appeared at a scale r (r s). The scale s is termed the aggregate scale and the scale r is called the property scale. At each scale s the similarity matrix W [s] , inherited from finer aggregate scales <ref type="bibr" target="#b6">(7)</ref>, is modified by the similarities arising from the set of aggregative properties obtained from multiple property scales. In the scope of this work aggregative properties of density, shape, dimensionality and orientation are computed and incorporated. The aggregative properties are used to obtain partition into clusters that differ in density, to separate background noise from clusters, and to resolve intersecting clusters. Moreover, they are utilized to identify the manifold that approximately span each aggregate. As a straightforward aggregative property the reader may consider the center of mass of an aggregate. For an original data point the center of mass is simply its spatial coordinates (in this case r = 0, s = 0). For an aggregate i at scale 1 the center of mass is the weighted average of the spatial coordinates induced by the data points associated with aggregate i, the weights being the interpolation weights (in this case r = 0, s = 1). Similarly, the center of mass of an aggregate at scale s is a weighted average of the center of mass of its sub-aggregates at scale (s -1) associated with it. This is exactly the center of mass as if explicitly calculated from the cloud of data points that assembles this aggregate.</p><p>The following formulas are applied to compute the aggregative properties. Let a property Q appear at the property scale r, where its set of values is</p><formula xml:id="formula_13">Q [r][r] = q [r] = (q [r] 1 , . . . , q [r] N [r]</formula><p>). Then the average of Q over aggregate k at scale s is given by Q</p><formula xml:id="formula_14">[r][s] k = j p [r][s] jk q [r] j / j p [r][s] jk , where p [r][s]</formula><p>jk is the (j, k) element in the product matrix</p><formula xml:id="formula_15">P [r][s] = P [r] • • • P [s-1]</formula><p>, which is the fraction of aggregate j at scale r in aggregate k at scale s. A fast computation of an aggregative property can be achieved by utilizing the following recursive relation:</p><formula xml:id="formula_16">Q [r][s] def = Q [r][s-1] P [s-1] , M [r][s] def = M [r][s-1] P [s-1] , (s &gt;r),<label>(9)</label></formula><p>where M [r][r] def = 1 = (1, . . . , 1) at length N [r] . Note that</p><formula xml:id="formula_17">M [r][s]</formula><p>k is the number of sub-aggregates at scale r that compose the aggregate k at scale s. In particular, M [0][s] k , which is the number of data points which compose aggregate k at scale s, is called aggregate size. From these recursive relations one can then calculate the required weighted average:</p><formula xml:id="formula_18">Q[r][s] k = Q [r][s] k M [r][s] k . (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>In this way the aggregative properties at each level s are calculated from information already accumulated at the immediately preceding level (s -1).</p><p>The geometrical volume.</p><formula xml:id="formula_20">Let x i = (x (1) i , . . . , x (d)</formula><p>i ) be the coordinates of a data point i. The center of mass of aggregate k at scale s is denoted by xk = (</p><p>x(1) k , . . . , x(d) k ) and computed by Eq. <ref type="bibr" target="#b9">(10)</ref>, where r = 0 and</p><formula xml:id="formula_21">Q [0][0] = (x 1 , . . . , x N ). The weighted covariance is the d × d matrix k =(x -xk ) T (x -xk ), i.e., ( k ) =(x ( ) x ( ) ) k - x( ) k x( ) k , where (x ( ) x ( ) ) k is Q[0][s]</formula><p>k calculated by Eq. <ref type="bibr" target="#b9">(10)</ref> with The eigenvalues are used to approximate the geometrical volume of a convex aggregate k at a scale r as follows:</p><formula xml:id="formula_22">Q [0][0] = (x ( ) 1 x ( ) 1 , . . . , x ( ) N x ( ) N ), ( , = 1, . . . , d). PCA is applied to find an eigenvector basis { v (1) k , . . . , v (d) k } of k</formula><formula xml:id="formula_23">V [r][r] k = d i=1 (i) k . (<label>11</label></formula><formula xml:id="formula_24">)</formula><p>The geometrical volume of a non-convex aggregate k at scale s is approximated by the accumulated geometrical volume of its sub-aggregates, i.e., the kth element of</p><formula xml:id="formula_25">V [r][s] =V [r][r] • P [r][s]</formula><p>. The notions of convex and non-convex are explained in the next section.</p><p>The density. The density h of an aggregate i at scale s is defined by the ratio between the number of data points that compose this aggregate and the accumulated geometrical volume:</p><formula xml:id="formula_26">h [r][s] i = M [0][s] i / V [r][s] i</formula><p>, where r is the scale at which PCA is applied (typically r = 3).</p><p>The typical distance of a data point i is the average Euclidean distance from its neighbors: b i = j : i,j xixj /n i , where n i is the number of neighbors. The typical distance b[0][s] i of an aggregate i at scale s is computed by using Eq. ( <ref type="formula" target="#formula_18">10</ref>), for</p><formula xml:id="formula_27">Q [0][0] = (b 1 , b 2 , . . . , b N ).</formula><p>The typical distance is inversely related to the density.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Manifold identification</head><p>The aggregative properties are utilized to identify the manifold that span an aggregate and to reveal the intrinsic dimensionality of the aggregate. The manifold identification significantly affects the formation of clusters with similar intrinsic dimensionality.</p><p>From a certain scale s (typically s=3) we examine for each aggregate k a convexity measure:</p><formula xml:id="formula_28">V [r][s] k /V [s][s] k . If the ratio is less than c (in our experiments c = 1</formula><p>2 ) then the aggregate is considered as non-convex, otherwise it is convex. A type of a manifold is defined directly for a convex aggregate and indirectly (recursively) for a non-convex one. To characterize the manifold type for a convex aggregate k, the definitions below are used.</p><formula xml:id="formula_29">FVAR k (i) def = (i) k d j =1 (j ) k , i = 1, . . . , d<label>(12)</label></formula><p>denotes the fraction of variance obtained in the direction of v (i) k , relatively to the total variance attained in all principal directions. Definition 1. A convex aggregate k is defined as wide in direction v (i) k if FVAR k (i) &gt; /d, for a given (typically = 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 2.</head><p>A convex aggregate is spanned by a 1Dmanifold if it is wide in l directions.</p><p>In case the examined aggregate is non-convex, the manifold type is determined by its sub-aggregates (the finer level aggregates which form it). The sub-aggregates of aggregate k are scanned and if all of them have the same dimensionality l then the manifold type of aggregate k is defined to be l. Otherwise, the manifold type of the aggregate is defined to be heterogenous and identified as a junction. A manifold identification pseudo-code is given in Fig. <ref type="figure">4</ref>. Manifold identification results are demonstrated in Fig. <ref type="figure">5</ref> for three different structures.</p><p>For scales at which aggregates become non-convex we define the notion of tips. Tips are those convex sub-aggregates that form endpoints of the manifold that spans the aggregate (see Fig. <ref type="figure">6</ref>). Tips are currently defined only for 1Dmanifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Algorithm outline</head><p>Aggregative properties of density and dimensionality are used to affect the aggregation so that fine aggregates that have similar dimensionality and density will merge to an aggregate on a coarser scale. Each aggregative property, obtained at a certain scale s, is formulated into a similarity measure between aggregates, and used to modify the coupling weights <ref type="bibr" target="#b6">(7)</ref> between the aggregates at scale s. Those similarity measures are usually an expression depending on the difference between two aggregative properties (an absolute value of the difference or the square of the difference). The utilization of the similarity measures and the coupling modification formulas are elaborated in Section 4. In addition to the bottom-up process, a top-down processes is used to split and merge aggregates of fine scales to correct inaccurate clustering which occurred during the bottom-up process. A more elaborated description of the top-down process is given in Section 4.4.</p><p>The clustering algorithm is summarized by an outline in Fig. <ref type="figure" target="#fig_4">7</ref>, with the following parameters. The top-down procedure is performed at scale s t (typically 6 s t 9) down to a finer scale (typically r = 2). The manifold identification is applied from scale r mn and on (typically r mn = 3). Aggregative properties reflect their similarity by modifying the coupling weights from scale s c and on (typically s c = 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Determining coherent clusters</head><p>In this work we have focused on several specific objectives: to discriminate between clusters with different densities, to remove background noise that may incorrectly merge with clusters, to isolate junctions (intersections) between clusters, and to determine the different clusters that cross the junction in terms of smooth continuation (i.e., the manifold which is formed by each of the intersecting clusters has low curvature). So far we have described the aggregative properties that are accumulated through the bottom-up weighted aggregation. The way that we combine the aggregative properties to achieve those objectives is explained in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Mahalanobis distance</head><p>Given two aggregates k and l at scale s, with respective centers of mass xk , xl and respective covariance matrices k , l , the mutual Mahalanobis distance is computed as follows. The Mahalanobis distance between xl and the cloud of points in aggregate k is given by Mahal</p><formula xml:id="formula_30">(l, k)= ( xk -xl ) • ( k ) -1 • ( xk -xl ) T .</formula><p>Similarly Mahal(k, l) is defined. The Mahalanobis distance can be considered as a weighted Euclidean distance between a point and a cloud of points, where the relevant axes are the principal directions of the cloud and the weights reflect the spread of the data points on the principal directions. The mutual Maha-</p><formula xml:id="formula_31">lanobis distance is Mut_Mahal(k, l) def = Max{Mahal(k, l), Mahal(l, k)}.</formula><p>Starting from a certain scale s (typically s = 3) we bias the aggregation to preserve smooth continuation by multiplying the coupling weights <ref type="bibr" target="#b6">(7)</ref> between any two neighboring aggregates k and l by exp(-c M • Mut_Mahal(k, l)), where in our experiments c M is set to values between 1 and 10. Note that the use of the mutual Mahalanobis distance is restricted to cases where both neighboring aggregates are considered convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Completion probability and manifolds</head><p>The biasing by the mutual Mahalanobis distance promotes continuation between two co-linear clouds of data-points. To promote also smooth continuation upon constant curvature, i.e. co-circularity, we rely on the elastica criterion. The elastica criterion is used extensively in perceptual grouping works (e.g. Refs. <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>). We exploit the elastica criterion to support smooth continuation of aggregates and in particular to discriminate between clusters that cross a junction.</p><p>For two neighboring aggregates, the elastica criterion provides an estimation, called completion probability, that the two aggregates form a smooth continuation. The comple-tion probability described for the 2D case in images <ref type="bibr" target="#b25">[26]</ref> is generalized for 2D and 3D cases of scattered points.</p><p>The completion probability function in the 2D case estimates the probability that two neighboring aggregates k and l, which are 1D-manifolds in R 2 , form a smooth 1Dmanifold (Fig. <ref type="figure" target="#fig_5">8</ref>). The completion probability is based on an energy function which is composed of two components. The first component is the ratio of the distance r between the tips of the two aggregates and a radius of curvature , defined by</p><formula xml:id="formula_32">= L (1) k + L (1) l 2 2 8 • L (2) k + L (2) l 2 ,</formula><p>where L <ref type="bibr" target="#b0">(1)</ref> i and L <ref type="bibr" target="#b1">(2)</ref> i are the length and width of aggregate i, respectively (L</p><formula xml:id="formula_33">(1) i = (1) i , L (2) i = (2) i ), for i = k, l.</formula><p>The ratio r / is denoted by E dist . The second component, denoted as E ang , is a function of the angles k and l , where i ∈ (-/2, /2) (for i =k, l), is the pitch angle of the firstprincipal direction of aggregate i with the line connecting the two centers of mass of the two aggregates. The square difference between the two angles ( kl ) 2 reflects deviation from co-circularity, whereas their combined magnitude ( 2 k + 2 l ) reflects deviation from co-linearity. The energy of co-circularity and co-linearity is given by</p><formula xml:id="formula_34">E ang = r 2 k + 2 l -k • l ,</formula><p>where r is the distance between the centers of mass of the aggregates. The completion probability between aggregates k and l at scale s in the 2D case is proportional to:</p><formula xml:id="formula_35">G [s] (k, l) = exp(-c d • (E dist (k, l)) p d ) • exp(-c g • (E</formula><p>ang (k, l)) p g ), where c d , c g , p d and p g , are predetermined parameters (see Fig. <ref type="figure">9</ref> and Table <ref type="table">1</ref>). These parameters are quite robust for different data sets, yet, an automatic procedure to learn them may be developed in future work.</p><p>The completion curve. Given a pair of aggregates k and l that have high completion probability, a smooth comple-   </p><formula xml:id="formula_36">-1 -1 -0.5 -0 -0.5 -1 -2 -3 -2 -2 -1 0 1 2 3 -1 0 1 2 3 -3 -2 -1 -2 -1<label>0</label></formula><formula xml:id="formula_37">(A) (B) (C) (D) (E) (F) (G) (H)</formula><p>Fig. <ref type="figure">9</ref>. Completion of 1D-manifolds in 2D. The thick lines are the 1st principal axis of each aggregate, the gray curve is the completion curve. The parameters for each example are given in Table <ref type="table">1</ref>.</p><p>Table <ref type="table">1</ref> A set of eight examples in 2D completion probability (see their display in Fig <ref type="figure">9</ref>) In all examples c g = 0.8, c d = 0.3, p d = 1.5, p g = 2; k denotes the left aggregate and l denotes the right one.</p><formula xml:id="formula_38">Exp k l L (1) k L (2) k L (1) l L (2) l r r G(k, l) A 0 0 2 0.</formula><p>tion curve that connects them can be constructed. The cubic spline approximates the elastica curve that minimizes the average curvature between a given pair of points p 1 and p 2 , and their respective gradient values. In our context p 1 = xk and p 2 = xl are the centers of mass of k and l, and the gradient values are given by tan( k ) and tan( l ), respectively (see also Ref. <ref type="bibr" target="#b25">[26]</ref>)).</p><p>To encourage smooth continuation aggregation, the coupling weight between aggregates k and l is replaced by max{max j {w kj }, max j {w lj }} when G [s] (k, l) &gt; t for some predetermined threshold 0 t 1. The completion probability is measured between aggregates that are identified as convex 1D-manifolds. In case the aggregates are identified as 1D-manifolds but one of them is non-convex, the completion probability is measured between their convex tips (see Section 3.3). The 3D case of completion probability is explained in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Density</head><p>In many tasks the density of data points is a meaningful criterion for separating clusters (e.g. Refs. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>), and detecting sparse background noise. The aggregative properties of density and typical distance (Section 3.2) are related measures. Both measures are utilized at the bottom-up process whereas the typical distance measure is also used in the top-down refinement.</p><p>We have defined the density of aggregate i at scale s:</p><formula xml:id="formula_39">h [r][s] i = M [0][s] i /V [r][s] i</formula><p>, which is the ratio between the number of elements that compose the aggregate and its accumulated geometrical volume. The variation of the density of an aggregate i is also of interest and defined by: 2 (h i ) [r][s] = 2] denotes the density of the sub-aggregates of aggregate i at scale s -2. To sup-    port aggregation between aggregates with similar density the coupling weight between any two aggregates k and l at scale s is multiplied by</p><formula xml:id="formula_40">((h [r][s-2] -h [r][s] i ) 2 ), where h [r][s-</formula><formula xml:id="formula_41">exp(-c dens • |h [r][s] k -h [r][s] l | ( (h k ) [r][s] + (h l ) [r][s] )) , (<label>13</label></formula><formula xml:id="formula_42">)</formula><p>where c dens is a non-negative constant; in our experiments c dens is set around 10. Some examples are given inFig. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Top-down</head><p>In addition to the bottom-up process, a top-down process is performed to cure incorrect cluster partitions, according to global features which are detected only on coarse (top) scales. The junction resolving top-down process is used to determine the different manifolds that cross a junction, which has already been detected and isolated during the bottom-up process. The top-down process is based on the information obtained from the manifold identification: the junction and the orientation of manifolds. This high-level information obtained at a coarse scale s is used to modify the coupling weights at a finer scale r. Then the modification is followed by a second bottom-up process starting at scale r. An incorrect clustering result obtained from the initial bottom-up aggregation is demonstrated in Fig. <ref type="figure" target="#fig_9">11</ref>. The outline of the junction top-down resolution is as follows (see Fig. <ref type="figure" target="#fig_12">12</ref> for illustration):</p><p>• For all neighboring aggregates of a suspected junction aggregate i at scale s:</p><p>(1) match a neighbor with high completion probability, and compute their completion curve (Section 4.2). ( <ref type="formula" target="#formula_1">2</ref>) strengthen coupling weights between r-scale subaggregates of aggregate i which reside close to the completion curve. • weaken coupling weights between junction r-scale subaggregates which do not reside on the same completion curve.</p><p>• detect the exact intersection domain: strengthen coupling weights between sub-aggregates that reside on more than one curve, weaken all their other coupling weights. • Perform bottom-up aggregation starting at scale r.</p><p>A set of identified junction examples and their resolving is shown in Fig. <ref type="figure" target="#fig_12">13</ref>.</p><p>The density refinement top-down process is used to detect and separate background noise which may mistakenly merge with clusters. The density refinement reflects the density information which is obtained at a coarse scale s and modifies the coupling weights at a finer scale r. Examples for inaccurate bottom-up clustering and cured top-down clustering are given in Fig. <ref type="figure" target="#fig_11">14</ref>. The outline of the density refinement top-down procedure is given below:</p><p>• For aggregates i at scale s:  (2) for all r-scale sub-aggregates j of i: for all k s.t. w jk &gt; 0:</p><formula xml:id="formula_43">if ( b[0][r] j &lt; b[0][s] i -(b i ) [r][s] or b [0][r] j &gt; b[0][s] i + (b i ) [r][s] ):</formula><p>weaken the coupling weight w [r]  jk , according to the typical distance.</p><p>• Perform bottom-up aggregation from scale r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Complexity</head><p>The high complexity of a clustering algorithm can be a significant barrier when the datasets are large. Also, the preprocessing of calculating the local similarities between the data points can be expensive if done naively. In the scope of this paper, we do not intend to solve efficiently the preprocessing. We use the KNN procedure to obtain a bounded number (5 k 40) of local similarities (neighbors) per a data point. In low dimension KNN has complexity of O(N log N ), whereas in high dimension the complexity is O(N 2 ), where N is the number of data points.  The manifold type of selected aggregates shown in Fig. <ref type="figure" target="#fig_14">15</ref> is determined and compared to the visual detection of the shape patterns.</p><p>The complexity of our multiscale clustering algorithm is O(N). At each scale s of the pyramid four steps are applied: the computation of coupling weights, the computation of the aggregative properties, the modification of W [s]  according to the aggregative properties, and the choice of coarse representative for the next scale. Each of these steps has O(N [s] ) complexity. Therefore, O(N [s] ) operations are done at each scale s. A single graph coarsening step produces a coarse graph with about half the number of nodes of the finer graph. Thereby the complexity of one bottom up process is O(N + N/2 + N/4 + • • •) = O(N ). The complexity of a top-down process starting at a top (coarse) scale s is influenced only by the number of operations performed on the fine scale r (O(N [r] )). Thus, the top-down process complexity is at most O(N ). Therefore, the total complexity of the algorithm is linear in the size of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Clustering of astrophysical data</head><p>The fast growth of exploratory tools in astrophysics yielded massive datasets awaiting to be explored. Some of the underlying tasks in this field are the exploration of the different structures that galaxies and clusters of galaxies form both in real observations and in simulated models. This may serve as a key for determination of the underlying astrophysical model parameters that explain observations. The cold and dark matter (CDM) is assumed to be the major fraction of the universe mass. Cosmological simulations of the universe evolution are based on applying a dynamical model on CDM particles. As time evolves, CDM particles form peculiar structures such as filaments, sheets and spherical clusters, i.e. different manifold types. We use our algorithm in this context to demonstrate its capabilities of detecting structures in such datasets. We also use our algorithm to infer the fitting between an astrophysical model and real observations by comparing the distribution of different manifold types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Cold and dark matter</head><p>The algorithm is applied on a sample of a 3D simulation which contains 20 000 CDM particles (Fig. <ref type="figure" target="#fig_14">15</ref>). The particles positions are the input data points for our algorithm. The advantages of using the aggregative properties and the manifold identification for discovering interesting structures are demonstrated in this example. In this dataset (Fig. <ref type="figure" target="#fig_14">15</ref>) a dense plane, that is composed of dense cores, is sought to be separated from the surrounding sparse noise. Below the plane there is a sparser plane that is almost orthogonal to it. We have used KNN with k = 40. The use of the density feature and the Mahalanobis distance successfully detected the structures in the data. Table <ref type="table" target="#tab_1">2</ref> demonstrates a comparison between our visual detection of structures and the manifold identification of our algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Comparison of a CDM model with a real observation</head><p>The use of our algorithm to compare an astrophysical model with a real observation is demonstrated. Specifically, we have checked a null hypothesis which claims that the multiscale distribution of manifolds of different dimensionality in an observation data set is similar to the distribution in 22 model-based realization data. We have used as an   observation the 2dF galaxy red-shift survey (2dF) and 22 CDM model realizations (i.e. those are 22 data sets of a CDM model simulations). The model and observation have been already found similar with respect to density based criteria <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>The probability of an aggregate to be an iD-manifold; i = 0, 1, 2, 3, is computed at each scale s (denoted by s i ( j )) for each model realization j , j = 1, . . . , 22, and for the 2dF dataset. Sparse clusters whose manifold cannot be identified, due to small number of galaxies belonging to it, are also counted and considered as 0D-manifolds. The average probability for a manifold of type i over all model realizations is computed (denoted by E{ s i }), i.e. E{ s i } = 22 j =1 s i ( j )/22 is the average fraction, over all 22 model realizations, of the clusters found at scale s, whose manifold is an id-manifold. The standard deviation of this measure is also computed (denoted by s i ). The standardized variable matrix 4 × 8 which measures the deviation of the 2dF man-ifold distribution from the model manifold distribution at 8 different scales of our algorithm is computed as Z(i, s) = (E{ s i } -s i (2dF ))/ s i / √ 22. For all i and s Z(i, s) values satisfy |Z(i, s)| &lt; 1.59, which confirms with a 95% confidence interval that the observation is fitted by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Comparison of algorithms</head><p>Several clustering algorithms and our fast multiscale clustering algorithm (FMSC) were applied to examples which are reported in relevant literature and to the CDM example. The algorithms compared are single-linkage <ref type="bibr" target="#b6">[7]</ref> (SL), k-means <ref type="bibr" target="#b32">[33]</ref>, SPC <ref type="bibr" target="#b11">[12]</ref> (also known as the granular magnet method), spectral clustering (SC) <ref type="bibr" target="#b4">[5]</ref>, connectivity-kernel clustering (CKC) <ref type="bibr" target="#b9">[10]</ref>, an algorithm that uses the k-means algorithm with the expectation maximization (EM) algorithm <ref type="bibr" target="#b33">[34]</ref> (KEM), and our FMSC. In all examples the data   points which belong to the same cluster are displayed in the same color. In Figs. <ref type="bibr" target="#b15">16</ref> and 17 we demonstrate our detection of a junction, and the separation between the clusters that cross the junction. It may be required that connected components will be clustered as one cluster. Such a task can be achieved by our algorithm (as shown in Fig. <ref type="figure" target="#fig_9">11</ref>) as well as by other algorithms. However, our intention is to demonstrate how FMSC uses the manifold identification and the orientation of aggregates to separate clusters even when ambiguity in cluster assignment exists, i.e a junction. In Figs.</p><p>18 and 19 we compare the performance of different algorithms in separating dense clusters from noise. Of particular interest is the comparison demonstrated in Fig. <ref type="figure" target="#fig_18">19</ref>, where we detected curved and elongated clusters and separated them from noise. In Fig. <ref type="figure" target="#fig_19">20</ref> we have found some of the underlying structures, yet some improvement in the use of aggregative properties is required in order to separate the whole dense plane in this sample. The k-means results shown in Fig. <ref type="figure" target="#fig_19">20</ref> have completely misclassified the dense clusters. Some of the other algorithms run out of memory resources when tested on the CDM example. Indeed, the comparisons manifest the need for aggregative properties in order to achieve desired clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We presented a novel multiscale clustering algorithm, inspired by AMG. Our AMG-like approach discovers the desired aggregation of the data set by following the similarities between the data points at different resolutions, using a bottom-up weighted aggregation process. Moreover, to achieve coherent clusters at all scales, our approach uses multiscale similarity features and incorporates manifold identification processes. The algorithm detects clusters that are distinguished by their multiscale nature, separates between clusters with different densities and identifies and resolves intersections between clusters. The flexibility of our algorithm which allows to combine other statistics, i.e. additional multiscale similarity features, along with its low complexity, offer a powerful tool for exploring massive data sets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Nested clusters (left): the different distributions of the dense patches at large scale give rise to separation of two different clusters. Intersection of elongated clusters (right): separation of intersecting shapes is possible by tracking their orientation at large scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Graph coarsening pseudo-code procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Nested clusters. Each cluster discovered by our algorithm has a different color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Manifold types. Aggregates that emerged at a certain scale are displayed in different colors. The attached labels explain the manifold type that spans each of the aggregates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The clustering algorithm outline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Completion between two aggregates in 2D. Each aggregate (approximated by a dashed ellipse) creates an angle i (i = k, l), between its 1st principle axis and the line connecting the two centers of mass. The completion curve is drawn between the two centers of mass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Applying the density criterion. Dense regions are identified.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Bottom-up aggregation. Left to right: scales 4-7. A bottom-up aggregation without use of junction detection and resolution is demonstrated. The two intersecting clusters are not separated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .Fig. 13 .</head><label>1213</label><figDesc>Fig.12. Top-down junction resolving. Left to right: scale 8, coarsest scale of the first bottom-up process in which the junction aggregate has been identified; scale 5, sub-aggregates of the junction neighbors are matched to each other by completion probability, and completion curves are drawn; scale 2, fine aggregates are reclassified according to the completion curves; scale 6, coarsest scale of the second bottom-up process where the junction is resolved and the two intersecting clusters are separated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Inaccurate bottom-up clustering and cured top-down clustering. Left to right: scale 9 after first bottom-up process obtained with misclassifications; scale 7, by applying top-down density refinement the background noise is separated from the clusters; scale 9, after first bottom-up process; scale 7, after applying top-down density refinement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>( 1 )</head><label>1</label><figDesc>compute typical distance b[0][s]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>i</head><label></label><figDesc>and standard deviation (b i )[0][s] (Section 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Clustering CDM simulation. Left to right: a sample sub-cube of 20 000 CDM particles, top and side views. Clustering of the data when utilizing the density and Mahalanobis features, top and side views.</figDesc><graphic coords="12,75.87,67.55,437.04,114.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Clustering an 'X' shape. Left to right: SL, SPC, KEM-a bad execution, FMSC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Clustering two intersecting circles. Left to right: k-means, SC, KEM, FMSC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Clustering two dense clusters with noise. Left to right: SL (two small clusters are indicated by arrows), k-means (k = 3), SPC, FMSC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Clustering dense spirals cluttered with noise. Left to right: k-means (k = 4), CKC: (the figure is taken from http://www.inf.ethz.ch/personal/ befische/ nips03/), FMSC.</figDesc><graphic coords="14,85.37,70.04,417.60,123.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Clustering a CDM example. Left to right: k-means k = 4, k-means k = 30, FMSC.</figDesc><graphic coords="14,77.87,240.85,439.20,146.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 21 .</head><label>21</label><figDesc>Fig. 21. Completion of manifolds in 3D. The 1D manifolds.(left): the completion curve is drawn between the two average coordinates. The 2D manifolds. (right): two aggregates k and l are estimated as planes. I (k) and I (l) are interpolated to each other through the completion curve, forming the completion surface. 2 is the roll angle difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Clustering a CDM simulation sample: identifying manifolds</figDesc><table><row><cell>Cluster no.</cell><cell>1</cell><cell>3</cell><cell>13</cell><cell>14</cell><cell>15</cell><cell>21</cell></row><row><cell>Manifold type</cell><cell>2D</cell><cell>2D</cell><cell>3D</cell><cell>2D</cell><cell>2D</cell><cell>3D</cell></row><row><cell>Visual detection</cell><cell>Dense plane</cell><cell>Dense plane</cell><cell>Noise</cell><cell>Lower plane</cell><cell>Dense plane</cell><cell>Noise</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported by the Israel Science Foundation Grant No. 295/01, by the German-Israel Foundation for scientific research and development (GIF) Grant no. I-718-135.6/2001, and by the European Commission Project IST-2002-506766 Aim Shape. We thank Jean-Luc Starck from Service d'Astrophysique, CEA/Saclay, France, for giving us astrophysical data sets. We also thank Ronen Basri, Dorit Ron, Boaz Nadler, Beverly Lewin, and our anonymous reviewers for their insightful remarks.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Completion probability in 3D</head><p>In the 3D case of completion probability there are two situations: 1D-manifolds and 2D-manifolds in R 3 space. In the 1D-manifold case (Fig. <ref type="figure">21</ref>) two angles are defined: the pitch angle between the Z = 0 plane and the first principal axis, and the yaw angle between the Y = 0 plane and the 1st principal axis. The completion probability function reflects co-circularity and co-linearity for both angles:</p><p>In the 2D-manifold case (Fig. <ref type="figure">21</ref>) the ag-gregates manifold structure is approximated by a plane. Given aggregates k and l, two angles are considered: the aggregate's roll angle difference <ref type="bibr" target="#b1">(2 )</ref>, and their pitch angles (). The roll angle difference is measured as the angle formed between the directions of the intersection lines of the aggregate's manifolds with the X = 0 plane and X = r plane, respectively, (the lines are denoted by I (k) and I (l) where I (l) is located at l center). We then fix the Z = 0 plane to intersect X = 0 where the bisector of the directions of I (k) and I (l) lays. The pitch angle of an aggregate k (denoted by (k)) is measured as follows: the intersection I (y, k) of Y = 0 with the planar manifold of k is computed. (k) is the angle between I (y, k) and the Xaxis. Similarly, I (y, l) and (l) are computed. The probability function G reflects co-circularity and co-linearity of the pitch angles, the difference in the roll angle 2 , and the distance between the two aggregates:</p><p>, where c d , c p , and c r are predefined parameters. is computed by using with L [1]  i equal to the length of I (p+, i), and L [2]   i equal to</p><p>(3) i , for i = k, l. In 3D, the completion curve of 1D-manifolds considers two angles: the yow and pitch angles, whereas in 2D-manifolds interpolation of the two planes via the cubic spline creates the completion surface (Fig. <ref type="figure">21</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Texture segmentation by multiscale aggregation of filter responses and shape elements</title>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast multiscale image segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="70" to="77" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algebraic Multigrid (amg) for Automatic Multigrid Solution with Application to Geodetic Computations, Institute for Computational Studies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">POB</title>
		<imprint>
			<biblScope unit="volume">1852</biblScope>
			<date type="published" when="1982">1982</date>
			<pubPlace>Fort Colins, Colorado</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On spectral clustering: analysis and an algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ng Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multigrid solvers and multiscale optimization strategies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multilevel Optimization and VLSICAD</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Cong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Shinnerl</surname></persName>
		</editor>
		<meeting><address><addrLine>Boston</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral grouping using the Nystrom method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="214" to="225" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Clustering with the connectivity kernel</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bagging for path-based clustering</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1411" to="1415" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Data clustering using a model granular magnet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Blatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wiseman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Domany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1805" to="1842" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self organization in vision: stochastic clustering for image segmentation, perceptual grouping, and image database organization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1053" to="1074" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inference of surfaces, 3D curves, and junctions from sparse, noisy, 3D data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1265" to="1277" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jollife</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Multidimensional</forename><surname>Scaling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Principal curves</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">406</biblScope>
			<biblScope unit="page" from="502" to="516" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive principal surfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leblanc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">425</biblScope>
			<biblScope unit="page" from="53" to="64" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finding curvilinear features in spatial point patterns: principal curve clustering with noise</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Stanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Raftery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="601" to="609" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Projection pursuit regression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Stuetzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">376</biblScope>
			<biblScope unit="page" from="817" to="823" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>De Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Kreveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Overmars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Schwarzkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geometry: Algorithms and Applications</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<pubPlace>Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fast multilevel clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Goldschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<idno>MCS05-09</idno>
		<imprint>
			<publisher>The Weizmann Institute of Science</publisher>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science and Applied Mathematics</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Completion energies and scale</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern. Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="70" to="77" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structural saliency: the detection of globally salient structures using a locally connected network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="321" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stochastic completion fields: a neural model of illusory contour shape and salience</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="837" to="858" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Peebles</surname></persName>
		</author>
		<title level="m">The Large Scale Structure of the Universe</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Murtagh</surname></persName>
		</author>
		<title level="m">Astronomical Image and Data Analysis</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The 2dF galaxy redshift survey correlation functions peculiar velocities and the matter density of the Universe</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hawkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mon. Not. R. Astron. Soc</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Implications of 2dFGRS results on cosmic structure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Peacock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIP Conference Proceedings</title>
		<imprint>
			<biblScope unit="volume">666</biblScope>
			<biblScope unit="page" from="275" to="290" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A k-means clustering algorithm</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hartigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Stat</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="100" to="108" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
