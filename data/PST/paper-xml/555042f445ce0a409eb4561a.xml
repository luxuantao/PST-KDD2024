<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Swarm Intelligence for Detecting Interesting Events in Crowded Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vagia</forename><surname>Kaltsa</surname></persName>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Alexia</forename><surname>Briassouli</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ioannis</forename><surname>Kompatsiaris</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Leontios</forename><surname>Hadjileontiadis</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Michael</forename><forename type="middle">G</forename><surname>Strintzis</surname></persName>
						</author>
						<title level="a" type="main">Swarm Intelligence for Detecting Interesting Events in Crowded Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1C8134D07F368B35099322C2F9A6C5E8</idno>
					<idno type="DOI">10.1109/TIP.2015.2409559</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2409559, IEEE Transactions on Image Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2409559, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>swarm intelligence</term>
					<term>crowd</term>
					<term>anomaly</term>
					<term>traffic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work focuses on detecting and localizing anomalous events in videos of crowded scenes, i.e. divergences from a dominant pattern. Both motion and appearance information are considered, so as to robustly distinguish different kinds of anomalies, for a wide range of scenarios. A newly introduced concept based on swarm theory, Histograms of Oriented Swarms (HOS), is applied to capture the dynamics of crowded environments. HOS, together with the well known Histograms of Oriented Gradients (HOG), are combined to build a descriptor that effectively characterizes each scene. These appearance and motion features are only extracted within spatiotemporal volumes of moving pixels to ensure robustness to local noise, increase accuracy in the detection of local, nondominant anomalies, and achieve a lower computational cost. Experiments on benchmark datasets containing various situations with human crowds, as well as on traffic data, led to results that surpassed the current state of the art, confirming the method's efficacy and generality. Finally, the experiments show that our approach achieves significantly higher accuracy, especially for pixel-level event detection compared to State of the Art (SoA) methods, at a low computational cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE widespread use of surveillance systems in roads, stations, airports or malls has led to a huge amount of data that needs to be analyzed for safety, retrieval or even commercial reasons. The task of automatically detecting frames with anomalous or interesting events from long duration video sequences has concerned the research community in the last decade. Event, and especially anomaly detection in crowded scenes is very important, e.g. for security applications, where it is difficult even for trained personnel to reliably monitor scenes with dense crowds or videos of long duration. Numerous methods have been proposed to assist in this direction.</p><p>The analysis of motions and behaviors in crowded scenes constitutes a challenging task for traditional computer vision methods, as barriers like occlusions, varying crowd densities and the complex stochastic nature of their motions are difficult to overcome. Computational cost is one more complicating factor, as it has to be kept within reasonable limits. In many practical situations, it is crucial to analyze crowded scenes in real time, or at least as fast as possible, considering the fact that security personnel should act quickly if something seems to be "not as usual". Furthermore, the ambiguity of the term Copyright (c) 2013 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org. (1) Aristotle University of Thessaloniki, (2) Information Technologies Institute, Multimedia Knowledge and Social Media Analytics Laboratory, CERTH "anomaly" sets its own limitations in our effort to identify it, as there is no commonly accepted definition, and it varies significantly depending on the given scenario. This means that an "anomaly" pattern in one video sequence may often be part of the "normal" pattern of another. In order to address these issues, we define as "anomalies" the events that display a low probability of occurring based on earlier observations. We deal with the challenging problem of detecting abnormal patterns in videos of crowded scenes that emerge as spatiotemporal changes, both in motion and appearance. An appearancerelated anomaly would be, e.g. a bicycle passing through a crowd. Moreover, sudden changes in velocity, like an abrupt increase of its magnitude and the dispersion of individuals in the crowd are detected, indicating that something unusual and potentially dangerous may have occurred.</p><p>In this work we propose a novel method for anomaly detection and localization that incorporates both motion and appearance information. We introduce a descriptor created from Histograms of Oriented Gradients (HOG) to capture appearance, and the newly introduced Histograms of Oriented Swarms (HOS), to capture frame dynamics. Swarm intelligence has been used in the past only in the framework of Particle Swarm Optimization (PSO) in <ref type="bibr" target="#b0">[1]</ref>, where PSO optimizes a fitness function minimizing the interaction force derived from the Social Force Model (SFM). However, in our work, swarms are used in a very different way: the core idea is to construct a prey based on optical flow values over a specific time window and deploy a compact swarm flying over it to acquire accurate and discriminative information of the underlying motion. The agents' motion is determined by forces acting on the swarm (Sec. IV), which, unlike <ref type="bibr" target="#b0">[1]</ref>, do not correspond to the SFM, but are used to determine the swarm motion and location.</p><p>Thus, this work introduces an innovative deployment of swarm intelligence, which, together with the HOG descriptor, forms a new feature capable of successfully determining a region's "normality" in an SVM framework. In order to capture "anomalies" appearing in a small part of the frame, our algorithm is applied only on regions of interest, and temporal information is incorporated to improve accuracy. Even though benchmark datasets of human crowds were mainly used for the algorithm's validation, results on other kinds of videos of crowded scenes, e.g. traffic, reveal that the proposed method can be extended and generalized to different scenarios. The experimental section shows that our algorithm outperforms state of the art (SoA) algorithms in accuracy and at a low computational cost. Our contribution can be summarized as follows:</p><p>1) Swarms are used in an original way, via Histograms of Oriented Swarms (HOS) that are introduced to characterize crowd motion for anomaly detection. They lead to credibly filtered flow in videos of crowds, resulting to very few noisy flow values. Thus, swarm intelligence captures the motion of crowded scenes in an efficient way that can be extended to other types of videos.</p><p>2) The method can be efficiently applied even when the motion in the crowded scene is non-uniform in space and time, and "anomalies" appear locally in a changing context. This is shown in the experiments of Sec. VI on the complete UCSD dataset, where our method's accuracy for pixel level anomaly detection surpasses the SoA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. STATE OF THE ART</head><p>Even though significant research has taken place on event and anomaly detection from static cameras <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> the majority of these works address non-crowded scenes, where detailed visual information can be exploited for each individual. However, real-world surveillance scenarios often involve crowds of people or dense traffic, where such information cannot be easily extracted with traditionally used methods. Therefore, a number of different approaches have been proposed to handle these situations. Several interesting works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> introduce tracking methods, nevertheless, they seem to be effective only in videos with crowds of low density, as tracking is otherwise hindered, due to the high degree of occlusions, while their computational cost is also greatly increased. As a result, the current SoA focuses mostly on analyzing entire frames spatially, temporally or both. Existing methods can be classified in two main categories: those that use only motion information to detect an abnormality in the scene, and those that use both appearance and motion information to describe the scene dynamics.</p><p>In the first category, Wu et al. <ref type="bibr" target="#b7">[8]</ref> use chaotic dynamics in particles' representative trajectories as a means to build a model capable of locating an outlier that moves with a different pattern. Even though this method works for very dense videos where a global motion pattern exists, it is unable to detect local abnormalities that take place in a small region in the frame, or in the absence of a global pattern. Activity recognition based exclusively on trajectories is also proposed by <ref type="bibr" target="#b8">[9]</ref>. However, this method is only based on motion information, completely ignoring the existence of "interesting" activities that exhibit a typical motion pattern. In the same category, Mehran et al. <ref type="bibr" target="#b9">[10]</ref> use the Social Force Model (SFM) to describe a crowd's normal behaviour based on motion characteristics, while Cui et al. <ref type="bibr" target="#b10">[11]</ref> make use of interaction energy potentials derived from the interest points' position and velocity. In <ref type="bibr" target="#b11">[12]</ref>, the min cut/max flow algorithm is used to define each block's dominant direction and crowd motion segmentation is performed by training the algorithm separately, for each spatial location. That method also only relies on motion patterns to detect an anomaly, completely ignoring appearance information. Another interesting work in the same domain, is that of Cong et al. <ref type="bibr" target="#b12">[13]</ref>, who introduce a sparse reconstruction cost to measure the normality of the testing sample, considering dictionary learning methods. Saligrama et al. <ref type="bibr" target="#b13">[14]</ref> extract local low-level motion descriptors and utilize score functions for anomaly detection, derived from local nearest neighbour distances. A different approach is used by Adam et al. <ref type="bibr" target="#b14">[15]</ref> who use fixed monitors to extract local low level features and determine a preset threshold for each monitor in order to declare an alert, while Kim et al. <ref type="bibr" target="#b15">[16]</ref> propose a Markov Random Field model in a Bayesian framework for the final inference. A Gaussian mixture model is used by Ryan <ref type="bibr" target="#b16">[17]</ref> for anomaly detection in crowded scenes based on textures of optical flow in 3D volumes. 3D Gaussian distributions that characterize the underlying motion patterns of spatiotemporal cuboids are used in <ref type="bibr" target="#b17">[18]</ref>. In this work, KL divergence is used as a distance measure to identify similar cuboids in the same location and new prototypes are created accordingly. Observations are then only evaluated according to distributions occurring in the same spatial location by creating a single HMM for each location. As a result the method proposed leads to many false positives in sparse videos, as the number of frames needed to work properly is huge, and it has to cover every region separately in order to train each HMM efficiently. Thus, that method is appropriate for crowded scenes of a very high density, but cannot handle videos of crowds with middle to low density, which are often captured by surveillance cameras. The same 3D gradient features are used by Lu et al. <ref type="bibr" target="#b18">[19]</ref> in a different framework: they propose the use of sparse coefficients to fit new data to a previously learned dictionary. Sparse combination learning is introduced instead of searching the whole search space as classic sparsitybased methods do, thus greatly reducing the computational cost. The method exhibits remarkably high speed performance but at the expense of its accuracy, as shown in the experiments. Finally, an almost real-time algorithm is suggested in <ref type="bibr" target="#b19">[20]</ref> for event detection, based on the clustering statistics derived from moving particles.</p><p>A common problem that is encountered by all the methods mentioned earlier, is their inability to successfully detect anomalies that move similarly to the "normal" motion pattern, as they rely solely on motion characteristics. A second category of methods tackles this issue by incorporating appearance information as well. One work that stands out in this category is that of <ref type="bibr" target="#b20">[21]</ref>, that uses mixtures of dynamic textures to describe each 3D cuboid extracted from video sequence and detect temporal and spatial abnormalities. However, the computational cost of that algorithm, around 25 sec per frame, makes it prohibitive for many applications. An improved version of this method, with a lower computational cost, that is similar to ours, is found in <ref type="bibr" target="#b21">[22]</ref>: that method's accuracy is also improved, but it still remains lower than ours as the experiments in Sec. VI show. The joint modelling of appearance and dynamics is also proposed by Ito et al. <ref type="bibr" target="#b22">[23]</ref> for detecting interesting events via density estimation ratio to classify frames in two classes, normal or abnormal. Despite the applicability of that method to many scenarios, it is only suited for detecting events that occur over the entire frame (e.g. global changes in motions, scene changes etc.) and it misses local abnormalities. Another work that uses features based on both motion and texture is that of <ref type="bibr" target="#b23">[24]</ref>. In that work, the input image is split into nonoverlapping cells and features based on motion, size and texture are extracted and are fed into two classifiers. The main drawback of the method is that the classification of each cell is determined by a pre-defined threshold, which makes the method sensitive to input video. Another interesting work is that of <ref type="bibr" target="#b24">[25]</ref> which proposes a method of detecting abnormalities indirectly after establishing a complete interpretation of the foreground, by using a set of hypotheses. Afterwards, anomalies are defined as those hypotheses that are required to explain the foreground but which themselves cannot be explained by normal training samples. That method works efficiently on the UCSD dataset, however, the need for interpretation of all foreground objects may arise difficulties in more dense crowd datasets. Results on the UCSD dataset provided in Sec.VI, also show that our method provides better accuracy. Finally, the work of <ref type="bibr" target="#b25">[26]</ref> uses densely sampled spatiotemporal video volumes at each pixel location to construct a low level codebook and bag of video words is used to detect anomalous events. However, that method only uses the HOG descriptor to capture both motion and appearance characteristics omitting essential information that could led to better results. As the experiments show, our approach outperforms all the methods described above, in the important pixel level criterion on the UCSD dataset, which is used by most SoA works, making it more suitable for spatiotemporally local anomaly detection.</p><p>The deployment of swarms involves the calculation of internal and external interaction forces, characterized by a number of parameters. In this work, an analytical description of the method's robustness to various parameter values is presented in Sec.VI-A. Currently, new methods are being developed for the evaluation of parameter sensitivity in <ref type="bibr" target="#b26">[27]</ref>, which may be taken into account in future extensions of this work.</p><p>This paper is organised as follows: Sec. III describes the problem formulation, Sec. IV extensively presents the mathematical background of the new descriptor, while anomaly detection and localization is described in Sec. V. Finally, a detailed experimental evaluation is discussed in Sec. VI and conclusions are summarized in Sec. VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROBLEM FORMULATION</head><p>In this work, we address the problem of detecting dynamically changing anomalies in both space and time in videos with crowds of varying densities. In order to effectively capture these anomalies for a wide range of situations, we incorporate both motion and appearance features. Our algorithm uses data derived from automatically extracted regions of interest (ROIs) instead of entire video frames, so as to only process pixels containing information relevant to the event taking place, while at the same time achieving a lower computational cost, fewer false alarms, greater precision and successful spatiotemporal localization of anomalies, both on a global and local scale.</p><p>In order to extract the ROIs, we apply background subtraction using weighted moving mean <ref type="bibr" target="#b27">[28]</ref>, as it has been shown to be robust and reliable, however other SoA background subtraction methods like Gaussian Mixture Models (GMMs) could also be used, leading to equivalent results. We define interest points on a dense grid in the resulting foreground and ROIs are described as rectangular areas of fixed size around each interest point. The size of the ROIs is determined at the beginning of each set of experiments, and depends on the camera viewpoint for each dataset. Due to the static nature of surveillance cameras, the block size needs to be set only once for each camera, or in our case for each dataset, and thus does not affect our algorithm's generality. For the UCSD dataset, a ROI of 20 × 20 pixels is used, as it is large enough to capture activity/appearance related details, but is not too large, so as to include noisy information in the descriptor.</p><p>Once ROIs are extracted, the interest points in them are tracked until the next frames using the KLT tracker, while the foreground grid is continuously updated, with new interest points defined in each new frame's foreground area. The resulting ROIs and the interest points in them are considered informative and are retained if at least 60% of that ROI contains motion, otherwise that interest point and its ROI are considered to be noisy and are ignored. The ROI needs to contain at least 60% moving pixels in order to be as informative as possible; if a ROI contains fewer moving pixels, noisy (motionless) data will also be taken into account, while if it is required to contain more moving pixels, potentially informative interest points may be ignored.</p><p>Spatiotemporal feature extraction from ROIs follows for a particular time window, to acquire descriptors that effectively describe the video dynamics, and help identify both local and global abnormalities. We consider both motion and appearance features, as their combined use allows the detection of anomalies, i.e. deviations of motion and/or appearance from usual patterns, leading to a generally applicable method. An overview of the procedure for extracting the descriptor is depicted in Fig. <ref type="figure" target="#fig_0">1(a)</ref>. The stages for modelling appearance and motion are discussed in more detail in the sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appearance modelling</head><p>In order to extract the appearance characteristics of a video sequence, the Histograms of Oriented Gradients (HOG) proposed in <ref type="bibr" target="#b28">[29]</ref> are used, as the HOG descriptor has several advantages over other appearance features: it is color invariant as it uses gray scale images, and is also invariant to illumination and local geometric transformations as a result of the normalization that takes place. At the same time, it effectively captures the local edge and gradient structure, so it can distinguish variations in appearance even in small areas of the image. The implementation of HOG that is adopted is that of <ref type="bibr" target="#b29">[30]</ref>, as it creates direction invariant HOGs by following a mirroring technique, where mirrored shapes are mapped into the same bin. Direction invariant appearance features (HOGs) decrease intra-class variation, e.g. for walking, which is the predominant activity in human crowds, resulting in similar appearance descriptors for motions in opposite directions. This leads to more robust appearance descriptors that are suitable for the needs of anomaly detection in crowded videos, which can describe, for example, the density or sparseness of a crowd more effectively by ignoring directionality (which is not relevant for appearance). The HOG descriptor is applied in ROI blocks that are tracked over time and are extracted as described in the previous section, so the final HOG descriptor for each block also incorporates temporal information. The procedure for this computation is as follows: each block k is first divided into 2 × 2 cells as suggested in <ref type="bibr" target="#b28">[29]</ref> for a more detailed description that also takes spatial location information into account and mitigates the effects of local noise. For example, if occlusions are present in a ROI, its division into 2×2 cells may limit their presence to only one of the cells, instead of the whole area, leading to a less noisy appearance descriptor. The division of a block into 2 × 2 cells was chosen, as it was found by Dalal et al. <ref type="bibr" target="#b28">[29]</ref> to retain a sufficient level of detail for describing appearance. A weighted histogram of gradients is then created for each cell using 9 bins, corresponding to the gradients' orientation. The HOG of the c th cell (1 ≤ c ≤ 4) in block k of frame j is thus represented by HOG k j (c), of dimension 1 × 9. Each histogram is normalized and the 4 resulting cell histograms are concatenated, forming a 1 × 36 block descriptor, which is also normalized for noise elimination. Once HOGs for each block are calculated for all frames in the temporal window under examination, they are averaged over 3 consecutive frames so as to include richer temporal information and at the same time achieve temporally local noise reduction. The final appearance descriptor is thus a concatenation of a 3 frame average for each cell c in block k:</p><formula xml:id="formula_0">HOG k j,j+2 (c) = E[HOG k j (c), HOG k j+1 (c), HOG k j+2 (c)]<label>(1)</label></formula><p>This means that a 15 frame time window will result in 5 concatenated triplets of 1×36 descriptors, resulting in a 1×180 final spatiotemporal appearance descriptor. The entire process for extracting HOG descriptor is depicted in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. For simplicity of notation, in the sequel, the HOG descriptor of Eq. ( <ref type="formula" target="#formula_0">1</ref>) for block k, averaged over frames j to j + 2 will be represented as HOG j,j+2 including the average over all 4 cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion modelling using HOS descriptor</head><p>This work introduces a novel method for capturing crowd dynamics based on the application of swarm intelligence, which is used to build a novel motion descriptor. Swarm intelligence in computer science is inspired from the behaviour and characteristics of real swarms encountered in nature. Swarms are comprised of individuals, which act autonomously, while following the specific rules of a swarm and interacting with each other. Although the decisions of a swarm's individuals take place locally, their aggregated behaviour can match events in crowded environments, which makes them relevant in many applications, as shown in Sec. VI.</p><p>Swarm based methods have been used in the literature for image filtering and noise reduction <ref type="bibr" target="#b30">[31]</ref>, but their incorporation for the analysis of motion in videos is an original concept first presented in <ref type="bibr" target="#b31">[32]</ref>. The core idea is the monitoring of movements in crowded scenes by a swarm of agents "flying" over them, to capture their dynamics in a collective way while also taking motion history into account. Swarms are thus deployed and the agents' positions are extracted from their accelerated motion, derived from the forces acting on the swarm as described in Sec. IV. They are then used to form Histograms of Oriented Swarms (HOS), which are used to capture the ROIs' underlying motion and detect anomalous events in them. The main concepts of our swarm descriptor are presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SWARM MODELLING FOR CROWDS DYNAMICS</head><p>In our implementation, we adopt physics-based modelling of crowded scenes, as their properties are highly correlated with those of a swarm in nature. The swarm model that is used is based on the general theory described in <ref type="bibr" target="#b30">[31]</ref> and on the behavior of natural swarms, consisting of predators, which "fly" over the "prey", following its dynamics. In <ref type="bibr" target="#b30">[31]</ref>, swarm modelling is used to filter noise in images, whereas in this work it is deployed to better characterize the highly complex and stochastic motion information from videos of crowds. In our implementation, swarms comprise of agents and a prey: the agents "track" the prey, but also interact with each other, as they would in nature. Hence, agents ("predators") are subject to three types of forces: "physical" forces, like inertia and friction, interaction forces between them, and external forces dependent on the prey. Interaction forces ensure the cohesion of the swarm of agents, friction forces maintain elementary memory of the agents' velocity, while external forces depend on the characteristics of the prey being tracked.</p><p>Consequently, in this approach, swarm intelligence maps the motion information into a more informative space by efficiently tracking the motion represented by the prey. Agents filter the prey motion, avoiding false alarms and local noise caused e.g. by occlusions or outlier optical flow values. The prey corresponds to the values of the variable that we want to leverage in the discriminative process. In our case, we are interested in the extraction of motion features via the swarm modelling, so optical flow (OF) values are used as a prey, as detailed in the next section. Thus, the use of swarms is expected to lead to better results than when using OF information alone, as they can capture the most important aspects of crowd behaviour while circumventing the effects of local noise, occlusions and the overall complexity of motion in crowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prey Generation</head><p>The prey that is tracked by the swarm comprises of OF magnitude values of pixels lying inside ROIs, instead of their luminance, which is the case in <ref type="bibr" target="#b30">[31]</ref>. Hence, the number of prey in each frame varies, as it is equal to the number of ROIs in the frame. In this section we describe how prey data is extracted, namely how it is mapped to be tracked by agents. As mentioned previously, ROIs correspond to rectangular areas around each interest point containing a fixed number of n pixels. In order to form the prey for a ROI in a temporal window of m frames, we consider the pixels of each ROI sequentially over time. Each pixel at position i in a particular ROI of frame j has OF magnitude equal to O ij , where 1 ≤ i ≤ n and 1 ≤ j ≤ m. For the prey construction, we consider the i th pixel's OF sequentially over time. The OF magnitude is used to determine the prey's position x p as follows:</p><formula xml:id="formula_1">x p (t) = O ij (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where t is a spatiotemporal index that spans all n ROI pixels over m frames, so that 1 ≤ t ≤ n • m. The selection of the sequence of pixels for prey construction is very important for capturing meaningful temporal information. As a result of the above processing, the final prey position data that the swarm will track for all pixels 1, . . . , nm in each ROI cuboid is defined as:  where the O ij represent the OF magnitude. This process of prey construction is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>.</p><formula xml:id="formula_3">[x p (1), . . . , x p (nm)] = [O 11 , . . . , O 1m , . . . , O n1 , . . . , O nm ] ,<label>(3)</label></formula><p>After each prey is extracted, a swarm of agents is generated to characterize its motion, leading to more accurate analysis of its behavior, as also shown in Sec. VI, where using swarms leads to better classification than when only using the OF.</p><p>The orientation of each pixel's OF is also taken into account for the construction of swarm histograms: the correlation of swarm behavior with OF orientations is high, as swarm behavior (agents' positions and accelerations) for each t is determined by the OF magnitude and orientation of the corresponding pixel. In the following section, we describe agents' dynamics which are then used to create HOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Extraction of Forces</head><p>In this section we present the manner in which the agents operate, i.e. the way they "fly over" the prey and track it. Agents are groups that we define to track the prey and characterize its state: they are initially located in random positions, which change over time according to agent-prey forces, agentto-agent forces and friction forces presented here. The result of these forces' interactions is the accelerated motion of the agents, which is affected and formed according to prey behaviour. These forces are inspired by crowd psychology and the analysis of movements of individuals in crowds <ref type="bibr" target="#b32">[33]</ref>, matching real world behaviors of people (or other entities, like cars or animals) in crowded situations: for example, when agents are too close to each other, repulsive forces develop between them, while the opposite occurs (attraction forces develop) when they are at a large distance, ensuring the cohesion of the swarm of agents. An illustrative example of the way the swarm follows the prey is given in Fig. <ref type="figure" target="#fig_2">3</ref>.</p><p>The interaction force F neigh is the force between agent i and all other agents of the swarm found in the neighbourhood of i, at a distance smaller than ρ. It can be attractive or repulsive depending on the agents' distances in the swarm, and its role is to prevent collisions of the agents and to ensure swarm cohesion. It is determined by the following equation:</p><formula xml:id="formula_4">F neigh (i, t) = j∈Vi F int (i, j, t),<label>(4)</label></formula><p>where F int is the interaction force between each agent i and all other agents j of the swarm in its vicinity V i , determined as the agents whose distance from i is smaller than ρ. We define each agent i's position at t as x i (t), so F int (i, j, t) is:</p><formula xml:id="formula_5">F int (i, j, t) = β • (x i (t -1) -x j (t -1)) d(i, j) 2<label>(5)</label></formula><p>when |x i (t -1) -x j (t -1)| ≤ d min and</p><formula xml:id="formula_6">F int (i, j, t) = -α • (x i (t -1) -x j (t -1)) d(i, j) 2<label>(6)</label></formula><formula xml:id="formula_7">when d min &lt; |x i (t -1) -x j (t -1)| ≤ ρ.</formula><p>Here d(i, j) denotes the distance between agents i and j, x i (t -1) is the previous position of agent i and α, β are weighting parameters set equal to 1, as agent-prey distances are found to be a sufficient measure of internal force strength, and do not need to be amplified or compressed. Nevertheless, the effect of these parameters on the anomaly detection accuracy is investigated in detail in Sec. VI-A, where experiments are run for a very wide range of values of α, β, showing that their values indeed do not greatly affect the outcomes of our method. The value of d min (0 ≤ d min ≤ ρ) sets the boundary that determines if the interaction force is attractive or repulsive.</p><p>The second force is the velocity dependent friction force F f ric that acts on each agent i, offering to the swarm a type of elementary memory. It depends on the velocity the agent formerly had, corresponding to the previous prey location t-1:</p><formula xml:id="formula_8">F f ric (i, t) = -µ • ẋi (t -1)<label>(7)</label></formula><p>where 0 ≤ µ ≤ 1 is the friction coefficient and ẋi (t -1) the former velocity of agent i. After experimentation, µ = 0.4 is found to provide the best tradeoff between tracking speed, smoothness and accuracy as shown analytically in Sec. VI-A.</p><p>Finally, the swarm is driven across the frame mainly by the external force F ext given by Eq. ( <ref type="formula" target="#formula_9">8</ref>) below, which is an elastic force that makes the swarm follow the trajectory of the prey, as every agent is attracted to it. It is an agent-prey force that guides the swarm "over" the prey, so it moves in parallel with it, in our case with the optical flow magnitude:</p><formula xml:id="formula_9">F ext (i, p, t) = λ • (x p (t -1) -x i (t -1)).<label>(8)</label></formula><p>It is clear that the external force between the swarm agents and the prey pixels is directly dependent on their relative position values (in practice the OF magnitude), with the force becoming weaker as the swarm agent diverges from its prey. This force is similar to the restoration force of a harmonic oscillator, so λ represents the positive spring constant, whose value is equal to λ = 1 in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. HOS Descriptor</head><p>In order to form the HOS descriptor, we examine the evolution of the agents' positions, determined by prey motion patterns and the forces affecting the agents. We modify Newton's second law of motion by inserting an elementary parameter γ that takes into account the previous velocity values, as in Eq. ( <ref type="formula" target="#formula_10">9</ref>) shown below. Then, the acceleration ẍi (t) of each agent i at position x i (t) is given by the vector sum of all forces acting on it, considering the fact that an agent's mass equals 1, along with the γ-weighted velocity of the previous time instant. Thus, the acceleration of each agent is given by:</p><formula xml:id="formula_10">ẍi (t) = (γ -1) ẋi (t -1) + F neigh (i, t) + F f ric (i, t) + F ext (i, p, t),<label>(9)</label></formula><p>where γ is a memory parameter, relating past values of the velocity with the current acceleration. When pixel flow undergoes a sudden change, it will be captured by the forces acting on it in Eq. ( <ref type="formula" target="#formula_10">9</ref>), so the influence of its previous value will be mitigated. As a result of the forces, the swarm follows accelerated motion and the velocity of agent i at location x i is:</p><formula xml:id="formula_11">ẋi (t) = γ • ẋi (t -1) + δ • ẍi (t),<label>(10)</label></formula><p>where δ constitutes a timestep parameter, essentially forming an autoregressive process with flow values changing slowly over space and time. Therefore, the positions of agents are continuously updated and their new values are given for each spatiotemporal location t by the following equation:</p><formula xml:id="formula_12">x i (t) = x i (t -1) + δ • ẋi (t -1) + 1 2 ẍi (t)δ 2 . (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>Swarm agents' positions are randomly generated for the first prey position t = 0, and their speeds and accelerations are initially set to zero. Their values change over time depending on prey locations, as described above, and the forces affecting the agents. During training, ROIs are extracted and the pixel OF in them is examined and tracked by the agents. We then compute the average of swarm agents' positions of Eq. ( <ref type="formula" target="#formula_12">11</ref>) for each t, and follow a process similar to the HOG extraction of Sec. III-A to extract weighted histograms of agents' positions (HOS), according to the corresponding OF orientation. As in Sec. III-A, each ROI (block) around each interest point, is partitioned into 2 × 2 cells, and the positions of the swarm agents that follow this particular block establish a weighted histogram of 18 bins according to the OF orientation in each cell. Subsequently, these 4 histograms are concatenated to form the block's HOS. In order to include temporal information, the final motion descriptor contains histograms of subsequent frames, averaged in triplets over each time window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ANOMALY DETECTION AND LOCALIZATION</head><p>Appearance and motion descriptors are combined to form the final descriptor for anomaly detection. In a time window of m frames, average triplets of HOG and HOS are consecutively concatenated, resulting in the feature vector of Eq. ( <ref type="formula" target="#formula_14">12</ref>):</p><formula xml:id="formula_14">f = {HOG 1,3 , HOS 1,3 , . . . , HOG m-2,m , HOS m-2,m }<label>(12)</label></formula><p>Here, HOG m-2,m is the average of HOG histograms corresponding to a block for frames m-2 to m and HOS m-2,m is the average of the corresponding HOS histograms taken from Eq. ( <ref type="formula" target="#formula_0">1</ref>). The overall process takes place in each ROI and it is depicted in Fig. <ref type="figure" target="#fig_3">4</ref>. A normalization step takes place to form the final descriptor so as to achieve scale invariance.</p><p>Afterwards, a Support Vector Machine (SVM) is used to determine each region's normality. SVMs are used, as they generally exhibit good performance relatively to other machine learning methods and are also fast to run, for reliable real time detection. Furthermore, they are able to handle large data sets, which generally appear in real life situations. Because of the infinite number of "anomalies" that can be derived in each case, it is impossible to provide examples of all possible anomaly classes, so a one class classifier is chosen. This way, we provide our system exclusively with normal situations, aiming to identify any irregularities deviating from the normal pattern. This leads to a more accurate and general classifier capable of detecting different kinds of anomalies, even when appearing for the first time in the dataset.</p><p>The Support Vector Data Description (SVDD) method of <ref type="bibr" target="#b33">[34]</ref> was chosen, as it is known to be best suited for outlier detection. According to this approach, spherical boundaries are used instead of planar ones around the provided data of the training set. The goal is to enclose nearly all n training examples in a hypersphere with center o and the smallest possible radius R, with the outliers lying outside this sphere. Thus, its purpose is to minimize the function:</p><formula xml:id="formula_15">min R,o R 2 + C n i=1 ξi (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>subject to:</p><formula xml:id="formula_17">ν i -o 2 ≤ R 2 + ξi , ξi ≥ 0 ∀i<label>(14)</label></formula><p>In order to create a soft margin and allow for outliers in the training set, slack variables ξi and a penalty parameter C describe the hypersphere. By using Lagrange multipliers to solve Eq.( <ref type="formula" target="#formula_15">13</ref>), subject to Eq.( <ref type="formula" target="#formula_17">14</ref>), with a Gaussian kernel, we conclude that a new "test object" z is accepted when:</p><formula xml:id="formula_18">z -o 2 = n i=1 λ i exp z -ν i 2 σ 2 ≥ - R 2 2 + C R (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>otherwise z is an outlier. C R is a constant, dependent only on support vectors ν i , while λ i are the Lagrange multipliers and σ represents the standard deviation of the Gaussian kernel.</p><p>After training, localization is straightforward, as descriptors are estimated spatially in specific ROIs around interest points. Our algorithm checks each frame's ROI independently, infers about its normality and then notifies the system. Hence, our method is capable of dealing with non-uniformly moving and evolving crowds, as the descriptors are examined and characterized separately in each ROI. It can accurately localize different anomalies in a wide range of videos, from human crowds to traffic, as the experiments that follow demonstrate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>In order to evaluate the effectiveness of our method, we applied it on four benchmark datasets of surveillance where different kinds of anomalies were detected. Our algorithm's speed and accuracy on a frame and pixel level were calculated and compared with the SoA, demonstating its effectiveness. An extensive sensitivity analysis has also taken place to examine the effect of varying all parameter values, showing that they do not significantly affect the accuracy of the results.</p><p>As mentioned in Sec. III, temporal information is exploited by extracting features over a specific window in time. The length of the window should be large enough to contain sufficient information and, at the same time, as small as possible to avoid undesirable delays during the detection process. Hence, we use a temporal window length that depends on the frame rate and the underlying dominant motion, which in our case is the mean walking frequency of a pedestrian. As an example, Fig. <ref type="figure" target="#fig_4">5</ref> depicts the optical flow values of a pedestrian for the "ped2" dataset: it can be seen that the entire motion displays periodicity over time, and therefore a temporal window of 15 frames sufficiently captures the entire cycle for  this case. Averaging of the extracted features over time to form triplets follows, as detailed in Sec. III-A, to mitigate the effects of local noise and include richer temporal information in the resulting descriptor.</p><p>The size of the extracted blocks is also scene-related and dependent on the camera view, so as to contain adequate information. As explained in Sec.III, its value is determined in the beginning for each dataset and is fixed for all the shots made by the same camera. In this work, we use a ROI of 20 × 20 for the UCSD dataset as it is small enough to capture anomaly localization, but at the same time large enough to capture useful information about the entities moving in the scene (people, bicylces etc in the UCSD data). The ROI size was determined experimentally, however small variations in its size do not significantly affect the algorithm's accuracy.</p><p>Because of the static nature of surveillance cameras, the size of the temporal window and ROI blocks are only set once for each camera, or in our case once for each dataset, and thus do not compromise the generality of our algorithm. The parameters defining the forces affecting the motion of the swarm agents also remain the same in all experiments. In the next section, the method's sensitivity to different parameters affecting the swarm generation is analyzed in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sensitivity Analysis</head><p>The parameters α, β of Eqs. ( <ref type="formula" target="#formula_5">5</ref>) and ( <ref type="formula" target="#formula_6">6</ref>) are positive constants representing attraction/repulsion internal forces. In our experiments we use a common value for them, as we want the impact of repulsive and attractive forces to be the same (α = β). Eqs. ( <ref type="formula" target="#formula_5">5</ref>) and <ref type="bibr" target="#b5">(6)</ref> show that increasing their values leads to larger internal forces, which move agents by larger distances, either taking them further away or closer to each other. However, in our method, the center of mass of agents' positions is used to determine the swarm's histogram HOS, which depends on the relative position of each group of agents, rather than the actual values of these internal forces. For lower values of α, β, the individual agents move less with respect to each other, and for larger α, β they move more, but on average the swarm's position stays the same. This is also shown in Fig. <ref type="figure" target="#fig_6">7</ref>, where the Equal Error Rate (EER) is shown for the UCSD dataset for an extremely wide range of values of β ∈ [0, 10000]: the EER fluctuates very little, even for these very large differentiations in β, proving that algorithm's performance is barely affected by these constants. Subsequently, we set α = β = 1 for all experiments. Fig. <ref type="figure" target="#fig_5">6</ref> shows the effect of changing the friction force parameter µ of Eq. ( <ref type="formula" target="#formula_8">7</ref>). The parameter µ is essentially the equivalent of the spring constant in the definition of a force, with a higher  value indicating the presence of more friction. For a lower µ (µ = 0.1), it can be seen that less friction leads to faster but less smooth tracking, as the effect of previous values, is given less weight. Lowering the value of µ can make the friction force more prone to errors, derived from OF values, while increasing its value can result in unstable oscillations, with F f ric reflecting temporally local noise, as Fig. <ref type="figure" target="#fig_5">6</ref>(c) depicts. In order to demonstrate the smoothness of the resulting tracking as a function of the parameter µ, we plot the variance of the agents' positions x i (t) for 0.1 ≤ µ ≤ 0.7 in Fig. <ref type="figure" target="#fig_7">8</ref>, for a video sample from the UCSD dataset. It can be seen that, for smaller values of µ, the variance does not show significant changes, while for µ ≥ 0.7 it rises dramatically. For these reasons, and based on the experimental data shown in Fig. <ref type="figure" target="#fig_5">6</ref>, we use µ = 0.4, even though values of µ ≤ 0.6 do not really greatly affect the accuracy of our system.</p><p>The parameter γ constitutes an elementary memory parameter that determines the effect of the previous agent's velocity on the current agent's acceleration. In Fig. <ref type="figure">9</ref>(a), the swarm's position in relation to the prey is depicted for different values of γ: for larger values of the parameter, the swarm moves more quickly, but less smoothly. For values greater or equal to 0.8, the system starts to oscillate, so we choose values for γ in the range [0, 0.7]. In Fig. <ref type="figure">9</ref>(b), the EER error for the UCSD dataset is depicted for values of γ in this range, where it can be seen that the EER at the frame level is barely affected, while the pixel level EER varies by about 10%, especially for γ &gt; 0.4. Hence, in our experiments we use γ = 0.4, as it leads to better results, even though other values of γ ∈ [0, 0.7] do not cause significant fluctuations in the method's performance.</p><p>One last parameter that needs to be set for the swarm formulation is δ. This is equivalent to a timestep parameter, as can be seen in Eqs. ( <ref type="formula" target="#formula_11">10</ref>), <ref type="bibr" target="#b10">(11)</ref>, that corresponds to the time that each agent needs to move from its previous position to the current one. Larger values of δ make the swarm respond more quickly to the prey, but can produce overshoots as Fig. <ref type="figure" target="#fig_0">10(a</ref>) depicts for δ ≥ 1.3. On the other hand, very small values δ ≤ 0.2 lead to a system that cannot follow the prey's trajectory. In our experiments we choose δ = 0.4, as it is shown that with this value the swarm is capable of following the prey to a sufficient extent, while maximizing our system's performance. Fig. <ref type="figure" target="#fig_0">10(b)</ref> shows the sensitivity of our algorithm to different values of δ.</p><p>It should be pointed out that in all the experiments the parameters described above remain fixed to their optimal values. As the extensive analysis proved, after defining the range of each variable that ensures system stability, our algorithm's performance is not particularly influenced by further variations in these parameter values.</p><p>Finally, the number of agents forming the swarm is fixed to 5, as it is empirically found that this number sufficiently represents the filtered motion dynamics of the scene without negatively affecting the algorithm's speed. Experiments showed that the use of more agents heavily increased the computational cost, with a computational time of 3.86 sec per frame if the number of agents increased to 50, while the algorithm's performance actually decreased. This can be attributed to the fact that the presence of too many agents may lead to noisy internal forces due to the density of the swarm, which eventually degrades the results. On the other hand, the use of fewer agents, as few as 2 agents for example in "ped1", also decreased algorithm's performance from 78.87% to 73.66%. The initial agents' speed and accelerations are set to zero, whereas their initial positions are randomly generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>In order to evaluate our method, we use the same criteria as the SoA literature for benchmark datasets. Thus, the frame and pixel level criteria described in <ref type="bibr" target="#b20">[21]</ref> are adopted for UCSD dataset in Sec. VI-C, while the Area Under the Curve (AUC) is used for the UMN and U-turn videos described in Sec. VI-D and Sec.VI-E respectively.</p><p>The frame level criterion localizes changes only in time, predicting which frames contain an anomaly, without finding its spatial location: a frame is thus characterized as abnormal if it contains at least one abnormality, wherever it is located. In contrast, the pixel level criterion includes both temporal and spatial anomaly localization, and is used in the literature <ref type="bibr" target="#b20">[21]</ref> as follows: if at least 40% of all anomalous pixels are found (as determined by the ground truth annotation), the detection is considered successful and the frame is characterized as abnormal. True positives and false positives are then derived by comparing the spatiotemporally detected anomalies with the ground truth, leading to Receiver Operating Characteristic (ROC) curves of true positives vs. false positives to evaluate the method's performance. It should be emphasized that the pixel level criterion is a more detailed, precise and reliable evaluation measure, since it localizes anomalies in both space and time. On the other hand, the frame level criterion's results, based on the correct detection of abnormal frames, may sometimes be coincidental, resulting from false positives appearing in frames that include true anomalies, without these anomalies having actually been detected.</p><p>The evaluation metrics used are derived from the ROC curves: the Equal Error Rate (EER) corresponds to the frame level criterion, while the Detection Rate (DR) corresponds to the pixel level criterion. These metrics have been widely used in the literature for the benchmark UCSD dataset, as they provide a reliable criterion to evaluate method's performance and to compare it with other SoA works. The EER corresponds to the error rate of a system when the false positives (detections of anomalies in a normal situation) are equal to the false negatives (missed anomaly detections). This is achieved by adjusting the threshold for accepting/rejecting a change until equal errors are achieved. The lower the EER, the higher the accuracy of the system. The DR, on the other hand, refers Equal Error Rate (EER) and Detection Rate (DR) in "ped1" SF <ref type="bibr" target="#b9">[10]</ref> MPPCA <ref type="bibr" target="#b15">[16]</ref> Adam <ref type="bibr" target="#b14">[15]</ref> Sparse <ref type="bibr" target="#b12">[13]</ref> PSO <ref type="bibr" target="#b0">[1]</ref> BVP <ref type="bibr" target="#b24">[25]</ref> Roshtkhari <ref type="bibr" target="#b25">[26]</ref> 150fps <ref type="bibr" target="#b18">[19]</ref> H-MDT (CRF) <ref type="bibr">[</ref>  to the successful detection rate of the anomalies happening at EER, with higher detection rates implying a better performance of our algorithm. The AUC criterion is simply the area under the ROC curve, derived for the UMN and U-turn datasets, as this criterion is also used in the literature for those videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UCSD dataset</head><p>The UCSD dataset is comprised of two subsets "ped1" and "ped2", containing different scenes recorded from different camera angles <ref type="bibr" target="#b20">[21]</ref>. Each "ped1", "ped2" subset is divided into a training set containing exclusively normal frames and a test set, including different kinds of anomalies. The dataset consists of crowds of medium density traversing the scene ("ped2") or moving towards and away from camera, adding some perspective ("ped1"). The UCSD dataset constitutes a challenging dataset, as it contains many occlusions, a variety of anomalies, sometimes co-occurring in the same frame, and its resolution is of low quality. Anomalies present in the test set include bicycles, skaters or other wheeled objects moving with different speeds and passing through the crowd, which are in some cases difficult to detect even for human observers. "Ped1" comprises of 34 normal training clips and 36 test clips of size of 158×238 pixels, while "ped2" consists of 16 training clips and 14 test clips of 240 × 360 pixels.</p><p>Table <ref type="table" target="#tab_0">I</ref> depicts the evaluation of the proposed algorithm for the "ped1" dataset, presenting the Equal Error Rate (EER) and the detection rate (DR) for the frame and pixel level criterion respectively. The full annotation provided by <ref type="bibr" target="#b24">[25]</ref> was used for pixel level criterion. The method is compared against 9 other SoA works, which use different approaches to detect spatiotemporal anomalies in this dataset. These include descriptors based on social force flow dynamics <ref type="bibr" target="#b9">[10]</ref>, the mixture of optical flow observations (MPPCA) <ref type="bibr" target="#b15">[16]</ref>, the use of local low level motion histograms <ref type="bibr" target="#b14">[15]</ref>, sparse reconstruction approaches in motion histograms <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, particle swarm optimization (PSO) to optimize a fitness function based on SFM <ref type="bibr" target="#b0">[1]</ref>, a Bayesian video parsing approach using a set of hypotheses that jointly explains the foreground <ref type="bibr" target="#b24">[25]</ref>, a bag of video words approach <ref type="bibr" target="#b25">[26]</ref> and finally the hierarchical mixture of dynamic textures (H-MDT) after applying CRF filtering <ref type="bibr" target="#b21">[22]</ref>, as this variation of their method led to SoA results.</p><p>As observed from Table <ref type="table" target="#tab_0">I</ref> our method greatly outperforms all other existing methods for the pixel level criterion, while for the frame level criterion it gives comparable results. However, as stated above, the frame level criterion is a less detailed and reliable descriptor of the performance of the algorithm than the pixel level criterion, as in some cases even perfect frame level anomaly detection can be achieved "coincidentally" by only detecting false positives. Therefore, we consider that overall our method significantly improves upon the SoA.</p><p>The results for "ped2" are presented in Table <ref type="table" target="#tab_0">II</ref>. In this case, our method is compared for both evaluation criteria with 4 SoA approaches, as in <ref type="bibr" target="#b21">[22]</ref>. Our method once again leads to better performance for the more precise pixel level criterion, while comparable results are obtained for the frame level evaluation. In Fig. <ref type="figure" target="#fig_2">13</ref> the ROC curves for the complete UCSD dataset are presented.</p><p>Fig. <ref type="figure" target="#fig_0">11</ref> and Fig. <ref type="figure" target="#fig_1">12</ref> depict screenshots from successful detections in the "ped1" and "ped2" datasets respectively. As can be seen, different kinds of "anomalies" are successfully localized even when they co-occur in the same frame, as in Fig. <ref type="figure" target="#fig_0">11(f)</ref>. A remarkable achievement of the proposed method is that deviations from normal patterns can be also detected in highly occluded scenes, as Fig. <ref type="figure" target="#fig_1">12</ref>(e)-(f) illustrates. In these cases, a bicycle and a skater respectively are correctly identified as "anomalies", even though their detection is a challenging task even for a human observer. Videos with the outcomes of our algorithm on the UCSD dataset can be found in the following link: http://mklab.iti.gr/people/vagiakal.</p><p>We conducted the same experiments without including the swarms and using instead only the OF, while the rest of our algorithm remained the same to evaluate the effect of swarm intelligence. Table <ref type="table" target="#tab_3">III</ref> shows the results for both cases in its first two rows, demonstrating that swarms effectively "filter" the OF values, leading to more accurate results and reinforcing our decision to incorporate them in our algorithm. For both "ped1" and "ped2", a higher detection rate is achieved when using swarms, while the EER is lower. Additionally, in order to examine the impact of the appearance and motion descriptors separately on performance, results are presented in the same table for when only the motion descriptor (HOS) or only the appearance descriptor (HOG) is used. For the "ped1" dataset, the use of both descriptors led to better results than using them separately. Nevertheless, for "ped2" the inclusion of the HOG descriptor worsened the algorithm's performance. However, as Table <ref type="table" target="#tab_3">III</ref> shows, the results when using both descriptors or only motion descriptor are still comparable, proving that our decision of combining both descriptors is justified as it can be generalised in more cases, giving better or sometimes comparable results.</p><p>In Table <ref type="table" target="#tab_0">IV</ref>, the computational cost of the method proposed for the UCSD dataset is presented, in comparison with 5 other SoA methods. Our method ranks on the average second in speed performance after the sparse approach of <ref type="bibr" target="#b18">[19]</ref>, which achieved a remarkably low computational cost, but at the expense of algorithm's detection performance. As it can be seen, their method achieves only a detection rate of 59.1% against ours of 78.87% and 74.92% for "ped1" and "ped2" dataset respectively. In comparison with the H-MDT (CRF) method, our approach exhibits lower cost with better detection performance for "ped1" dataset and comparable cost for "ped2", while also achieving better detection performance. Overall, Table <ref type="table" target="#tab_0">IV</ref> shows that our algorithm achieves the best detection performance results at a low computational cost. All experiments were conducted on a 16GB RAM computer with a 3.5 GHz CPU. The algorithm runs in C++, without being optimized, meaning that the computational cost can be reduced even further, making it applicable to real world situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. UMN dataset</head><p>The UMN dataset <ref type="bibr" target="#b34">[35]</ref> consists of 7739 frames of 320×240 pixels in 3 different scenes (umn 1 , umn 2 , umn 3 ) including respectively 2, 6 and 3 scenarios of crowd escape events. The first frames in each event depict a normal crowd situation,  with people walking or standing in the scene, while "anomaly" takes place with a sudden evacuation. This data is quite straightforward, as the "anomaly" is global and can be easily detected even by only using the average frame motion. As a result, many methods have been proposed for this data, achieving near perfect scores.</p><p>The main drawback of this dataset is its limited size, in combination with the absence of a separate training set. The limited number of training frames results in a not well defined "normal" class. Our descriptor uses detailed appearance and motion information, however these change significantly, even in the "normal" frames, so it requires more training data for a better defined description of the "normal" events. As a result of its limited size, this data does not allow us to demonstrate the true potential of our method, which uses many complex features so as to be applicable to more difficult videos.</p><p>For training, normal frames of one scenario from scene 1 and two scenarios from scenes 2 and 3 were used to model normal crowd behaviors, while the rest of the frames were used for testing. Fig. <ref type="figure" target="#fig_3">14</ref> has screenshots of our algorithm's outcome for all 3 scenes, while in Table V comparisons with 5 SoA Fig. <ref type="figure" target="#fig_2">13</ref>. ROC curves for the full UCSD dataset -top row: pixel level criterion, bottom row: frame level criterion.</p><p>methods <ref type="bibr" target="#b21">[22]</ref> are provided. Anomalies are correctly detected and localized in all cases, however, some false positives appear due to the above mentioned lack of adequate training. The total performance of our algorithm reached 99.59% for umn 1 , 93.38% for umn 2 and 98.08% for umn 3 using the Area Under Curve (AUC) criterion, which is used in the literature for UMN. Its performance is therefore shown to be near perfect, comparable with the SoA, with the exception of umn 2 , where it achieved very high, but not perfect, results. This is attributed to the sparseness of the training data in umn 2 , which were even less informative than those of umn 1 and umn 3 . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. U-turn</head><p>In order to confirm our method's robustness, we also applied it to a non-crowd dataset. We used the U-turn dataset of <ref type="bibr" target="#b35">[36]</ref>, which shows normal traffic in a crossroad and some cars making illegal U-turns ("anomaly"). The dataset comprises of 6117 frames of 360 × 240 pixels. The scenes are quite sparse and, in combination with the dataset's limited size, there is not much training data. However, even with limited training samples, all anomalies are perfectly detected and localized, as can be seen in Fig. <ref type="figure" target="#fig_4">15</ref>. It is remarkable that in the first frame in Fig. <ref type="figure" target="#fig_4">15</ref>, our algorithm correctly distinguishes between an illegal turn and a legal one. Around 3400 frames depicting normal traffic were used for training, and the rest were used for testing. In Fig. <ref type="figure" target="#fig_5">16</ref>, the ROC curve of our method for the U-turn data is compared with the results provided by <ref type="bibr" target="#b21">[22]</ref>. As it is shown, we achieve the highest AUC at the frame level, equal to 95, 31%, with all "anomalies" having been correctly detected and localized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Love Parade</head><p>The algorithm was also tested on the surveillance data of Love Parade 2010 <ref type="bibr" target="#b36">[37]</ref>, which contains videos of high density crowds. Snapshots are provided in Fig. <ref type="figure" target="#fig_12">17</ref> and, as it can be observed, deviations from normal crowd patterns are correctly detected and localized, despite the little motion present, and the high number of occlusions, due to the high crowd density. Around 1000 frames were used for training with the rest of the frames used for testing. The truck and ambulance are successfully detected while traversing a highly dense crowd, whereas people in the crowd jumping over railings are also detected as an anomalous behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work, we propose a novel framework for anomaly detection in different scenarios, recorded from static surveillance cameras. Swarm intelligence is exploited for the extraction of robust motion characteristics and together, with appearance features, form a descriptor capable of effectively describing each scene. Its remarkable performance in 4 completely different kinds of datasets proves the method's generality and its applicability in real life situations. The high detection rate in the UCSD dataset, that greatly outperforms various state-of-the-art approaches, especially on the most challenging pixel level criterion, demonstrates that the proposed algorithm can be effectively used for challenging crowd videos with many occlusions, local noise and local scale variations. This fact in combination with its low computational cost and its effectiveness in different environments, make our algorithm very appropriate for a variety of surveillance applications.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Problem formulation. (a) Overview of final motion-appearance descriptor calculation. (b) Extraction of appearance descriptor (HOG). Each block is divided into 4 cells and HOG histograms are calculated for each of them. The block is tracked over time and the final HOG descriptor results from the average of consecutive triplets after a normalization step. The HOG k j (c) symbol represents the HOG histogram, calculated from the c th cell of block k, at frame j.</figDesc><graphic coords="4,114.20,157.95,368.50,129.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Prey extraction in a m frame window occurs sequentially in a cuboid of m frames. First, m "OF values" of the 1 st pixel are taken into account, then m instances of the 2 nd pixel and so on, until m instances of the n th pixel, where n is the number of pixels in each ROI.</figDesc><graphic coords="5,337.58,56.04,199.92,140.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A swarm following prey: dashed lines show agents' trajectories while the continuous line depicts prey trajectory.</figDesc><graphic coords="5,352.91,250.71,169.18,96.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Overview of method proposed in a time window of m frames.</figDesc><graphic coords="7,68.29,56.19,212.54,235.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Walking frequency of a pedestrian in ped2. A time window of 15 frames can capture the whole period of the motion.</figDesc><graphic coords="7,352.47,615.39,170.07,102.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Effect of different µ values on agents tracking the prey for the first 100 timesteps, with the other swarm parameters kept stable. For µ = 0.8 the system is unstable and uncontrollable oscillations appear, making values of µ ≥ 0.8 unsuitable for our system.</figDesc><graphic coords="8,345.38,174.96,184.24,72.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Sensitivity analysis for β. The EER for UCSD is depicted for different values of β: the EER fluctuates very little, even for very large differentiations in β, proving that algorithm's performance is not significantly affected by β.</figDesc><graphic coords="8,345.38,292.30,184.25,88.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Variance of the mean position of agents tracking the prey for µ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Sensitivity analysis for γ. (a) Swarm position in relation to prey (red line) for different values of γ, (b) EER for UCSD "ped1" dataset for γ ∈ [0, 0.7]. Both frame level and pixel level EERs are shown.</figDesc><graphic coords="9,75.09,285.17,184.25,98.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .Fig. 12 .</head><label>1112</label><figDesc>Fig. 11. Ped1 anomalies in red blocks. Red contours show the foreground where the algorithm is applied and yellow points are interest points in them.</figDesc><graphic coords="11,53.70,231.49,70.86,52.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .Fig. 15 .</head><label>1415</label><figDesc>Fig. 14. Screenshots of our results for UMN. The first row depicts normal situations in all 3 scenes and the second row shows abnormal events (evacuation). Red areas demonstrate anomaly localization.</figDesc><graphic coords="12,53.70,259.09,70.86,54.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>ACKNOWLEDGMENT</head><label></label><figDesc>Fig. 16. ROC curve for frame level criterion in U-turn dataset</figDesc><graphic coords="12,352.47,586.47,170.08,133.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Our results for Love Parade. The truck and ambulance are successfully detected going through a very dense crowd. People from the crowd climbing over the railings are also detected as anomalous behaviour.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I "</head><label>I</label><figDesc>PED1" DATASET.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>OF VARIOUS VARIATIONS.</figDesc><table><row><cell></cell><cell cols="2">EER</cell><cell>DR</cell><cell></cell></row><row><cell></cell><cell>ped1</cell><cell>ped2</cell><cell>ped1</cell><cell>ped2</cell></row><row><cell>with swarm</cell><cell cols="2">27.02% 26.92%</cell><cell cols="2">78.87% 74.92%</cell></row><row><cell>without swarm</cell><cell>28.52%</cell><cell>27.09%</cell><cell>68.74%</cell><cell>64.94%</cell></row><row><cell>only HOS</cell><cell>29.03%</cell><cell>26.20%</cell><cell>75.76%</cell><cell>76.89%</cell></row><row><cell>only HOG</cell><cell>40.11%</cell><cell>47.32%</cell><cell>69.49%</cell><cell>48.56%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>1057-7149 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1057-7149 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2015.2409559, IEEE Transactions on Image Processing</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing interaction force for global anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Raghavendra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Del Bue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops (ICCVW), IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="136" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activity modeling using event probability sequences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yegnanarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="607" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Latent hierarchical model of temporal structure for complex activity classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="810" to="822" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active contour-based visual tracking by integrating colors, shapes, and motions</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1778" to="1792" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning object motion patterns for anomaly detection and improved object detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Circuits and Systems for Video Technology (CSVT)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1544" to="1554" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Trajectory-based anomalous event detection</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video event detection: From subvolume localization to spatiotemporal path search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="404" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Activity recognition using a mixture of vector fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Marques</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1712" to="1725" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abnormal crowd behavior detection using social force model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Abnormal detection using interaction energy potentials</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3161" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowd motion segmentation and anomaly detection via multi-label optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Conci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition Workshop (ICPRW), IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on local statistical aggregates</title>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2112" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust realtime unusual event detection using multiple fixed-location monitors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI), IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A spacetime mrf for detecting abnormal activities with incremental updates</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Textures of optical flow for real-time anomaly detection in crowds</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Video and Signal-Based Surveillance (AVSS), 8th IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="230" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1446" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV), IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Timely, robust crowd event characterization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2697" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Anomaly detection and localization in crowded scenes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="18" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting interesting events using unsupervised density ratio estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop (ECCVW), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7585</biblScope>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved anomaly detection in crowded scenes via cell-based analysis of foreground speed, size and texture</title>
		<author>
			<persName><forename type="first">V</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sanderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lovell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="55" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video parsing for abnormality detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interantional Conference on Computer Vision (ICCV), IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2415" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Online dominant and anomalous behavior detection in videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2611" to="2618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Swarm decomposition: Novel nonstationary signal analysis using swarm intelligence</title>
		<author>
			<persName><forename type="first">G</forename><surname>Apostolidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Hadjileontiadis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BGSLibrary: An opencv c++ background subtraction library</title>
		<author>
			<persName><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IX Workshop de Viso Computacional (WVC)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognition of activities of daily living for smart home environments</title>
		<author>
			<persName><forename type="first">K</forename><surname>Avgerinakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Environments (IE), International Conference on</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="173" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swarm Filtering Procedure and Application to MRI Mammography</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M H</forename><surname>Teodorescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Malan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">scielomx</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Swarmbased motion features for anomaly detection in crowds</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kaltsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Briassouli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kompatsiaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Strintzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Social force model for pedestrian dynamics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Molnár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev. E</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="4282" to="4286" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<ptr target="http://mha.cs.umn.edu/movies/crowdactivity-all.avi/" />
		<title level="m">Unusual crowd activity dataset made available by the university of minnesota at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Abnormal events detection based on spatio-temporal co-occurences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benezeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rosenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), IEEE Conference on</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2458" to="2465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automatic video analysis of a crowd disaster</title>
		<author>
			<persName><forename type="first">B</forename><surname>Krausz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2010">2010. 2012</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="307" to="319" />
		</imprint>
	</monogr>
	<note>Loveparade. special issue on Semantic Understanding of Human Behaviors in Image Sequences</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
