<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Active Feature-Value Acquisition for Classifier Induction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Prem</forename><surname>Melville</surname></persName>
							<email>melville@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">Univ. of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maytal</forename><surname>Saar-Tsechansky</surname></persName>
							<email>maytal@mail.utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Red McCombs School of Business Univ. of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Foster</forename><surname>Provost</surname></persName>
							<email>fprovost@stern.nyu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Stern School of Business</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
							<email>mooney@cs.utexas.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Dept. of Computer Sciences</orgName>
								<orgName type="institution">Univ. of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Active Feature-Value Acquisition for Classifier Induction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B3AD34960C1ABBDB97E7B08986654FF9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T02:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many induction problems include missing data that can be acquired at a cost. For building accurate predictive models, acquiring complete information for all instances is often expensive or unnecessary, while acquiring information for a random subset of instances may not be most effective. Active feature-value acquisition tries to reduce the cost of achieving a desired model accuracy by identifying instances for which obtaining complete information is most informative. We present an approach in which instances are selected for acquisition based on the current model's accuracy and its confidence in the prediction. Experimental results demonstrate that our approach can induce accurate models using substantially fewer feature-value acquisitions as compared to alternative policies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Many predictive modeling tasks include missing data that can be acquired at a cost, such as customers' buying preferences and lifestyle information that can be obtained through an intermediary. For building accurate models, ignoring instances with missing values leads to inferior model performance <ref type="bibr" target="#b6">[7]</ref>, while acquiring complete information for all instances often is prohibitively expensive or unnecessary.</p><p>To reduce the cost of information acquisition, it is desirable to identify instances for which complete information is most informative to acquire.</p><p>In this paper we address this problem of active featurevalue acquisition (AFA) for classifier induction: given a feature acquisition budget, identify the instances with missing values for which acquiring complete feature information will result in the most accurate model. Formally, assume Ñ instances, each represented by Ò features ½ Ò . For all instances, the values of a subset of the features ½ are known, along with the class labels. The values of the remaining features •½ Ò are unknown and can be acquired at a cost. The problem of feature-value acquisition is different from active learning <ref type="bibr" target="#b1">[2]</ref> and optimum experi-mental design <ref type="bibr" target="#b2">[3]</ref>, where the class labels rather than featurevalues are missing and costly to obtain.</p><p>The approach we present for active feature acquisition is based on the following three observations: (1) Most classification models provide estimates of the confidence of classification, such as estimated probabilities of class membership. Therefore principles underlying existing activelearning methods like uncertainty sampling <ref type="bibr" target="#b1">[2]</ref> can be applied. (2) For the data items subject to active featurevalue acquisition, the correct classifications are known during training. Therefore, unlike with traditional active learning, it is possible to employ direct measures of the current model's accuracy for estimating the value of potential acquisitions. (3) Class labels are available for all complete and incomplete instances. Therefore, we can exploit all instances (including incomplete instances) to induce models, and to guide feature acquisition.</p><p>The approach we propose is simple-to-implement, computationally efficient and results in significant improvements compared to random sampling and a computationally-intensive method proposed earlier for this problem <ref type="bibr" target="#b10">[11]</ref>. Ò are missing and the set can be acquired at a fixed cost. We refer to these instances as incomplete instances, and the set is denoted as Á. The class labels of all instances in Ì are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Task Definition and Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pool</head><p>Unlike prior work <ref type="bibr" target="#b10">[11]</ref>, we assume that models are induced from the entire training set (rather than just from</p><p>). This is because models induced from all available data have been shown to be superior to models induced when instances with missing values are ignored <ref type="bibr" target="#b6">[7]</ref>. <ref type="foot" target="#foot_0">1</ref> Beyond im-proved accuracy, the choice of model induction setting also bears important implications for the acquisition mechanism, because the estimation of an acquisition's marginal utility is derived with respect to the model. Note that induction algorithms either include an internal mechanism for incorporating instances with missing feature-values <ref type="bibr" target="#b6">[7]</ref> or require that missing values be imputed first. Henceforth, we assume that the induction algorithm includes some treatment for instances with missing values.</p><p>We study active feature-value acquisition policies within a generic iterative framework, shown in Algorithm 1. Each iteration estimates the utility of acquiring complete feature information for each incomplete example. The missing feature-values of a subset Ë ¾ Á of incomplete instances with the highest utility are acquired and added to Ì (these examples move from Á to ). A new model is then induced from Ì, and the process is repeated. Different AFA policies correspond to different measures of utility. Our baseline policy, random sampling, selects acquisitions at random, which tends to select a representative set of examples <ref type="bibr" target="#b7">[8]</ref>.</p><p>Error Sampling: For a model trained on incomplete instances, acquiring missing feature-values is effective if it enables a learner to capture additional discriminative patterns that improve the model's prediction. Specifically, acquired feature-values are likely to have an impact on subsequent model induction when the acquired values pertain to a misclassified example and may embed predictive patterns that can be potentially captured by the model and improve the model. In contrast, acquiring feature-values of instances for which the current model already embeds correct discriminative patterns is not likely to impact model accuracy considerably. Motivated by this reasoning, our approach Error Sampling prefers to acquire feature-values for instances that the current model misclassifies. At each iteration, it randomly selects Ñ incomplete instances that have been misclassified by the model. If there are fewer than Ñ misclassified instances, then Error Sampling selects the remaining instances based on the Uncertainty score which we describe next. The uncertainty principle originated in work on optimum experimental design <ref type="bibr" target="#b2">[3]</ref> and has been extensively applied in the active learning literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. The Uncertainty score captures the model's ability to distinguish between cases of different classes and prefers acquiring information regarding instances whose predictions are most uncertain. The acquisition of additional information for these cases is more likely to impact prediction, whereas information pertaining to strong discriminative patterns captured by the model is less likely to change the model. For a probabilistic model, the absence of discriminative patterns in the data results in the model assigning similar likelihoods for class membership of different classes. Hence, the Uncertainty score is calculated as the absolute difference between the estimated class probabilities of the two most likely classes. Formally, for an instance Ü, let È Ý ´Üµ be the estimated probability that Ü belongs to class Ý as predicted by the model. Then the Uncertainty score is given by È Ý½ ´Üµ È Ý¾ ´Üµ, where È Ý½ ´Üµ and È Ý¾ ´Üµ are the firsthighest and second-highest predicted probability estimates respectively. Formally, the Error Sampling score for a potential acquisition is set to -1 for misclassified instances; and for correctly classified instances we employ the Uncertainty score. At each iteration of the AFA algorithm, complete feature information is acquired for the Ñ incomplete instances with the lowest scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Active Feature-Value Acquisition Framework</head><p>Given:</p><p>-set of complete instances Acquire values for missing features for each instance in Ë 6.</p><p>Remove instances in Ë from Á and add to 7.</p><p>Update training set, Ì Á 8. Return Ä´Ì µ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Evaluation</head><p>Methodology: We first compared Error Sampling to random feature acquisition. The performance of each system was averaged over 5 runs of 10-fold cross-validation. In each fold, the learner initially has access to all incomplete instances, and is given complete feature-values for a randomly selected subset of size Ñ. For the active strategies, a sample of instances is then selected from the pool of incomplete instances based on the measure of utility. The missing values for these instances are acquired and the process is repeated until the pool of incomplete instances is exhausted. In the case of random sampling, the incomplete instances are selected uniformly at random. Each system is evaluated on the held-out test set after each iteration of feature acquisition. As in <ref type="bibr" target="#b10">[11]</ref>, the test data set contains only complete instances, since we want to estimate the true accuracy of the model given complete data. To maximize the gains of AFA, it is best to acquire features for a single instance in each iteration; however, to make our experiments computationally feasible, we selected instances in batches of 10 (i.e., sample size Ñ = 10).</p><p>To compare the performance of any two schemes, and we compute the percentage reduction in error of over for a given number of acquisitions and report the average over all points on the learning curve. The reduction in error is considered to be significant if the average errors across the points on the learning curve of is lower than that of according to a paired t-test (Ô ¼ ¼ ).</p><p>All the experiments were run on 5 web-usage datasets (used in <ref type="bibr" target="#b5">[6]</ref>) and 5 datasets from the UCI machine learning repository <ref type="bibr" target="#b0">[1]</ref>. 2 The web-usage data contain information from popular on-line retailers about customer behavior and purchases. This data exhibit a natural dichotomy with a subset of features owned by a particular retailer and a set of features that the retailer may acquire at a cost. The learning task is to induce models to predict whether a customer will purchase an item during a visit to the store. Hence the pool of incomplete instances was initialized with the features privately owned by each retailer. For the UCI datasets, 30% of the features were randomly selected to be used in the incomplete instances. A different set of randomly selected features was used for each train-test split of the data.</p><p>The active framework we have proposed can be implemented using an arbitrary probabilistic classifier as a learner. For the results in this paper, we used J48, which is the Weka implementation of C4.5 decision-tree induction <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>The results comparing Error Sampling to random sampling are summarized in Table <ref type="table" target="#tab_1">1</ref>. All error reductions reported are statistically significant. As mentioned above, the main impact of AFA is lower on the learning curve. To capture this, we also report the percentage error reduction averaged over only the 20% of points on the learning curve where the largest improvements are produced. We refer to this as the top-20% percentage error reduction, which is similar to a measure reported in <ref type="bibr" target="#b7">[8]</ref>.</p><p>The results show that for all data sets using Error Sampling significantly improves on the model accuracy compared to random sampling. Figures <ref type="figure" target="#fig_1">1</ref> and<ref type="figure" target="#fig_2">2</ref> present learning curves that demonstrate the advantage of using an AFA scheme over random acquisition. Apart from average reduction in error, a good indicator of the effectiveness of an AFA scheme is the number of acquisitions required to obtain a desired accuracy. For example, on the qvc dataset once Error Sampling acquires approximately 400 complete instances, it induces a model with an accuracy of 87%; however, random sampling requires approximately 1200 complete instances to achieve the same accuracy. We also evaluated a policy that uses only the Uncertainty score for estimating the utility of potential acquisitions. This Uncertainty Sampling results in significantly better performance 2 The details of the datasets used can be found in <ref type="bibr" target="#b4">[5]</ref>. compared to random sampling, but is inferior to Error Sampling. Detailed results comparing alternative AFA policies can be found in the extended version of this paper <ref type="bibr" target="#b4">[5]</ref>.  Comparison with GODA: The most closely related work to this paper is the study by Zheng and Padmanabhan <ref type="bibr" target="#b10">[11]</ref> of the active feature-value acquisition scheme GODA. GODA measures the utility of acquiring feature-values for a particular incomplete instance in the following way. It adds the instance to the training set, imputing the values that are missing and then induces a new model. The instance that leads to the model with the best performance on the complete training data is selected for acquisition. GODA has two important differences from Error Sampling: it employs a different utility measure and it induces its models from only the complete instances. To compare to our approach, we implemented GODA as described in <ref type="bibr" target="#b10">[11]</ref>, using J48 tree induction as the learner and multiple imputation for missing value imputation. Experiments comparing Error Sampling to GODA were run as before; however, due to GODA's tremendous computational requirements, we only ran one run of 10-fold cross-validation on 5 of the datasets. Some datasets were also reduced in size. A summary of the results, along with the reduced dataset sizes, is presented in Table <ref type="table" target="#tab_2">2</ref>. The results show that in spite of the high computational complexity of GODA, it results in inferior performance compared to Error Sampling for all 5 domains. All improvements obtained by Error Sampling with respect to GODA are statistically significant. These results suggest that the ability of Error Sampling to capitalize on information from incomplete instances, and to utilize this knowledge in feature acquisition, allows it to capture better predictive patterns compared to those captured by GODA. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work and Conclusions</head><p>Recent work on budgeted learning <ref type="bibr" target="#b3">[4]</ref> also addresses the issue of active feature-value acquisition. However, the policies developed in <ref type="bibr" target="#b3">[4]</ref> assume feature-values are discrete, and consider the acquisition of individual feature-values for randomly selected instances of a given class, rather than for specific incomplete instances. Some work on cost sensitive learning <ref type="bibr" target="#b8">[9]</ref> has addressed the issue of inducing economical classifiers but it assumes that the training data are complete and focuses on learning classifiers that minimize the cost of classifying incomplete test instances. Traditional active learning <ref type="bibr" target="#b1">[2]</ref> assumes access to unlabeled instances with complete feature-values and attempts to select the most use-ful examples for which to acquire class labels. Active feature acquisition is a complementary problem that assumes labeled data and attempts to acquire the most useful featurevalues.</p><p>We have presented a general framework for active feature acquisition that can be applied to different learners and can use alternate measures of utility for ranking acquisitions. Within this framework, we propose an effective and simple-to-implement policy that results in superior accuracy and is also significantly more efficient computationally compared to an existing approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>-based Active Feature Acquisition: Assume a classifier induction problem, where each instance is represented with Ò feature values and a class label. For the set of complete instances of the training set Ì, the values of all Ò features are known. For all other instances in Ì, only the values of a subset of the features ½ are known. The values of the remaining features •½</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Error Sampling vs. Random Sampling on anneal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Error Sampling vs. Random Sampling on qvc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . Error reduction of Error Sampling with respect to random sampling.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="4">%Error Reduction</cell><cell cols="4">Top-20% %Err. Red.</cell></row><row><cell></cell><cell>bmg</cell><cell></cell><cell cols="2">10.67</cell><cell></cell><cell></cell><cell>17.77</cell><cell></cell></row><row><cell></cell><cell>etoys</cell><cell></cell><cell cols="2">10.34</cell><cell></cell><cell></cell><cell>23.88</cell><cell></cell></row><row><cell></cell><cell>expedia</cell><cell></cell><cell cols="2">19.83</cell><cell></cell><cell></cell><cell>29.12</cell><cell></cell></row><row><cell></cell><cell>priceline</cell><cell></cell><cell cols="2">24.45</cell><cell></cell><cell></cell><cell>34.49</cell><cell></cell></row><row><cell></cell><cell>qvc</cell><cell></cell><cell cols="2">15.44</cell><cell></cell><cell></cell><cell>24.75</cell><cell></cell></row><row><cell></cell><cell>anneal</cell><cell></cell><cell cols="2">22.65</cell><cell></cell><cell></cell><cell>49.27</cell><cell></cell></row><row><cell></cell><cell>soybean</cell><cell></cell><cell cols="2">8.03</cell><cell></cell><cell></cell><cell>14.79</cell><cell></cell></row><row><cell></cell><cell>autos</cell><cell></cell><cell cols="2">4.24</cell><cell></cell><cell></cell><cell>10.50</cell><cell></cell></row><row><cell></cell><cell>kr-vs-kr</cell><cell></cell><cell cols="2">36.82</cell><cell></cell><cell></cell><cell>53.23</cell><cell></cell></row><row><cell></cell><cell>hypo</cell><cell></cell><cell cols="2">16.79</cell><cell></cell><cell></cell><cell>40.48</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell></cell><cell cols="2">16.93</cell><cell></cell><cell></cell><cell>29.83</cell><cell></cell></row><row><cell></cell><cell>99</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>98.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>98</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>97.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>96.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Error Sampling</cell><cell></cell></row><row><cell></cell><cell>96</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Random Sampling</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>100</cell><cell>200</cell><cell>300</cell><cell>400</cell><cell>500</cell><cell>600</cell><cell>700</cell><cell>800</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Number of complete instances</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 . Error reduction of Error Sampling with respect to GODA.</head><label>2</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell>Size</cell><cell>% Error Reduction</cell></row><row><cell>etoys</cell><cell>270</cell><cell>37.58</cell></row><row><cell>priceline</cell><cell>447</cell><cell>14.19</cell></row><row><cell>bmg</cell><cell>200</cell><cell>19.48</cell></row><row><cell>expedia</cell><cell>200</cell><cell>22.96</cell></row><row><cell>qvc</cell><cell>100</cell><cell>20.03</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It was also noted in<ref type="bibr" target="#b10">[11]</ref> that such a setting may result in better models.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Balaji Padmanabhan and Zhiqiang Zheng for providing us with the web usage datasets. Prem Melville and Raymond Mooney were supported by DARPA grant HR0011-04-1-007.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/˜mlearn/MLRepository.html" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving generalization with active learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="221" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Theory of optimal experiments</title>
		<author>
			<persName><forename type="first">V</forename><surname>Federov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Budgeted learning of naive-bayes classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lizotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 19th Conf. on Uncertainty in Artificial Intelligence (UAI-03)</title>
		<meeting>of 19th Conf. on Uncertainty in Artificial Intelligence (UAI-03)<address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Active feature acquisition for classifier induction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saar-Tsechansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<idno>UT-AI-TR-04-311</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
		<respStmt>
			<orgName>University of Texas at Austin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Personalization from incomplete data: what you don&apos;t know can hurt</title>
		<author>
			<persName><forename type="first">B</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Kimbrough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 7th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD-2001)</title>
		<meeting>of 7th ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining (KDD-2001)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="154" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unknown attribute values in induction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of 6th Intl. Workshop on Machine Learning</title>
		<meeting>of 6th Intl. Workshop on Machine Learning<address><addrLine>Ithaca, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-06">June 1989</date>
			<biblScope unit="page" from="164" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active sampling for class probability estimation and ranking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saar-Tsechansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="153" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Types of cost in inductive concept learning</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Workshop on Cost-Sensitive Learning at the 17th Intl. Conf. on Machine Learning</title>
		<meeting>of the Workshop on Cost-Sensitive Learning at the 17th Intl. Conf. on Machine Learning<address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On active learning for data acquisition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Padmanabhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE Intl. Conf. on Data Mining</title>
		<meeting>of IEEE Intl. Conf. on Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
