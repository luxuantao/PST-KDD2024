<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data Stream Clustering with Affinity Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
							<email>xiangliang.zhang@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cyril</forename><surname>Furtlehner</surname></persName>
							<email>cyril.furtlehner@lri.fr</email>
							<affiliation key="aff0">
								<orgName type="department">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cécile</forename><surname>Germain-Renaud</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
							<email>michele.sebag@lri.fr</email>
							<affiliation key="aff0">
								<orgName type="department">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">•</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">University of Paris</orgName>
								<address>
									<addrLine>Sud 11</addrLine>
									<postCode>91405</postCode>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data Stream Clustering with Affinity Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">06D1C4DE305BB3C6CC94142B4E934405</idno>
					<idno type="DOI">10.1109/TKDE.2013.146</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Streaming data clustering</term>
					<term>affinity propagation</term>
					<term>grid monitoring</term>
					<term>autonomic computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data stream clustering provides insights into the underlying patterns of data flows. This paper focuses on selecting the best representatives from clusters of streaming data. There are two main challenges: how to cluster with the best representatives and how to handle the evolving patterns that are important characteristics of streaming data with dynamic distributions. We employ the Affinity Propagation (AP) algorithm presented in 2007 by Frey and Dueck for the first challenge, as it offers good guarantees of clustering optimality for selecting exemplars. The second challenging problem is solved by change detection. The presented STRAP algorithm combines AP with a statistical change point detection test; the clustering model is rebuilt whenever the test detects a change in the underlying data distribution. Besides the validation on two benchmark data sets, the presented algorithm is validated on a real-world application, monitoring the data flow of jobs submitted to the EGEE grid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D EALING with non-stationary distributions is a key issue for many application domains, e.g., sensor network or traffic data monitoring <ref type="bibr" target="#b0">[1]</ref>. At the crossroad of databases, data mining and machine learning, Data Streaming is the discipline specifically concerned with handling large-scale datasets in an online fashion <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>; many data streaming algorithms have been adapted from clustering algorithms, e.g., the partitioning method k-means <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b7">[8]</ref> or the density-based method DbScan <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>.</p><p>This paper focuses on learning a generative model of a data stream, with some specific features:</p><p>• the generative model is expressed through a set of exemplars, i.e., actual data items as opposed to centroids in order to accommodate complex (e.g., relational) domains;</p><p>• the generative model is available at any time, for the sake of monitoring applications;</p><p>• the changes in the underlying distribution are detected through statistical hypothesis testing. These specificities are motivated by the targeted domain of application, Autonomic Computing <ref type="bibr" target="#b10">[11]</ref>, rooted in the fact that the ever increasing complexity of computational systems calls for new approaches to system management <ref type="bibr" target="#b11">[12]</ref>, enforcing self-modeling, self-configuring, self-healing and self-optimizing facilities <ref type="bibr" target="#b12">[13]</ref>. The vision behind Autonomic Computing is to provide a computational system with an analog of the biological immune system, seamlessly enforcing quite a few vital activities (e.g., breathing and heart-beating) according to the demands of the external environment and the internal state of the organism.</p><p>The presented work specifically concerns a large-scale grid system, the EGEE/EGI Grid, which is the largest escience grid infrastructure worldwide. Created through a series of EU projects 1 , it involves over 400,000 cores and 200 Petabytes storage. It supports more than 1 million jobs per day on a 24/24, 7/7 basis. The applicative goal of the presented approach is to process the log files that describe the flow of jobs submitted to and processed by gLite, the major EGEE/EGI middleware, and to provide the system administrator with a dashboard of the job stream.</p><p>The proposed approaches proceed by extending the Affinity Propagation (AP) algorithm, a message passingbased clustering method proposed by Frey and Dueck <ref type="bibr" target="#b13">[14]</ref>. Formulating the clustering problem in terms of energy minimization, AP outputs a set of clusters, each of which is characterized by an actual data item, referred to as an exemplar; the penalty value parameter controls the cost of adding another exemplar. AP provides some asymptotic guarantees of optimality of the solution. The price to pay for these properties is AP's quadratic computational complexity, forbidding its usage on large scale datasets.</p><p>The online version of AP, called STRAP 2 , was firstly proposed and studied in our previous work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The proposed algorithm proceeds by incrementally updating the current model if the current data item fits the model, and putting it in a reservoir otherwise. A Change Point Detection (CPD) test, the Page-Hinkley test <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, detects the changes of distribution by monitoring the proportion of data items sent to the reservoir (the proportion of outliers). Upon triggering the CPD test, a new model is rebuilt from the current model and the data items in the reservoir.</p><p>Extending our previous work <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, this paper presents a complete STRAP algorithm for data stream clustering with comprehensive analysis in theoretical and empirical manners. The improvements over our previous work are summarized as follows. First, this paper investigates a real-time adapted CPD test, rather than using empirically setting or model-based optimization in CPD test <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. The CPD test plays an important role in STRAP for catching the evolving distribution. The adaptive threshold can achieve clusters in better quality, cause less outliers and require less computing time.</p><p>Second, the performance of STRAP algorithm for extracting representative exemplars is theoretically analyzed. STRAP extracts exemplars from streaming data for building a summary model. CPD test enables STRAP to catch drifting exemplars that significantly deviate away. A minor drifting exemplar modifies the synopsis of its associated cluster but cannot replace its associated exemplar. For the first time, we theoretically analyze the upper bound of the distortion loss caused by minor drifting exemplars.</p><p>Third, we study the complexity and memory usage of STRAP, which are important efficiency factors for streaming algorithms. Our analysis shows that the memory usage of STRAP mainly depends on the number of exemplars and outliers, which is small and varies a little in the streaming process. The time complexity of STRAP is quadratic w.r.t. the number of exemplars and outliers. Experimental evaluation confirms our analysis regarding the efficiency of STRAP.</p><p>Last but not least, extensive sensitivity analysis and experimental evaluation are conducted. We employed the KDD'99 intrusion detection benchmark data, comparatively to the DenStream algorithm <ref type="bibr" target="#b8">[9]</ref>, a recent URL stream data set <ref type="bibr" target="#b18">[19]</ref> and the EGEE job stream. The new experimental results validate the effectiveness of the proposed method.</p><p>This paper is organized as follows. Section 2 briefly reviews related work. Section 3 presents Affinity Propagation for the sake of completeness and describes the proposed Weighted AP. Section 4 describes STRAP, extending AP to data streaming. Section 5 reports the experimental validation on KDD'99 data, a URLs stream and a real-world application, monitoring the EGEE job data flows. The paper concludes with perspectives for further research in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">STATE OF THE ART</head><p>Data streaming, one of the major Data Mining tasks <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, faces two additional difficulties compared to traditional data clustering problem. Both are related to the non-stationarity of the data distribution. On one hand, functionally, a streaming algorithm must maintain an efficient trade-off between noise filtering (outliers must be discarded) and model updating (most generally, upon detecting a change in the data distribution, the model must be appropriately updated). On the other hand at a more general level, a streaming algorithm must adjust its own parameters, e.g., the number k of clusters can hardly be determined beforehand.</p><p>One-scan Divide-and-Conquer approaches have been widely used to cluster data streams, e.g., extending kmeans <ref type="bibr" target="#b21">[22]</ref> or k-median <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> approaches. The basic idea is to segment the data stream and process each subset in turn, which might prevent the algorithm from catching the distribution changes in a timely manner; likewise, it adversely affects the adjustment of the number k of clusters. In <ref type="bibr" target="#b22">[23]</ref>, data samples flowing in are categorized as discardable (outliers), or compressible (accounted for by the current model), or to be retained in the RAM buffer. Clustering, e.g., k-means, is iteratively applied, considering the sufficient statistics of compressed and discarded points, and the retained points in RAM.</p><p>Recently, Ackermann et al. proposed an online k-means by maintaining a small sketch of the input using the mergeand-reduce technique <ref type="bibr" target="#b6">[7]</ref>. The proposed StreamKM++ algorithm can obtain better clustering results (minimizing the sum of squared errors) than BIRCH <ref type="bibr" target="#b23">[24]</ref> but takes more computing time than BIRCH. Shindler et al. improved the streaming approximation for Euclidean k-means where k is not known as input and data points are sequentially read to form clusters <ref type="bibr" target="#b7">[8]</ref>. Their approach can provide better approximation guarantee and is efficient with complexity o(nk). Both of these two recent algorithms aim at efficiently grouping data into clusters with high quality. Efficiency in computing time and memory usage is achieved by sequentially loading large-scale data as streams. When the algorithms terminate, they produce clusters of the large data set, instead of forming clusters at any time step when data flow in.</p><p>As emphasized by Dong et al. <ref type="bibr" target="#b24">[25]</ref>, a core issue in data streaming is to detect the changes in the data flow distribution. A two-level scheme first proposed by Aggarwal et al. <ref type="bibr" target="#b5">[6]</ref> proceeds as follows: the evolving data stream is processed and summarized online; these summaries are grouped into clusters offline, using k-means <ref type="bibr" target="#b5">[6]</ref> or densitybased clustering methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The latter approaches can reveal clusters of arbitrary shapes while k-means only reveal spherical clusters. These two approaches are respectively parameterized from the number k of clusters, and the density threshold; whereas one might argue that the number of clusters is more brittle than the density threshold in the non-stationary case, it is most generally desirable to adjust the parameters online.</p><p>Let us detail the DenStream algorithm, which upgrades the density-based clustering algorithm DbScan to data streaming along the lines of the two-level scheme <ref type="bibr" target="#b8">[9]</ref>. Within the online level, micro-clusters are built online to reflect the stream, creating a new one iff new data items arrive and fall outside of the scope of the nearest micro-cluster; otherwise, the new data item is merged to the nearest micro-cluster. In the former case, an outlier micro-cluster is created; for the sake of the memory management, an (old) micro-cluster is deleted or two microclusters are merged. DenStream maintains a weight attached to each micro-cluster, which is used to decide whether it should be deleted upon building an outlier micro-cluster, or upgrading an outlier micro-cluster into a potential one. While the online micro-clusters reflect the underlying probability density function, the actual clusters are only built upon the user's query, using a variant of DbScan on the micro-clusters.</p><p>Based on the online maintenance and offline clustering strategy, Dai et al. also proposed a Clustering on Demand framework <ref type="bibr" target="#b25">[26]</ref>. In the online phase, wavelet and regression analyses were used to construct summary hierarchies. In the offline phase, approximations of desired substreams are retrieved from summary hierarchies according to clustering queries. Like DenStream, the clustering model is available upon request.</p><p>It must be emphasized that both Divide-and-Conquer and two-level schemes keep their computational load within reasonable limits as they only build the data model upon the user's explicit request. In the rest of time, they only maintain a summary of the data, which makes them ill-suited to applications such as system monitoring where the data model has to be continuously available at all times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">(WEIGHTED) AFFINITY PROPAGATION</head><p>For the sake of self-containedness, this section first describes the Affinity Propagation (AP) algorithm. An extension, Weighted AP, is then presented to deal with duplicated and generally weighted data items. The notations used in this paper are presented in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Affinity Propagation</head><p>Let X = {x 1 , . . . , x N } be a set of items, and K denote a positive integer. The k-medoids problem aims at finding K items in X , referred to as exemplars and denoted as x i 1 , . . . , x i K , such that they minimize the sum, over all items x j , of the minimal squared distance between x j and x i k , k = 1 . . . K. The Affinity Propagation approach proposes an equivalent formalization of the k-medoids problem, defined in terms of energy minimization. It solves the optimization problem</p><formula xml:id="formula_0">c * = argmin E[c] ,<label>(1)</label></formula><p>with</p><formula xml:id="formula_1">E[c] = - N i=1 S(x i , x c i ) - N i=1 ln χ (p) i [c],<label>(2)</label></formula><p>where c = (c 1 , . . . , c N ) is the mapping between data and exemplars, S(x i , x c i ) is the similarity between x i and its exemplar x c i ∈ X , set to negative squared distance -d(x i , x c i ) 2 if i = c i . A free parameter called the preference penalty is the cost incurred for being oneself exemplar:</p><formula xml:id="formula_2">σ def = S(x i , x i ), ∀i,<label>(3)</label></formula><formula xml:id="formula_3">χ (p) i [c] is a set of constraints controlling the clustering struc- ture. ln χ (p) i [c] → -∞ if x c i = x i</formula><p>and ∃j x c j = x i , which implies that if x i is selected as an exemplar by some items, it has to be its own exemplar. Otherwise, ln</p><formula xml:id="formula_4">χ (p) i [c] = 0.</formula><p>The energy function thus enforces a tradeoff between the distortion, i.e., the sum over all items of the squared error d(x i , x c i ) 2 , and the cost of the model, that is σ × |c| if |c| denotes the number of exemplars retained. Equation (2) thus does not directly specify the number of exemplars to be found, as opposed to k-medoids. Instead, the number of exemplars in the solution depends on penalty σ ; note that σ = 0 yields a trivial solution, selecting every item as an exemplar.</p><p>A message passing algorithm is employed to solve the optimization problem defined by Equation (2), considering two types of messages: availability messages a(i, k) express the accumulated evidence for x k to be selected as the best exemplar for x i ; responsibility messages r(i, k) express the fact that x k is suitable to be the exemplar of x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weighted AP</head><p>The first extension, called Weighted AP (WAP), is proposed to deal with multiply-defined items and dense aggregations of items. Let dataset E = {(x i , n i )} involve n i copies of item x i or n i items aggregated into a single item x i with small average mutual distance i , for i = 1 . . . L. Let us consider the similarity matrix S defined as: </p><formula xml:id="formula_5">S (x i , x j ) = n i S(x i , x j ) if i = j σ + (n i -1) × i otherwise, with i ≥ 0.</formula><formula xml:id="formula_6">E [c] = - L i=1 S (x i , x c i ) - L μ=1 ln χ (p) μ [c] (<label>4</label></formula><formula xml:id="formula_7">)</formula><p>is equivalent, for i = 0, to the optimization problem defined by Equation ( <ref type="formula" target="#formula_1">2</ref>) for E made of the union of n i copies of x i , for i = 1 . . . L.</p><p>Proof. In the optimization problem defined by Equation (2), assume that x i actually represents a set of n i identical copies; the penalty S(x i , x j ) of selecting x j as exemplar of x i is the cost of selecting x j as exemplar for each one of these copies. Therefore S (x i , x j ) = n i × S(x i , x j ). Likewise, let x i be unfolded as a set of n i (almost) identical copies {x i 1 , . . . , x i n i }, and let us assume that one of them, say x i 1 is selected as exemplar. One thus pays the penalty σ , plus the sum of the dissimilarities between x i 1 and the other copies in x i , modeled as (n i -1) i . Constant i thus models the average dissimilarity among the n i copies of x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STRAP: EXTENDING AP TO DATA STREAMING</head><p>This section aims at extending AP to data streaming, specifically achieving online clustering in the case of non-stationary data distributions. The resulting algorithm, called STRAP, involves four main steps (Algorithm 1 with a diagram in Fig. <ref type="figure" target="#fig_1">1</ref>):</p><p>1) The first bunch of data is used by AP to identify the first exemplars and initialize the stream model. 2) As the stream flows in, each data item x t is compared to the exemplars; if too far from the nearest exemplar, x t is put in the reservoir, otherwise the stream model is updated accordingly (Section 4.1).</p><p>3) The data distribution is checked for change point detection, using a statistical test, the Page-Hinkley (Section 4.2). 4) Upon triggering the change detection test, or if the number of outliers exceeds the reservoir size, the stream model is rebuilt based on the current model and reservoir, using WAP (Section 4.3). Contrasting with the state of the art (Section 2), STRAP builds a model of the data flow which is available at any time step. Its performance will be empirically assessed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">AP-based Model and Update</head><p>The model of the data stream used in STRAP is inspired from DbScan <ref type="bibr" target="#b26">[27]</ref> and DenStream <ref type="bibr" target="#b8">[9]</ref>. It consists of a set of 4-tuple (e i , n i , i , t i ), where e i ranges over the exemplars, n i is the number of items associated to exemplar e i , i is the distortion of e i (sum of d(x, e i ) 2 , where x ranges over all items associated to e i ), and t i is the last time stamp when an item was associated to e i .</p><p>At time t, item x t is considered and its nearest (w.r.t. distance d) exemplar e i in the current model is selected; if d(x t , e i ) is less than some threshold ε, heuristically set to the average distance between points and exemplars in the initial model, x t is affected to the i-th cluster and the model is updated accordingly; otherwise, x t is considered to be an outlier, and put into the reservoir.</p><p>In order to prevent the number of exemplars from growing beyond control, one must be able to forget the exemplars that have not been visited for a long time, as they may describe an obsolete model. Accordingly, a userspecified window length is introduced; when item x t is associated to exemplar x i , the model update is thus defined as:</p><formula xml:id="formula_8">n i : = n i × +(t-t i ) + 1 n i +1 i : = i × +(t-t i ) + n i n i +1 d(x t , x i ) 2 t i : = t (5)</formula><p>Calculations show that the above update rules enforce the model stability if exemplar x i is selected on average by n i examples during the last time steps. The sensitivity analysis w.r.t. is discussed in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Restart Criterion</head><p>A key difficulty in Data Streaming is to detect a change in the generative process underlying the data stream, referred to as drift, which requires the stream model to be updated as soon as possible. The main difficulty lies in the fact that the first data samples reflecting the new components can hardly be distinguished from outliers. Masud et al. proposed to detect the concept drift as appearance of novel classes <ref type="bibr" target="#b27">[28]</ref>.</p><p>In their work, supervised methods were used to determine the decision boundary of training data. Instances lying outside the decision boundary and with strong cohesion among each other were reported as a novel class drifting from the current model.</p><p>In continuous settings, the model update is most usually dealt with by gradually moving the cluster centers <ref type="bibr" target="#b26">[27]</ref>. In discrete settings, (part of) the cluster centers must be recomputed and a new optimization phase must be launched. A restart criterion is thus needed to trigger the stream model update.</p><p>The simplest option is based on the number of data items in the reservoir; when it exceeds the reservoir size, the restart criterion is automatically triggered.</p><p>A more sophisticated restart criterion 3 is based on statistical change point detection (CPD) tests. While quite a few multi-variate CPD tests have been proposed, in particular to deal with structured data (see <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> and references therein), a computationally frugal test is needed in the data streaming context since it is run at every time step. For this reason, a scalar CPD test was chosen, specifically the Page-Hinkley (PH) change point detection test <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Let us briefly remind the basics of PH test, before describing how it is integrated in STRAP framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">The Page-Hinkley Test</head><p>Let p u denote the realization of the observed random variable p at time u. Denote pt (respectively m t ) the empirical average of the p u (respectively the sum of the differences between p u and pu ) on the time interval <ref type="bibr">[1, t]</ref>:</p><formula xml:id="formula_9">pt = 1 t t u=1 p u m t = t u=1 (p u -pu + δ). (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>The PH test proceeds by maintaining the maximum value M t of the m u (M t = max{m 1 , . . . m t }), and triggering the test if the gap between M t and m t is above a threshold λ, parameterizing the PH test,</p><formula xml:id="formula_11">PH triggered iff PH t = M t -m t &gt; λ.</formula><p>The PH test is theoretically grounded for testing a negative jump in the mean of a Gaussian distribution (a symmetric form exists for a positive jump). More precisely, p u is assumed to be a Gaussian variable with mean μ 0 before the change point, and μ 1 after, with μ 1 &lt; μ 0 . PH t is essentially a running (up to time t) estimator of the loglikelihood of distributions without and with jump. δ is a positive real-value that controls the test model: it should be half the size of the jump when μ 0 and μ 1 are known; if unknown, it is usually set to a small value, e.g., half the minimum jump considered significant model wise.</p><p>To get an intuition of the PH test, let us consider a simplistic example, where the value of the random variable is always equal to its mean: up to time t 0 , p u = μ 0 , and after t 0 , p u = μ 1 . For t ≤ t 0 , m t = tδ, thus M t = m t and PH t = 0. For t &gt; t 0 , m t = t 0 δ + t u=t 0 +1 (μ 1 -pu + δ). Shortly after the change, the influence of new values on the empirical average is limited, thus pu ≈ μ 0 , m t &lt; M t 0 ,</p><formula xml:id="formula_12">PH t ≈ (t -t 0 )(μ 0 -μ 1 -δ) (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>PH t is positive and quasi linearly increases w.r.t. the number of time steps. The parameter λ is usually set after observing p u for some time and depends on the desired alarm rate. Its adaptive adjustment is described in next section.</p><p>3. In the following, the most new outlier replaces the oldest one in case the number of outliers exceeds the reservoir size. The total number of outliers since the current model was built is maintained and will serve to build the next model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">The Test Variables</head><p>As aforementioned, scalar CPD tests are preferred for the sake of computational efficiency, which requires scalar variables to be defined and to be able to reflect the state of the streaming process. In our earlier work, the scalar variable was set to the fraction of outliers observed in a sliding time window <ref type="bibr" target="#b15">[16]</ref>. A new approach is presented in this paper.</p><p>Each data item x t is associated to its nearest exemplar e. In the case where x t is an outlier (d(x t , e) ≥ ε), x t is marked to visit e at time step t. We note t 1 &lt; t 2 • • • &lt; t j the time steps of the consecutive outliers, and u t = d(x t , e).</p><p>The general goal is to detect two distinct modalities of the apparition of new clusters. One is spatial: outliers begin coalescing in a region of space with small volume compared to the closest cluster; in this case, the variance of u t j will decrease. The second is temporal: the intensity of the stochastic process counting the outliers increases; this may for instance account for the case where widely separated clusters of equal spatial volume appear.</p><p>It then comes naturally to associate to each outlier the random variable defined as an inverse intensity-weighted empirical variance of the distance. For sake of memory, and also in order to forget stale information, this variable is computed on a sliding window of size l.</p><formula xml:id="formula_14">p t l = 1 l l i=1 (1 + log(t i -t i-1 ))(u t i - 1 l l k=1 u t k ) 2 .</formula><p>In the following, we note p t instead of p t l for simplicity. A decreasing of p t suggests outliers are both frequent and dense in a given region of the item space.</p><p>The drift/change in p t is monitored throughout the Page-Hinkley test. A heuristic justification for this is possible when the intensity is constant and the u t is normally distributed. Then, the empirical variance follows a chi-squared distribution with l degrees of liberty before the change point, and a translated χ 2 (l) after. As the square root of a chi-squared distribution converges fast to a normal distribution, testing for a negative jump (decrease) in mean is reasonable.</p><p>The next question is how to calibrate the threshold parameter λ, which controls the flexibility vs brittleness of the stream model. At one extreme, small λ values result in frequently rebuilding the model and including noise patterns in it; at the other extreme, large λ values lead to discard the first representative of new clusters and cause the stream model to lag behind the data distribution. Therefore, an appropriate setting of λ is desired to enable the stream model to keep pace with changes in data distribution. A problem is that λ should depend on the scale of the observed phenomenon, but the triggering of the rebuilding should not. In this paper, we propose a self-calibrating definition of the threshold, instead of a fixed pre-defined value as in <ref type="bibr" target="#b14">[15]</ref> or the optimized value under the Bayesian Information Criterion related to the stream model <ref type="bibr" target="#b15">[16]</ref>. From Equation <ref type="formula" target="#formula_12">7</ref>, we see that PH t ≈ (tt 0 )(p t 0 -μ 1 -δ). As we are tracking situations where μ 1 becomes small, and δ is always negligible, the threshold can be set as:</p><formula xml:id="formula_15">λ t 0 = 0 ifPH t = 0 f * pt 0 otherwise, (<label>8</label></formula><formula xml:id="formula_16">)</formula><p>where f is a constant called the λ-factor. f is the "critical mass" of a new cluster, and can be interpreted as the number of required witnesses seeing the changes. This way, the change point detection is scale-invariant. In order to cope with the variability of the p t , the threshold can be evaluated at each time step, as f * pt . The sensitivity analysis w.r.t. the λ-factor, f , is presented in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Rebuilding</head><p>Upon triggering the change point detection test (or when the reservoir is full), a new model is rebuilt by launching Weighted AP on the dataset E made of the model exemplars together with their number of represented items (e i , n i ) and the outliers in the reservoir (x j , 1). Along the same lines as discussed in Section 3.2, the energy terms are defined as:</p><p>S(e i , e i ) = σ + i S(x j , x j ) = σ S(e i , e j ) = -n i d(e i , e j ) 2 S(e i , x j ) = -n i d(e i , x j ) 2 S(x j , e i ) = -d(x j , e i ) 2 <ref type="bibr" target="#b8">(9)</ref> Note that S is asymmetric. The cost of selecting current exemplar e i to be the exemplar of x j is ordinary similarity -d(x j , e i ) 2 , while the cost of selecting x j to be the exemplar of e i is increased by a factor of n i . Therefore, the current exemplar e i will have more chance to be an exemplar again. As discussed in Section 3.2, the optimization problem defined by this asymmetric matrix is equivalent to the optimization problem defined on the whole data by unfolding each exemplar e i to all of its associated items.</p><p>WAP accordingly selects the optimal exemplars in E. The new stream model is finally defined as follows. Let e denote a new exemplar representing previous exemplars e 1 , . . . , e m and outliers x 1 , . . . , x m . With no difficulty, the number n of items associated to e is set to n 1 + • • • + n m + m . The associated distortion is estimated as follows. Let x be an item associated to e 1 . Indeed x is no longer available after visiting the streaming model; but assuming an Euclidean space, x can be modeled as a random item e 1 + Xv, where v is a random vector in the unit sphere, and X is a scalar random variable. It comes:</p><formula xml:id="formula_17">d(e -x) 2 = d(e -e 1 ) 2 + d(e 1 -x) 2 -2X e -e 1 , v .</formula><p>Assuming that v is uniformly distributed in the unit sphere, and that the distribution of the norm and the angle are independent, summing over x in the cluster represented by exemplar e 1 and taking the expectation give:</p><formula xml:id="formula_18">IE[ x∈C e 1</formula><p>d(e, x) 2 ] = n 1 d(e, e 1 ) 2 + 1 , as the expectation of the scalar product w.r.t the uniform distribution is 0. Accordingly,</p><formula xml:id="formula_19">= m i=1 n i d(e, e i ) 2 + i + m i=1 d(e, x i ) 2 .</formula><p>Finally, t is set to the maximal time stamp associated to e i and x j , for e i and x j ranging among the exemplars and outliers associated to e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Theoretical Analysis of Distortion Loss in Minor Exemplar Drift</head><p>Exemplars in STRAP are used to summarize streaming items and reconstruct models, as introduced in Section 4.1 For example in Fig. <ref type="figure" target="#fig_2">2</ref>, an exemplar e drifts to ě, which has a distance less than ε to e. Items of new distribution with ě are then partially absorbed by e (including ě itself), and partially reported as outliers in reservoir. After model rebuilding, the new exemplar will be either the previous e or a new one e from reservoir since ě is absent. The optimal exemplar ě achieves the minimum distortion, while e and e lead to an increase of distortion. In this section, we theoretically analyze the distortion loss caused by discarding items after updating the model when minor exemplar drift happens.</p><p>Assume that all items from the distribution with exemplar ě are composed of m outside (E m , in reservoir) and m inside ε-neighborhood of e (E m ). If keeping all of them available, ě is extracted from a combination of e and m + m items with the optimal distortion:</p><formula xml:id="formula_20">D ě = x i ∈E m x i -ě 2 + x i ∈E m x i -ě 2 + n e -ě 2 + e ,</formula><p>where n and e are model parameters w.r.t. e without absorbing m items.</p><p>According to the update equation of distortion in Section 4.3, if m items were absorbed by e, a new e+E m is obtained:</p><formula xml:id="formula_21">( +1 ) m e + m i=1,x i ∈E m ( +1 ) m -i n i n i +1 x i -e 2 .</formula><p>In the case that e is selected again as the new exemplar from a combination of e and E m , the distortion is:</p><formula xml:id="formula_22">D e = x i ∈E m x i -e 2 + e+E m .</formula><p>In the case that e is selected as the new exemplar from a combination of e and E m , the distortion is:</p><formula xml:id="formula_23">D e = x i ∈E m x i -e 2 + e+E m + (n + m ) e -e 2 .</formula><p>We first verify that the distortion loss in the first case selecting e as the new exemplar is negligible.</p><formula xml:id="formula_24">D e -D ě = x i ∈E m ( x i -e 2 -x i -ě 2 ) -n e -ě 2 +[ e+E m -e - x i ∈E m x i -ě 2 ]. (<label>10</label></formula><formula xml:id="formula_25">) Let μ = 1 m x i ∈E m</formula><p>x i be the center-mass of E m , and μ =</p><formula xml:id="formula_26">1 m x i ∈E m</formula><p>x i be the center-mass of E m . We define a = e-μ , b = ě -μ , a = e -μ , a = e -μ and b = ě -μ . We have</p><formula xml:id="formula_27">x i ∈E m ( x i -e 2 -x i -ě 2 ) = m (e -ě) • (e + ě -2μ ) = m ( a 2 -b 2 ) (<label>11</label></formula><formula xml:id="formula_28">)</formula><p>and</p><formula xml:id="formula_29">e+E m -e - x i ∈E m x i -ě 2 = + 1 m -1 e + m i=1, x i ∈E m + 1 m -i n i n i + 1 x i -e 2 -x i -ě 2 &lt; x i ∈E m x i -e 2 -x i -ě 2 = m ( a 2 -b 2 ). (<label>12</label></formula><formula xml:id="formula_30">)</formula><p>Taking Eq. ( <ref type="formula" target="#formula_27">11</ref>), and (12) into Eq. ( <ref type="formula" target="#formula_24">10</ref>), we have</p><formula xml:id="formula_31">D e -D ě &lt; m ( a 2 -b 2 ) +m ( a 2 -b 2 ) -n( a 2 + b 2 ) + 2n a b S.</formula><p>Considering that a &lt; a (e is closer to μ than to μ ), b &lt; b (ě is closer to μ than to μ ), and b &lt; a (μ is closer to ě than to e), we can have</p><formula xml:id="formula_32">D e -D ě &lt; (m + m + n)( a 2 -b 2 ).</formula><p>Since ě is the optimal exemplar, center-mass μ and μ are near to ě. Hence, b is close to 0, and a is close to ε. Then, we can have the distortion loss of D e -D ě bounded by (m + m + n)ε 2 . The distortion loss per item is ε 2 , which is usually small and is acceptable. We now verify the distortion loss in the second case selecting e as the new exemplar.</p><formula xml:id="formula_33">D e -D ě = x i ∈E m ( x i -e 2 -x i -ě 2 ) -n e -ě 2 +[ e+E m -e - x i ∈E m x i -ě 2 ] + (n + m ) e -e 2 . (<label>13</label></formula><formula xml:id="formula_34">)</formula><p>Similar to the first case, we can have</p><formula xml:id="formula_35">D e -D ě &lt; (m + n)( a 2 -b 2 ) + 2na • (b -a ) +m ( a 2 -b 2 ) + m (a -a ) 2 .</formula><p>Since a &lt; b (μ is closer to e than to ě), aa = ee, a 2 &lt; ε 2 and ba = ěe , it comes</p><formula xml:id="formula_36">D e -D ě &lt; m ( e -e 2 + ε 2 ) + 2n a ě -e .</formula><p>Since e is near to the optimal exemplar ě ( ěe is close to 0), ee and a are close to ε. Then, the upper bound of D e -D ě is around 2m ε 2 , which is an acceptable distortion loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Memory Usage and Complexity Analysis</head><p>Streaming algorithms are required to have a fixed and small memory usage and a short computing time in the whole process. The memory usage of STRAP algorithm mainly consists of all exemplars in model (e i , n i , i , t i ) and the outliers in the reservoir. The statistical variables in change detection consumes ignorable memory. Since the number of exemplars is usually small and the reservoir has a limited size, the overall memory usage is much smaller than the load of intuitively keeping in memory all items in a sliding window. The memory usage varies a little in the streaming process. More exemplars are included in the model when streaming data are from a more complex underlying distribution. However, memory is released by eliminating out-of-date exemplars so that the model would not get ponderous. Keeping only recent and active exemplars is also beneficial to the computational complexity, which is analyzed in the next paragraph. Suppose there are m exemplars in the current model and m outliers in the reservoir. At each time t when a new item arrives, the updating operation in Section 4.1 has linear complexity w.r.t. m. The change detection method in Section 4.2 calculates p t , pt , m t , M t and λ t by simple operations like summation and multiplication on results from last time step, and thus takes a constant computing time. The model rebuilding in Section 4.3 applies WAP and updates model (e i , n i , i , t i ) on a combination of m exemplars and m reservoir items. The complexity of this step is O((m + m ) 2 ). The overall time complexity of STRAP algorithm is quadratic w.r.t. the number of exemplars and reservoir items. Eliminating outdated elements from the model and reservoir as aforementioned improves the time complexity of STRAP algorithm.</p><p>The experimental results of memory usage and computing time are reported in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL VALIDATION</head><p>This section reports the empirical validation of STRAP algorithm on two benchmark data sets and the application to a real grid monitoring system. Let us first present the assessment criteria before detailing the experimental settings and discussing the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Assessment Criteria</head><p>The streaming model is evaluated on two aspects, quality and time-effectiveness. With the help of external class labels, the quality of online clustering is measured by clustering accuracy and purity. When assessing clustering accuracy, the labels of items grouped in one cluster are determined by the class label of their exemplar. An item is misclassified iff it is associated to an exemplar belonging to a different class. In the following, the clustering accuracy stands for the percentage of data items that are correctly classified.</p><p>The clustering accuracy criterion however poorly accounts for the case of imbalanced datasets and the correct classification of examples in the minority class(es). A second criterion, referred to as clustering purity, is thus used. It averages the fraction of items belonging to the majority class of in each cluster. The third criterion, the percentage of outliers, assesses the time-effectiveness of the model. By construction, outliers are far from the current model and their information is thus poorly accounted for; this makes it desirable to keep the number of outliers as low as possible. On the other hand, a healthy fraction of outliers contributes to keeping the computational load and the number of model rebuilt within reasonable limits; furthermore, the outliers in the reservoir are used to seed the new model with relevant information about the new clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setting</head><p>The goal of the experiments is to comparatively assess STRAP w.r.t. all above three criteria. Two data sets, the thoroughly studied Intrusion Detection benchmark (KDD'99) <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> and a recent streaming data of benign and malicious URLs <ref type="bibr" target="#b18">[19]</ref>, are employed for the evaluation.</p><p>The applicative goal of KDD'99 is to tell attacks from normal connections. A connection is a sequence of TCP packets starting and ending at some well specified times, flowing from a source IP address to a target IP address under well defined protocols. It is described by 41 attributes; following <ref type="bibr" target="#b8">[9]</ref> we only used the 34 numeric ones and normalized them 4 .</p><p>Each connection is labeled among 23 classes, the normal class and the specific kinds of attack, such as buffer_overflow, ftp_write, guess_passwd, and neptune. The relevance of this data from a stream perspective comes from the fact that it involves a non-stationary data distribution: attacks evolve along a typical arm race between the system administrators and the intruders; and the normal usages also evolve. STRAP is expected to build a dynamic model of the different types of connections, as they appear in the flow. Each cluster would include connections of a same type.</p><p>The streaming URLs are studied to predict malicious URLs from benign URLs <ref type="bibr" target="#b18">[19]</ref>, and thus protect users from  We applied our algorithm on clustering the URLs stream to produce pure clusters of benign or malicious URLs sharing similar feature values. Newly coming URLs would thus be classified after the type of the exemplar they are associated to, or identified as outliers (and thus be considered as malicious ones). To make an efficient clustering, we used the 64 numeric attributes among millions of attributes.</p><p>The initialization of the stream model (Algorithm 1) considers the first 1000 connections of the KDD'99 data set which totally includes 494,021 network connection records (71MB), and the first 1000 URLs of the 2,396,130 URLs data set (500MB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sensitivity Analysis</head><p>Beyond measuring the three performance criteria, we also intend to measure the sensitivity of the results w.r.t. the STRAP parameters: the window length (Section 4.1) and the restart triggering parameters (Figs. <ref type="figure" target="#fig_3">3,</ref><ref type="figure" target="#fig_4">4</ref>, 5 and Table <ref type="table" target="#tab_1">2</ref>). Fig. <ref type="figure" target="#fig_3">3</ref> compares the performance of STRAP on KDD'99 data depending on the window length . Fig. <ref type="figure" target="#fig_3">3(a)</ref> shows that STRAP achieves high accuracy (more than 97% for &gt; 15000). The percentage of outliers of PH is 0.3% (around 1500 network connections) lower than that of MaxR, as shown in Fig. <ref type="figure" target="#fig_3">3(b)</ref>. The two ways of setting λ for PH criterion have comparable impact on clustering accuracy. PH with λ t 0 has a marginally higher percentage of outliers.</p><p>Figs. <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_5">5</ref> displays the influence of the restart parameters in KDD'99 and ULRs data. On each of different settings on |Reservoir| (bottom x axis in Figs.  PH criterion has higher accuracy when f ≥ 30, lower percentage of outliers and also higher purity.</p><p>Table <ref type="table" target="#tab_1">2</ref> gives the computational cost 5 and memory usage of STRAP on various parameter settings, focusing on the most representative results. Note that the presented results are measured on clustering the whole data stream. Under the value of time cost/memory usage, we also give the range of number of clusters during the stream clustering process. The memory usage, which is mainly proportional to the number of clusters, varies along time. We thus give its mean with std in the whole process.</p><p>The computational cost increases with ; when using MaxR as restart criterion, the computational cost also increases with the size of the reservoir. Both effects are blamed on the fact that larger reservoir size and values entail that the model is rebuilt from a larger dataset and yield more clusters (Section 4.3). In contrast, the computational cost decreases by 10% on average (more significant on URLs) when using the PH restart criterion; this reduction is explained as the PH criterion fires when new patterns arrived and enough examples of them are collected, thus avoiding to take irrelevant clusters in the model. The small amount of memory usage is almost fixed (with small variances), which verifies the analysis in Section 4.5.</p><p>Summarizing Figs. 4, 5 and Table <ref type="table" target="#tab_1">2</ref>, the PH criterion improves on the MaxR criterion, with a higher accuracy and purity (1%), a lower percentage of outliers (0.2%) and a smaller computational time (10%). It is worth noting that STRAP only needs 1% of the data (initial subset plus the outliers) in order to produce an accurate model (more than 97% accuracy) in KDD'99. Considering the KDD'99 data as 5. measured on an Intel 2.66GHz Dual-Core PC with 4 GB memory for KDD'99 (for being comparable to results in <ref type="bibr" target="#b8">[9]</ref> obtained on a 3.4 GHz PentiumIV PC with 1GB memory) and a Dell T7500 workstation which has 6 Intel Xeon processor X5650 2.67GHz with 12 cores and 24GB of RAM for URLs data. a binary classification problem, this problem includes about 20% normal connections and 80% attacks. The online clustering results of STRAP yield 99.18% True Detection rate and 1.39% False Alarm rate, to be compared with <ref type="bibr" target="#b32">[33]</ref> using a supervised method and yielding 98.8% True Detection rate and 0.4% False Alarm rate. STRAP achieves an accuracy around 91-92% for URLs data, which is not comparable to that obtained by supervised algorithms in <ref type="bibr" target="#b18">[19]</ref> considering that STRAP is unsupervised. Please also note we only used 64 numeric attributes rather than all the millions of attributes as used in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Online Performance Comparison</head><p>The online performance of STRAP on KDD'99 is compared to DenStream in Fig. <ref type="figure" target="#fig_6">6</ref>. The clustering purity of DenStream on the KDD'99 dataset was evaluated during four time windows of length 1000 when some attacks happened. For a fair comparison, the clustering purity of STRAP was computed during the same time windows, considering the same 23 classes. Fig. <ref type="figure" target="#fig_6">6</ref> respectively displays the STRAP results obtained for one of the worst settings ( = 10000 and |Reservoir| = 300) and an average setting ( = 10000 and λ t 0 = 30 * pt 0 ), together with the DenStream results reported from <ref type="bibr" target="#b8">[9]</ref>. STRAP outperforms DenStream on all assessed windows.  Fig. <ref type="figure" target="#fig_7">7</ref> displays the accuracy along time of STRAP on URLs data for = 10000 and λ t = 300 * pt in PH criterion. Each accuracy rate measured in window of 10000 evaluates the clustering quality for URLs in around half a day. It can be observed that the online clustering accuracy is above 91% in most of the time. Fig. <ref type="figure" target="#fig_8">8</ref> shows the online clustering purity of STRAP. After each model rebuilt, the purity is computed by averaging the purity in each cluster. The number of clusters K is also reported (as increasing K would mechanically improve the purity), showing that K remains circa 300 and yields a purity above 90% in most of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Application to Grid Monitoring</head><p>As presented in the introduction, the motivating application of STRAP is to provide the Grid system administrator with a dashboard monitoring the current status of the system, for the early detection of undesired events.</p><p>In the EGEE/EGI grid, Resource Brokers (RB) are the software components that dispatch the incoming jobs to the queues of the local batch systems. 5,268,564 EGEE jobs are extracted from the logs of 39 Research Brokers of all gLiteoperated jobs in the whole EGEE grid during 5 months, from 2006-01-01 to 2006-05-31.</p><p>In the following, each job is described by 6 attributes measuring the time duration for different services. During the job lifecycle, a job is first submitted, then waiting for the Workload Management System (WMS) to match a resource for it. Once a resource is found, the job becomes ready for transfer; it is thereafter transferred to the resource, where its state is scheduled, meaning that it is enqueued in the local batch system. When selected, the job is running, until successfully finished (done OK),  or failed (done failed). Accordingly, the six time duration attributes are: 1) T submission : time between job registration and transfer to WMS 2) T waiting : time to find a matching resource 3) T transfer : time acceptation and transfer (waiting + ready time), reported by the JobController (JC) 4) T CE_accept : the same as T transfer , but reported by the LogMonitor (LM) 5) T scheduled : queuing delay 6) T running : execution time. Note that attributes 3 and 4 are functionally redundant: JC is a standalone logging service, while the LM integrates various logs, and returns them in the logging and bookkeeping (L&amp;B) database; we shall nevertheless see that their discrepancies offer valuable insights into the state of the system.</p><p>All attributes differ by orders of magnitude; they are thus normalized x → x-μ s where μ (respectively s) denotes the mean (respectively std) of attribute x measured on the initial bunch of data (set to the first 1000 jobs;). Furthermore, in case the job fails at some point along its lifecycle, the durations associated to the subsequent services are not available and set to 0 by default. Six additional boolean attributes are thus also considered, indicating whether the job reaches any of the six states. Finally, the similarity between two jobs is set to the Euclidean distance on IR 12 .</p><p>Besides the descriptive attributes, each job is labeled after its final state, successfully finished (good job) or failed (bad job). The failure cases are further categorized into 45 error types, e.g., Cancel requested by WorkloadManager, RB Cannot plan. The 5 million samples considered in the experiments involve 20 main error types, including more than 1,500 occurrences each.</p><p>STRAP is applied to the job data flow, after their description has been normalized. Its control parameters are set as follows. STRAP model is initialized by launching AP on the first 1,000 jobs. The preference penalty σ in AP is set to the median value of similarity matrix. The window length used in STRAP is set to 10, 000. The parameters of the PH restart criterion are δ = 0.01 and λ t = 30 * pt .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1">Online Monitoring Snapshots</head><p>The output of STRAP is displayed on Fig. <ref type="figure" target="#fig_9">9</ref>, showing two snapshots at different time. Each snapshot illustrates the data distribution as a histogram; each bar stands for a cluster, topped with the exemplar description and whose height is the fraction of jobs attached to this exemplar since the model rebuilt. For the sake of readability, only clusters with sufficient representativity (more than 1%) are represented. The left snapshot of Fig. <ref type="figure" target="#fig_9">9</ref> illustrates a standard regime; 60% of jobs are successfully finished, with typical time profile "[8 18 <ref type="bibr">24 30</ref> 595 139]". A small fraction of successful jobs are computationally heavy jobs (execution time circa 19190s) and they spent a long time waiting in the queue (circa 9728s), 10% of the jobs (exemplar [7 0 0 0 0 0]) fail almost immediately (they stop after their registration); 20% of the jobs (exemplar [10 47 54 129 0 0]) fail at a later stage, before arriving at the local computing site.</p><p>The right snapshot of Fig. <ref type="figure" target="#fig_9">9</ref>, recorded 3 days later, illustrates an emergency situation. It includes clusters similar to those in the left snapshot, plus two untypical clusters. Cluster 8, which includes about 40% of the jobs (exemplar [14 <ref type="bibr">23 33 45792</ref> 233 87]), is considered to be anomalous as its T CE_accept is very long. This situation is interpreted as the LogMonitor (LM) becoming clogged, with a typical LMtime value of 45792s, compared to tens of seconds in the former snapshots. This conjecture is supported by the LMtime in Cluster 5 (exemplar "[25 26 43 45809 0 0]"), where jobs fail before arriving at the local site (likewise the cluster with exemplar [10 47 54 129 0 0]), but has much larger LM-time. Interestingly, this problem was discovered without using the job labels; and no failure case corresponds to the clogging of the LM.</p><p>The online monitoring thus yields a compact and intuitive summary of the job distribution, which can be interpreted on the spot by the system administrator. As will be seen (Fig. <ref type="figure" target="#fig_9">9</ref>), the system also provides a day-, week-and month-view of the job data flow 6 . The processing meets the real-time requirements (40,000 jobs are processed in 1 min).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2">Online Clustering Accuracy, Purity and Efficiency</head><p>In order for the stream model to be usable, it is desirable that the clusters are pure w.r.t. the job labels to the best possible extent. Fig. <ref type="figure" target="#fig_10">10</ref> reports the online accuracy of the clusters w.r.t. the job labels, together with the percentage of outliers, depending on the restart parameters.</p><p>The baseline results are provided by a Streaming kmedoids algorithm, replacing the AP component in STRAP with a k-medoids algorithm, and retaining the best results out of 20 independent runs; in this way, AP and k-medoids require the same computational time. When rebuilding the model, k-medoids is launched on a dataset gathering copies of the model and items of the reservoir. To imitate WAP 6. Other monitoring results in avi format are provided at http://www.lri.fr/~xlzhang/GridMonitor/. in STRAP, k-medoids is used with memory in which 80% of the initially selected of items are from the model and others are from the reservoir. The number k of clusters in k-medoids is set to 133, which is the average number of exemplars in STRAP.</p><p>Note that the use of DenStream was excluded as the DenStream model is only made explicit upon request. Requiring it to classify every job and demonstrating each cluster by a representative exemplar were beyond the available computational resources. Fig. <ref type="figure" target="#fig_10">10</ref> shows that STRAP improves on Streaming kmedoids by circa 5%, while involving circa 4% less outliers. The restart parameters with real-time adapted λ t were found to provide more stable and satisfactory results than the fixed λ used in our previous work <ref type="bibr" target="#b15">[16]</ref>.</p><p>Likewise, Fig. <ref type="figure" target="#fig_11">11</ref> shows the average clustering purity over all clusters after each model rebuilt; the number k of clusters is also depicted, to confirm that the high purity cannot be attributed by an extra-large number of clusters. Interestingly, the purity is circa 90% for k = 200 clusters on average, and is thus higher than the clustering accuracy; this higher purity is attributed to the fact that some large clusters are mixed, while many small clusters are pure.</p><p>The running time and memory usage on EGEE job stream are shown in Table <ref type="table" target="#tab_3">3</ref> (run on the workstation). The high computing cost (more than an hour for clustering more than 5 millions jobs) is mainly due to the long string labels in the data set. It took more time to update clusters with long strings than numeric/binary labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Discussion</head><p>While many data streaming algorithms actually focus on the extraction of statistical information from data streams <ref type="bibr" target="#b33">[34]</ref>- <ref type="bibr" target="#b35">[36]</ref>, ranging from the approximation of frequent patterns <ref type="bibr" target="#b36">[37]</ref> to the construction of decision trees <ref type="bibr" target="#b37">[38]</ref>, the most related work is <ref type="bibr" target="#b8">[9]</ref>, similarly addressing unsupervised learning and clustering from data streams. The DenStream algorithm upgrades the DbScan clustering algorithm <ref type="bibr" target="#b26">[27]</ref> to dynamic environments; it mainly differs from STRAP regarding the creation and the updating of clusters. Actually, DenStream does not construct the final clusters unless requested to do so by the user; upon such a request, the (most recent) items will be labeled after the clusters. While this "lazy" clustering and labeling behavior is more computationally efficient, it is suggested that it is not well-suited to e.g., monitoring applications, when the goal is to identify behavioral drifts as soon as they appear (Section 5.5.1).</p><p>Comparatively to DenStream, STRAP provides a model at any time step; further, this model was shown to be more accurate than DenStream on KDD'99 dataset. In counterpart, STRAP computational time is higher than DenStream (4 minutes against 7 seconds). A question opened for further investigation is whether this performance is compatible with real-time applications; an alternative is to design a distributed version of STRAP, e.g., sharing the reservoir.</p><p>Another relevant work, presented by Cormode et al. <ref type="bibr" target="#b38">[39]</ref>, aims at the structure of clusters in the stream. Interestingly, an extension of AP referred to as Soft-Constraint AP (SCAP) has been proposed by Leone et al. <ref type="bibr" target="#b39">[40]</ref>. Further research will investigate the extension of SCAP to data streaming, to address the structured cluster extraction from data streams.</p><p>The exemplars built on the fly by STRAP can be used to provide a retrospective and macro-scale view of the streaming data. "Super-exemplars" can be extracted by applying AP on all exemplars selected during the target period (e.g., 5 months in the past). These super-exemplars provide a common reference, enabling to describe the overall distribution observed in that period, and thus to detect the long-run trends.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Our proposed algorithm STRAP aims at clustering data streams with evolving data distributions. STRAP confronts the arriving items to the current AP model, storing the outliers in a reservoir and monitoring the ratio of outliers using the PH change point detection test. Upon triggering the PH test, the clustering model is rebuilt from the current one and the reservoir using WAP (Section 4.3). The key issue here was to build the change indicator, monitored by the PH test, in order to preserve the computational cost vs accuracy tradeoff. In this paper, we monitored the ratio of outliers over a sliding window and adapted the change detection threshold in real-time (Section 4.2).</p><p>The proposed approach STRAP was theoretically analyzed on guaranteeing acceptable distortion loss when exemplars slightly drift from the already selected ones, on consuming small amount of memory with little variation, and on requiring acceptable computing time that depends on the complexity of underlying distribution. The performance of STRAP in clustering quality and efficiency is empirically validated on KDD'99 benchmark problem and the URLs stream. While STRAP improves on DenStream w.r.t. clustering purity (shown in Fig. <ref type="figure" target="#fig_6">6</ref>), it is slower; the interpretation offered for this fact is that STRAP provides a model of the stream at any time step, whereas DenStream only builds the model upon request. The validation on URLs stream also demonstrates that STRAP achieves above 90% accuracy and purity for grouping benign and malicious URLs, given that STRAP is unsupervised and worked on only 64 out of millions of attributes.</p><p>The last contribution of this paper is the application of STRAP on monitoring of the grid job streams motivated by the field of Autonomic Grid. STRAP is used on a 5-million job traces, and it showed the feasibility of providing the grid administrators with a real-time dashboard of the job data flow. This online report enables the instant detection of regime drifts (e.g., clogging of LogMonitor as shown in Fig. <ref type="figure" target="#fig_9">9</ref>). This work opens several perspectives for further research. A recurrent question for clustering problems is to define the "natural" number of clusters, or for AP the best value of the penalty parameter σ . Our recent work has shown how to directly generate a given number of clusters by modifying the AP formulation <ref type="bibr" target="#b40">[41]</ref>. Another perspective is to organize the streaming model in a hierarchical manner, to speed up the retrieval phase in STRAP. Presently, each new item is confronted to all exemplars; it would be thus desirable to organize these exemplars, and investigate how e.g., Locality Sensitive Hashing <ref type="bibr" target="#b41">[42]</ref> can support the pre-selection of the nearest exemplars. Last, in the Autonomic Grid application, clustering can be used to group users, who are represented by the exemplars of their submitted jobs. These clusters will be instrumental in defining user-friendly grid interfaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 3 . 1 .</head><label>31</label><figDesc>The combinatorial optimization problem of finding c:{1 . . . L} minimizing</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Diagram of STRAP algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of exemplars built with and without discarded items inside the ε-neighborhood of an exemplar e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performance of STRAP on KDD'99 dataset: Comparing restart criterion PH (f = 30) and MaxR (|Reservoir | = 300), depending on window length . (a) Clustering accuracy. (b) Percentage of outliers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Performance of STRAP on KDD'99 dataset: Comparing restart criterion PH and Reservoir size depending on parameter setting. (a) Clustering accuracy. (b) Percentage of outlier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance of STRAP on URLs streams: Comparing restart criterion PH and Reservoir size depending on parameter setting. (a) Clustering accuracy. (b) Percentage of outlier. (c) Averaged clustering purity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparative performances of STRAP and DenStream on intrusion detection dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Online clustering accuracy of STRAP on URLs data set when = 10000 and λ t factor f = 300.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Online clustering purity of STRAP on URLs data set when = 10000 and λ t factor f = 300.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Snapshots from the monitoring output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Online clustering accuracy when summarizing the job flow of EGEE, in comparison among STRAP with real-time adapted λ t = 30 * pt , STRAP with λ fixed by a given value 40, and Streaming k -medoids.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Clustering purity after each restart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc></figDesc><table><row><cell>Notations in Algorithms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Time</head><label>2</label><figDesc>Cost/Memory Usage (N. of Evolving Clusters) of STRAP on Different Parameter Settings when Applying to the Whole KDD'99 and URLs Stream (in mins/MB)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 Time</head><label>3</label><figDesc>Cost/Memory Usage (N. of Evolving Clusters) of STRAP on Different Parameter Settings When Applying to the Whole EGEE Job Stream (in mins/MB) accessing on rogue Web sites. The 120-day streaming data consists of more than 2 million URLs, each of which is described by lexical features, e.g., the unusual appearance of '.com' in the middle of a URL, and host-based features, e.g., the ownership and IP prefix of a web site host. A URL is labeled as benign or malicious. Ma et al. in [19] designed an online supervised learning algorithm, which has a high accuracy of 98-99%.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Dr. J. Perez for valuable discussions about the parametrization of the change point detection test. This work has been partially supported in part by the EGEE-III project funded by the European Union INFSO-RI-222667, and in part by the Network of Excellence PASCAL, IST-2002-506778, and in part by the King Abdullah University of Science and Technology (KAUST).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning from Data Streams: Processing Techniques in Sensor Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Gaber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-12">Dec. 2007</date>
			<publisher>Springer</publisher>
			<pubPlace>Guildford, U.K.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">C</forename></persName>
		</author>
		<title level="m">Data Streams: Models and Algorithms</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conquering the divide: Continuous clustering of distributed data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 23rd ICDE</title>
		<meeting>IEEE 23rd ICDE<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1036" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Clustering data streams: Theory and practice</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="515" to="528" />
			<date type="published" when="2003-06">May/Jun. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Clustering data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>O'callaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Found. Comput. Sci</title>
		<meeting>IEEE Symp. Found. Comput. Sci</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A framework for clustering evolving data streams</title>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th Int. Conf. VLDB</title>
		<meeting>29th Int. Conf. VLDB<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="81" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">StreamKM++: A clustering algorithm for data streams</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ackermann</surname></persName>
		</author>
		<idno>pp. 2.4:2.1- 2.4:2.30</idno>
	</analytic>
	<monogr>
		<title level="j">CM J. Exp. Algorithmics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012-05">May 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate Kmeans for large datasets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2375" to="2383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Density-based clustering over an evolving data stream with noise</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SDM</title>
		<meeting>SDM</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="326" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Density-based clustering for real-time stream data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th ACM SIGKDD</title>
		<meeting>13th ACM SIGKDD<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><surname>Ibm</surname></persName>
		</author>
		<ptr target="http://www.research.ibm.com/autonomic/" />
		<title level="m">IBM manifesto for Autonomic Computing</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive diagnosis in distributed systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. (special issue on Adaptive Learning Systems in Communication Networks)</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1088" to="1109" />
			<date type="published" when="2005-09">Sept. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The vision of autonomic computing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Kephart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="50" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">5814</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data streaming with affinity propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Furtlehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECML/PKDD</title>
		<meeting>ECML/PKDD</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="628" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward autonomic grids: Analyzing the job flow with affinity streaming</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Furtlehner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Germain-Renaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th ACM SIGKDD</title>
		<meeting>15th ACM SIGKDD<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Continuous inspection schemes</title>
		<author>
			<persName><forename type="first">E</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="100" to="115" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inference about the change-point from cumulative sum tests</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hinkley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="509" to="523" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identifying suspicious URLs: An application of large-scale online learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Voelker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26th ICML</title>
		<meeting>26th ICML<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active mining of data streams</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SDM</title>
		<meeting>SDM</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<title level="m">Data Streams: Algorithms and Applications (Foundations and trends in theoretical computer science)</title>
		<meeting><address><addrLine>Hanover, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Now</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="117" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scaling clustering algorithms for massive data sets using data streams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nittel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Braverman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th ICDE</title>
		<meeting>20th ICDE</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">830</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling clustering algorithms to large databases</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. KDD</title>
		<meeting>4th Int. Conf. KDD</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BIRCH: An efficient data clustering method for very large databases</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Livny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD<address><addrLine>Montreal, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="103" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Online mining of changes from data streams: Research problems and preliminary results</title>
		<author>
			<persName><forename type="first">G</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGMOD</title>
		<meeting>ACM SIGMOD<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive clustering for multiple evolving streams</title>
		<author>
			<persName><forename type="first">B.-R</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1166" to="1180" />
			<date type="published" when="2006-09">Sept. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD</title>
		<meeting>SIGKDD</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification and novel class detection in concept-drifting data streams under time constraints</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Masud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thuraisingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="859" to="874" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Kernel methods for detection (méthodes à noyaux pour la détection)</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>TELECOM ParisTech</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kernel change-point analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">KDD Cup 1999 Data (Computer Network Intrusion Detection</title>
		<ptr target="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" />
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A data mining framework for building intrusion detection models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Stolfo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Mok</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Symp. Security Privacy</title>
		<meeting>IEEE Symp. Security Privacy<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="120" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Processing of massive audit data streams for real-time anomaly intrusion detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adaptive, hands-off stream mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th VLDB</title>
		<meeting>29th VLDB<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="560" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Approximate counts and quantiles over sliding windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arasu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Manku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd PODS</title>
		<meeting>23rd PODS<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="286" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed top-k monitoring</title>
		<author>
			<persName><forename type="first">B</forename><surname>Babcock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="28" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Online mining of frequent sets in data streams with error guarantee</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">H</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-L</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="258" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accurate decision trees for mining high speed data streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Medas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD</title>
		<meeting>SIGMOD<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="523" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Finding hierarchical heavy hitters in streaming data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Korn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Clustering by soft-constraint affinity propagation: Applications to gene-expression data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Leone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sumedha</surname></persName>
		</author>
		<author>
			<persName><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="2708" to="2715" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">K-AP: Generating specified K clusters by efficient affinity propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Norvag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 10th ICDM</title>
		<meeting>IEEE 10th ICDM<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1187" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Localitysensitive hashing scheme based on p-stable distributions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Datar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Mirrokni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th Annu. SCG</title>
		<meeting>20th Annu. SCG<address><addrLine>Brooklyn, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="253" to="262" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
