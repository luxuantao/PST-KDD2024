<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks</title>
				<funder ref="#_vbXD2Hn">
					<orgName type="full">Interdisciplinary and integrated innovation of JLU</orgName>
				</funder>
				<funder ref="#_3JyAvPb #_8ZEeupF">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_kZRWpdZ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingchen</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
							<email>kaixiong.zhou@rice.edu</email>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Wang</surname></persName>
							<email>wangying2010@jlu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>xinwang@jlu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Rice University Houston</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Xin He Jilin University Changchun</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Jilin University</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">KDD &apos;22</orgName>
								<address>
									<addrLine>August 14-18</addrLine>
									<postCode>2022</postCode>
									<settlement>Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Mingchen Sun</orgName>
								<address>
									<addrLine>Kaixiong Zhou, Xin He, Ying Wang, and Xin Wang</addrLine>
									<postCode>2022</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GPPT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539249</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>? Information systems ? Data mining</term>
					<term>? Computing methodologies ? Learning latent representations</term>
					<term>? Theory of computation ? Semi-supervised learning</term>
					<term>? Networks ? Network algorithms</term>
					<term>Graph neural networks, pre-training, prompt tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the promising representation learning of graph neural networks (GNNs), the supervised training of GNNs notoriously requires large amounts of labeled data from each application. An effective solution is to apply the transfer learning in graph: using easily accessible information to pre-train GNNs, and fine-tuning them to optimize the downstream task with only a few labels. Recently, many efforts have been paid to design the self-supervised pretext tasks, and encode the universal graph knowledge among the various applications. However, they rarely notice the inherent training objective gap between the pretext and downstream tasks. This significant gap often requires costly fine-tuning for adapting the pre-trained model to downstream problem, which prevents the efficient elicitation of pre-trained knowledge and then results in poor results. Even worse, the naive pre-training strategy usually deteriorates the downstream task, and damages the reliability of transfer learning in graph data. To bridge the task gap, we propose a novel transfer learning paradigm to generalize GNNs, namely graph pre-training and prompt tuning (GPPT). Specifically, we first adopt the masked edge prediction, the most simplest and popular pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. The token pair is consisted of candidate label class and node entity. Therefore, the pre-trained GNNs could be applied without tedious fine-tuning to evaluate the linking probability of token pair, and produce the node classification decision. The extensive experiments on eight benchmark datasets demonstrate the superiority of GPPT, delivering an average improvement of 4.29% in few-shot graph analysis</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref> have become a prominent technique to analyze graph structured data in many real-world systems, including social networks <ref type="bibr" target="#b35">[36]</ref>, recommender systems <ref type="bibr" target="#b44">[45]</ref>, and knowledge graph <ref type="bibr" target="#b38">[39]</ref>. The general approach of GNNs treats the input graph as an underlying computation graph, and learns the node representations by passing messages across the edges. The generated node representations could be used for different downstream tasks, such as link prediction <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, node classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b47">48]</ref>, and biochemical module classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Recent efforts in transfer learning have advanced GNNs to capture the transferable graph patterns and generalize to different downstream tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Specifically, most of them follow the "pre-train, fine-tune" learning strategy: using the easily accessible information as pretext task (e.g., edge prediction) to pre-train GNNs, and fine-tuning over downstream task with the pre-trained model as initialization. By leveraging the substantial corpus of unlabeled structure, the pre-training has become an attractive solution for fewshot graph analysis <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref>, where the labeled data is scarce. Under the "pre-train, fine-tune" framework, the existing works mainly focus on objective engineering: tailoring the pre-training objectives to better inform the downstream applications. The prevalent pretraining approaches include edge prediction <ref type="bibr" target="#b12">[13]</ref> and contrastive learning <ref type="bibr" target="#b24">[25]</ref>.</p><p>One of key limitations in "pre-train, fine-tune" is the training objective gap between the constructed pretext and dedicated downstream tasks. This gap would hinder us from directly and efficiently eliciting the pre-trained graph knowledge. We take the pretext edge prediction and downstream node classification as an example. Traditionally, the pre-trained GNNs were transferred by simply replacing the final classification layer, and fine-tuned together with the new parameters. While the pre-trained parameters optimize the binary edge predictions of node pairs, they need to be tuned time consumingly towards the parameter space conducting multi-class node predictions of standalone nodes, especially when the connected nodes have distinct classes. It is observed that such fine-tuning even results in worse downstream performance than training from scratch <ref type="bibr" target="#b12">[13]</ref>, meaning the negative and failing knowledge elicitation. The previous objective engineering often carefully designs the pretext task close to every new downstream application, which requires both expert knowledge and tedious manual trials.</p><p>Instead of being limited in the objective engineering, we propose to explore the novel strategy of "pre-train, prompt, fine-tune" to bridge the task gap in GNNs. The prompt technique is motivated by natural language processing (NLP) <ref type="bibr" target="#b18">[19]</ref>, where it is applied to reformulate the downstream task looking similar to the pretext one. We use the example of language model pre-trained with masked word prediction, and the downstream classification task with raw input text (e.g., emotion classification of "I missed the bus today"). The prompting function aims at modifying the input text to a prompt by appending the semantic textual template (e.g., "I missed the bus today. I felt so [MASK]"). In this way, the classification task is reformulated to fill the masked emotion word (e.g., "wonderful"), which is in line with the pre-training objective. The pre-trained model could be applied with or without slight fine-tuning to quickly elicit the pre-trained knowledge. This unique advantage of efficient fine-tuning has improved the applicability of generally pre-trained model to a wide series of language tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>However, it is non-trivial to design the prompting function to generalize GNNs due to following two challenges. First, given the symbolic graph data, it is infeasible to apply the semantic prompting function to bridge the various graph machine learning tasks. One cannot use the textual template (e.g., "I felt so [MASK]" as adopted in NLP) to reformulate the node classification to edge prediction, which are defined to learn the abstract nodes instead of semantic texts. Second, even with the feasible prompting function, it is unaware how to design the informative prompt to better reformulate the downstream application. Using the example of node classification, the prompt should take into account the relative neighborhoods of target node to boost the classification accuracy. In view of these two challenges, we propose the research question: how to design the graph-aware prompting function to bridge the pretext and downstream tasks in fine-tuning the pre-trained GNNs? Presented work. To mitigate the training objective gap, we present graph pre-training and prompt tuning (GPPT) framework. Notably, most of the graph pre-training learn the structure knowledge via edge prediction or contrastive learning to evaluate the similarity score of any instance pair, while the downstream application is node classification since the local node labels are scarce. Without loss of generality and following the previous efforts, we adopt the masked edge prediction, the most simplest and universal pretext task, to pre-train GNNs. Based on the pre-trained model, we propose the graph prompting function to reformulate the downstream node classification problem looking the same as edge prediction. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the graph prompting function applies the pairwise token template to modify the standalone node into token pairs: the task token representing the downstream problem and the structure token containing the node information. Specifically, they respectively tackle the above two challenges as below:</p><p>First, to unify as edge prediction (challenge #1), the task token represents a node label waiting to be classified with trainable continuous vector, and is appended to the target sample. We measure the linking probability between task token and target sample directly through the pre-trained model. Given the multiple possible labels, the node classification is realized by choosing the task token/node label with the highest linking probability. Meanwhile, the orthogonal prompt initialization and regularization are proposed to separate the trainable vectors of different label tokens.</p><p>Second, to inform the node classification (challenge #2), the structure token represents the target sample with its multi-hop neighborhoods. The intuition is that nodes are prone to share the similar patterns with their proximal neighbors. The structure token would make it much easier to accurately classify a node by automatically selecting the relative neighbors.</p><p>Finally, we fine-tune the graph prompting function together with the pre-trained GNNs. We design extensive experiments on eight benchmark datasets and two downstream applications. The empirical results show that GPPT consistently outperforms all the other training strategies, including supervised learning, joint training and traditional transfer learning. In the few-shot graph analysis problem, GPPT delivers the average improvement of 4.29%, and saves the fine-tuning time cost up to 4.32X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Graph Neural Networks. GNNs apply the recursive message passing across vertices to learn the complex dependencies in graph data <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b20">21]</ref>. Recently, there has been a lot of researches that seek to improve learning the structural information from graph <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, which is known as graph representation learning. The main idea is to optimize the low-dimensional vector mapping so that the low-dimensional embeddings can effectively reflect the structure of the original graph. The learned embeddings can be used as input features for downstream tasks. There are various types of GNNs such as graph recurrent neural networks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38]</ref>, graph convolutional networks <ref type="bibr" target="#b3">[4]</ref>, graph autoencoders <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b36">37]</ref>, etc. As an effective technology of graph structured data mining, GNNs are an important foundation of our framework.</p><p>Graph Pre-training. Graph pre-training aims to capture several structural patterns across the input graph in a self-supervised manner. The goal is to pre-train a generic GNN that can deal with different tasks <ref type="bibr" target="#b23">[24]</ref>. Recently, some representative pre-training models have made great progress in different aspects. Wu et al. <ref type="bibr" target="#b8">[9]</ref> develop an effective strategy for pre-training GNNs, and demonstrate its effectiveness for out-of-distribution generalization. GCC <ref type="bibr" target="#b24">[25]</ref> leverages contrastive learning to capture the universal network topological properties across multiple networks and transfers the learned prior knowledge to downstream tasks. GPT-GNN <ref type="bibr" target="#b9">[10]</ref> introduces a self-supervised attributed graph generation task to pre-train GNN models that can capture the structural and semantic properties of the graph. L2P-GNN <ref type="bibr" target="#b21">[22]</ref> utilizes meta-learning to learn the fine-tune strategy during the pre-training process.</p><p>Prompt-based Learning. Instead of fine-tuning the pre-trained language models (LMs), researchers gradually focus on a new NLP paradigm in recent years <ref type="bibr" target="#b19">[20]</ref>, namely "pre-train, prompt, and predict". This paradigm fine-tune LMs via task-specific objective functions on downstream tasks, which makes downstream tasks are reformulated to look more like those solved during the original LM training with the help of a textual prompt. One of the most distinctive and essential aspects of these prompt-based leaning NLP methods is prompt engineering, which means that choosing an appropriate prompt template has a great impact not only on accuracy, but also on the learning process of the model. Recent prompt engineering approaches can be summarized into two categories. Manual Template Engineering is a natural prompt engineering approach to create prompts, which is to manually create intuitive templates based on human introspection. For example, ZSC <ref type="bibr" target="#b19">[20]</ref>, PET-SGLUE <ref type="bibr" target="#b27">[28]</ref> and Petal <ref type="bibr" target="#b26">[27]</ref>, which create manually crafted prefix prompts to handle a wide variety of downstream tasks. Automated Template Learning searches or learning tokens as prompt templates, such as discrete prompts <ref type="bibr" target="#b34">[35]</ref> and continuous prompts <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In this section, we introduce the preliminary of graph neural networks and the transfer learning scheme of "pre-train, fine-tune".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Neural Networks</head><p>Let tuple G = (?, ?) denote the undirected graph, where ? ? R ? ?? is the node features and ? ? R ? ?? is the adjacency matrix. Note that ? and ? denote the number of nodes and feature dimensions, respectively. Each node ? ? is described by feature vector ? ? ? R ? , which is given by the ?-th row in matrix ? . The graph link is denoted by ? ? ? = 1 if nodes ? ? and ? ? are connected; otherwise ? ? ? = 0.</p><p>Following the spatial message passing strategy <ref type="bibr" target="#b4">[5]</ref>, GNNs learn the node representation by recursively aggregating the representations of its neighbors. Formally, at the ?-th layer, the representation learning of node ? ? is:</p><formula xml:id="formula_0">? (?) ? = AGGREGATE({? (?-1) ? , ? ? ? N (? ? ) ? ? ? }, ? (?) ). (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? (?) ?</head><p>? R ? is the embedding vector of node ? ? learned at the ?-th layer; and ? (0) ? at the initial layer is given by ? ? . N (? ? ) is the set of first-order neighbors adjacent to node ?; and ? (?) ? R ??? is the trainable weights. AGGREGATE denotes the aggregation function to pass the neighborhood embeddings to center node ? ? , and then combines them (e.g., through sum, mean or max pooling) to generate the new node representation. Suppose the number of graph convolutional layers is ?. To facilitate the following expression, we use ? ? = ? ? (G, ? ? ) to represent the final node representation learned from ?-layer GNNs, where ? = {? (1) , ? ? ? , ? (?) } denotes the concatenated trainable parameters. It has been widely demonstrated the optimized representation ? ? delivers superior performance in many applications, such as edge prediction (e.g., item exploration in recommender system <ref type="bibr" target="#b40">[41]</ref>) and node classification (e.g., topic categorization in scientific citation networks <ref type="bibr" target="#b11">[12]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-train and Fine-tune GNNs</head><p>The supervised learning of node representation usually requires plenty of annotated data from each dedicated downstream task <ref type="bibr" target="#b23">[24]</ref>. However, while it is resource expensive to manually tag the plentiful nodes and links in large-scale graphs, it is also inefficient to train GNNs from scratch for each downstream task. The transfer learning framework of "pre-train, fine-tune" has recently shown the potential to provide the attractive solution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. The core of pre-training GNNs is to use easily-accessible information to encode the intrinsic graph structure. The pre-trained GNNs could serve as initialization to generalize other downstream applications.</p><p>There are several efforts proposed to construct the self-supervised pretext tasks to pre-train GNNs. To encode the intrinsic structure knowledge, masked edge prediction <ref type="bibr" target="#b6">[7]</ref> and contrastive learning <ref type="bibr" target="#b31">[32]</ref> are the most effective and popular pretext tasks in literature. While the masked edge prediction computes the linking probabilities of node pairs to generate edges, the contrastive learning compares the similarity scores of graph instance pairs. Actually, the contrastive learning could be understood from the view of edge prediction, where the positive graph instance pair should be assigned with the highest similarity (i.e., linking probability). Therefore, without loss of generality and to ease the understanding of graph prompt, we use the masked edge prediction as pretext task to introduce the pre-training. Our work could be easily extended to any other pretext task which optimizes the pairwise score loss.</p><p>The edge prediction pretext task works as follows: we first randomly mask partial edges and then train GNNs to reconstruct them. Formally, let G pre = (?, ? pre ) denote the masked graph, where a set of edges is randomly sampled and masked to be ? ? ? = 0. Therefore, the node representation learning of node ? ? is given by ? ? = ? ? (G pre , ? ? ). The pretext task is to decide whether a node pair is connected, where GNNs is pre-trained to optimize following loss:</p><formula xml:id="formula_1">min ?,? ?? (? ? ,? ? ) L pre (? pre ? (? ? , ? ? ); ?(? ? , ? ? )).<label>(2)</label></formula><p>Node pair (? ? , ? ? ) is given by the masked edges, or sampled from the negative unconnected pairs. ? pre ? is the projection head with trainable parameters ? to evaluate similarity score of node pair (? ? , ? ? ), such as multiple layer perceptron (MLP) as shown in Figure <ref type="figure" target="#fig_0">1</ref>. L pre is the pretext loss function, such as cross entropy. ?(? ? , ? ? ) denotes the ground-truth label of node pair, which is indicator ? ? ? in our edge prediction pretext task. The optimized parameters ? pre are expected to encode the graph connectivity structure, and provide a good initialization to be fine-tuned for the downstream tasks.</p><p>The downstream application in graph data is often defined by the local-level node classification <ref type="bibr" target="#b12">[13]</ref>, e.g., classifying node label or attribute, where the ground truth is hard to be obtained. Under the traditional "pre-train, fine-tune" transfer learning framework, GNNs are fine-tuned to optimize the following loss:</p><formula xml:id="formula_2">min ?,? ? ? L down (? down ? (? ? ); ?(? ? )), s.t. ? init = ? pre .<label>(3)</label></formula><p>The constraint means GNNs are initialized by the optimized parameters from pretext task. ? down ? (? ? ) denotes the new projection head accompanied with parameters ?, while the pretext projection head is discarded. L down is the downstream loss function (e.g., cross entropy), and ?(? ? ) denotes the ground-truth label of node ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GRAPH PROMPTING FRAMEWORK</head><p>Due to the training objective gap existing between the pretext and downstream tasks, the pre-trained GNNs may fail to be efficiently leveraged by the new application, and even lead to negative transfer. First, we note that GNNs' parameters ? are optimized to generate close embeddings for connected node pairs, instead of nodes of the same class. If the disconnected pairs share the same class, the pretrained model requires to be tuned with many epochs to adapted to the new problem. This time-consuming fine-tuning prevents us from efficiently using the pre-trained model. The pre-trained knowledge will also be gradually filtered out in the long tuning process. Second, considering parameters ? of the new projection head, they are hard to be coupled with the pre-trained GNNs at the initial stage. Therefore, the downstream projection head tends to conduct wrong classifications. In the experiment part, we empirically show that the vanilla transfer learning in Eq. ( <ref type="formula" target="#formula_2">3</ref>) even results in poor results, comparing with the plain GNNs without any pre-training.</p><p>In this section, we propose to bridge the training objective gap with prompt, which reformulates the downstream task looking the same as pretext task. We are inspired by the successful applications of prompt tuning in NLP, where all the downstream applications are unified according to the pre-trained language model <ref type="bibr" target="#b19">[20]</ref>. The details of NLP prompt tuning are in the related work. We define the new transfer learning framework of "pre-train, prompt, fine-tune" for GNNs, and tailor the prompting function to graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pre-train, Prompt, Fine-tune</head><p>Instead of changing the projection head and classifying the standalone nodes, the graph prompt-based learning framework modifies the input node to token pair, which is applied directly to the pretrained model. We lay out the general mathematical definition of graph prompting function as below, and then give the concrete prompting function to design the token pair.  Rethinking the node classification downstream task, the task token ? task (?) could be given by a node label waiting to be classified, while the structure token ? srt (? ? ) could be represented by the subgraph surrounding target node ? ? to provide more structural information. Given the token pairs [? task (?),? srt (? ? )], by embedding them into continuous tensors, one is able to conduct the classification task by fitting the linking probability between the two tokens. Our GPPT shown in Figure <ref type="figure" target="#fig_0">1</ref>, the new learning paradigm of "pretrain, prompt, fine-tune" for GNNs, consists of three components: Prompt addition. In this step, the graph prompting function generates a series of token pairs to be classified. Assuming that there are total ? classes</p><formula xml:id="formula_3">[? 1 , ? ? ? , ? ? ], we construct their corresponding token pairs [? task (? ? ),? srt (? ? )], for ? = 1, ? ? ? , ?.</formula><p>Prompt answer. Given each token pair [? task (? ? ),? srt (? ? )], we embed them into continuous vectors. We then concatenate them as input to the pre-trained projection head, and obtain the linking probability. We answer and classify target node ? ? with label ? ? if it obtains the highest probability. Prompt tuning. Following the pretext training objective, we optimize the following loss to fine-tune GNNs: min </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Prompting Function Design</head><p>As defined in Definition 4.2, choosing an appropriate prompting function has great impacts on the learning processes of downstream task and expressive structure. We propose two simple but effective designs for the task and structure token generation. They could be easily replaced depending on the realistic applications on hand.</p><p>4.2.1 Task Token Generation. Motivated by the continuous token representation in NLP, the task token ? task (? ? ) is embedded into a trainable vector: ? ? = ? task (? ? ) ? R ? . For the total ? classes in the downstream node classification, the task token embeddings are defined by: ?</p><formula xml:id="formula_4">= [? 1 , ? ? ? , ? ? ] ? ? R ??? .</formula><p>The task token could be understood as a class-prototype node added to the original graph, where the node classification is performed by querying every classprototype node. The optimal embedding of task token ? task (? ? ) should be at the center of node embeddings of class ? ? .</p><p>Existing prompt-based learning methods often design the common tokens shared by all training samples <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>. However, it would be hard for the distinct nodes over graph to use and tune the single task token embeddings ?. In real-world networks, one of the inherent graph characteristics is cluster structure, and each node belongs to one cluster. Nodes within the same cluster have the dense connections to each other, while they are sparsely linked to the nodes from other clusters. Given the edge prediction pretext task, the pre-trained node embeddings will also be clustered in the embedding space. As explained before, the optimal embedding of task token ? task (? ? ) should thus varies with clusters. To better conduct the node classification job at each cluster, we propose the cluster-based task token generation, which consists of three steps:</p><p>? First, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, we adopt the scalable clustering module (e.g., METIS <ref type="bibr" target="#b13">[14]</ref>) to pre-process and split nodes into multiple non-overlapped clusters:</p><formula xml:id="formula_5">{G 1 , ? ? ? , G ? },</formula><p>where ? is the hyper-parameter of cluster number. ? Second, for each cluster ?, we train an independent task token embeddings:</p><formula xml:id="formula_6">? ? = [? ? 1 , ? ? ? , ? ? ? ] ? ? R ??? .</formula><p>? Third, given task token ? task (? ? ) of node ? ? at cluster ?, it is embeded by vector ? ? ? .</p><p>4.2.2 Structure Token Generation. Instead of exclusively using target node ? ? for the downstream classification, we apply structure token ? str (? ? ) to leverage the expressive neighborhood information.</p><p>According to the social influence theory, the proximal nodes tend to possess the similar feature attributes and class patterns. One would be much easier to classify a node by taking into account the positively relative patterns, which also provide the redundant information to make the classification decision robust. Structure token ? str (? ? ) denotes the subgraph centered at node ? ? . Herein we leverage the first-order adjacent nodes, the most simple subgraph, to formulate ? str (? ? ). We then embed structure token ? str (? ? ) to continuous vector as:</p><formula xml:id="formula_7">? ? ? = ? ? * ? ? + ?? ? ? ?N (? ? ) ? ? * ? ? . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>? ? is the weight to aggregate the neighborhood representations, and is learned from attention function. Note that the prompting function in NLP <ref type="bibr" target="#b19">[20]</ref> relies on the defined semantic template to inform the downstream task. Comparing with the textual language template, we define the structure token within the graph prompt by the informative neighbors of target nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Prompt Initialization and Orthogonal Prompt Constraint</head><p>Regarding the task token embeddings</p><formula xml:id="formula_9">? ? = [? ? 1 , ? ? ? , ? ? ? ]</formula><p>? at cluster ?, the simplest initialization way is to use random initialization, and then train from scratch. However, such vanilla initialization may deteriorate the node classification at the initial training stage, since the optimal task token embeddings should be at the center of node representations. Conceptually, our prompt-based transfer learning paradigm uses the pre-trained GNNs as a good initialization. It follows that pre-trained node representations might serve as good initialization spots. Therefore, we initialize token embedding ? ? ? by the mean of node representations, which are given by the training nodes of class ? ? at cluster ?. This means initialization provides the valid task tokens, and ensures the correct classification at the initial stage.</p><p>To conduct correct node classification, the prime condition is the task token embeddings of different classes should be irrelevant to each other. We utilize orthogonal constraint to keep the orthogonality of task token embeddings during model fine-tuning:</p><formula xml:id="formula_10">L ? = ?? ? ?? ? (? ? ) ? -? ? 2 ? .<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Learning Process</head><p>As mentioned in Section 4. ? init = ? pre , ? init = ? pre . (7) ? is the loss hyper-parameter. ? pre is the pre-trained parameters of projection head according to Eq. ( <ref type="formula" target="#formula_1">2</ref>). The pseudo algorithm is listed in Algorithm 1 of Appendix.</p><p>Why prompting in GNNs. Besides the benefit of bridging the task gap, the graph prompting function has more other advantages, including tuning efficiency, informative template for downstream applications, and easy deployment. (I) Tuning efficiency. Compared with the traditional transfer learning, the pre-trained GNNs and projection head are both reused without introducing the new projection head. The pre-trained knowledge could be elicited directly to accelerate the convergence in the downstream task. We provide extensive experiments below to demonstrate the efficiency of our GPPT. (II) Informative template. The graph prompting function provides the flexible template to include the informative signals for the downstream task. The task and structure token can be tailored to boost each specific case. (III) Easy deployment. As verified by the following studies, the pre-trained model shows promising results with only a few tuning epochs or even without fine-tuning. One only needs to carefully update the graph prompting function. This property is much preferred in the large-scale graph, where the training of GNNs requires costly computation and memory. We are able to fix GNNs once they are pre-trained, and tune prompt efficiently and scalablely similar to MLP for each new application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We experiment in the node classification task on benchmark datasets with the goal of answering the following research questions. Q1: Compared with supervised training, joint optimization and pretraining baselines, how effective is GPPT to boost the node classification? Q2: How efficient is the graph prompting function to adapt the pre-trained model for the downstream application with only a few epochs or even without fine-tuning? Q3: How does GPPT perform in the more difficult few-shot setting? Q4: How does the graph clustering affect the task token design and the following performance. Q5: How does the prompt initialization and regularization affect the prompt tuning? Q6: How does GPPT perform in other node classification task?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>5.1.1 Dataset. We evaluate the proposed framework GPPT on eight popular benchmark datasets, including citation networks <ref type="bibr" target="#b15">[16]</ref> (i.e, Cora, Citeseer, Pubmed), Reddit <ref type="bibr" target="#b6">[7]</ref>, CoraFull <ref type="bibr" target="#b0">[1]</ref>, Amazon-Co-Buy <ref type="bibr" target="#b28">[29]</ref> (i.e., Computer and Photo), ogbn-arxiv <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Approaches.</head><p>To evaluate the proposed "pre-train, prompt, fine-tune" learning framework of GPPT, we compare with stateof-the-art training schemes, including supervised learning, joint optimization, and pre-training. They are introduced below:</p><p>? Supervised learning: It uses the training nodes to learn GNNs, which are directly applied for model inference. We consider the following widely-studied GNN baselines, including Graph-SAGE(GS) <ref type="bibr" target="#b6">[7]</ref>, GCN <ref type="bibr" target="#b15">[16]</ref>, and GAT <ref type="bibr" target="#b32">[33]</ref>. ? Joint optimization: By simply combining the self-supervised learning objective with downstream objective, the joint learning updates GNNs to optimize both tasks. GAE <ref type="bibr" target="#b14">[15]</ref> uses graph autoencoder, and jointly optimizes the node classification loss with refactoring loss. We further apply GraphSAGE, the most popular model, as backbone and consider two joint optimization methods: EdgeMask <ref type="bibr" target="#b12">[13]</ref> learning the masked edge prediction and DGI <ref type="bibr" target="#b33">[34]</ref> leveraging the contrastive learning of deep graph infomax. They have been generally adopted to learn the graph structure. To separate from the following pre-training setting, we use Joint-Edge and Joint-DGI to denote the joint optimization. ? Pre-training: Following the transfer learning strategy of "pretrain, fine-tune", the pre-trained models are fine-tuned in the downstream applications. Similar to the joint optimization, we compare with Pre-Edge and Pre-DGI, where EdgeMask DGI are instead applied as pretext tasks, respectively.  <ref type="table" target="#tab_3">1</ref>. We make the following major observations. ? Prompt-based learning methods generally obtain the best performance on the benchmarks. While most of joint optimization methods outperform the supervised learning by leveraging the self-supervised structure signals, the pre-training based methods tend to damage the downstream node classification. As analyzed before, there exists the significant training objective gap between pretext and downstream tasks. Under the paradigm of "pre-train, fine-tune", the pre-trained knowledge may be gradually filtered out during the long fine-tuning process. The pre-training thus fails to server as good initialization for the downstream application, and results in the comparable or even worse classification accuracies. For example, Pre-DGI has 3.55% dropping compared with supervised learning model GraphSAGE on Cora, which means the negative transfer. With the graph prompting function, GPPT directly bridges the task gap to guarantee the effectiveness of pre-trained model in the downstream applications. Comparing with all the other training schemes, the prompt-based learning shows the powerful capability to elicit the pre-trained graph knowledge. The average improvement over the baselines could be up to 3.19%.</p><p>? The graph clustering and neighborhood structure are crucial for the informative prompt token designs. By ablating any of them, it is observed that the generalization performance is generally decreased. The experimental results are in line with our motivations. First, since the node embeddings are pre-trained to memorize the cluster structure, the separating task tokens (which representing the label prototypes) should be adopted for different clusters to better conduct the node classification. The more studies of cluster number are provided in the following experiments. Second, the local neighborhood structure is informative for the node classification. The proximal nodes tend to possess the similar feature attributes and class patterns. One would be much easier to classify a node by taking into account the positively relative patterns, which also provide the redundant information to make the classification decision robust. In the future work, the more task and structure tokens need to be tailored for the specific downstream applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Fine-tuning Efficiency Analysis.</head><p>Compared with other training strategies, the prompt could efficiently adapt the pre-trained model to downstream applications with only a few fine-tuning steps or even without fine-tuning. To answer Q2, we compare the training loss dynamics between GraphSAGE, Joint-Edge, Pre-Edge, and GPPT on Cora and Citeseer in Figure <ref type="figure" target="#fig_3">2</ref>. The comparisons on other datasets are in Figure <ref type="figure" target="#fig_10">5</ref> of Appendix. Furthermore, given the pretrained models in pre-training based methods and prompt-learning based approaches, we fix them and apply to the downstream tasks to compare their adaptability. The test accuracy is listed in Table <ref type="table" target="#tab_4">2</ref>. We make the following major observations.  ? GPPT consistently shows the fastest convergence towards the optimal downstream points. In general, GPPT only takes 23 finetuning epochs to optimize the downstream tasks, given the average convergence steps of 100 in other training strategies. GPPT speeds up the fine-tuning efficiency by up to 4.32X. This is because the graph prompting function in GPPT directly bridges the task gap, which enables the pre-trained GNNs and projection head could be applied to downstream task without any modification. Therefore, the pre-trained knowledge is directly elicited in the downstream application, which brings the high fine-tuning efficiency.</p><p>? The prompt-based learning paradigm even enables the pretrained GNNs to be transferred without any tuning, comparing with the traditional pre-training methods. In Table <ref type="table" target="#tab_4">2</ref>, Pre-Edge and Pre-DGI are fine-tuned with one step to adapt the new node classification layer to the downstream task. In contrast, all the pre-trained parameters are fixed in GPPT since the node classification has already been reformulated to the pretext edge prediction. We only learn the graph prompting function, a small adaption module easily to be updated. It is observed that GPPT delivers the significant improvements over Pre-Edge and Pre-DGI. The pre-training based methods without careful fine-tuning have very poor performances, e.g., the only 5.96% accuracy of Pre-DGI on Cora. These observations positively validate the applicability and reliability of GPPT to real-world systems: Without conducting the message passing and training GNNs again, the pre-trained model (or node embeddings) could be applied directly to any downstream task by only tuning the prompt similar to MLP. Such efficiency is much preferred for the large-scale graph, where the training in the whole graph is time consuming and memory expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Few-Shot Setting Analysis. The self-supervised pretext task provides unlabeled information to better tackle the few-shot learning problem. To answer research question Q3, we consider the more difficult few-shot node classification setting, by randomly masking a portion of training labels on the concerned datasets. The test accuracy obtained with 30% and 50% masking ratio is listed in Tables <ref type="table">5</ref> and<ref type="table">6</ref> in the Appendix. We make the following observations. ? Supervised learning and joint optimizing methods have poor results in the few-shot setting. The general supervised learning methods utilize empirical risk minimization. As the amount of labeled data decreases, the prior knowledge that the model can acquire will also decrease, which will lead to worse performance.</p><p>? The prompt-based learning methods effectively elicit the pretrained knowledge to improve few-shot learning. Compared with the supervised learning and joint optimization, the pre-training approaches purely optimize the pre-text objective to capture the graph structure knowledge, without suffering from the masked training labels. The pre-trained model serves as good initialization to the following few-shot problem. Furthermore, compared with pre-training approaches, the prompt-based methods train the extra prompting function to better inform the downstream node classification, could thus reduce the need of costly annotated data. GPPT delivers the average improvements ranging from 0.57% to 7.90%. 5.2.4 The Crucial Role of Graph Clustering in Task Token. The appropriate choice of cluster number, i.e., hyperparameter ?, is crucial to the task token designs and fine-tuning results. To answer Q4, we evaluate GPPT with hyperparameter range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> on Cora, and list the results in the right of Figure <ref type="figure" target="#fig_4">3</ref>. Furthermore, we visualize the node embeddings and prompt variables on Cora in the left of Figure <ref type="figure" target="#fig_4">3</ref>. The experiments on other citation network datasets are in Figure <ref type="figure" target="#fig_8">4</ref> of Appendix. We make the following observations. ? Without clustering, the graph prompting function has relatively poor result. The cluster number of ? = 1 means the graph is treated integrally without clustering, where all the nodes share the same set of task tokens. Due to the inherent clustering characteristic in graph data, the pretext task of edge prediction will assign the close embeddings for nodes within the same cluster. Meanwhile, the embedding distance between clusters is generally large. It is hard to tune the single task token of specific class, which should be close to node embeddings of the corresponding class but far away from the diverse clusters. Thus, GPPT without clustering consistently has worse node classification accuracy.</p><p>? The proper hyperparameter ? varies with datasets. The adoption of hyperparameter ? depends on the inherent cluster structure in the given graph data. The usage of ? should minimize the crosscluster connections, while maximizing the intra-cluster edges.</p><p>? The task token embeddings are close to their corresponding nodes. Using t-SNE, we visualize the high-dimensional embeddings obtained from one specific cluster on Cora. The colored circles denote nodes and the colored stars represent task tokens. The different colors are applied to indicate label classes. It is observed that task token and nodes of the same classes are tuned to be proximal to each other. This property demonstrates the reliability of using task token to perform the node classification, since there exists high linking probability between a node and its corresponding task token. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Prompt Tuning Analysis.</head><p>To study the impacts of prompt initialization and orthogonal regularization on the prompt tuning and to answer Q5, we respectively ablate these two components, and show the test accuracies in Table <ref type="table" target="#tab_5">3</ref>.</p><p>? The proper prompt initialization and orthogonal regularization can effectively improve the performance of GPPT. Compared with GPPT in Table <ref type="table" target="#tab_3">1</ref>, the ablation of any of them generally results in performance dropping. The dropping scale range from 0.3% to 3.2%. Since the optimal task token embeddings should be at the center of node representations, the vanilla initialization may deteriorate model at the initial training stage, and misleads the following training process. The orthogonal constrain in Eq. ( <ref type="formula" target="#formula_10">6</ref>) is important to separate task tokens to accurately conduct the node classification. 5.2.6 Node Degree Classification. Besides the node label classification defined in the benchmark datasets, there are many other problems concerned in graph data, such as node degree prediction and pairwise node distance estimation <ref type="bibr" target="#b12">[13]</ref>. To answer Q6, we adopt the node degree prediction as new downstream application, and test the generality of pre-trained models. Specifically, we randomly mask part of edges, and train GNNs to predict the ground-truth degrees of nodes. We list the comparison results on the three citation networks in Table <ref type="table">7</ref> in the appendix. It is observed GPPT shows the powerful generality to the new application. The improvement obtained by GPPT is up to 46%. This empirical property greatly improve the applicability of pre-trained: By training on one pretext task, GPPT can easily adapt it to new application by only changing the prompting function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>We innovatively propose GPPT, the first transfer learning paradigm of "pre-train, prompt, fine-tune" for GNNs. The graph prompting function is developed to reformulate the downstream task looking similar to the pretext one, which aims to reduce their training objective gap. Moreover, we design the task and structure token generation methods, which are used to define node prompt for the node classification applications. The mean prompt initialization and orthogonal regularization are proposed to improve the prompt tuning. Extensive experiments demonstrate that GPPT consistently outperforms the traditional training paradigms on benchmark graphs, accompanied with improved tuning efficiency and better adaptability to the downstream tasks. In the future work, we will explore the graph prompting function in the more challenging knowledge graph, and try meta learning to improve prompt tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DATASET STATISTICS</head><p>The dataset statistics for the concerned node classification are summarized in Table <ref type="table" target="#tab_6">4</ref>. We follow the official splitting of training/validation/testing for all the benchmark datasets. The datasets are public, and have been included in Torch Geometric. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PSEUDO ALGORITHM</head><p>The pseudo algorithm of GPPT is given in Algorithm 1. Specifically, we use batch training to handle both of the small and large graphs.  We embed the token pair into continuous vectors. We then concatenate token pair vectors as input to the pretrained projection head, and obtain the edge connection probability.</p><p>13:</p><p>Calculate cross entropy loss L pre and orthogonality prompt constraint loss L ? by using Eq.( <ref type="formula" target="#formula_10">6</ref>).</p><p>14:</p><p>Update the parameters by minimizing the objective in Eq.( <ref type="formula">7</ref>). 15: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HYPERPARAMETER STUDY OF CLUSTER NUMBER</head><p>The hyperparameter study reault on citation network datasets Citseer and Pubmed, which are shown in Figure <ref type="figure" target="#fig_8">4</ref>. Similar to our previous analysis, without graph clustering via setting cluster number ? = 1, the classification accuracy of GPPT is significantly damaged.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of GPPT. The masked edge prediction is leveraged to pre-train GNNs. To bridge training objective gap, the graph prompting function modifies the standalone node into token pairs, and directly applies the pre-trained model without changing classification layer (i.e., MLP). The token pair contains task token (denoting node label) and structure token (describing node). The node classification is reformulated to measure the linking score of token pair. The task token with the highest score is decided as node label.</figDesc><graphic url="image-2.png" coords="2,317.96,83.69,240.24,105.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 4 . 1 (</head><label>41</label><figDesc>Graph prompting function). A graph prompting function ? prompt is applied to change the standalone node into prompt: ? ? ? = ? prompt (? ? ). Prompt ? ? ? has the same input shape to the pretext projection head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Definition 4 . 2 (</head><label>42</label><figDesc>Pairwise prompting function). Considering the edge prediction or contrastive learning pretext task, prompt ? ? ? is described by the template of token pairs: ? ? ? = ? prompt (? ? ) = [? task (?),? srt (? ? )]. ? task (?) is the task token to describe the downstream application, and ? srt (? ? ) is the structure token to represent target node ? ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Training losses with epochs on Cora and Citeseer.</figDesc><graphic url="image-3.png" coords="7,63.51,417.39,105.93,62.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Left: Node and task token visualization on Cora. Right: Cluster number study on Cora.</figDesc><graphic url="image-5.png" coords="8,56.79,534.16,232.03,102.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 6 : while not done do 7 : 8 : 9 :</head><label>16789</label><figDesc>The batch sizes are 256, 256, 2048, 256, 4096, 2048, 4096, and 4096 for Cora, Citeseer, CoraFull, Pubmed, Ogbn-arxiv, AmazonCoBuy-Computer, AmazonCoBuyPhoto, and Reddit, respectively. Training process of GPPT Input: Input graph G = (?, ?). Output: The learned parameters ?, ?, task token embeddings ? 1 , ? ? ? , ? ? . 1: Initialize GNN model ? ? with the pre-trained parameters ? ? . 2: Hyper-parameter: Cluster number ?. 3: Split nodes into multiple non-overlapped clusters {G 1 , ? ? ? , G ? }. 4: For each cluster G ? we train an independent task token embeddings ? ? = [? ? 1 , ? ? ? , ? ? ? ] ? ? R ??? . 5: Given task token ? task (? ? ) of node ? ? at cluster ?, it is embeded by vector ? ? ? Sample bach of nodes G ? ? G to fin-tuning our model. for each node ? ? G ? do Construct the prompt ? ? = ? prompt (?) to obtain token pair [? task (? ? ),? srt (?)], for ? = 1, ? ? ? , ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Cluster number study on Citeseer. Right: Clusnumber study on Pubmed. D FINE-TUNING EFFICIENCY The comparisons of the training loss dynamics between Graph-SAGE, Joint-Edge, Pre-Edge, and GPPT on Pubmed, CoraFull, Computer, Photo, ogbn-arxiv and Reddit in Figure 5. It is observed GPPT consistently shows the most efficient convergence, compared with the traditional training paradigms.</figDesc><graphic url="image-6.png" coords="10,324.59,169.16,226.98,121.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Training losses with epochs benchmark datasets.</figDesc><graphic url="image-11.png" coords="10,328.79,593.30,105.93,62.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? task (? ? ),? srt (? ? )); ?(? ? , ? ? )).?(? ? , ? ? ) denotes the ground truth connection between label class ? ? and target node ? ? . ?(? ? , ? ? ) = 1 if node ? ? belongs to class ? ? ; otherwise it is zero. Therefore, we have the same training objective (i.e., edge prediction) with the pretext task, and bridge the optimization objective gap. Below we introduce how to design the task and structure tokens tailored to graph data.</figDesc><table><row><cell></cell><cell>??</cell><cell>L pre (?</cell><cell>(4)</cell></row><row><cell>?,?</cell><cell>(? ? ,? ? )</cell><cell></cell></row></table><note><p>pre ? (</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1, the transfer learning framework of "pre-train, prompt, finee-tune" contains three steps, namely prompt addition, prompt answer, and prompt tuning. Based on the designs of graph prompting function and orthogonal prompt constraint, we reiterate the learning process. First, the prompt addition step modifies target node ? ? with token pair [? task (?),? srt (? ? )], and represents them with embeddings [? ? ? , ? ? ? ]. Second, the prompt answer evaluates a series of token pairs for the different classes, and classifies a node by task token accompanied with the highest linking probability. The prompt tuning optimizes the pre-trained GNNs and token embeddings as below: min ?,?,? 1 ,??? ,? ? (? ? ,? ? ) L pre (?</figDesc><table /><note><p>pre ? (? ? ? , ? ? ? ); ?(? ? , ? ? )) + ?L ? , s.t.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>For our GPPT, we use the orthogonality prompt constraint coefficient ? of 0.01. The clustering numbers in Cora, Citeseer, Pubmed, CoraFull, Computer, Photo, ogbn-arxiv, Reddit are 7, 6, 3, 5, 10, 8, 10, 10, respectively.</figDesc><table><row><cell>5.2 Performance Analysis</cell></row><row><cell>5.2.1 Node Classification Studies. To answer research question</cell></row><row><cell>Q1, we list the comparison between different categories of training</cell></row><row><cell>methods in Table</cell></row><row><cell>? Prompt-based tuning: Our GPPT is realized by following the</cell></row><row><cell>learning paradigm of "pre-train, prompt, fine-tune". Meanwhile,</cell></row><row><cell>two simple versions are developed: GPPT (w/o C) and GPPT (w/o</cell></row><row><cell>N). Specifically, GPPT (w/o C) removes the graph clustering and</cell></row><row><cell>applies a single task token matrix; GPPT (w/o N) replaces the</cell></row><row><cell>structure token with target node itself to ablate the neighbor-</cell></row><row><cell>hoods during classification.</cell></row><row><cell>5.1.3 Implementations. Following the previous efforts and ensur-</cell></row></table><note><p>ing the fair comparison, we adopt the layer number of 2 and the hidden units of 128 for all GNNs. The Adam optimizer accompanied with weight decay of 5e-4 is applied at both the pre-training and fine-tuning stages. The learning rate is chosen from [0.001,0.005] depending on datasets.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Node classification accuracy in percent. The best case is in bold. Imp(%) at the last column means the average improvement of GPPT over baseline.</figDesc><table><row><cell>Training Methods</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Test accuracies (in percent) of pre-trained models without fine-tuning. The higher one has better adaptability. Edge 46.68?0.39 23.96?0.94 21.96?0.26 21.75?0.24 45.84?0.79 42.69?0.13 38.59?0.22 88.72?0.33 101.26 Pre-DGI 5.96?0.34 16.95?0.85 40.75?0.18 5.76?0.13 37.26?0.99 22.30?0.25 9.16?0.48 65.39?0.11 396.85 Prompt GPPT(fix) 77.19?0.31 63.84?0.45 78.90?0.18 41.36?0.44 77.27?0.25 87.15?0.14 60.22?0.67 88.77?0.42 -</figDesc><table><row><cell cols="2">Training Methods schemes</cell><cell>Cora</cell><cell>Data Citeseer Pubmed CoraFull Computer Photo ogbn-arxiv Reddit</cell><cell>Imp(%)</cell></row><row><cell>Pre-train</cell><cell>Pre-</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Test accuracy in percent of GPPT by ablating prompt initialization (init.) or orthogonal prompt constraint (cons.). The ?/? arrow means the decreasing/improvement compared with the corresponding GPPT in Table1.</figDesc><table><row><cell>Methods</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>CoraFull</cell><cell>Computer</cell><cell>Photo</cell><cell>ogbn-arxiv</cell><cell>Reddit</cell></row><row><cell cols="2">GPPT(w/o cons.) 80.81?0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell cols="2">#Feature #Labels</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell>5,429</cell><cell>1433</cell><cell>7</cell></row><row><cell>Citeseer</cell><cell>3,327</cell><cell>4,732</cell><cell>3703</cell><cell>6</cell></row><row><cell>Pubmed</cell><cell>19,717</cell><cell>44,338</cell><cell>500</cell><cell>3</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 11,606,919</cell><cell>602</cell><cell>41</cell></row><row><cell>CoraFull</cell><cell>19,793</cell><cell>126,84</cell><cell>126,84</cell><cell>70</cell></row><row><cell>Computer</cell><cell>13,752</cell><cell>491,722</cell><cell>767</cell><cell>10</cell></row><row><cell>Photo</cell><cell>7,650</cell><cell>238,163</cell><cell>745</cell><cell>8</cell></row><row><cell cols="3">ogbn-arxiv 169,343 1,166,243</cell><cell>128</cell><cell>40</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGMENTS</head><p>We express gratitude to the anonymous reviewers for their hard work and kind comments. This work is supported by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61872161</rs>, No. <rs type="grantNumber">61976102</rs>), the Foundation of the <rs type="projectName">Major</rs> Project <rs type="projectName">of Science and Technology Innovation 2030 -New Generation of Artificial Intelligence</rs> (No. <rs type="grantNumber">2021ZD0112500</rs>), the <rs type="funder">Interdisciplinary and integrated innovation of JLU</rs> (No. <rs type="grantNumber">JLUXKJC2020207</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3JyAvPb">
					<idno type="grant-number">61872161</idno>
				</org>
				<org type="funded-project" xml:id="_8ZEeupF">
					<idno type="grant-number">61976102</idno>
					<orgName type="project" subtype="full">Major</orgName>
				</org>
				<org type="funded-project" xml:id="_vbXD2Hn">
					<idno type="grant-number">2021ZD0112500</idno>
					<orgName type="project" subtype="full">of Science and Technology Innovation 2030 -New Generation of Artificial Intelligence</orgName>
				</org>
				<org type="funding" xml:id="_kZRWpdZ">
					<idno type="grant-number">JLUXKJC2020207</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E FEW-SHOT LEARNING WITH MASKING</head><p>The test accuracy obtained with 30% and 50% masking ratio is listed in Table <ref type="table">5</ref> and Table <ref type="table">6</ref>. Similar to the masking ratio of 50%, GPPT consistently delivers the most outperforming results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F NODE DEGREE PREDICTION</head><p>We adopt the node degree prediction as new downstream application, and test the generality of pre-trained models. We list the comparison results on the three citation networks in Table <ref type="table">7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ENVIRONMENT</head><p>All the experiments are implemented with PyTorch and graph package of Torch Geometric. They are tested on the machine with one NVIDIA3090 GPU.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Gaussian Embedding of Graphs: Unsupervised Inductive Learning via Ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Training Deeper Graph Neural Networks: A Comprehensive Benchmark Study</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-Scale Learnable Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Kai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11338</idno>
		<title level="m">Orthogonal Graph Neural Networks</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Strategies for Pre-training Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GPT-GNN: Generative Pre-Training of Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1857" to="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph meta learning via local subgraphs</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5862" to="5874" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-Network Learning with Partially Aligned Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="746" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Variational Graph Auto-Encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The power of scale for parameter-efficient prompt tuning</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4582" to="4597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>abs/2107.13586</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">EXACT: Scalable graph neural networks training via extreme activation compression</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to Pretrain Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yuanfu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xunqiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4276" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Streaming Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
	<note>SIGIR 2020</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transfer Graph Neural Networks for Pandemic Forecasting</title>
		<author>
			<persName><forename type="first">George</forename><surname>Panagopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4838" to="4845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GCC: Graph Contrastive Coding for Graph Neural Network Pre-Training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph Representation Learning via Ladder Gamma Variational Autoencoders</title>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Rai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5604" to="5611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5569" to="5578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the NAACL-HLT</title>
		<meeting>the 2021 Conference of the NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation. Relational Representation Learning Workshop</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4222" to="4235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.15980</idno>
		<title level="m">Eliciting knowledge from language models with automatically generated prompts</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">MOTIF-Driven Contrastive Learning of Graph Representations</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Subramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15980" to="15981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax. ICLR (Poster)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Universal Adversarial Triggers for Attacking and Analyzing NLP</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Kandpal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EMNLP-IJCNLP 2019</title>
		<meeting>the EMNLP-IJCNLP 2019</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2153" to="2162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Billion-scale commodity embedding for e-commerce recommendation in alibaba</title>
		<author>
			<persName><forename type="first">Jizhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pipei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dik</forename><surname>Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="839" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph convolutional autoencoders with co-learning of graph structure and node attributes</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiye</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page">108215</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Streaming Graph Neural Networks via Continual Learning</title>
		<author>
			<persName><forename type="first">Junshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1515" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Acekg: A large-scale knowledge graph for academic data mining</title>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM international conference on information and knowledge management</title>
		<meeting>the 27th ACM international conference on information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1487" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Advanced graph and sequence neural networks for molecular property prediction and drug discovery</title>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoning</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2579" to="2586" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Towards Content Provider Aware Recommender Systems: A Simulation Study on the Interplay between User and Provider Utilities</title>
		<author>
			<persName><forename type="first">Ruohan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantina</forename><surname>Christakopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayden</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Mladenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3872" to="3883" />
		</imprint>
	</monogr>
	<note>In WWW &apos;21: The Web Conference. ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-gnn: On few-shot node classification in graph meta-learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goce</forename><surname>Trajcevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2357" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal Augmented Graph Neural Networks for Session-Based Recommendations</title>
		<author>
			<persName><forename type="first">Huachi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1798" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4917" to="4928" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dirichlet energy constrained learning for deep graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03184</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multi-channel graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1352" to="1358" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
