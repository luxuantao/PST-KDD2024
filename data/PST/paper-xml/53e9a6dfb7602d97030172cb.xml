<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multiple graph regularized nonnegative matrix factorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-03-16">16 March 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing-Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer, Electrical and Mathematical Sciences and Engineering Division</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Halima</forename><surname>Bensmail</surname></persName>
							<email>hbensmail@qf.org.qa</email>
							<affiliation key="aff1">
								<orgName type="institution">Qatar Computing Research Institute</orgName>
								<address>
									<postCode>5825</postCode>
									<settlement>Doha</settlement>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
							<email>xin.gao@kaust.edu.sa</email>
							<affiliation key="aff0">
								<orgName type="department">Computer, Electrical and Mathematical Sciences and Engineering Division</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Computational Bioscience Research Center</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Computer, Electrical and Mathematical Sciences and Engineering Division</orgName>
								<orgName type="institution">King Abdullah University of Science and Technology (KAUST)</orgName>
								<address>
									<postCode>23955-6900</postCode>
									<settlement>Thuwal</settlement>
									<country key="SA">Saudi Arabia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multiple graph regularized nonnegative matrix factorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-03-16">16 March 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">E3DA4E708D7F44AC3405FAAB3FAFB3BC</idno>
					<idno type="DOI">10.1016/j.patcog.2013.03.007</idno>
					<note type="submission">Received 12 June 2012 Received in revised form 11 December 2012 Accepted 5 March 2013</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Data representation Nonnegative matrix factorization Graph Laplacian Ensemble manifold regularization</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Non-negative matrix factorization (NMF) has been widely used as a data representation method based on components. To overcome the disadvantage of NMF in failing to consider the manifold structure of a data set, graph regularized NMF (GrNMF) has been proposed by Cai et al. by constructing an affinity graph and searching for a matrix factorization that respects graph structure. Selecting a graph model and its corresponding parameters is critical for this strategy. This process is usually carried out by crossvalidation or discrete grid search, which are time consuming and prone to overfitting. In this paper, we propose a GrNMF, called MultiGrNMF, in which the intrinsic manifold is approximated by a linear combination of several graphs with different models and parameters inspired by ensemble manifold regularization. Factorization metrics and linear combination coefficients of graphs are determined simultaneously within a unified object function. They are alternately optimized in an iterative algorithm, thus resulting in a novel data representation algorithm. Extensive experiments on a protein subcellular localization task and an Alzheimer's disease diagnosis task demonstrate the effectiveness of the proposed algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Non-negative matrix factorization (NMF) techniques have become popular data representation methods in recent years for a number of problems such as bioinformatics, medical imaging, computer vision, etc. When a non-negative matrix X is given, NMF attempts to find two lower dimensional non-negative matrices H and W, the product of which provides a good approximation of the original product. A standard NMF determines factorization matrices H and W by minimizing the loss function defined by Euclidean distance or the divergence between X and HW. Recently, a great number of studies have been conducted to improve the NMF method. For example, Sandler and Lindenbaum proposed two new NMF algorithms that minimize Earth mover's distance error between data and matrix product <ref type="bibr" target="#b0">[1]</ref>. Bonettini applied the cyclic block gradient method on large-scale problems resulting from the NMF approach <ref type="bibr" target="#b1">[2]</ref>. <ref type="bibr">Guan et al.</ref> proposed a new and efficient NeNMF solution to simultaneously overcome the problems of slow convergence rate, numerical instability, and nonconvergence <ref type="bibr" target="#b2">[3]</ref>. <ref type="bibr">Cichocki et al.</ref> proposed a class of multiplicative algorithms for NMF by formulating a new family of generalized divergences referred to as alpha-beta divergences (AB-divergences) <ref type="bibr" target="#b3">[4]</ref>. Guan et al. introduced manifold regularization and margin maximization to NMF, and obtained manifold regularized discriminative NMF (MD-NMF) as a result <ref type="bibr" target="#b4">[5]</ref>. Das Gupta and Jing pursued a discriminative decomposition process by coupling NMF objective with a support vector machine (SVM), and proposed an SVM regularizer based on NMF <ref type="bibr" target="#b5">[6]</ref>. Hsieh and Dhillon presented a variable selection scheme for NMF that uses the gradient of the objective function to develop a new coordinate descent method <ref type="bibr" target="#b6">[7]</ref>. Guan et al. presented a non-negative patch alignment framework to combine popular NMF techniques by proposing a fast gradient descent <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr">Lefevre et al.</ref> proposed an unsupervised inference procedure for separating audio sources by automatically grouping the components in an NMF in audio sources via a penalized maximum likelihood approach <ref type="bibr" target="#b8">[9]</ref>. Rezaei et al. enhanced NMF performance by using fuzzy c-means clustering as an efficient initialization method for estimating initial NMF factors <ref type="bibr" target="#b9">[10]</ref>.</p><p>By conducting this knowledge in Euclidean space, NMF fails to discover the intrinsic geometrical and discriminating structure of data space <ref type="bibr" target="#b10">[11]</ref>. To avoid such limitation, Cai et al. introduced the graph regularized NMF (GrNMF) algorithm by incorporating a geometrically based regularizer <ref type="bibr" target="#b10">[11]</ref>. The local geometric structure is modeled by a P-nearest neighbor graph on a scattering of data points. As argued by <ref type="bibr" target="#b10">[11]</ref>, graph construction is critical for GrNMF. A number of methods on defining the P-nearest neighbor graph Contents lists available at SciVerse ScienceDirect journal homepage: www.elsevier.com/locate/pr and its corresponding weight matrix are available. The performance of these methods is known to hinge heavily on the choice of graph. Unfortunately, we do not know which graph is most suitable for a particular task. Moreover, an exhaustive search on a predefined pool of graphs and their parameters will be time consuming. Therefore, efficiently determining an appropriate graph to make the performance of the employed graphregularized representation method robust, or even better, is crucial.</p><p>To address the aforementioned problems, and inspired by ensemble manifold regularization (EMR) <ref type="bibr" target="#b11">[12]</ref>, we propose a multiple graph regularization framework for GrNMF, which combines automatic intrinsic manifold approximation and NMF. By providing a series of initial guesses of the graph Laplacian, the framework learns how to combine them to approximate the intrinsic manifold. At the same time, factorization matrices for NMF are solved and restricted to run smoothly along the estimated graph.</p><p>The rest of the paper is organized as follows. Section 2 introduces our multiple graph regularized NMF (MultiGrNMF) algorithm. Experimental results on two data sets are presented in Section 3. Finally, we give our concluding remarks and suggestions for future work in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MultiGrNMF</head><p>In this section, we present our proposed MultiGrNMF algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Objective function</head><p>Given N data points in the training set with their non-negative feature vector set X ¼ fx n g, n ¼ 1…,N, we organize them as a nonnegative data matrix X ¼ ½x 1 ,…,x n ∈R DÂN þ , where n-th column x n of X is the feature vector of n-th data point. NMF aims to find two non-negative matrices H and W, the product of which can approximate well the original matrix X as X≈HW, where H∈R DÂR þ and W∈R RÂN þ . We commonly have R⪡D and R⪡N. In reality, each feature vector x n is approximated by a linear combination of the columns of H, and weighted by the components of W, as</p><formula xml:id="formula_0">x n ≈ ∑ R r ¼ 1 h r w rn<label>ð1Þ</label></formula><p>Therefore, H can be regarded as containing a set of basis vectors. Let w n ¼ ½w 1n ,…,w Rn ⊤ denote the n-th columns of W. w n can be regarded as the coding vector or as a new representation of the n-th data point with respect to the basis H. To learn the basis matrix H and the coding matrix W, we propose a novel objective function O MultiGrNMF . This objective function considers the local manifold structure of data space for the regularization of NMF and approximates the intrinsic local manifold by combining several initial graphs. The proposed objective function is composed of two terms: (1) the original loss function of NMF and (2) the multiple graph regularization term.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">NMF loss term</head><p>The most commonly used NMF loss function is based on l 2 norm distance between two matrices <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_1">O NMF ðH,WÞ ¼ ∥X-HW∥ 2 ¼ TrðX ⊤ XÞ-2 TrðX ⊤ HWÞþTrðW ⊤ H ⊤ HWÞ:<label>ð2Þ</label></formula><p>The aforementioned objective function can be minimized by the iterative update algorithm proposed by Lee and Seung <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Multiple graph regularization term</head><p>In GrNMF, local invariance assumption was imposed to NMF in the following manner. If two feature vectors x n and x m are close in the intrinsic geometry of data distribution, then w n and w m , the coding vectors of these two feature vectors with respect to the new basis, are also close to each other, and vice versa <ref type="bibr" target="#b10">[11]</ref>. The local geometric structure is modeled by a P nearest neighbor graph G on a scattering of data points <ref type="bibr" target="#b10">[11]</ref>. For each feature vector x n ∈X , its P nearest neighbors N n in X , is first found. A P nearest neighbor graph is then constructed for X as G ¼ fV,E,Ag. The node set V corresponds to N data points. E is the edge set, and ðn,mÞ∈E if x m ∈N n or x n ∈N m . A∈R NÂN is the weight matrix on the graph with A nm equal to the weight of edge (n, m). With weight matrix A, we can use the following graph regularization term to measure the smoothness of low-dimensional coding vector representations in</p><formula xml:id="formula_2">W O Gr ðWÞ ¼ 1 2 ∑ N n,m ¼ 1 ∥w n -w m ∥ 2 A nm ¼ TrðWUW ⊤ Þ-TrðWAW ⊤ Þ ¼ TrðWLW ⊤ Þ,<label>ð3Þ</label></formula><p>where U is a diagonal matrix, the entries of which are column sums of A, U nn ¼ ∑ N n ¼ 1 A nm , and L ¼ U-A is the graph Laplacian. By minimizing O Gr ðW; AÞ with regard to W, we can expect that if two feature vectors x n and x m are close to one another (i.e., A nm is big), then w n and w m will also be close to each other.</p><p>A number of techniques for defining weight matrix A are available. Four of the most commonly used methods are as follows: 0-1 weighting is used to regularize NMF and sparse coding in <ref type="bibr" target="#b10">[11]</ref>. It is defined as</p><formula xml:id="formula_3">A nm ¼ 1, if ðn,mÞ∈E, 0, else: (<label>ð4Þ</label></formula><p>Heat kernel weighting is also widely used in most manifold learning algorithms <ref type="bibr" target="#b10">[11]</ref>. This technique is defined as</p><formula xml:id="formula_4">A nm ¼ e -ð∥xn-xm∥ 2 Þ=s 2 , if ðn,mÞ∈E,<label>0, else:</label></formula><p>(</p><p>Histogram intersection kernel weighting is usually used to con- struct similarity graphs of scale-invariant feature transform features. It is defined as</p><formula xml:id="formula_6">A nm ¼ ∑ D d ¼ 1 minðx dn ,x dm Þ, if ðn,mÞ∈E, 0, else: 8 &gt; &lt; &gt; :<label>ð6Þ</label></formula><p>Given such numerous graph weight matrix model definitions with different parameters, we can compute several corresponding graph Laplacians. The number of nearest neighbors should also be noted as a parameter for nearest neighbor graph construction. With different numbers of nearest neighbors, we can produce various graphs. As such, this parameter is not controlled by the user, but is selected automatically by the algorithm introduced as follows, by weighting graphs constructed using different numbers of nearest neighbors. Suppose that we have already computed a set of candidate graph Laplacians fL 1 ,…,L K g. Similar to EMR, we assume that the intrinsic manifold is located in the convex hull of the previously given manifold candidates <ref type="bibr" target="#b11">[12]</ref>. This assumption constrains the search space of possible graph Laplacians as linear combinations of M candidate Laplacians,</p><formula xml:id="formula_7">L ¼ ∑ K k ¼ 1 τ k L k , s:t: ∑ K k ¼ 1 τ k ¼ 1, τ k ≥0,<label>ð7Þ</label></formula><p>where τ k is the combination weight of k-th graph Laplacian. To avoid negative contribution, we further constrain</p><formula xml:id="formula_8">∑ K k ¼ 1 τ k ¼ 1, τ k ≥0.</formula><p>As such, we attempt to determine optimal linear combination weights for a group of pre-computed graph candidates instead of selecting the optimal graph model and estimating its parameters.</p><p>By substituting ( <ref type="formula" target="#formula_7">7</ref>) to (3), we obtain the augmented objective function of multiple graph regularization in an enlarged parameter space,</p><formula xml:id="formula_9">O MultiGr ðW,τÞ ¼ Tr W ∑ K k ¼ 1 τ k L k W ⊤ ! ¼ ∑ K k ¼ 1 τ k TrðWL k W ⊤ Þ s:t: ∑ K k ¼ 1 τ k ¼ 1, τ k ≥0,<label>ð8Þ</label></formula><p>where τ ¼ ½τ 1 ,…,τ K ⊤ is the graph weight vector. MultiGr is different from the EMR introduced in <ref type="bibr" target="#b11">[12]</ref>. The latter tries to minimize classifier complexity over the composite manifold, whereas the former aims to restrict coding vectors of NMF to enable them to move smoothly along the estimated composite manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Object function of MultiGrNMF</head><p>Combining the multiple graph-based regularizer O MultiGr ðW,τÞ with the original NMF objective function O NMF ðH,WÞ results in the loss function of MultiGrNMF as,</p><formula xml:id="formula_10">O MultiGrNMF ðH,W,τÞ ¼ O NMF ðH,WÞþαO MultiGr ðWÞþβ∥τ∥ 2 ¼ TrðX ⊤ XÞ-2 TrðX ⊤ HWÞþTrðW ⊤ H ⊤ HWÞ þ α ∑ K k ¼ 1 τ k TrðWL k W ⊤ Þþβ∥τ∥ 2 :<label>ð9Þ</label></formula><p>To avoid parameter τ overfitting to a single graph, we also introduce a l 2 norm regularization term ∥τ∥ 2 to τ. In ( <ref type="formula" target="#formula_10">9</ref>), α and β are tradeoff parameters that balance the three terms. Thus, the MultiGrNMF problem turns into a minimization problem as, min</p><formula xml:id="formula_11">H,W,τ O MultiGrNMF ðH,W,<label>τÞ</label></formula><formula xml:id="formula_12">s:t: H≥0, W≥0, ∑ K k ¼ 1 τ k ¼ 1, τ k ≥0:<label>ð10Þ</label></formula><p>The relationship between MultiGrNMF and GrNMF is explained as follows. As we can see from the previously discussed objective function, GrNMF is a special case that occurs when only one graph is present in the graph pool. When only one graph is used, the weight of this graph will be solved as one, and MultiGrNMF will degenerate into GrNMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Optimization</head><p>Instead of optimizing (10) directly, we optimize NMF factorization matrices (H, W) and graph weights τ by using an iterative, two-step strategy because direct optimization to (10) is difficult. At each iteration, either (H, W) or τ is optimized first while the other is fixed, and then the roles of (H, W) and τ are reversed. Iterations are repeated until convergence is achieved or a maximum number of iterations are reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">On optimizing (H, W)</head><p>By fixing τ, using the matrix property TrðXÞ ¼ TrðX ⊤ Þ and TrðXYÞ ¼ TrðYXÞ, and removing irrelevant items, the optimization problem <ref type="bibr" target="#b9">(10)</ref> </p><formula xml:id="formula_13">is reduced to min H,W -2 TrðX ⊤ HWÞþTrðW ⊤ H ⊤ HWÞþα TrðWLW ⊤ Þ s:t: H≥0, W≥0:<label>ð11Þ</label></formula><p>Let ϕ dr and ψ rn be the Lagrange multipliers for constraints h dr ≥0 and w rn ≥0, respectively, and Φ ¼ ½ϕ dr , Ψ ¼ ½ψ rn , the Lagrange L of ( <ref type="formula" target="#formula_13">11</ref>) is</p><formula xml:id="formula_14">L ¼ -2 TrðX ⊤ HWÞþTrðW ⊤ H ⊤ HWÞþα TrðWLW ⊤ ÞþTrðΦH ⊤ ÞþTrðΨ W ⊤ Þ:<label>ð12Þ</label></formula><p>The partial derivatives of L, with respect to basis matrix H and coding matrix W are</p><formula xml:id="formula_15">∂L ∂H ¼ -2XW ⊤ þ2HWW ⊤ þ Φ and ∂L ∂W ¼ -2H ⊤ X þ 2H ⊤ HW þ 2αWL þ Ψ :<label>ð13Þ</label></formula><p>By using the Karush-Kuhn-Tucker conditions ϕ dr h dr ¼ 0 and ψ rn w rn ¼ 0, and substituting L ¼ U-A to (13), we obtain the following equations for h dr and w rn :</p><formula xml:id="formula_16">-ðXW ⊤ Þ dr h dr þðHWW ⊤ Þ dr h dr ¼ 0 and -ðH ⊤ X þ αWAÞ rn w rn þðH ⊤ HW þ αWUÞ rn w rn ¼ 0:<label>ð14Þ</label></formula><p>These equations lead to the following updating rules:</p><formula xml:id="formula_17">h dr ← ðXW ⊤ Þ dr ðHWW ⊤ Þ dr h dr and w rn ← ðH ⊤ X þ αWAÞ rn ðH ⊤ HW þ αWUÞ rn w rn :<label>ð15Þ</label></formula><p>2.2.2. On optimizing τ By fixing (H, W) and removing irrelevant terms, the optimization problem <ref type="bibr" target="#b9">(10)</ref> is transformed into</p><formula xml:id="formula_18">min τ ∑ K k ¼ 1 τ k s k þ γ∥τ∥ 2 s:t: ∑ K k ¼ 1 τ k ¼ 1, τ k ≥0,<label>ð16Þ</label></formula><p>where</p><formula xml:id="formula_19">s k ¼ TrðWL k W ⊤ Þ and γ ¼ β=α. Additional constraints ∑ K k ¼ 1 τ k ¼ 1, τ k ≥0</formula><p>cause the optimization presented in <ref type="bibr" target="#b15">(16)</ref> to turn into a constrained quadratic programming (QP) problem <ref type="bibr" target="#b14">[15]</ref>. As such, the optimization can now be efficiently solved by the algorithm based on coordinate descent <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Algorithm</head><p>Listed as Algorithm 1, the procedure for MultiGrNMF requires an initial guess for both τ and (H, W) in the alternating optimization. We have tried the following initialization strategies:</p><p>1. τ is initialized by setting all of its elements as 1=K to base graphs with equal weights; 2. (H, W) is initialized by performing the original NMF to X.</p><p>We average performances obtained by the models with different initializations. In our empirical testing, initialization strategies exhibit stable performances.</p><p>Algorithm 1. The training procedure of the proposed MultiGrNMF algorithm.</p><p>Input: Training data matrix X; Input: K graph Laplacians fL 1 ,…,L K g constructed from X with different graph models and parameters; Make an initial guess for τ 0 and ðH 0 ,W 0 Þ; for t ¼ 1,…,T do Update the basis matrix H t and coding matrix W t by ( <ref type="formula" target="#formula_16">14</ref>) while fixing τ t-1 ; Update graph weights τ t by ( <ref type="formula" target="#formula_18">16</ref>) while fixing H t and W t ; end for Output: U T , W T and τ T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>Two experiments have been conducted to demonstrate the effectiveness of the proposed MultiGrNMF algorithm on two challenging tasks: (1) protein subcellular localization and (2) Alzheimer's disease (AD) diagnosis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experiment I: Protein subcellular localization of fluorescence imagery</head><p>Predicting protein subcellular locations is crucial in the complete understanding of various protein functions. Currently, fluorescence microscopy is the most suitable method for proteome-wide determination of subcellular location. In this experiment, we evaluate MultiGrNMF as a feature representation method for protein subcellular localization of fluorescence imagery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Data set and setup</head><p>In this empirical evaluation, we use the 2D HeLa image data set <ref type="bibr" target="#b16">[17]</ref>. This data set consists of 862 single-cell images, each measuring 382 Â 382. Each image contains a single cell from one of the 10 major classes of protein localization patterns. Subcellular location patterns in these collections include endoplasmic reticulum (ER), the Golgi complex, lysosomes, mitochondria, nucleoli, actin microfilaments, endosomes, microtubules, and nuclear DNA <ref type="bibr" target="#b16">[17]</ref>. Several sample images from this data set is shown in Fig. <ref type="figure">1</ref>.</p><p>In this experiment, we use various texture-based feature extraction strategies, including Haralick textures, local binary patterns, local ternary patterns, etc. The feature vector x of each image is constructed by fusing the aforementioned features as hybrid features. The features are further entered as inputs to MultiGrNMF for feature representation. We used the following graph types to construct multiple graphs for MultiGrNMF: 0-1 weighted graph, heat kernel weighted graph, and histogram intersection kernel weighted graph. By varying neighborhood size parameters for all graph types and the bandwidth parameter s for heat kernel weighted graph, we have obtained a total of 25 graphs for this experiment. Moreover, a10-fold cross-validation is employed to test the performance of MultiGrNMF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Results</head><p>We first tested the classification performance of MultiGrNMF, GrNMF, and NMF against the number of basis vectors R. Results are shown in Fig. <ref type="figure">2(a)</ref>. Classification accuracies of MultiGrNMF, GrNMF, and NMF have all increased when more basis vectors are used to represent data. This figure shows that the proposed MultiGrNMF coding outperforms the other two algorithms. For this data set, classification performances of the original NMF with l 2 norm metrics are generally inferior to those in low-dimensional subspaces selected by graph-based data representation methods. Based on the results, we arrived at the conclusion that NMF methods based on manifold assumption, such as GrNMF and MultiGrNMF, perform better than the original NMF. The reason is that, after graph regularization, discriminative information is contained by coding vectors embedded in an R-dimensional subspace. Classification result of GrNMF using only one graph is already competitive. We attribute this result to the manifold distribution property of fluorescent images. However, compared with the results of GrNMF shown in Fig. <ref type="figure">2</ref>(a), one can see that the recognition performance of our proposed MultiGrNMF is generally superior in almost all R-dimension reduced subspaces than that of GrNMF. This finding provides strong evidence that using multiple graphs is more robust than using a single graph for NMF regularization. stable with respect to regularization parameter α. However, based on empirical observations, a choice of α values between 8 and 128 is recommended. Fig. <ref type="figure">2(b)</ref> shows that, in the case of α ¼ 32 and 128, MultiGrNMF significantly improves its performance in all classes; however, its performance decreases when α becomes as large as 512. A probable reason for this finding is that when α is too small, the effect of graph regularization is not enough for the matrix factorization procedure, whereas when it is too large, the matrix factorization procedure will be overfitted to the graph. As such, choosing an α value in the middle range, such as 128 for MultiGrNMF, is recommended. However, we can see from this figure that both MultiGrNMF and GrNMF are robust to the α parameter to some extent. Overall, MultiGrNMF performs comparably with GrNMF and significantly better than NMF. GrNMF outperforms NMF significantly on five out of six cases. The poor performance of GrNMF on α ¼ 512 may be ascribed to the graph used by GrNMF. GrNMF uses a kernel-based graph, which may not be as suitable for protein subcellular-location tasks as a linear kernel. From our experiments, we see that MultiGrNMF is a viable alternative to GrNMF-based data representation approaches.</p><p>To compare our method with state-of-the-art NMF-based data representation methods, we match the classification performance of six NMF-based data representation methods with identical initializations: MultiGrNMF MultiGrNMF. GrNMF <ref type="bibr" target="#b10">[11]</ref>. Multiple kernel NMF (MK-NMF) <ref type="bibr" target="#b17">[18]</ref>. MD-NMF <ref type="bibr" target="#b7">[8]</ref>. Sparse NMF <ref type="bibr" target="#b18">[19]</ref>.</p><p>Local NMF <ref type="bibr" target="#b19">[20]</ref>. Discriminant NMF (DNMF) <ref type="bibr" target="#b20">[21]</ref>. Original NMF. Principal component analysis.</p><p>.</p><p>For MultiGrNMF, multiple graph regularization is performed to coding vectors in W, whereas for MK-NMF, multiple kernel functions are performed to data vectors X and basis vectors H. Results are given in Fig. <ref type="figure">3(a)</ref>. Moreover, we are also interested in determining whether MultiGrNMF outperforms GrNMG using the best graph selected by MultiGrNMF (denoted as GrNMF n in Fig. <ref type="figure">3</ref>). An experimental comparison is also conducted and the result of GrNMF n is shown in Fig. <ref type="figure">3</ref>. Experiments on classification are insufficient in demonstrating the superiority of the novel method. Previous studies have shown that NMF methods demonstrate favorable clustering results in many applications. As such, we also proposed clustering performance of the methods, as presented in Fig. <ref type="figure">3(b)</ref>.</p><p>From Fig. <ref type="figure">3</ref>, we can see the following observations and conclusions:</p><p>1. MultiGrNMF outperforms NMF and most of its sparse versions because of the use of multiple graphs, except for MD-NMF. Not surprisingly, MD-NMF exhibits better performance than Mul-tiGrNMF because MD-NMF is a supervised method that uses class label information, whereas MultiGrNMF is an unsupervised method. Interestingly, the performance of DNMF, which is also a supervised algorithm, is per, which means it does not use class information as effectively as MD-NMF. 2. Using the same linear combination of multiple Gaussian kernels as multiple graph and multiple kernel strategies, MultiGrNMF is much better than MK-NMF because of two possible explanations. (a) First, formulating the objective function by regularizing coding vectors directly is more effective compared with applying multiple kernels to the original data space and the basis vectors. A possible reason for this result is that when original data vectors and basis vectors are mapped in a non-linear space via multiple kernels, final coding vectors are still determined by matrix factorization procedure, which cannot guarantee that coding vectors lies on a proper manifold. By contrast, when a multiple graph is used to regularize coding vectors directly, the manifold assumption is directly implemented by multiple graph regularization. (b) Second, the regularizer proposed in this study can more effectively exploit the intrinsic manifold structure of data space by applying a l 2 norm regularization to linear combination coefficients of initial graphs. The explanation for this finding is that this term can prevent combination coefficients from overfitting in one graph. 3. MultiGrNMF achieves the best result, which further demonstrates that an algorithm based on ensemble manifold regularization outperforms algorithms based on a single manifold, which are widely used in many existing data representation and classification methods. 4. The single-graph method that uses the best graph selected by MultiGrNMF does not outperform MultiGrNMF, but is comparable to MultiGrNMF. The possible reason for this outcome is that MultiGrNMF prevents the algorithm from overfitting to a single graph by introducing the l 2 norm to graph weights.</p><p>Lastly, proving the convergence of the proposed algorithm is difficult. As an alternative, we plot the convergence curve in Fig. <ref type="figure">4</ref> to show the convergence of the proposed algorithm.</p><p>As shown in Fig. <ref type="figure">4</ref>, the objective value appears to be stable after approximately 100 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Experiment II: Diagnosis of AD using positron emission tomography (PET) images</head><p>In this section, we will test the proposed MultiGrNMF as a PET image representation method for application in AD diagnosis <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Data set and setup</head><p>A PET data set is selected from the AD Neuroimaging Initiative (ADNI) database <ref type="bibr" target="#b22">[23]</ref> to validating the computer-aided design (CAD) tool using MultiGrNMF as data representation method. The main purpose of ADNI is to measure the progression of AD during its initial stages <ref type="bibr" target="#b22">[23]</ref>. A total of 800 participants from the United States and Canada were recruited to develop this database, including approximately 200 normal participants without symptoms, about 400 subjects with cognitive impairment, and 200 patients with early AD symptoms. We have selected PET data from 340 ADNI participants to build our PET340 data set. The participants in the PET340 data set are classified into two groups: 168 AD patients and 172 normal control subjects.</p><p>The voxels of a brain functional three-dimensional PET image will be organized as data vector x, and further represented into coding vector w by MultiGrNMF. Coding vectors will be used as feature vectors of an SVM classifier to separate AD patients (positive subjects) from normal subjects (negative subjects). These classification results are evaluated using the leave-one-out crossvalidation strategy. To evaluate the developed MultiGrNMF-based CAD tool, receiver operating characteristic (ROC) curve and recallprecision curve, are obtained. At the same time, the area under the curve (AUC) of an ROC curve is also computed as single measure of classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Results</head><p>Fig. <ref type="figure">5</ref>(a) shows the ROC curve of a true positive rate versus a false positive rate by using SVM as the classification algorithm. Each curve in the figure represents a PET image representation algorithm based on NMF. As can be seen from Fig. <ref type="figure">5(a)</ref>, our proposed MultiGrNMF algorithm performs the best, whereas GrNMF outperforms NMF. As decision threshold decreases, Mul-tiGrNMF performs slightly better than GrNMF and NMF. Mul-tiGrNMF and GrNMF perform comparably with each other when decision thresholds are very large or very small. NMF performs the worst for all cases. Fig. <ref type="figure">6</ref> shows the AUC for each algorithm. Our MultiGrNMF algorithm yields the highest AUC. positive subjects exist. Therefore, all the algorithms yield low precision rates in this case. As recall value increases, precision rates of all algorithms decrease. However, the precision rate of MultiGrNMF tends to converge when the recall value is smaller than 0.95, whereas GrNMF and NMF can consistently benefit from a smaller recall value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion and future work</head><p>This paper introduced a new data representation method by improving GrNMF <ref type="bibr" target="#b10">[11]</ref>. The method depends on a graph constructed by a linear combination of several initial graphs with different models and parameters. The main idea behind the model is that the intrinsic manifold can be approximated by multiple nearest-neighbor graphs. A unified object function framework is proposed to derive the MultiGrNMF form in terms of NMF loss function and multiple graph regularization. The resulting coding matrix between nodes is a data representation technique regularized by multiple graphs. Graph weights can also be computed efficiently based on the derived coding matrix. Two data classification experiments show that MultiGrNMF performs well compared with other NMF-based data representation methods.</p><p>Other optimization methods can also be used with our proposed algorithm instead of multiplicative update rules, such as the optimal gradient method proposed in <ref type="bibr" target="#b2">[3]</ref> by Guan et al. However, for a fair comparison, we have used the same multiplicative update rule of GrNMF to demonstrate the advantage of multiple graph regularization. In our future work, we will reconsider optimization algorithms by using more efficient algorithms. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 (Fig. 1 .</head><label>21</label><figDesc>Fig. 1. Sample images from the 2D HeLa image data set. (a) ActinFilaments, (b) endosome and (c) microtubules.</figDesc><graphic coords="4,81.12,462.08,423.36,270.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Classification accuracies of MultiGrNMF, GrNMF, and NMF versus parameters R and α. (a) Accuracies v.s. R and (b) accuracies v.s. α.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 (Fig. 4 .</head><label>54</label><figDesc>Fig. 4. Convergence curve of MultiGrNMF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. (a) ROC curve and (b) recall-precision curve of MultiGrNMF, GrNMF, and NMF on ADNI data set.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>J.J.-Y Wang et al. / Pattern Recognition 46 (2013) 2840-2847</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The study was supported by grants from 2011 Qatar Annual Research Forum Award (Grant No. ARF2011) and King Abdullah University of Science and Technology (KAUST), Saudi Arabia.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest statement</head><p>None declared.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonnegative matrix factorization with Earth mover&apos;s distance metric for image analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1590" to="1602" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inexact block coordinate descent methods with application to the nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bonettini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IMA Journal of Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1431" to="1452" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NeNMF: an optimal gradient method for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2882" to="2898" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generalized alpha-beta divergences and their application to robust nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cichocki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cruces</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold regularized discriminative nonnegative matrix factorization with fast gradient descent</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="170" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization as a feature selection tool for maximum margin classifiers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Das</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast coordinate descent methods with variable selection for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</title>
		<meeting>the 17th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1064" to="1072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-negative patch alignment framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1218" to="1230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Itakura-saito nonnegative matrix factorization with group sparsity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lefevre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An efficient initialization method for nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Boostani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rezaei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph regularized nonnegative matrix factorization for data representation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.231</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2010.231" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1548" to="1560" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble manifold regularization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.57</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2012.57" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1227" to="1233" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shape analysis of elastic curves in euclidean spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Jermyn</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.184</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2010.184" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Algorithms for non-negative matrix factorization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="556" to="562" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving a Lagrangian decomposition for the unconstrained binary quadratic programming problem</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Mauri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorena</forename></persName>
		</author>
		<idno type="DOI">10.1016/j.cor.2011.09.008</idno>
		<ptr target="http://dx.doi.org/10.1016/j.cor.2011.09.008" />
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1577" to="1581" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Group coordinate descent algorithms for nonconvex penalized regression</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csda.2011.08.007</idno>
		<ptr target="http://dx.doi.org/10.1016/j.csda.2011.08.007" />
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="316" to="326" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Boosting accuracy of automated classification of fluorescence microscope images for location proteomics</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="DOI">10.1186/1471-2105-5-78</idno>
		<ptr target="http://dx.doi.org/10.1186/1471-2105-5-78" />
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multiple kernel nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1976" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using under approximations for sparse nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gillis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Glineur</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2009.11.013</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2009.11.013" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1676" to="1687" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning spatially localized, parts-based representation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Visual and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Visual and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Buciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="683" to="695" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NMF-SVM based CAD tool applied to functional brain images for the diagnosis of Alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">P</forename><surname>Padilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gorriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salas-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Alvarez</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2011.2167628</idno>
		<ptr target="http://dx.doi.org/10.1109/TMI.2011.2167628" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large scale multivariate parallel ICA method reveals novel imaginggenetic relationships for Alzheimer&apos;s disease in the ADNI cohort</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Meda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I</forename><surname>Perrone-Bizzozero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Calhoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Glahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Risacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Saykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Pearlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D N</forename><surname>Initia</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.Neuroimage.2011.12.076</idno>
		<ptr target="http://dx.doi.org/10.1016/j.Neuroimage.2011.12.076" />
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1608" to="1621" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Currently, he is a postdoctoral fellow at the Computer, Electrical and Mathematical Sciences and Engineering Division</title>
		<author>
			<persName><forename type="first">Jim Jing-Yan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests are machine learning, data mining, bioinformatics, and biometrics</title>
		<meeting><address><addrLine>China; Saudi Arabia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2102">2102</date>
		</imprint>
		<respStmt>
			<orgName>Graduate University of Chinese Academy of Sciences ; King Abdullah University of Science and Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">After winning the prestigious French Educational Award, she joined the University of Washington, Seattle, as a visiting scientist. After a short period at the Fred Hutchinson Cancer Research Center, she joined the University of Social and Behavioral Sciences of Leiden as a postdoc. She was appointed the assistant professor position at the University of Tennessee in 2000, was tenured, and promoted to Associate Professor in 2005. She joined the Eastern Virginia Medical School as an Associate Professor of Biostatistics and Bioinformatics in 2006. Currently, she is a senior scientist at the Qatar Computing Research Institute, Qatar where she is leading the Bioinformatics and scientific computing center</title>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing Journal</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Halima Bensmail obtained her Ph</publisher>
			<pubPlace>Paris, France</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Statistics and Mathematics in Pierre &amp; Marie Curie (Paris IV) University and National institute of Automatics and informatics (INRIA)</orgName>
		</respStmt>
	</monogr>
	<note>Biomedicine and Biotechnology</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">He worked as a Lane Fellow in Lane Center for Computational Biology</title>
	</analytic>
	<monogr>
		<title level="m">Currently, he is an assistant professor at the Computer, Electrical and Mathematical Sciences and Engineering Division</title>
		<title level="s">Saudi Arabia. His research interests are bioinformatics and computational biology</title>
		<meeting><address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004. 2009. 2009 to 2010</date>
		</imprint>
		<respStmt>
			<orgName>Xin Gao received the BS degree from Computer Science and Technology Department, Tsinghua University ; Cheriton School of Computer Science, University of Waterloo, Canada ; Carnegie Mellon University, US ; King Abdullah University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note>and the PhD degree from David R</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
