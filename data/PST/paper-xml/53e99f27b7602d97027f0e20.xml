<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Filtering and Clustering Methods for Temporal Video Segmentation and Visual Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">Mu</forename><surname>¨fit Ferman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and Center for Electronic Imaging Systems</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Murat Tekalp</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering and Center for Electronic Imaging Systems</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<settlement>Rochester</settlement>
									<region>New York</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Filtering and Clustering Methods for Temporal Video Segmentation and Visual Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F2213493715A4F340D71B01E6B722DE5</idno>
					<note type="submission">Received February 26, 1998; accepted September 28, 1998</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>objective reference for users and applications alike in terms Automatic temporal segmentation and visual summary gen-of absolute frame numbers or time codes. The lack of such eration methods that require minimal user interaction are key well-defined generic segments is one of the reasons higher requirements in video information management systems. Cluslevel content representations (e.g., in terms of scenes of tering presents an ideal method for achieving these goals, as sequences) are difficult to obtain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>it allows direct integration of multiple information sources.</head><p>Identifying the shot boundaries within a video sequence This paper proposes a clustering-based framework to achieve expedites random nonlinear access; however, the user must these tasks automatically and with a minimum of user-defined still view a shot in its entirety to access its visual content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>parameters. The use of multiple frame difference features and</head><p>In other words, simple temporal segmentation provides short-time techniques are presented for efficient detection of only the backward/forward shuttling capability, but it does cut-type shot boundaries. Generic temporal filtering methods not enable browsing. A compact representation of the viare used to process the signals used in shot boundary detection, resulting in better suppression of false alarms. Clustering is sual content for each shot must be provided to reduce also extended to the key frame extraction problem: Color-based storage requirements and to introduce true browsing funcshot representations are provided by average and intersection tionality. Perhaps the simplest way to describe shot content histograms, which are then used in a clustering scheme to is by textual keywords. While text-based descriptions of identify reference key frames within each slot. The technique content can relay a great deal of information, their generaachieves good compaction with a minimum number of visually tion requires intense human interaction. Furthermore, connonredundant key frames.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>tions, possibly in the form of static frames selected from each shot, provide a better alternative for compaction by Advances in communications, multimedia, and comexploiting the associative ability of the user. Construction puter technologies have made video accessible to profesof such visual summaries can be carried out automatically sionals and everyday users alike. The ubiquity and volume and will often lead to more consistent depictions of shot of video data have led to the emergence of large-scale content [2, 3]. video information management systems which offer such Temporal segmentation and visual summarization of features as browsing, visual compaction, and query provideo streams have been active research areas since the cessing. A significant design concern in these systems is introduction of automatic video information management the level(s) at which access to video content is provided for systems. The reader is referred to <ref type="bibr">[1,</ref><ref type="bibr">4]</ref> for up-to-date the user. At the most basic level, a video stream comprises reviews of the various available methods. This paper premultiple frames. Carrying out content analysis of video sents a clustering-based framework for both unsupervised frames individually, however, leads to high computational shot boundary detection and key frame extraction. Earlier overhead. More importantly, such an approach neglects versions of individual components of this framework have the time-varying nature of video and discards significant been discussed in <ref type="bibr">[5]</ref><ref type="bibr">[6]</ref><ref type="bibr" target="#b1">[7]</ref>. The segmentation method involves amounts of valuable semantic information. A more approcomputing a difference measure for each frame pair in priate mode of representation for video is in terms of the the video stream and then utilizes unsupervised k-means shot which has been used widely in video indexing applications <ref type="bibr">[1]</ref>. Shot-based video representation offers a generic, clustering with k ϭ 2 to label each frame as a shot boundary or an intrashot frame. The algorithm has been shown to 2.2. Filtering Methods for Improved Temporal Segmentation perform well for detecting cut-type shot boundaries. In this paper, the use of multiple features and short-time</p><p>The majority of temporal segmentation methods rely on clustering techniques are presented to significantly iminterframe difference measures to identify shot boundaries prove segmentation performance. Temporal filtering in a video sequence. Frame differences can be defined in methods, inspired by various image processing techniques, terms of pixel values, histograms, motion vectors, frame are proposed to enhance the frame difference signals used statistics, or any other appropriate feature(s) in the raw or in shot boundary detection. Clustering is used with multiple compressed video domains <ref type="bibr">[1,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b4">10]</ref>. The main assumption color-based shot representations for automatic extraction here is that within a single shot, interframe variations take of key frames and provides a powerful, unsupervised on low values, resulting in a slowly varying signal. Across method.</p><p>a shot boundary, however, an abrupt change in global The rest of the paper is organized as follows. Section 2 frame characteristics occurs, and a sharp peak is observed. presents various processing techniques for efficient These peaks can then be detected by thresholding or clusclustering-based temporal segmentation, including tempotering the difference signal. The selected features, and their ral filtering, the use of multiple features, and short-time robustness to the detrimental effects of camera and object methods. Automatic key frame selection using shot repremotion, as well as noise, strongly influence the segmentasentative histograms and clustering is discussed in Section tion performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A comprehensive set of experimental results and obser-</head><p>The level of temporal activity generally varies throughvations are presented in Section 4. Section 5 is dedicated out a video sequence, due to the effects of camera and to concluding remarks and future directions.</p><p>object motion and sudden changes in intensity. Strong interframe activity within a shot can generate spurious peaks,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">TEMPORAL SEGMENTATION</head><p>which in turn cause thresholding-or clustering-based methods to produce false alarms. One way to eliminate these A consistent evaluation criterion is crucial in temporal false alarms is to reduce the spatial resolution of the input segmentation applications for appropriate assessment of sequence <ref type="bibr" target="#b5">[11,</ref><ref type="bibr" target="#b6">12]</ref>, thereby limiting the effects of noise and system performance. In the following section we briefly motion. Block-based approaches <ref type="bibr" target="#b7">[13]</ref>, where each input discuss the evaluation criteria we have employed in our frame is subdivided into blocks of equal size and correexperiments before introducing out segmentation sponding blocks are compared, aim to suppress object momethods.</p><p>tion and local variations in color and intensity. False detections may also be reduced by assuming a model for the 2.1. Performance Evaluation distribution of shot boundaries in the video stream and A good temporal segmentation method should minimize discarding any candidates that fail to match this model. the number of false detections while maximizing the num-Various density functions have been suggested to model ber of correctly identified shot boundaries. It is possible shot durations, among them Poisson <ref type="bibr" target="#b8">[14]</ref> and lognormal to quantitatively express these qualities in terms of two <ref type="bibr" target="#b9">[15]</ref>. A simpler variation of this approach involves imposparameters, recall and precision. Originally introduced to ing a hard limit on the temporal frequency of scene changes assess the performance of information retrieval systems <ref type="bibr" target="#b5">[11]</ref>; any candidate shot boundary that violates this limit <ref type="bibr" target="#b2">[8]</ref>, recall and precision have also been used, perhaps for is rejected. Such modeling is highly dependent on the charlack of better alternatives, as evaluation criteria for shot acteristics of the input sequence, however, and the risk boundary detection methods <ref type="bibr" target="#b3">[9]</ref>. In a retrieval situation, of discarding true boundaries while preserving false ones recall reflects the proportion of relevant material that is is considerable. retrieved, whereas precision is a measure of how relevant An alternative to these approaches is to process the the retrieved or selected information is: ference signal is analogous to the first-order pixel differ-ences generated in edge detection applications. The peaks filtering operations alter the signal statistics and influence the distribution of the samples in the feature space. Apart that identify shot boundaries within the frame difference signal constitute its high frequency components, just like from reducing the dynamic range of the signal, the zeroclipping operation reduces the variance of the signal; after the gray level discontinuities that form the edges in an image. It is therefore possible to augment the performance all of the negative values in d [n] are removed the majority of the samples in the output signal g <ref type="bibr">[n]</ref> have values equal of a temporal segmentation method by adapting similar appropriate techniques to temporal filtering.</p><p>or close to zero. This leads to a significant increase in the number of members of the nonboundary cluster and causes Perhaps the simplest such temporal filtering technique is an extension of unsharp masking <ref type="bibr" target="#b10">[16]</ref>, which can be a shift in the cluster centroid towards the origin. As a result, the separation of the two clusters in the feature defined for the one-dimensional frame difference signal f [n] as space is increased. While no clear boundary exists between the two clusters in (a) (whose centers are denoted by the triangles) prior to processing; after filtering, they are dis-</p><formula xml:id="formula_0">d [n] ϭ af [n] Ϫ bf ˜[n];</formula><p>tinctly disjoint in (b) and (c). Further improvement in the suppression of false alarms</p><formula xml:id="formula_1">g[n] ϭ ͭ d [n], d [n] Ͼ 0, 0, otherwise,<label>(2)</label></formula><p>can be achieved by using a combined filtering scheme.</p><p>After enhancement of shot boundaries by either Eq. (2) or Eq. ( <ref type="formula" target="#formula_4">4</ref>), the resulting signal can be filtered again, this where a and b are scalar coefficients, with a Ն b, and f ˜ <ref type="bibr">[n]</ref> represents the frame difference signal after lowpass time using the alternate method. The effects of the two possible ways-mean followed by median, and median filtering. The output signal g[n] is obtained by clipping all negative values in d [n] to zero.</p><p>followed by mean-are depicted in Figs. <ref type="figure" target="#fig_3">4</ref> and<ref type="figure">5</ref>, respectively. It is observed in both Fig. <ref type="figure" target="#fig_0">1</ref> and Fig. <ref type="figure">3b</ref> that mean Alternatively, f [n] can be viewed as a slowly varying signal, h[n]-interframe differences due to intrashot activ-filtering achieves better suppression of intrashot activity and cluster separation than median filtering. When median ity, digitization, and equipment noise, etc. <ref type="bibr" target="#b5">[11]</ref>-corrupted by impulsive noise impulsive , i.e., the sharp discontinuities filtering is applied after mean filtering, signal values are not attenuated any further at the boundary locations (Fig. at shot boundaries: 4); however, fluctuations in intrashot regions are reduced again, and cluster separation is even more enhanced (Fig.</p><formula xml:id="formula_2">f [n] ϭ h[n] ϩ impulsive .</formula><p>(3) 3d). In the case of [median, mean] ordering (Fig. <ref type="figure">5</ref>), signal values at boundaries may be attenuated more than desired, This signal model motivates the use of median filtering for extracting impulsive from f [n]. Median filtering <ref type="bibr" target="#b10">[16]</ref> is leading to an increased number of misses. Our experiments have also confirmed that with the [mean, median] com-a popular technique for eliminating the impulsive noise in an input image without sacrificing edge resolution. Sub-bined filtering method, it is possible to generate fewer false alarms while attaining the same detection performance. tracting from the frame difference signal a median filtered version of itself yields ˆimpulsive , an estimate of the noise A parameter that has a direct consequence on detection performance is filter duration. Longer filter durations may signal, which is dominated by the samples at shot boundaries:</p><p>degrade both recall and precision, whereas shorter filter lengths are expected to produce significantly better results. This is due to the fact that longer filters (both mean and</p><formula xml:id="formula_3">d [n] ϭ ˆimpulsive ϭ f [n] Ϫ F median ͕ f [n]͖;</formula><p>median) cannot achieve as significant a reduction in the sample values in high activity temporal regions as shorter</p><formula xml:id="formula_4">g[n] ϭ ͭ d [n], d [n] Ͼ 0, 0, otherwise.<label>(4)</label></formula><p>filters can. Using a single filter size for the entire signal, however, may not yield optimum results, since signal characteristics vary over time. Filter size can be adaptively This operation is in fact analogous to using median filtering to obtain f ˜ <ref type="bibr">[n]</ref> in Eq. (2). defined and modified as a function of local variance, much like adaptive noise filtering techniques. In our implementa-Figures <ref type="figure" target="#fig_0">1</ref> and<ref type="figure" target="#fig_1">2</ref> depict the results of the filtering process using mean and median filters, respectively. It is observed tion, selection of the appropriate filter size is carried out in the following way: At each sample, the local variance that the processed signals are quite similar, with both filtering methods removing the fluctuations in intrashot regions is computed in N noncausal windows of duration L i , i ϭ 1, . . . , N. For mean filtering, the window size L k with the and attenuating the signal values slightly at the shot boundaries. This attenuation is less evident in median filtering, largest sample variance is the filter duration used for the given sample; for median filtering, it is L m , the window due to its edge preserving nature. The effects of filtering on the features themselves and the clustering results are size that yields the smallest variance, that is selected. Using the window with the largest sample variance for mean better observed in the 2D scatter plot shown in Fig. <ref type="figure">3</ref>. The when this criterion is used. The method requires that the set Histogram-based comparison methods are highly faof possible filter durations be defined prior to processing; vored in temporal segmentation applications, as they are however, the need to define variance-related parameters or robust to adverse effects of camera and object motion, thresholds is thus avoided. N need not be a large number; in changes in scale, and rotation. However, such methods our experiments, it has been set to 5, with respective filter occasionally fail to identify transitions between shots with sizes of 7, 11, 15, 21, and 31. similar color content or intensity distribution. Pairwise comparison methods, on the other hand, consider differ-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multiple Features for Temporal Segmentation</head><p>ences between corresponding pixels in adjacent frames and, hence, are very susceptible to movement of objects A significant advantage of clustering-based temporal or the camera. Using these two features simultaneously in a segmentation is that multiple features can be used simultaneously to improve detection performance. The selected clustering scheme without preprocessing generates a large number of false alarms, due to the sensitivity of pixel differ-the spurious maxima have been eliminated. A more explicit portrayal of the positive effects of filtering on detection ences <ref type="bibr" target="#b11">[17]</ref>. This is illustrated in Fig. <ref type="figure">6</ref>, where both features performance is given in the scatter plots in Fig. <ref type="figure">3</ref>. (histogram difference and pairwise comparison) are plotted on the same set of axes. Both features register high 2.4. Short-Time Methods for Temporal Segmentation values at shot boundaries (marked by the arrows), but the extreme sensitivity of pixel differences to object and As demonstrated in the previous sections, the clustering method, coupled with the prefiltering process, provides a camera movement generates a large number of false positives (note the interval [1027, 1082]). Consequently, even powerful tool for temporally partitioning generic video input. However, the algorithm operates at a global level though the majority of shot boundaries are correctly identified, the number of false positives is unacceptably high and and, as such, may be biased by a few uncharacteristically high sample values in high-action scenes, causing some renders the results unreliable.</p><p>When unsharp masking is applied to each of the signals shot boundaries even in temporally distant locations to be missed. Furthermore, the entire video stream needs to be prior to clustering, one finds that the filtered features supplement one another, and considerable improvements are parsed before clustering can be performed, which proves limiting for long video sequences. A technique that oper-registered in the recall figures. Figure <ref type="figure">6b</ref>   considers a group of temporally proximate frames rather short-time temporal segmentation method can be summarized as follows: than the entire stream and one that can generate shot boundaries on the fly while the input stream is being 1. Select rejection threshold T R ; parsed, would prove more effective.</p><p>2. Select interval [t 1 , t 2 ]; One way to achieve these goals is to process the input 3. Perform 2-class clustering on the sample set in sestream in temporal blocks and perform shot boundary delected interval; tection independently on each block. Since 2-class cluster-4. Compute cluster means Ȑ 1 , Ȑ 2 ; ing automatically partitions a given data set into two classes, certain criteria that check for the presence of shot 5. If ͉Ȑ 1 Ϫ Ȑ 2 ͉ Ͻ T R boundaries within the selected interval need to be intro-(a) Discard candidate shot boundaries in [t 1 , t 2 ], duced <ref type="bibr" target="#b11">[17]</ref>. Otherwise, clustering in an arbitrarily chosen (b) Define new interval [t 1 , t 3 ], t 3 Ͼ t 2 ; temporal window without any constraints may lead to a rally sampled version of the input stream is used to com-The selection of the rejection threshold T R is seemingly pute frame differences initially). This ''coarse'' signal is a data-dependent task; however, since the prefiltering step then clustered into two classes to yield a set of temporal does a very effective job of separating the boundaries from regions of various size that are likely to contain multiple intrashot variations (see Fig. <ref type="figure">3</ref>), T R can be set to a universal shot boundaries. Both methods are illustrated in Figs. 7a value for all types of input data. The existence of such a and b, respectively. After the initial temporal intervals are parameter as T R also gives the user control over detection identified, shot boundary detection is carried out within sensitivity; by adjusting the rejection threshold the number each by clustering the filtered frame difference signal g <ref type="bibr">[n]</ref>. of missed shot boundaries can be minimized-at the ex-Each temporal region is then tested and, if required, pense, obviously, of an increased number of false positives. merged with neighboring regions using the cluster dis-Two different methods were developed to select the tance test. appropriate initial temporal intervals. The first one in-A significant difference between the two methods is that, volves processing the input signal in nonoverlapping segin the latter approach, the entire video stream is required in order to obtain the coarse-level signal, which in turn ments of equal temporal duration L w . The other is a two-implies that detection results can be presented to the user only after a certain time delay. Depending on the input format, however, this coarse signal can be computed very quickly (e.g., for MPEG sequences, which provide random access functionality), and hence, the delay is minimal. Once the coarse signal is available, detection of shot boundaries is performed on the fly as the video stream is processed frame by frame.</p><p>A complete block diagram of the short-time temporal segmentation system is shown in Fig. <ref type="figure">8</ref>. Based on the desired method, one of blocks A or B is used to identify the initial temporal intervals for processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">KEY FRAME SELECTION BY CLUSTERING</head><p>Representation of individual shot content has generally been achieved by selecting reference or key frames from each shot. Such methods have been inspired by motion picture production <ref type="bibr" target="#b12">[18]</ref>, where a series of sketches, each one corresponding to a particular shot or phase of a shot, are drawn and arranged in the rough order they will appear in the finished film. Key frame-based visual summaries are static renderings of evolving content and rely on the viewer's instinct to deduce an event in its entirety from a few snapshots. Even though alternative summarization techniques such as video posters <ref type="bibr" target="#b6">[12]</ref> and mosaic representations <ref type="bibr" target="#b13">[19]</ref> have been suggested, key frame-based methods remain the most popular and natural visual summarization tools for developers and users.</p><p>Key frame selection can be as simple as picking from every shot one or more frames in prescribed locations <ref type="bibr" target="#b14">[20]</ref>. To provide a more faithful representation of a shot, methods that take as selection criteria the rate of change in content are required. One such method is to select the first frame in each shot as a key frame and to progressively compare subsequent frames against it using color and/or motion-based criteria [2]. A similar method is presented in <ref type="bibr" target="#b15">[21]</ref>, where accumulated frame differences are used as a measure of the content variation within a shot. Based on this measure the appropriate number of key frames for each shot is automatically determined from the total allocated to the entire sequence, and representative frames are selected using a minimization process.</p><p>Clustering has previously been utilized for identifying similar scenes or shots <ref type="bibr" target="#b16">[22]</ref> and generating multilevel, hierarchical representations of video sequences <ref type="bibr" target="#b17">[23]</ref>. We further extend its application to key frame selection, since it provides a powerful tool for fully or semi-automated applications. The following section is dedicated to the discussion of a clustering-based technique for automatic key frame selection. key frame selection algorithms. Overall, performance is evaluated by the temporal compaction achieved (i.e., the number of frames in the sequence vs the number of frames used to represent them), and the relative redundancy of the selected frames. The latter is performed by a human operator and is clearly a subjective, qualitative measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Automatic Key Frame Extraction by Clustering</head><p>A major requirement for automatic key frame extraction is to define a content representation that captures the common aspects or characteristics of the shot. Once such a representation is obtained, the frames in the shot can be compared against it to determine how much each one varies from this core or essence. Frame differences provide sufficient clues for detecting rapid content change across shot boundaries, but they are not appropriate for key frame extraction applications.</p><p>The histogram provides a compact and reliable representation of frame color characteristics. In order to quantitatively describe the color content of a shot, we define and utilize the average and intersection histograms for a shot. The average histogram <ref type="bibr" target="#b17">[23,</ref><ref type="bibr">6]</ref> is computed by accumulating the individual frame histograms and then normalizing each bin by the number of frames M S in shot S:</p><formula xml:id="formula_5">H S avg (i) ϭ 1 M S M S Ϫ1 jϭ0 H S j (i), i ϭ 0, . . . , L Ϫ 1. (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>Here L denotes the number of bins in the histograms. The intersection histogram [6], on the other hand, is the color histogram obtained by ''intersecting'' <ref type="bibr" target="#b18">[24]</ref> the histograms of all the frames in shot S:</p><formula xml:id="formula_7">H S int (i) ϭ min͕H j (i)͖, i ϭ 0, . . . , L Ϫ 1, j ʦ [b S , e S ]; (6)</formula><p>b S and e S denote the first and last frames in shot S, respectively. H S avg is a good representative of the color content of the entire shot, while H S int provides the common color characteristics of the constituent frames. The two histograms together provide a faithful portrayal of the collective color content of a shot. Cases may occur where both histograms fail to reflect the actual color characteristics of the shot. In particular, the intersection histogram may be biased unfavorably by a frame with low luminance variance, or when a large object occludes the camera, while sudden changes in luminance may result in an average histogram that is misleading. Such adverse effects can be eliminated by comparing Ȑ (S) Lum (i), i ϭ 1, . . . , M S , the average luminance of each frame i in shot S, against the mean luminance of shot S, Ȑ (S)  Lum . If</p><formula xml:id="formula_8">Ȑ (S) Lum (k), k ʦ [1, M S ] deviates significantly FIG. 8. Block diagram of the clustering-based temporal segmen-from Ȑ (S)</formula><p>Lum , then frame k is not taken into account in the tation method. The system successfully integrates filtering methods, computation of the average and intersection histograms, multiple features, and short-time clustering techniques to attain high but it is directly labeled as a key frame for shot S.</p><p>recall and precision performance. Only one of the subblocks A or B is selected and used for when short-time clustering is desired.</p><p>Once H S avg and H S int are determined, each frame j in the shot is compared against them separately using the modified 2 test, 2</p><formula xml:id="formula_9">S ( j) ϭ LϪ1 iϭ0 [A и H j (i) Ϫ B и H (avg͉int) (i)] 2 H j (i) ϩ H (avg͉int) (i) ,<label>(7)</label></formula><p>where</p><formula xml:id="formula_10">A ϭ Ί H (avg͉int) H j , B ϭ 1 A , (<label>8</label></formula><formula xml:id="formula_11">)</formula><formula xml:id="formula_12">H (avg͉int) ϭ i H (avg͉int) (i), H j ϭ j H j (i)</formula><p>These operations yield an M S ϫ 2 feature matrix for the shot, whose members are then clustered into k classes with k Յ M S for all shots S. Once the initial k clusters are obtained, further analysis is required to eliminate any redundancies, clusters that contain very few elements, or those that are too close to others. Clusters are compared in pairs; for each cluster c i , the closest cluster c j , i ϶ j, is determined by comparison of the cluster centers. If the centers of c i and c j are closer than a merging threshold T C , they are combined and the center of this new cluster is computed. Cluster comparison is performed until (i) all clusters are merged into a single cluster, or (ii) no cluster pairs closer than T C exist. This operation yields q clusters with q Յ k.</p><p>The selection of key frames from within each cluster can be performed in various ways. As a faithful representative of cluster content, the sample closest to the cluster center can be selected. In cases where intercluster variance is large, the sample farthest from the cluster center may differ significantly from the closest and, hence, possess a high level of information. Both the closest and the farthest samples are selected from the cluster if the distance between the two exceeds T C . In the end, the shot is represented by the frame(s) corresponding to the sample(s) picked from each cluster. The number of selected key frames for the entire video sequence with Q shots can take on values in the range [Q, 2 ϫ k ϫ Q]. This allows the user to generate visual summaries at multiple levels of detail by changing k, T C , or both. A block diagram of the complete key frame extraction system is shown in Fig. <ref type="figure" target="#fig_6">9</ref>.</p><p>An important parameter in the performance of the key frame selection algorithm is the color space used. The RGB space is the most widely used color format in everyday applications, yet it does not properly reflect the properties of the human visual system. The YCrCb color space provides a better alternative to the RGB system, especially than the chrominance channels. It is also the color format adopted in the compression standards M-JPEG and MPEG and were then compressed in MPEG-1 format at 30 fps. was performed prior to clustering.</p><p>The entire data set consisted of 10 sequences, each ranging from 30 s to 4 min in duration for a total of ȁ25 min of video data. The test sequences comprised of commercials, clips from the motion picture Days of Thunder, and the clustering of histogram differences [5] yields fairly good TV drama NYPD Blue. The motion picture sequences detection performance, with over 91% of all shot boundcontained high-speed car race scenes, as well as dialogues. aries detected. The recall for pairwise comparison is even NYPD Blue sequences, on the other hand, were shot in better. However, in both cases, the user has to weed out cine ´ma verite ´style, with fast pans, unsteady camera movean excessive number of false alarms to obtain the actual ments, and extensive object activity. All sequences inshot boundaries; for pairwise comparison, more than 10% cluded gradual transitions (fades and dissolves), along with of all the frames in the test set are candidate shot boundcuts. Each input frame was spatially subsampled by a factor aries! If the signals are filtered prior to clustering, the of 2 in the x-and y-directions prior to processing in order number of false positives are drastically reduced, although to reduce the computational overhead per frame, yielding some compromise in detection performance is registered an effective frame size of 176 ϫ 120. Color histograms for the histogram difference metric. Pairwise comparison, were generated in YCrCb space and were obtained by on the other hand, yields excellent detection performance concatenation of the individual channel histograms. A with a very small number of false alarms. These results threshold of 16 was used in pairwise comparison computasuggest that the proposed filtering methods perform better tion. The filtering type used in all presented experiments on frame difference metrics that are highly susceptible to was Eq. (2), followed by Eq. ( <ref type="formula" target="#formula_4">4</ref>), which we denote by any kind of variation in frame content.</p><p>[mean, median]. The adaptive filter size selection algorithm The bottom two rows of Table <ref type="table">1</ref> illustrate the detection presented in Section 2.2 was used in for all filtering applicaperformance when with multiple features for both the untion, with N ϭ 5 and ͕L i ͖ ϭ ͕7, 11, 15, 21, 31͖.</p><p>filtered and filtered cases. Using both histogram differences The results of global clustering with and without filtering and pairwise comparison, direct clustering without filtering are presented in Table <ref type="table">1</ref> for both of the features. Global yields a very high recall value, but dismal precision; such a high false alarm rate renders the final result practically unusable. After filtering is applied individually to both of  windowing yields slightly better performance overall. The difference in performance increases as window size L w and temporal sampling rate j are increased. Increasing the value of T R improves precision but degrades detection performance, as expected. Selection of T R is direct and simple, as both features lie in the range [0, 1] prior to filtering.</p><p>We have also applied a thresholding-based shot detection method to compare its performance to that of the clustering algorithm and, also, to observe the effects of the proposed filtering methods on other techniques. We have implemented the method presented in <ref type="bibr" target="#b5">[11]</ref>, where the cut detection threshold T b is defined as Ȑ and are the mean and standard deviation of the frame thresholding method are clearly observed in Fig. <ref type="figure" target="#fig_7">10</ref>. For the same Ͱ values, the filtered signals lead to recall-difference signal and Ͱ denotes a constant. When n features are to be used simultaneously, a threshold T (i) b , i ϭ 1, . . . , precision pairs that are closer to unity. Selection of the threshold is more complicated in the n, is selected for each of the features using a set of constants ͕Ͱ i ͖. A frame is declared a shot boundary if all its feature multifeature case, since the constants may vary independently. For brevity, we present in Table <ref type="table" target="#tab_5">4</ref> the best perfor-values exceed the corresponding thresholds. Selection of the constants Ͱ and, hence, the threshold itself is the major mance we have obtained using the multilevel thresholding approach with and without filtering. It is observed that problem in this approach. Whereas clustering selects the decision boundary automatically, this and other thresh-both recall and precision values are improved if filtering is employed, and the overall performance is on a par with olding methods rely on user supervision, and the difficulty in determining the right threshold increases with the di-the short-term clustering methods.</p><p>Key frame extraction is performed on the individual mensionality of the problem.</p><p>Figure <ref type="figure" target="#fig_7">10</ref> depicts the recall-precision plots obtained for shots obtained after temporal segmentation. Table <ref type="table">5</ref> depicts the results obtained for the test sequences when the the two features separately, both for unfiltered and filtered cases. In each case, the threshold T b has been computed number of clusters k was set to 3. A maximum of two key frames, those corresponding to the closest and farthest according to Eq. ( <ref type="formula">9</ref>), using six Ͱ values evenly distributed in the range [0.5, 3.0]. The selected Ͱ values have not been samples to the cluster centers, were selected from each cluster. The cluster merging threshold T C was set to 3000 tuned to the individual test sequences. In our experiments, the clustering method has consistently outperformed the invariably for all sequences, regardless of individual characteristics of each data set. Overall, the algorithm yields thresholding method. It should be noted that performance is measured in terms of both precision and recall; hence, good compaction performance consistent with shot content. Even though only color-based features are employed, a threshold value that misses a few shot boundaries but generates a few thousand false alarms is unacceptable in significant levels of object or camera motion are also captured by the technique. In cases where object movement a practical environment. The effects of filtering on the was observed against a uniform background, a single frame multiple features for improving detection performance was illustrated by incorporating two simple features into the was deemed sufficient for the user to infer the action. However, when the movement was significantly large or clustering method. Detection performance of the algorithm was further improved by temporal filtering tech-its duration long so that the color characteristics of the scene changed dramatically, the key frame selection niques, which helped significantly to reduce the number of false alarms. For automatic key frame extraction, aver-method successfully captured the changes. In various cases redundancies were observed in selected key frames, mainly age and intersection histograms have been used as compact color representations of shots and used in a clustering-due to variations in the luminance component.</p><p>One drawback of the system was the selection of thresh-based scheme to yield visually meaningful and nonredundant reference frames. The method gives the user freedom old T C . Unlike frame differences, the effective range of values the modified 2 metric of Eq. ( <ref type="formula" target="#formula_9">7</ref>) will take on is to generate visual summaries at the desired level of detail by varying the number of clusters. difficult to determine. Normalization to a predefined range alters feature characteristics significantly, resulting in al-Various features can be introduced to enhance the key frame selection process. In particular, a representation of tered performance. As such, threshold selection may prove unfriendly to an uninitiated user. This problem may be the motion characteristics of a shot can significantly help in the identification of appropriate key frames. Clustering overcome by carrying out several trial runs for the same data set for the most appropriate outcome, or the number methods can further be employed to obtain high-level hierarchical representations of a video stream by grouping of members each cluster has can be taken as the frame selection criteria.</p><p>visually similar shots <ref type="bibr" target="#b17">[23]</ref>. High-level content representation requires the identification of associations between shots, however, and cannot be made solely on the basis of 5. CONCLUSIONS temporal proximity or low-level similarity. These issues We have presented in this paper a clustering-based are currently under investigation <ref type="bibr" target="#b1">[7]</ref>. framework for temporal segmentation and key frame extraction, two fundamental functionalities in video data-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REFERENCES TABLE 5 Results of the Automatic Key Frame Extraction Algorithm</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIG. 1 .</head><label>1</label><figDesc>FIG. 1. Unsharp masking for enhancing temporal segmentation performance. The figures portray, from top to bottom, the original pixel difference signal f [n]; the lowpass filtered signal f ˜[n]; the difference signal d [n] ϭ f [n] Ϫ f ˜[n]; and the output signal g[n] after zero-clipping. The filter used is a simple averaging filter of variable duration.</figDesc><graphic coords="4,39.19,40.69,472.51,387.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 2 .</head><label>2</label><figDesc>FIG. 2. Median filtering for improving temporal segmentation performance. The figures portray, from top to bottom, the original pairwise comparison signal f [n]; median filtered signal F median ͕ f [n]͖; the difference signal d [n] ϭ f [n] Ϫ F median ͕ f [n]͖; and the output signal g[n] after zero-clipping. The size of the median filter is adaptively selected.</figDesc><graphic coords="5,39.19,40.69,472.51,387.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>FIG. 3. Scatter plots of frame comparison measures in the 2D feature space (a) without filtering; (b) after unsharp masking by mean filtering; (c) after unsharp masking by median filtering; and (d) after combined filtering of [mean, median] type. The triangles denote the cluster centers that the k-means algorithm converges to for each case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIG. 4 .</head><label>4</label><figDesc>FIG. 4. Effects of combined filtering of [mean, median] type. The figures portray, from top to bottom, the unsharp-masked signal g[n]; median filtered signal F median ͕g[n]͖; the difference signal d [n] ϭ g[n] Ϫ F median ͕g[n]͖; and the output signal g 2 [n] after zero-clipping. The size of the median filter is adaptively selected.</figDesc><graphic coords="7,39.19,40.69,472.51,387.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>FIG. 5. Effects of combined filtering of [median, mean] type. The figures portray, from top to bottom, the masked signal g[n] (obtained using Eq. (4)); the lowpass filtered signal g ˜[n]; the difference signal d [n] ϭ g[n] Ϫ g ˜[n]; and the output signal g 2 [n] after zero-clipping. The filter used is a simple averaging filter of variable duration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIG. 6 .FIG. 7 .</head><label>67</label><figDesc>FIG. 6. Suppression of intershot activity by temporal filtering:</figDesc><graphic coords="9,23.17,40.45,242.04,586.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIG. 9 .</head><label>9</label><figDesc>FIG. 9. Block diagram of the clustering-based key frame extrac-</figDesc><graphic coords="12,330.27,40.53,152.34,600.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIG. 10 .</head><label>10</label><figDesc>FIG. 10. Recall vs precision plot of thresholding results.</figDesc><graphic coords="14,285.80,40.89,241.56,308.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>bases. Methods have been introduced to perform unsupervised, clustering-based temporal segmentation on-the-fly, This work is supported by a National Science Foundation SIUCRC grant and a New York State Science and Technology Foundation grant to eliminating the need for a detection threshold. The use of the Center for Electronic Imaging Systems at the University of Rochester.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,39.19,40.69,472.51,387.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,96.59,40.49,357.51,581.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,21.68,40.37,244.54,595.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>and, hence, is computationally appropriate as well. Percep-</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Results of Short-Time Temporal Segmentation Using a tually</head><label></label><figDesc>uniform color systems such as CIE L*a*b* and CIEFixed Window SizeL*u*v*<ref type="bibr" target="#b19">[25]</ref> are closer to human color perception but are nonlinear spaces, where proper nonuniform quantization Rejection False methods need to be employed for accurate processing<ref type="bibr" target="#b17">[23]</ref>. threshold Correct Missed alarms Note. Window size W i was set to 50. Filtering of type[mean, median]    </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>4. EXPERIMENTAL RESULTS</cell><cell>0.15 0.20</cell><cell>517 515</cell><cell>5 7</cell><cell>70 41</cell><cell>0.99042146 0.98659004</cell><cell>0.88074957 0.94322344</cell></row><row><cell>The test data used in the temporal segmentation experi-ments were digitized at 352 ϫ 240 (SIF) spatial resolution</cell><cell>0.25 0.30 0.40</cell><cell>515 513 512</cell><cell>7 9 10</cell><cell>36 33 30</cell><cell>0.98659004 0.98275862 0.98084291</cell><cell>0.93466425 0.93956044 0.94464945</cell></row><row><cell>from consumer grade video recordings of TV broadcasts</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Note. HD denotes histogram difference, PC denotes pairwise comparibest results and was used in the experiments. Tables2 and son, and HD ϩ PC means that both features have been used simultane-3 depict the results obtained for various values of T R , ously; no filt. and filt. specify whether filtering was applied to the features using windowing and temporal sampling, respectively. The prior to clustering. Filtering of type [mean, median] was used in each filtering application.</figDesc><table><row><cell cols="5">TABLE 1 The Effects of Filtering and Clustering on Temporal Segmentation</cell><cell>the features, though, significant improvements in perfor-mance are observed. The number of false alarms is cut down to an agreeable level, and the detection performance</cell></row><row><cell></cell><cell></cell><cell></cell><cell>False</cell><cell></cell><cell>is better than that attained by either feature alone.</cell></row><row><cell></cell><cell cols="3">Correct Missed alarms</cell><cell>Recall</cell><cell>Precision</cell><cell>The performance of short-time methods were tested with</cell></row><row><cell>HD; no filt.</cell><cell>478</cell><cell>44</cell><cell cols="3">155 0.91570881 0.75513428</cell><cell>two features, using both the windowing and temporal sam-</cell></row><row><cell>PC; no filt.</cell><cell>516</cell><cell>6</cell><cell cols="3">4171 0.98850575 0.11009174</cell><cell>pling methods of Section 2.4. The results are summarized in</cell></row><row><cell>HD; filt.</cell><cell>473</cell><cell>49</cell><cell cols="3">42 0.90613027 0.9184466</cell><cell>Table 2 and Table 3 for windowing and temporal sampling,</cell></row><row><cell>PC; filt.</cell><cell>508</cell><cell>14</cell><cell cols="3">33 0.97318008 0.93900185</cell><cell>respectively. The selection of the appropriate windowing/</cell></row><row><cell>HD ϩ PC; no filt. HD ϩ PC; filt.</cell><cell>517 511</cell><cell>5 11</cell><cell cols="3">3936 0.99042146 0.1161015 26 0.9789272 0.95158287</cell><cell>sampling size did not pose a problem, as elimination of improper intervals was carried out successfully. A window</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>size/sampling rate of 50 frames was found to produce the</cell></row></table><note><p>performance of both approaches are quite comparable, but</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 3 Results of Short-Time Temporal Segmentation Using Sampling</head><label>3</label><figDesc></figDesc><table><row><cell>Rejection</cell><cell></cell><cell></cell><cell>False</cell><cell></cell><cell></cell></row><row><cell>threshold</cell><cell>Correct</cell><cell>Missed</cell><cell>alarms</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>0.15</cell><cell>514</cell><cell>8</cell><cell>72</cell><cell>0.98467433</cell><cell>0.87713311</cell></row><row><cell>0.20</cell><cell>512</cell><cell>10</cell><cell>44</cell><cell>0.98084291</cell><cell>0.92086331</cell></row><row><cell>0.25</cell><cell>512</cell><cell>10</cell><cell>38</cell><cell>0.98084291</cell><cell>0.93090909</cell></row><row><cell>0.30</cell><cell>512</cell><cell>10</cell><cell>34</cell><cell>0.98084291</cell><cell>0.93772894</cell></row><row><cell>0.40</cell><cell>511</cell><cell>11</cell><cell>29</cell><cell>0.97892720</cell><cell>0.94629630</cell></row><row><cell cols="6">Note. The sampling rate j was set to 50. Prefiltering of type [mean,</cell></row><row><cell cols="4">median] was performed prior to clustering.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4 Results of Multilevel Thresholding with Multiple Features</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>(Ͱ 0 , Ͱ 1 )</cell><cell>Correct</cell><cell>Missed</cell><cell>False alarms</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell>HD ϩ PC; no filt.</cell><cell>(1.50, 1.50)</cell><cell>512</cell><cell>10</cell><cell>280</cell><cell>0.98084291</cell><cell>0.64646465</cell></row><row><cell>HD ϩ PC; filt.</cell><cell>(1.50, 1.50)</cell><cell>514</cell><cell>8</cell><cell>67</cell><cell>0.98467433</cell><cell>0.88468158</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1. P. Aigrain, H. J. Zhang, and D. Petkovic, Content-based representa-Note. Q denotes the number of temporal segments in the sequence, shop on Applications of Computer Vision, Florida, 1996, pp. 90-95. while N k is the number of key frames allocated to represent the entire sequence. Cluster distance threshold T C was set to 3000.</figDesc><table><row><cell>Seq. no.</cell><cell>No. of frames</cell><cell>Q</cell><cell>max(N k )</cell><cell>N k</cell><cell>tion and retrieval of visual media: A state-of-the-art review, Multime-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>dia Tools Appl. 3(3), 1996, 3-26.</cell></row><row><cell>1 2 3 4</cell><cell>3598 1831 4372 2878</cell><cell>98 25 30 57</cell><cell>588 144 180 342</cell><cell>174 27 38 71</cell><cell>2. H. J. Zhang, S. Y. Tan, S. W. Smoliar, and Y. Gong, Video parsing, retrieval and browsing: An integrated and content-based solution, in Proc. ACM Multimedia '95, San Francisco, Nov. 1995, pp. 15-24.</cell></row><row><cell>5</cell><cell>4735</cell><cell>59</cell><cell>354</cell><cell>103</cell><cell>3. H. J. Zhang, J. Wu, D. Zhong, and S. W. Somaliar, An integrated</cell></row><row><cell>6</cell><cell>7258</cell><cell>82</cell><cell>492</cell><cell>94</cell><cell>system for content-based video retrieval and browsing, Pattern Recog-</cell></row><row><cell>7</cell><cell>2641</cell><cell>23</cell><cell>138</cell><cell>41</cell><cell>nit. 30(4), 1997, 643-658.</cell></row><row><cell>8</cell><cell>7606</cell><cell>75</cell><cell>450</cell><cell>87</cell><cell>4. F. Idris and S. Panchanathan, Review of image and video indexing</cell></row><row><cell>9</cell><cell>5893</cell><cell>70</cell><cell>420</cell><cell>78</cell><cell>techniques, J. Visual Commun. Image Rep. 8(2), 1997, 146-166.</cell></row><row><cell>10</cell><cell>3178</cell><cell>13</cell><cell>78</cell><cell>21</cell><cell>5.</cell></row></table><note><p>B. Gu ¨nsel, A. M. Ferman, and A. M. Tekalp, Video indexing through integration of syntactic and semantic features, in Proc. IEEE Work-6. A. M. Ferman and A. M. Tekalp, Multiscale content extraction and</p></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">representation for video indexing</title>
	</analytic>
	<monogr>
		<title level="m">Proc. Multimedia Storage and Archiving Systems II</title>
		<meeting>Multimedia Storage and Archiving Systems II<address><addrLine>Dallas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-11">Nov. 1997</date>
			<biblScope unit="volume">3229</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal video segmentation using unsupervised clustering and semantic object tracking</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gunsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="604" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Automatic Information Organization and Retrieval</title>
		<author>
			<persName><forename type="first">G</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Comparison of video shot boundary detection techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Boreczsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Storage and Retrieval for Image and Video Databases IV</title>
		<meeting>Storage and Retrieval for Image and Video Databases IV</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">2670</biblScope>
			<biblScope unit="page" from="170" to="179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of technologies for parsing A. Mu ¨fit Ferman received the B.Sc. and M.Sc. degrees in Electronics and indexing digital video</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ahanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D C</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Turkey, in 1993 and 1995, and the M.S. degree in Electrical Engineering from the University of Rochester</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996. 1997</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="28" to="43" />
		</imprint>
		<respStmt>
			<orgName>and Communication Engineering from Istanbul Technical University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. He</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic partiis currently a Research Assistant and Ph.D. candidate at the Department of full-motion video</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Somaliar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">His research interests include content-based video indexing and</title>
		<meeting><address><addrLine>New</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10" to="28" />
		</imprint>
		<respStmt>
			<orgName>Electrical and Computer Engineering, University of Rochester</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video visualization for compact repre-retrieval, visual information management systems, and pattern recognisentation of pictorial content</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Yeo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IBM Research Report</title>
		<meeting><address><addrLine>Research Triangle Park, NC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">20615</biblScope>
		</imprint>
	</monogr>
	<note>He is a student member of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knowledge guided parsing in video databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Swanberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Symp. on Elec. IS&amp;T/SPIE</title>
		<meeting>Symp. on Elec. IS&amp;T/SPIE<address><addrLine>San Jose, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image activity characteristics in broadcast television</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Choma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Commun. Oct</title>
		<imprint>
			<biblScope unit="page" from="1201" to="1206" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Salt</surname></persName>
		</author>
		<title level="m">Film Style and Technology: History and Analysis</title>
		<meeting><address><addrLine>Starwood, London</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<title level="m">Two-Dimensional Signal and Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A high performance algorithm for shot boundary puter, and Systems Engineering from Rensselaer Polytechnic Institute detection using multiple cues</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ferman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murat ; Comand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tekalp received M.S. and Ph.D. degrees in Electrical</title>
		<meeting><address><addrLine>Troy, New York; Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Eastman Kodak Company</publisher>
			<date type="published" when="1982">1982. 1984. Oct. 1998. 1984 to August 1987</date>
		</imprint>
	</monogr>
	<note>From December Proc. ICIP&apos;98</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">How Movies Work</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Kawin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>Macmillan Corp</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>He joined the Electrical Engineering Department at the University of</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mosaic based representations of Rochester, New York, as an assistant professor in September 1987, where video sequences and their applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth International Conferhe is currently a professor. His current research interests are in the areas ence on Computer Vision</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-06">June 1995</date>
			<biblScope unit="page" from="605" to="611" />
		</imprint>
	</monogr>
	<note>of digital image and video processing, including image restoration, motion analysis. object tracking, model-based coding, 3-D video, automatic image</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image processing on compressed annotation and retrieval, and magnetic resonance imaging. data for large video databases</title>
		<author>
			<persName><forename type="first">F</forename><surname>Arman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1st ACM Int. Conf. on Multi-Professor Tekalp is a senior member of IEEE, and a member of Sigma media</title>
		<meeting>1st ACM Int. Conf. on Multi-Professor Tekalp is a senior member of IEEE, and a member of Sigma media</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
	<note>Xi. He received the NSF Research Initiation Award in 1988. He has been</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automathe Chair of the Image and Multidimensional Signal Processing Technical tion of systems enabling search on stored video data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hanjalic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ceccarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lagendijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Biemond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIEas an Associate Editor for the IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="1990">1996-1998. 1997. 1990-1992. 1994-1996</date>
			<biblScope unit="volume">3022</biblScope>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
	<note>Proc. Storage Committee of the IEEE Signal Processing Society</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient matching and clustering of video Kluwer&apos;s Multidimensional Systems and Signal Processing (1993-1996). shots</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop and the Special Sessions Chair for 1995 IEEE International Conference on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="338" to="341" />
		</imprint>
	</monogr>
	<note>Proc. IEEE International Conference on Image Processing. He has been the organizer and first</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering methods for Chairman of the Rochester Chapter of the IEEE Signal Processing Socivideo browsing and annotation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Storage and Retrieval for ety</title>
		<meeting>Storage and Retrieval for ety</meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1994">1996. 1994-1995</date>
			<biblScope unit="volume">2670</biblScope>
			<biblScope unit="page" from="239" to="246" />
		</imprint>
	</monogr>
	<note>He was elected as the Chair of the IEEE Rochester Section in Image and Video Databases IV. At present, he is on the editorial boards of</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Color indexing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Swain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Representation; and EURASIP&apos;s Im</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11" to="32" />
		</imprint>
	</monogr>
	<note>Journal of Vision</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Principles of Color Technology</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Billmeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saltzman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Prentice-Hall book, Digital Video Processing</publisher>
		</imprint>
	</monogr>
	<note>age Communication. He is the author of the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
