<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Body Structure Aware Deep Crowd Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siyu</forename><surname>Huang</surname></persName>
							<email>siyuhuang@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
							<email>zhongfei@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@cs.zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<email>jirongrong@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<email>junweihan2010@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Xi</forename><forename type="middle">Li</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Information Science and Elec-tronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Alibaba-Zhejiang</orgName>
								<orgName type="institution">University Joint Institute of Frontier Technologies</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">College of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">College of Information Science and Electronic Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Computer Science Department</orgName>
								<orgName type="department" key="dep2">Watson School</orgName>
								<orgName type="institution">The State University of New York Binghamton University</orgName>
								<address>
									<postCode>13902</postCode>
									<settlement>Binghamton</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">School of Information Science and Tech-nology</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<postCode>201210</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<settlement>Xiamen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Body Structure Aware Deep Crowd Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8828B9DC959CDFC1F9FD011A6526F996</idno>
					<idno type="DOI">10.1109/TIP.2017.2740160</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2740160, IEEE Transactions on Image Processing received December 14, 2016; revised April 05, 2017; accepted August 06, 2017. Date of publication XXX, 2017; date of current version XXX, 2017. This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2017.2740160, IEEE Transactions on Image Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Crowd counting</term>
					<term>pedestrian semantic analysis</term>
					<term>visual context structure</term>
					<term>convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crowd counting is a challenging task, mainly due to the severe occlusions among dense crowds. This work aims to take a broader view to address crowd counting from the perspective of semantic modelling. In essence, crowd counting is a task of pedestrian semantic analysis involving three key factors: pedestrians, heads, and their context structure. The information of different body parts is an important cue to help us judge whether there exists a person at a certain position. Existing methods usually perform crowd counting from the perspective of directly modelling the visual properties of either the whole body or the heads only, without explicitly capturing the composite body-part semantic structure information that is crucial for crowd counting. In our approach, we first formulate the key factors of crowd counting as semantic scene models. Then, we convert the crowd counting problem into a multi-task learning problem such that the semantic scene models are turned into different sub-tasks. Finally, the deep convolutional neural networks (CNNs) are used to learn the sub-tasks in a unified scheme. Our approach encodes the semantic nature of crowd counting and provides a novel solution in terms of pedestrian semantic analysis. In experiments, our approach outperforms the state-of-the-art methods on four benchmark crowd counting datasets. The semantic structure information is demonstrated to be an effective cue in scene of crowd counting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Count = 61</head><p>Fig. <ref type="figure">1</ref>. Brief illustration of our approach. We build semantic scene models including the body part map and the structured density map to encode the semantic nature of a crowd scene. The crowd count is estimated based on the two semantic scene models. attention from researchers because of a series of practical demands including crowd control and public safety. As illustrated in the top left of Fig. <ref type="figure">1</ref>, there is a common crowd scene. The occlusions among people are severe and the perspective distortions vary significantly in different areas. In addition, the crowd distributions are visually diverse. These difficulties have restricted the performance of existing crowd counting methods.</p><p>In principle, crowd counting seeks for pedestrian semantic analysis involving three key factors: pedestrians, heads and their context structure. Most existing methods focus on modelling the visual properties of either the whole pedestrians or the heads only, while ignoring the context structure of different body parts which is also significant for counting the crowds. For instance, when we humans count the pedestrians, we will naturally use the composite body-part semantic structure information as an auxiliary cue to judge whether a head seen by us is exactly a pedestrian at that position or something else. It indicates that the semantic structures of pedestrians could provide abundant information for recognizing the pedestrians. However, many existing detection-based crowd counting methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> model the pedestrians by constructing pedestrian detectors or head-shoulder detectors that they are limited by the severe occlusions in dense crowds. In more recent literatures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, researchers focus on modelling the density distributions of pedestrians, while, ignoring the bodypart semantic structure information that is essential for the cognition of human beings.</p><p>Motivated by the above observations, we address the crowd counting problem from the viewpoint of semantic modelling in this work. The three key factors of crowd counting, including pedestrians, heads, and their context structure, are formulated as two types of semantic scene models. The first semantic scene model is denoted as the body part map in this paper. It models the visual appearance and context structure of pedestrian body parts. In body part map, different pedestrian body parts are formulated as different semantic categories, in the meantime, the spatial context structure of different parts are also formulated into the map. Fig. <ref type="figure">1</ref> provides an intuitive illustration of our approach. The body part map is created based on the single pedestrian parsing model <ref type="bibr" target="#b4">[5]</ref>, which is a pre-trained neural network model calculating the semantic segmentation mask of an input pedestrian image. And then we merge the segmentation masks of all the pedestrians to create the body part map. The top right of Fig. <ref type="figure">1</ref> shows the body part map highlighted by the areas of head, body, and legs respectively with colors of light blue, yellow, and red.</p><p>The second semantic scene model is denoted as the structured density map in this paper. The conventional density maps in existing works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> are proposed to model the density distributions of crowds, while the shapes of individual pedestrians are ignored. Motivated by this, we create the structured density map according to specific shapes of individual pedestrians which are provided by the body part map. As an improvement of the conventional density map, the structured density map aims to model more fine-grained semantic structure information and so it can provide more accurate pixel-wise labels. As illustrated in bottom left of Fig. <ref type="figure">1</ref>, the structured density map denotes the density information of crowds, meanwhile, preserving the shapes of specific pedestrians. In summary, the two semantic scene models, body part map and structured density map, are proposed to encode the semantic nature of crowd counting and they recover rich semantic structure information from crowd images.</p><p>For the purpose of accurately estimating the count of pedestrians, we reformulate crowd counting as a multi-task learning problem. There are three sub-tasks: inferring two types of semantic scene models and estimating the crowd count. We build deep convolutional neural networks (CNNs) to jointly learn these sub-tasks. The CNNs first model the mappings from scene image to semantic scene models including the body part map and structured density map, followed by calculating the crowd count based on them. The CNNs are able to extract powerful visual representations from images. The feature extraction and multi-task crowd counting problem are addressed in a unified scheme.</p><p>We summarize our main contributions as follows:</p><p>1) We provide a novel solution for crowd counting in terms of pedestrian semantic analysis. We formulate three key factors of crowd counting and model them as two types of semantic scene models. The models recover rich semantic structure information from images and are effective in learning our crowd counting framework. 2) We reformulate the crowd counting problem as a multitask learning problem such that the semantic scene models are converted into its sub-tasks. We present a unified framework to jointly learn these sub-tasks based on the CNNs. Experiments show that our method achieves better results compared to the state-of-the-art methods.</p><p>II. RELATED WORK We introduce the literatures related to our work in this section. We first discuss the crowd counting methods proposed in existing literatures. And then we discuss several related works on pedestrian semantic analysis, as we address the crowd counting problem from the viewpoint of pedestrian semantic modelling in this paper. In addition, we introduce the background of CNNs, as our crowd counting framework is built upon the deep neural networks. Crowd counting: In general, most of the methods for crowd counting can be grouped into three categories: (1) detectionbased, (2) global regression and (3) density estimation. The earlier literatures of crowd counting propose the detectionbased methods <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> to model the semantic structure of pedestrians. Various kinds of detectors are employed to match individual pedestrians in images. Li et al. <ref type="bibr" target="#b9">[10]</ref> use a HOG based head-shoulder detector to detect heads within foreground areas. Wu and Nevatia <ref type="bibr" target="#b0">[1]</ref> detect local human body parts by part detectors and combine their responses to form people detections. Lin and Davis <ref type="bibr" target="#b1">[2]</ref> learn a generic human detector by matching a part-template tree to images hierarchically. The detection-based methods perform better in relatively low dense scenes, while, they are limited by the heavy occlusions in dense crowds. Different from them, we model the semantic structure of pedestrians as semantic scene models. They are more robust to learn under the crowded scene and are more suitable for deep learning based framework.</p><p>To overcome the difficulties of detection-based methods in high dense scenes, researchers take a different way that they propose the global regression based methods to learn the mapping between low-level features and pedestrian counts. These methods are more suitable for crowded environments than the detection-based approaches. Diverse kinds of lowlevel features are employed, including textures <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, edge information <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, and segment shape <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. In addition, regression algorithms including linear regression <ref type="bibr" target="#b16">[17]</ref>, Bayesian regression <ref type="bibr" target="#b12">[13]</ref>, ridge regression <ref type="bibr" target="#b11">[12]</ref>, and Gaussian process regression <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> are commonly used. The global regression based methods only utilize the information of pedestrian counts, while, the spatial information and body structure information of pedestrians are ignored.</p><p>To model the spatial information of pedestrians, researchers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref> formulate the latent density distributions of crowds as an intermediate ground truth, namely, the density estimation based crowd counting. Lempitsky and Zisserman <ref type="bibr" target="#b5">[6]</ref> first generate the density map based on the annotated points with a 2D Gaussian kernel and learn a linear regression function between scene image and density map. Following their work, other density estimation methods including random forest <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> and deep neural networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref> are proposed. These methods demonstrate good performance on crowd counting. But from the perspective of semantic modelling, the bodypart structures of individual pedestrians are ignored in these approaches. In this work, we focus on analyzing the semantic nature of crowd counting. We build semantic scene models by recovering rich semantic structure information from images and take them as novel supervised labels for crowd counting. Pedestrian semantic analysis: The semantic analysis of pedestrian is an important prerequisite to many practical applications for intelligent surveillance systems operating in real world environments, including several typical computer vision topics like pedestrian detection <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>, pedestrian parsing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29]</ref> and crowd segmentation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. In recent years, some high-level tasks of pedestrian analysis have also drawn much attention from researchers in recent years, including action recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, crowd attribute analysis <ref type="bibr" target="#b33">[34]</ref>, and pedestrian path prediction <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. What these approaches have in common is that they learn and model different aspects of semantic structure prior of pedestrians.</p><p>In this work, we address the crowd counting problem by focusing on pedestrian semantic analysis, because the visual cues of pedestrian body-part appearance can provide abundant information for recognizing the crowds. The success of partsbased methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> on pedestrian detection also demonstrates this idea. Rather than directly detecting the holistic pedestrian, the parts-based methods utilize the information of pedestrian body structure and is able to handle occlusions more robustly. Different from the conventional parts-based methods, we formulate the body-part semantic structure of pedestrians as the semantic scene models in our approach, which are more suitable for learning under deep neural network framework and are more effective and robust in dense crowded scenes. Convolutional Neural Networks: Our crowd counting framework is built upon the CNNs. The CNNs are a popular and leading visual representation technique, for they are able to learn powerful and interpretable visual representations <ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. Specifically, we use the fully convolutional networks (FCNs) to learn the semantic scene models proposed in our approach. As a kind of CNN architecture, FCNs are end-to-end models for pixelwise problems. They have given the state-ofthe-art performance on many scene analysis tasks, including scene parsing <ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref>, crowd segmentation <ref type="bibr" target="#b30">[31]</ref> and action estimation <ref type="bibr" target="#b45">[46]</ref>. For crowd counting, Zhang et al. <ref type="bibr" target="#b3">[4]</ref> propose a multi-column FCN to map the crowd image to the density map. Their models are adaptive to the variations in pedestrian size and achieve the state-of-the-art performance on the benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem formulation</head><p>In this work, we aim to address the problem of single image crowd counting. Given a crowd image X , our goal is to estimate the pedestrian number C in the image. It can be formulated as a mapping X F -→ C. From the perspective of semantic modelling, we reformulate the original problem as a multi-task learning problem that contains three sub-tasks: the inference of two semantic scene models and the estimation of pedestrian number. The first semantic scene model is the body part map B, which is built to model the body-part semantic </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variables Description</head><formula xml:id="formula_0">X ∈ R m•n•3 the scene image M ∈ R m•n the perspective map B ∈ R m•n•4 the body part map D ∈ R m•n the structured density map C ∈ R the pedestrian count x ∈ R m•n•3 the image patch b ∈ R m•n•4 the body part map of image patch d ∈ R m•n the density map of image patch c ∈ R the pedestrian count of image patch p ∈ R 2</formula><p>the coordinate of arbitrary location P ∈ R 2 the coordinate of specific location N the 2D Gaussian kernel F the mapping function L the loss function of neural network structures of pedestrians. The second one is the structured density map D, which is built to model the density distributions and shapes of pedestrians. Both two models are data-dependent that they respectively encode different semantic attributes of a crowd image. To address the multi-task learning problem, we build the CNNs to jointly learn the three sub-tasks in a unified framework. The learning process can be written as</p><formula xml:id="formula_1">X F1 -→ (B, D) F2</formula><p>-→ C, such that B and D are used as auxiliary ground truths to better estimate C. For reading convenience, we summarize a collection of important notations used in this paper as Table <ref type="table" target="#tab_0">I</ref>. We discuss our approach in more details in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Body part map</head><p>As one of the semantic scene models, the body part map B is proposed to model the body-part semantic structures of individual pedestrians, which can serve as an important cue to judge whether there exists a person at a certain location. We introduce it into our framework as a novel supervised label to address the difficulties in crowd counting problem.</p><p>The body part map B is generated based on the given scene image X , perspective map M and the locations of head points P i h . First, we have to obtain single pedestrian images. Due to the perspective distortions, we use the perspective map M (given in datasets) to normalize the scales of pedestrians. The pixel value M(p) denotes the number of pixels in the image representing one meter at location p in the actual scene. With the head location P h = (P x h , P y h ) of a person, the top left corner P tl and bottom right corner P br of the person's bounding box are estimated as</p><formula xml:id="formula_2">P tl = (P x h -α 1 M(P h ), P y h -β 1 M(P h )) , P br = (P x h + α 2 M(P h ), P y h + β 2 M(P h )) ,<label>(1)</label></formula><p>where the parameters are manually set as α 1 = 0.5, α 2 = 0.5, β 1 = 0.25, β 2 = 1.75 in the experiments to best approximate the actual situations, such that the width of bounding box is assumed as α 1 + α 2 = 1 meter and the height of bounding box is assumed as β 1 + β 2 = 2 meters in the actual scene.</p><p>After obtaining the pedestrian images, we normalize them to the same size followed by inputting them into the single pedestrian parsing model <ref type="bibr" target="#b4">[5]</ref> to calculate their semantic segmentation masks. The pedestrian parsing model uses a deep neural network to parse a single pedestrian image into several semantic regions, including hair, head, body, legs, and feet. The model is pre-trained that we only use it to generate the semantic segmentation of pedestrians. We merge the regions of hair into head, and also merge the regions of feet into legs. Finally, we resize the semantic masks of individual pedestrians to their original sizes to create the body part map B.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Structured density map</head><p>The structured density map is proposed to capture both the density distributions and shapes of pedestrians. Different from the existing crowd counting methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6]</ref>, it is datadependent in our approach that it is generated according to specific shapes of individual pedestrians.</p><p>We first discuss the conventional density map D N proposed in existing works <ref type="bibr" target="#b2">[3]</ref> . It is usually created by a sum of 2D Gaussian kernels centered on the locations of pedestrians as:</p><formula xml:id="formula_3">D N (p) = C i=1 1 Z N i h (p; P i h , σ i h ) + N i b (p; P i b , σ i b ) , (<label>2</label></formula><formula xml:id="formula_4">)</formula><p>where N h is a standard 2D Gaussian kernel for modelling the head part of a pedestrian and N b is a bivariate 2D Gaussian kernel for modelling the body part of a pedestrian. p is the location of a pixel on D N , P i h and P i b are respectively the i-th locations of person heads and bodies. In order to approximate the sizes of head and body in actual scene, we manually set the variance σ h of kernel N h as σ h = 0.25M(P h ), and set the variance σ b of kernel N b as σ bx = 0.25M(P b ) and σ by = M(P b ). The body location P b is set as P b = P h + 0.8M(P h ). Z is the normalization factor which normalizes the sum of density values for each person to 1, such that the sum of density values for all the persons is the count C. Fig. <ref type="figure">2(c)</ref> shows the density map D N corresponding to the scene image in Fig. <ref type="figure">2(a)</ref>.</p><p>Since D N cannot well model the specific shapes of individual pedestrians, we further propose the structured density map D:</p><formula xml:id="formula_5">D N (p) = C i=1 1 Z N i h (p; P i h , σ i h ) + N i b (p; P i b , σ i b ) • B m (p)<label>(</label></formula><p>3) The pedestrian mask B m characterizes the shape of each pedestrian. It is obtained by binarizing the body part map B, where the pixel values of foregrounds and backgrounds are respectively set to 1 and 0. The structured density map D is calculated by the element-by-element multiplication of D N and B m , followed by normalization. Fig. <ref type="figure">2(d)</ref> shows the structured density map generated by our approach. Compared to the conventional density map in Fig. <ref type="figure">2</ref>(c), we can see that the structured density map not only denotes the latent density distributions of crowds but also maintains specific shapes of every individual pedestrian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Multi-task crowd counting framework</head><p>To estimate the accurate pedestrian number C, we reformulate the original crowd counting problem as a multitask learning problem including three sub-tasks: the inference of semantic scene models B and D, and the estimation of pedestrian number C. To jointly learn these three sub-tasks, we propose a unified multi-task learning framework based on the CNNs. See Fig. <ref type="figure" target="#fig_2">3</ref> for an illustration of our framework. We take a patch-wise strategy in which the input of networks is an image patch x cropped from scene image X , where x is constrained to cover a 3-meter by 3-meter square in the actual scene according to the perspective map M. In conventional neural network based density estimation method <ref type="bibr" target="#b2">[3]</ref>, there is only one stream of the mapping from scene patch x to density map d: x </p><formula xml:id="formula_6">L d = d -d 2 2 .<label>(4)</label></formula><p>The networks between patch x and body part map b are also FCNs which contain 9 convolutional layers. </p><p>where b(h, w) stands for the output of conv b 9 layer at spatial position (h, w) and channel of ground truth category. b(h, w, i) is the output of conv b 9 layer at position (h, w) and i-th channel.</p><p>The two outputs d and b are concatenated to the fully connected layers f c1(512)-f c2(128)-f c3(1) for estimating the counts c, where fc represents a fully connected layer. We use Euclidean loss to measure the difference between the estimated count c and ground truth ĉ, as</p><formula xml:id="formula_8">L c = (c -ĉ) 2 . (<label>6</label></formula><formula xml:id="formula_9">)</formula><p>The three loss functions L d , L b and L c are combined as a joint multi-task loss L:</p><formula xml:id="formula_10">L = L c + λ d L d + λ b L b .<label>(7)</label></formula><p>λ d and λ b are loss weights respectively set to 10 and 1 in experiments. The whole network is jointly trained under L by back propagation and stochastic gradient descent. Finally, the count C an entire image is a sum of all the patch counts c of the image. Because C is composed of local count information within different areas, we further use the ground truth count Ĉ of image to fine-tune the count C predicted by our neural networks, as</p><formula xml:id="formula_11">Ĉ = Cω + .<label>(8)</label></formula><p>We employ the linear regression for fine-tuning. The regression coefficients ω and are estimated by the training data. In the testing phase, we use ω and to fine-tune the pedestrian count which is estimated by the neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>We give the details on the networks, datasets, and the evaluation metric in the following. Networks: We use the popular Caffe toolbox <ref type="bibr" target="#b46">[47]</ref> to implement the proposed deep convolutional neural networks. Due to the effect of gradient vanishing for deep neural networks, it is not easy to learn all the parameters simultaneously. We use a trick in training phase that we first separately pre-train the two CNNs of mapping between the patch and two maps, and then use the pre-trained weights to initialize the entire CNNs and fine-tune all the parameters simultaneously. The network from patch to body part map is trained for 100K iterations with a batch size of 100 and learning rate of 10 -5 . The network from   Datasets: We evaluate our method in four benchmark datasets including the WorldExpo'10 dataset <ref type="bibr" target="#b2">[3]</ref>, the Shanghaitech-B dataset <ref type="bibr" target="#b3">[4]</ref>, the UCSD dataset <ref type="bibr" target="#b14">[15]</ref> and the UCF CC 50 dataset <ref type="bibr" target="#b47">[48]</ref>. The details of the four datasets are summarized in Table <ref type="table" target="#tab_0">II</ref>, where Scenes is the number of scenes; Frames is the number of frames; Resolution is the resolution of images; FPS is the number of frames per second; Counts is the minimum and maximum numbers of people in the ROI of a frame; Average is the average pedestrian count; Total is the total number of labeled pedestrians. Fig. <ref type="figure" target="#fig_3">4</ref> shows example frames of the four datasets. The scenes, crowd densities, crowd distributions, and perspective distortions vary significantly among these datasets such that they can be used to comprehensively evaluate the crowd counting methods. Evaluation metric: By following the convention of existing works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> for crowd counting, we use the mean absolute error (MAE) and mean squared error (MSE) to evaluate the performance of different crowd counting methods:</p><formula xml:id="formula_12">MAE = 1 N N i=1 |C i -Ĉi |,<label>(9)</label></formula><formula xml:id="formula_13">MSE = 1 N N i=1 (C i -Ĉi ) 2 , (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where N is the number of test images, C i and Ĉi are respectively the estimated people count and ground truth people count in the i-th image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. WorldExpo'10 dataset</head><p>The WorldExpo'10 dataset <ref type="bibr" target="#b2">[3]</ref> contains 1132 annotated video sequences captured by 108 surveillance cameras, all from Shanghai 2010 WorldExpo. This dataset provides a total of 199,923 annotated pedestrians at the centers of their heads in 3980 frames. The testing dataset includes five video sequences of different scenes, and each video sequence contains 120 labeled frames. The regions of interest (ROI) and perspective maps of scenes are provided for the train and test scenes.</p><p>Table III reports the MAE errors of different methods on WorldExpo'10 dataset. The results of LBP features based ridge (RR) regression method are listed at the top row. Zhang et al. <ref type="bibr" target="#b2">[3]</ref> propose the Crowd CNN model to estimate the density maps and crowd counts of image patches based on deep neural networks. Results of their model are listed at the second row. The third row lists the results of the Crowd CNN model with the scene-specific fine-tuning technique which utilizes the information of test scenes. Zhang et al. <ref type="bibr" target="#b3">[4]</ref> propose a Multicolumn CNN (MCNN) model which uses filters of different sizes to estimate the geometry-adaptive density map. Results of their model are listed at the fourth row. The last row lists the results of our method. Our method achieves the best performance in terms of average MAE. In scene 3, 4, and 5, our method achieves the best performance compared with the other methods. It indicates that the semantic structure information modelled by our method is effective in different scenes. In scene 2, the performance of our method is relatively worse, mainly because the crowds of this scene are extremely dense within small areas such that there are very few body-part semantic cues. As a comparison, the fifth row lists the results of our method using conventional density map instead of structured density map. The introduction of structured density map makes 11% improvement over conventional density map for our model. Fig. <ref type="figure" target="#fig_5">5</ref> shows some qualitative results of our method on the test scenes of WorldExpo'10 dataset. The figures are respectively the (a) scene images, (b) body part maps, (c) density maps and (d) head density maps from left to right. In the body part maps, colors of light blue, green, yellow and dark blue respectively denote the regions of head, body, legs and background. The body part maps show that our method can detect precise pedestrian body parts even under the heavy occlusions among crowds. The density maps are constructed to model the density distributions of pedestrians. The head density maps are inferred based on the body part maps and density maps, indicating that our method is also able to predict precise locations and densities of pedestrian heads. Thus the accurate pedestrian counts are estimated based on these effective body part maps and density maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shanghaitech-B dataset</head><p>The Shanghaitech-B dataset is a part of Shanghaitech dataset which was first introduced by Zhang et al. <ref type="bibr" target="#b3">[4]</ref>. It contains 716 annotated images which are taken from different cameras at the busy streets of metropolitan areas in Shanghai. This dataset has a total of 88,488 annotated pedestrians at the centers of their heads. In this dataset, 400 images are used for training and 316 images are used for testing. Because the perspective maps are not provided and the perspective distortions of scenes are similar among scenes, we manually create a single perspective map which is used for all the images.  <ref type="bibr" target="#b3">[4]</ref>, we compare our method with the LBP+RR method, the Crowd CNN method <ref type="bibr" target="#b2">[3]</ref>, and the MCNN method <ref type="bibr" target="#b3">[4]</ref>. The MCNN-CCR is the MCNN model trained without the ground truth of the density map. Our method outperforms the state-of-the-art methods by large margins in terms of both the MAE and MSE. It indicates that our method has a good generalization capability over many different scenes. Compared to MCNN-CCR which is based on pedestrian count regression, the MCNN method performs much better because it preserve more density information of the image. Likewise, our method proves better performance than MCNN because we further capture the pedestrian body-part structure information to help improve the count accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. UCSD dataset</head><p>The UCSD dataset <ref type="bibr" target="#b14">[15]</ref> contains 2000 frames of a single scene. The video in this dataset is recorded at 10 fps with the frame size of 158×238. The crowd density of this dataset is relatively low that there are only about 25 persons on average in each frame. The annotations of pedestrian head locations and ROI are provided. Following the convention of the existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref>, we use frames 601-1400 as the training data, and the remaining 1200 frames as the test data. Since the perspective map is not provided in this dataset and the perspective distortions of the scene are not severe, we fix the perspective values of all the pixels to the same.  <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Four hand-crafted features based regression methods are compared in Table <ref type="table" target="#tab_4">V</ref>, including the kernel ridge regression <ref type="bibr" target="#b48">[49]</ref>, the multi-output ridge regression <ref type="bibr" target="#b11">[12]</ref>, the Gaussian process regression <ref type="bibr" target="#b14">[15]</ref>, and the cumulative attribute regression <ref type="bibr" target="#b49">[50]</ref>. Results of the CNN based density estimation methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> are also listed in Table <ref type="table" target="#tab_4">V</ref>. Our method outperforms both the regression based methods and the CNN based methods in terms of MAE. The MSE of our method is a little larger than the MCNN method, mainly because the multi-size filters proposed in their methods are more robust for datasets without annotated perspective values. Except the MCNN method, other methods including ours do not specifically optimize the perspective distortions. Our method outperforms these methods by large margins in terms of both two metrics, because there are abundant pedestrian body-part information in relatively lowdensity scenarios. It also demonstrates the effectiveness of semantic scene models proposed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. UCF CC 50 dataset</head><p>The UCF CC 50 dataset <ref type="bibr" target="#b47">[48]</ref> contains 50 images of different scenes. It is very challenging, because of not only the limited number of images, but also the extremely dense crowds in images. Following the conventional setting <ref type="bibr" target="#b47">[48]</ref>, we split the dataset randomly and perform 5-fold cross validation. Because of the limitation of the training samples, we randomly crop 1000 patches from each image for training. The perspective values are fixed as the perspective maps are not provided.</p><p>Table VI reports the MAE and MSE errors of our method and the other methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48]</ref>. Rodriguez et al. <ref type="bibr" target="#b18">[19]</ref> propose the density map estimation to improve the head detection performance in crowd scenes. Lempitsky and Zisserman <ref type="bibr" target="#b5">[6]</ref> learn the density regression model based on dense SIFT features and the MESA distance. Idrees et al. <ref type="bibr" target="#b47">[48]</ref> estimate the crowd counts based on multi-source features. The deep learning based approach <ref type="bibr" target="#b2">[3]</ref> are also evaluated on this dataset. Our method performs the best in terms of MAE, indicating that the semantic structure information is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MAE MSE Density-aware detection <ref type="bibr" target="#b18">[19]</ref> 655.7 697.8 Density estimation <ref type="bibr" target="#b5">[6]</ref> 493.4 487.1 Multi-source fusion <ref type="bibr" target="#b47">[48]</ref> 419.5 541.6 Crowd CNN <ref type="bibr" target="#b2">[3]</ref> 467.  The whole model performs the best on both the training set and test set. It indicates that the joint modelling of different semantic attributes of crowd images provides an effective and robust solution for crowd counting. The direct regression network performs well on the train set but the worst on the test set. It is in line with our intuition that the crowd counting model trained by only the count information is short of generalization capability. On the test set, the body part map network performs better than the direct regression network, indicating that the body part map is an effective supervised label in crowd counting. While, it performs a little worse than the density map networks, indicating that the density map may make bigger contribution to our framework than body part map. In addition, the structured density map performs better than conventional density map in most cases, indicating that the structured density map can help improve the crowd counting performance.</p><p>In addition, Fig. <ref type="figure" target="#fig_8">7</ref> shows more qualitative results of our crowd counting models which are trained by different supervised labels. From left to right, the figures are respectively the scene images, the ground truth body part maps, the body part maps inferred by our model, and the pedestrian numbers estimated by different models. The scene images are of the test set of WorldExpo'10 dataset. The body part maps inferred by our model can well model the body-part semantic structures of pedestrians in scenes of different crowd densities and diverse crowd distributions, thus helps estimate the accurate pedestrian numbers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a novel approach to accurately estimate the count of crowds in images. Our approach has focused on discovering the semantic nature of crowd counting. We have built two semantic scene models to recover rich semantic structure information from images. In addition, we have reformulated the crowd counting problem as a multitask learning problem such that the semantic scene models have been turned into different sub-tasks. We have built the CNNs to jointly learn these sub-tasks in a unified scheme. In experiments, our approach has achieved better performance compared to the state-of-the-art methods on four benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 2(a) shows a crowded scene image, and Fig. 2(b) shows the body part map corresponding to the scene image. Colors of light blue, yellow, red and dark blue respectively denote areas of head, body, legs and background. The body part maps containing labelled pixels of four categories models the semantic structure of every individual pedestrian in the scene images. And they are prepared for learning our crowd counting framework as discussed in subsection III-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. Illustration of the semantic scene models. (a) is the scene image. The red points on (a) denote the annotations of pedestrian heads. (b) is the body part map. (c) is the conventional density map created by 2D Gaussian kernels only. (d) is the structured density map generated based on (b) and (c), modelling both the density information and shape information of individual pedestrians.</figDesc><graphic coords="4,58.80,402.09,110.46,88.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of the proposed networks. The image patches are cropped from the scene image and are inputted into the CNNs. The convolutional layers denoted in blue blocks are built to infer the density distributions, and they are trained under the structured density map d with Euclidean loss. The layers denoted in orange blocks are built to infer the pedestrian body-part structures, and they are trained under the body part map b with Softmax loss. Finally, the crowd count c is regressed based on the two types of maps with fully connected layers as denoted in green blocks. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>4 i=1</head><label>4</label><figDesc>The architecture is conv b 1(9,32)-pool b 1(2)-LRN-conv b 2(9,32)-conv b 3(5,64)pool b 3(2)-LRN-conv b 4(5,64)-conv b 5(5,64)-conv b 6(3,128)conv b 7(3,128)-conv b 8(1,256)-conv b 9(1,1). We use a sum of Softmax loss at all 32×32 positions to measure the difference between the estimated body part map b and ground truth b: exp(b(h, w, i)) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Example frames of (a) UCSD dataset, (b) UCF CC 50 dataset, (c) WorldExpo'10 dataset, and (d) Shanghaitech-B dataset.</figDesc><graphic coords="6,59.31,251.08,79.20,58.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Qualitative results of our method on test scenes of WorldExpo'10 dataset, including (a) scene images, (b) body part maps, (c) density maps, and (d) head density maps. These maps model the body-part structures and the density distributions of crowds.</figDesc><graphic coords="7,52.80,356.02,118.75,95.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>extremely dense scenes. In addition, we also evaluate the performance of our model without the global finetuning operation described as Eq. 8 in subsection III-D. The results show that the global fine-tuning operation is able to help improve the performance of the patch-wise based crowd counting models.F. The effectiveness of different supervised labelsThere are three different ground truth supervised labels used in this work, including the structured density map D, the body part map B and the pedestrian number C. We compares the effectiveness of themselves in crowd counting, as shown in Fig. 6. We evenly group the training images and testing images of WorldExpo'10 dataset into 10 groups according to increasing pedestrian number. The vertical axis denotes the MAE error in each group. The black curve represents the direct regression network trained by C only. The purple curve represents the network trained by conventional density map D N and C. The blue curve represents the network trained by structured density map D and C. The green curve represents the network trained by B and C. The red curve represents our whole model which is trained by all the three ground truths B, D, and C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of different ground truths on WorldExpo'10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. From left to right: scene images, ground truth body part maps, body part maps generated by our method, and estimated pedestrian numbers. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>DETAILED DESCRIPTION OF THE VARIABLES</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III MEAN</head><label>III</label><figDesc>ABSOLUTE ERRORS (MAE) OF THE WORLDEXPO'10 DATASET</figDesc><table><row><cell>Method</cell><cell cols="6">Scene 1 Scene 2 Scene 3 Scene 4 Scene 5 Average</cell></row><row><cell>LBP+RR</cell><cell>13.6</cell><cell>59.8</cell><cell>37.1</cell><cell>21.8</cell><cell>23.4</cell><cell>31.0</cell></row><row><cell>Crowd CNN [3]</cell><cell>10.0</cell><cell>15.4</cell><cell>15.3</cell><cell>25.6</cell><cell>4.1</cell><cell>14.1</cell></row><row><cell>Fine-tuned Crowd CNN [3]</cell><cell>9.8</cell><cell>14.1</cell><cell>14.3</cell><cell>22.2</cell><cell>3.7</cell><cell>12.9</cell></row><row><cell>MCNN [4]</cell><cell>3.4</cell><cell>20.6</cell><cell>12.9</cell><cell>13.0</cell><cell>8.1</cell><cell>11.6</cell></row><row><cell>Ours (conventional density map)</cell><cell>4.0</cell><cell>20.9</cell><cell>15.8</cell><cell>12.6</cell><cell>5.6</cell><cell>11.8</cell></row><row><cell>Ours</cell><cell>4.1</cell><cell>21.7</cell><cell>11.9</cell><cell>11.0</cell><cell>3.5</cell><cell>10.5</cell></row><row><cell cols="2">patch to structured density map is trained for 100K iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">with a batch size of 100 and learning rate of 10 -4 . Finally,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">the entire network is initialized with these pre-trained weights</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and is trained for 300K iterations with a batch size of 40 and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">learning rate of 10 -5 . The input image patches are uniformly</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">resized to 128×128. For a fair comparison with other crowd</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">counting methods, we do not use pre-trained weights of other</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>deep learning models.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table IV reports the MAE and MSE errors of different methods on the Shanghaitech-B dataset. Following the convention of Zhang et al.</figDesc><table><row><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="3">COMPARISON OF DIFFERENT METHODS ON THE SHANGHAITECH-B</cell></row><row><cell cols="2">DATASET.</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">MAE MSE</cell></row><row><cell>LBP+RR</cell><cell>59.1</cell><cell>81.7</cell></row><row><cell>Crowd CNN [3]</cell><cell>32.0</cell><cell>49.8</cell></row><row><cell cols="2">MCNN-CCR [4] 70.9</cell><cell>95.9</cell></row><row><cell>MCNN [4]</cell><cell>26.4</cell><cell>41.3</cell></row><row><cell>Ours</cell><cell>20.2</cell><cell>35.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF DIFFERENT METHODS ON THE UCSD DATASET.</figDesc><table><row><cell>Method</cell><cell cols="2">MAE MSE</cell></row><row><cell>Kernel Ridge Regression [49]</cell><cell>2.16</cell><cell>7.45</cell></row><row><cell>Multi-output Ridge Regression [12]</cell><cell>2.25</cell><cell>7.82</cell></row><row><cell>Gaussian Process Regression [15]</cell><cell>2.24</cell><cell>7.97</cell></row><row><cell cols="2">Cumulative Attribute Regression [50] 2.07</cell><cell>6.86</cell></row><row><cell>Crowd CNN [3]</cell><cell>1.60</cell><cell>3.31</cell></row><row><cell>MCNN [4]</cell><cell>1.07</cell><cell>1.35</cell></row><row><cell>Ours</cell><cell>1.00</cell><cell>1.40</cell></row><row><cell cols="3">Table V reports the MAE and MSE errors of our method</cell></row><row><cell>and other methods</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF DIFFERENT METHODS ON THE UCF CC 50 DATASET.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. XX, NO. X, 201X</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work is supported in part by NSFC (61672456, U1509206, 61472353), the fundamental research funds for central universities in China, Zhejiang provincial engineering research center on media data cloud processing and analysis technologies, ZJU Converging Media Computing Lab, and the Alibaba-Zhejiang University Joint Institute of Frontier Technologies. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Shuicheng Yan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Xi Li is currently a full professor at the Zhejiang University, China. Prior to that, he was a senior researcher at the University of Adelaide, Australia. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detection of multiple, partially occluded humans in a single image by bayesian combination of edgelet part detectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="90" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape-based human detection and segmentation via hierarchical part-template matching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="604" to="618" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cross-scene crowd counting via deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Single-image crowd counting via multi-column convolutional neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="589" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian parsing via deep decompositional network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2648" to="2655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. NIPS</title>
		<meeting>Adv. NIPS</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Marked point processes for crowd counting</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf</title>
		<meeting>IEEE Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2913" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic adaptation of a generic pedestrian detector to a specific traffic scene</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3401" to="3408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Counting people in the crowd using a generic head detector</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Subburaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Descamps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Carincotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. AVSS</title>
		<meeting>IEEE Conf. AVSS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="470" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating the number of people in crowded scenes by mid based foreground segmentation and head-shoulder detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICPR</title>
		<meeting>IEEE ICPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On the efficacy of texture analysis for crowd monitoring</title>
		<author>
			<persName><forename type="first">A</forename><surname>Marana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lotufo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velastin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE SIBGRAPI</title>
		<meeting>IEEE SIBGRAPI</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="354" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature mining for localised crowd counting</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counting people with low-level features and bayesian regression</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2160" to="2177" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A viewpoint invariant approach for crowd counting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICPR</title>
		<meeting>IEEE ICPR</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1187" to="1190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-S</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crowd counting using multiple local features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE DICTA</title>
		<meeting>IEEE DICTA</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A mrf-based approach for real-time subway monitoring</title>
		<author>
			<persName><forename type="first">N</forename><surname>Paragios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1034</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gaussian process density counting from weak supervision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Borstel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="365" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Density-aware person detection and tracking in crowds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2423" to="2430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Example-based visual object counting with a sparsity constraint</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICME</title>
		<meeting>IEEE ICME</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast visual object counting via example-based density estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICIP</title>
		<meeting>IEEE ICIP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3653" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to count with regression forest and structured labels</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fiaschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICPR</title>
		<meeting>IEEE ICPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2685" to="2688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation</title>
		<author>
			<persName><forename type="first">V.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Okada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3253" to="3261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to count with cnn boosting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Walach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="660" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human detection in images via piecewise linear support vector machines</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="778" to="789" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human detection by quadratic classification on subspace of extended histogram of gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Satpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Eng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="287" to="297" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Shape-based pedestrian parsing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2265" to="2272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segmentation and tracking of multiple humans in crowded environments</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1198" to="1211" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fully convolutional neural networks for crowd segmentation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4464</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. NIPS</title>
		<meeting>Adv. NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multitask linear discriminant analysis for view invariant action recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5599" to="5611" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeply learned attributes for crowded scene understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4657" to="4666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Patch to the future: Unsupervised visual prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3302" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning driven visual path prediction from a single image</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5892" to="5904" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. NIPS</title>
		<meeting>Adv. NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An exploration of parameter redundancy in deep networks with circulant projections</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2857" to="2865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cnnpack: Packing convolutional neural networks in the frequency domain</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. NIPS</title>
		<meeting>Adv. NIPS</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="253" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Actionness estimation using hybrid fully convolutional networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2708" to="2717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Face recognition using kernel ridge regression</title>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. CVPR</title>
		<meeting>IEEE Conf. CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">His advisors are Prof. Zhongfei Zhang and Prof. Xi li. Earlier, he received his bachelor&apos;s degree in information and communication engineering from Zhejiang University</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Hangzhou, China; China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Science and Electronic Engineering at Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests are primarily in computer vision, machine learning and deep learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
