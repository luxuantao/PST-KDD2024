<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies</title>
				<funder ref="#_Ujz2Ney">
					<orgName type="full">European Research Council</orgName>
				</funder>
				<funder ref="#_tPGgPEN">
					<orgName type="full">ERC REACH)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-08-03">3 Aug 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
							<email>knutchen@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yusong</forename><surname>Wu</surname></persName>
							<email>wu.yusong@mila.quebec</email>
							<affiliation key="aff1">
								<orgName type="department">Quebec Artificial Intelligence Institute</orgName>
								<orgName type="institution">Universit? de Montr?al</orgName>
								<address>
									<settlement>Mila</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haohe</forename><surname>Liu</surname></persName>
							<email>haohe.liu@surrey.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">Centre for Vision Speech and Signal Processing</orgName>
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marianna</forename><surname>Nezhurina</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">LAION</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">San</forename><surname>Diego</surname></persName>
						</author>
						<title level="a" type="main">MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-03">3 Aug 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2308.01546v1[cs.SD]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation.. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text-guided generation tasks have gained increasing attention in recent years and have been applied to various modailties, including text-to-image, text-to-video, and text-to-audio generation. Textto-image generation has been used to create both realistic and stylized images based on textual descriptions, which can be useful in various scenarios including graphic design. Text-to-audio generation is a relatively new, but rapidly growing area, where the goal is to generate audio pieces, such as sound events, sound effects, and music, based on textual descriptions. Diffusion models have shown superior performance in these types of cross-modal generation tasks, including systems like DALLE-2 <ref type="bibr" target="#b28">[29]</ref> and Stable Diffusion <ref type="bibr" target="#b30">[31]</ref> for text-to-image; and AudioGen <ref type="bibr" target="#b21">[22]</ref>, AudioLDM <ref type="bibr" target="#b23">[24]</ref>, and Make-an-Audio <ref type="bibr" target="#b15">[16]</ref> for text-to-audio.</p><p>As a special type of audio generation, text-to-music generation has many practical applications <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>. For instance, musicians could use text-to-music generation to quickly build samples based on specific themes or moods and speed up their creative process. Amateur music lovers could leverage generated pieces to learn and practice for the purpose of musical education. However, text-to-music generation presents several specific challenges. One of the main concerns is the limited availability of text-music parallel training data <ref type="bibr" target="#b0">[1]</ref>. Compared to other modalities such as text-to-image, there are relatively few text-music pairs available, making it difficult to train a high-quality conditional model. Large and diverse training sets may be particularly imperative for music generation, which involves many nuanced musical concepts, including melody, harmony, rhythm, and timbre. Further, the effectiveness of diffusion models trained on more modest training sets has not been fully explored. Finally, a related concern in text-to-music generation is the risk of plagiarism or lack of novelty in generated outputs <ref type="bibr" target="#b0">[1]</ref>. Music is often protected by copyright laws, and generating new music that sounds too similar to existing music can lead to legal issues. Therefore, it is important to develop text-to-music models that can generate novel and diverse music while avoiding plagiarism, even when trained on relatively small training datasets.</p><p>In this paper, we focus on both these challenges: we develop a new model and training strategy for learning to generate novel text-conditioned musical audio from limited parallel training data. Currently, since there is no open-source model for text-to-music generation, we first construct a state-of-the-art text-to-music generation model, MusicLDM, which adapts the Stable Diffusion <ref type="bibr" target="#b30">[31]</ref> and AudioLDM <ref type="bibr" target="#b23">[24]</ref> architectures to the music domain. Next, to address the limited availability of training data and to encourage novel generations, we adapt an idea from past work in other modalities: mixup <ref type="bibr" target="#b40">[41]</ref>, which has been applied to computer vision and audio retrieval tasks, augments training data by recombining existing training points through linear interpolation. This type of augmentation encourages models to interpolate between training data rather than simply memorizing individual training examples, and thus may be useful in addressing data limitations and plagiarism in music generation. However, for music generation, the naive application of mixup is problematic. Simply combining waveforms from two distinct musical pieces leads unnatural and ill-formed music: tempos and beats (as well as other musical elements) are unlikely to match. Thus, we propose two novel mixup strategies, specifically designed for music generation: beat-synchronous audio mixup (BAM) and beat-synchronous latent mixup (BLM), which first analyze and beat-align training samples before interpolating between audio samples directly or encoding and then interpolating in a latent space, respectively.</p><p>We design new metrics that leverage a pretrained text and audio encoder (CLAP) to test for plagiarism and novelty in text-to-music generation. In experiments, we find that our new beat-synchronous mixup augmentation strategies, by encouraging the model to generate new music within the convex hull of the training data, substantially reduce the amount of copying in generated outputs. Further, our new model, MusicLDM, in combination with mixup, achieves better overall musical audio quality as well as better correspondence between output audio and input text. In both automatic evaluations and human listening tests, MusicLDM outperforms state-of-the-art models at the task of text-to-music generation while only being trained on 9K text-music sample pairs. Music samples and qualitative results are available at musicldm.github.io. Code and models are available at https://github.com/RetroCirce/MusicLDM/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-to-Audio Generation</head><p>Text-to-audio generation (TTA) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref> is a type of generative task that involves creating audio content from textual input. In years past, text-to-speech (TTS) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b34">35]</ref> achieved far better performance than other types of audio generation. However, with the introduction of diffusion models, superior performance in various generation tasks became more feasible. Recent work has focused on text-guided generation in general audio, with models such as Diffsound <ref type="bibr" target="#b39">[40]</ref>, AudioGen <ref type="bibr" target="#b21">[22]</ref>, AudioLDM <ref type="bibr" target="#b23">[24]</ref>, and Make-an-Audio <ref type="bibr" target="#b15">[16]</ref> showing impressive results. In the domain of music, text-to-music models include the retrieval-based MuBERT <ref type="bibr" target="#b25">[26]</ref>, language-model-based MusicLM <ref type="bibr" target="#b0">[1]</ref>, diffusion-based Riffusion <ref type="bibr" target="#b7">[8]</ref> and Noise2Music <ref type="bibr" target="#b14">[15]</ref>. However, a common issue with most recent text-to-audio/music models is the lack of open-source training code. Additionally, music models often requires large amounts of privately-owned music data that are inaccessible to the public, which makes it difficult for researchers to reproduce and build upon their work. Among all these models, AudioLDM is based on open-source Stable Diffusion <ref type="bibr" target="#b30">[31]</ref>, CLAP <ref type="bibr" target="#b38">[39]</ref>, and HiFi-GAN <ref type="bibr" target="#b18">[19]</ref> architectures. Therefore, we base our text-to-music generation model on the AudioLDM architecture, to create MusicLDM for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Plagiarism on Diffusion Models</head><p>Diffusion models have been shown to be highly effective at generating high-quality and diverse samples for text-to-image tasks. However, a potential issue with these models is the risk of plagiarism <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3]</ref>, or the generation novelty. As stated by <ref type="bibr" target="#b32">[33]</ref>, diffusion models are capable of memorizing and combining different image objects from training images to create replicas, which can lead to highly similar or even identical samples to the training data. <ref type="bibr" target="#b2">[3]</ref> explores different methods that could extract the training data with a generate-and-filter pipeline, showing that new advances in privacy-preserving training of diffusion models are required. Such issues are especially concerning in the domain of music, where copyright laws are heavily enforced and violations can result in significant legal and financial consequences. Therefore, there is a need to develop strategies to mitigate the risk of plagiarism in text-to-music generation using diffusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Mixup on Data Augmentation</head><p>Mixup <ref type="bibr" target="#b40">[41]</ref> is a widely used data augmentation technique that has shown remarkable success in improving model generalization and mitigating overfitting. The basic principle of mixup is to linearly combine pairs of training samples to effectively construct new samples that lie on the line connecting the original samples in the feature space, encouraging the model to learn a more continuous and robust decision boundary. In this paper, we explore the mixup technique in the context of text-to-music generation based on latent diffusion models. Different from the mixup in other modalities, music mixup involves a delicate balance of musical elements to prevent the mixed music from being chaotic noise. Moreover, in diffusion models, mixup also can refer to the combination of latent features, rather than music signals. We propose two mixup strategies tailored for music latent diffusion models and explore their potential benefits for data augmentation and generation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MusicLDM</head><p>As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, MusicLDM has similar architecture as AudioLDM: a contrastive languageaudio pretraining (CLAP) model <ref type="bibr" target="#b38">[39]</ref>, an audio latent diffusion model <ref type="bibr" target="#b23">[24]</ref> with a pretrained variational auto-encoder (VAE) <ref type="bibr" target="#b17">[18]</ref>, and a Hifi-GAN neural vocoder <ref type="bibr" target="#b18">[19]</ref>.</p><p>Formally, given an audio waveform x ? R T its corresponding text, where T is the length of samples, we feed the data into three modules:</p><p>1. We pass x through the audio encoder <ref type="bibr" target="#b4">[5]</ref> of CLAP f audio (?), to obtain the semantic audio embedding E a x ? R D , where D is the embedding dimension. 2. We pass the text of x through the text encoder <ref type="bibr" target="#b24">[25]</ref> of CLAP f text (?), to to obtain the semantic text embedding E t x ? R D . 3. We transform x into in the mel-spectrogram x mel ? R T ?F . Then we pass x mel into the VAE encoder, to obtain an audio latent representation y ? R C? T P ? F P , where T is the mel-spectrogram frame size, F is the number of mel bins, C is the latent channel size of VAE, and P is the downsampling rate of VAE. The VAE is pretrained to learn to encoder and decode the melspectrogram of music data.</p><p>In MusicLDM, the latent diffusion model has a UNet architecture where each encoder or decoder block is composed of a ResNet layer <ref type="bibr" target="#b10">[11]</ref> and a spatial transformer layer <ref type="bibr" target="#b30">[31]</ref>. During the training, the semantic embedding of the input audio E x is concatenated with the latent feature of each UNet encoder and decoder block by the FiLM mechanism <ref type="bibr" target="#b26">[27]</ref>. The output of the diffusion model is the estimated noise ? ? (z n , n, E x ) from n to (n -1) time step in the reverse process, where ? is the parameter group of the diffusion model, and z n is the n-step feature generated by the forward process. We adopt the training objective <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref> as the mean square error (MSE) loss function:</p><formula xml:id="formula_0">L n (?) = E z0,?,n ||? -? ? (z n , n, E x )|| 2 2 (1)</formula><p>where z 0 = y is the audio latent representation from VAE (i.e., the groundtruth), and ? is the target noise for training. More details regarding the training and the architecture of the latent diffusion model can be referred in Appendix A.</p><p>For MusicLDM, we make two changes from the original AudioLDM to enhance its performance on text-to-music generation. First, since the original contrastive language-audio pretraining (CLAP) model is pretrained on text-audio pair datasets dominated by sound events, sound effects and natural sounds, we retrained the CLAP on text-music pair datasets (details in Appendix B) to improve its understanding of music data and corresponding texts. We also retrained the Hifi-GAN vocoder on music data to ensure high-quality transforms from mel-spectrograms to music waveforms. Second, in the original AudioLDM, the model is only fed with audio embeddings as the condition during the training process, i.e., E x = E a x ; and it is fed with text embeddings to perform the text-to-audio generation, i.e., E x = E t</p><p>x . This approach leverages the alignment of text and audio embeddings inside CLAP to train the latent diffusion model with more audio data without texts. However, this audio-to-audio training ? ? (z n , n, E a</p><p>x ) is essentially an approximation of the text-to-audio generation ? ? (z n , n, E t x ). Although CLAP is trained to learn joint embeddings for text and audio, it does not explicitly enforce the embeddings to be distributed similarly in the latent space, which can make it challenging for the model to generate coherent text-to-audio outputs solely with audio-to-audio training. This problem becomes more severe when the available text-music pair data is limited. Moreover, relying solely on audio embeddings ignores the available text data, which means that we are not leveraging the full potential of our dataset. Consequently, generating accurate and realistic text-to-audio generations may not be effective.</p><p>To further investigate this task, we introduce two additional training approaches for comparison:</p><p>1. Train the MusicLDM directly using the text embedding as the condition, i.e., ? ? (z n , n, E t x ) 2. Train the MusicLDM using the audio embedding as the condition, then finetune it with text embedding, i.e.,?</p><formula xml:id="formula_1">? (z n , n, E a x ) ? ? ? (z n , n, E t x )</formula><p>The first approach follows the original target of text-to-audio, serving as a comparison with audio-toaudio training. The second approach is proposed as an improvement on audio-to-audio generation. as we shift the condition distribution from the audio embedding back to the text embedding during the training of the diffusion model. In section 4.2, we compared the above two approaches with the original audio-to-audio training approaches to determine the best approach for generating high-quality and highly correlated text-to-music outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Beat-Synchronous Mixup</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we propose two mixup strategies to augment the data during the MusicLDM training: Beat-Synchronous Audio Mixup (BAM) and Beat-Synchronous Latent Mixup (BLM). Beat-tracking via Beat Transformer Musical compositions are made up of several elements, such as chord progressions, timbre, and beats. Of these, beats play a crucial role in determining the musical structure and alignment. In most audio retrieval tasks, mixup is a popular technique that involves randomly mixing pairs of audio data to augment the training data. However, when mixing two music samples that have different tempos (beats per minute), the mixture can be chaotic and unappealing. To avoid this, we use a state-of-the-art beat tracking model, Beat Transformer <ref type="bibr" target="#b41">[42]</ref>, to extract the tempo and downbeat map of each music track, as shown in the left of Figure <ref type="figure" target="#fig_1">2</ref>. We categorize each music track into different tempo groups and during training, we only mixed tracks within the same tempo group to ensure the tracks were in similar tempos. Furthermore, we align the tracks by comparing their downbeat maps and selecting a certain downbeat to serve as the starting position for the mixup track. This preprocessing approach allows us to better select the music data available for mixup, resulting in mixup tracks that are neatly ordered in terms of tempo and downbeats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Beat-Synchronous Audio Mixup</head><p>As depicted in the upper part of Figure <ref type="figure" target="#fig_1">2</ref>, once we select two aligned music tracks x 1 and x 2 , we mix them by randomly selecting a mixing ratio from the beta distribution ? ? B(5, 5), as:</p><formula xml:id="formula_2">x = ?x 1 + (1 -?)x 2<label>(2)</label></formula><p>We then use the mixed data x to obtain the CLAP embedding E x and the audio latent variable y.</p><p>We train the latent diffusion model using the standard pipeline. This beat-synchronous audio mixup strategy is referred to as BAM.</p><p>Beat-Synchronous Latent Mixup As depicted in the lower part of Figure <ref type="figure" target="#fig_1">2</ref>, in the latent diffusion model, the mixup process can also be applied on the latent variables, referred as beat-synchronous latent mixup (BLM). After selecting two aligned music tracks x 1 and x 2 , we feed them into the VAE encoder to obtain the latent variables y 1 and y 2 . We then apply the mixup operation to the latent variables:</p><formula xml:id="formula_3">y = ?y 1 + (1 -?)y 2<label>(3)</label></formula><p>In contrast to BAM, BLM applies the mixup operation to the latent space of audio, where we cannot ensure that the mixture of the latent variables corresponds to the actual mixture of the music features in the appearance. Therefore, we first generate a mixed mel-spectrogram x mel by feeding the mixed latent variable y into the VAE decoder. Then, we feed x mel to the Hifi-GAN vocoder to obtain the mixed audio x as the input music. With x and y, we follow the pipeline to train the MusicLDM.</p><p>What are BAM and BLM doing? As shown in the right of Figure <ref type="figure" target="#fig_1">2</ref>, we demonstrate the interpolation between the feature space of audio when using BAM and BLM. In the feature space of audio signals, the "?" represents the feature point of music data, while the "?" denotes the feature point of other audio signals, such as natural sound, audio activity, and noise. During the pretraining process of VAE, a latent space is constructed for encoding and decoding the music data. The VAE aims to learn the distribution of the latent variables that can best represent the original data and transform the original feature space into a lower-dimensional manifold. This manifold is designed to capture the underlying structure of the music data. Therefore, any feature point within this manifold is considered to be a valid representation of music.</p><p>BAM and BLM are concerned with augmenting the data at different levels of feature space. As shown in right of Figure <ref type="figure" target="#fig_1">2</ref>, BAM linearly combines two points in audio space to form a new point on the red line. BLM, represented by the blue line, performs a similar operation, but result in a new point in the VAE-transformed latent space, which will be decoded back onto the music manifold of audio space.</p><p>Both BAM and BLM offer unique advantages and disadvantages. BAM applies mixup in the original feature space, resulting in a smooth interpolation between feature points. However, BAM cannot ensure a reasonable music sample that lies within the music manifold. This issue is more problematic using the simple audio mixup strategy without tempo and downbeat alignments. BLM, conversely, augments within the music manifold, fostering robust and diverse latent representations. However, BLM is computationally more expensive as it requires computing the latent feature back to audio via VAE decoder and Hifi-GAN. Furthurmore, when the ill-defined or collapsed latent exists in VAE, BLM may be out of effectiveness.</p><p>Both BAM and BLM are effective data augmentation techniques that encourage the model to learn a more continuous and robust decision boundary on the audio feature space, or implicitly from the latent space to the audio space, which can improve the model's generalization performance and mitigate overfitting. In the context of text-to-music generation, these mixup strategies can have a potential to mitigate the limitations of data size and help avoid plagiarism issues. By introducing small variations through mixup, the model can touch a more rich space of music data and generate music samples that are correlated to the texts but show differences to the original training data. In Section 4.2, we evaluated whether these strategies mitigate the data limitation and plagiarism issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conducted four experiments on our proposed methods. First, we retrained a new CLAP model to provide the condition embedding for MusicLDM. Second, we trained MusicLDM with different mixup strategies and compared them with available baselines. Third, we evaluated MusicLDM in terms of text-music relevance, novelty and plagiarism risk via metrics based on CLAP scores. Finally, we conducted a subjective listening test to give an additional evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Dataset The original CLAP model trained on mostly acoustic events and sound effect datasets.</p><p>In Following evaluation techniques used in past work on audio generation <ref type="bibr" target="#b23">[24]</ref>, we use frechet distance (FD), inception score (IS), and kullback-leibler (KL) divergence to evaluate the quality of generated musical audio outputs. Frechet distance evaluates the audio quality by using an audio embedding model to measure the similarity between the embedding space of generations and that of targets. In this paper, we use two standard audio embedding models: VGGish <ref type="bibr" target="#b11">[12]</ref> and PANN <ref type="bibr" target="#b19">[20]</ref>. The resulting distances we denote as F D vgg and F D pann , respectively. Inception score measures the diversity and the quality of the full set of audio outputs, while KL divergence is measured on individual pairs of generated and groundtruth audio samples and averaged. We use the audioldm_eval library<ref type="foot" target="#foot_0">1</ref> to evaluate all the metrics mentioned above, comparing the groundtruth audio from the Audiostock 1000-track test set with the 1000 tracks of music generated by each system based on the corresponding textual descriptions.</p><p>Table <ref type="table" target="#tab_0">1</ref> presents the FD, IS, and KL results for our models in comparison with baseline models.</p><p>In the first section, we utilized textual descriptions from the test set, sending them to the offical APIs of Riffusion and MuBERT to generate corresponding results. Both Riffusion and MuBERT were unable to achieve results comparable to the remaining models. Upon reviewing the generated music from the two systems, we found that the sub-optimal performance of Riffusion resulted from poor music generation quality, with many samples either inaudible or outside the desired musical range. MuBERT, while generating high-quality pieces from real music sample libraries, fell short in replicating the distribution of Audiostock dataset. Due to the unavailability of their detailed architectures, training scripts, and data, Riffusion and MuBERT's evaluations offered only partial comparisons.</p><p>We also retrained the original AudioLDM model on the Audiostock dataset, comparing it to Musi-cLDM variants. The distinction between AudioLDM and MusicLDM lies in the different CLAP models used for condition embeddings. Our comparison revealed that MusicLDM outperforms AudioLDM in terms of F D pann , F D vgg , and IS. This underscores the efficacy of the novel CLAP model pretrained for music, providing more suitable music embeddings as conditioning information.</p><p>Comparing MusicLDM's performance with audio-to-audio training (? ? (z n , n, Ex a )) and text-toaudio training (? ? (z n , n, Ex t )), denoted by "MusicLDM (Only TA-Training)", we note inferior results in the latter approach. This may be suggests that a gap exists between distribution of text embedding and audio embedding, making it challenging for the diffusion model to generate highquality audio solely from text embedding. In contrast, CLAP's audio embedding may leak low-level audio information to the diffusion model during initial training stages, hurting the model's ability to generalize to text embedding inputs. This hypothesis is further supported by the results of MusicLDM with combined audio-to-audio training and text-to-audio fine-tuning. We observe a significant decrease in F Dvgg with small changes in F Dpann and IS, indicating a substantial improvement in music generation quality, driven by leveraging both audio and text embeddings during training. The former facilitates good audio reconstruction during early training, while the latter shifts the distribution from audio to text to align with the eventual test-time task of text-to-music generation.  Last, we compared MusicLDM with different mixup strategies, namely simple mixup <ref type="bibr" target="#b40">[41]</ref>, BAM, and BLM. The comparison reveals the negative impact of the simple mixup on all metrics. This degradation in generated sample quality, characterized by instrumental interference and noise, is attributed to the simple mixup's inability to guarantee musicality in the mix. Similar observations are evident in the BAM results, indicated by a drop in F D pann and IS. However, BAM's tempo and downbeat alignment, along with the original mixup benefits, counterbalance this defect to a certain extent, enhancing the model's generalization ability and improving certain metrics. BLM, as a latent space mixing method, aligns with our hypothesis in Section 3.2 that latent space mixup yield audio closely resembling music. This technique allows us to largely bypass the potential confusion issues tied to audio mixing, thus capitalizing on mixup's ability to drive generalization and prevent copying via data augmentation. Furthermore, incorporating text-finetuning results in a comprehensive improvement of music generation quality, solidifying BLM as the most effective strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Text-Audio Relevance, Novelty and Plagiarism</head><p>We proposed two metric groups, text-audio similarity and nearest-neighbor audio similarity ratio to assess text-audio relevance, novelty, and plagiarism risk in various models.</p><p>First, text-audio similarity measures the relevance between the text and the audio. It is defined as the dot product between the groundtruth text embedding E t gd from the test set and the audio embedding E a from music generated by models, i.e., E t gd ? E a . The embeddings from both text and audio are normalized in CLAP model, thus the dot product computes the cosine similarity between text and audio embeddings.</p><p>Second, we would also like to measure the extent to which models are directly copying samples from the training set. We verify this by first computing the dot products between the audio embedding of each generated music output to all audio embeddings from the Audiostock training set and returning the maximum -i.e., the similarity of the nearest-neighbor in the training set. Then, we compute the fraction of generated outputs whose nearest-neighbors are above a threshold similarity. We refer this as the nearest-neighbor audio similarity ratio, providing SIM AA @90 where the threshold is 0.9, and SIM AA @95 with 0.95. The lower this fraction, the lower the risk of plagiarism -i.e., fewer samples have very close training neighbors. In the Appendix D, we show pairs of examples with both high and low similarity scores to give further intuition for this metric.</p><p>As shown in the left and middle column of Table <ref type="table" target="#tab_1">2</ref>, we present the average text-audio similarity and nearest-neighbor audio similarity ratios for two thresholds on the 1000 tracks in the Audiostock test set and the generated music from MuBERT and different variants of MusicLDM. We also provide two reference points for text-audio similarity: "Test Set" and "Retrieval Max". Specifically, "Test Set" refers to computing the cosine similarity between the groudtruth text embedding and the groudtruth audio embedding. And "Retrieval Max" refers to first computing the cosine similarities between each text embedding from the test set to all audio embeddings from the training set, then picking the highest score as the score of this text, and taking the average over all text scores. We can observe that the original MusicLDM without mixup achieves the highest text-audio relevance with an average score of 0.281, but also the highest (worst) nearest-neighbor audio similarity ratio. MusicLDM with the simple mixup strategy achieves the lowest SIM AA @90 ratio while sacrificing a lot in the relevance of the generation. The MusicLDM with BAM and BLM achieve a balance between the audio similarity ratios and the text-to-audio similarity. In combination with the quality evaluation results in Table <ref type="table" target="#tab_0">1</ref>, we can conclude that all mixup strategies are effective as a data augmentation techniques to improve generalization of the model to generate more novel music. However simple mixup degrades the generation quality, which affects the relevance score between audio and text, and also thus makes it less similar to the tracks in the training set. BAM and BLM apply the tempo and downbeat filtering on the music pairs to mix, allowing the model to maintain superior generation quality (Table <ref type="table" target="#tab_0">1</ref>) and text-audio relevance (Table <ref type="table" target="#tab_1">2</ref>), while still utilizing the benefit brought by the mixup technique to make the generation more novel (less plagiarism). Among the objective metrics, BLM is the best mixup strategy in terms of quality, relevance and novelty of the generated audio. This indicates mixing in the latent space is more efficient than mixing directly in audio space, perhaps because the latent embedding approach implicitly projects the mixture to the learned manifold of well-formed music. We show the detailed distribution of these metrics over 1000 generated tracks in Figure <ref type="figure" target="#fig_2">3</ref>, where, for example, audio-audio similarity denotes the individual scores used to calculate the average SIM AA . We find that the original MusicLDM without mixup has more samples with high training similarity than other models, which further reflects that it is more prone to copying.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Subjective Listening Test</head><p>As shown in the right of Table <ref type="table" target="#tab_1">2</ref>, we conduct the subjective listening test on four models, namely MuBERT, the original MusicLDM, and that with BAM or BLM strategy, to further evaluate the actual hearing experience of the generated music. We do not include the simple mixup MusicLDM because its generation is at a low quality while we avoid confusing subjects with too many models in the same time. We invite 15 subjects to listen to 6 groups of the generations randomly selected from the test set. Each of group contains four generations from four models and the corresponding text description. The subjects are required to rate the music in terms of quality, relevance, and musicality (detailed in Appendix E). We observe that the samples of MusicLDM with BAM or BLM mixup strategy achieve a better relevance and quality than those of MuBERT and the original MusicLDM, this strengths our above analysis. The MuBERT samples achieve the best Musicality, because its generation is combined from the real music samples. Combined with the objective metrics, beat-synchronous latent mixup stands to be the most effectiveness method for enhancing the text-to-music generation in terms of quality, text-music relevance and novelty (i.e., reducing the risk of plagiarism).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations</head><p>In this section we outline the recognized limitations of our study, serving as a roadmap for future improvements. Firstly, MusicLDM is trained on the music data in a sampling rate of 16 kHz, while most standard music productions are 44.1 kHz. This constraint, tied to the Hifi-GAN vocoder's subpar performance at high sampling rates, impedes practical text-to-music application and necessitates further improvements. Secondly, resource constraints such as limited real text-music data and GPU processing power prevent us from scaling up MusicLDM's training. We are unable to determine if mix-up strategies could yield similar trends as observed with the Audiostock dataset. This issue exists in the image generation task as well. Lastly, while we recognize beat information as crucial for music alignment, there is scope for exploring other synchronization techniques like key signature and instrument alignment. We also intend to investigate the application of different audio space filters to select suitable music pairs for mixing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduce MusicLDM, a text-to-music generation model that incorporates CLAP, VAE, Hifi-GAN, and latent diffusion models. We enhance MusicLDM by proposing two efficient mixup strategies: beat-synchronous audio mixup (BAM) and beat-synchronous latent mixup (BLM), integrated into its training process. We conduct comprehensive evaluations on different variants of MusicLDM using objective and subjective metrics, assessing quality, text-music relevance, and novelty. The experimental results demonstrate the effectiveness of BLM as a standout mixup strategy for text-to-music generation.  <ref type="bibr" target="#b22">[23]</ref> and 1 spatial transformer layer <ref type="bibr" target="#b36">[37]</ref>. The channel dimensions of encoder blocks are 128, 256, 384, and 640 and reversed in decoder blocks. For Hifi-GAN, we adopt its official repository <ref type="foot" target="#foot_2">3</ref> along with the configuration <ref type="foot" target="#foot_3">4</ref> . We change the number of mel-bins to 128 to fit the processing of MusicLDM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation and Training Details</head><p>For the training of VAE, we use the Adam optimizer with a learning rate of 4.5 ? 10 -<ref type="foot" target="#foot_5">6</ref> with a batch size of 24. We apply the mel-spectrogram loss, adversarial loss, and a Gaussian constraint loss as the training object of VAE. For the training of Hifi-GAN, we use the batch size of 96 and the AdamW optimizer with ? 1 = 0.8, ? 2 = 0.99 at the learning rate of 2 ? 10 -4 . For the training of MusicLDM, we use the batch size of 24 and the AdamW optimizer with the basic learning of 3 ? 10 -<ref type="foot" target="#foot_4">5</ref> . In the forward process, we use 1000-step of a linear noise schedule from ? 1 = 0.0015 to ? 1000 = 0.0195. In the sampling process, we use the DDIM <ref type="bibr" target="#b33">[34]</ref> sampler with 200 steps. We adopt the classifier-free guidance <ref type="bibr" target="#b13">[14]</ref> with a guidance scale w = 2.0. When applying the mixup strategy, we use the mixup rate p = 0.5. The CLAP model is trained on 24 A100 GPUs. The VAE and HifiGAN model are trained on 4 A60 GPUs. Last, the latent diffusion model is trained on single NVIDIA A40. All models are converged at the end of the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation of Comparison Model</head><p>For generating from Riffusion and MuBERT, we use the official API of Riffusion 5 and MuBERT 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B CLAP Details B.1 Hyperparameters</head><p>For model hyperparameters, we refer to the official repository <ref type="foot" target="#foot_6">7</ref> to conduct the training process of CLAP. The audio encoder of CLAP is HTS-AT-base <ref type="bibr" target="#b4">[5]</ref> and the text encoder is RoBERTa-base <ref type="bibr" target="#b24">[25]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Training Details</head><p>For the training of CLAP, we use the batch size of 2304 and the Adam <ref type="bibr" target="#b16">[17]</ref> optimizer with ? 1 = 0.99, ? 2 = 0.9 with a warm-up <ref type="bibr" target="#b8">[9]</ref> and cosine learning rate decay at a basic learning rate of 1 ? 10 -4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Zero-shot Classification Performance</head><p>We follow previous works on audio-language contrastive learning <ref type="bibr" target="#b8">[9]</ref> to evaluate the performance of CLAP on the zero-shot audio classification tasks, namely on the benchmark datasets of ESC-50 <ref type="bibr" target="#b27">[28]</ref>, UrbanSound 8K <ref type="bibr" target="#b31">[32]</ref>, and VGGSound <ref type="bibr" target="#b3">[4]</ref>. To demonstrate that the retrained CLAP involves more understandings of music data, we further add a music genre classification benchmark dataset GTZAN <ref type="bibr" target="#b35">[36]</ref> into the evalution. As shown in Table <ref type="table" target="#tab_2">3</ref>, our retained CLAP achieves best performance acoustic event classification in UrbanSound 8K and VGGSound dataset, while still maintaining comparable performance in ESC-50 dataset and on par performance in GTZAN music classification dataset.</p><p>Although the performance on GTZAN music dataset is not improved, the extra data used for training CLAP might result in a better representation space which is beneficial for text-to-music generation model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Nearest-Neighbor Audio Similarity Samples</head><p>As mentioned in section 4.2.2, we introduced the computation of the nearest-neighbor audio similarity ratio by comparing the cosine similarity between generated music and music tracks in the training set of Audiostock.</p><p>In this section, we provide visualizations of the similarity between the generated music and the training music using spectrograms, showcasing how well the cosine similarity between CLAP audio embeddings captures this similarity.</p><p>As shown in Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref>, display both three examples of music pairs with high and low similarity. To achieve this, we divided the training music tracks into 10-second segments and determined the most similar segment to the generated music track (i.e., the query track).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Music Tracks in the Training Set</head><p>Generated Samples from Models Similarity=0.803 Similarity=0.813 Similarity=0.820 For instances with high similarity, the cosine similarity of CLAP audio embeddings reveals highly similar structural patterns, indicating a close resemblance in the music arrangements. Conversely, low CLAP cosine similarity indicates significant differences between the spectrograms of the generated music and the training music. This demonstrates the effectiveness of CLAP embeddings in assessing the similarity between music tracks and serving as a means to detect novelty and potential instances of plagiarism in the generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Subjective Listening Test</head><p>The subjective listening test was conducted in an online survey format to gather feedback and insights on the text-to-music generation using MusicLDMs and MuBERT. The generation of Riffusion was not included due to its lower quality and relevance compared to the standard. The test had an estimated duration of approximately 10 minutes.</p><p>At the beginning of the test, participants were asked to provide their age range and music background as metadata. Subsequently, participants were randomly assigned six groups of generated songs. Each group consisted of four songs generated from MusicLDM, MusicLDM with BAM, MusicLDM with BLM, and MuBERT, all based on the same textual description. The order of the songs within each group was shuffled to eliminate positional bias during rating. Participants were required to rate each song based on three metrics:</p><p>? Relevance: Determine how well the song matches the given music description. Rate the song based on how closely it aligns with the provided description.</p><p>? Quality: Assess the overall quality of the music. Consider factors such as clarity, absence of noise, and general audio quality. Rate the song based on these criteria.</p><p>? Musicality: Evaluate the musical attributes of the song, including rhythm, melodies, and textures. Rate the song based on its overall musical appeal.</p><p>Each song in the subjective listening test had a duration of approximately 10 seconds and included a fade-in and fade-out to mitigate bias from the song's beginning and ending sections. The rating scale used for evaluating the songs was designed such that a higher score indicates better quality.</p><p>Participants were asked to rate each song based on the provided metrics, taking into account the song's overall quality, relevance to the given text, and personal preference on its musicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Broader Impact</head><p>The development and implementation of MusicLDM, or generally a text-to-music generation model offers potential benefits and also raises concerns that must be addressed responsibly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive Impacts</head><p>? Promoting Creativity: This model can serve as a tool to augment human creativity. Artists, composers, and music amateurs can use it to transfer their textual ideas into music, broadening the realm of artistic exploration and making music creation more accessible and convenient. ? Cultural Preservation and Evolution: The model provides a unique platform to archive, digitize, and even evolve cultural musical expressions. Textual descriptions of traditional and folk music can be transformed into the actual music, thereby helping to preserve heritage while simultaneously allowing for creative adaptations. Literature, such as poetry, can be interpreted as music to explore more relations between different types of cultural expression forms. ? Education and Research: In academia, this model can be used as a pedagogical tool in music education. It can aid in understanding the complex relationship between music and linguistic structures, enriching interdisciplinary research in musicology, linguistics, and artificial intelligence. ? Entertainment Industry Innovation: The entertainment industry could use this model to generate soundtracks for movies, games, and other media based on scripts. This could potentially revolutionize the way music is produced for media, reducing time and costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Impacts</head><p>? Artistic Job Displacement: While this model can augment human creativity, it may also lead to job losses in the music industry if widely adopted for composing and production. The model could potentially replace human composers in certain contexts, particularly in industries such as film and gaming that require a significant amount of background music. ? Copyright Issues: In this paper, one of the targets is to mitigate the copyright issues and plagiarism. The generated music could unintentionally resemble existing works, raising complex copyright infringement issues. It is crucial to implement measures to ensure that the model does not violate intellectual property rights. ? Ethical Misuse: The model could be misused to create music promoting hate speech, misinformation, or other harmful content if the input text has such characteristics. Thus, it is essential to develop safeguards to mitigate the risk of misuse. ? Cultural Appropriation and Homogenization: While the model can help preserve music, there is a risk of homogenizing unique cultural musical styles or misappropriating them without proper credit or context.</p><p>The design and application of this model should be carried out responsibly, considering the potential ethical, social, and economic consequences. Balancing its many benefits with its potential downsides will require the collective effort of developers, users, policy makers, and society at large.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The architecture of MusicLDM, which contains a contrastive language-audio pretraining (CLAP) model, an audio latent diffusion model with VAE, and a Hifi-GAN nerual vocoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Mixup strategies. Left: tempo grouping and downbeat alignment via Beat Transformer. Middle: BAM and BLM mixup strategies. Right: How BAM and BLM are applied in the feature space of audio signals and audio latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The violin plot of the audio-audio similarity, and the text-to-audio similarity.</figDesc><graphic url="image-12.png" coords="8,63.96,214.53,484.81,121.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The spectrograms of music pairs indicated by high cosine similiarity score of CLAP audio embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The spectrograms of music pairs indicated by low cosine similarity score of CLAP audio embeddings.</figDesc><graphic url="image-19.png" coords="15,108.46,176.96,120.49,60.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>this work, we trained a CLAP model on music datasets in addition to its original training data, allowing it to better understand the relation between music and textual descriptions. The new CLAP model is trained on dataset of 2.8 Million text-audio pairs, in an approximate total duration of 20 000 hours. Compared to previous CLAP model, the newly trained CLAP model performs well in zero-shot classification for both acoustic event and music. Please refer further details on training and performance of the new CLAP in Appendix B. For MusicLDM, we used the Audiostock dataset for training, along with VAE and Hifi-GAN. Specifically, the Audiostock dataset contains 9000 music tracks for training and 1000 tracks for testing. The total duration is 455.6 hours. It provides a correct textual description of each music track. Although CLAP is trained on more text-music data pairs, the large number of them are pseudo-captions composed primarily of non-specific metadata, such as author, song title, and album information (e.g., [a song by author A from the album B]). These captions do not align with our specific objective of text-to-music generation.Hyperparameters and Training DetailsWe trained all MusicLDM modules with music clips of 10.24 seconds at 16 kHz sampling rate. In both the VAE and diffusion model, music clips are The evaluation of generation quality among MusicLDMs and baselines. AA-Train. and TA-Train. refer to the audio-audio training scheme and the text-audio traning scheme.Model AA-Train. TA-Train. FD pann ? FD vgg ? Inception Score ? KL Div. ?</figDesc><table><row><cell>Riffusion [8]</cell><cell>?</cell><cell>?</cell><cell>68.95</cell><cell>10.77</cell><cell>1.34</cell><cell>5.00</cell></row><row><cell>MuBERT [26]</cell><cell>-</cell><cell>-</cell><cell>31.70</cell><cell>19.04</cell><cell>1.51</cell><cell>4.69</cell></row><row><cell>AudioLDM</cell><cell>?</cell><cell>?</cell><cell>38.92</cell><cell>3.08</cell><cell>1.67</cell><cell>3.65</cell></row><row><cell>MusicLDM</cell><cell>?</cell><cell>?</cell><cell>26.67</cell><cell>2.40</cell><cell>1.81</cell><cell>3.80</cell></row><row><cell>MusicLDM (Only TA-Training)</cell><cell>?</cell><cell>?</cell><cell>32.40</cell><cell>2.51</cell><cell>1.49</cell><cell>3.96</cell></row><row><cell>MusicLDM w/. mixup</cell><cell>?</cell><cell>?</cell><cell>30.15</cell><cell>2.84</cell><cell>1.51</cell><cell>3.74</cell></row><row><cell>MusicLDM w/. BAM</cell><cell>?</cell><cell>?</cell><cell>28.54</cell><cell>2.26</cell><cell>1.56</cell><cell>3.50</cell></row><row><cell>MusicLDM w/. BLM</cell><cell>?</cell><cell>?</cell><cell>24.95</cell><cell>2.31</cell><cell>1.79</cell><cell>3.40</cell></row><row><cell>MusicLDM w/. Text-Finetune</cell><cell>?</cell><cell>?</cell><cell>27.81</cell><cell>1.75</cell><cell>1.76</cell><cell>3.60</cell></row><row><cell>MusicLDM w/. BAM &amp; Text-Finetune</cell><cell>?</cell><cell>?</cell><cell>28.22</cell><cell>1.81</cell><cell>1.61</cell><cell>3.61</cell></row><row><cell>MusicLDM w/. BLM &amp; Text-Finetune</cell><cell>?</cell><cell>?</cell><cell>26.34</cell><cell>1.68</cell><cell>1.82</cell><cell>3.47</cell></row><row><cell>4.2 MusicLDM Performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.2.1 Generation Quality</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>represented as mel-spectrograms with T = 1024 frames and F = 128 mel-bins. Unlike AudioLDM, MusicLDM's VAE utilizes a downsampling rate of P = 8 and a latent dimension of C = 16. The architecture of MusicLDM's latent diffusion model follows that of AudioLDM. The training process of MusicLDM aligns with AudioLDM's approach. For additional hyperparameters and training details, refer to Appendix A.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The objective metrics to measure the relevance and novelty (plagiarism). And the subjective listening test to evaluate the quality, relevance, and musicality. Audio Similarity? SIM AA @90 ? SIM AA @95 ? Quality? Relevance? Musicality?</figDesc><table><row><cell>Model</cell><cell>Relevance</cell><cell cols="2">Objective Metrics Novelty and Plagiarism Risk</cell><cell cols="3">Subjective Listening Test</cell></row><row><cell>Text-Test Set (Ref.)</cell><cell>0.325</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Retrieval Max (Ref.)</cell><cell>0.423</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MuBERT</cell><cell>0.131</cell><cell>0.107</cell><cell>0</cell><cell>2.02</cell><cell>1.50</cell><cell>2.33</cell></row><row><cell>MusicLDM (original)</cell><cell>0.281</cell><cell>0.430</cell><cell>0.047</cell><cell>1.98</cell><cell>2.17</cell><cell>2.19</cell></row><row><cell>MusicLDM w/. mixup</cell><cell>0.234</cell><cell>0.391</cell><cell>0.028</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MusicLDM w/. BAM</cell><cell>0.266</cell><cell>0.402</cell><cell>0.027</cell><cell>2.04</cell><cell>2.21</cell><cell>2.01</cell></row><row><cell>MusicLDM w/. BLM</cell><cell>0.268</cell><cell>0.401</cell><cell>0.020</cell><cell>2.13</cell><cell>2.31</cell><cell>2.07</cell></row><row><cell cols="3">Audio-Audio Similarity</cell><cell></cell><cell cols="2">Text-Audio Similarity</cell><cell></cell></row><row><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MusicLDM (mix-up)</cell><cell cols="2">MusicLDM (original)</cell><cell>MusicLDM (BLM)</cell><cell cols="2">MusicLDM (BAM)</cell><cell>MuBERT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of zero-shot classification performance of the CLAP used in this work with previous audio-language contrastive learning models. For audio signal processing, we use the sampling rate of 16 kHz to convert all music samples for the training of MusicLDM. Each input data is a chunk of 10.24 seconds randomly selected from the dataset, i.e., L = 163840. We use the hop size 160, the window size 1024, the filter length 1024, and the number of mel-bins 128 to compute the short-time Fourier transform (STFT) and mel-spectrograms. As the result, the input mel-spectrogram has the time frame T = 1024 and the mel-bins F = 128.We adopt a convolutional VAE as the latent audio representation model, consisting of a 4-block downsampling encoder and a 4-block upsampling decoder. The downsampling and upsampling rate P = 8 and the latent dimension C = 16, i.e., the bottleneck latent variable y has a shape of (C ? T P ? F P ) = (16 ? 128 ? 16). For the latent diffusion model, we refer the UNet latent diffusion model 2 . It contains 4 encoder blocks, 1 bottleneck block, and 4 decoder blocks. Each block contains 2 residual CNN layers</figDesc><table><row><cell></cell><cell cols="4">ESC-50 US8K VGGSound GTZAN</cell></row><row><cell>Wav2CLIP [38]</cell><cell>41.4</cell><cell>40.4</cell><cell>10.0</cell><cell>-</cell></row><row><cell>audioCLIP [10]</cell><cell>68.6</cell><cell>68.8</cell><cell>-</cell><cell>-</cell></row><row><cell>CLAP (Elizalde et al. [6])</cell><cell>82.6</cell><cell>73.2</cell><cell>-</cell><cell>25.2</cell></row><row><cell>CLAP (Wu et al. [39])</cell><cell>91.0</cell><cell>77.0</cell><cell>46.2</cell><cell>71.0</cell></row><row><cell>CLAP (ours on music data)</cell><cell>90.1</cell><cell>80.6</cell><cell>46.6</cell><cell>71.0</cell></row><row><cell>A MusicLDM Details</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Hyperparameters</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/haoheliu/audioldm_eval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/spaces/multimodalart/latentdiffusion</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/jik876/hifi-gan</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/jik876/hifi-gan/blob/master/config_v1.json</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/riffusion/riffusion-model-v1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/MubertAI/Mubert-Text-to-Music</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/LAION-AI/CLAP</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgments</head><p>We would like to thank the <rs type="institution">Institute for Research and Coordination in Acoustics and Music (IRCAM)</rs> and Project <rs type="projectName">REACH: Raising Co-creativity in Cyber-Human Musicianship</rs> for supporting this project. This project has received funding from the <rs type="funder">European Research Council</rs> (<rs type="funder">ERC REACH)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (Grant Agreement #<rs type="grantNumber">883313</rs>). We would like to thank the support of computation infrastructure from LAION.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Ujz2Ney">
					<orgName type="project" subtype="full">REACH: Raising Co-creativity in Cyber-Human Musicianship</orgName>
				</org>
				<org type="funding" xml:id="_tPGgPEN">
					<idno type="grant-number">883313</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Agostinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zal?n</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Borsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Verzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Caillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Tagliasacchi</surname></persName>
		</author>
		<idno>arXiv preprint:2301.11325</idno>
		<title level="m">Generating music from text</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep learning techniques for music generation</title>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>Briot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ga?tan</forename><surname>Hadjeres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran?ois-David</forename><surname>Pachet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Tramer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Borja</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Ippolito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<idno>arXiv preprint:2301.13188</idno>
		<title level="m">Extracting training data from diffusion models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vggsound: A large-scale audio-visual dataset</title>
		<author>
			<persName><forename type="first">Honglie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="646" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clap learning audio concepts from natural language supervision</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soham</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoud</forename><forename type="middle">Al</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The machine learning algorithm as creative musical tool</title>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Fiebrink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Caramiaux</surname></persName>
		</author>
		<idno>arXiv preprint:1611.00379</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Riffusion -Stable diffusion for real-time music generation</title>
		<author>
			<persName><forename type="first">Seth</forename><surname>Forsgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayk</forename><surname>Martiros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audioclip: Extending clip to image, text and audio</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rn</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="976" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cnn architectures for large-scale audio classification</title>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourish</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aren</forename><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R Channing</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rif</forename><forename type="middle">A</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName><surname>Seybold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="131" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.12598</idno>
		<title level="m">Classifier-free diffusion guidance</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Noise2music: Text-conditioned music generation with diffusion models</title>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Ly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Frank</surname></persName>
		</author>
		<idno>arXiv preprint:2302.03917</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion models</title>
		<author>
			<persName><forename type="first">Rongjie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhui</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<idno>arXiv preprint:2301.12661</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HifiGAN: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName><forename type="first">Jungil</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaekyoung</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17022" to="17033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffwave: A versatile diffusion model for audio synthesis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AudioGen: Textually guided audio generation</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Kreuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>D?fossez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jade</forename><surname>Copet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaniv</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">AudioLDM: Text-to-audio generation with latent diffusion models</title>
		<author>
			<persName><forename type="first">Haohe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Mandic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno>arXiv preprint:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Mubert: A simple notebook demonstrating prompt-based music generation</title>
		<author>
			<persName><surname>Mubertai</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harm</forename><forename type="middle">De</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ESC: dataset for environmental sound classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Karol</surname></persName>
		</author>
		<author>
			<persName><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimed</title>
		<meeting>ACM Multimed</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno>arXiv preprint:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>arXiv preprint:2006.04558</idno>
		<title level="m">FastSpeech 2: Fast and high-quality end-to-end text to speech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Highresolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A dataset and taxonomy for urban sound research</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimed</title>
		<meeting>ACM Multimed</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Gowthami</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasu</forename><surname>Singla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno>arXiv preprint:2212.03860</idno>
		<title level="m">Diffusion art or digital forgery? investigating data replication in diffusion models</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Xu Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhao</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><surname>He</surname></persName>
		</author>
		<idno>arXiv preprint:2205.04421</idno>
		<title level="m">End-to-end text to speech synthesis with human-level quality</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Musical genre classification of audio signals</title>
		<author>
			<persName><forename type="first">George</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on speech and audio processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wav2clip: Learning robust audio representations from clip</title>
		<author>
			<persName><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prem</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kundan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4563" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Large-scale contrastive language-audio pretraining with feature fusion and keywordto-caption augmentation</title>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discrete diffusion model for text-to-sound generation</title>
		<author>
			<persName><forename type="first">Dongchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Diffsound</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">Mixup: Beyond empirical risk minimization. Proc. ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beat transformer: Demixed beat and downbeat tracking with dilated self-attention</title>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gus</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISMIR</title>
		<meeting>ISMIR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
