<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling Protein Using Large-scale Pretrain Language Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-17">17 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yijia</forename><surname>Xiao</surname></persName>
							<email>xiaoyiji18@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
							<email>kimhsieh@tencent.com</email>
							<affiliation key="aff2">
								<orgName type="department">Tencent Quantum Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
							<email>jietang@tsinghua.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Beijing Academy of Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling Protein Using Large-scale Pretrain Language Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-17">17 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2108.07435v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>protein modeling</term>
					<term>language model</term>
					<term>high performance computing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Protein is linked to almost every life process. Therefore, analyzing the biological structure and property of protein sequences is critical to the exploration of life, as well as disease detection and drug discovery. Traditional protein analysis methods tend to be laborintensive and time-consuming. The emergence of deep learning models makes modeling data patterns in large quantities of data possible. Interdisciplinary researchers have begun to leverage deep learning methods to model large biological datasets, e.g. using long short-term memory and convolutional neural network for protein sequence classification. After millions of years of evolution, evolutionary information is encoded in protein sequences. Inspired by the similarity between natural language and protein sequences, we use large-scale language models to model evolutionary-scale protein sequences, encoding protein biology information in representation. Significant improvements are observed in both token-level and sequence-level tasks, demonstrating that our large-scale model can accurately capture evolution information from pretraining on evolutionary-scale individual sequences. Our code and model are available at https://github.com/THUDM/ProteinLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As an indispensable part of life activities, protein is responsible for catalysis (such as enzymes), transportation (such as hemoglobin), etc. Therefore, understanding the structure and functionality of protein is critical to the study of life science, as well as disease detection and drug discovery. Traditional protein analysis paradigms can be divided into experimental and analytical. Experimental methods usually require purification, crystallization, and X-ray crystallography. Analytical methods, like sequence alignment <ref type="bibr" target="#b15">[16]</ref>, and molecular dynamics simulation <ref type="bibr" target="#b9">[10]</ref>, tend to be incapable of handling large-scale protein data. Sequence alignment and similarity analysis leverage the idea that "structure determines properties", that sequential molecules with similar sequence order tend to have common ancestors and are relatively similar in structure and functionality. So similarity analysis often requires a large-scale annotated database, the properties of the sequence to be analyzed can be inferred from the labels of aligned sequences in the database. However, labeling such large databases requires lots of manpower and material resources. Molecular dynamics simulation (MD) and Monte Carlo (MC) simulations can be applied to protein analysis <ref type="bibr" target="#b10">[11]</ref> <ref type="bibr" target="#b13">[14]</ref>, and can be quite accurate (simulation at atom-scale). However, requires a lot of computing resources and is time-consuming.</p><p>Generally speaking, most of the proteins that exist stably in nature have undergone millions of years of natural selection and evolution, and are in a low-energy stable state. The polarity of some amino acids makes certain amino acid arrangements in a lower energy state, and motifs in proteins are also made up of specific amino acid stretches folded. Such patterns can be captured by deep learning models. Researchers have explored various strategies. Inspired by Word2Vec <ref type="bibr" target="#b16">[17]</ref>, BioVec <ref type="bibr" target="#b2">[3]</ref> proposed ProtVec for proteins GeneVec for gene sequences modeling. However, the vocabulary size grows exponentially with dependence range (n-gram), making the cost of modeling long-range dependencies unbearable (n-grams representation). With the rise of representation learning, sequence representation learning <ref type="bibr" target="#b0">[1]</ref> and transfer learning <ref type="bibr" target="#b11">[12]</ref> are also introduced to protein analysis. Recent years, the emergence of the attention mechanism <ref type="bibr" target="#b25">[26]</ref>, which can compute hidden representations in parallel, allows researchers to better model long sequential data. ProtTrans <ref type="bibr" target="#b7">[8]</ref> also show that large-scale auto-regressive language models can model protein sequences quite well. Besides, the information encoded in an individual sequence is limited, MSA Transformer <ref type="bibr" target="#b19">[20]</ref>, ESM <ref type="bibr" target="#b20">[21]</ref> leverage sequence alignment information to model protein even better. Other research like Neural Potts Model <ref type="bibr" target="#b23">[24]</ref> obtained inspiration from the Potts model.</p><p>Thanks to the advancement of high-throughput sequencing technology, we have larger amino acid sequence databases than ever before. However, most of these data are unlabeled primary structures of proteins, the labeled sequences (like structure, stability) are relatively scarce. The amazing achievements of BERT <ref type="bibr" target="#b5">[6]</ref> reveal the fact that data patterns can be extracted using unsupervised learning from massive unlabeled data, which inspired us to train language models on massive protein sequences. We have trained multiple large-scale models on the PFAM <ref type="bibr" target="#b6">[7]</ref> dataset, the largest with 3 billion parameters, outperforming TAPE <ref type="bibr" target="#b18">[19]</ref>'s performance. researchers are often evaluated on different datasets with different evaluation metrics. To solve this dilemma, TAPE <ref type="bibr" target="#b18">[19]</ref> put forward a set of five biologically related tasks (secondary structure prediction <ref type="bibr" target="#b14">[15]</ref>[4] <ref type="bibr" target="#b17">[18]</ref>, contact prediction <ref type="bibr" target="#b8">[9]</ref>[4] <ref type="bibr" target="#b17">[18]</ref>, remote homology detection <ref type="bibr" target="#b8">[9]</ref>, fluorescence <ref type="bibr" target="#b22">[23]</ref>, and stability <ref type="bibr" target="#b21">[22]</ref>). Commonly used models, like LSTM <ref type="bibr" target="#b12">[13]</ref>, Transformer <ref type="bibr" target="#b25">[26]</ref>, and ResNet <ref type="bibr" target="#b27">[28]</ref> are implemented for these tasks, serving as benchmarks for semisupervised representation learning. One of their conclusions is that self-supervised training is beneficial for almost all models on all tasks, doubling performance in some downstream tasks. Our work is based on standardized datasets and evaluation metrics provided in TAPE <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Large-scale Pretraining</head><p>The success of pretraining makes researchers wonder whether the in language model scale can always bring about improved performance. ProtTrans <ref type="bibr" target="#b7">[8]</ref> is one of the representatives, the researchers trained a series of language models with tens of billions of parameters, the largest one ProtT5-XXL with 11B parameters, and achieved excellent performance on downstream tasks such as secondary structure prediction and solubility prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Efficient Pretraining of Language Models</head><p>Different from the usual pretraining, large-scale pretraining requires distributed training techniques, including model parallelism, data parallelism, memory optimization, data synchronization, etc. Fortunately, Megatron-LM <ref type="bibr" target="#b24">[25]</ref> provides us with an efficient training framework for language models. We have implemented and trained our protein language model within this framework, as well as downstream classification and regression tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY 3.1 Pretrain Tasks Description</head><p>The goal for protein pretraining is modeling data patterns in massive unlabeled sequences. One closed-related model is BERT <ref type="bibr" target="#b5">[6]</ref> from natural language processing. We made some modifications to its loss function and model structure.</p><p>Dataset Our work takes the dataset put forward by TAPE <ref type="bibr" target="#b18">[19]</ref>. So some date descriptions are inherited. PFAM <ref type="bibr" target="#b6">[7]</ref> is a widely-used database consisting of more than 32 million protein sequences. Sequences in PFAM are clustered into evolutionarily related groups (protein families). Leveraging this family information, TAPE constructed a test set (about 1% of the data) of fully held out families. The remaining data are used for constructing training and test sets using a random 95%/5% split. We use preprocessed PFAM from TAPE as the pretrain corpus.</p><p>Training Objective BERT <ref type="bibr" target="#b5">[6]</ref> original loss consists of masked language model loss and next sentence prediction loss.</p><formula xml:id="formula_0">L BERT = ∑︁ L 𝑚𝑎𝑠𝑘𝑒𝑑 𝑥 𝑖 MLM (𝑥 𝑖 ) + ∑︁ L 𝑠𝑒𝑛𝑡𝑒𝑛𝑐𝑒𝑝𝑎𝑖𝑟 𝑝 𝑖 NSP (𝑝 𝑖 )</formula><p>For protein pretraining, we inherited the masking strategy of the masked language model (MLM) in BERT <ref type="bibr" target="#b5">[6]</ref>, randomly masking 15% of all the amino acid tokens, and then train the protein model to be able to predict the masked token from the rest of tokens. As for next sentence prediction (NSP), considering that the input sequences are randomly shuffled, we assume there is no evident semantic/biological correlation between sequences. So we discard the next sentence prediction loss, only keep the masked language model loss.</p><p>As for the training objective function, we modified the loss function of BERT: BERT's loss function includes masked language model and next sentence prediction. Considering that there is no obvious contextual semantic relationship between protein and protein, we only retain masked language model loss.</p><formula xml:id="formula_1">L MLM = − ∑︁ x ∈𝑚 (x) log 𝑝 x | X \𝑚 (x)</formula><p>In terms of model structure, Megatron-LM <ref type="bibr" target="#b24">[25]</ref> proposes that when the scale of the model grows huge, the position of the layernorm becomes critical. Therefore, the sublayers in the transformer layer have been reordered. The original layernorm is in the output layer, but now it is placed ahead of the input layer to prevent the input data from drifting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Downstream Classification Tasks</head><p>There are three classification tasks, corresponding to token, sequence, and token-pair classification.  Input</p><formula xml:id="formula_2">𝐼𝑛 𝑠𝑒𝑞 = (𝑥 1 , . . . , 𝑥 𝐿 ) Output 𝑂𝑢𝑡 𝑠𝑒𝑞 = (𝑦 1 , . . . , 𝑦 𝐿 ).𝑦 𝑖 ∈ 𝑄 𝑖 Dataset</formula><p>The dataset for secondary structure task is the CB513 <ref type="bibr" target="#b4">[5]</ref> dataset. Training Objective A one-dimensional convolution layer can be applied to secondary structure prediction <ref type="bibr" target="#b26">[27]</ref>. However, due to the powerful modeling capabilities of our model, the encoding output from the protein language model already contains sufficient information for this task, so we take ProteinLM followed by a multilayer perceptron as the secondary structure classifier.   Unlike the commonly used residual connected 2D-convolution network, we adopted a simple predictor, concatenating embedding pairs and using multilayer perceptron to do this binary classification. Numerous hidden units, presentation layers, as well as huge corpus guarantee that our model can capture even more long-range dependence information than common models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>The dataset from ProteinNet <ref type="bibr" target="#b1">[2]</ref>. And evaluation metric is 𝐿/5, 𝐿/2, 𝐿 most likely contact prediction accuracy contacts (𝐿 is the length of protein sequence).  Distinguishing protein sequences with different mutations can be difficult, since the computational cost grows exponentially with the number of mutations 𝑚. The computational complexity for a sequence with 𝑚 mutation away is 𝑂 (𝐿 𝑚 ). The fluorescence prediction task can evaluate the model's capacity to distinguish between very similar protein sequences, as well as its ability to generalize to unseen combinations of mutations <ref type="bibr" target="#b18">[19]</ref>. Accurate predictions can facilitate the exploration of the protein landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Downstream Regression Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>The train set <ref type="bibr" target="#b22">[23]</ref> is made up of 𝐷𝑖𝑠𝑡𝑎𝑛𝑐𝑒 𝐻𝑎𝑚𝑚𝑖𝑛𝑔 = 3 neighborhoods from the parent green fluorescent protein <ref type="bibr" target="#b22">[23]</ref>, while the test set sequences with four or more mutations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Stability.</head><p>Description Stability is very important in the design of protein drugs, because drugs with low stability are often degraded before they take effect. The stability of one protein sequence is measured experimentally and indirectly: the upper limit of concentration at which the protein can maintain its original folding structure <ref type="bibr" target="#b21">[22]</ref>. Therefore, for this task, The input is the amino acid sequence, while the output is a continuous value predicting to which extent the protein can maintain its fold structure. Dataset The train set consists of proteins from four rounds of experimental design, while the test set contains Hamming distance-1 neighbors of top candidate proteins <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RESULTS</head><p>Our model has obtained amazing results in downstream tasks. There are great improvements in four tasks: secondary structure prediction, distant homology detection, stability, and contact prediction. It is worth mentioning that the performance of the 3B model on contact prediction has almost doubled compared with the baseline model.</p><p>Besides, we used 10 sets of model hyper-parameters in total, and conducted very sufficient experiments on a series of tasks. The pretraining details can be found in Supplementary Materials, and all the results can be found inTable 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training</head><p>We pretrained two large models on a 480 GPUs (Tesla-V100-32GB) cluster for about three weeks. The MLM loss and PPL of the pretrained models can be found in Table <ref type="table" target="#tab_0">1</ref>.</p><p>The 3B parameters model reached language model loss  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>Out of the five tasks, the results of four tasks have been improved.</p><p>(1) Contact Prediction: Table 2  (2) Remote Homology: Table <ref type="table" target="#tab_3">3</ref> (3) Secondary Structure: Table <ref type="table" target="#tab_4">4</ref> (4) Fluorescence: Table <ref type="table" target="#tab_5">5</ref> (5) Stability: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONTACT MAP VISUALIZATION</head><p>Generally, the accuracy of predictions on the anti-diagonal can reflect the model's ability to capture long-range dependency. Therefore, we also visualized the ground truth contact maps, as well as contact maps predicted by our model and TAPE. The contact map below demonstrates that our model is good at capturing long-range dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Factual Contact Map</head><p>We visualize the contact map of protein #TBM-T0889 in Figure <ref type="figure" target="#fig_11">10</ref>, and we can intuitively see that there are many long-range contacts (contacts that are separated by at least 24 residues). This protein sequence can be used to distinguish the ability to capture longdistance dependence of different models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">TAPE Contact Map</head><p>Through the visualized predictions from TAPE <ref type="bibr" target="#b10">(11)</ref>, we can see that the TAPE model (small-scale transformer) can capture mediumrange contacts (contacts that are separated by 12-23 residues). As for the long-range contact prediction, there are lots of missings on the anti-diagonal belt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ProteinLM Contact Map</head><p>ProteinLM-3B model shows very good performance in contact prediction, and the visualized prediction map confirmed this. ProtrinLM-3B can capture medium-range and long-range dependencies, with lots of hits on the anti-diagonal belt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis</head><p>In the visual contact map of our model, the contacts on the antidiagonal line can be accurately predicted. The prediction results on the anti-diagonal line are more accurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SUMMARY</head><p>We propose ProteinLM, a large-scale pretrain model for protein sequences. The optimizations we introduced into training make billion-level model training possible and efficient. And the significantly improved performance of downstream tasks shows that as the scale of the model increases, the biological information in the sequence and the long-term dependence can be captured more accurately. In addition, through a large number of controlled experiments, we also found some rules of thumb for hyperparameter selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance</head><p>Task CC@L/5 CC@L/2 CC@L Fluorescence RH SS Q@3 SS Q@ 𝐶𝐶@𝑋 means 𝑐𝑜𝑛𝑡𝑎𝑐𝑡 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛@𝑋 . 𝑆𝑆 𝑄@𝑋 means 𝑋 𝑐𝑎𝑡𝑒𝑔𝑜𝑟𝑦 classification for secondary structure.</p><p>𝑅𝐻 means remote homology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUPPLEMENTARY MATERIALS</head><p>The curves of training MLM loss, validation MLM loss, and validation PPL are provided below.</p><p>Model Description MLM and PPL curve hidden size = 256 transformer layers = 8 attention heads = 16 With the limited amount of computing resources and computing time, the selection of hyper-parameters is critical to the model's performance. A trade-off is necessary. The depth (transformer layers) of the model has a greater impact on the performance than the width (hidden size).</p><p>On the one hand, the model that is too flat (#𝑡𝑟𝑎𝑛𝑠 𝑓 𝑜𝑟𝑚𝑒𝑟 𝑙𝑎𝑦𝑒𝑟𝑠 &lt; 8) performs poorly, even though it has a large hidden size. We trained a model with ℎ𝑖𝑑𝑑𝑒𝑛 𝑠𝑖𝑧𝑒 = 8192, #𝑡𝑟𝑎𝑛𝑠 𝑓 𝑜𝑟𝑚𝑒𝑟 𝑙𝑎𝑦𝑒𝑟𝑠 = 3. Although its training speed (average time for each iteration) is the fastest among all models, it failed to converge after 5 days of training.</p><p>On the other hand, the model that is too deep is not a feasible choice in this scenario (limited training time). The training time of Figure <ref type="figure" target="#fig_8">14</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3. 2 . 1</head><label>21</label><figDesc>Secondary Structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Secondary Structure Task</figDesc><graphic url="image-1.png" coords="2,355.67,381.06,164.46,65.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>𝐿𝑜𝑠𝑠 (𝑆𝑒𝑞) = ∑︁ 𝑥 𝑖 ∈𝑆𝑒𝑞 𝐶𝑟𝑜𝑠𝑠𝐸𝑛𝑡𝑟𝑜𝑝𝑦𝐿𝑜𝑠𝑠 (𝑚𝑜𝑑𝑒𝑙 (𝑥 𝑖 ), 𝑥 𝑖 ) 3.2.2 Remote Homology.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Remote Homology Task</figDesc><graphic url="image-2.png" coords="3,87.99,150.49,169.77,157.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Contact Prediction Task</figDesc><graphic url="image-3.png" coords="3,88.90,564.10,165.59,92.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>3. 3 . 1</head><label>31</label><figDesc>Fluorescence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Fluorescence Task</figDesc><graphic url="image-4.png" coords="3,353.25,319.93,170.77,95.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Stability Task</figDesc><graphic url="image-5.png" coords="4,85.71,76.71,185.07,112.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>𝑀𝐿𝑀𝑙𝑜𝑠𝑠 3𝐵 = 1 .</head><label>1</label><figDesc>318, perplexity 𝑃𝑃𝐿 3𝐵 = 3.736. The 1.2B parameters model reached language model loss 𝑀𝐿𝑀𝑙𝑜𝑠𝑠 1.2𝐵 = 1.335, perplexity 𝑃𝑃𝐿 1.2𝐵 = 3.802. In pretraining, although the overall training iteration for the 3 billion model is only half of that for the 1.2 billion model, it reached even smaller MLM loss and PPL. The training MLM loss, validation MLM loss and PPL curves can be found below. (1) 1.2B model's training MLM loss: Figure 6 (2) 3B model's training MLM loss: Figure 7 (3) 1.2B model's validation MLM loss and PPL Figure 8 (4) 3B model's validation MLM loss and PPL Figure 9 Model Protein LM (1.2B) Protein LM (3B) MLM Loss 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 : 1 . 2 Figure 7 : 3</head><label>61273</label><figDesc>Figure 6: 1.2 Billion Model Training Loss</figDesc><graphic url="image-6.png" coords="4,341.98,83.69,192.20,144.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 : 1 . 2</head><label>812</label><figDesc>Figure 8: 1.2 Billion Model Validation</figDesc><graphic url="image-8.png" coords="4,341.98,461.86,192.20,144.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Factual Contact Map</figDesc><graphic url="image-10.png" coords="5,317.96,269.42,240.25,180.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: TAPE Prediction</figDesc><graphic url="image-11.png" coords="6,53.80,83.69,240.25,180.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 hidden</head><label>13</label><figDesc>Figure 13</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: 12 layers, hidden size = 1024, 16 attention heads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 19 :Figure 20 :Figure 21 :</head><label>192021</label><figDesc>Figure 19: 12 layers, hidden size = 1024, 32 attention heads</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>MLM loss and PPL</figDesc><table><row><cell></cell><cell>335</cell><cell>1.318</cell></row><row><cell>PPL</cell><cell>3.802</cell><cell>3.736</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 6</head><label>6</label><figDesc>Figure 9: 3 Billion Model Validation</figDesc><table><row><cell>Task</cell><cell>contact prediction</cell></row><row><cell>Metric</cell><cell>P@L/5</cell></row><row><cell>TAPE</cell><cell>0.36</cell></row><row><cell cols="2">ProteinLM (200M) 0.52</cell></row><row><cell>ProteinLM (3B)</cell><cell>0.75</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Contact Prediction</figDesc><table><row><cell>Task</cell><cell>remote homology</cell></row><row><cell>Metric</cell><cell>Top 1 Accuracy</cell></row><row><cell>TAPE</cell><cell>0.21</cell></row><row><cell cols="2">ProteinLM (200M) 0.26</cell></row><row><cell>ProteinLM (3B)</cell><cell>0.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Remote Homology</figDesc><table><row><cell>Task</cell><cell>secondary structure</cell></row><row><cell>Metric</cell><cell>Accuracy (Q-3)</cell></row><row><cell>TAPE</cell><cell>0.73</cell></row><row><cell cols="2">ProteinLM (200M) 0.75</cell></row><row><cell>ProteinLM (3B)</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Secondary Structure</figDesc><table><row><cell>Task</cell><cell>fluorescence</cell></row><row><cell>Metric</cell><cell>Spearman's rho</cell></row><row><cell>TAPE</cell><cell>0.68</cell></row><row><cell cols="2">ProteinLM (200M) 0.68</cell></row><row><cell>ProteinLM (3B)</cell><cell>0.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Fluorescence</figDesc><table><row><cell>Task</cell><cell>stability</cell></row><row><cell>Metric</cell><cell>Spearman's rho</cell></row><row><cell>TAPE</cell><cell>0.73</cell></row><row><cell cols="2">ProteinLM (200M) 0.77</cell></row><row><cell>ProteinLM (3B)</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Stability</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results for comparative experiments.</figDesc><table><row><cell>8 Stability</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>is 3.5 times that of Figure 20. And it takes about 25 days to train the model with 32 transformer layers Figure 14. Our empirical conclusion is that the model parameters of 512 &lt; ℎ𝑖𝑑𝑑𝑒𝑛 𝑠𝑖𝑧𝑒 &lt; 3072, 8 &lt; #𝑡𝑟𝑎𝑛𝑠 𝑓 𝑜𝑟𝑚𝑒𝑟 𝑙𝑎𝑦𝑒𝑟𝑠 &lt; 24 are feasible and can well balance training efficiency and resource consumption.</figDesc><table><row><cell></cell><cell>30 14</cell><cell></cell><cell>ppl-validation ppl-validation</cell><cell>2.8 2.9 2.6</cell><cell></cell><cell>lm-loss-train lm-loss-train</cell><cell>3.4 2.6</cell><cell>lm-loss-validation lm-loss-validation</cell></row><row><cell>ppl-validation ppl-validation</cell><cell>20 12 25 10</cell><cell></cell><cell>lm-loss-train lm-loss-train</cell><cell>2.7 2.4 2.5 2.6 2.2</cell><cell></cell><cell>lm-loss-validation lm-loss-validation</cell><cell>3.2 2.4 2.8 3.0 2.2</cell></row><row><cell></cell><cell>15 8</cell><cell></cell><cell></cell><cell>2.4 2.0</cell><cell></cell><cell>2.6 2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.4</cell></row><row><cell></cell><cell>10 6</cell><cell cols="2">0 100000 200000 300000 400000 500000 600000 700000 800000 # iterations 0 100000 200000 300000 400000 500000 # iterations</cell><cell>2.3 1.8</cell><cell cols="2">0 100000 200000 300000 400000 500000 600000 700000 800000 # iterations 0 100000 200000 300000 400000 500000 # iterations</cell><cell>2.2 1.8</cell><cell>0 100000 200000 300000 400000 500000 600000 700000 800000 # iterations 0 100000 200000 300000 400000 # iterations 500000</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 13: 8 layers, hidden size = 256, 16 attention heads Figure 17: 16 layers, hidden size = 768, 24 attention heads</cell></row><row><cell></cell><cell>16 14</cell><cell></cell><cell>ppl-validation ppl-validation</cell><cell>2.8 2.6</cell><cell></cell><cell>lm-loss-train lm-loss-train</cell><cell>2.8 2.6</cell><cell>lm-loss-validation lm-loss-validation</cell></row><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell>2.6</cell><cell></cell><cell>2.6</cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>2.4</cell><cell></cell><cell>2.4</cell></row><row><cell>ppl-validation ppl-validation</cell><cell>10 12 10</cell><cell></cell><cell>lm-loss-train lm-loss-train</cell><cell>2.2 2.4 2.2</cell><cell></cell><cell>lm-loss-validation lm-loss-validation</cell><cell>2.2 2.4 2.2</cell></row><row><cell></cell><cell>8 8</cell><cell></cell><cell></cell><cell>2.0 2.0</cell><cell></cell><cell>2.0 2.0</cell></row><row><cell></cell><cell>6 6</cell><cell></cell><cell></cell><cell>1.8 1.8</cell><cell></cell><cell>1.8 1.8</cell></row><row><cell></cell><cell></cell><cell>0 0</cell><cell>50000 100000 150000 200000 250000 300000 350000 400000 # iterations 100000 200000 300000 400000 500000 600000 # iterations</cell><cell></cell><cell>0 0</cell><cell>50000 100000 150000 200000 250000 300000 350000 400000 # iterations 100000 200000 300000 400000 500000 600000 # iterations</cell><cell>0 0</cell><cell>50000 100000 150000 200000 250000 300000 350000 400000 # iterations 100000 200000 300000 400000 500000 600000 # iterations</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 14: 32 layers, hidden size = 512, 8 attention heads</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ppl-validation</cell><cell></cell><cell></cell><cell>lm-loss-train</cell><cell>lm-loss-validation</cell></row><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell>2.6</cell><cell></cell><cell>2.6</cell></row><row><cell>ppl-validation</cell><cell>10 12</cell><cell></cell><cell>lm-loss-train</cell><cell>2.2 2.4</cell><cell></cell><cell>lm-loss-validation</cell><cell>2.2 2.4</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>2.0</cell><cell></cell><cell>2.0</cell></row><row><cell></cell><cell>6</cell><cell cols="2">0 100000 200000 300000 400000 500000 600000 700000 800000 # iterations</cell><cell></cell><cell cols="2">0 100000 200000 300000 400000 500000 600000 700000 800000 # iterations</cell><cell>0 100000 200000 300000 400000 500000 600000 700000 800000 # iterations</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 15: 12 layers, hidden size = 768, 6 attention heads</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ppl-validation</cell><cell></cell><cell></cell><cell>lm-loss-train</cell><cell>lm-loss-validation</cell></row><row><cell></cell><cell>14</cell><cell></cell><cell></cell><cell>2.6</cell><cell></cell><cell>2.6</cell></row><row><cell></cell><cell>12</cell><cell></cell><cell></cell><cell>2.4</cell><cell></cell><cell>2.4</cell></row><row><cell>ppl-validation</cell><cell>10</cell><cell></cell><cell>lm-loss-train</cell><cell>2.2</cell><cell></cell><cell>lm-loss-validation</cell><cell>2.2</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell></cell><cell>2.0</cell><cell></cell><cell>2.0</cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell>1.8</cell><cell></cell><cell>1.8</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>100000 200000 300000 400000 500000 600000 # iterations</cell><cell></cell><cell>0</cell><cell>100000 200000 300000 400000 500000 600000 # iterations</cell><cell>0</cell><cell>100000 200000 300000 400000 500000 600000 # iterations</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Figure 16: 16 layers, hidden size = 768, 16 attention heads</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unified rational protein engineering with sequenceonly deep representation learning</title>
		<author>
			<persName><forename type="first">Ethan</forename><forename type="middle">C</forename><surname>Alley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigory</forename><surname>Khimulya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surojit</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">M</forename><surname>Church</surname></persName>
		</author>
		<idno type="DOI">10.1101/589333</idno>
		<ptr target="https://doi.org/10.1101/589333arXiv:https://www.biorxiv.org/content/early/2019/03/26/589333.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ProteinNet: a standardized data set for machine learning of protein structure</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Alquraishi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00249</idno>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">3B</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Prediction</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics</title>
		<author>
			<persName><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Rk</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">e0141287</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">John</forename><surname>Helen M Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zukang</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Talapady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helge</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><forename type="middle">N</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">E</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
		<idno type="DOI">10.1002/(SICI)1097-0134(19990301)34:4&lt;508::AID-PROT10&gt;3.0.CO;2-4</idno>
		<idno>AID-PROT10&gt;3.0.CO;2-4</idno>
		<ptr target="https://doi.org/10.1002/(SICI)1097-0134" />
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="508" to="519" />
			<date type="published" when="1999">1999. 1999. 19990301</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<ptr target="http://arxiv.org/abs/1810.04805" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Pfam protein families database in 2019</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>El-Gebali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaina</forename><surname>Mistry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bateman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">R</forename><surname>Eddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Luciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">C</forename><surname>Potter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matloob</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorna</forename><forename type="middle">J</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><forename type="middle">A</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfredo</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L L</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Layla</forename><surname>Sonnhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisanna</forename><surname>Hirsh</surname></persName>
		</author>
		<author>
			<persName><surname>Paladin</surname></persName>
		</author>
		<author>
			<persName><surname>Damiano Piovesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C E</forename><surname>Silvio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Tosatto</surname></persName>
		</author>
		<author>
			<persName><surname>Finn</surname></persName>
		</author>
		<idno type="DOI">10.1093/nar/gky995</idno>
		<ptr target="https://doi.org/10.1093/nar/gky995arXiv:https://academic.oup.com/nar/article-pdf/47/D1/D427/27436497/gky995.pdf" />
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D427" to="D432" />
			<date type="published" when="2018-10">2018. 10 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ProtTrans: Towards Cracking the Language of Life&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing. CoRR abs</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghalia</forename><surname>Rihawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamas</forename><surname>Feher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debsindhu</forename><surname>Bhowmik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06225</idno>
		<ptr target="https://arxiv.org/abs/2007.06225" />
		<imprint>
			<date type="published" when="2007">2020. 2007. 2020</date>
			<biblScope unit="page">6225</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SCOPe: Structural Classification of Proteins-extended, integrating SCOP and ASTRAL data and classification of new structures</title>
		<author>
			<persName><forename type="first">Naomi</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Marc</forename><surname>Chandonia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="D304" to="D309" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applications of Molecular Dynamics Simulation in Structure Prediction of Peptides and Proteins</title>
		<author>
			<persName><forename type="first">Fangfang</forename><surname>Hao Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csbj.2019.07.010</idno>
		<ptr target="https://doi.org/10.1016/j.csbj.2019.07.010" />
	</analytic>
	<monogr>
		<title level="j">Computational and Structural Biotechnology Journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1162" to="1170" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Molecular dynamics simulations of protein folding from the transition state</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Gsponer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amedeo</forename><surname>Caflisch</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.092686399</idno>
		<ptr target="https://www.pnas.org/content/99/10/6719.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="6719" to="6724" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Modeling the language of life -Deep Learning Protein Sequences</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Matthes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
		</author>
		<idno type="DOI">10.1101/614313</idno>
		<ptr target="https://doi.org/10.1101/614313arXiv:https://www.biorxiv.org/content/early/2019/09/10/614313.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Molecular dynamics and protein function</title>
		<author>
			<persName><forename type="first">M</forename><surname>Karplus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kuriyan</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.0408930102</idno>
		<ptr target="https://www.pnas.org/content/102/19/6679.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="6679" to="6685" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schantz Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Closter Jespersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamilla</forename><surname>Kjaergaard Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanessa</forename><forename type="middle">Isabell</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Soenderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morten</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bent</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Protein Structure Prediction by Protein Alignments</title>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.05682</idno>
		<ptr target="http://arxiv.org/abs/1510.05682" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations, ICLR 2013</title>
				<editor>
			<persName><forename type="first">Yann</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02">2013. May 2-4, 2013</date>
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">John</forename><surname>Moult</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Fidelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torsten</forename><surname>Schwede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Tramontano</surname></persName>
		</author>
		<idno type="DOI">10.1002/prot.25415</idno>
		<ptr target="https://doi.org/10.1002/prot.25415" />
	</analytic>
	<monogr>
		<title level="m">Critical assessment of methods of protein structure prediction (CASP)-Round XII</title>
				<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="7" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><forename type="middle">S</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08230[cs.LG]</idno>
		<title level="m">Evaluating Protein Transfer Learning with TAPE</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Roshan</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">F</forename><surname>Canny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.02.12.430858</idno>
		<ptr target="https://www.biorxiv.org/content/early/2021/02/13/2021.02.12.430858.full.pdf" />
	</analytic>
	<monogr>
		<title level="j">MSA Transformer. bioRxiv</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="DOI">10.1101/622803</idno>
		<ptr target="https://doi.org/10.1101/622803" />
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global analysis of protein folding using massively parallel design, synthesis, and testing</title>
		<author>
			<persName><surname>Gabriel J Rocklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tamuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inna</forename><surname>Chidyausiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Goreshnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Houliston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lauren</forename><surname>Lemak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashmi</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Vikram K Mulligan</surname></persName>
		</author>
		<author>
			<persName><surname>Chevalier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="168" to="175" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Local fitness landscape of the green fluorescent protein</title>
		<author>
			<persName><surname>Karen S Sarkisyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margarita</forename><forename type="middle">V</forename><surname>Dmitry A Bolotin</surname></persName>
		</author>
		<author>
			<persName><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><surname>Dinara R Usmanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">V</forename><surname>Alexander S Mishin</surname></persName>
		</author>
		<author>
			<persName><surname>Sharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><forename type="middle">G</forename><surname>Dmitry N Ivankov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><forename type="middle">S</forename><surname>Bozhanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onuralp</forename><surname>Baranov</surname></persName>
		</author>
		<author>
			<persName><surname>Soylemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">533</biblScope>
			<biblScope unit="page">397</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Verkuil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rives</surname></persName>
		</author>
		<idno type="DOI">10.1101/2021.04.08.439084</idno>
		<ptr target="https://www.biorxiv.org/content/early/2021/04/11/2021.04.08.439084.full.pdf" />
	</analytic>
	<monogr>
		<title level="m">Neural Potts Model</title>
				<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053[cs.CL]</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762[cs.CL]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Protein Secondary Structure Prediction Using Deep Convolutional Neural Fields</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep18962</idno>
		<ptr target="https://doi.org/10.1038/srep18962" />
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">01</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dilated Residual Networks</title>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.75</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.75" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="636" to="644" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
