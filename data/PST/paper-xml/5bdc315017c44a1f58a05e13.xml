<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yihui</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ji</forename><surname>Lin</surname></persName>
							<email>jilin@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Song</forename><surname>Han</surname></persName>
							<email>songhan@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Compressed</forename><surname>Nn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology ‡ Carnegie Mellon University Google</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">AMC: AutoML for Model Compression and Acceleration on Mobile Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>AutoML</term>
					<term>Reinforcement learning</term>
					<term>Model compression</term>
					<term>CNN acceleration</term>
					<term>Mobile vision</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-ofthe-art model compression results in a fully automated way without any human efforts. Under 4× FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53× on the GPU (Titan Xp) and 1.95× on an Android phone (Google Pixel 1), with negligible loss of accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In many machine learning applications (e.g., robotics, self-driving cars, and advertisement ranking), deep neural networks are constrained by latency, energy and model size budget. Many works have been proposed to improve the hardware efficiency of neural networks by model compression <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b22">22]</ref>. The core of model compression technique is to determine the compression policy for each layer as they have different redundancy, which conventionally requires hand-crafted heuristics and domain experts to explore the large design space trading off among model size, speed, and accuracy. The design space is so large that human heuristic is usually sub-optimal, and manual model compression is time-consuming. To this end, we aim to automatically find the compression policy for an arbitrary network to achieve even better performance than human strategy. Fig. <ref type="figure">1</ref>. Overview of AutoML for Model Compression (AMC) engine. Left: AMC replaces human and makes model compression fully automated while performing better than human. Right: Form AMC as a reinforcement learning problem. We process a pretrained network (e.g., MobileNet-V1) in a layer-by-layer manner. Our reinforcement learning agent (DDPG) receives the embedding st from a layer t, and outputs a sparsity ratio at. After the layer is compressed with at, it moves to the next layer Lt+1. The accuracy of the pruned model with all layers compressed is evaluated. Finally, as a function of accuracy and FLOP, reward R is returned to the reinforcement learning agent.</p><p>As the layers in deep neural networks are correlated in an unknown way, determining the compression policy is highly non-trivial. The problem also has exponential complexity as the network goes deeper, which is infeasible to be solved by brute-force methods. Reinforcement learning methods have been widely approved to have better sample efficiency than random exploration and achieve better solution. Therefore, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and greatly improve the model compression quality. Figure <ref type="figure">1</ref> illustrates our AMC engine. When compressing a network, rather than relying on human experience or hand-crafted heuristics, our AMC engine automates this process and frees the compression pipeline from human labor.</p><p>We observe that the accuracy of the compressed model is very sensitive to the sparsity of each layer, requiring a fine-grained action space. Therefore, instead of searching over a discrete space, we come up with a continuous compression ratio control strategy with a DDPG <ref type="bibr" target="#b32">[32]</ref> agent to learn through trials and errors: penalizing accuracy loss while encouraging model shrinking and speedup. The actor-critic structure also helps to reduce variance, facilitating stabler training. Specifically, our DDPG agent processes the network in a layer-wise manner. For each layer L t , the agent receives a layer embedding s t which encodes useful characteristics of this layer, and then it outputs a precise compression ratio a t . After layer L t is compressed with a t , the agent moves to the next layer L t+1 . The validation accuracy of the pruned model with all layers compressed is evaluated without fine-tuning, which is an efficient delegate of the fine-tuned accuracy. This Table <ref type="table">1</ref>. Comparisons of reinforcement learning approaches for models searching (NAS: Neural Architecture Search <ref type="bibr" target="#b57">[57]</ref>, NT: Network Transformation <ref type="bibr" target="#b5">[6]</ref>, N2N: Network to Network <ref type="bibr" target="#b1">[2]</ref>, and AMC: AutoML for Model Compression. AMC distinguishes from other works by getting reward without fine-tuning, continuous search space control, and can produce both accuracy-guaranteed and hardware resource-constrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAS NT N2N AMC</head><p>optimize for accuracy optimize for latency simple, non-RNN controller fast exploration with few GPUs continuous action space simple approximation can improve the search time not having to retrain the model, and provide high quality search result.</p><p>After the policy search is done, the best-explored model is fine-tuned to achieve the best performance.</p><p>We proposed two compression policy search protocols for different scenarios. For latency-critical AI applications (e.g., mobile apps, self-driving cars, and advertisement rankings), we propose resource-constrained compression to achieve the best accuracy given the maximum amount of hardware resources (e.g., FLOPs, latency, and model size), For quality-critical AI applications (e.g., Google Photos) where latency is not a hard constraint, we propose accuracy-guaranteed compression to achieve the smallest model with no loss of accuracy.</p><p>We achieve resource-constrained compression by constraining the search space, in which the action space (pruning ratio) is constrained such that the model compressed by the agent is always below the resources budget. For accuracyguaranteed compression, we define a reward that is a function of both accuracy and hardware resource. With this reward function, we are able to explore the limit of compression without harming the accuracy of models.</p><p>To demonstrate the wide and general applicability, we evaluate our AMC engine on multiple neural networks, including VGG <ref type="bibr" target="#b45">[45]</ref>, ResNet <ref type="bibr" target="#b21">[21]</ref>, and MobileNet-V1 <ref type="bibr" target="#b23">[23]</ref>, and we also test the generalization ability of the compressed model from classification to object detection. Extensive experiments suggest that AMC offers better performance than hand-crafted heuristic policies. For ResNet-50, we push the expert-tuned compression ratio <ref type="bibr" target="#b15">[16]</ref> from 3.4× to 5× with no loss of accuracy. Furthermore, we reduce the FLOPs of MobileNet-V1 <ref type="bibr" target="#b23">[23]</ref> by 2×, achieving top one accuracy of 70.2%, which is on a better Pareto curve than 0.75 MobileNet-V1, and we achieve a speedup of 1.53× on the Titan XP and 1.95× on an Android phone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>CNN Compression and Acceleration. Extensive works <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b17">17]</ref> have been done on accelerating neural networks by compression. Quantization <ref type="bibr" target="#b55">[55,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">41]</ref> and special convolution implementations <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b2">3]</ref> can also speed up the neural networks. Tensor factorization <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b35">35]</ref> decomposes weights into light-weight pieces, for example <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> proposed to accelerate the fully connected layers with truncated SVD; Jaderberg et al . <ref type="bibr" target="#b26">[26]</ref> proposed to factorize layers into 1×3 and 3×1; and Zhang et al . <ref type="bibr" target="#b53">[53]</ref> proposed to factorize layers into 3×3 and 1×1. Channel pruning <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">38]</ref> removes the redundant channels from feature maps. A common problem of these methods is how to determine the sparsity ratio for each layer. Neural Architecture Search and AutoML. Many works on searching models with reinforcement learning and genetic algorithms <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b37">37]</ref> greatly improve the performance of neural networks. NAS <ref type="bibr" target="#b57">[57]</ref> aims to search the transferable network blocks, and its performance surpasses many manually designed architectures <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b8">9]</ref>. Cai et al . <ref type="bibr" target="#b5">[6]</ref> proposed to speed up the exploration via network transformation <ref type="bibr" target="#b7">[8]</ref>. Inspired by them, N2N <ref type="bibr" target="#b1">[2]</ref> integrated reinforcement learning into channel selection. In Table <ref type="table">1</ref>, we demonstrate several merits of our AMC engine. Compared with previous work, AMC engine optimizes for both accuracy and latency, requires a simple non-RNN controller, can do fast exploration with fewer GPU hours, and also support continuous action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We present an overview of our AutoML for Model Compression(AMC) engine in Figure <ref type="figure">1</ref>. We aim to automatically find the redundancy for each layer, characterized by sparsity. We train an reinforcement learning agent to predict the action and give the sparsity, then perform form the pruning. We quickly evaluate the accuracy after pruning but before fine-tuning as an effective delegate of final accuracy. Then we update the agent by encouraging smaller, faster and more accurate models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Model compression is achieved by reducing the number of parameters and computation of each layer in deep neural networks. There are two categories of pruning: fine-grained pruning and structured pruning. Fine-grained pruning <ref type="bibr" target="#b19">[19]</ref> aims to prune individual unimportant elements in weight tensors, which is able to achieve very high compression rate with no loss of accuracy. However, such algorithms result in an irregular pattern of sparsity, and it requires specialized hardware such as EIE <ref type="bibr" target="#b18">[18]</ref> for speed up. Coarse-grained structured pruning <ref type="bibr" target="#b31">[31]</ref> aims to prune entire regular regions of weight tensors (e.g., channel, row, column, block, etc.). The pruned weights are regular and can be accelerated directly with off-the-shelf hardware and libraries. Here we study structured pruning that shrink the input channel of each convolutional and fully connected layer.</p><p>Our goal is to precisely find out the effective sparsity for each layer, which used to be manually determined in previous studies <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22]</ref>. Take convolutional layer as an example. The shape of a weight tensor is n × c × k × k, where n, c are output and input channels, and k is the kernel size. For fine-grained pruning, the sparsity is defined as the number of zero elements divided by the number of total elements, i.e. #zeros/(n × c × k × h). For channel pruning, we shrink the weight tensor to n × c ′ × k × k (where c ′ &lt; c), hence the sparsity becomes c ′ /c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automated Compression with Reinforcement Learning</head><p>AMC leverages reinforcement learning for efficient search over action space. Here we introduce the detailed setting of reinforcement learning framework.</p><p>The State Space For each layer t, we have 11 features that characterize the state s t :</p><formula xml:id="formula_0">(t, n, c, h, w, stride, k, F LOP s[t], reduced, rest, a t−1 ) (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where t is the layer index, the dimension of the kernel is n×c×k ×k, and the input is c × h × w. F LOP s[t] is the FLOPs of layer L t . Reduced is the total number of reduced FLOPs in previous layers. Rest is the number of remaining FLOPs in the following layers. Before being passed to the agent, they are scaled within [0, 1]. Such features are essential for the agent to distinguish one convolutional layer from another.</p><p>The Action Space Most of the existing works use discrete space as coarsegrained action space (e.g., {64, 128, 256, 512} for the number of channels). Coarsegrained action space might not be a problem for a high-accuracy model architecture search. However, we observed that model compression is very sensitive to sparsity ratio and requires fine-grained action space, leading to an explosion of the number of discrete actions (Sec. 4.2). Such large action spaces are difficult to explore efficiently <ref type="bibr" target="#b32">[32]</ref>. Discretization also throws away the order: for example, 10% sparsity is more aggressive than 20% and far more aggressive than 30%.</p><p>As a result, we propose to use continuous action space a ∈ (0, 1], which enables more fine-grained and accurate compression.</p><p>DDPG Agent As illustrated in Figure <ref type="figure">1</ref>, the agent receives an embedding state s t of layer L t from the environment and then outputs a sparsity ratio as action a t . The underlying layer is compressed with a t (rounded to the nearest feasible fraction) using a specified compression algorithm (e.g., channel pruning). Then the agent moves to the next layer L t+1 , and receives state s t+1 . After finishing the final layer L T , the reward accuracy is evaluated on the validation set and returned to the agent. For fast exploration, we evaluate the reward accuracy without fine-tuning, which is a good approximation for fine-tuned accuracy (Sec. 4.1).</p><p>We use the deep deterministic policy gradient (DDPG) for continuous control of the compression ratio, which is an off-policy actor-critic algorithm. For the exploration noise process, we use truncated normal distribution:</p><formula xml:id="formula_2">µ ′ (s t ) ∼ TN µ (s t | θ µ t ) , σ 2 , 0, 1<label>(2)</label></formula><p>During exploitation, noise σ is initialized as 0.5 and is decayed after each episode exponentially.</p><p>Following Block-QNN <ref type="bibr" target="#b54">[54]</ref>, which applies a variant form of Bellman's Equation <ref type="bibr" target="#b50">[50]</ref>, each transition in an episode is (s t , a t , R, s t+1 ), where R is the reward after the network is compressed. During the update, the baseline reward b is subtracted to reduce the variance of gradient estimation, which is an exponential moving average of the previous rewards <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b5">6]</ref>:</p><formula xml:id="formula_3">Loss = 1 N i y i − Q s i , a i | θ Q 2 y i = r i − b + γQ(s i+1 , µ(s i+1 ) | θ Q ) (3)</formula><p>The discount factor γ is set to 1 to avoid over-prioritizing short-term rewards <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Search Protocols</head><p>Resource-Constrained Compression By limiting the action space (the sparsity ratio for each layer), we can accurately arrive at the target compression ratio. Following <ref type="bibr" target="#b57">[57,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b54">54]</ref>, we use the following reward:</p><formula xml:id="formula_4">R err = −Error<label>(4)</label></formula><p>This reward offers no incentive for model size reduction, so we the achieve target compression ratio by an alternative way: limiting the action space. Take finegrained pruning for model size reduction as an example: we allow arbitrary action a at the first few layers; we start to limit the action a when we find that the budget is insufficient even after compressing all the following layers with most aggressive strategy. Algorithm 1 illustrates the process. (For channel pruning, the code will be longer but similar, since removing input channels of layer L t will also remove the corresponding output channels of L t−1 , reducing parameters/FLOPs of both layers). Note again that our algorithm is not limited to constraining model size and it can be replaced by other resources, such as FLOPs or the actual inference time on mobile device. Based on our experiments (Sec. 4.1), as the agent receives no incentive for going below the budget, it can precisely arrive at the target compression ratio.</p><p>Accuracy-Guaranteed Compression By tweaking the reward function, we can accurately find out the limit of compression that offers no loss of accuracy. We empirically observe that Error is inversely-proportional to log(F LOP s) or log(#P aram) <ref type="bibr" target="#b6">[7]</ref>. Driven by this, we devise the following reward function:</p><formula xml:id="formula_5">R FLOPs = −Error • log(FLOPs)<label>(5)</label></formula><formula xml:id="formula_6">R Param = −Error • log(#Param)<label>(6)</label></formula><p>This reward function is sensitive to Error; in the meantime, it provides a small incentive for reducing FLOPs or model size. Based on our experiments in Figure <ref type="figure" target="#fig_0">4</ref>.1, we note that our agent automatically finds the limit of compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>For fine-grained pruning <ref type="bibr" target="#b19">[19]</ref>, we prune the weights with least magnitude. The maximum sparsity ratio a max for convolutional layers is set to 0. </p><formula xml:id="formula_7">W all ← k W k W rest ← k=t+1 W k</formula><p>⊲ Compute the number of parameters we have to reduce in the current layer if all the later layers are pruned with the maximum sparsity ratio. α is the target sparsity ratio of the whole model.</p><formula xml:id="formula_8">W duty ← α • W all − action max • W rest − W reduced ⊲ Bound action t if it is too small to meet the target model size reduction action t ← max(action t , W duty /W t )</formula><p>⊲ Update the accumulation of reduced model size W reduced ← W reduced + action t • W t return action t fully connected layer is set to 0.98. For channel pruning, we use max response selection (pruning the weights according to the magnitude <ref type="bibr" target="#b20">[20]</ref>), and preserve Batch Normalization <ref type="bibr" target="#b25">[25]</ref> layers during pruning instead of merging them into convolutional layers. The maximum sparsity ratios a max for all layers are set to 0.8. Note that the manual upper bound a max is only intended for faster search, one can simply use a max = 1 which also produces similar results. Our actor network µ has two hidden layers, each with 300 units. The final output layer is a sigmoid layer to bound the actions within (0, 1). Our critic network Q also had two hidden layers, both with 300 units. Actions arr included in the second hidden layer. We use τ = 0.01 for the soft target updates and train the network with 64 as batch size and 2000 as replay buffer size. Our agent first explores 100 episodes with a constant noise σ = 0.5, and then exploits 300 episodes with exponentially decayed noise σ. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CIFAR-10 and Analysis</head><p>We conduct extensive experiments and fully analyze our AMC on CIFAR-10 <ref type="bibr" target="#b28">[28]</ref>to verify the effectiveness of the 2 search protocols. CIFAR dataset consists of 50k training and 10k testing 32 × 32 tiny images in ten classes. We split the training images into 45k/5k train/validation. The accuracy reward is obtained on validation images. Our approach is computationally efficient: the RL can finish searching within 1 hour on a single GeForce GTX TITAN Xp GPU.</p><p>FLOPs-Constrained Compression. We conducted FLOPs-constrained experiments on CIFAR-10 with channel pruning. We compare our approach with three empirical policies <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b22">22]</ref> illustrated in Figure <ref type="figure">2</ref>: uniform sets compression ratio uniformly, shallow and deep aggressively prune shallow and deep layers respectively. Based on sparsity distribution of different networks, a different strategy might be chosen.</p><p>In Table <ref type="table" target="#tab_1">2</ref>, we show us using reward R err to accurately find the sparsity ratios for pruning 50% for Plain-20 and ResNet-56 <ref type="bibr" target="#b21">[21]</ref> and compare it with empirical policies. We outperform empirical policies by a large margin. The best pruning setting found by AMC differs from hand-crafted heuristic (Figure <ref type="figure">2</ref>). It learns a bottleneck architecture <ref type="bibr" target="#b21">[21]</ref>.</p><p>Accuracy-Guaranteed Compression. By using the R Param reward, our agent can automatically find the limit of compression, with smallest model size and little loss of performance. As shown in Table <ref type="table" target="#tab_1">2</ref>, we compress ResNet-50 with finegrained pruning on CIFAR-10. The result we obtain has up to 60% compression ratio with even a little higher accuracy on both validation set and test set, which might be in light of the regularization effect of pruning.  <ref type="figure">2</ref>. Comparisons of pruning strategies for Plain-20 under 2×. Uniform policy sets the same compression ratio for each layer uniformly. Shallow and deep policies aggressively prune shallow and deep layers respectively. Policy given by AMC looks like sawtooth, which resembles the bottleneck architecture <ref type="bibr" target="#b21">[21]</ref>. The accuracy given by AMC outperforms hand-crafted policies. (better viewed in color )</p><formula xml:id="formula_9">0</formula><p>Since our reward R Param focuses on Error and offers very little incentive to compression in the meantime, it prefers the high-performance model with harmless compression. To shorten the search time, we obtain the reward using validation accuracy without fine-tuning. We believe if reward were fine-tuned accuracy, the agent should compress more aggressively, because the fine-tuned accuracy is much closer to the original accuracy.</p><p>Speedup policy exploration. Fine-tuning a pruned model usually takes a very long time. We observe a correlation between the pre-fine-tune accuracy and the post fine-tuning accuracy <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22]</ref>. As shown in Table <ref type="table" target="#tab_1">2</ref>, policies that obtain higher validation accuracy correspondingly have higher fine-tuned accuracy. This enables us to predict final model accuracy without fine-tuning, which results in an efficient and faster policy exploration.</p><p>The validation set and test set are separated, and we only use the validation set to generate reward during reinforcement learning. In addition, the compressed models have fewer parameters. As shown in Table <ref type="table" target="#tab_1">2</ref>, the test accuracy and validation accuracy are very close, indicating no over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageNet</head><p>On ImageNet, we use 3000 images from the training set to evaluate the reward function in order to prevent over-fitting. The latency is measured with 224 × 224 input throughout the experiments.</p><p>Push the Limit of Fine-grained Pruning. Fine-grained pruning method prunes neural networks based on individual connections to achieve sparsity in both weights and activations, which is able to achieve higher compression ratio and can be accelerated with specialized hardware such as <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b39">39]</ref>. However, it requires iterative prune &amp; fine-tune procedure to achieve decent performance <ref type="bibr" target="#b20">[20]</ref>, and single-shot pruning without retraining will greatly hurt the prediction accuracy Crests: our RL agent automatically learns 3x3 convolutions have more redundancy and can be pruned more.</p><p>Peaks: our RL agent automatically learns 1x1 convolutions have less redundancy and can be pruned less.</p><p>Fig. <ref type="figure">3</ref>. The pruning policy (sparsity ratio) given by our reinforcement learning agent for ResNet-50. With 4 stages of iterative pruning, we find very salient sparsity pattern across layers: peaks are 1 × 1 convolution, crests are 3 × 3 convolution. The reinforcement learning agent automatically learns that 3 × 3 convolution has more redundancy than 1 × 1 convolution and can be pruned more. with large compression rate (say 4×), which cannot provide useful supervision for reinforcement learning agent.</p><p>To tackle the problem, we follow the settings in <ref type="bibr" target="#b15">[16]</ref> to conduct 4-iteration pruning &amp; fine-tuning experiments, where the overall density of the full model is set to [50%, 35%, 25% and 20%] in each iteration. For each stage, we run AMC to determine the sparsity ratio of each layer given the overall sparsity. The model is then pruned and fine-tuned for 30 epochs following common protocol. With that framework, we are able to push the expert-tuned compression ratio of ResNet-50 on ImageNet from 3.4× to 5× (see Figure <ref type="figure" target="#fig_0">4</ref>) without loss of performance on ImageNet (original ResNet50's [top-1, top-5] accuracy=[76.13%, 92.86%]; AMC pruned model's accuracy=[76.11%, 92.89%]). The density of each layer during each stage is displayed in Figure <ref type="figure">3</ref>. The peaks and crests show that the RL agent automatically learns to prune 3 × 3 convolutional layers with larger sparsity, Table <ref type="table">3</ref>. Comparison with handcrafted heuristics. AMC improves the performance over human heuristics. (For reference, the baseline VGG-16 has 70.5% top-1 and 89.9% top-5 accuracy; the baseline MobileNet-V1 has 70.9% top-1 and 89.9% top-5 accuracy; the baseline MobileNet-V2 has 71.8% top-1 and 91% top-5 accuracy).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>policy</head><p>FLOPs ∆acc % VGG-16 FP (handcraft) <ref type="bibr" target="#b31">[31]</ref> 20% -14.6 RNP (handcraft) <ref type="bibr" target="#b33">[33]</ref> -3.58 SPP (handcraft) <ref type="bibr" target="#b49">[49]</ref> -2.3 CP (handcraft) <ref type="bibr" target="#b22">[22]</ref> -1.  <ref type="figure" target="#fig_0">4</ref>. We can find that the density distribution of AMC is quite different from human expert's result shown in Table <ref type="table">3</ref>.8 of <ref type="bibr" target="#b15">[16]</ref>, suggesting that AMC can fully explore the design space and allocate sparsity in a better way.</p><p>Comparison with Heuristic Channel Reduction. Here we compare AMC with existing state-of-the-art channel reduction methods: FP <ref type="bibr" target="#b31">[31]</ref>, RNP <ref type="bibr" target="#b33">[33]</ref> and SPP <ref type="bibr" target="#b49">[49]</ref>. All the methods proposed a heuristic strategy to design the pruning ratio of each layer. FP <ref type="bibr" target="#b31">[31]</ref> proposed a sensitive analysis scheme to estimate the sensitivity of each layer by evaluating the accuracy with single layer pruned. Layers with lower sensitivity are pruned more aggressively. Such method assumes that errors of different pruned layers can be summed up linearly, which does not stand according to our experiments. RNP <ref type="bibr" target="#b33">[33]</ref> groups all convolutional channels into 4 sets and trains a RL agent to decide on the 4 sets according to input image. However, the action space is very rough (only 4 actions for each layer), and it cannot reduce the model size. SPP <ref type="bibr" target="#b49">[49]</ref> applies PCA analysis to each layer and takes the reconstruction error as the sensitivity measure to determine the pruning ratios. Such analysis is conducted based on one single layer, failing to take the correlations between layers into consideration. We also compare our method to the original channel pruning paper (CP in Table <ref type="table">3</ref>), in which the sparsity ratios of pruned VGG-16 <ref type="bibr" target="#b45">[45]</ref> are carefully tuned by human experts (conv5 are skipped, sparsity ratio for conv4 and remaining layers is 1 : 1.5). The results of pruned VGG-16 are presented in Table <ref type="table">3</ref>. Consistent with our CIFAR-10 experiments (Sec. 4.1), AMC outperforms all heuristic methods by more than 0.9%, and beats human expert by 0.3% without any human labor. Apart from VGG-16, we also test AMC on modern efficient neural networks MobileNet-V1 <ref type="bibr" target="#b23">[23]</ref> and MobileNet-V2 <ref type="bibr" target="#b44">[44]</ref>. Since the networks have already been very compact, it is much harder to further compress them. The easiest way to reduce the channels of a model is to use uniform channel shrinkage, i.e. use a width multiplier to uniformly reduce the channels of each layer with a fixed ratio. Both MobileNet-V1 and MobileNet-V2 present the performance of different multiplier and input sizes, and we compare our pruned result with models of same computations. The format are denoted as uniform (depth multiplier -input size). We can find that our method consistently outperforms the uniform baselines. Even for the current state-of-the-art efficient model design MobileNet-V2, AMC can still improve its accuracy by 1.0% at the same computation (Table <ref type="table">3</ref>). The pareto curve of MobileNet-V1 is presented in Figure <ref type="figure">5a</ref>. Speedup Mobile Inference. Mobile inference acceleration has drawn people's attention in recent years. Not only can AMC optimize FLOPs and model size, it can also optimize the inference latency, directly benefiting mobile developers. For all mobile inference experiments, we use TensorFlow Lite framework for timing evaluation.</p><p>We prune MobileNet-V1 <ref type="bibr" target="#b23">[23]</ref>, a highly compact network consisting of depthwise convolution and point-wise convolution layers, and measure how much we can improve its inference speed. Previous attempts using hand-crafted policy to prune MobileNet-V1 led to significant accuracy degradation <ref type="bibr" target="#b31">[31]</ref>: pruning MobileNet-V1 to 75.5% original parameters results in 67.2% top-1 accuracy ⋆ , which is even worse than the original 0.75 MobileNet-V1 (61.9% parameters with ⋆ http://machinethink.net/blog/compressing-deep-neural-nets/ Table <ref type="table">4</ref>. AMC speeds up MobileNet-V1. Previous attempts using hand-crafted policy to prune MobileNet-V1 lead to significant accuracy degradation <ref type="bibr" target="#b31">[31]</ref> while AMC pruned models well preserve the accuracy. On NVIDIA Titan XP GPU AMC achieves 1.53× speedup with batch size 50; On Google Pixel-1 CPU, AMC achieves 1.95× speedup with batch size one, while saving the memory by 34%. The AMC pruned MobileNet is not only faster but also more accurate than 0.75 MobileNet. The image size is 224×224 for all experiments and no quantization is applied for apple-to-apple comparison.  <ref type="figure">5a</ref>, human expert's hand-crafted policy achieves slightly worse performance than that of the original MobileNet-V1 under 2× FLOPs reduction. However, with AMC, we significantly raise the pareto curve, improving the accuracy-MACs trade-off of original MobileNet-V1.</p><p>By substituting FLOPs with latency, we can change from FLOPs-constrained search to latency-constrained search and directly optimize the inference time. Our experiment platform is Google Pixel 1 with Qualcomm Snapdragon 821 SoC. As shown in Figure <ref type="figure">5b</ref>, we greatly reduce the inference time of MobileNet-V1 under the same accuracy. We also compare our learning based policy with a heuristic-based policy <ref type="bibr" target="#b52">[52]</ref>, and AMC better trades off accuracy and latency. Furthermore, since AMC uses the validation accuracy before fine-tuning as the reward signal while <ref type="bibr" target="#b52">[52]</ref> needs local fine-tuning after each step, AMC is more sampling-efficient, requiring fewer GPU hours for policy search.</p><p>We show the detailed statistics of our pruned model in Table <ref type="table">4</ref>. Model searched with 0.5× FLOPs and 0.5× inference time are profiled and displayed. For 0.5× FLOPs setting, we achieve 1.81× speed up on a Google Pixel 1 phone, and for 0.5× FLOPs setting, we accurately achieve 1.95× speed up, which is very close to actual 2× target, showing that AMC can directly optimize inference time and achieve accurate speed up ratio. We achieve 2.01× speed up for 1×1 convolution but less significant speed up for depth-wise convolution due to the small computation to communication ratio. AMC compressed models also consumes less memory. On GPUs, we also achieve up to 1.5× speedup, less than Table <ref type="table">5</ref>. Compressing Faster R-CNN with VGG16 on PASCAL VOC 2007. Consistent with classification task, AMC also results in better performance under the same compression ratio on object detection task. mAP (%) mAP [0.5, 0.95] (%) baseline 68.7 36.7 2× handcrafted <ref type="bibr" target="#b22">[22]</ref> 68.3 (-0.4) 36.7 (-0.0) 4× handcrafted <ref type="bibr" target="#b22">[22]</ref> 66.9 (-1.8)</p><p>35.1 (-1.6) 4× handcrafted <ref type="bibr" target="#b53">[53]</ref> 67.8 (-0.9) 36.5 (-0.2) 4× AMC (ours) 68.8 (+0.1) 37.2 (+0.5) mobile phone, which is because a GPU has higher degree of parallelism than a mobile phone.</p><p>Generalization Ability. We evaluate the generalization ability of AMC on PASCAL VOC object detection task <ref type="bibr" target="#b12">[13]</ref>. We use the compressed VGG-16 (from Sec 4.2) as the backbone for Faster R-CNN <ref type="bibr" target="#b43">[43]</ref>. In Table <ref type="table">5</ref>, AMC achieves 0.7% better mAP[0.5, 0.95]) than the best hand-crafted pruning methods under the same compression ratio. AMC even surpasses the baseline by 0.5% mAP. We hypothesize this improvement as that the optimal compression policy found by the RL agent also serves as effective regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Conventional model compression techniques use hand-crafted features and require domain experts to explore a large design space and trade off between model size, speed, and accuracy, which is usually suboptimal and labor-consuming. In this paper, we propose AutoML for Model Compression (AMC), which leverages reinforcement learning to automatically search the design space, greatly improving the model compression quality. We also design two novel reward schemes to perform both resource-constrained compression and accuracy-guaranteed compression.</p><p>Compelling results have been demonstrated for MobileNet-V1, MobileNet-V2, ResNet and VGG on Cifar and ImageNet. The compressed model generalizes well from classification to detection tasks. On the Google Pixel 1 mobile phone, we push the inference speed of MobileNet-V1 from 8.1 fps to 16.0 fps. AMC facilitates efficient deep neural networks design on mobile devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Our reinforcement learning agent (AMC) can prune the model to a lower density compared with human experts without losing accuracy. (Human expert: 3.4× compression on ResNet50. AMC : 5× compression on ResNet50.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>AMC ( a )Fig. 5 .</head><label>a5</label><figDesc>Fig. 5. (a) Comparing the accuracy and MAC trade-off among AMC, human expert, and unpruned MobileNet-v1. AMC strictly dominates human expert in the pareto optimal curve. (b) Comparing the accuracy and latency trade-off among AMC, NetAdapt, and unpruned MobileNet-V1. AMC significantly improves the pareto curve of MobileNet-V1. Reinforcement-learning based AMC surpasses heuristic-based NetAdapt on the pareto curve (inference time both measured on Google Pixel 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>8, and a max for Algorithm 1 Predict the sparsity ratio action t for layer L t with constrained model size (number of parameters) using fine-grained pruning ⊲ Initialize the reduced model size so far if t is equal to 0 then W reduced ← 0 end if ⊲ Compute the agent's action and bound it with the maximum sparsity ratio action t ← µ ′ (s t ) action t ← min(action t , action max ) ⊲ Compute the model size of the whole model and all the later layers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Pruning policy comparison of Plain-20, ResNets<ref type="bibr" target="#b21">[21]</ref> on CIFAR-10<ref type="bibr" target="#b28">[28]</ref>. RErr corresponds to FLOPs-constrained compression with channel pruning, while RParam corresponds to accuracy guaranteed compression with fine-grained pruning. For both shallow network Plain-20 and deeper network ResNets, AMC outperforms hand-crafted policies by a large margin. This enables efficient exploration without fine-tuning. Although AMC makes many trials on model architecture, we have separate validation and test dataset. No over-fitting is observed.</figDesc><table><row><cell>Model</cell><cell>Policy</cell><cell>Ratio</cell><cell cols="3">Val Acc. Test Acc. Acc. after FT.</cell></row><row><cell></cell><cell>deep (handcraft)</cell><cell></cell><cell>79.6</cell><cell>79.2</cell><cell>88.3</cell></row><row><cell>Plain-20 (90.5%)</cell><cell>shallow (handcraft) uniform (handcraft)</cell><cell>50% FLOPs</cell><cell>83.2 84.0</cell><cell>82.9 83.9</cell><cell>89.2 89.7</cell></row><row><cell></cell><cell>AMC (R Err )</cell><cell></cell><cell>86.4</cell><cell>86.0</cell><cell>90.2</cell></row><row><cell>ResNet-56 (92.8%)</cell><cell>uniform (handcraft) deep (handcraft) AMC (R Err )</cell><cell>50% FLOPs</cell><cell>87.5 88.4 90.2</cell><cell>87.4 88.4 90.1</cell><cell>89.8 91.5 91.9</cell></row><row><cell>ResNet-50 (93.53%)</cell><cell cols="3">AMC (R Param ) 60% Params 93.64</cell><cell>93.55</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Quoc Le, Yu Wang and Bill Dally for the supportive feedback. We thank Jiacong Chen for drawing the cartoon on the left of Figure <ref type="figure">1</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09639</idno>
		<title level="m">Compact deep convolutional neural networks with coarse pruning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Rhinehart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beainy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06030</idno>
		<title level="m">N2n learning: Network to network compression via policy gradient reinforcement learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Bagherinezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06473</idno>
		<title level="m">Lcnn: Lookup-based convolutional neural network</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02167</idno>
		<title level="m">Designing neural network architectures using reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05344</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Reinforcement learning for architecture search by network transformation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04873</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">An analysis of deep neural network models for practical applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Canziani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07678</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05641</idno>
		<title level="m">Net2net: Accelerating learning via knowledge transfer</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
		<title level="m">Binarynet: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting linear structure within convolutional networks for efficient evaluation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1269" to="1277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">More is less: A more complicated network with less inference complexity</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08651</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The PAS-CAL Visual Object Classes Challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient methods and hardware for deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<ptr target="https://stacks.stanford.edu/file/druid:qf934gh3708/EFFICIENT%20METHODS%" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m">20AND%20HARDWARE%20FOR%20DEEP%20LEARNING-augmented</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ese: Efficient speech recognition engine with sparse lstm on fpga</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Eie: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International Symposium on Computer Architecture</title>
				<meeting>the 43rd International Symposium on Computer Architecture</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03250</idno>
		<title level="m">Network trimming: A data-driven neuron pruning approach towards efficient deep architectures</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3866</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06530</idno>
		<title level="m">Compression of deep convolutional neural networks for fast and low power mobile applications</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.09308</idno>
		<title level="m">Fast algorithms for convolutional neural networks</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rakhuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6553</idno>
		<title level="m">Speeding-up convolutional neural networks using fine-tuned cp-decomposition</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<title level="m">Pruning filters for efficient convnets</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Runtime neural pruning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2178" to="2188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06342</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domainadaptive deep network compression</title>
		<author>
			<persName><forename type="first">M</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<title level="m">Fast training of convolutional networks through ffts</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Francon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Navruzyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duffy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hodjat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00548</idno>
		<title level="m">Evolving deep neural networks</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pruning convolutional neural networks for resource efficient transfer learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>CoRR, abs/1611.06440</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scnn: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Channel-level acceleration of deep face representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2163" to="2175" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xnor-net: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="525" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01041</idno>
		<title level="m">Large-scale evolution of image classifiers</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04381</idno>
		<title level="m">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Evolving neural networks through augmenting topologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Evolutionary computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7580</idno>
		<title level="m">Fast convolutional nets with fbfft: A gpu performance evaluation</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06994</idno>
		<title level="m">Structured probabilistic pruning for deep convolutional neural network acceleration</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning from delayed rewards</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">King&apos;s College</title>
				<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Restructuring of deep neural network acoustic models with singular value decomposition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="2365" to="2369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Go</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03230</idno>
		<title level="m">Netadapt: Platform-aware neural network adaptation for mobile applications</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Practical network blocks design with q-learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05552</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01064</idno>
		<title level="m">Trained ternary quantization</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07012</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
