<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Hallucination Using Neighbor Embedding over Visual Primitive Manifolds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yan</forename><surname>Yeung</surname></persName>
							<email>dyyeung@cse.ust.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Image Hallucination Using Neighbor Embedding over Visual Primitive Manifolds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">190044BE485ED2A56517244180810B38</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel learning-based method for image hallucination, with image super-resolution being a specific application that we focus on here. Given a low-resolution image, its underlying higher-resolution details are synthesized based on a set of training images. In order to build a compact yet descriptive training set, we investigate the characteristic local structures contained in large volumes of small image patches. Inspired by recent progress in manifold learning research, we take the assumption that small image patches in the low-resolution and high-resolution images form manifolds with similar local geometry in the corresponding image feature spaces. This assumption leads to a super-resolution approach which reconstructs the feature vector corresponding to an image patch by its neighbors in the feature space. In addition, the residual errors associated with the reconstructed image patches are also estimated to compensate for the information loss in the local averaging process. Experimental results show that our hallucination method can synthesize higher-quality images compared with other methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image super-resolution refers to the process by which a higher-resolution enhanced image is synthesized from one or more low-resolution images. It finds a number of real-world applications, which include restoring historic photographs, enlarging "thumbnail" images on web pages, and image-based rendering for high-quality display purposes. Practical super-resolution methods may make use of a single still image or a sequence of consecutive video frames with sub-pixel translation for synthesizing a higherresolution image. In this paper, we focus on the problem of single-image "hallucination" with the goal of inferring some high-resolution details missing in the original image that cannot be achieved by simple sharpening.</p><p>In recent years, there has been a good deal of research into learning-based approaches for image hallucination as well as other related low-level vision problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>. These learning-based methods share the common characteristic of using a training set of image (observation) and scene (state) pairs to build a co-occurrence model. With the learnt model, one can then predict the missing details in the observed input image by "borrowing" information from some similar examples in the training set.</p><p>Due to the contiguous nature of objects and surfaces in visual environments, images from natural scenes only constitute a minuscule fraction of the space of all possible images <ref type="bibr" target="#b8">[9]</ref>. However, it is difficult, if not totally impossible, to precisely model the probability distribution of natural images for the generic super-resolution task. Instead, what we can do is to study the distribution of small image patches and see what kinds of local image structures (e.g., edges or corners) are likely to occur in the image. These local image patches, from either the low-resolution or high-resolution image, are the building blocks of our super-resolution or image hallucination approach. They are expected to lie along a continuous nonlinear manifold embedded in a high-dimensional image space. Inspired by a well-known manifold learning method called locally linear embedding (LLE) <ref type="bibr" target="#b9">[10]</ref>, we assume that small image patches in the low-resolution and high-resolution images form manifolds with similar local geometry in the corresponding image spaces. This assumption generally holds as long as the image patches are associated with image primitives and the feature descriptions for the two corresponding images are both isometric. Our contribution is to devise an effective method for generic image hallucination using locally linear fitting and a learnt image primitive model.</p><p>A flowchart of our image hallucination approach is shown in Figure <ref type="figure" target="#fig_0">1</ref>. We highlight the major steps of our approach here. In the learning phase, large volumes of image primitive patches are extracted from both the low-resolution and high-resolution images used for training. A training set is constructed by analyzing the local neighborhood relation-1 1-4244-1180-7/07/$25.00 ©2007 IEEE ships between the low-resolution and high-resolution image patches and keeping only the isometric regions along the two manifolds. During the synthesis phase, a low-resolution image is presented to the system. Each element in the target high-resolution image comes from an optimal linear reconstruction by its nearest neighbors in the training set. Moreover, the residual errors associated with the reconstructed image patches are also estimated to compensate for the information loss in the local averaging process. Finally, we enforce the local compatibility and smoothness constraints between patches in the target high-resolution image through overlapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local neighbor embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual error esimation</head><p>Training Set The rest of this paper is organized as follows. In Section 2, we give a brief overview of the background and some related work. The problem setting and detailed algorithm are described in Sections 3 and 4, respectively. Section 5 presents some experimental results, demonstrating the effectiveness and efficiency of our method. Finally, we conclude our paper in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image super-resolution is intrinsically an ill-posed problem since, theoretically, many high-resolution images can give rise to the same low-resolution image through some operations such as smoothing and subsampling. Traditional pixel interpolation methods with smoothness priors, such as pixel replication and cubic-spline interpolation, introduce artifacts and blurred edges. Reconstruction-based methods <ref type="bibr" target="#b7">[8]</ref> aim to restore the lost image details by requiring the down-sampled version of the high-resolution reconstructed image to be as close to the original low-resolution image as possible. However, in the absence of additional information, these generic constraints are not every effective for synthesizing perceptually plausible images which are of higher quality than the original low-resolution images.</p><p>Over the past few years, learning-based approaches have produced compelling results for various low-level vision tasks, including image hallucination <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>, image analogy <ref type="bibr" target="#b5">[6]</ref>, and texture synthesis <ref type="bibr" target="#b2">[3]</ref>. Despite some implementation-level differences, these algorithms are all similar in spirit. In the learning stage, they learn the underlying scene details that correspond to different image regions observed in the input. During the inference stage, they use those learned relationships to predict missing details in another image, which is the target high-resolution image for super-resolution problems. Thus each element in the target image comes from only one "best" example in the training set. The neighbor embedding method proposed by <ref type="bibr" target="#b1">[2]</ref> introduces a more general way of using the training examples. In their method, multiple training examples can contribute simultaneously to the generation of each image patch in the high-resolution image. The underlying assumption of the method is that small image patches in the low-resolution and high-resolution images form manifolds with similar local geometry in the two corresponding image spaces. However, they use uniformly sampled image patches from several training images to build a medium-sized training set, which may not satisfy the manifold assumption and hence may not lead to good generalization.</p><p>Motivated by the work of <ref type="bibr" target="#b10">[11]</ref> in applying primal sketch priors for image hallucination, we conjecture that the image patches associated with the image primitives (e.g., edges, corners, and blobs, similar to the primal sketches in <ref type="bibr" target="#b10">[11]</ref>) are of greater significance for building a good training set. This conjecture will be verified empirically in Section 4 through statistical analysis, showing that these image patches preserve well the local isometric relationships between the manifolds for the low-resolution and highresolution image patches.</p><p>Our method also benefits from the face hallucination work of <ref type="bibr" target="#b6">[7]</ref>, in which the relationships between the lowresolution and high-resolution residues are learnt by coupled PCA to refine the final hallucinated face image. In our case, the residual error vectors of neighboring examples are consistent, since they are expected to lie on a subspace perpendicular to the linear tangent plane of the curved manifold. Adding a simple average of these residues is sufficient for error compensation.</p><p>In the next two sections, we will formulate the problem more precisely and then present details of different components of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem setting</head><p>The single-image super-resolution problem that we want to solve can be formulated as follows. Given a lowresolution image L t as input, we estimate the target highresolution image H t with the help of a training set of one or more low-resolution images L s and the corresponding highresolution images H s . We represent each low-resolution or high-resolution image as a set of small overlapping image patches.</p><p>Ideally, each patch generated for the high-resolution image H t should not only be related appropriately to the corresponding patch in the low-resolution image L t , but should also preserve some inter-patch relationships with adjacent patches in H t . The former determines the accuracy while the latter determines the local compatibility and smoothness of the high-resolution image. To satisfy these requirements as much as possible, our method has the following properties: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Details of our method 4.1. Preprocessing</head><p>In the preprocessing step, a high-resolution natural image H (Figure <ref type="figure" target="#fig_2">2(c)</ref>) is blurred and sub-sampled to generate a corresponding low-resolution image L (Figure <ref type="figure" target="#fig_2">2</ref>(a)). Applying an initial enhancement through bilinear interpolation to L, we obtain an image H l (Figure <ref type="figure" target="#fig_2">2(b</ref>)) which has the same size as H but lacks the high-resolution details. In the training set, we only need to store the differences between H and H l (Figure <ref type="figure" target="#fig_2">2</ref>(e)), which correspond to the missing high-frequency components caused by the image degradation process. Through band-pass filtering, we further decompose each interpolated image H l into the sum of two images containing the medium and low spatial frequencies, respectively. Following the assumption in <ref type="bibr" target="#b4">[5]</ref> that the highest spatial-frequency components of the low-resolution image are most important for predicting the extra details of H, we only store the example patches from the medium frequency layer (Figure <ref type="figure" target="#fig_2">2(d)</ref>). Finally, to achieve good generalization, the high-and medium-frequency image pairs are contrast normalized by a local measure of energy in the image. We undo this normalization step later when we reconstruct the high-resolution image. The final output is the sum of the interpolated low-resolution image and the highfrequency predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image primitives for training</head><p>An essential factor attributing to the success of learningbased approaches is how to construct a good training set, which is descriptive enough in giving useful information about the image-scene relationships and is also compact enough for computational efficiency and good generalization. The patches extracted from the preprocessed images can be regarded as points in a vector space with each dimen- sion corresponding to one pixel in the patch. As natural images contain characteristic statistical regularities, we expect these feature points to lie on a continuous nonlinear manifold. Different regions of the manifold may correspond to characteristic image primitives such as edges, corners, blobs, etc. When building our training set, we put emphasis on the patches associated with these image primitives for two main reasons. First, the missing high-frequency details to be estimated are densely distributed over the regions of image primitives, as shown in Figure <ref type="figure" target="#fig_3">3</ref>(b) and 3(c). Focusing on these regions can lead to significant speedup as fewer patches need to be transformed. Second, we believe the local neighborhood relationships between lowresolution and high-resolution primitive patches in the two feature spaces are more consistent than those between general image patches. This is supported by our experimental investigation to be reported in Section 4.</p><p>The primitive patches are extracted by convolving the interpolated low-resolution image H l with a bank of maximum response (MR) filters (Figure <ref type="figure" target="#fig_3">3(a)</ref>) <ref type="bibr" target="#b12">[13]</ref>. The filter bank consists of a Gaussian kernel and a Laplacian of Gaussian kernel, which are arranged in three scales and six orientations each. To achieve scale invariance, the outputs are "collapsed" by recording only the maximum filter response across all scales. This reduces the number of responses for each pixel from 36 (six orientations at three scales for each of two oriented filters) to 12 (six orientations for each of two filters). Figure <ref type="figure" target="#fig_3">3</ref>(c) depicts the magnitude map of the filtered responses for a low-resolution image. Compared with the high-frequency difference image shown in Figure <ref type="figure" target="#fig_3">3</ref>(b), we can clearly see the consistent relationships between them.</p><p>To sum up, each example in the training set is in the form of a pair of primitive patches. These pairs capture the statistical relationships that we are interested in. We represent each image primitive by a 7 × 7 image patch, which is selected from the region in Figure <ref type="figure" target="#fig_3">3</ref>(c) with high magnitude value or energy level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Local neighbor embedding</head><p>The manifold structure of image patches characterizes the smooth variation corresponding to some regular transformations in natural images. For instance, we can expect that the manifold coordinates of the edge structure correspond to its orientation, translation, and blurring variations. In super-resolution problems, we further assume that manifolds of the small patches in the low-resolution and highresolution images bear similar local geometry in the two spaces. This assumption holds as long as the patches are associated with image primitives and the two feature descriptions are isometric.</p><p>In recent years, manifold learning (or nonlinear dimensionality reduction) methods have emerged as powerful tools for discovering a "faithful" low-dimensional representation of the original data embedded in some highdimensional observation space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref>. The main computations for these methods are based on tractable, polynomial-time optimizations, such as shortest path problems, least squares fits, semidefinite programming, and matrix diagonalization. Our super-resolution method to be described below has been inspired by the LLE algorithm <ref type="bibr" target="#b9">[10]</ref>. Its key idea is that the local geometry in the neighborhood of each data point can be characterized by linear coefficients that reconstruct the data point from its neighbors.</p><p>For convenience, we use l p s , h p s , l q t and h q t to denote the feature vectors as well as the corresponding low-and highresolution image patches, and L s , H s , L t and H t to denote the sets of feature vectors as well as the corresponding im-ages. The neighbor embedding algorithm of our method can be summarized as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm -Local neighbor embedding</head><p>Input: low-resolution test patches</p><formula xml:id="formula_0">L t = {l 1 t , l 2 t • • • , l nt t } Output: high-resolution patches H t = {h 1 t , h 2 t • • • , h nt t } Begin 1.</formula><p>For each patch l q t in image L t : (a) Find the set N q of K nearest neighbors in L s . (b) Compute the reconstruction weights of the neighbors that minimize the error of reconstructing l q t . (c) Compute the initial high-resolution embedding h q t using the appropriate high-resolution features of the K nearest neighbors and the reconstruction weights. (d) Estimate the high-resolution residual error vector e q t using the average residual error vector of the K nearest neighbors in N q , and update h q t with h q t + e q t . 2. Construct the target high-resolution image H t by enforcing the local compatibility and smoothness constraints between adjacent patches obtained in step 1(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End</head><p>We implement step 1(a) by using Euclidean distance to define neighborhood. Based on the K nearest neighbors identified, step 1(b) seeks to find the best reconstruction weights for each patch l q t in L t . Optimality is achieved by minimizing the local reconstruction error for l q t ε q = l q t -</p><formula xml:id="formula_1">l p s ∈Nq ω qp l p s 2<label>(1)</label></formula><p>which is the squared distance between l q t and its reconstruction, subject to the constraints l p s ∈Nq ω qp = 1 and ω qp = 0 for any l p s ∈ N q . Minimizing ε q subject to the constraints is a constrained least squares problem. We define a local Gram matrix G q for l q t as</p><formula xml:id="formula_2">G q = (l q t 1 T -L) T (l q t 1 T -L) (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where 1 is a column vector of ones and L is a D × K matrix with its columns being the neighbors of l q t . Moreover, we group the weights of the neighbors to form a Kdimensional weight vector w q by reordering the subscript p of each weight ω qp . The constrained least squares problem has the following closed-form solution:</p><formula xml:id="formula_4">w q = G -1 q 1 1 T G -1 q 1<label>(3)</label></formula><p>Instead of inverting G q , we apply a more efficient method by solving the linear system of equations G q w q = 1 and then normalizing the weights so that l p s ∈Nq ω qp = 1. After repeating steps 1(a) and 1(b) for all N t patches in L t , the reconstruction weights obtained form a weight matrix W = [ω qp ] Nt×Ns .</p><p>Step 1(c) computes the initial value of h q t based on W :</p><formula xml:id="formula_5">h q t = l p s ∈Nq ω qp h p s (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>Step 1(d) estimates the high-resolution residual error vector e q t of the linear reconstruction</p><formula xml:id="formula_7">e q t = 1 K l p s ∈Nq e p s = 1 K l p s ∈Nq (h p s - h r s ∈Np ω rp h r s )<label>(5)</label></formula><p>where e p s is the residual error of the neighboring patch l p s ∈ N q . It is calculated in the learning phase and stored together with its associated l p s . We use the average of e p s to estimate e q t , with the assumption that these neighboring residual error vectors are in approximately the same direction which is perpendicular to the tangent plane at h q t . In step 2, we use a simple method to enforce interpatch relationships by averaging the feature values in regions where adjacent patches overlap. Other more sophisticated methods may also be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training set revisited</head><p>An essential assumption of our local neighbor embedding method is that the linear reconstruction weight of l p s and that of its corresponding h p s should be approximately the same in the two corresponding image spaces. In this subsection, we evaluate this assumption by computing the standard linear correlation coefficient R(w l , w h ) between two groups of weight vectors, w l and w h . Evaluation is performed based on two settings, either using a randomly sampled patch set or using an image primitive patch set. Each data set contains around 25,000 pairs of low-resolution and high-resolution patches. Figure <ref type="figure" target="#fig_4">4</ref> shows the histograms of the correlation coefficient under the two settings, with its value ranging from -1 to 1. From this comparison, we can clearly see that the local neighborhood relationships between low-resolution and high-resolution primitive patches are more consistent than those between general image patches.</p><p>However, since the image primitives are extracted using a hard threshold on the filtered low-resolution image, it is inevitable that some "noisy" patches will be included in the training set, resulting in the low correlation part of Figure <ref type="figure" target="#fig_4">4</ref>(b). In our experiment, we only keep those primitive patches and their K nearest neighbors if their correlation coefficients are greater than 0.7. These patches form a compact training set which can characterize well the manifold structure of natural images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We build training sets for the super-resolution algorithm from band-pass and high-pass pairs taken from a set of training images (see Figure <ref type="figure" target="#fig_5">5</ref>). All the eight representative natural images were downloaded from a public web site. <ref type="foot" target="#foot_0">1</ref>They were taken with a Canon EOS D60 digital camera with a resolution of 500 × 433 pixels. About 400,000 primitive examples have been extracted from these training images. Our method has only three parameters to determine. The first parameter is the number of nearest neighbors K for neighbor embedding. Our experiments show that the superresolution result is not very sensitive to the choice of K. We set K to 5 for all our experiments. The second and third parameters are the patch size and the degree of overlap between adjacent patches. For both the low-resolution and high-resolution images, we use 7 × 7 pixel patches and let the overlap between adjacent patches be 4 pixels. The corresponding low-resolution and high-resolution image patches are properly aligned by their geometrical centers in the image plane. Principal component analysis (PCA) is performed on the low-resolution patch set to reduce its dimensionality to 15, which covers more than 98% of the total variance. The high-resolution feature vector is represented by concatenating all 7 × 7 pixels in the patch, since we cannot find an 'elbow' at which the eigenvalue curve ceases to decrease significantly with added dimensions. Note that we perform hallucination on the image intensity only because humans are more sensitive to the brightness information. The color channels are simply interpolated by a bilinear function.</p><p>We compare our approach with bicubic interpolation and the neighbor embedding method of Chang et al. <ref type="bibr" target="#b1">[2]</ref> on different super-resolution examples, all with a magnification factor of 3 (Figure <ref type="figure">6</ref>). When implementing the method in <ref type="bibr" target="#b1">[2]</ref>, we use uniformly sampled patches in the training images to build a training set with the same size as ours. It is clear to see that bicubic interpolation gives the smoothest result. Chang et al.'s method gives better result for some details in the images. On the other hand, sharper and smoother contours are hallucinated by our approach (e.g., see the edges of the leaf in the first example). <ref type="foot" target="#foot_1">2</ref>We also calculate the RMS error between the superresolution image generated and the ground-truth image as the number of nearest neighbors K varies over a range. Figure <ref type="figure">7</ref> shows the results for the four examples discussed above. As we can see, the RMS error attains its lowest value when K is between 4 and 6, showing that using multiple nearest neighbors (as opposed to only one nearest neighbor as in the existing methods) does give improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have proposed a learning-based method for image hallucination based on local neighbor embedding of the image primitive manifold constructed from an input image. In particular, we study image hallucination in the context of the single-image super-resolution problem.</p><p>A compact yet descriptive training set is constructed from characteristic regions in images where the manifold assumption holds well. Compared with other generic singleimage super-resolution methods, our method can synthesize higher-quality images. In our future work, we will apply a similar approach to other image hallucination problems. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Flowchart of our image hallucination approach.</figDesc><graphic coords="2,81.42,338.21,77.20,51.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Each patch in H t is associated with multiple patch transformations learned from the training set. (b) Local relationships between patches in L t should be preserved in H t . (c) Neighboring patches in H t are constrained through overlapping to enforce local compatibility and smoothness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Image preprocessing steps. (a) low-resolution image; (b) initial interpolation of (a) to a higher resolution; (c) original high-resolution image; (d) band-pass filtered and contrast normalized version of (b); (e) high-pass filtered and contrast normalized version of (c).</figDesc><graphic coords="3,355.03,167.23,77.29,51.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Filter bank used for primitive extraction; (b) highfrequency difference image to be estimated; (c) magnitude map of the filtered responses for the low-resolution image.</figDesc><graphic coords="4,71.84,139.85,101.21,67.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Histograms of correlation coefficient R(w l , w h ) between reconstruction weight vectors for l p s and h p s in the two image spaces under two settings: (a) randomly sampled image patches; (b) image primitive patches. The red arrows indicate the median values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Eight training images (downloaded from http://www.thedigital-picture.com/Gallery/) used in our experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Test images magnified by three times using (a) bicubic interpolation, (b) Chang et al.'s method, and (c) our approach; (d) original high-resolution image.</figDesc><graphic coords="7,196.30,311.84,105.91,70.68" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.the-digital-picture.com/Gallery/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Readers are recommended to see the enlarged electronic version of the figure.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has been supported by research grants HKUST621305 and N-HKUST602/05 from the Research Grants Council (RGC) of Hong Kong and the National Natural Science Foundation of China (NSFC).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003-06">June 2003. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004. 1, 2, 6</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh IEEE International Conference on Computer Vision</title>
		<meeting>the Seventh IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning lowlevel vision</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pasztor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Carmichael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hallucinating faces: Tensorpatch super-resolution and coupled residue compensation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image magnification using level-set reconstruction</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwartzwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Natural image statistics and efficient coding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Computation in Neural Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="333" to="339" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2000-12">December 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image hallucination with primal sketch priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000-12">December 2000. 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A statistical approach to texture classification from single images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised learning of image manifolds by semidefinite programming</title>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
