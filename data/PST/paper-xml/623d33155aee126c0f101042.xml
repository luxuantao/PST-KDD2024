<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Rationale-Centric Framework for Human-in-the-loop Machine Learning</title>
				<funder ref="#_YabP869">
					<orgName type="full">Pioneer and &quot;Leading Goose&quot; R&amp;D Program of Zhejiang</orgName>
				</funder>
				<funder ref="#_Ew8QbdX">
					<orgName type="full">Science Foundation Ireland</orgName>
					<orgName type="abbreviated">SFI</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinghui</forename><surname>Lu</surname></persName>
							<email>jinghui.lu@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">The Insight Centre for Data Analytics</orgName>
								<address>
									<country>University College Dublin</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
							<email>yanglinyi@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Advanced Technology</orgName>
								<orgName type="department" key="dep2">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brian</forename><forename type="middle">Mac</forename><surname>Namee</surname></persName>
							<email>brian.macnamee@ucd.ie</email>
							<affiliation key="aff0">
								<orgName type="department">The Insight Centre for Data Analytics</orgName>
								<address>
									<country>University College Dublin</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
							<email>zhangyue@westlake.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">Westlake University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Institute of Advanced Technology</orgName>
								<orgName type="department" key="dep2">Westlake Institute for Advanced Study</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Rationale-Centric Framework for Human-in-the-loop Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel rationale-centric framework with human-in-the-loop -Rationalescentric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios. By using static semi-factual generation and dynamic human-intervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation. Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios. We also perform extensive ablation studies to support in-depth analyses of each component in our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work finds that natural artefacts <ref type="bibr" target="#b3">(Gururangan et al., 2018)</ref> or spurious patterns <ref type="bibr" target="#b7">(Keith et al., 2020;</ref><ref type="bibr" target="#b30">Srivastava et al., 2020)</ref> in datasets can cause sub-optimal model performance for neural networks. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the bold phrases-"100% bad" and "brain cell killing"-are underlying causes for a negative sentiment prediction that most human readers would recognise. These are defined as rationales in this paper. The underlined phrase-"acting and plot"has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.</p><p>Spurious patterns (or associations) are caused by natural artefacts or biases in training data <ref type="bibr" target="#b13">(Lertvittayakumjorn and Toni, 2021)</ref>, and are usually useless, or even harmful, at test time. This issue can be severe in few-shot learning (FSL) scenarios. For instance, <ref type="bibr" target="#b11">Kulesza et al. (2010)</ref> suggests that when a model is trained with a small subset of labelled data, it is prone to exploiting spurious patterns leading to poor generalisability that is evident in the performance decay in outof-distribution (OOD) datasets. In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications <ref type="bibr" target="#b17">(Lu and MacNamee, 2020;</ref><ref type="bibr" target="#b16">Lu et al., 2021)</ref>.</p><p>There is a strand of research addressing this scenario that seeks to improve model performance by "introducing methods and resources for training models less sensitive to spurious patterns" <ref type="bibr" target="#b5">(Kaushik et al., 2020)</ref>. Most of this work relies on generating counterfactual augmented data (CAD), either manually <ref type="bibr" target="#b6">(Kaushik et al., 2021)</ref> or automatically <ref type="bibr" target="#b1">(Feng et al., 2021;</ref><ref type="bibr" target="#b23">Qian et al., 2021;</ref><ref type="bibr" target="#b41">Yang et al., 2021</ref><ref type="bibr">Yang et al., , 2020a;;</ref><ref type="bibr" target="#b0">Delaney et al., 2021)</ref>. For example, <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref> proposed a humanin-the-loop framework where human annotators are required to make minimal changes to original movie reviews to produce sentiment-flipped counterfactual reviews, which enables models to learn useful associations between input texts and output labels <ref type="bibr" target="#b6">(Kaushik et al., 2021)</ref>.</p><p>Generating manual counterfactuals, however, is expensive and time-consuming- <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref> report the cost of revising 2.5k instances at over $10,000. On the other hand, fully automatic methods are task-specific and therefore have weak robustness across domains and less reliabil- False Rationales:</p><p>Film is good.</p><p>Missed Rationale:</p><p>I like it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Augmented Training Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Human-intervened Correction</head><p>Model Rationales ity compared to manual counterfactuals. To address these issues, we propose Rationales-centric Double-robustness Learning (RDL), a human-inthe-loop framework for data augmentation in a few-shot setting, which is efficient, robust, modelagnostic, and general across tasks.</p><p>Our main idea is a rationale-centric strategy for eliminating the effect of spurious patterns by leveraging human knowledge as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Our double-robustness framework consists of two main modules. The first is a Static Semi-factual Generation module that generates a set of semifactual data automatically for a given instance by using human-identified rationales. Such labelling requires less human input compared to fully manual counterfactual generation (see Section 3.1). In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals <ref type="bibr" target="#b20">(McCloy and Byrne, 2002;</ref><ref type="bibr" target="#b8">Kenny and Keane, 2021)</ref>, as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same). Second, we apply a Dynamic Human-intervened Correction module, where the most salient features are identified for model predictions over a set of training examples, and human workers intervene by checking the correctness of the rationale in case first-round modifications introduce new artefacts. We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.</p><p>Results on a sentiment analysis task, which is also used in <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref> To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios. *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Data augmentation has been used for resolving artefacts in training datasets before <ref type="bibr" target="#b3">(Gururangan et al., 2018;</ref><ref type="bibr" target="#b30">Srivastava et al., 2020;</ref><ref type="bibr" target="#b6">Kaushik et al., 2021)</ref>. In particular, previous work <ref type="bibr" target="#b5">(Kaushik et al., 2020)</ref> relied on large-scale crowd-sourcing to generate useful augmented data. More recently, <ref type="bibr" target="#b41">Yang et al. (2021)</ref>, and <ref type="bibr" target="#b37">Wang and Culotta (2021)</ref> investigated the efficacy of the automatically generated counterfactuals for sentiment analysis. Similar to our work, these methods also consider the most salient features that a model uses when generating augmented data, which is in line with our rationale definition. However, they use sentiment lexicon matching for identifying rationales, which is task-specific and not necessarily fully relevant. In contrast, we employ human annotators to identify rationales, which can be task-agnostic and robust. Moreover, our method generates semi-factuals instead of counterfactuals used in previous work. Human-the-loop Machine Learning <ref type="bibr" target="#b39">(Wu et al., 2021)</ref> has received increasing research attention. Active learning <ref type="bibr" target="#b27">(Settles, 2009;</ref><ref type="bibr" target="#b19">Margatina et al., 2021)</ref>, the most common example of human-in-theloop machine learning, asks human annotators only to provide high-level annotations (i.e. labels) for important examples. There is also some work exploring more explainable AI systems by exploiting feature-based information. Such methods use relatively simple models such as Na?ve Bayes <ref type="bibr" target="#b32">(Stumpf et al., 2009;</ref><ref type="bibr" target="#b10">Kulesza et al., 2015)</ref> and Linear Regression with bag-of-words features <ref type="bibr">(Jia and Liang, 2017;</ref><ref type="bibr" target="#b34">Teso and Kersting, 2019;</ref><ref type="bibr" target="#b2">Ghai et al., 2021;</ref><ref type="bibr" target="#b28">Shao et al., 2021)</ref>, because these classifiers are relatively intuitive in generating explanations and amenable to incorporating human feedback.</p><p>Some other work uses simple neural networks such as multi-layer perceptrons <ref type="bibr" target="#b28">(Shao et al., 2021)</ref> and shallow CNNs <ref type="bibr" target="#b12">(Lertvittayakumjorn et al., 2020;</ref><ref type="bibr" target="#b31">Stammer et al., 2021;</ref><ref type="bibr" target="#b33">Teso et al., 2021)</ref> because the predictions of such models can be explained in the form of features. Very recently, <ref type="bibr">Yao et al. (2021)</ref> proposed a human-in-the-loop method to inspect more complicated models (e.g. <ref type="bibr">BERT)</ref> with the help of model-agnostic post-hoc explanation algorithms <ref type="bibr" target="#b24">(Ribeiro et al., 2018)</ref> that can explain predictions of any linear or non-linear model without exploiting its weights. However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance <ref type="bibr" target="#b14">(Li et al., 2020;</ref><ref type="bibr">Yang et al., 2020b)</ref>, instead of improving model robustness or generalisation ability. Also, they assume access to a large amount of labelled data. In contrast, we focus on few-shot learning scenarios which are more compelling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The RDL pipeline is shown in Figure <ref type="figure" target="#fig_2">2</ref> and consists of two modules: Static Semi-factual Generation and Dynamic Human-intervened Correction.</p><p>Static semi-factual generation is a more efficient alternative to manually generated counterfactuals <ref type="bibr" target="#b5">(Kaushik et al., 2020)</ref>. In the first phase, Rationale Marking (Section 3.1), human annotators review each document in the training set to provide rationales (i.e. phrases that support the document classification decisions shown as bold text in Figure <ref type="figure" target="#fig_2">2</ref>). The second phase is a semi-factual generation method based on synonym replacement (Section 3.2) that produces augmented examples (blue text in Figure <ref type="figure" target="#fig_2">2</ref> indicates replaced words), which are added into the training set.</p><p>Dynamic human-intervened correction (Section 3.3) is a rationales-powered human-in-the-loop framework to dynamically correct the model's behaviours. At the outset, sampling and sensitivity of contextual decomposition (SCD) <ref type="bibr" target="#b4">(Jin et al., 2019)</ref> is applied to detect the rationales given by the model that is obtained in the previous step. Then, all model-identified rationales (underlined texts in Figure <ref type="figure" target="#fig_2">2</ref>) are examined by human annotators to iden-tify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model). Both false rationales and missing rationales are corrected to produce augmented examples. Finally, newly generated examples are added into the training set to re-train the deep learning model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Rationale Marking</head><p>Following <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref> and <ref type="bibr" target="#b41">Yang et al. (2021)</ref>, we use the IMDb movie review dataset <ref type="bibr" target="#b18">(Maas et al., 2011)</ref> in our experiments. It consists of positive and negative movie reviews that are easy for human participants to understand, re-annotate, and provide feedback upon <ref type="bibr" target="#b44">(Zaidan et al., 2007)</ref>.</p><p>We use a crowdsourcing company to recruit editors and annotators for marking rationales that support classification decisions. At the outset, annotators were given instructions and examples that gently guided them to annotate rationales. Only adjectives, adverbs, nouns, and verbs were considered as rationales. Besides, rationales were required to carry complete semantic information. For example, for a phrase starting with a negation word such as "not great", annotators are instructed to mark the whole phrase "not great" as a rationale instead of just marking "not". We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams). Phrases consisting of numerical scores are not counted as rationales (e.g. 5 or 10 stars) since different datasets may use different rating scales, and annotating digits may hurt OOD performance.</p><p>Overall, we encouraged annotators to try their best to mark as many rationales as possible to explain classification labels. However, to guarantee the quality of rationale marking and prevent annotators from over including non-rationales for more payment, we also manually inspected annotated examples and rejected examples that contained incorrect rationales. After inspection, we rejected 10.6% of negative reviews and 7.6% of positive reviews. Editors and annotators re-annotated the rejected examples, which were then presented to us for another inspection. All re-annotated examples were approved only if all authors were happy with the quality of the annotations. Otherwise, the examples were re-annotated again.</p><p>Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments). Human annotators spent on average 183.68 seconds to identify rationales in a review and our method generated semi-factual examples automatically. On the contrary, workers spent on average 300 seconds to revise a review to generate a counterfactual manually as reported by <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref>. Note that our approach using 100 labelled examples can outperform manual CAD <ref type="bibr" target="#b5">(Kaushik et al., 2020)</ref> using the entire training set of 1,707 examples (see Section 5.3), making our approach 300?1707 183.68?100 ? 27.88 times more efficient than manually generated CAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Static Semi-factual Generation</head><p>We take a simple replacement strategy, which has been taken by <ref type="bibr" target="#b41">Yang et al. (2021)</ref>, to generate semifactual examples. Given a human-identified rationale, our method constructs augmented examples by automatically replacing non-rationale words, thus leading to examples with the same labels. This augmentation is consistent with semi-factual thinking: even if those non-rationales were changed, the label would not change.</p><p>Formally, given a training example x i = [t i1 , t i2 , ..., t ij ] (where t ij is the j th token of the i th document) and its ground truth label y i , we create a rationale vector r i = [a i1 , a i2 , ..., a ij ] where a ij is the value that indicates whether t ij is a rationale or not (we set a ij = 1 to indicate that t ij is a rationale and 0 otherwise). To generate a semi-factual example, x i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms. The synonyms can be provided by a human, retrieved automatically from a lexicon such as WordNet <ref type="bibr" target="#b21">(Miller, 1995)</ref>, or generated using the mask-filling function of a pretrained context-aware language model <ref type="bibr" target="#b15">(Liu et al., 2019)</ref>.</p><p>In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x i , with some replaced non-rationales and all the other tokens identical to x i . The label, y i , of a newly generated example is the same as the label of the original example, x i . Examples of generated data are shown in Table <ref type="table" target="#tab_1">1</ref>. Afterwards, the augmented examples are added into the training set used to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamic Human-intervened Correction</head><p>Dynamic human-intervened correction further improves the robustness of the model by allowing human annotators to correct the model rationales online. Firstly, SCD is applied to detect unigrams, bigrams or trigrams that are salient to the model. SCD is a technique to assess the importance of terms by continuously removing terms and measuring changes in prediction <ref type="bibr" target="#b4">(Jin et al., 2019)</ref>. Human annotators examine all rationales given by the model from all documents to discover two types of incorrect rationale: false rationales and missing rationales. The next phase allows human feedback to influence the learning process. To this end, for each type of incorrect rationale, we propose a corresponding strategy to correct them.</p><p>For false rationales (i.e. phrases that actually do not support classifications but are incorrectly identified by the model), we use synonym replacement again to generate semi-factual examples. Unlike the static semi-factual generation (Section 3.2), in this component we replace all false rationales with their synonyms instead of randomly replacing 5% of non-rationales in a document. Examples of generated data are shown in Table <ref type="table" target="#tab_2">2</ref>.</p><p>For missing rationales (i.e. phrases that actually support classifications but are not identified by the model), we take another simple semi-factual generation strategy, that is, extracting sentences that contain missing rationales to form semi-factual data. Specifically, given a sentence containing missing rationales, we use this sentence as a new example, and the label of this newly generated example is identical to that of the document where the sentence is extracted. For example, there is a positive movie review (bold font for rationales) "Robert Urich was a fine actor, and he makes this TV movie believable . I remember watching this film when I was 15 ....". The model fails to identify "fine" and "believable" as rationales. Thus we extract the text ""Robert Urich was a fine actor, and he makes this TV movie believable ." as a new example, and the class of this example is still positive. We extract the whole sentence rather than just the missing rationales to reserve more semantic information.</p><p>Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Examples Negative</head><p>Origin: The attempt at a "lesbian scene" was sad.</p><p>Augment 1: The hint at a "lesbian scene" was sad . Augment 2: The attempt at a "kiss scene" was sad .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive</head><p>Origin: I recommended this film a lot, specially in this difficult times for the planet . Augment 1: I recommended you film a lot, specially in this difficult times for the planet . Augment 2: I recommended this movie a lot, specially in this difficult times for the planet .  Underlined spans were false rationales given by the model through SCD. Blue spans were synonyms used as replacements, and bold font were rationales identified by human annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Why Does RDL Work?</head><p>Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns <ref type="bibr" target="#b35">(Tu et al., 2020;</ref><ref type="bibr">Wang et al., 2021)</ref> in favour of focusing on useful mappings of rationales to labels. More specifically, by using static semi-factual generation (Section 3.2) and false rationale correction (Section 3.3), we expect to break spurious associations. For example, if a model incorrectly determines that "Soylent Green" is associated with positive sentiment (Table <ref type="table" target="#tab_2">2</ref>), the augmented examples that replace "Soylent Green" with other phrases such as "Gang Orange" break the spurious association. Besides, using synonym replacement can generate examples that are similar to the original one, which is equivalent to adding noisy data to prevent models from overfitting <ref type="bibr" target="#b38">(Wei and Zou, 2019)</ref>.</p><p>Missing rationale correction (Section 3.3) emphasizes the ground truth associations between rationales and labels, enabling the model to better estimate the generally useful underlying distributions for OOD datasets, even in few-shot learning scenarios. In the next section, we present experiments and empirical evidence to demonstrate the utility of the proposed RDL framework in improving model robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Our intention is to improve the generalisability of models, and we use both in-distribution and OOD performance for evaluation. Our experiments are designed to address the following research questions:</p><p>? RQ1 Can we use static semi-factual generation to achieve better in-distribution and OOD performance?</p><p>? RQ2 Does dynamic human-intervened correction improve generalisability of models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>For fair comparison with previous work <ref type="bibr" target="#b5">(Kaushik et al., 2020;</ref><ref type="bibr" target="#b41">Yang et al., 2021)</ref>, we use the IMDb sentiment classification dataset <ref type="bibr" target="#b18">(Maas et al., 2011)</ref> as the in-distribution dataset. Following <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref>, all models were trained with the IMDb dataset predefined training, validation and test partitions containing 1, 707, 245, and 488 reviews respectively and an enforced 50:50 class ratio.</p><p>To measure the generalisation ability of different models, we focus on OOD performance. To this end, we test models on another four binary sentiment classification datasets: the sampled Amazon reviews dataset <ref type="bibr" target="#b22">(Ni et al., 2019)</ref> (100,000 positives and 100,000 negatives) from six genres: beauty, fashion, appliances, gift cards, magazines, and software; the Yelp review dataset <ref type="bibr" target="#b45">(Zhang et al., 2015)</ref> (19,000 positives and 19,000 negatives); the SST-2 dataset <ref type="bibr" target="#b29">(Socher et al., 2013)</ref> (1,067 positives and 1,143 negatives), and the SemEval-2017 Twitter dataset <ref type="bibr" target="#b26">(Rosenthal et al., 2017)</ref> (2,339 positives and 2,339 negatives). These datasets were sampled to ensure a nearly 50:50 class balance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluating Static Semi-factual Generation</head><p>To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static. We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experiment Setup</head><p>To simulate the few-shot training scenario, we randomly sample 50 examples (we also forced a 50:50 class balance) from the IMDb dataset as training data. For each experiment, the training is repeated 10 times with training datasets sampled by 10 different random seeds. We report the average result of these 10 repetitions and use accuracy to measure the classification performance.</p><p>Our experiments rely on an off-the-shelf cased "RoBERTa-base" model implemented by Hugging Face * to either perform mask-filling to provide synonyms or as a predictive model. Following <ref type="bibr" target="#b5">Kaushik et al. (2020)</ref>, we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).</p><p>We also explore the impact of the number of semi-factual examples on model performance. We use the Adam optimizer (Kingma and Ba, 2014) with a batch size of 4. We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Results and Analysis</head><p>As shown in Table <ref type="table" target="#tab_3">3</ref>, all static semi-factual generation (Static+n) methods can outperform the baseline method (Static) in both in-distribution and OOD tests, demonstrating the utility of static semifactual generation. Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy. Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively. Although the improvement on the Amazon dataset appears modest, given that there are 200,000 examples in the Amazon test set, this actually stands for nearly 1,000 documents being correctly classified.</p><p>The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset. The performance of models with the full training set is best on the in-distribution dataset but the worst on the SemEval dataset, which can be caused by the big difference between underlying distributions of these two datasets. In other words, a model that fits well with one dataset can cause performance decay on others. In this case, training with a smaller training set is more likely to reduce overfitting with the indistribution dataset and fit well with the SemEval dataset, which explains the big improvement. It is interesting to note that models trained with the en-tire training set perform slightly better on the OOD Yelp dataset (93.66?0.84) than on the in-distribution dataset (93.23?0.46), which could also be explained by the high similarity between the underlying distributions of these two datasets. As shown in Table <ref type="table" target="#tab_3">3</ref>, in most cases, DP underperforms other algorithms and is even worse than Static, demonstrating that solely increasing the dataset size cannot improve the performance. We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models. Second, synonym replacement has been used previously for data augmentation <ref type="bibr" target="#b38">(Wei and Zou, 2019)</ref>, and we compare static semi-factual generation with simply replacing any words (i.e. both rationales and non-rationales). Following <ref type="bibr" target="#b38">Wei and Zou (2019)</ref>, we replace 5% of words at random and set the training set size to 400 to ensure fair comparison (we use RoBERTa and the same hyperparameters of Static+350). We call this Random Replacement (RR hereafter).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Benefits of Static Semi-factual Generation</head><p>As shown in Table <ref type="table" target="#tab_3">3</ref>, RR is slightly better than the baseline Static approach. This result is similar to that reported in <ref type="bibr" target="#b38">Wei and Zou (2019)</ref>, since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent. However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals. These observations show that the model trained with Static+n does improve both in-distribution and OOD performance, and the improvement is actually derived from static semi-factual generation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluating Dynamic Human-intervened Correction</head><p>As shown in Table <ref type="table" target="#tab_3">3</ref> and Figure <ref type="figure" target="#fig_4">3</ref>, the performance gain of static semi-factual generation (Static+n) marginalises when augmented data is increased.</p><p>Using too much augmented data even hurts the Static+1150 performance. This observation is consistent with existing work on data augmentation <ref type="bibr" target="#b38">(Wei and Zou, 2019)</ref>. We believe one reason could be that the use of static augmented examples could also introduce new spurious patterns that degrade model performance, necessitating a method that exploits rationales without generating too many augmented examples. Human-in-the-loop can address this issue by dynamically correcting the model.</p><p>To address RQ2, we compare the performance of models trained by dynamic human-intervened correction with a popular few-shot human-in-theloop learning framework, Active Learning, as well as two other state-of-the-art CAD-based methods <ref type="bibr" target="#b5">(Kaushik et al., 2020;</ref><ref type="bibr" target="#b41">Yang et al., 2021)</ref>. Lastly, we provide an ablation study to examine the influence of different correction methods, as well as an analysis regarding model sensitivity to spurious patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Experiment Setup</head><p>We build up an active learning procedure as a baseline based on the model trained with <ref type="bibr">Static</ref> To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter). Again, experiments all rely on a RoBERTa model and all hyperparameters are identical to those described in Section 5.2.1, except for the learning rate of AL which is set to 1.25e-5 (we found this value optimised AL performance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Results and Analysis</head><p>As shown in Table <ref type="table" target="#tab_5">4</ref>, both AL and Dynamic outperform Static in in-distribution and OOD datasets which makes sense, because we use Uncertainty Sampling to add new labelled data to minimise model uncertainty and increase model performance. However, AL fails to compete with Static+350 even if more original data is added, which again demonstrates the utility of static semi-factual generation. On the contrary, Dynamic does better than Static+350 with a 0.68% in-distribution improvement in average accuracy. Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Ama- zon datasets, but no improvement for the SemEval dataset. Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets. Overall, these observations demonstrate that applying dynamic human-intervened correction (i.e. Missing Rationale Correction and False Rationale Correction) can further increase the robustness of a model on generalisation ability, effectively avoiding the improvement marginalisation caused by the increased volume of augmented data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing Rationales vs. False Rationales</head><p>We conduct an ablation study by examining the performance of Dynamic-MR and Dynamic-FR in Table <ref type="table" target="#tab_5">4</ref>. Interestingly, Dynamic-FR is specifically good at improving model performance on the in-distribution and SemEval datasets while Dynamic-MR does a good job on the SST-2 dataset. We believe that it is because Dynamic-MR biases the model to estimate an underlying distribution that is useful for SST-2 and in-distribution datasets, while Dynamic-FR biases the model to estimate a distribution similar to SemEval dataset. The performance of Dynamic can be explained as a compromise of two correction methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensitivity to Spurious Patterns</head><p>We conduct an analysis to explore whether the double-robust models are less sensitive to spurious patterns. We compute models mean sensitivity to all rationales and non-rationales through SCD in the IMDb test set. As shown in Table <ref type="table" target="#tab_6">5</ref>, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a rationale-centric human-in-the-loop framework, RDL, for better model generalisability in few-shot learning scenarios. Experimental results show that our method can boost performance of deep neural networks in both in-distribution and OOD datasets and make models less sensitive to spurious patterns, enabling fast generalisation. In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Ethical Statement</head><p>We honor the ACL Code of Ethics. No private data or non-public information was used in this work. All annotators have received labor fees corresponding to the amount of their annotated instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A negative movie review with human annotated causal terms (bold text) and spurious patterns recognised by the model (underlined text).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The procedure of the Rationale-centric Double-robustness Learning framework. Red text highlights rationales identified by human annotators. Blue text indicates words replaced in raw text. Underlined text shows spurious patterns identified by the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>First</head><label></label><figDesc>, we test whether the improvement in model performance is brought about by static semi-factual generation (Static+n) or simply by an increase in the size of the training set. We compare Static+350 (due to its relatively good performance) with another baseline called Duplication (DP heareafter). We multiply the original training set (50 examples) up into 400 examples identical to the size of the training set of Static+350, and fine-tune RoBERTa on this dataset with the same hyperparameters as Static+350.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average performance gain of different static semi-factual generation methods with different augmentation size over four OOD datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Fragments of augmented data generated by static semi-factual generation (Original/Augmented, in order). Blue spans were synonyms used as replacements and bold font were rationales identified by human annotators. Micawber was nothing more than a mid-nineteenth century Kramer.SCD: but this is pathetic! Micawber was nothing more than a mid-nineteenth century Kramer. Augment 1: but this is pathetic! Perkins became nothing more than a mid-nineteenth century Kramer. Augment 2: but this is pathetic! It had nothing more than a mid-nineteenth century Kramer.PositiveOrigin: Soylent Green is a wild movie that I enjoyed very much . SCD: Soylent Green is a wild movie that I enjoyed very much . Augment 1: Gang Orange is a wild movie that I enjoyed very much . Augment 2: Village Spring is a wild movie that I enjoyed very much .</figDesc><table><row><cell cols="2">Sentiment Examples</cell></row><row><cell>Negative</cell><cell>Origin: but this is pathetic!</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Fragments of augmented data generated by false rationale correction (Original/SCD/Augmented, in order).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results on in-distribution and OOD data. Values in brackets are the training set size. Static: uses 50 gold examples. Full: uses the full training set. Static + n: our static semi-factual generation method where n is the number of semi-factuals. RR: Random Replacement (Wei and Zou, 2019). DP: Duplication.</figDesc><table><row><cell>Training Data</cell><cell>In-domain</cell><cell cols="2">SemEval-2017 SST-2</cell><cell>Yelp</cell><cell>Amazon</cell></row><row><cell>Static (50 gold)</cell><cell>88.60?1.11</cell><cell>77.28?9.11</cell><cell>79.29?5.14</cell><cell>91.53?2.06</cell><cell>89.63?1.65</cell></row><row><cell>Full (1,707 gold)</cell><cell>93.23?0.46</cell><cell>71.17?2.54</cell><cell>80.23?2.09</cell><cell>93.66?0.84</cell><cell>90.29?0.57</cell></row><row><cell>DP (Static + 350 auto) (400)</cell><cell>86.70?2.92</cell><cell>74.36?2.92</cell><cell>77.33?6.01</cell><cell>89.60?2.51</cell><cell>89.15?1.89</cell></row><row><cell>RR (Static + 350 auto) (400)</cell><cell>89.65?1.27</cell><cell>79.20?1.27</cell><cell>78.89?5.95</cell><cell>91.93?2.10</cell><cell>89.73?1.26</cell></row><row><cell>Our Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Static + 150 auto (200)</cell><cell>90.08?1.25</cell><cell>78.88?6.67</cell><cell>79.40?3.28</cell><cell>92.19?1.51</cell><cell>89.81?1.73</cell></row><row><cell>Static + 350 auto (400)</cell><cell>90.16?0.85</cell><cell>80.54?2.81</cell><cell>81.26?1.97</cell><cell>93.03?1.08</cell><cell>90.09?1.79</cell></row><row><cell>Static + 550 auto (600)</cell><cell>90.04?1.50</cell><cell>80.69?3.42</cell><cell>81.23?1.83</cell><cell>92.10?3.07</cell><cell>89.67?1.27</cell></row><row><cell>Static + 750 auto (800)</cell><cell>90.08?1.01</cell><cell>80.55?3.96</cell><cell>80.75?2.30</cell><cell>92.36?1.87</cell><cell>90.18?1.44</cell></row><row><cell>Static + 950 auto (1000)</cell><cell>89.83?1.28</cell><cell>80.90?3.29</cell><cell>80.58?2.57</cell><cell>92.30?2.19</cell><cell>90.62?1.29</cell></row><row><cell>Static + 1150 auto (1200)</cell><cell>90.12?1.82</cell><cell>79.31?1.82</cell><cell>79.52?3.15</cell><cell>91.47?3.61</cell><cell>90.16?1.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>https://huggingface.co/transformers/model_doc/roberta.html this Static+n, where n is the number of generated semi-factuals).</figDesc><table><row><cell>To</cell></row><row><cell>this end, we conduct static semi-factual generation</cell></row><row><cell>with a different number of augmented examples for</cell></row><row><cell>each instance: {3, 7, 11, 15, 19, 23}. Considering</cell></row><row><cell>we have 50 original examples, this would result</cell></row><row><cell>in {150, 350, 550, 750, 950, 1,150} additional</cell></row><row><cell>examples in the training set, respectively (we call</cell></row></table><note><p>*</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results on in-distribution and OOD data. Values in brackets are the training set size. AL: Active Learning. Manual CAD<ref type="bibr" target="#b5">(Kaushik et al., 2020)</ref>, Automatic CAD<ref type="bibr" target="#b41">(Yang et al., 2021)</ref>. Our methods are Dynamic-MR:</figDesc><table><row><cell>. In</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Static versus Dynamic models on average sensitivity (normalised) to rationales and non-rationales for IMDb test samples.</figDesc><table><row><cell></cell><cell cols="2">Non-rationales Rationales</cell></row><row><cell>Static</cell><cell>0.572</cell><cell>0.428</cell></row><row><cell cols="2">Dynamic 0.433</cell><cell>0.567</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We acknowledge with thanks the discussion with <rs type="person">Chenyang Lyu</rs> from <rs type="affiliation">Dublin City University</rs>, as well as the many others who have helped. We would also like to thank anonymous reviewers for their insightful comments and suggestions to help improve the paper. This publication has emanated from research conducted with the financial support of the <rs type="funder">Pioneer and "Leading Goose" R&amp;D Program of Zhejiang</rs> under Grant Number <rs type="grantNumber">2022SDXHDX0003</rs> and <rs type="funder">Science Foundation Ireland (SFI)</rs> under Grant Number <rs type="grantNumber">[12/RC/2289_P2</rs>]. <rs type="person">Yue Zhang</rs> is the corresponding author.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YabP869">
					<idno type="grant-number">2022SDXHDX0003</idno>
				</org>
				<org type="funding" xml:id="_Ew8QbdX">
					<idno type="grant-number">[12/RC/2289_P2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Uncertainty estimation and out-of-distribution detection for counterfactual explanations: Pitfalls and solutions</title>
		<author>
			<persName><forename type="first">Eoin</forename><surname>Delaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">T</forename><surname>Keane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09734</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Empowering language understanding with counterfactual reasoning</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.196</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2226" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Explainable active learning (xal): Toward ai explanations as interfaces for machine teachers</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Bhavya Ghai</surname></persName>
		</author>
		<author>
			<persName><surname>Vera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Bellamy</surname></persName>
		</author>
		<author>
			<persName><surname>Mueller</surname></persName>
		</author>
		<idno type="DOI">10.1145/3432934</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Hum.-Comput</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Interact., 4(CSCW3</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1215</idno>
		<idno type="arXiv">arXiv:1803.02324</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-09">2018. 2017. 2017. September 9-11, 2017</date>
			<biblScope unit="page" from="2021" to="2031" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Annotation artifacts in natural language inference data. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models</title>
		<author>
			<persName><forename type="first">Xisen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Xiangyang Xue, and Xiang Ren</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the difference that makes a difference with counterfactually augmented data</title>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explaining the efficacy of counterfactually augmented data</title>
		<author>
			<persName><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrith</forename><surname>Setlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text and causal inference: A review of using text to remove confounding from causal estimates</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan O'</forename><surname>Connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5332" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On generating plausible counterfactual and semi-factual explanations for deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Eoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">T</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><surname>Keane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Principles of explanatory debugging to personalize interactive machine learning</title>
		<author>
			<persName><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<idno type="DOI">10.1145/2678025.2701399</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Intelligent User Interfaces, IUI &apos;15</title>
		<meeting>the 20th International Conference on Intelligent User Interfaces, IUI &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Explanatory debugging: Supporting end-user debugging of machine-learned programs</title>
		<author>
			<persName><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Riche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Travis</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Oberst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amber</forename><surname>Shinsel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Mcintosh</surname></persName>
		</author>
		<idno type="DOI">10.1109/VLHCC.2010.15</idno>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Symposium on Visual Languages and Human-Centric Computing</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Find: Human-in-the-loop debugging deep text classifiers</title>
		<author>
			<persName><forename type="first">Piyawat</forename><surname>Lertvittayakumjorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Toni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Explanation-based human debugging of nlp models: A survey</title>
		<author>
			<persName><forename type="first">Piyawat</forename><surname>Lertvittayakumjorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Toni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.15135</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maec: A multimodal aligned earnings conference call dataset for financial risk prediction</title>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihai</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3063" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized bert pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>ArXiv, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A sentence-level hierarchical bert model for document classification with limited labelled data</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maeve</forename><surname>Henchion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Bacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">Mac</forename><surname>Namee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discovery Science</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="231" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Investigating the effectiveness of representations based on pretrained transformer-based language models in active learning for labelling text datasets</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Macnamee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13138</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Active learning by acquiring contrastive examples</title>
		<author>
			<persName><forename type="first">Katerina</forename><surname>Margatina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgos</forename><surname>Vernikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceddings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>eddings of the 2021 Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<publisher>Underline Science Inc</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Lo?c Barrault, and Nikolaos Aletras</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semifactual &quot;even if</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Mccloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth Mj</forename><surname>Byrne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">thinking. Thinking &amp; Reasoning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="67" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1145/219717.219748</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Justifying recommendations using distantly-labeled reviews and fine-grained aspects</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Counterfactual inference for text classification debiasing</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5434" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anchors: High-precision modelagnostic explanations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Counterfactual thinking</title>
		<author>
			<persName><forename type="first">Roese</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">133</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 4: Sentiment analysis in Twitter</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation</title>
		<meeting>the 11th International Workshop on Semantic Evaluation<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="502" to="518" />
		</imprint>
	</monogr>
	<note>SemEval-2017</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Active learning literature survey</title>
		<author>
			<persName><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Right for better reasons: Training differentiable models by constraining their influence functions</title>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arseny</forename><surname>Skryagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Stammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9533" to="9540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robustness to spurious correlations via human annotations</title>
		<author>
			<persName><forename type="first">Megha</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9109" to="9119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Right for the right concept: Revising neuro-symbolic concepts by interacting with their explanations</title>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Stammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Schramowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19">2021. June 19-25, 2021</date>
			<biblScope unit="page" from="3619" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interacting meaningfully with machine learning systems: Three experiments</title>
		<author>
			<persName><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidya</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lida</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herlocker</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ijhcs.2009.03.004</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="639" to="662" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Interactive label cleaning with example-based explanations</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Teso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bontempelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fausto</forename><surname>Giunchiglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Passerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-fifth Conference on Neural Information Processing Systems</title>
		<meeting>the Thirty-fifth Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Explanatory interactive machine learning</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Teso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3314293</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="239" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An empirical study on robustness to spurious correlations using pre-trained language models</title>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garima</forename><surname>Lalwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="621" to="633" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Identifying and mitigating spurious correlations for improving robustness in nlp models</title>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07736</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robustness to spurious correlations in text classification via automatically generated counterfactuals</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">EDA: Easy data augmentation techniques for boosting performance on text classification tasks</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6382" to="6388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Xingjiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luwei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.00941</idno>
		<title level="m">A survey of human-in-the-loop for machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generating plausible counterfactual explanations for deep transformers in financial text classification</title>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eoin</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tin</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihai</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6150" to="6160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Exploring the efficacy of automatically generated counterfactuals for sentiment analysis</title>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padraig</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihai</forename><surname>Dong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.26</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Html: Hierarchical transformerbased multi-task learning for volatility prediction</title>
		<author>
			<persName><forename type="first">Linyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tin</forename><surname>Lok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riuhai</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="441" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Xisen Jin, and Xiang Ren. 2021. Refining neural networks with compositional explanations</title>
		<author>
			<persName><forename type="first">Huihan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10415</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Using &quot;annotator rationales&quot; to improve machine learning for text categorization</title>
		<author>
			<persName><forename type="first">Omar</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Piatko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="260" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
