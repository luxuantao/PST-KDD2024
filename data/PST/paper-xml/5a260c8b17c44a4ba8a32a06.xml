<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiong</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
							<email>lishen@robots.ox.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Omkar</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Engineering Science</orgName>
								<orgName type="laboratory">Visual Geometry Group</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VGGFace2: A dataset for recognising faces across pose and age</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5DF670A596E64460F7A9C748B57768F5</idno>
					<idno type="DOI">10.1109/FG.2018.00020</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>face dataset</term>
					<term>face recognition</term>
					<term>convolutional neural networks 67</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a new large-scale face dataset named VGGFace2. The dataset contains 3.31 million images of 9131 subjects, with an average of 362.6 images for each subject. Images are downloaded from Google Image Search and have large variations in pose, age, illumination, ethnicity and profession (e.g. actors, athletes, politicians).</p><p>The dataset was collected with three goals in mind: (i) to have both a large number of identities and also a large number of images for each identity; (ii) to cover a large range of pose, age and ethnicity; and (iii) to minimise the label noise. We describe how the dataset was collected, in particular the automated and manual filtering stages to ensure a high accuracy for the images of each identity.</p><p>To assess face recognition performance using the new dataset, we train ResNet-50 (with and without Squeeze-and-Excitation blocks) Convolutional Neural Networks on VG-GFace2, on MS-Celeb-1M, and on their union, and show that training on VGGFace2 leads to improved recognition performance over pose and age. Finally, using the models trained on these datasets, we demonstrate state-of-the-art performance on the IJB-A and IJB-B face recognition benchmarks, exceeding the previous state-of-the-art by a large margin. The dataset and models are publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Concurrent with the rapid development of deep Convolutional Neural Networks (CNNs), there has been much recent effort in collecting large scale datasets to feed these datahungry models. In general, recent datasets (see Table <ref type="table" target="#tab_0">I</ref>) have explored the importance of intra-and inter-class variation. The former focuses on depth (many images of one subject) and the latter on breadth (many subjects with limited images per subject). However, none of these datasets was specifically designed to explore pose and age variation. We address that here by designing a dataset generation pipeline to explicitly collect images with a wide range of pose, age, illumination and ethnicity variations of human faces.</p><p>We make the following four contributions: first, we have collected a new large scale dataset, VGGFace2, for public release. It includes over nine thousand identities with between 80 and 800 images for each identity, and more than 3M images in total; second, a dataset generation pipeline is proposed that encourages pose and age diversity for each subject, and also involves multiple stages of automatic and manual filtering in order to minimise label noise; third, we provide template annotation for the test set to explicitly explore pose and age recognition performance; and, finally, we show that training deep CNNs on the new dataset substantially exceeds the state-of-the-art performance on the IJB benchmark datasets <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b21">[22]</ref>. In particular, we experiment with the recent Squeeze and Excitation network <ref type="bibr" target="#b8">[9]</ref>, and also investigate the benefits of first pre-training on a dataset with breadth (MS-Celeb-1M <ref type="bibr" target="#b6">[7]</ref>) and then fine tuning on VGGFace2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATASET REVIEW</head><p>In this section we briefly review the principal "in the wild" datasets that have appeared recently, inspired by the original Labelled Faces in the Wild (LFW) dataset <ref type="bibr" target="#b9">[10]</ref> of 2007. This dataset had 5, 749 identities with 13, 000 images.</p><p>The CelebFaces+ dataset <ref type="bibr" target="#b19">[20]</ref> was released in 2014, with 202, 599 images of 10, 177 celebrities. The CASIA-WebFace dataset <ref type="bibr" target="#b24">[25]</ref> released the same year has 494, 414 images of 10, 575 people. The VGGFace dataset <ref type="bibr" target="#b15">[16]</ref> released in 2015 has 2.6 million images covering 2, 622 people, making it amongst the largest publicly available datasets. The curated version, where label noise is removed by human annotators, has 800, 000 images with approximately 305 images per identity. Both the CASIA-WebFace and VGGFace datasets were released for training purposes only.</p><p>MegaFace dataset <ref type="bibr" target="#b11">[12]</ref> was released in 2016 to evaluate face recognition methods with up to a million distractors in the gallery image set. It contains 4.7 million images of 672, 057 identities as the training set. However, an average of only 7 images per identity makes it restricted in its per identity face variation. In order to study the effect of pose and age variations in recognising faces, the MegaFace challenge <ref type="bibr" target="#b11">[12]</ref> uses the subsets of FaceScrub <ref type="bibr" target="#b13">[14]</ref> containing 4, 000 images from 80 identities and FG-NET <ref type="bibr" target="#b14">[15]</ref> containing 975 images from 82 identities for evaluation.</p><p>Microsoft released the large Ms-Celeb-1M dataset <ref type="bibr" target="#b6">[7]</ref> in 2016 with 10 million images from 100k celebrities for training and testing. This is a very useful dataset, and we employ it for pre-training in this paper. However, it has two limitations: (i) while it has the largest number of training images, the intra-identity variation is somewhat restricted because of an average of 81 images per person; (ii) images in the training set were directly retrieved from a search engine without manual filtering, and consequently there is label noise. The IARPA Janus Benchmark-A (IJB-A) <ref type="bibr" target="#b12">[13]</ref> and Benchmark-B (IJB-B) <ref type="bibr" target="#b21">[22]</ref> datasets were released as evaluation benchmarks (only test) for face detection, recognition and clustering in images and videos. Unlike the above datasets which are geared towards image-based face recognition, the Youtube Face (YTF) <ref type="bibr" target="#b22">[23]</ref> and UMDFaces-Videos <ref type="bibr" target="#b3">[4]</ref> datasets aim to recognise faces in unconstrained videos. YTF contains 1, 595 identities and 3, 425 videos, whilst UMDFaces-Videos is larger with 3, 107 identities and 22, 075 videos (the identities are a subset of those in UMDFaces <ref type="bibr" target="#b4">[5]</ref>).</p><p>Apart form these public datasets, Facebook and Google have large in-house datasets. For instance, Facebook <ref type="bibr" target="#b20">[21]</ref> trained a face identification model using 500 million images of over 10 million subjects. The face recognition model by Google <ref type="bibr" target="#b17">[18]</ref> was trained using 200 million images of 8 million identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. AN OVERVIEW OF THE VGGFACE2 A. Dataset Statistics</head><p>The VGGFace2 dataset contains 3.31 million images from 9131 celebrities spanning a wide range of ethnicities, e.g. it includes more Chinese and Indian faces than VGGFace (though, the ethnic balance is still limited by the distribution of celebrities and public figures), and professions (e.g. politicians and athletes). The Images were downloaded from Google Image Search and show large variations in pose, age, lighting and background. The dataset is approximately gender-balanced, with 59.3% males, varying between 80 and 843 images for each identity, with 362.6 images on average. It includes human verified bounding boxes around faces, and five fiducial keypoints predicted by the model of <ref type="bibr" target="#b25">[26]</ref>. In addition, pose (yaw, pitch and roll) and apparent age information are estimated by our pre-trained pose and age classifiers.</p><p>The dataset is divided into two splits: one for training having 8631 classes, and one for evaluation (test) with 500 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose and Age Annotations</head><p>The VGGFace2 provides annotation to enable evaluation on two scenarios: face matching across different poses, and face matching across different ages. Pose templates. A template here consists of five faces from the same subject with a consistent pose. This pose can be frontal, three-quarter or profile view. For a subset of 300 subjects of the evaluation set, two templates (5 images per template) are provided for each pose view. Consequently there are 1.8K templates with 9K images in total. An examples is shown in Figure <ref type="figure" target="#fig_0">1</ref> (left). Age templates. A template here consists of five faces from the same subject with either an apparent age below 34 (deemed young), or 34 or above (deemed mature). These are provided for a subset of 100 subjects from the evaluation set with two templates for each age period, therefore, there are 400 templates with a total of 2K images. Examples are show in Figure <ref type="figure" target="#fig_0">1</ref> (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET COLLECTION</head><p>In this section, we describe the dataset collection process, including: how a list of candidate identities was obtained; how candidate images were collected; and, how the dataset was cleaned up both automatically and manually. The process is summarised in Table <ref type="table" target="#tab_0">II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Stage 1: Obtaining and selecting a name list</head><p>We use a similar strategy to that proposed by <ref type="bibr" target="#b15">[16]</ref>. The first stage is to find as many subjects as possible that have a sufficiently distinct set of images available, for example, celebrities and public figures (e.g. actors, politicians and athletes). An initial list of 500k public figures is obtained from the Freebase knowledge graph <ref type="bibr" target="#b1">[2]</ref>.</p><p>An annotator team is then used to remove identities from the candidate list that do not have sufficient distinct images. To this end, for each of the 500K names, 100 images are downloaded using Google Image Search and human annotators are instructed to retain subjects for which approximately 90% or more of the 100 images belong to a single identity. This removes candidates who do not have sufficient images or for which Google Image Search returns a mix of people for a single name. In this manner, we reduce the candidates to only 9244 names. Attribute information such as ethnicity and kinship is obtained from DBPedia <ref type="bibr" target="#b0">[1]</ref>. Table <ref type="table" target="#tab_0">II</ref>: Dataset statistics after each stage of processing in the collection pipeline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stage 2: Obtaining images for each identity</head><p>We query in Google Image Search and download 1000 images for each subject. To obtain images with large pose and age variations, we then append the keyword 'sideview' and 'very young' to each name and download 200 images for each. This results in 1400 images for each identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stage 3: Face detection</head><p>Faces are detected using the model provided by <ref type="bibr" target="#b25">[26]</ref>. We use the hyper-parameters recommended in that work to favor a good trade-off between precision and recall. The face bounding box is then extended by a factor of 0.3 to include the whole head. Moreover, five facial landmarks are predicted by the same model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stage 4: Automatic filtering by classification</head><p>The aim of this stage is to remove outlier faces for each identity automatically. This is achieved by learning a classifier to identify the faces, and removing possible erroneous faces below a classification score. To this end, 1-vs-rest classifiers are trained to discriminate between the 9244 subjects. Specifically, faces from the top 100 retrieved images of each identity are used as positives, and the top 100 of all other identities are used as negative for training. The face descriptor features are obtained from the VGGFace <ref type="bibr" target="#b15">[16]</ref> model. Then, the scores (between 0 and 1) from the trained model is used to sort images for each subject from most likely to least likely. By manually checking through images from a random 500 subjects, we choose a threshold of 0.5 and remove any faces below this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Stage 5: Near duplicate removal</head><p>The downloaded images also contain exact or near duplicates due to the same images being found at different internet locations, or images differing only slightly in colour balance or JPEG artifacts for example. To alleviate this, duplicate images are removed by clustering VLAD descriptors for all images remaining at stage 4 and only retaining one image per cluster <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Stage 6: Final automatic and manual filtering</head><p>At this point, two types of error may still remain: first, some classes still have outliers (i.e. images that do not belong to the person); and second, some classes contain a mixture of faces of more than one person, or they overlap with another class in the dataset. This stage addresses these two types of errors with a mix of manual and automated algorithms.</p><p>Detecting overlapped subjects. Subjects may overlap with other subjects. For instance, 'Will I Am' and 'William James Adams' in the candidate list refer to the same person. To detect confusions for each class, we randomly split the data for each class in half: half for training and the other for testing. Then, we train a ResNet-50 <ref type="bibr" target="#b7">[8]</ref> and generate a confusion matrix by calculating top-1 error on the test samples. In this manner, we find 20 subjects confused with others. In this stage, we removed 19 noisy classes. In addition, we remove 94 subjects with samples less than 80 images, which results in a final list of 9131 identities.</p><p>Removing outlier images for a subject. The aim of this filtering, which is partly manual, is to achieve a purity greater than 96%. We found that for some subjects, images with very high classifier scores at stage 4 can also be noisy. This happens when the downloaded images contain couples or band members who always appear together in public. In this case, the classifiers trained with these mixed examples at stage 4 tend to fail.</p><p>We retrain the model based on the current dataset, and for each identity the classifier score is used to divide the images into 3 sets: H (i.e. high score range [1, 0.95]), I (i.e. intermediate score range (0.95, 0.8]) and L (i.e. low score range (0.8, 0.5]). Human annotators clean up the images for each subject based on their scores, and the actions they carry out depends on whether the set H is noisy or not. If the set (H) contains several different people (noise) in a single identity folder, then set I and L (which have lower confidence scores), will undoubtedly be noisy as well, so all three sets are cleaned manually. In contrast, if set H is clean, then only set L (the lowest scores which is supposed to be the most noisy set) is cleaned up. After this, a new model is trained on the cleaned set H and L, and set I (intermediate scores, noise level is also intermediate) is then cleaned by model prediction. This procedure achieves very low label noise without requiring manual checking of every image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Pose and age annotations</head><p>We train two networks to obtain the pose and age information for the dataset. To obtain head pose (roll, pitch, yaw), a 5-way classification ResNet-50 <ref type="bibr" target="#b7">[8]</ref> is trained on the CASIA-WebFace dataset <ref type="bibr" target="#b24">[25]</ref>. Then, this trained model is used to predict pose for all the images in the dataset.</p><p>Similarly, to estimate the apparent age, a 8-way classification ResNet-50 <ref type="bibr" target="#b7">[8]</ref> is trained on IMDB-WIKI -500k+ dataset <ref type="bibr" target="#b16">[17]</ref>. Ages of faces are then predicted by this model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In this section, we evaluate the quality of the VGGFace2 dataset by conducting a number of baseline experiments. We report the results on the VGGFace2 test set, and evaluate on the public benchmarks IJB-A <ref type="bibr" target="#b12">[13]</ref> and IJB-B datasets <ref type="bibr" target="#b21">[22]</ref>. The subjects in our training dataset are disjoint with the ones in IJB-A and IJB-B dataset. We also remove the overlap between MS-Celeb-1M and the two benchmarks when training the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>Architecture. ResNet-50 <ref type="bibr" target="#b7">[8]</ref> and SE-ResNet-50 <ref type="bibr" target="#b8">[9]</ref> (SENet for short) are used as the backbone architectures for the comparison amongst training datasets. The Squeeze-and-Excitation (SE) blocks <ref type="bibr" target="#b8">[9]</ref> adaptively recalibrate channelwise feature responses by explicitly modelling channel relationships. They can be integrated with modern architectures, such as ResNet, and improve its representational power. This has been demonstrated for object and scene classification, with a Squeeze-and-Excitation network winning the ILSVRC 2017 classification competition.</p><p>The following experiments are developed under four settings: (a) networks are learned from scratch on VGGFace <ref type="bibr" target="#b15">[16]</ref> (VF for short); (b) networks are learned from scratch on MS-Celeb-1M (MS1M for short) <ref type="bibr" target="#b6">[7]</ref>; (c) networks are learned from scratch on VGGFace2 (VF2 for short); and, (d) networks are first pre-trained on MS1M, and then fine-tuned on VGGFace2 (VF2 ft for short).</p><p>Similarity computation. In all the experiments (i.e. for both verification and identification), we need to compute the similarity between subject templates. A template is represented by a single vector computed by aggregating the face descriptors of each face in the template set. In section V-B, the template vector is obtained by averaging the face descriptors of the images and SVM classifiers are used for identification. In sections V-C and V-D for IJB-A and IJB-B, where the template may contain both still images and video frames, we first compute the media vector (i.e. from images or video frames) by averaging the face descriptors in that media. A template vector is then generated by averaging the media vectors in that template, which is then L2 normalised. Cosine similarity is used to represent the similarity between two templates.</p><p>A face descriptor is obtained from the trained networks as follows: first the extended bounding box of the face is resized so that the shorter side is 256 pixels; then the centre 224 × 224 crop of the face image is used as input to the network. The face descriptor is extracted from from the layer adjacent to the classifier layer. This leads to a 2048 dimensional descriptor, which is then L2 normalised.</p><p>Training implementation details. All the networks are trained for classification using the soft-max loss function.</p><p>During training, the extended bounding box of the face is resized so that the shorter side is 256 pixels, then a 224×224 pixels region is randomly cropped from each sample. The mean value of each channel is subtracted for each pixel.</p><p>Monochrome augmentation is used with a probability of 20% to reduce the over-fitting on colour images. Stochastic gradient descent is used with mini-batches of size 256, with a balancing-sampling strategy for each mini-batch due to the unbalanced training distributions. The initial learning rate is 0.1 for the models trained from scratch, and this is decreased twice with a factor of 10 when errors plateau. The weights of the models are initialised as described in <ref type="bibr" target="#b7">[8]</ref>. The learning rate for model fine-tuning starts from 0.005 and decreases to 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiments on the new dataset</head><p>In this section, we evaluate ResNet-50 trained from scratch on the three datasets as described in the Sec. V-A, and VGGFace2 test set. We test identification performance and also similarity over pose and age, and validate the capability of VGGFace2 to tackle pose and age variations.</p><p>Face identification. This scenario aims to predict, for a given test image, whose face it is. Specifically, for each of the 500 subjects in the evaluation set, 50 images are randomly chosen as the testing split and the remaining images are used as the training split. This training split is used to learn 1-vs-rest SVM classifiers for each subject. A top-1 classification error is then used to evaluate the performance of these classifiers on the test images. As shown in Table <ref type="table" target="#tab_2">III</ref>, there is a significant improvement for the model trained on VGGFace2 rather than on VGGFace. This demonstrates the benefit of increasing data variation (e.g, subject number, pose and age variations) in the VGGFace2 training dataset. More importantly, models trained on VGGFace2 also achieve better result than that on MS1M even though it has tenfold more subjects and threefold more images, demonstrating the good quality of VGGFace2. In particular, the very low top-1 error of VGGFace2 provides evidence that there is very little label noise in the dataset -which is one of our design goals. Probing across pose. This test aims to assess how well templates match across three pose views: front, three-quarter and profile views. As described in section III-B, 300 subjects in the evaluation set are annotated with pose templates, and there are six templates for each subject: two each for front, three-quarter view and profile views. These six templates are divided into two sets, one pose for each set, and a 3 × 3 similarity matrix is constructed between the two sets. Figure <ref type="figure" target="#fig_2">3</ref> visualises two example of these cosine similarity scores for front-to-profile templates.</p><p>Table <ref type="table" target="#tab_4">IV</ref> compares the similarity matrix averaged over the 300 subjects. We can observe that (i) all the three models perform better when matching similar poses, i.e., front-to-front, three-quarter-to-three-quarter and profile-to-profile; and (ii) the performance drops when probing for different poses, e.g., front-to-three-quarter and front-to-profile, showing that recognition across poses is a much harder problem. Figure <ref type="figure" target="#fig_1">2</ref> shows histograms of similarity scores. It is evident that the mass of the VGGFace2 trained model is to the right of the MS1M and VGGFace trained models. This clearly demonstrates the benefit of training on a dataset with larger pose variation.</p><p>Probing across age. This test aims to assess how well templates match across age, for two ages ranges: young and mature ages. As described in section III-B, 100 subjects in the evaluation set are annotated with age templates, and there are four templates for each subject: two each for young and mature faces.</p><p>For each subject a 2 × 2 similarity matrix is computed, where an element is the cosine similarity between two templates. Figure <ref type="figure" target="#fig_4">5</ref> shows two examples of the young-tomature templates, and their similarity scores.</p><p>Table <ref type="table">V</ref> compares the similarity matrix averaged over the 100 subjects as the model changes. For all the three models, there is always a big drop in performance when matching across young and mature faces, which reveals that young-to-mature matching is substantially more challenging than young-to-young and mature-to-mature. Moreover, young-to-young matching is more difficult than mature-tomature matching. Figure <ref type="figure" target="#fig_3">4</ref> illustrates the histograms of the young-to-mature template similarity scores.</p><p>Discussion. In the evaluation of pose and age protocols, models trained on VGGFace2 always achieve the highest similarity scores, and MS1M dataset the lowest. This can be explained by the fact that the MS1M dataset is designed to focus more on inter-class diversities, and this harms the matching performance across different pose and age, illustrating the value of VGGFace2 in having more intraclass diversities that cover large variations in pose and age.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experiments on IJB-A</head><p>In this section, we compare the performance of the models trained on the different datasets on the public IARPA Janus Benchmark A (IJB-A dataset) <ref type="bibr" target="#b12">[13]</ref>.</p><p>The IJB-A dataset contains 5712 images and 2085 videos from 500 subjects, with an average of 11.4 images and 4.2 videos per subject. All images and videos are captured from unconstrained environment and show large variations in expression and image qualities. As a pre-processing, we detect the faces using MTCNN <ref type="bibr" target="#b25">[26]</ref> to keep the cropping consistent between training and evaluation.</p><p>IJB-A provides ten-split evaluations with two standard protocols, namely, 1:1 face verification and 1:N face identification, where we directly extract the features from the models for the test sets and use cosine similarity score. For verification, the performance is reported using the true accept rates (TAR) vs. false positive rates (FAR) (i.e. receiver operating characteristics (ROC) curve). For identification, the performance is reported using the true positive identification rate (TPIR) vs. false positive identification rate (FPIR) (equivalent to a decision error trade-off (DET) curve) and the Rank-N (i.e. the cumulative match characteristic (CMC) curve).       ResNet-50 (Table <ref type="table" target="#tab_0">VI</ref>), and start with networks trained from scratch. we can observe that the model trained on VGGFace2 outperforms the one trained on VGGFace by a large margin, even though VGGFace has a similar scale (2.6M images) it has fewer identities and pose/age variations (and more label noise). Moreover, the model of VGGFace2 is significantly superior to the one of MS1M which has 10 times subjects over our dataset. Specially, it achieve ∼ 4.4% improvement over MS1M on FAR=0.001 for verification, ∼ 3.7% on FPIR=0.01 and ∼ 1.5% on Rank-1 for identification.</p><p>When comparing with the results of existing works, the model trained on VGGFace2 surpasses previously reported results on all metrics (best to our knowledge, reported on IJB-A 1:1 verification and 1:N identification protocols), Table VII: Performance evaluation on the IJB-B dataset. A higher value is better. The results of <ref type="bibr" target="#b21">[22]</ref> are read from the curves reported in the paper. Note, <ref type="bibr" target="#b21">[22]</ref> has a different evaluation for the verification protocol where pairs generated from different galleries are evaluated separately and averaged to get the final results.</p><p>Figure <ref type="figure">7</ref>: Results on the IJB-B dataset across gallery sets S1 and S2.</p><p>Left: ROC (higher is better); Middle: DET (lower is better); Right: CMC (higher is better).</p><p>expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright annotation thereon.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: VGGFace2 template examples. Left: pose templates from three different viewpoints (arranged by row) -frontal, three-quarter, profile. Right: age templates for two subjects for young and mature ages (arranged by row).</figDesc><graphic coords="3,154.70,167.20,116.53,116.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histograms of similarity scores for front-to-profile matching for the models trained on different datasets.</figDesc><graphic coords="6,153.42,241.91,113.68,53.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two example templates of front-to-profile matching. Left: the similarity scores produced by VGGFace, MS1M, VGGFace2 are 0.41, 0.35 and 0.59, respectively; Right: the scores are 0.41, 0.31 and 0.57, respectively.</figDesc><graphic coords="6,327.04,242.25,113.68,53.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histograms of similarity scores for young-to-mature matching for the models trained on different datasets.</figDesc><graphic coords="6,327.04,499.78,113.68,54.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Two example templates of young-to-mature matching. Left: the similarity scores produced by VGGFace, MS1M, VGGFace2 are 0.42, 0.30 and 0.58, respectively; Right: the scores are 0.43, 0.41 and 0.73, respectively.</figDesc><graphic coords="6,153.42,500.80,113.68,53.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Statistics for recent public face datasets. The three entries in the 'per subject' column are the minimum/average/maximum per subject.</figDesc><table><row><cell>Datasets</cell><cell># of subjects</cell><cell># of images</cell><cell cols="3"># of images per subject manual identity labelling pose</cell><cell>age</cell><cell>year</cell></row><row><cell>LFW [10]</cell><cell>5, 749</cell><cell>13, 233</cell><cell>1/2.3/530</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2007</cell></row><row><cell>YTF [23]</cell><cell>1, 595</cell><cell>3, 425 videos</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2011</cell></row><row><cell>CelebFaces+ [20]</cell><cell>10, 177</cell><cell>202, 599</cell><cell>19.9</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2014</cell></row><row><cell>CASIA-WebFace [25]</cell><cell>10, 575</cell><cell>494, 414</cell><cell>2/46.8/804</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2014</cell></row><row><cell>IJB-A [13]</cell><cell>500</cell><cell>5, 712 images, 2, 085 videos</cell><cell>11.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2015</cell></row><row><cell>IJB-B [22]</cell><cell>1, 845</cell><cell>11, 754 images, 7, 011 videos</cell><cell>36.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2017</cell></row><row><cell>VGGFace [16]</cell><cell>2, 622</cell><cell>2.6 M</cell><cell>1, 000/1, 000/1, 000</cell><cell>-</cell><cell>-</cell><cell cols="2">Yes 2015</cell></row><row><cell>MegaFace [12]</cell><cell>690, 572</cell><cell>4.7 M</cell><cell>3/7/2469</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2016</cell></row><row><cell>MS-Celeb-1M [7]</cell><cell>100, 000</cell><cell>10 M</cell><cell>100</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2016</cell></row><row><cell>UMDFaces [5]</cell><cell>8, 501</cell><cell>367, 920</cell><cell>43.3</cell><cell>Yes</cell><cell>Yes</cell><cell cols="2">Yes 2016</cell></row><row><cell>UMDFaces-Videos [4]</cell><cell>3, 107</cell><cell>22, 075 videos</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>2017</cell></row><row><cell>VGGFace2 (this paper)</cell><cell>9, 131</cell><cell>3.31 M</cell><cell>80/362.6/843</cell><cell>Yes</cell><cell>Yes</cell><cell cols="2">Yes 2018</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table III :</head><label>III</label><figDesc>Identification performance (top-1 classification error) on the VGGFace2 test set for ResNet models trained on different datasets. A lower value is better.</figDesc><table><row><cell cols="2">Training dataset VGGFace</cell><cell cols="2">MS1M VGGFace2</cell></row><row><cell>Top-1 error (%)</cell><cell>10.6</cell><cell>5.6</cell><cell>3.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table VI and Figure 6 presents the comparison results.</figDesc><table><row><cell>Training dataset</cell><cell></cell><cell>VGGFace</cell><cell></cell><cell></cell><cell>MS1M</cell><cell></cell><cell></cell><cell>VGGFace2</cell><cell></cell></row><row><cell></cell><cell>front</cell><cell>three-quarter</cell><cell>profile</cell><cell>front</cell><cell>three-quarter</cell><cell>profile</cell><cell>front</cell><cell>three-quarter</cell><cell>profile</cell></row><row><cell>front</cell><cell>0.5781</cell><cell>0.5679</cell><cell>0.4821</cell><cell>0.5661</cell><cell>0.5582</cell><cell>0.4715</cell><cell>0.6876</cell><cell>0.6821</cell><cell>0.6222</cell></row><row><cell>three-quarter</cell><cell>0.5706</cell><cell>0.5957</cell><cell>0.5345</cell><cell>0.5628</cell><cell>0.5766</cell><cell>0.5036</cell><cell>0.6859</cell><cell>0.6980</cell><cell>0.6481</cell></row><row><cell>profile</cell><cell>0.4859</cell><cell>0.5379</cell><cell>0.5682</cell><cell>0.4776</cell><cell>0.5064</cell><cell>0.5094</cell><cell>0.6264</cell><cell>0.6515</cell><cell>0.6488</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">The effect of training set. We first investigate the effect</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">of different training sets based on the same architecture</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table IV :</head><label>IV</label><figDesc>Face probing across poses. Similarity scores are evaluated across pose templates. A higher value is better.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.robots.ox.ac.uk/ ∼ vgg/data/vgg face2/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://elancerits.com/ https://momenta.ai/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training dataset</head><p>Arch.</p><p>1:1 Verification TAR 1:N Identification TPIR FAR=0.001 FAR=0.01 FAR=0.1 FPIR=0.01 FPIR=0.1 Rank-1 Rank-5 Rank-10 VGGFace <ref type="bibr" target="#b15">[16]</ref> ResNet-50 0.620 ± 0.043 0.834 ± 0.021 0.954 ± 0.005 0.454 ± 0.058 0.748 ± 0.024 0.925 ± 0.008 0.972 ± 0.005 0.983 ± 0.003 MS1M <ref type="bibr" target="#b6">[7]</ref> ResNet-50 0.851 ± 0.030 0.939 ± 0.013 0.980 ± 0.003 0.807 ± 0.041 0.920 ± 0.012 0.961 ± 0.006 0.982 ± 0.004 0.990 ± 0. Table VI: Performance evaluation on the IJB-A dataset. A higher value is better. The values with † are read from <ref type="bibr" target="#b3">[4]</ref>.</p><p>Figure <ref type="figure">6</ref>: Results on the IJB-A dataset (average over 10 splits). Left: ROC (higher is better); Middle: DET (lower is better); Right: CMC (higher is better).</p><p>which further demonstrate the advantage of the VGGFace2 dataset. In addition, the generalisation power can be further improved by first training with MS1M and then fine-tuning with VGGFace2 (i.e. "VGGFace2 ft"), however, the difference is only 0.908 vs. 0.895. Many existing datasets are constructed by following the assumption of the superiority of wider dataset (more identities) <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b24">[25]</ref>, where the huge number of subjects would increase the difficulty of model training. In contrast, VGGFace2 takes both aspects of breath (subject number) and depth (sample number per subject) into account, guaranteeing rich intra-variation and inter-diversity.</p><p>The effect of architectures. We next investigate the effect of architectures trained on VGGFace2 (Table <ref type="table">VI</ref>). The comparison between ResNet-50 and SENet both learned from scratch reveals that SENet has a consistently superior performance on both verification and identification. More importantly, SENet trained from scratch achieves comparable results to the fine-turned ResNet-50 (i.e. first pre-trained on the MS1M dataset), demonstrating that the diversity of our dataset can be further exploited by an advanced network. In addition, the performance of SENet can be further improved by training on the two datasets VGGFace2 and MS1M, exploiting the different advantages that each offer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experiments on IJB-B</head><p>The IJB-B dataset is an extension of IJB-A, having 1, 845 subjects with 21.8K still images (including 11, 754 face and 10, 044 non-face) and 55K frames from 7, 011 videos. We evaluate the models on the standard 1:1 verification protocol (matching between the Mixed Media probes and two galleries) and 1:N identification protocol (1:N Mixed Media probes across two galleries).</p><p>We observe a similar behaviour to that of the IJB-A evaluation. For the comparison between different training sets (Table VII and Figure <ref type="figure">7</ref>), the models trained on VG-GFace2 significantly surpass the ones trained on MS1M, and the performance can be further improved by integrating the advantages of the two datasets. In addition, SENet's superiority over ResNet-50 is evident in both verification and identification with the two training settings (i.e. trained from scratch and fine-tuned). Moreover, we also compare to the results reported by others on the benchmark <ref type="bibr" target="#b21">[22]</ref> (as shown in Table <ref type="table">VII</ref>), and there is a considerable improvement over their performance for all measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we have proposed a pipeline for collecting a high-quality dataset, VGGFace2, with a wide range of pose and age. Furthermore, we demonstrate that deep models (ResNet-50 and SENet) trained on VGGFace2, achieve stateof-the-art performance on the IJB-A and IJB-B benchmarks. The dataset and models are available at https://www.robots. ox.ac.uk/ ∼ vgg/data/vgg face2/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We would like to thank Elancer and Momenta for their part in preparing the dataset 2 . This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract number 2014-14071600010. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://wiki.dbpedia.org/" />
		<title level="m">Dbpedia</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.freebase.com/" />
		<title level="m">Freebase</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The do&apos;s and don&apos;ts for cnn-based face verification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07426</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nanduri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01484</idno>
		<title level="m">Umdfaces: An annotated face dataset for training deep networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08221</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-Excitation networks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An overview of research activities in facial age estimation using the fg-net aging database</title>
		<author>
			<persName><forename type="first">G</forename><surname>Panis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="737" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for face recognition in unlabeled videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02191</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1891" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Web-scale training for face identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2746" to="2754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-b face dataset</title>
		<author>
			<persName><forename type="first">C</forename><surname>Whitelam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on Biometrics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4362" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<title level="m">Learning face representation from scratch</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
