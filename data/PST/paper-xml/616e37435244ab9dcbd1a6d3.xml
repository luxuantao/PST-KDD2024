<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DIFFERENTIABLE NETWORK PRUNING FOR MICROCONTROLLERS</title>
				<funder ref="#_cyM8ZKH">
					<orgName type="full">European Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Samsung AI</orgName>
				</funder>
				<funder ref="#_QcXeVWa">
					<orgName type="full">UK&apos;s Engineering and Physical Sciences Research Council (EP-SRC</orgName>
				</funder>
				<funder ref="#_3BaqNtB #_ynhAY7V">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-08">8 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Edgar</forename><surname>Liberis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Samsung AI Centre Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
						</author>
						<title level="a" type="main">DIFFERENTIABLE NETWORK PRUNING FOR MICROCONTROLLERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-08">8 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3569468</idno>
					<idno type="arXiv">arXiv:2110.08350v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedded and personal IoT devices are powered by microcontroller units (MCUs), whose extreme resource scarcity is a major obstacle for applications relying on on-device deep learning inference. Orders of magnitude less storage, memory and computational capacity, compared to what is typically required to execute neural networks, impose strict structural constraints on the network architecture and call for specialist model compression methodology. In this work, we present a differentiable structured network pruning method for convolutional neural networks, which integrates a model's MCU-specific resource usage and parameter importance feedback to obtain highly compressed yet accurate classification models. Our methodology (a) improves key resource usage of models up to 80?; (b) prunes iteratively while a model is trained, resulting in little to no overhead or even improved training time; (c) produces compressed models with matching or improved resource usage up to 1.4? in less time compared to prior MCU-specific methods. Compressed models are available for download.</p><p>1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>There's an increasing interest in bringing on-device neural networks to what is arguably the smallest-scale viable compute platform: microcontroller-powered IoT devices. The roadblock to this, however, is the limited compute power. Microcontroller units (MCUs) are single-chip systems, typically consisting of a single-core processor (such as ARM M-series), on-chip persistent Flash and temporary SRAM memories, other peripherals (e.g. sensors, microphones, radio). Table <ref type="table" target="#tab_0">1</ref> compares technical specifications of MCUs against data center, mobile and micro-computer hardware. MCUs are designed with cost and power-efficiency in mind, which is primarily achieved by reducing the on-board memory and compute resources. The data shows that, even when compared to micro-computers like Raspberry Pi, MCUs have orders of magnitude less storage and SRAM (GBs vs 100s KB) and a slower processor (GHz vs 100s MHz).</p><p>For many practitioners, it is not trivial to work around the resource scarcity of microcontrollers. As a result, applications choose to offload the execution of neural networks to a more capable device, such as a smartphone or a remote "cloud" backend server. This brings numerous disadvantages, such as a lowered degree of privacy, complete reliance on a work-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Load weights</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Load input</head><p>Write output 1:</p><p>2: 3:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flash</head><p>Neural network weights</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Framework</head><p>Operating system</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SRAM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Activation matrices</head><p>Other buffers</p><p>Figure <ref type="figure">1</ref>. Deep learning inference on an MCU illustrated using relevant components. A flat memory hierarchy and its limited capacity impose significant constraints on the network architecture.</p><p>ing communication link, increased response latency, and increased power usage for radio communication.</p><p>To see what properties the resource scarcity imposes on neural networks, let us take a more detailed look at how a neural network's execution may be mapped to a microcontroller (Figure <ref type="figure">1</ref>), such as via the TensorFlow Lite Micro runtime <ref type="bibr" target="#b7">(David et al., 2021)</ref>:</p><p>? Each layer of a neural network will be executed in some predefined order, one at a time and in its entirety. ? A layer is executed by loading its parameters <ref type="bibr">(weights)</ref> from Flash, loading its input from SRAM and writing the output back to SRAM. and program code, are stored in the read-only Flash memory. Therefore, the size of a neural network is limited by Flash (e.g. ? 64KB). ? All temporary intermediate data, including activation matrices, will be stored in SRAM. Therefore, a neural network's peak activation size / memory usage is limited by the amount of SRAM (e.g. ? 64 KB).</p><p>As a compute platform for deep learning, microcontrollers pose unique challenges that do not similarly manifest on other platforms. The need to constrain SRAM usage, a relatively slow processor and orders of magnitude fewer resources requires further specialist research than what is typically explored in GPU/mobile-scale neural network design and compression. In fact, the need for MCUspecific methodology has spawned a research subfield called TinyML, which includes runtime design <ref type="bibr" target="#b7">(David et al., 2021;</ref><ref type="bibr" target="#b23">Liberis &amp; Lane, 2020)</ref>, optimised layer implementations <ref type="bibr" target="#b20">(Lai et al., 2018)</ref> and task-specific MCU-sized model discovery <ref type="bibr" target="#b11">(Fedorov et al., 2019;</ref><ref type="bibr" target="#b24">Liberis et al., 2021;</ref><ref type="bibr" target="#b3">Banbury et al., 2021)</ref>.</p><p>In this work (Figure <ref type="figure">2</ref>), we tackle this problem through the use of budgeted differentiable network pruning. We employ bi-level gradient descent optimisation to learn the sizes of each layer in the network, resulting in iterative compression that is interleaved with network training and has negligible overhead. Our methodology formulates model size, peak memory usage and latency constraints as resource budget requirements and incorporates them into pruning as differentiable objectives. This achieves MCU specialisation currently absent from pruning literature and, compared to other MCU-level compression methods, the use of pruning and accurate objectives achieves faster compression while correctly targeting resource bottlenecks.</p><p>Differentiable pruning is evaluated on benchmark audio and image classification tasks, suitable for microcontrollers, and similar to real-world sensing applications: Speech Commands keyword spotting <ref type="bibr" target="#b41">(Warden, 2018)</ref>, Visual Wake Words <ref type="bibr" target="#b6">(Chowdhery et al., 2019)</ref> person detection, CIFAR-10 ( <ref type="bibr" target="#b19">Krizhevsky, 2009)</ref> and ImageNet <ref type="bibr" target="#b8">(Deng et al., 2009)</ref> image classification. Additionally, a range of investigative studies is carried out to establish that: (a) the resource objectives are required for finding microcontroller-compatible models; (b) an accurate minimal peak memory usage computation is essential for correctly allocating the SRAM budget;</p><p>(c) pruning allocates resources better than popular manual uniform scaling on MobileNet-v2 <ref type="bibr" target="#b33">(Sandler et al., 2018)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section will describe how our methodology fits within the wider taxonomy of pruning algorithms and compare it to existing methods for MCU deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Network pruning</head><p>Network pruning removes parameters from a network that are deemed the least important for generalisation. This inherently introduces a trade-off between the predictive performance (here, classification accuracy) and the resource footprint of a network. Perhaps surprisingly, having conducted a wide literature survey of 81 pruning techniques, Figure <ref type="figure">2</ref>. A graphical summary of the proposed method. Both task-specific and resource usage-specific objectives influence pruning. <ref type="bibr" target="#b4">Blalock et al. (2020)</ref> cannot find significant chronological trends in improving this trade-off.</p><p>What is, therefore, the utility of yet another pruning method? Despite comparable gains in compression rates, pruning methodologies exhibit a variety of secondary characteristics. These include: (a) computational overhead or cost of pruning; (b) the ability to produce sparse or dense models; (c) the amount of additional training required to recover from parameter loss; (d) the ability to optimise metrics, defined as a function of the network architecture; (e) ease of use and hyperparameter sensitivity; (f) over-regularisation of surviving parameters of the network. The characteristics needed for microcontroller specialisation or the usability of this methodology can be selected by carefully considering these (un-)desirable properties during the design stage.</p><p>We classify our methodology as structured, during-training, iterative, budgeted and differentiable pruning, with each category explained below.</p><p>Structured vs unstructured pruning. Pruning is often used to obtain sparse models, which require sparse matrix multiplication implementations or hardware support to execute efficiently <ref type="bibr" target="#b0">(Anwar et al., 2015)</ref>. This work implements structured pruning, which effectively performs hyperparameter adjustment by removing entire feature maps. This results in a dense (not sparse) model that can be efficiently executed using widely-supported microcontroller deep learning software.</p><p>Budgeted vs "opportunistic" pruning. Resource usageunaware pruning methods typically use the total sparsity (the proportion of feature maps or channels removed) in the model across all layers to describe the extent of model compression. In practice, this is not a descriptive metric that can be used to target a particular resource budget because the same sparsity level can result in a range of model sizes and MAC operations. A popular way to achieve resource awareness is to add a sparsifying group L 1 regularisation <ref type="bibr" target="#b13">(Gordon et al., 2018;</ref><ref type="bibr" target="#b28">Liu et al., 2017)</ref> to all parameters of the network, followed by tuning its strength until the desired resource usage is reached. Purpose-made budgeted pruning <ref type="bibr" target="#b22">(Lemaire et al., 2019;</ref><ref type="bibr" target="#b30">Ning et al., 2020)</ref>, such as this work, allocates resources outside of the training loss and, therefore, does not rely on remaining channels to generalise despite additional regularisation.</p><p>Pre-, during-and post-training pruning. Post-training pruning removes parameters from a converged model, often followed by fine-tuning to recover any lost accuracy <ref type="bibr" target="#b14">(Han et al., 2015)</ref>. Fine-tuning and training can be combined by performing pruning during training, saving time <ref type="bibr">(Lin et al., 2020b)</ref>. Pre-training methods use initialisation and gradient-flow information <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> to decide what to prune before training or very early on. Performing pruning as early as possible avoids the computational overhead associated with extraneous training and enables the compressed model to be used for remaining training, reducing the overall training time.</p><p>Differentiable vs non-differentiable pruning. An essential consideration in pruning is the distribution of sparsity across network layers. Per-layer sparsity can be decided by a separate optimisation process, which can support custom resource usage metrics expressed as functions of layer width. The introduction of an architectural optimisation process brings concerns previously seen in NAS, as the optimisation needs efficient estimates of the network classification accuracy (e.g. a proxy model <ref type="bibr" target="#b27">(Liu et al., 2020)</ref>) to push back against resource usage objectives. In this instance, differentiable optimisation methods allow for efficient iterative bi-level optimisation-both pruning (learning layer sizes) and training are performed using gradient descent <ref type="bibr" target="#b30">(Ning et al., 2020)</ref>-that improves upon time-consuming alternative search algorithms like evolutionary search.</p><p>Iterative vs single-shot pruning. Compared to removing all required parameters at once, repeatedly removing a small number of parameters was shown to give better recovery from parameter loss at high pruning ratios <ref type="bibr" target="#b14">(Han et al., 2015)</ref>. Differentiable pruning is performed iteratively, which allows this recovery to happen during training and not at a subsequent fine-tuning stage.</p><p>Among prior network pruning methodologies, Differen-tiable Sparsity Allocation (DSA) <ref type="bibr" target="#b30">(Ning et al., 2020)</ref> is the closest work. The methodologies share the same insight to patch through classification loss gradients from masks to learnable channel width multipliers (see Section 3). Otherwise, methodologies differ in design and implementation details: DSA uses slow ADMM optimisation and lacks microcontroller-specific optimisation objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning for microcontrollers</head><p>Designing MCU-compatible neural network architectures is a challenging task due to the need to obtain a sufficiently high predictive performance while maintaining a sufficiently small resource footprint. Existing MCU-specific deep learning methods that achieve this can be classified into (a) model compression methods applied to manually designed networks, such as weight quantisation or binarisation <ref type="bibr" target="#b28">(Zhang et al., 2017;</ref><ref type="bibr" target="#b29">Mocerino &amp; Calimera, 2019)</ref>, pruning (this work) or channel search <ref type="bibr" target="#b3">(Banbury et al., 2021)</ref>; (b) neural architecture search (NAS) <ref type="bibr">(Lin et al., 2020a)</ref>. Search methods can leverage simple pruning as a nested step to further reduce the resource footprint of a candidate network <ref type="bibr" target="#b24">(Liberis et al., 2021;</ref><ref type="bibr" target="#b11">Fedorov et al., 2019)</ref>.</p><p>In the extreme, NAS methods may only search for layer width hyperparameters to maximise search efficiency, which is, traditionally, the territory of structured pruning. As a result, the closest microcontroller specialist work to ours is the "MicroNets" <ref type="bibr" target="#b3">(Banbury et al., 2021)</ref> NAS system, which finds layer sizes for particular task-specific architectures using differentiable optimisation. Provided with a backbone architecture, the method works by masking each layer at a predefined set of widths (10%, 20%, . . . , 100%)-eventually, only one will be included in the final architecture, followed by fine-tuning. Experimentally, we find that both methods can discover models with similar classification accuracy and resource usage given the same backbone architecture. However, differentiable pruning offers additional benefits in comparison: (a) the pruning is guided by an improved peak memory usage calculation which is more accurate for network architectures with branches and parallel paths; (b) improved running time: compared to architecture search, pruning has negligible overhead, requires no fine-tuning and can even reduce the amount of computation needed for training by switching to a pruned model after layer widths have been established; (c) pruning allocates the resource budget more efficiently by operating at a per-channel granularity, not in increments of 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DIFFERENTIABLE PRUNING FOR MCUS</head><p>Here, we describe (a) the neural network resource usage calculation for MCUs; (b) the task-specific pruning objective; (c) how the two are combined into a differentiable function of a network's layer sizes; (d) how the latter is used to create a differentiable budgeted pruning algorithm. The unique addition of MCU-specific resource usage to pruning allows for the fast discovery of MCU-friendly models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Execution constraints</head><p>We formulate model latency, peak memory usage and size constraints that are both differentiable and faithful to the real resource usage of a neural network to guide pruning towards MCU-compatible models.</p><p>Model latency. To ensure that the pruning algorithm creates models that run sufficiently quickly, we implement a model latency constraint. For GPU-powered systems, model latency can be either directly measured or predicted using sophisticated models. <ref type="bibr" target="#b10">(Dudziak et al., 2020)</ref> However, on MCU-powered systems, model latency can be predicted using simple metrics, such as the number of floatingpoint operations (FLOPs) or multiply-accumulate operations (MACs), due to the lack of any performance-enhancing features, like parallelism or caching <ref type="bibr" target="#b24">(Liberis et al., 2021)</ref>. Latency has also been shown to be a good predictor of power consumption on MCUs <ref type="bibr" target="#b3">(Banbury et al., 2021)</ref>, due to the processor staying in the highest power state throughout the inference.</p><p>Peak memory usage. Peak memory usage at inference is an uncommon resource property to target in model compression. Most consumer hardware, such as devices presented in Table <ref type="table" target="#tab_0">1</ref>, have a multi-level memory hierarchy, with each layer offering more capacity at a slower access speed: multiple levels of fast but small SRAM data and instruction caches co-located with the CPU, followed by at least 1 GB of RAM. The latter is sufficient for most compressed neural networks. MCUs, on the other hand, lack a memory that resides on the same capacity vs access cost trade-off as RAM, which forces applications and the operating system to work within the SRAM (here, ?64-512 KB). Additionally, it is prohibitively expensive to use persistent storage, such as an SD card, for storing temporary results. <ref type="bibr" target="#b23">(Liberis &amp; Lane, 2020)</ref> Thus, to produce MCU-compatible architectures, we must ensure that the size of activation matrices at its largest remains below the SRAM capacity.</p><p>On an MCU, the network's layers are executed one by one, following a predetermined order, which is a valid topological ordering of layers (nodes) in a computation graph. However, not all topological orders yield the same peak memory usage: whenever the runtime has multiple layers that are ready to be executed (i.e. all of their inputs have been computed), the choice of which layer to execute determines which tensor memory buffers are allocated and de-allocated at any given time, which affects the peak memory usage. Thus by considering different topological orders, we can compute the minimum attainable peak memory usage.</p><p>Computing the minimum achievable peak memory usage is not straightforward for arbitrary neural network computation graphs. In fact, the few previous works <ref type="bibr" target="#b3">(Banbury et al., 2021;</ref><ref type="bibr" target="#b11">Fedorov et al., 2019)</ref> which have explored this problem, resorted to an imprecise under-approximation, which computes the maximum total size of input and output buffers for each operator individually, which does not consider their position within the execution order. This under-approximation is incorrect for networks that contain branches, which are commonplace in state-of-the-art convolutional networks <ref type="bibr" target="#b33">(Sandler et al., 2018;</ref><ref type="bibr" target="#b35">Tan &amp; Le, 2019;</ref><ref type="bibr" target="#b5">Cheng et al., 2019;</ref><ref type="bibr" target="#b15">He et al., 2016)</ref>.</p><p>Instead, we adopt an accurate peak memory usage metric based on dynamic programming, which efficiently enumerates all topological orderings of the computation graph (feasible for most CNNs) to find the one with the smallest peak memory usage <ref type="bibr" target="#b23">(Liberis &amp; Lane, 2020)</ref>. In practice, to obtain gradients of layer sizes with respect to peak memory usage, it is sufficient to only run this algorithm on the forward pass of computing the objective and record which tensors formed the memory bottleneck-the peak memory usage is given by the sum of bottleneck tensor sizes, which is differentiable.</p><p>In Section 4, we will empirically show that the two options for calculating peak memory usage-precise and imprecise-diverge during pruning on MobileNet-v2 and ResNet networks, with the imprecise objective causing a violation of resource budget constraints.</p><p>Model size. All constant data, including program code and the weights of a neural network, are stored in the Flash memory. The small amount of storage available (&lt; 64-128 KB) imposes a significant constraint on model design.</p><p>Our methodology uses quantisation to reduce the storage requirement of a network. Due to its widespread support, we employ affine quantisation <ref type="bibr" target="#b18">(Jacob et al., 2018)</ref>, which reduces the per-parameter memory requirement to 8 bits; however, other types of quantisation can be used instead. Therefore, the model size objective is the sum of all weight tensor sizes at 1 byte per parameter.</p><p>Here, quantisation plays a dual role: in addition to reducing storage usage, it enables execution on the most underpowered microcontrollers, which do not support floating-point computation. Using -bits, in particular, is a convenient choice for most compute platforms, as it is a natively supported data type: compared to &lt; 8-bit or variable encoding schemes, it does not require any decoding at runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pruning objectives</head><p>To have fine-grained control over the resource usage of a network, the width of each prunable layer, i.e. the number of feature maps, channels or units in convolutional and fully-connected layers, is set individually. The remainder of the section will refer to feature maps as channels, borrowing CNN terminology. As layers have differing numbers of channels, the algorithm maintains layerwise width multipliers ? ? = (? 1 , . . . , ? L , . . .), where each ? L ? (0; 1] determines the fraction of channels to be kept alive (or, conversely, pruned away) within each layer. Each ? L is treated as a continuous variable, and the number of channels is determined by rounding down the product of ? L and the original layer width.</p><p>The optimisation requires a suitable loss P, such that ?P ?? L can be computed and used to adjust each ? L with gradient descent. A linear combination of (a) a resource constraint loss P RES , which calculates the extent to which the resource budget is violated, and (b) a task loss P TSK , which encourages maintaining a low classification loss, is used:</p><formula xml:id="formula_0">P = ? RES P RES + ? TSK P TSK (1)</formula><p>Resource budget constraints are, therefore, treated as additional objectives and not as optimisation equality constraints. P is minimised in parallel to network training as an outerlevel optimisation. Therefore, P is not a part of the network training loss and is not a regulariser directly. The trade-off hyperparameters control the contribution of the two losses.</p><p>In practice, the optimisation has low sensitivity to the values of ? and the pruning learning rate (established experimentally in Appendix A).</p><p>Note that P is not a part of a network's training loss and is not a regulariser directly, since pruning is carried out in parallel to network training as an outer-level optimisation. The hyperparameter ? is set depending on the extent of constraint violation and is constant throughout training: we measure the initial values both losses and set ? = rP init RES /P init TSK , where r is usually between 1 /2 and 3 /4, typically 2 /3. Definition of P RES . Differentiable pruning uses the three resource usage objectives: model size, latency (MACs) and peak memory usage. The latter is implemented by the minimal accurate peak memory usage objective that considers layer execution order. The objectives can be expressed in terms of, and (piece-wise) differentiated with respect to, the layerwise multipliers ? ?. All objectives are collapsed into one through the use of random scalarisations <ref type="bibr" target="#b31">(Paria et al., 2020)</ref> to randomly prioritise different objectives at each update iteration (t) while maintaining a bias towards constraints that are violated to a greater extent. Coefficients ?</p><formula xml:id="formula_1">? t = (? t 1 , ? t 2 , ?<label>t</label></formula><p>3 ) are sampled randomly ? i ? 1/UNIFORM(0, 1).</p><formula xml:id="formula_2">P t RES (? ?) = max ? ? ? ? ? ? ? ? ? ? ? ? t 1 PEAKMEMUSAGE(? ?)/b 1 , ? t 2 MODELSIZE(? ?)/b 2 , ? t 3 MACS(? ?)/b 3 -1 (2)</formula><p>The optimisation requires the user to specify the desired limit (upper bound) for each resource metric, b i . If randomness is undesirable, the budget-scaled objectives may also be combined via an unweighted sum. For implementation purposes, coefficients ? ? are only used to choose the objective within "max" and do not scale the gradients. P RES is clipped to be &gt; 0 after the constraints are satisfied to prevent further compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pruning algorithm</head><p>The pruning of neural network weights is implemented by applying channel-wise binary masks M to the output of each layer. This effectively sets specific output channels of a layer all to 0, allowing the parameters responsible for computing and consuming the masked-out channels to be safely removed from the network. An unpruned neural network is denoted as f and its pruned counterpart as f M .</p><p>Definition of P TSK . Pruning seeks to minimise (or improve upon) the difference between the classification loss of the unpruned and the pruned network on unseen data while only adjusting elements of the binary mask M:</p><formula xml:id="formula_3">argmin M [L ce (f M , D) -L ce (f, D)] = argmin M L ce (f M , D)<label>(3)</label></formula><p>where L ce stands for classification loss (cross-entropy) and D for a validation dataset. Therefore, the classification loss can be used as task-specific feedback for pruning:</p><formula xml:id="formula_4">P TSK def = L ce (f M , D)<label>(4)</label></formula><p>Channel importance. Pruning algorithms typically use a parameter importance (salience) criterion to determine which parameters to keep. Both for sparse (unstructured) and structured pruning, popular choices include magnitudebased <ref type="bibr" target="#b14">(Han et al., 2015)</ref>, norm-based (L 1 or L 2 ) <ref type="bibr">(Lin et al., 2020b)</ref>, Hessian-based <ref type="bibr" target="#b37">(Theis et al., 2018)</ref>, batch normalisation (BN)-specific <ref type="bibr" target="#b28">(Liu et al., 2017)</ref>, or gradient flowbased <ref type="bibr" target="#b21">(Lee et al., 2019;</ref><ref type="bibr" target="#b40">Wang et al., 2020)</ref> salience metrics.</p><p>The methodology focuses on convolutional neural networks (CNNs), assembled out of "Conv-BatchNorm (BN)-ReLU" layer sequences, fully connected (FC) layers, and other parameter-free layers. In the former case, the salience ? s is chosen to be scaling coefficients ? ? within batch normalisation, and in the latter case, the L 2 norm of weights of a neuron. More specifically, the importance of a particular channel/neuron i is s i , given by:</p><formula xml:id="formula_5">Conv-BN-ReLU(x) = ReLU[? ? Norm(K * x) + ? ?] s i = |? i | FC(x) = Wx + ? b s i = ||W ?,i || 2</formula><p>where x is the input tensor, K, W and ? b are parameter tensors, * is a convolution operation and "Norm" is a meanzero variance-one normalisation function, and ? ? and ? ? are learned batch normalisation scaling and offset parameters.</p><p>Pruning by salience. Suppose that within a layer L with C output channels, a proportion ? L is set to be kept and not pruned. During pruning, all channels are ranked according to their salience ( ? s L ) and top ? L (? th L quantile) are kept (entries in M are set to 1) and bottom 1 -? L are discarded (M set to 0). Therefore, for a given layer L, M is computed by a hard threshold function, denoted M , of channel salience, with the salience threshold ? L picked to satisfy the constraint that only ? L proportion of channels have salience s L,i &gt; ? L . Remaining text refers to ? s L and s L,i as ? s and s i , respectively, making the layer implicit for brevity.</p><p>Differentiable pruning. Gradient update to ? L is calculated assuming a continuous relaxation of the hard threshold function on the backwards pass only. Following DSA <ref type="bibr" target="#b30">(Ning et al., 2020)</ref>, we use a sigmoid function in the log-domain of saliences, which is differentiable with respect to ? L :</p><formula xml:id="formula_6">M (s i , s 0 ) = 1 1 + e -[ln s i -ln s 0 ] = 1 1 + s 0 /s i<label>(5)</label></formula><p>and enforce a constraint that the proportion of kept channels should equal the desired channel width multiplier ? L :</p><formula xml:id="formula_7">1 C C j M (s j , s 0 ) = ? i<label>(6)</label></formula><p>Differentiating Eqn. 6 w.r.t. ? i and rearranging gives:</p><formula xml:id="formula_8">1 C C j ?M (s j , s 0 ) ?s 0 ?s 0 ?? i = 1 (7) ?s 0 ?? i = C ? C j ?M (s j ,s 0 ) ?s 0<label>(8)</label></formula><p>This is used to connect task loss gradients with respect to the mask M back to each ? L :</p><formula xml:id="formula_9">?P TSK ?? i = [ C j ?P TSK ?M (s j , s 0 ) ?M (s j , s 0 ) ?s 0 ] ?s 0 ?? i Eqn. 8 = C C j ?P TSK ?M (s j , s 0 ) ?M (s j , s 0 )/?s 0 ? C k ?M (s k , s 0 )/?s 0 (9)</formula><p>Focus regime for bottleneck objectives. One out of three resource usage objectives considered is a bottleneck objective: only a few layers (the bottleneck) contribute to peak memory usage at a time. This differs from model size and MACs, to which all layers contribute to a certain extent.</p><p>As per Equation 1, gradients from both P TSK and P RES are combined to update ? ?, which may needlessly shrink layers that are not in the bottleneck when other resource constraints are already satisfied. To prevent this, ? L s is updated only for bottleneck layers, i.e. layers for which ?P RES ?? L &gt; 0.</p><formula xml:id="formula_10">+ ... + ... M 1 M 1 M 2 M 1 M 2 M 1 M 3 + M 1 M 1 M 2 M 3 Figure 3</formula><p>. Three examples of pruning over Add, such as residual connections. All summands share the same pruning mask.</p><p>Pruning residual connections. Many modern CNNs feature multiple processing paths aggregated using addition.</p><p>To correctly preserve the addition of pruned feature maps, their surviving (unpruned) channels must have matching positions, i.e. their pruning masks have to be equal <ref type="bibr" target="#b30">(Ning et al., 2020;</ref><ref type="bibr" target="#b39">van Amersfoort et al., 2020)</ref>. It is also possible to work around this requirement with padding and channel rearrangement, albeit at extra implementation overhead <ref type="bibr" target="#b22">(Lemaire et al., 2019)</ref>.</p><p>Figure <ref type="figure">3</ref> shows examples of tying pruning masks in practice, including residual connections <ref type="bibr" target="#b15">(He et al., 2016)</ref>. Groups of layers that share the pruning mask are pre-computed in advance using connected components search; the combined salience is the maximum of saliences of each layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>Pruning iterations are interleaved with model training steps, providing compression with no subsequent fine-tuning: the accuracy loss from the marginal reduction of layer sizes can be recovered before the next pruning iteration.</p><p>Layer masks and layerwise multipliers ? ? are updated using functions in Algorithm 1 every 20 training steps: ? ? is updated using stochastic gradient descent (SGD) with learning rate ? ? within WIDTHUPDATE, followed by recalculating masks using MASK (without computing ?M ?? L ). To enforce ? L ? (0, 1], ? ? is stored as an unconstrained real variable vector internally with a scaled "exp" function applied to it (? L starts at 1.0 and, by design, can only decrease in value). The flat shape of "exp" for small values of the underlying variable also discourages shrinkage of small ? i 's, akin to L 2 regularisation.</p><p>Algorithm 1 A high-level implementation of the update procedure for width multipliers ? (WIDTHUPDATE). f , P RES , P TSK , D, ? s, ? L as defined in Sections 3.2, 3.3. DIFF(f , x) computes ?f ?x . SALIENCE and PERCENTILE are selfdescriptive helper functions.</p><formula xml:id="formula_11">function WIDTHUPDATE(f , ? ?, D) M, ?M ?? L ? [MASK(Layer i , ? L ) for Layer i ? f ] ?P TSK ?M ? DIFF(P ce (f M , D), M) ?P TSK ? ? ? ? use Equation 9 with ?P TSK ?M and ?M ?? L ?P RES ? ? ? ? DIFF(P RES (? ?), ? ?) ? Equation 2 ?P ? ? ? ? ? RES ?P RES ? ? ? + ? TSK ?P TSK ? ? ? ? Equation 1 ? ? ? ? ? -? ? ?P ? ? ? ? SGD update with learn. rate ? ? end function function MASK(L, ? L ) ? s ?SALIENCE(L) ? as per Section 3.3 ? L ?PERCENTILE(? s, ? L ) ? ? th L percentile of ? s M ? (? s &gt; ? L ) ?M ?? L ? DIFF(M (? s, ? L ), ? L ) ? Equation 5 return M, ?M ?? L end function</formula><p>Pruning stops-that is, channel width multipliers ? ? are no longer updated-when the user-specified resource usage bounds have been reached. However, channel masks M are not frozen until the final quantisation-aware training stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>Differentiable pruning for microcontrollers is empirically evaluated as follows:</p><p>1. The utility of resource usage objectives within pruning is tested through an ablation study to show that this feedback is essential for targeting microcontrollerspecific bottlenecks. 2. An accurate peak memory usage objective is compared to memory usage metrics used in prior work (if any) to show that the precise calculation is required for correctly allocating the SRAM budget. 3. Pruning is compared to the uniform scaling of layers on MobileNet-v2 <ref type="bibr" target="#b33">(Sandler et al., 2018)</ref> to show that pruning allocates resources more efficiently. 4. Pruned models for fourteen architecture and task pairs are presented, with an improved classification accuracy, resource usage or time taken for compression, compared to related network pruning and microcontrollerspecialist work. 5. The low sensitivity of the methodology to feedback trade-off hyperparameters ? and pruning learning rate ? ? is established (Appendix A).</p><p>Further analysis and discussion of the applicability of this methodology to real-world TinyML tasks are presented in Section 5.</p><p>Baseline comparisons. Differentiable pruning is tested on Speech Commands <ref type="bibr" target="#b41">(Warden, 2018)</ref> To the best of our knowledge, this work is the first network pruning methodology constructed to optimise for microcontroller resource bottlenecks. As there are no directly comparable microcontroller-specialist pruning methods, the comparisons are performed against, firstly, prior differentiable larger-scale network pruning methods and, secondly, microcontroller NAS systems for TinyML tasks. More specifically, the evaluation aims to:</p><p>(a) Verify that the compressed models have comparable or better resource usage than GPU-/mobile-scale differentiable budgeted pruning methods, DSA <ref type="bibr" target="#b30">(Ning et al., 2020)</ref> and AutoCompress <ref type="bibr" target="#b27">(Liu et al., 2020)</ref> (auto filter pruning), for CIFAR-10. (b) Show that this methodology can produce comparable or better models than the closest prior MCU specialist work, MicroNets <ref type="bibr" target="#b3">(Banbury et al., 2021)</ref> and MCUNet <ref type="bibr">(Lin et al., 2020a)</ref>, for other datasets, and do so faster due to the use of pruning. (c) Verify that differentiable pruning would not deliver high-performant models without being guided by the microcontroller-specific resource usage objectives.</p><p>Unless otherwise specified, the baseline models have been reimplemented, and their resource usage was recalculated within the same framework 5 to facilitate a fair comparison.</p><p>All models use the same training and data augmentation protocols and are quantised to 8-bit precision. Distillation, fine-tuning using the validation set, or other adjustments which may confound the results are not used. For "time" comparison with NAS methods, the computation required to train a supernetwork model and subsequently fine-tune the discovered smaller model is compared against the computation needed for a single training run of the full model (this work), with the amounts of computation estimated using MAC operation counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation studies</head><p>Task-only or resource-only pruning. To explore the impact of the two feedback losses, pruning can be performed using only the task loss<ref type="foot" target="#foot_2">6</ref> (P RES = 0) or only the resource-constraint loss (P TSK = 0). The data shows that both losses are required to correctly prioritise unimportant and resource-expensive channels for removal, although tested models can largely successfully recover from resource-driven pruning alone:</p><p>? Pruning with P TSK relegates this work to a class of resource-unaware structured pruning methods. More channels are expected to be pruned away until the desired resource usage is eventually reached due to a reduced ability to target computationally-expensive layers first, resulting in lower accuracy overall. ? Pruning with P RES results in an iterative layer adjustment in proportion to the constraint violation, with no awareness of which layers contribute more to generalisation. The lack of protection of more salient layers (no P TSK ) results in lower accuracy overall, although the accuracy differences are minor, suggesting a greater ability to compensate for removed channels when sparsity is low. In general, one cannot predict the resulting level of sparsity compared to using both losses, as it depends on the relative contribution of each channel to resource usage.</p><p>Summary. Adding microcontroller-specific resource usage feedback to pruning, one of the main contributions of this work, is justified: parameter importance-based pruning alone is inadequate for obtaining accurate highlycompressed neural networks.</p><p>Usefulness of peak memory usage. One of the essential MCU resource constraints to be addressed, which does not similarly manifest on other platforms, is peak memory usage. Differentiable pruning uses the accurate minimal peak memory usage metric, which optimises layer execution order. Three pruning configurations are considered for testing the contribution of the memory usage objective: (a) no peak memory usage objective, leaving only latency and model size; (b) an imprecise under-approximating peak memory usage objective, used in prior work <ref type="bibr" target="#b3">(Banbury et al., 2021;</ref><ref type="bibr" target="#b11">Fedorov et al., 2019)</ref>; (c) the full set of resource objectives.</p><p>The results are expected to show a gap between the precise and the imprecise measurements; only pruning with the precise objective should produce a model whose true peak memory usage lies within the memory budget.</p><p>Figure <ref type="figure" target="#fig_0">4</ref> shows the evolution of peak memory usage (PMU) during pruning. The data shows that: (a) using an underapproximation to PMU would cause pruning to be terminated early, potentially while the true memory usage still exceeds the memory budget; (b) the two PMU measures may diverge as the network architecture changes; (c) using PMU as a pruning objective is necessary to ensure that the final memory usage lays within the budget; that is, the peak memory usage requirement is not satisfied via other objectives.</p><p>Summary. Targeting model size or latency (MACs) alone, as done in some prior GPU-or mobile-level resource-aware pruning work <ref type="bibr" target="#b13">(Gordon et al., 2018;</ref><ref type="bibr" target="#b30">Ning et al., 2020)</ref>, is insufficient for obtaining microcontroller-compatible models, as peak memory usage would not be sufficiently reduced.</p><p>Using an accurate memory usage objective is essential for both correctly identifying which tensors contribute to the memory bottleneck and determining when the desired resource usage has been reached to stop pruning.  the number of convolutional channels across the network layers. Similarly, pruning achieves a range of trade-off points and is expected to perform better due to the ability to adjust each layer individually.</p><p>To compare pruning and uniform scaling, different scales of the MobileNet-v2 model are considered-?0.75, ?0.50, ?0.25-as defined by the authors. Pruning is applied to the base architecture (?1.0) to produce a network that matches or improves upon the resource usage of each of the uniformly scaled models while preserving or improving upon the classification accuracy.</p><p>Figure <ref type="figure">5</ref> shows the accuracy and resource usage Pareto front for scaled or pruned MobileNet-v2 models. The results confirm the hypothesis that pruning can find a more efficient resource budget allocation, reducing the size of the final model by up to 46%. Pruning is known to have a regularising effect at low pruning ratios <ref type="bibr" target="#b16">(Hoefler et al., 2021)</ref>, resulting in a boost in performance compared to the baseline model at ?0.75 scaling.</p><p>Summary. Computational resource budget can be allocated more efficiently by pruning instead of uniformly scaling network layers. Differentiable pruning improves the classification accuracy vs resource usage Pareto front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Discovered models</head><p>Table <ref type="table" target="#tab_6">3</ref> presents several low-footprint models obtained by differentiable pruning and using related work, as per evaluation settings described earlier. The methodology targets more capable hardware than previously considered in ?NAS: "mid-to-high-tier" MCUs with &gt; 64 kB of storage and &gt; 32 kB of memory. For each dataset and architecture (backbone) combination, we consider the relative amount of computation required to obtain the model ("compression time"), the classification accuracy, and resource usage of (a) a full unpruned model (where illustrative), (b) a baseline compressed model from prior work and (c) a pruned model obtained using this methodology. 'INT8' represents a reimplemented quantised model, 'FP32' denotes a full precision model as reported by its authors; "unk." is unreported data.</p><p>The data shows that differentiable pruning improves upon the accuracy, resource usage or relative speed of compression compared to baseline methods. Most notably, this work (a) improves key resource usage of neural networks up to 80?; (b) has negligible overhead compared to prior microcontroller-specific NAS methodologies, and can even reduce the amount of computation needed to train the network with early termination; (c) produces compressed models with matching or improved resource usage by up to 40% compared to prior work.</p><p>Summary. Differentiable pruning produces superior models for a range of microcontroller resource usage budgets compared to related work without any task-specific adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>The following section discusses points relating to the practical application of differentiable pruning:</p><p>? Terminating pruning at the earliest possible opportunity can reduce the amount of computation required for training, compared to training without pruning. ? Two pruned models are analysed using power use and latency measurements on two MCU development boards to establish the viability of on-device inference. ? The two models are analysed layer-wise to identify which layers are targeted by pruning to satisfy the resource budget (Appendix B). ? Differentiable pruning is broadly applicable to tasks typically tackled by the "smart" software running on embedded, wearable and IoT devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Early termination</head><p>The stages of network training at which the layer sizes ? ? are learned can be controlled by adjusting the learning rate and the epoch at which the learning starts. Once the architecture satisfies the resource constraints, the learning of ? ? stops; however, the pruning masks M can continue to change. Previously pruned channels can be re-enabled if the pruning threshold ? L falls below salience s i . This continuing change is akin to spatial Dropout regularisation <ref type="bibr" target="#b38">(Tompson et al., 2015)</ref>, which may either improve or hinder performance.</p><p>The computation required for training can be reduced by switching to training the pruned model once ? ? has converged (or soon after). This computation saving can be maximised if (a) pruning starts early in the training process and (b) converges quickly (high learning rate ? ? ), and (c) the regularising effect of continuing mask change can be forgone without significant impact to the final performance of the model.</p><p>To illustrate how the starting point of pruning affects the training of the network, Figure <ref type="figure" target="#fig_1">6</ref>   early termination is used, the final model performance may vary to a sizeable fraction of mask change happening after the layer size learning has stopped. The sensitivity to the pruning learning rate is investigated in Appendix A, which shows marginally better classification accuracy for lower learning rates (longer pruning). Overall, any performance loss or gain due to early termination will depend on a particular case of the architecture and compression rates requested, and is likely to be minor, as previously seen in the MobileNet-v2 example in Figure <ref type="figure">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Deployment analysis</head><p>Differentiable pruning makes a practical difference in deploying neural networks onto microcontroller-powered devices: only pruned versions of networks considered here would satisfy strict MCU computational constraints. To establish that pruned models have viable latency and power usage properties when executed on MCUs, the MobileNet-v2 and VGG-16 networks are deployed on two MCU development boards-NUCLEO-F767ZI (ARM mbed, 2022a) and NUCLEO-H743ZI2 (ARM mbed, 2022b)-using the off-the-shelf TensorFlow Lite Micro <ref type="bibr" target="#b7">(David et al., 2021)</ref> runtime. Both boards are powered by the ARM Cortex-M7 chip, with clock speeds of 216 MHz and 480 MHz.  Overall, pruning instrumental in enabling on-device deep learning inference and produced models have viable latency and energy usage to be used in real-world microcontroller-powered devices and systems. A variety of latency and energy usage requirements can be targeted by considering different model architectures or hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Application to ubiquitous computing</head><p>Considerable effort has been devoted to applying on-device deep learning to new domains, particularly in ubiquitous computing, where microcontroller-powered wearable, embedded, and IoT devices are used for diverse tasks, such as fitness tracking, health monitoring, home security and voice assistance. Developing a successful deep learning solution for ubiquitous tasks requires domain knowledge of the task at hand, a neural network architecture that can capture the structure of the domain-specific input data and learn from it, and working around the computational limitations of microcontroller hardware. The development requires a wide range of competencies that a single developer or a small team may not possess, creating a high barrier to entry.</p><p>On-device deep learning can be made more accessible to domain specialists by automating the resource usage reduction stage of the development process. Creating a general solution for automated model compression requires finding a compromise between modifying the architecture of the network yet largely preserving the expertise that went into its creation. This work bridges the gap between the resource footprint of well-performing neural network architectures and the resource scarcity of ubiquitous computing platforms by introducing a structured network pruning algorithm, specialised for microcontroller hardware.</p><p>Differentiable pruning has been evaluated on common lowfootprint classification tasks for comparison purposes; however, these tasks have parallels to challenges faced by the real-world "smart" ubiquitous computing systems. For example, object classification datasets of different difficulties mirror person detection for the privacy of third parties <ref type="bibr" target="#b9">(Dimiccoli et al., 2018)</ref>; akin to keyword spotting (via spectral feature extraction), many applications apply CNNs to acoustic and motion sensor data, preprocessed using spectral analysis techniques, e.g. filter bank coefficients or MFCCs, which are generic and used across many tasks <ref type="bibr" target="#b32">(Radu et al., 2018)</ref>, such as human activity recognition <ref type="bibr" target="#b17">(Hossain et al., 2018;</ref><ref type="bibr" target="#b43">Xue et al., 2020)</ref>, object recognition <ref type="bibr" target="#b12">(Gong et al., 2019)</ref> or handwriting recognition <ref type="bibr" target="#b44">(Yin et al., 2020)</ref> through acoustic sensing. The evaluation has shown no sensitivity to the nature of the task, which suggests the methodology may scale beyond the considered tasks.</p><p>Overall, differentiable pruning is a worthwhile addition to the model compression toolkit for ubiquitous computing developers who want to leverage on-device deep learning.</p><p>The methodology can produce models with comparable or improved resource usage to state-of-the-art solutions while offering fast compression and microcontroller specialisation. Domain experts can introduce differentiable pruning to their model development pipeline with negligible overhead, straightforward hyperparameter tuning (Appendix A), and request models just by specifying available computational resources. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A HYPERPARAMETER SENSITIVITY</head><p>Differentiable pruning learns layer sizes ? ? by optimising a linear combination of the task-specific and resource-specific losses, as an outer-level gradient descent optimisation process to network training. The full SGD update step to ? ? is given as follows. Note the use of a clipping function (omitted from the Algorithm 1 for conciseness), which is used to clip gradient values to [0, 0.025) for all experiments.</p><formula xml:id="formula_12">? ? ? = ? ? -? ? clip(? RES ?P RES ? ? ? + ? TSK ?P TSK ? ? ? )</formula><p>From the equation, one can see that ? TSK and ? RES must be proportional to each other and bounded, for the following reasons:</p><p>? If ? RES ? ? TSK (or ? TSK ? 0), the contribution of P TSK is erased. ? If ? RES ? ? TSK (or ? RES ? 0), pruning fails to converge because ?P TSK ? ? ? is always negative and greater than ?P RES ? ? ? , which prevents ? ? from being reduced. ? If ? RES ? 0 and ? TSK ? 0, the sum of two gradients has high variance, resulting in a smaller useful range of values contained within the clipping bounds.</p><p>Figure <ref type="figure" target="#fig_4">9</ref> tabulates the validation accuracy vs ? RES and ? TSK for MobileNet-v2 on VWW (50?50 input, matching the resource usage of the MicroNets VWW-2 baseline) and VGG-16 on CIFAR-10 (matching the resource usage of the DSA baseline). For completeness, cases with ? TSK = 0 are also included, corresponding to resource feedback-only pruning. The validation accuracy, however, has multiple sources of noise, such as initial weight values and quantisation errors. The results show the accuracy difference between the best and worst configurations of ?1.8% and ?2.1% for either model, respectively. The largest difference between adjacent cells (excluding ? TSK = 0), which can be seen as a proxy for noise, is ?0.65% and ?1.38% for either model, respectively.</p><p>Overall, the data exhibits a trend towards the largest possible values for ? TSK that do not quite prevent pruning from converging. However, a large fraction of the differences can be explained by noise, and regardless of the hyperparameter choice, the validation accuracy lies within a narrow range. This leads to the conclusion that the method has low sensitivity to hyperparameters ?.</p><p>How does one determine acceptable ranges of ? to try? Luckily, there is no need to run multiple training iterations until completion. In practice, we found that starting with ? RES and ? TSK set such that the first ?P RES ? ? ? and ?P TSK ? ? ? lie within the same order of magnitude as the gradient cap (0.025) is sufficient-even if the hyperparameters are not optimal, the result will still be close to the best possible configuration.</p><p>Figure <ref type="figure" target="#fig_5">10</ref> shows the relationship between the validation accuracy and the learning rate ? ? for the same pair of models. When ? ? is too low, pruning fails to converge in time. The results show a wide range of near-optimal values for ? ? , with the best-worst configuration difference of ?1% and ?2.5% for either model, respectively. Adjacent-cell value differences are within ?0.5% for both models. As with hyperparameters ?, the data shows a wide range of permissible values with a large fraction of accuracy differences explained by noise.</p><p>The results show a trend towards the lowest possible learning rates, which extend pruning throughout the entire training process. This results in the model reducing its capacity very gradually throughout training to meet the resource requirements, and ostensibly prevents the use of early termination (see Section 5.1). Therefore, a reduction in the training computation may come at the cost of classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B POST-PRUNING ARCHITECTURE ANALYSIS</head><p>To gain more insight into how differentiable pruning changes the network architecture and resource usage distribution across layers, two models are analysed: MobileNet-v2 trained on the VWW (50x50) dataset and VGG-16 trained on the CIFAR-10 dataset.</p><p>Figure <ref type="figure">11</ref> (page 19) shows layer sizes and memory bottlenecks of both architectures before and after pruning. The results show that: (a) pruning does target the memory bottleneck, reducing it by over 2? for both models; (b) in MobileNet-v2, layers on the main feature extraction path (output layers of the bottleneck blocks, as well as all layers in the bottleneck blocks without a residual connection) have been pruned more to reduce the memory usage; (c) in VGG-16, the pruning has undone the increment in channel dimension as depth increases, resulting in a model that is approximately uniformly wide across all layers.</p><p>Figure <ref type="figure" target="#fig_3">8</ref> shows per-layer resource usage before and after pruning: memory usage (working set size for each layer), the number of MACs and parameter size. The tails of convolutional architectures have a small spatial resolution and a high number of channels, which results in a large increase in size and greater model capacity. In both instances, pruning has effectively performed depth adjustment of the models by pruning more in the tail end of the model (most easily seen in the "Memory use" graph).</p><p>Through manual inspection of pruned network architectures, differentiable pruning is confirmed to target microcontroller resource bottlenecks, in a way that is consistent with removing extraneous complexity from the model.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Precise (solid) and imprecise (dashed) peak memory usage on MobileNet v2 and RES15 (ResNet-like) backbones. The three colours represent different pruning experiments which have different PMU objectives: the imprecise calculation, the precise calculation and no PMU objective. In all cases, the imprecise objective underapproximates the true peak memory usage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The change in training loss (left) and pruning masks, as a percentage of channels that were masked or unmasked from the last update (right), during early and late pruning-starting from the 1 st and 20 th epoch, respectively, on VGG-16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Per-layer latency and power draw of the two analysed models-MobileNet v2 for VWW and VGG-16 for CIFAR-10measured on Nucleo H743ZI2 and F767ZI development boards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Layer-wise resource usage before and after pruning for (left) MobileNet-v2 on VWW (50?50) and (right) VGG-16 on CIFAR-10.</figDesc><graphic url="image-2.png" coords="18,321.54,414.16,198.84,96.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Validation accuracy vs ? RES and ? TSK for (left) MobileNet-v2 on VWW (50?50) and (right) VGG-16 on CIFAR-10. Brighter is better; "N/A" refers to failure to converge before the network training completes.</figDesc><graphic url="image-1.png" coords="18,92.88,414.16,198.84,96.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Validation accuracy vs learning rate ? ? for (top) MobileNet-v2 on VWW (50?50) and (bottom) VGG-16 on CIFAR-10. Brighter is better; "N/A" refers to failure to converge in time for training end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>? All constant data, such as weights of a neural network Specifications of a high-end GPU server, a smartphone, a micro-computer and Nucleo development boards with ARM's Cortex M7 and M4 microcontrollers. Microcontrollers are the most resource-constrained yet the most power-efficient and cheapest platform.</figDesc><table><row><cell>Device</cell><cell cols="6">Cores Freq. (GHz) RAM (GB) Storage (GB) Power (W) Price ($)</cell></row><row><cell>NVIDIA A100 GPU</cell><cell>6912</cell><cell>1.4</cell><cell>40</cell><cell>-</cell><cell>400</cell><cell>12,500</cell></row><row><cell>Galaxy S22 Smartphone</cell><cell>8</cell><cell>&lt; 2.8</cell><cell>8</cell><cell>128</cell><cell>?3.6</cell><cell>750</cell></row><row><cell>Raspberry Pi 3B+</cell><cell>4</cell><cell>1.4</cell><cell>1</cell><cell>32-512</cell><cell>2.3</cell><cell>40</cell></row><row><cell>NUCLEO-F767ZI (M7)</cell><cell>1</cell><cell>0.216</cell><cell>0.000512</cell><cell>0.002</cell><cell>0.3</cell><cell>9</cell></row><row><cell>NUCLEO-F446RE (M4)</cell><cell>1</cell><cell>0.180</cell><cell>0.000128</cell><cell>0.000512</cell><cell>0.1</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>MCU-sized architectures discovered by differentiable network pruning vs others. The last column compares a baseline to our pruned model in the group (in light grey). "Time" is the relative total training time (search or pruning), estimated using MACs as a proxy for training time. 'INT8' represents a reimplemented quantised model, 'FP32' denotes a full precision model as reported by its authors ("unk." is unreported data). Differentiable pruning can discover MCU-sized models on par or better than the related work with negligible overhead (or faster).</figDesc><table><row><cell>Backbone</cell><cell>Model</cell><cell cols="2">Time Acc. (%)</cell><cell>Size</cell><cell>PMU</cell><cell>MACs</cell><cell>Difference</cell></row><row><cell>VWW (50x50) on</cell><cell>MobileNet v2 (unpruned) INT8</cell><cell>?1.00</cell><cell cols="5">84.93 2.32 M 76.2 KB 22.58 M ? MAC 8.9?</cell></row><row><cell>MobileNet v2</cell><cell>MicroNets VWW-2 INT8</cell><cell>?1.15</cell><cell>83.50</cell><cell cols="4">103 K 27.9 KB 3.383 M ? MAC 1.3?</cell></row><row><cell></cell><cell>ours #1 (matching VWW-2) INT8</cell><cell>?1.00</cell><cell>83.40</cell><cell>101 K</cell><cell cols="2">25 KB 2.528 M</cell></row><row><cell></cell><cell>ours #2 (matching VWW-2) INT8</cell><cell>?1.00</cell><cell>83.80</cell><cell cols="3">101 K 27.8 KB 3.342 M</cell></row><row><cell>VWW (50x50) on</cell><cell>EfficientNet-B0 (unpruned) INT8</cell><cell>?1.00</cell><cell>86.23</cell><cell>366 K</cell><cell cols="2">300 KB 9.468 M</cell><cell>? PMU 11?</cell></row><row><cell>EfficientNet-B0</cell><cell cols="2">ours (m/ VWW-2, early term.) INT8 ?0.49</cell><cell>83.34</cell><cell cols="3">95.0 K 27.8 KB 3.245 M</cell></row><row><cell>VWW (160x160)</cell><cell>MobileNet v2 (unpruned) INT8</cell><cell>?1.00</cell><cell cols="2">90.13 2.32 M</cell><cell cols="3">768 KB 164.8 M ? MAC 3.2?</cell></row><row><cell>on MobileNet v2</cell><cell>MicroNets VWW-1 INT8</cell><cell>?1.43</cell><cell>88.49</cell><cell>616 K</cell><cell cols="3">200 KB 71.58 M ? MAC 1.4?</cell></row><row><cell></cell><cell>ours (matching VWW-1) INT8</cell><cell>?1.00</cell><cell>89.05</cell><cell>606 K</cell><cell cols="2">198 KB 58.33 M</cell></row><row><cell>VWW (160x160)</cell><cell>EfficientNet-B0 (unpruned) INT8</cell><cell>?1.00</cell><cell cols="2">89.39 3.66 M</cell><cell cols="2">768 KB 187.6 M</cell><cell>? Size 6?</cell></row><row><cell>on EfficientNet-B0</cell><cell>ours (matching VWW-1) INT8</cell><cell>?1.00</cell><cell>88.77</cell><cell>601 K</cell><cell cols="2">199 KB 52.13 M</cell></row><row><cell>CIFAR10 on</cell><cell>VGG-16 (unpruned) INT8</cell><cell>?1.00</cell><cell cols="2">93.52 14.7 M</cell><cell cols="2">131 KB 313.3 M</cell><cell>? Size 80?</cell></row><row><cell>VGG-16</cell><cell>A/C (from 93.7% acc.) FP32</cell><cell>?1.00</cell><cell>88.78</cell><cell>311 K</cell><cell cols="3">unk. 22.38 M ? Acc. -2.4%</cell></row><row><cell></cell><cell>ours #1 (matching A/C) INT8</cell><cell>?1.00</cell><cell>91.15</cell><cell cols="3">309 K 52.2 KB 22.38 M</cell></row><row><cell></cell><cell>ours #2 (matching A/C) INT8</cell><cell>?1.00</cell><cell>90.38</cell><cell cols="3">184 K 62.5 KB 22.15 M</cell></row><row><cell></cell><cell>DSA (from 93.5% acc.) FP32</cell><cell>?1.00</cell><cell>90.16</cell><cell>unk.</cell><cell cols="2">unk. 15.35 M</cell><cell>-</cell></row><row><cell></cell><cell>ours (matching DSA) INT8</cell><cell>?1.00</cell><cell>90.36</cell><cell cols="3">834 K 34.8 KB 15.28 M</cell></row><row><cell>CIFAR10 on</cell><cell>DSA (from 94% acc.) FP32</cell><cell>?1.00</cell><cell>93.10</cell><cell>unk.</cell><cell cols="2">unk. 97.51 M</cell><cell>-</cell></row><row><cell>ResNet-18</cell><cell cols="2">ours (m/ DSA from 95% acc.) INT8 ?1.00</cell><cell cols="4">94.49 2.44 M 88.0 KB 97.20 M</cell></row><row><cell></cell><cell>ours (MCU-sized) INT8</cell><cell>?1.00</cell><cell>91.98</cell><cell cols="3">256 K 81.9 KB 29.58 M</cell></row><row><cell>KWS on</cell><cell>DS-CNN (MN-L, unpruned) INT8</cell><cell>?1.00</cell><cell>96.79</cell><cell>582 K</cell><cell cols="2">170 KB 74.27 M</cell><cell>? Size 14%</cell></row><row><cell>DS-CNN</cell><cell>MicroNets KWS-L INT8</cell><cell>?1.89</cell><cell>96.56</cell><cell>512 K</cell><cell cols="3">170 KB 65.75 M ? Speed 89%</cell></row><row><cell></cell><cell>ours (matching KWS-L) INT8</cell><cell>?1.00</cell><cell>96.56</cell><cell>511 K</cell><cell cols="2">159 KB 65.47 M</cell></row><row><cell></cell><cell cols="2">DS-CNN (MN-M, unpruned) INT8 ?1.00</cell><cell>96.36</cell><cell>420 K</cell><cell cols="3">170 KB 54.61 M ? MAC 3.6?</cell></row><row><cell></cell><cell>MicroNets KWS-M INT8</cell><cell>?1.29</cell><cell>95.73</cell><cell cols="4">117 K 86.1 KB 15.58 M ? Speed 29%</cell></row><row><cell></cell><cell>ours (matching KWS-M) INT8</cell><cell>?1.00</cell><cell>96.03</cell><cell cols="3">115 K 83.0 KB 15.56 M</cell></row><row><cell></cell><cell>MicroNets KWS-S INT8</cell><cell>?1.15</cell><cell>95.40</cell><cell cols="4">63.6 K 51.7 KB 8.351 M ? Speed 15%</cell></row><row><cell></cell><cell>ours (matching KWS-S) INT8</cell><cell>?1.00</cell><cell>95.75</cell><cell cols="3">61.9 K 49.8 KB 8.342 M</cell></row><row><cell>KWS on</cell><cell>RES-15 (unpruned) INT8</cell><cell>?1.00</cell><cell>96.48</cell><cell cols="4">240 K 66.1 KB 116.4 M ? MAC 7.8?</cell></row><row><cell>RES-8/15</cell><cell>ours (matching KWS-M) INT8</cell><cell>?1.00</cell><cell>95.42</cell><cell cols="3">32.4 K 27.4 KB 14.96 M</cell></row><row><cell></cell><cell>RES-8 (unpruned) INT8</cell><cell>?1.00</cell><cell>93.25</cell><cell cols="3">112 K 23.7 KB 4.162 M</cell><cell>? PMU 73%</cell></row><row><cell></cell><cell>ours (matching KWS-S) INT8</cell><cell>?1.00</cell><cell>92.78</cell><cell cols="3">63.2 K 13.7 KB 2.354 M</cell></row><row><cell>ImageNet on</cell><cell>MCUNet-S INT8</cell><cell>?1.25</cell><cell>59.78</cell><cell>748 K</cell><cell cols="3">333 KB 67.41 M ? Speed 25%</cell></row><row><cell>MCUNet</cell><cell>ours (matching MCUNet-S) INT8</cell><cell>?1.00</cell><cell>59.78</cell><cell>740 K</cell><cell cols="2">256 KB 66.59 M</cell></row><row><cell></cell><cell>MCUNet-M INT8</cell><cell>?1.26</cell><cell>60.98</cell><cell>756 K</cell><cell cols="3">341 KB 81.94 M ? Speed 26%</cell></row><row><cell></cell><cell>ours (matching MCUNet-M) INT8</cell><cell>?1.00</cell><cell>60.92</cell><cell>647 K</cell><cell cols="2">280 KB 81.66 M</cell></row><row><cell>ImageNet on</cell><cell>EtinyNet ?1.0 (unpruned) INT8</cell><cell>?1.00</cell><cell>60.18</cell><cell>979 K</cell><cell cols="3">201 KB 105.9 M ? MAC 1.7?</cell></row><row><cell>EtinyNet</cell><cell>EtinyNet ?0.75 INT8</cell><cell>?1.00</cell><cell>55.75</cell><cell>660 K</cell><cell cols="3">151 KB 63.35 M ? Acc. -0.8%</cell></row><row><cell></cell><cell>ours (matching ?0.75) INT8</cell><cell>?1.00</cell><cell>56.55</cell><cell>638 K</cell><cell cols="2">149 KB 63.32 M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>plots the training loss and the percentage of channels that have changed their mask value since the previous update for VGG-16 architecture. The pruning is shown in two modes: starting at the 1</figDesc><table><row><cell></cell><cell cols="2">3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Early pruning</cell></row><row><cell></cell><cell cols="2">2.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Late pruning</cell></row><row><cell>Training loss</cell><cell cols="2">1.0 1.5 2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Early pruning stops Late pruning stops</cell></row><row><cell></cell><cell cols="2">0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Early pruning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Late pruning</cell></row><row><cell cols="2">Mask change, %</cell><cell>2 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Early pruning stops Late pruning stops</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Epochs</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>st</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>epoch (early pruning) and 20</cell><cell>th epoch (late pruning). The</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>data shows that pruning eventually removes channels that</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>contribute to generalisation, resulting in a spike in training</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>loss (visible in late pruning yet smoothed over by rapid im-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>provement in early training stages). Early pruning has more</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>epochs remaining to recover from generalisation loss but</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>has relatively unstable pruning masks (slow convergence)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>due to less knowledge of which layers are more important</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>at the start (salience values are closer to uniform).</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>From the training loss graph, one can conclude that early</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pruning is a safe choice of mode for pruning. However, if</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows mean latency and energy usage for all four</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Full</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>3-channel colour inputs are used (does not violate the SRAM constraint, unlike in Banbury et al. (2021)); full "val" split is used for final evaluation.3 For MCUNet, the architectures instantiated at full channel conv. counts ("width"), excl. MCUNet-L (already at full width), and are pruned to the resource usage of models presented by the authors.4  The architectures use downsampled input, instead of downsampling in the first pooling layer, to minimise PMU.5 Standard execution strategy, like in the TensorFlow Lite Micro runtime, with support for accumulating into one of the summands of the 'Add' operator; no other non-standard system-level improvements (4-bit quantisation, custom kernel implementations, etc.) are considered.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Ordinarily, the task-specific loss only provides "negative" feedback against the reduction of ? L for salient layers. For this experiment, however, we allow only positive ?P TSK /?? L , as there is no resource usage loss feedback to reduce ? L .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was supported by <rs type="funder">Samsung AI</rs> and by the <rs type="funder">UK's Engineering and Physical Sciences Research Council (EP-SRC</rs>) with grants <rs type="grantNumber">EP/R018677/1</rs> (the <rs type="projectName">OPERA</rs> project), <rs type="grantNumber">EP/ M50659X/1</rs> and <rs type="grantNumber">EP/S001530/1</rs> (the <rs type="projectName">MOA</rs> project) and the <rs type="funder">European Research Council</rs> via the <rs type="projectName">REDIAL</rs> project (Grant Agreement ID: <rs type="grantNumber">805194</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_QcXeVWa">
					<idno type="grant-number">EP/R018677/1</idno>
					<orgName type="project" subtype="full">OPERA</orgName>
				</org>
				<org type="funding" xml:id="_3BaqNtB">
					<idno type="grant-number">EP/ M50659X/1</idno>
				</org>
				<org type="funded-project" xml:id="_cyM8ZKH">
					<idno type="grant-number">EP/S001530/1</idno>
					<orgName type="project" subtype="full">MOA</orgName>
				</org>
				<org type="funded-project" xml:id="_ynhAY7V">
					<idno type="grant-number">805194</idno>
					<orgName type="project" subtype="full">REDIAL</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Structured pruning of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Sung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08571</idno>
		<ptr target="https://os.mbed.com/platforms/" />
	</analytic>
	<monogr>
		<title level="m">ARM mbed. NUCLEO-F767ZI</title>
		<imprint>
			<date type="published" when="2015-02">2015. 2015. February 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>St-Nucleo</surname></persName>
		</author>
		<ptr target="https://os.mbed.com/platforms/" />
		<title level="m">ARM mbed. NUCLEO-H743ZI2, February 2022b</title>
		<imprint>
			<date type="published" when="2022-02-01">February 1, 2022</date>
			<biblScope unit="volume">767</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>St-Nucleo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-02-01">February 1, 2022</date>
			<biblScope unit="volume">743</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural network architectures for deploying TinyML applications on commodity microcontrollers</title>
		<author>
			<persName><forename type="first">C</forename><surname>Banbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Janapa Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><surname>Micronets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="517" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What is the state of neural network pruning?</title>
		<author>
			<persName><forename type="first">D</forename><surname>Blalock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gonzalez Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guttag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="129" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Teague</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Swiftnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08305</idno>
		<title level="m">Using graph propagation as meta-knowledge to search highly representative neural architectures</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rhodes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05721</idno>
		<title level="m">Visual wake words dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TensorFlow Lite Micro: Embedded machine learning on TinyML systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jeffries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kreeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nappier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Natraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Regev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="800" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mitigating bystander privacy concerns in egocentric activity recognition with deep learning and intentional image degradation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dimiccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mar?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BRP-NAS: Prediction-based NAS using GCNs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abdelfattah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10480" to="10490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SpArSe: Sparse architecture search for CNNs on resource-constrained microcontrollers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whatmough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4977" to="4989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vibroacoustic-based object recognition with smartphones</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Knocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast &amp; simple resourceconstrained structure learning of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Morphnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sparsity in deep learning: Pruning and growth for efficient inference and training in neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Peste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">241</biblScope>
			<biblScope unit="page" from="1" to="124" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deactive: scaling activity recognition with active deep learning</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al Haiz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integerarithmetic-only inference</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kligys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2704" to="2713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><surname>Cmsis-Nn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06601</idno>
		<title level="m">Efficient neural network kernels for ARM Cortex-M CPUs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SNIP: Single-shot network pruning based on connection sensitivity</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured pruning of neural networks with budget-aware regularization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lemaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9108" to="9116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural networks on microcontrollers: saving memory at inference via operator reordering. On-device Intelligence Workshop at the 3rd Machine Learning and Systems Conference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constrained neural architecture search for microcontrollers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Liberis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?</forename><surname>Dudziak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><surname>?nas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Machine Learning and Systems</title>
		<meeting>the 1st Workshop on Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="70" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MCUNet: Tiny deep learning on IoT devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11711" to="11722" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic model pruning with feedback</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dmitriev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 8th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">AutoCompress: An automatic DNN structured pruning framework for ultra-high compression rates</title>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4876" to="4883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CoopNet: Cooperative convolutional neural network for low-power MCUs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mocerino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Calimera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="414" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DSA: More efficient budgeted pruning via differentiable sparsity allocation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="592" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A flexible framework for multi-objective bayesian optimization using random scalarizations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Paria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kandasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="s">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="766" to="776" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for activity and context recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mascolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Marina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kawsar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Proceedings of the</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MobileNet V2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep residual learning for smallfootprint keyword spotting</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5484" to="5488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Korshunova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05787</idno>
		<title level="m">Faster gaze prediction with dense networks and fisher pruning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00389</idno>
		<title level="m">Single shot structured pruning before training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Picking winning tickets before training by preserving gradient flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations (ICLR)</title>
		<meeting>the 8th International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">EtinyNet: Extremely tiny network for TinyML</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multi-view deep learning for device-free human activity recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Deepmv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to recognize handwriting input with acoustic features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</title>
		<meeting>the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
