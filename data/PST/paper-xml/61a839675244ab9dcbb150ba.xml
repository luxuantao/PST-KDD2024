<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting the Transferability of Supervised Pretraining: an MLP Perspective</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
							<email>yizhouwang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Zhu</surname></persName>
							<email>zhufengx@mail.ustc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Bai</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<email>zhaorui@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donglian</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wanli.ouyang@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting the Transferability of Supervised Pretraining: an MLP Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The pretrain-finetune paradigm is a classical pipeline in visual learning. Recent progress on unsupervised pretraining methods shows superior transfer performance to their supervised counterparts. This paper revisits this phenomenon and sheds new light on understanding the transferability gap between unsupervised and supervised pretraining from a multilayer perceptron (MLP) perspective. While previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> focus on the effectiveness of MLP on unsupervised image classification where pretraining and evaluation are conducted on the same dataset, we reveal that the MLP projector is also the key factor to better transferability of unsupervised pretraining methods than supervised pretraining methods. Based on this observation, we attempt to close the transferability gap between supervised and unsupervised pretraining by adding an MLP projector before the classifier in supervised pretraining. Our analysis indicates that the MLP projector can help retain intra-class variation of visual features, decrease the feature distribution distance between pretraining and evaluation datasets, and reduce feature redundancy. Extensive experiments on public benchmarks demonstrate that the added MLP projector significantly boosts the transferability of supervised pretraining, e.g. +7.2% top-1 accuracy on the concept generalization task, +5.8% top-1 accuracy for linear evaluation on 12-domain classification tasks, and +0.8% AP on COCO object detection task, making supervised pretraining comparable or even better than unsupervised pretraining. Codes will be released upon acceptance. 1 In the paper, we specifically use the notation "SL" to indicate the conventional supervised learning with the cross-entropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">MLP Can Enhance Supervised Pretraining</head><p>4.1. SL-MLP: Adding an MLP Projector to SL Motivated by the empirical results in Sec. 3, an interesting question is whether the MLP projector can also pro-3 We do not directly compare Mocov1 with Mocov2 because Mocov2 has more augmentations and the different learning rate schedule.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While Supervised Learning with the cross-entropy loss 1  (SL) were the de facto pretraining paradigm in computer vision for a long period, recent unsupervised learning methods <ref type="bibr">[6-11, 16, 18, 19, 54, 56]</ref> show better transfer learn-ing performance on various visual tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b57">57]</ref>. This raised the question of why unsupervised pretraining surpasses supervised pretraining even though supervised pretraining uses annotations with rich semantic information.</p><p>Several works have attempted to explain the better transferability of unsupervised pretraining than supervised pretraining by the following two reasons: (1) Learning without semantic information in annotations <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b57">57]</ref>, which makes the backbone less-overfitted to semantic labels to preserve instance-specific information which may be useful in transfer tasks, and (2) Special design of the contrastive loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b57">57]</ref>, which helps the learned features to contain more low/mid-level information for effective transfer to downstream tasks. Starting from the perspective of supervision and loss design, these works provide intuitive explanations for better transferability.</p><p>In this paper, we shed new light on understanding transferability by considering the multilayer perception (MLP) projector. While previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> verified its effectiveness on the unsupervised image classification task: unsupervised training and evaluating the model on the same ImagNet-1K dataset, they did not explore its effectiveness on transfer tasks thoroughly and rigorously. It is not straightforward to extend the effectiveness of MLP on the unsupervised image classification task to downstream tasks if not supported by rigorous experiments or theoretical analysis, because the performance on the pretraining task is not always predictive of the performance on transfer tasks when there exists a large semantic gap <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47]</ref>. To our best knowledge, we are the first to identify the MLP projector as the core factor for the transferability with deep empirical and theoretical analysis. With this new viewpoint, we find that a simple yet effective method, adding an MLP projector, can promote the transferability of the conventional supervised pretraining methods with the cross-entropy loss (SL) to be comparable or even better than representative unsupervised pretraining methods. Specifically, we use the concept generalization task <ref type="bibr" target="#b41">[42]</ref> on ImageNet-1K, where the pretraining and the evaluation datasets have a large semantic distance, as a probe to ana-arXiv:2112.00496v1 [cs.CV] 1 Dec 2021 lyze the transferability of different models. Our experimental results and corresponding analysis indicate that the MLP projector in unsupervised pretraining methods is important for their better transferability. Motivated by this observation, we insert an MLP projector before the classifier in SL, forming SL-MLP. The added MLP can improve the transferability of supervised pretraining, making supervised pretraining comparable or even better than unsupervised pretraining. Experimental results on SL and SL-MLP show three interesting findings: 1) The added MLP preserves the intra-class variation on the pretraining dataset. 2) The added MLP decreases the feature distribution distance between the pretraining and the evaluation dataset; 3) The added MLP decreases the feature redundancy in the pretraining dataset. We also provide theoretical analysis on how the preserved intra-class variation and the decreased feature distribution distance improve the performance on the target dataset, by adding an MLP projector.</p><p>Extensive experimental results confirm that adding an MLP projector into the supervised pretraining method (SL) can consistently improve the transferability of the model on various downstream tasks. Specifically, on the concept generalization task <ref type="bibr" target="#b41">[42]</ref>, SL-MLP boosts the top-1 accuracy compared to SL (55.9%â†’63.1%) by +7.2%. It also achieves better performance (64.1%) than Byol (62.3%) by +1.8% on the 300-epochs pretraining setting. In classification tasks on 12 cross-domain datasets <ref type="bibr" target="#b23">[24]</ref>, SL-MLP improves SL by +5.8% accuracy on average. Moreover, SL-MLP shows better transferability than SL on COCO object detection <ref type="bibr" target="#b27">[28]</ref> by +0.8% AP. These improvements brought by the MLP projector can largely bridge the transferability gap between supervised and unsupervised pretraining as detailed in Sec. 5.2.</p><p>The main contributions of our paper are three-fold. (1) We reveal that the MLP projector is the main factor for the transferability gap between existing unsupervised and supervised learning methods. <ref type="bibr" target="#b1">(2)</ref> We empirically demonstrate that, by adding an MLP projector, supervised pretraining methods can have comparable or even better transferability than representative unsupervised pretraining methods.</p><p>(3) We theoretically prove that the MLP projector can improve transferability of pretrained models by preserving intra-class feature variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>MLP in unsupervised learning methods. Adding a multilayer perceptron (MLP) projector after the encoder was first introduced in SimCLR <ref type="bibr" target="#b7">[8]</ref> and followed by recent unsupervised learning frameworks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b53">54]</ref>. SimCLR claims that the MLP can reduce the loss of information caused by the contrastive loss, and various works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> have verified that the MLP projector can enhance the discriminative ability of unsupervised models on the unsupervised image classification task, where unsupervised training and evaluation are conducted on the same dataset, e.g., ImageNet-1K. However, the relation between the MLP and the transferability of unsupervised learning is under-explored. In this paper, we reveal that the MLP projector is also important for the desirable transferability of unsupervised learning. MLP in supervised learning methods. The typical supervised learning method only uses the cross-entropy loss and shows inferior performance on various transfer tasks than recent unsupervised learning methods. Inspired by <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b52">53]</ref>, recent researches <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> introduced the contrastive loss equipped with an MLP projector into supervised learning to improve its transferability. Nonetheless, those works ignored the ablation on the MLP projector and attributed the better transfer performance to the contrastive mechanism in the loss. In this paper, we propose that the MLP projector is important for the improved transferability of recent supervised learning methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>, and further provide some empirical and theoretical analysis to justify its importance. Transferability gap between supervised and unsupervised learning. Previous works attributed the superior transferability of unsupervised learning to lack of annotation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b57">57]</ref> or special design of the contrastive loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b57">57]</ref>. Different from both reasons, we explain the transferability gap by considering the architectural difference between the supervised and the unsupervised learning frameworks. From this perspective, we analyze the role of the MLP projector in both supervised and unsupervised learning methods, and are the first to identify its key importance to model transferability to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Transferability Analysis of the Unsupervised</head><p>and Supervised Pretraining Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Concept Generalization Task</head><p>We use the concept generalization task <ref type="bibr" target="#b41">[42]</ref> to analyze the transferability gap between the unsupervised and supervised pretraining methods. Data preparation. Sariyildiz et al. <ref type="bibr" target="#b41">[42]</ref> evaluated the transferability of methods when the pretraining and evaluation dataset have semantic distance. Their experimental results show that larger semantic distance will lead to more accuracy differences among different pretraining methods. Therefore, we enlarge the semantic gap between the pretraining and the evaluation dataset to help us compare different pretraining methods. Sariyildizet al. <ref type="bibr" target="#b41">[42]</ref> use the hierarchy in WordNet <ref type="bibr" target="#b30">[31]</ref> and divide ImageNet-21K <ref type="bibr" target="#b14">[15]</ref> into six class-exclusive datasets with different semantic distance -one for pretraining, and others for evaluation. Without loss of generality, we construct a smaller pretraining dataset (pre-D) and evaluation dataset (eval-D) based on ImageNet-1K <ref type="bibr" target="#b39">[40]</ref> to reduce the experimental burden. Pre-D contains 652 classes mostly of organisms, and eval-D contains the other 348 classes of instrumentality. Transferability evaluation. Following <ref type="bibr" target="#b41">[42]</ref>, to assess the transferability, we freeze all parameters in the pretrained backbone <ref type="foot" target="#foot_0">2</ref> , and finetune the classifier with the ImageNet-1K training samples in eval-D for reporting top-1 accuracy on ImageNet-1K validation samples in eval-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stage-wise Evaluation on Existing Methods</head><p>Motivated by works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b57">57]</ref> we make a more thorough stage-wise investigation of the conventional supervised pretraining method (SL) and the existing representative unsupervised pretraining methods (Mocov1, Mocov2, Byol) by evaluating the transferability of intermediate feature maps (Fig. <ref type="figure" target="#fig_0">1</ref>). After pretraining the model on pre-D, we freeze all model parameters and use the extracted intermediate feature maps of images in eval-D to finetune a stage-wise classifier for a stage-wise linear evaluation.</p><p>The evaluation results of these existing methods are depicted in Fig. <ref type="figure" target="#fig_1">2</ref> (underlined on the legend). Our stage-wise evaluation shows two new findings that have not been reported by existing works. First, on stage-wise evaluation from stage 1 to stage 4, SL is consistently higher than Byol, Mocov1, and Mocov2, which suggests that the semantic information in annotations can benefit the transferability of low/middle-level feature maps. Second, on stage-wise evaluation from stage 4 to stage 5, the performance of Byol and Mocov2 still increase while SL and Mocov1 have a transferability drop. By carefully inspecting these methods, we notice an architectural difference between SL, Mocov1, Mocov2, and Byol after stage 5: An MLP projector is inserted after stage 5 in Byol and Mocov2, which does not exist in SL and Mocov1. Such difference, together with the experimental results in Fig. <ref type="figure" target="#fig_1">2</ref>, leads to a new hypothesis that the MLP projector might be the core factor of the desirable transferability of unsupervised pretraining. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">MLP Improves the Transferability of Unsupervised Pretraining Methods</head><p>To confirm our hypothesis of the effectiveness on unsupervised pretraining methods, we ablate the MLP projectors on existing unsupervised methods,<ref type="foot" target="#foot_1">3</ref> using stage-wise evaluation. Specifically, we remove the MLP projector in Byol and Mocov2 as Byol w/o MLP and Mocov2 w/o MLP, and add an MLP projector in Mocov1 as Mocov1 w/ MLP. The stage-wise evaluation results of these ablations are summarized in Fig. <ref type="figure" target="#fig_1">2</ref>. We use solid lines for methods that have an MLP projector and dash lines for those that do not have.</p><p>These ablation results offer us two observations. First, when evaluating the layer4-pooled-features (depicted in the legend), unsupervised learning methods with an MLP projector achieve better transferability than their variants without the MLP projector, e.g., Byol, Mocov1 w/ MLP, Mo-cov2 achieve higher accuracy than Byol w/o MLP, Mocov1, and Mocov2 w/o MLP by +23.3%, +5.1% and +3.7%, respectively. Second, on stage-wise evaluation from stage 4 to stage 5, the MLP projector can help unsupervised learning methods without the MLP projector to avoid the transferability drop. These consistent improvements by adding an MLP projector empirically show that the MLP projector is important for the transferability of unsupervised pretraining. While there might exist some other non-linear structures that can boost the transferability, we only explore from an MLP perspective in this paper because of its simplicity and demonstrated effectiveness.   mote the transferability of supervised pretraining? We attempt to insert an MLP projector before the classifier on SL for better transferability. We denote this supervised pretraining method as SL-MLP (see Fig. <ref type="figure" target="#fig_2">3</ref> for their comparison). Specifically, SL-MLP includes a feature extractor f (â€¢), an MLP projector g(â€¢), and a classifier W. Given an input image x, the feature extractor outputs a feature f = f (x). For example, f (x) transforms an image x to a 2048 dimensional feature f when using the ResNet-50 backbone. The MLP projector maps f into a projection vector g = g(f ). Following Byol, the MLP projector consists of two fully connected layers, a batch normalization layer, and a ReLU layer, which can be mathematically formulated as</p><formula xml:id="formula_0">g(f ) = f c 2 (ReLU (BN (f c 1 (f )))) âˆˆ R Dg</formula><p>, where f c 1 and f c 2 are fully connected layers, the hidden feature dimension in the MLP projector is set to 4096, and D g is set to 256. Given the label denoted by y for image x, the objective function for SL-MLP can be formulated as</p><formula xml:id="formula_1">L(x) = CE(W â€¢ g(f (x)), y),<label>(1)</label></formula><p>where CE(â€¢) is the cross-entropy loss. Same as SL, only the learned feature extractor f (â€¢) is utilized in downstream transfer tasks after supervised pretraining.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Empirical Findings of MLP in SL-MLP</head><p>MLP projector avoids transferability drop at stage 5 in supervised pretraining. We conduct stage-wise evaluation as Sec. 3.2 again to see whether the transferability drop from stage 4 to stage 5 exists in SL-MLP. In Fig. <ref type="figure">6</ref>(a), the transferability of SL-MLP continuously increases from stage 1 to 5, avoiding a decrease at stage 5 as SL. Besides, we observe that the transferability of SL-MLP is higher than that of Byol from stage 1 to 4, indicating that annotations benefit the transferability of intermediate feature maps.</p><p>MLP projector enlarges the intra-class variation of features. According to <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b57">57]</ref>, features with large intra-class variation can preserve more instance discriminative information, which is beneficial for transfer learning. We reveal that adding an MLP projector also can enlarge the intraclass variation. We compare two supervised pretraining methods, i.e., SL, SupCon <ref type="bibr" target="#b24">[25]</ref>, and one unsupervised pretraining method, i.e., Byol, with their variants with/without MLP. Qualitatively, we visualize their features learned on pre-D by t-SNE in Fig. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we pretrain SL, SL-MLP, and Byol for 300 epochs.  <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we pretrain SL, SL-MLP, and Byol for 300 epochs. D in the representation space can benefit transfer learning.</p><p>Intuitively, the distribution distance between two sets of features is small when features are well mixed (visualization provided in Supplementary A). Therefore, we compare the mixtureness of features in pre-D and eval-D to indicate the feature distribution distance between SL and SL-MLP. Graphically, we visualize features from pre-D and eval-D by t-SNE in Fig. <ref type="figure" target="#fig_6">5</ref>. We observe that features from pre-D and eval-D are more mixed comparing SL and SL-MLP, indicating that MLP projector can mitigate the distribution distance between pre-D and eval-D. Quantitatively, we define Feature Mixtureness Î  in the feature space as</p><formula xml:id="formula_2">Î  = 1 âˆ’ 1 C C i=1 top eval k (i) k âˆ’ C eval C ,<label>(2)</label></formula><p>where C = 1000 is total number of classes in ImageNet-1K, C eval represents the number of classes in eval-D, and top eval k (i) represents the number of classes in eval-D found by top k neighbors search of any class i âˆˆ C. Since the percentage of finding a sample from eval-D in k nearest neighbors is C eval /C when pre-D and eval-D are uniformly mixed, Feature Mixtureness measures the similarity of the current and the uniformly mixed distribution between pre-D and eval-D in the feature space. We examine Feature Mixtureness of SL, SL-MLP, and Byol during different pretraining epochs in Fig. <ref type="figure" target="#fig_8">7(a)</ref>. Feature Mixtureness of SL gradu-ally decreases, which indicates that SL will enlarge the distribution difference between pre-D and eval-D. In contrast, SL-MLP and Byol show consistently high Feature Mixtureness, indicating that the MLP projector can reduce the distribution distance between pre-D and eval-D. We visualize the evolution of Feature Mixtureness in Supplementary B.2. MLP projector reduces feature redundancy. Inspired by <ref type="bibr" target="#b56">[56]</ref>, high channel redundancy limits the capability of feature expression in deep learning. Mathematically, we compute Pearson correlation coefficient among feature channels to evaluate feature redundancy R, i.e,</p><formula xml:id="formula_3">R = 1 d 2 d i=1 d j=1 |Ï(i, j)|, Ï(i, j) = N n=1 fn,i â€¢ fn,j N n=1 ||fn,i|| 2 N n=1 ||fn,j || 2<label>(3)</label></formula><p>where d = 2048 is the feature dimension, Ï(i, j) is Pearson correlation coefficient of feature channel i and j. As shown in Fig. <ref type="figure" target="#fig_8">7</ref>(b), SL-MLP has lower feature redundancy than SL, which indicates that the MLP projector can reduce feature redundancy. In Tab. 1, we further confirm that the MLP projector can reduce the feature redundancy and thus increase the accuracy on eval-D by ablating the MLP projector on various pretraining methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Theoretical Analysis for Empirical Findings</head><p>In this section, we provide a theoretical analysis to reveal that: 1) maximizing the discriminative ratio Ï†(I pre ) of</p><formula xml:id="formula_4">ğœ™(ğ¼ !"# ) ğœ™(ğ¼ #$%&amp; )</formula><p>Small semantic gap</p><formula xml:id="formula_5">ğ’• ğ¬ ğ’• ğ’</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large semantic gap</head><p>Figure <ref type="figure">9</ref>. Insights for transferability. Ï†(I pre ) and Ï†(I eval ) are the discriminative ratios (Eq. 4) on the pretraining and evalution datasets. Higher Ï†(I eval ) indicates better model transferability. Green and Blue line show the performance curve on the evaluation dataset with small and large semantic gap, respectively. a model on the pretraining dataset above a certain threshold will lead to a transferability descrease (shown by blue/green lines in Fig. <ref type="figure">9</ref>); 2) the threshold is smaller when the semantic gap between the pretraining and evalutaion dataset is larger (t l &lt; t s in Fig. <ref type="figure">9</ref>).</p><p>Mathematically, the discriminative ratio Ï†(I) on the dataset I can be defined by LDA <ref type="bibr" target="#b1">[2]</ref> as</p><formula xml:id="formula_6">Ï†(I) = Dinter(I)/Dintra(I),<label>(4)</label></formula><p>where</p><formula xml:id="formula_7">D inter (I) = 1 C(Câˆ’1) C j=1 C k=1,k =j ||Âµ(I j ) âˆ’ Âµ(I k )|| 2 is the inter-class distance, D intra (I) = 1 C C j=1 ( 1 |Ij | (xi,yi)âˆˆIj ||f i âˆ’ Âµ(I j )|| 2 )</formula><p>is the intra-class distance, and C is the number of classes. Âµ(I j ) = 1 Ij (xi,yi)âˆˆIj f i is the center of features in class I j , and f is the feature in Sec. 4.1. Higher discriminative ratio Ï† indicates higher classification accuracy. Inspired by <ref type="bibr" target="#b28">[29]</ref>, we analyze the relation between Ï†(I pre ) and Ï†(I eval ) in Theorem 1 (Supplementary C).</p><p>Theorem 1. Given Ï† 1 (I pre ) &lt; Ï† 2 (I pre ), Ï† 1 (I eval ) &gt; Ï† 2 (I eval ) when Ï† 1 (I pre ) &gt; t, where t is a threshold that is negatively related to the feature distribution distance.</p><p>Insights for the intra-class variation. Theorem 1 reveals that continuously minimizing the intra-class variation (maximizing the discriminative ratio) on the pretraining dataset will decrease the transferability of the model when the discriminative ratio Ï†(I pre ) is larger than t. It explains the observation in Fig. <ref type="figure">6</ref>(b) and Fig. <ref type="figure">6(c</ref>) that training with more than 210 epochs leads to better performance on pre-D, but a worse transferability on eval-D. This insight suggests that we should not make the intra-class variation on the pretraining dataset too small when designing the objective function or network architecture (adding an MLP projector is such a design). Insights for the relation between the feature distribution distance and threshold t. When the feature distribution distance between the pretraining and evaluation dataset is large, the threshold t is small, in which case it is easier to have the undesirable effect of increasing the discriminative ratio Ï†(I pre ) on pre-D leading to decreasing the discriminative ratio Ï†(I eval ) on eval-D (and thus the accuracy on the evaluation data). This insight suggests that we should maintain more intra-class variation on the pretraining dataset when transferring the model to a target dataset which has a larger semantic distance from the pretraining dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Setup</head><p>Datasets. For backbone analysis, we keep using the concept generalization setting described in Sec. 3.1. For generalization to other classification tasks, we follow the setup in <ref type="bibr" target="#b23">[24]</ref>, which pretrains the models on the whole ImageNet-1K dataset and then evaluates the transferability on 12 classification datasets <ref type="bibr">[12-14, 23, 27, 33-35, 37, 44, 46, 48]</ref> from different domains. Furthermore, the COCO <ref type="bibr" target="#b27">[28]</ref> dataset is used to evaluate the performance of SL-MLP pretrained by ImageNet-1K <ref type="bibr" target="#b39">[40]</ref> on object detection task. Details. For SL and SL-MLP pretraining, the cross-entropy is deployed as the loss function. The MLP projector deployed in SL-MLP is described in Sec. 4.1. Following <ref type="bibr" target="#b21">[22]</ref>, we use the SGD optimizer with a cosine decay learning rate of 0.4 to optimize SL and SL-MLP, and set the batch size to 1024. Byol is used as a representative method for comparisons in backbone analysis and object detection. Following <ref type="bibr" target="#b17">[18]</ref>, we use LARS optimizer <ref type="bibr" target="#b55">[55]</ref> with a cosine decay learning rate schedule and a warm-up of 10 epochs to pretrain the network. The initial learning rate is set to 4.8. We set the batch size to 4096 and the initial exponential moving average parameter Ï„ to 0.99. Except for the backbone analysis, we use ResNet50 as the default backbone. More detailed pretraining setups of different backbones and different methods are provided in Supplementary H.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experimental Results</head><p>Generalize to unseen concepts with diverse backbones. We verify the effectiveness of the added MLP projector on SL using concept generalization task with different backbones. Following evaluation method mentioned in Sec. 3.1, we train a linear classifier with the frozen backbone for 100 epochs, and report the top-1 accuracy on eval-D in Tab. 2. Firstly, SL-MLP obtains better performance than SL among different backbones. Specifically, with ResNet50, SL-MLP improves SL to 63.1 (+7.2%) when we pretrain the model by only 100 epochs, which bridges the performance gap between SL and Byol. In 300 epochs setting, SL has a transferability drop compared to 100 epochs setting (55.9%â†’54.4%), but the transferability of SL-MLP continue to increase (63.1%â†’64.1%). Secondly, SL-MLP (64.1%) performs better than Byol (62.3%) when both are trained by 300 epochs. Experimental results in Tab. 2 also confirm that SL-MLP can consistently improve the transferability of SL on various backbones, e.g. ResNet101 <ref type="bibr" target="#b21">[22]</ref>, MobileNetv2 <ref type="bibr" target="#b40">[41]</ref>, and EfficientNetb2 <ref type="bibr" target="#b42">[43]</ref>.</p><p>Generalize to other classification tasks. To evaluate if the added MLP can help SL to transfer better on crossdomain tasks, following <ref type="bibr" target="#b23">[24]</ref>, we pretrain the model on ImageNet-1K, and evaluate the transferability on 12 classification datasets from different domains. As illustrated in Tab. 4, supervised pretraining methods with the MLP projector, i.e., SL-MLP and SupCon, outperform their no MLP counterparts, i.e., SL and SupCon w/o MLP on linear evaluation, by 5.79%, 13.71% on the averaged Top-1 accuracy, respectively. Consistent results can be observed on finetuning and few-shot learning settings. Besides, by comparing SupCon, SL-MLP and SupCon w/o MLP, SL, we conclude that the MLP projector instead of the contrastive loss plays the key role in increasing transferability. Our conclusion contrasts with previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b57">57]</ref> because they ignore the MLP projector before the contrastive loss.</p><p>Generalize to object detection. We assess the transferability improvement by the MLP projector on COCO object detection task. We follow the settings in <ref type="bibr" target="#b18">[19]</ref> to finetune the whole network with 1Ã— schedule. In Tab. We analyze the influence of different components on discriminative ratio Ï† on pre-D, Feature Mixtureness Î , feature redundancy R qualitatively and quantitatively in Supplementary D.3. Besides, we also observe an interesting phenomenon on Tab. 5(e) that only inserting a lightweight BN-ReLU also achieves good transfer performance. As this is not our main focus, we will investigate it in future works. Epochs and layers. Fig. <ref type="figure" target="#fig_9">10</ref>(a) shows that adding one MLP projector achieves the optimal transferability. In addition, larger pretraining epochs benefit the transferability of SL-MLP when one MLP projector is added, but it has little influence when more MLP projectors are used. SL-MLP is less sensitive to batch size. Most unsupervised methods depend on big mini-batches to train a representation with strong transferability. To investigate the sensitivity of SL-MLP to batch size, we do experiments with batch size from 256 to 4096 for Byol and to 1024 for SL-MLP over 300 epochs. As shown in Fig. <ref type="figure" target="#fig_9">10</ref>(b), the transferability of Byol drops when the batch size decreases. In contrast, the transferability of SL-MLP retains when batch size changes. SL-MLP is less sensitive to augmentation. Unsupervised methods benefit from a broader space of colors and more intensive augmentations during pretraining, which always lead to undesirable degradation when some augmentations are missing. Supervised models trained merely with hori-   zontal flipping may perform well <ref type="bibr" target="#b57">[57]</ref>. We set Byol's augmentations as a baseline setting for both SL-MLP and Byol.</p><p>We then compare the robustness on augmentation between SL-MLP and Byol by removing augmentation step by step. Experiments of SL-MLP and Byol are all constructed on their default condition with only augmentations changed.</p><p>The results are illustrated on Fig. <ref type="figure" target="#fig_9">10(c</ref>). We find that SL-MLP inherits the robustness of SL and shows a little accuracy drop with simple augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Conclusions</head><p>In this paper, we study the transferability gap between supervised and unsupervised pretraining. Based on empirical results, we identify that the MLP projector is a key factor for the good transferability of unsupervised pretraining methods. By adding an MLP projector into supervised pretraining methods, we close the gap between supervised and unsupervised pretraining and even make supervised pretraining better. Our finding is confirmed with extensive experiments on diverse backbone networks and various downstream tasks, including the concept generalization tasks, cross-domain image classifications, and objection detection. While the MLP is a simple design for better transferability, there might exist some straightforward designs on the objective function, which we leave for future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Visualization of Feature Mixtureness</head><p>We provide an intuitive understanding of the relation between Feature Mixtureness and the feature distribution distance by manually generating two sets of features with different distribution distance. We use red and blue to represent class centers from pre-D and eval-D, respectively. The visualization results are illustrated in Fig. <ref type="figure" target="#fig_10">11</ref>. From (a) to (c), when the distribution distance between pre-D and eval-D increases, Feature Mixtureness decreases accordingly. When we fix the variance of features in pre-D and gradually enlarge the variance of features in eval-D (from (d) to (f)), Feature Mixtureness will decrease as well. Based on the observations above, we conclude that our Feature Mixtureness can empirically measure the feature distribution distance between pre-D and eval-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization of Feature Distribution during Pretraining</head><p>In this section, we provide an illustration to establish an intuition about how intra-class variation and Feature Mixtureness evolve during different pretraining epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Intra-class Variation on pre-D</head><p>We visualize the feature distribution using samples from 10 randomly selected classes in pre-D in Fig. <ref type="figure" target="#fig_1">12</ref> to illustrate the evaluation results of the intra-class variation on pre-D. Different colors represent different classes. In SL, the intra-class variation will continuously decrease to a small value with more training epochs. In contrast, the intra-class variance of SL-MLP and Byol retains even though we pretrain the networks at large pretraining epochs. This visualization graphically validates that the MLP projector can enlarge the intra-class variation of features in pre-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Feature Mixtureness between pre-D and eval-D</head><p>We randomly select features from 5 classes in pre-D and 5 classes in eval-D, and then visualize them by t-SNE in Fig. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Theoretical Analysis of Theory 1 C.1. Proof of Theory 1</head><p>Proof. Denote the pretrained feature extractor with the parameters Î¸ as f (â€¢; Î¸). The softmax function is built upon the feature representation of the backbone f i = f (x i ; Î¸) âˆˆ R D , where x i is an image, and D is the dimension of features. We compute the class center Âµ(I j ) for class j as the mean of the feature embeddings as</p><formula xml:id="formula_8">Âµ(I j ) = 1 I j (xi,yi)âˆˆIj f i ,<label>(5)</label></formula><p>where I j denotes the images in the j-th class. Then we define the inter-class distance D inter (I), and intra-class distance D intra (I) on a datatset with C classes as</p><formula xml:id="formula_9">D inter (I) = 1 C(C âˆ’ 1) C j=1 C k=1,k =j ||Âµ(I j ) âˆ’ Âµ(I k )|| 2 , (<label>6</label></formula><formula xml:id="formula_10">)</formula><formula xml:id="formula_11">D intra (I) = 1 C C j=1 ( 1 |I j | (xi,yi)âˆˆIj ||f i âˆ’ Âµ(I j )|| 2 ).<label>(7)</label></formula><p>Substituting Eq. 5 into Eq. 6 and Eq. 7, we have</p><formula xml:id="formula_12">D inter (I) = 1 C(C âˆ’ 1) C j=1 C k=1,k =j ï£« ï£­ 1 2|I j ||I k | (xi,yi)âˆˆIj (x l ,y l )âˆˆI k ||f i âˆ’ f l || 2 ï£¶ ï£¸ ,<label>(8)</label></formula><formula xml:id="formula_13">D intra (I) = 1 C C j=1 ï£« ï£­ 1 2|I j | 2 (xi,yi)âˆˆIj (x l ,y l )âˆˆIj ||f i âˆ’ f l || 2 ï£¶ ï£¸ .<label>(9)</label></formula><p>Taking expectation to Eq. 8 and Eq. 9, for any pair of data (x i , y i ), (x l , y l ) âˆˆ I, we have</p><formula xml:id="formula_14">E(||f i âˆ’ f l || 2 ) = 2D intra (I), y i = y l 2D inter (I), y i = y l . (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>For ease of analysis, we denote I pre , I eval as pre-D and eval-D, respectively. For any pair of data (x i , y i ), (x l , y l ) âˆˆ I eval in eval-D in the same class, i.e., y i = y l , we have</p><formula xml:id="formula_16">D intra (I eval ) = 1 2 E ||f i âˆ’ f l || 2 = 1 2 E [P (y i = y l )2D intra (I pre ) + P (y i = y l )2D inter (I pre )] = P D intra (I pre ) + (1 âˆ’ P )D inter (I pre ),<label>(11)</label></formula><p>where y i is the label of an image x i assigned by the classifier trained on pre-D, and f = f (x , Î¸). Here, P represents the possibility that a pair of images in eval-D that belong to the same class is classified into the same classes in pre-D. We denote Ïˆ(Ï† âˆ’1 (I pre )) = D inter (I eval )/D inter (I pre ) as the ratio of the model's inter-class distance on eval-D and the model's inter-class distance on pre-D. When the model is optimized on pre-D, its discriminative ratio on pre-D Ï†(I pre ) becomes larger with the increase of D inter (I pre ) and the decease of D intra (I pre ). In most cases, D inter (I eval )/D inter (I pre ) is a monotonic decreasing function of Ï†(I pre ), and is a monotonic increasing function of Ï† âˆ’1 (I pre ), which has been empirically proven by <ref type="bibr" target="#b28">[29]</ref>. Mathematically, it can be formulated as</p><formula xml:id="formula_17">Ïˆ(Ï† âˆ’1 2 (I pre )) &gt; Ïˆ(Ï† âˆ’1 1 (I pre )), if Ï† âˆ’1 2 (I pre ) &gt; Ï† âˆ’1 1 (I pre ).<label>(12)</label></formula><p>By substituting D intra (I eval ) = P D intra (I pre ) + (1 âˆ’ P )D inter (I pre ) (Eq. 11) into the discriminative ratio inequality Ï† 2 (I eval ) &lt; Ï† 1 (I eval ) given Ï† 2 (I pre ) &gt; Ï† 1 (I pre ), we have</p><formula xml:id="formula_18">Ï† 2 (I eval ) &lt; Ï† 1 (I eval ) (13) â‡â‡’ D 2 inter (I eval ) D 2 intra (I eval ) &lt; D 1 inter (I eval ) D 1 intra (I eval )<label>(14)</label></formula><formula xml:id="formula_19">â‡â‡’ D 2 inter (I eval ) P D 2 intra (I pre ) + (1 âˆ’ P )D 2 inter (I pre ) &lt; D 1 inter (I eval ) P D 1 intra (I pre ) + (1 âˆ’ P )D 1 inter (I pre ) ,<label>(15)</label></formula><formula xml:id="formula_20">â‡â‡’ P &lt; D 1 inter (I eval ) D 1 inter (I pre ) âˆ’ D 2 inter (I eval ) D 2 inter (I pre ) D 1 inter (I eval ) D 1 inter (I pre ) â€¢ 1 âˆ’ D 2 intra (I pre ) D 2 inter (I pre ) âˆ’ D 2 inter (I eval ) D 2 inter (I pre ) â€¢ 1 âˆ’ D 1 intra (I pre ) D 1 inter (I pre ) ,<label>(16)</label></formula><formula xml:id="formula_21">â‡â‡’ P &lt; Ïˆ(Ï† âˆ’1 1 (I pre )) âˆ’ Ïˆ(Ï† âˆ’1 2 (I pre )) Ïˆ(Ï† âˆ’1 1 (I pre )) 1 âˆ’ Ï† âˆ’1 2 (I pre ) âˆ’ Ïˆ(Ï† âˆ’1 2 (I pre )) 1 âˆ’ Ï† âˆ’1 1 (I pre ) , (<label>17</label></formula><formula xml:id="formula_22">) â‡â‡’ P &lt; 1 1 âˆ’ Ï† âˆ’1 1 (I pre ) + Ï† âˆ’1 2 (I pre )âˆ’Ï† âˆ’1 1 (I pre ) Ïˆ(Ï† âˆ’1 2 (I pre ))âˆ’Ïˆ(Ï† âˆ’1 1 (I pre )) Ïˆ(Ï† âˆ’1 1 (I pre )) , (<label>18</label></formula><formula xml:id="formula_23">) â‡â‡’ P &lt; 1 1 âˆ’ Ï† âˆ’1 1 (I pre ) + rÏˆ(Ï† âˆ’1 1 (I pre )) ,<label>(19)</label></formula><formula xml:id="formula_24">â‡â‡’ rÏˆ(Ï† âˆ’1 1 (I pre )) âˆ’ Ï† âˆ’1 1 (I pre ) &lt; P âˆ’1 âˆ’ 1,<label>(20)</label></formula><formula xml:id="formula_25">â‡â‡’ dÏ† âˆ’1 1 (I pre ) dÏˆ(Ï† âˆ’1 1 (I pre )) Ïˆ(Ï† âˆ’1 1 (I pre )) âˆ’ Ï† âˆ’1 1 (I pre ) &lt; P âˆ’1 âˆ’1,<label>(21)</label></formula><formula xml:id="formula_26">â‡â‡’ dÏ† âˆ’1 (I pre ) P âˆ’1 âˆ’ 1 + Ï† âˆ’1 (I pre ) &lt; 1 Ïˆ(Ï† âˆ’1 (I pre )) dÏˆ(Ï† âˆ’1 (I pre )),<label>(22)</label></formula><p>where</p><formula xml:id="formula_27">r = Ï† âˆ’1 2 (I pre ) âˆ’ Ï† âˆ’1 1 (I pre ) Ïˆ(Ï† âˆ’1 2 (I pre )) âˆ’ Ïˆ(Ï† âˆ’1 1 (I pre ))<label>(23)</label></formula><formula xml:id="formula_28">â‰ˆ dÏ† âˆ’1 (I pre ) dÏˆ(Ï† âˆ’1 (I pre )) , when Ï† âˆ’1 2 (I pre ) âˆ’ Ï† âˆ’1 1 (I pre ) â†’ 0.<label>(24)</label></formula><p>We take integration of Eq. 22 as</p><formula xml:id="formula_29">â‡â‡’ Ï† âˆ’1 (I pre ) 0 dÏ† âˆ’1 (I pre ) P âˆ’1 âˆ’ 1 + Ï† âˆ’1 (I pre ) &lt; Ïˆ(Ï† âˆ’1 (I pre )) Ïˆ(0) 1 Ïˆ(Ï† âˆ’1 (I pre )) dÏˆ(Ï† âˆ’1 (I pre )),<label>(25)</label></formula><formula xml:id="formula_30">â‡â‡’ ln Ï† âˆ’1 (I pre ) + P âˆ’1 âˆ’ 1 &lt; ln Ïˆ(Ï† âˆ’1 (I pre ))) + ln P âˆ’1 âˆ’ 1 Ïˆ(0) ,<label>(26)</label></formula><formula xml:id="formula_31">â‡â‡’ Ï† âˆ’1 (I pre ) + P âˆ’1 âˆ’ 1 &lt; Ïˆ(Ï† âˆ’1 (I pre )) P âˆ’1 âˆ’ 1 Ïˆ(0) ,<label>(27)</label></formula><formula xml:id="formula_32">â‡â‡’ Ï† âˆ’1 (I pre ) &lt; 1 âˆ’ P âˆ’1 + Ïˆ(Ï† âˆ’1 (I pre )) P âˆ’1 âˆ’ 1 Ïˆ(0) ,<label>(28)</label></formula><formula xml:id="formula_33">â‡â‡’ Ï† âˆ’1 (I pre ) &lt; ( Ïˆ(Ï† âˆ’1 (I pre )) Ïˆ(0) âˆ’ 1)(P âˆ’1 âˆ’ 1)<label>(29)</label></formula><p>â‡â‡’ Ï†(I pre ) &gt; t</p><p>where the threshold t is defined as</p><formula xml:id="formula_35">t = ( Ïˆ(Ï† âˆ’1 (I pre )) Ïˆ(0) âˆ’ 1)(P âˆ’1 âˆ’ 1) âˆ’1 .<label>(31)</label></formula><p>According to Formulation 12, Ïˆ(Ï† âˆ’1 (I pre )) &gt; Ïˆ(0) because Ï† âˆ’1 (I pre ) &gt; 0. Therefore, Ïˆ(Ï† âˆ’1 (I pre ))</p><formula xml:id="formula_36">Ïˆ(0)</formula><p>âˆ’ 1 &gt; 0, which means that increasing P will lead to increasing the threshold t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Analysis of P</head><p>In the following, we explain how P in Equation 11 can be theoretically computed, and how P negatively relates to the feature distribution distance briefly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Computational Method of P</head><p>Given a fixed backbone pretrained f (â€¢; Î¸) on pre-D, we denote the classifier trained by pre-D as W = (w 1 , w 2 , ..., w C pre ). The possibility of an image x of the class j in eval-D classified by the classifier W into the class k in pre-D can be defined as</p><formula xml:id="formula_37">P jk = 1 |I eval j | (xi,yi)âˆˆI eval j exp(w k â€¢ f (x; Î¸)) C pre k =1 exp(w k â€¢ f (x; Î¸)) ,<label>(32)</label></formula><p>where |I eval j | denotes the number of images in the j-th class in eval-D. Then the probability of a pair of samples in the same class j in eval-D classified into the same class in eval-D is</p><formula xml:id="formula_38">P j = C pre k=1 P 2 jk .<label>(33)</label></formula><p>The average probability of P j is</p><formula xml:id="formula_39">P = 1 C eval C eval j=1 P j .<label>(34)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 P is Negatively Related to the Feature Distribution Distance</head><p>In this part, we only use two extreme cases to briefly analyze the relation between P and the feature distribution distance. Specifically, we first deduce the upper bound and the lower bound of P . We find that the upper bound is reached when the feature distribution distance between pre-D and eval-D is extremely small, and the lower bound is reached when the feature distribution distance between pre-D and eval-D is extremely large, which indicates P is negatively related to the feature distribution distance.</p><p>For the upper bound of P ,</p><formula xml:id="formula_40">P = 1 C eval C eval j=1 P j (35) = 1 C eval C eval j=1 C pre k=1 P 2 jk (36) â‰¤ 1 C eval C eval j=1 C pre k=1 P jk 2 (37) = 1 C eval C eval j=1 1 (38) = 1,<label>(39)</label></formula><p>where Inequality 37 is derived by Cauchy Schwarz Inequality <ref type="bibr" target="#b49">[50]</ref>, and if and only if P jk = 1 and P jk = 0 for âˆ€k = k, P reaches its upper bound 1.</p><p>For the lower bound of P ,</p><formula xml:id="formula_41">P = 1 C eval C eval j=1 P j (40) = 1 C eval C eval j=1 C pre k=1 P 2 jk (41) â‰¥ 1 C eval C eval j=1 1 C pre C pre k=1 P jk 2 (42) = 1 C eval C eval j=1 1 C pre (43) = 1 C pre ,<label>(44)</label></formula><p>where Inequality 42 is derived by Fundamental Inequality <ref type="bibr" target="#b2">[3]</ref>, and if and only if P jk = 1 C pre for âˆ€k âˆˆ [1, C pre ], P reaches its lower bound 1  C pre . Analysis on Small Feature Distribution Distance between pre-D and eval-D. When pre-D and eval-D have small feature distribution distance, a pair of two images (x m , y m ) and (x n , y n ) belong to the same class j in eval-D, i.e., y m = y n will be classified to the same class k in pre-D when classified by W with high confidence. That is, only P jk will have high confidence close to 1 and P jk , âˆ€k = k will be close to 0, which is similar to the condition when P reaches its upper bound.</p><p>Analysis on Large Feature Distribution Distance between pre-D and eval-D. When pre-D and eval-D have large feature distribution distance, a pair of two images (x m , y m ) and (x n , y n ) belong to the same class in eval-D, i.e., y m = y n will be randomly classified to the classes in pre-D using W. Mathematically, P jk â‰ˆ 1 C pre , which is similar to the condition when P reaches its lower bound.</p><p>Based on the analysis above, we can conclude that P is negatively related to feature distribution distance, and larger P often means less feature distribution distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. MLP components</head><p>In this section, we provide the detailed analysis about how each component of the MLP projector influences the intra-class variation (represented by discriminative ratio Ï† pre ) on pre-D, Feature Mixtureness Î  between pre-D and eval-D, and feature redundancy R. Based on SL which does not include MLP, we ablate the structure of the MLP projector by adding the input fully connected layer, the output fully connected layer, the batch normalization layer and the ReLU layer incrementally. The input fully connected layer and the output fully connected layer are both set to have hidden units of 2048 and output dimensions of 2048 to keep same output feature dimensions as SL. All experiments are pretrained over 100 epochs. Testing results of the discriminative ratio on pre-D, Feature Mixtureness Î  and feature redundancy R are illustrated in Tab. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Visualization of intra-class variation</head><p>We randomly select features from 10 classes in pre-D and visualize their intra-class variation in Fig. <ref type="figure" target="#fig_13">14</ref>. Different colors denote features from different classes. We specify the components in the MLP projector below each visualization image. Comparing (a) with (b), we can see that adding a fully connected layer can slightly enlarge intra-class variation, which indicates that linear transformation helps transferability marginally. Instead, comparing (a-b) with (c-e), we can observe that the batch normalization layer and the ReLU layer are important components in the MLP projector, which can significantly enlarge the intra-class variation in the feature space of pre-D. In general, comparing SL-MLP with (a-e), we can conclude that all components in MLP projector help enlarge the intra-class variation of features in pre-D while the batch normalization layer and the ReLU layer play the most important roles. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Visualization of Feature Mixtureness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Quantitative Analyse of MLP components</head><p>With the discriminative ratio Ï† pre , Feature Mixtureness Î  and feature redundancy R defined in Sec. 4.2, we quantitatively examine the effect of different components in the MLP projector. The results are presented in Tab. 6. Firstly, the fully connected layer has little influence on three metrics. Comparing (a) and (b), when adding a fully connected layer, the model shows slight improvement on Feature Mixtureness and feature redundancy, and slight decrease of the discriminative   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Generalization Task with Small Semantic Gap</head><p>Following <ref type="bibr" target="#b41">[42]</ref>, to investigate how semantic difference between pre-D and eval-D can influence the transfer results on Concept generalization task, we randomly choose 652 classes as pre-D and 348 classes as eval-D from ImageNet-1K to establish a benchmark where pre-D and eval-D have small semantic gap. We denote the setting where pre-D and eval-D are constructed as Sec. 3.1 as large semantic gap setting (dubbed as semantic), and denote the setting where pre-D and eval-D are randomly selected as small semantic gap setting (dubbed as random). . Linear evaluation accuracy on eval-D with small semantic gap. Following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we pretrain SL, SL-MLP, and Byol on randomly chosen pre-D for 300 epochs. Compare the transfer performance on 300 epochs, SL shows a comparable transferability with Byol and SL-MLP when the semantic gap between pre-D and eval-D is small. In addition, unlike what we observe in Fig. <ref type="figure">6</ref>(b), no performance drop during the last epochs appear. SL-MLP has a similar performance with SL from 60 to 240 epochs while has a consistently better performance on 300 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Visualization of Feature Mixtureness</head><p>We visualize features from pre-D and eval-D in small semantic gap setting and large semantic gap setting in Fig. <ref type="figure" target="#fig_0">16</ref>. Specifically, SL (random), SL-MLP (random), and Byol (random) denote feature visualization of SL, SL-MLP, and Byol pretraining on the benchmark where pre-D and eval-D are randomly chosen. SL (semantic), SL-MLP (semantic), and Byol (semantic) denote feature visualization of SL, SL-MLP, and Byol pretraining on the benchmark where pre-D and eval-D are split according to semantic difference in WordNet, which is the same as Sec. 4.2. Our findings are two-fold. First, comparing with (a), (c), (e), pre-D features in (b), (d), (f) have large Feature Mixtureness, which indicates semantic difference influences the feature distribution distance between pre-D and eval-D in the feature space. Second, comparing (b) with (d), we find that Feature Mixtureness between pre-D and eval-D is enlarged by adding an MLP projector, which indicates that the MLP projector can significantly mitigate the feature distribution distance between pre-D and eval-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Quantitative Results</head><p>We first pretrain all the models on pre-D over 300 epochs, then examine linear evaluation results on eval-D. Our findings are three-fold. Firstly, compare the top-1 accuracy on 300 epochs, SL shows a comparable transferability with Byol and SL-MLP when the semantic gap between pre-D and eval-D is small. Second, unlike what we observe in Fig. <ref type="figure">6</ref>(b), no performance drop during the last epochs appears, which indicates that the intra-class variation of SL is not above the threshold (defined in Sec. 4.3) when pre-D and eval-D have a small semantic gap. Third, SL-MLP has a similar performance with SL from 60 to 240 epochs while has a consistently better performance on 300 epochs, which verifies the effectiveness of the added MLP projector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Replacing Softmax with Cosine-Softmax</head><p>In order to prove that our findings can be compatible with different loss functions, we replace the softmax cross-entropy loss with the cosine-softmax cross-entropy loss in the pretraining stage. Specifically, the cosine-softmax cross-entropy loss is defined as </p><formula xml:id="formula_42">L cos (x i , y i ) = âˆ’ log exp(Î² â€¢ cos(w yi , f (x i ))) C i=j exp(Î² â€¢ cos(w j , f (x i ))) ,<label>(45)</label></formula><formula xml:id="formula_43">(x i )))) ,<label>(46)</label></formula><p>where w i is the i-th class prototype, Î² = 30 is the scale factor. We train for 100 epochs with a warm-up of 10 epochs and cosine decay learning schedule using the SGD optimizer. The base learning rate is set to 0.4. Weight decay of 10 âˆ’4 is applied during pretraining. We report the top-1 accuracy on eval-D in Tab. 7. The results illustrate that when the model pretrained by cosine-softmax cross-entropy loss, adding an MLP projector can also facilitate transferability of supervised pretraining methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Visualize Convolution Channels by Optimization</head><p>According to <ref type="bibr" target="#b57">[57]</ref> and <ref type="bibr" target="#b0">[1]</ref>, transfer performance is largely unaffected by the high-level semantic content of the pretraining data. To investigate that whether adding an MLP projector can influence what the convolution channels can learn. By using the method proposed in <ref type="bibr" target="#b35">[36]</ref>, we visualize the maximum response of convolution channels in layer 4 of ResNet50 (seen in Fig. <ref type="figure" target="#fig_0">1</ref>) pretrained with methods without-MLP, i.e. SL, Mocov1, and Byol w/o MLP, and methods with-MLP, i.e. SL-MLP, Mocov1 w/ MLP, and Byol. Specifically, given a backbone with fixed parameters Î¸ as f (â€¢; Î¸), we denote the parameters before the convolution channel j as f (â€¢; Î¸ j ), we optimize the most representative sample x i of the convolution channel j by maximizing the output logits f (x; Î¸ j ), i.e., x i = argmax x (f (x; Î¸ j )), where x is optimized from a random initialized image x 0 .</p><p>As shown in Fig. <ref type="figure" target="#fig_0">18</ref>, methods without-MLP (Mocov1, Byol w/o MLP, SL) learn more knowledge about animals from pre-D, highlighted by red rectangles. This is due to that we select classes of organisms to construct pre-D. Instead, we find that methods with-MLP (Mocov1 w/ MLP, Byol, SL-MLP) learn more texture information. According to <ref type="bibr" target="#b57">[57]</ref>, high-level semantic information is less critical to transfer learning, which explains effectiveness of the MLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Detailed Training Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1. Pretraining</head><p>For SL and SL-MLP, we use the SGD optimizer with a cosine decay learning rate of 0.4 with Nesterov momentum of 0.9 to optimize all the networks and set the batch size to 1024. A 3 epochs warm-up with a starting learninig rate of 0.1 is applied. The weight decay of ResNets, MobileNetv2, EfficientNetb2 is set to 1 Ã— 10 âˆ’4 , 5 Ã— 10 âˆ’5 , 1 Ã— 10 âˆ’5 , respectively. Data augmentations include random-crop (224x224), color-jitter, and random horizontal flip. For SupCon and SupCon w/o MLP pretraining, we set the temperature parameter to Ï„ = 0.07, and queue size to 65596. We use random-crop (224x224), color-jitter, random gray-scale, Gaussian blur, random horizontal flip for pretraining data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Concept Generalization Task</head><p>In unseen generalization task, we divide ImageNet-1K into two class-exclusive datasets following the hierarchical structure built in WordNet <ref type="bibr" target="#b30">[31]</ref> -one for pretraining (denoted as pre-D) and the other for evaluation (denoted as eval-D). Eval-D has 348 classes of instrumentality, and pre-D contains 652 classes mostly of organisms. All the networks are pretrained on pre-D, and then examined by linear evaluation protocal on eval-D. As in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">38]</ref>, we train a linear classifier with the frozen backbone for 100 epochs. During evaluation, images are resized to 256 pixels, after which 224 Ã— 224 center crop is used. We optimize the cross-entropy loss with SGD optimizer with cosine decay scheduler with Nesterov momentum of 0.9 over 100</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Schematic illustration of stage-wise evaluation. We flatten intermediate feature maps from different stages and then use them to train stage-wise classifiers. Top-1 accuracy is reported by evaluating images in eval-D with the stage-wise classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Top-1 accuracy of stage-wise evaluation. All methods use ResNet50 as their backbones and are trained by 300 epochs with the setting in original papers. The results of linear evaluation of layer4-pooled-features (see Fig. 1) are reported in the legend.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The difference between SL and SL-MLP. Our SL-MLP adds an MLP before the classifier compared to SL. Only the encoders in both methods are utilized for downstream tasks.</figDesc><graphic url="image-4.png" coords="4,128.17,196.01,79.45,59.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Byol w/o MLP (iii) SupCon w/o MLP (ii) SL-MLP (iv) SupCon (vi) Byol</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of different methods with 10 randomly selected classes on pre-D. Different colors denote different classes. Features extracted by pretrained models without an MLP projector (top row) have less intra-class variation than those extracted by pretrained models with an MLP projector (bottom row).</figDesc><graphic url="image-5.png" coords="4,50.11,263.69,79.64,59.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visualization of Feature Mixtureness between pre-D and eval-D. Cold colors denote features from 5 classes that are randomly selected from pre-D, and warm colors denote features from 5 classes that are randomly selected from eval-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>4 .Figure 6 .</head><label>46</label><figDesc>Figure 6. (a) Stage-wise evaluation on eval-D. (b) Linear evaluation accuracy on eval-D. (c) Discriminative ratio of features on pre-D.Following<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we pretrain SL, SL-MLP, and Byol for 300 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) Feature Mixtureness between pre-D and eval-D. (b) Redundancy R of pretrained features during different epochs. Following<ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, we pretrain SL, SL-MLP, and Byol for 300 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. (Left to right) (a) Top-1 accuracy with different pretraining epochs and number of MLP projectors. (b) Top-1 accuracy with different batch sizes shows that SL-MLP has more robust transferability to small batch sizes. (c) Top-1 accuracy with different pretraining augmentations shows SL-MLP is robust to augmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Visualization of Feature Mixtureness with different manually generated feature distribution. Red and blue represent pre-D and eval-D class centers, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .Figure 13 .</head><label>1213</label><figDesc>Figure 12. Evolution of intra-class variation of features in pre-D with different epochs. Different colors denote different classes. The intra-class variation of SL will be very small when the pretraining epoch is large enough. Instead, the intra-class variation of SL-MLP and Byol still retains even though the model is pretrained by large epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>We randomly select features from 5 classes in pre-D and 5 classes in eval-D to visualize Feature Mixtureness with different MLP components. The results are summarized in Fig.15. The features with cold colors come from pre-D, the features with warm colors come from eval-D. Comparing (a) and (b), we can see adding a fully connected layer can hardly increase Feature Mixtureness between pre-D and eval-D. Comparing (c-d) with (b), we can conclude that the batch normalization layer and the ReLU layer can increase Feature Mixtureness between pre-D and eval-D. Comparing (b-d) with (e), we can summarize that the batch normalization and the ReLU layer are the most important components. A batch normalization layer with a ReLU layer can significantly increase Feature Mixtureness between pre-D and eval-D, which has already been similar to Feature Mixtureness when the MLP projector has the complete architectural.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Visualization of intra-class variation by different components. We randomly select 10 classes in pre-D. Different colors denote different classes. Comparing (a) wth (b), we can see the fully-connected layer can slightly help enlarge the intra-class variation. Comparing (a-b)and (d-e), we can observe the batch normalization layer and the ReLU layer can significantly enlarge the intra-class variation in the feature space. In general, all components in the MLP layer is beneficial to enlarge intra-class variation, which proves their effectiveness in enhancing transferaiblity of pretraining models.</figDesc><graphic url="image-46.png" coords="18,210.30,139.58,75.65,57.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Visualization of Feature Mixtureness of features pretrained by different MLP components. Different colors denote different classes. Points with cold colors denote the features from pre-D, and points with warm colors denote the features from eval-D. Comparing (c-d) with (a-b), we can see that adding BN and ReLU can increase Feature Mixtureness between pre-D and eval-D. Comparing (e) with (a-d), we can conclude that BN and ReLU play the main roles in the MLP projector as (e) shows larger Feature Mixtureness. An MLP projector with all components achieves the largest Feature Mixtureness.</figDesc><graphic url="image-47.png" coords="18,463.98,139.61,77.42,58.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 16 .Figure 17</head><label>1617</label><figDesc>Figure 16. Visualization of Feature Mixtureness between pretraining dataset (pre-D) and evaluation dataset (eval-D). Different colors denote different classes. Classes in pre-D are denoted by cold colors, and classes in eval-D are denoted by warm colors. Comparing (a,c,e) and (b,d,f), we can conclude that large semantic gap between pre-D and eval-D will lead to small Feature Mixtureness between pre-D and eval-D. Comparing (b) and (d-f), we can observe that the MLP projector can increase Feature Mixtureness between pre-D and eval-D, and can bridge the semantic gap between pre-D and eval-D.</figDesc><graphic url="image-57.png" coords="19,136.12,141.68,73.08,55.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>EP</cell><cell>Top-1(â†‘)</cell><cell>R(â†“)</cell></row><row><cell>SL</cell><cell>100</cell><cell>55.9</cell><cell>0.078</cell></row><row><cell>SL-MLP</cell><cell>100</cell><cell>63.1</cell><cell>0.035</cell></row><row><cell>SL</cell><cell>300</cell><cell>54.4</cell><cell>0.087</cell></row><row><cell>SL-MLP</cell><cell>300</cell><cell>64.1</cell><cell>0.034</cell></row><row><cell>Byol w/o MLP</cell><cell>300</cell><cell>39.0</cell><cell>0.247</cell></row><row><cell>Byol</cell><cell>300</cell><cell>62.3</cell><cell>0.037</cell></row><row><cell>Mocov1</cell><cell>300</cell><cell>54.1</cell><cell>0.069</cell></row><row><cell>Mocov1 w/ MLP</cell><cell>300</cell><cell>59.2</cell><cell>0.058</cell></row></table><note>. Redundancy R of pretrained features. Methods with an MLP obtain lower channel redundancy and transfer better.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Concept generalization task. We report Top-1 accuracy on eval-D of SL-MLP, Byol, and SL on various backbones. SL-MLP and Byol share the same MLP projector.</figDesc><table><row><cell>Method</cell><cell>Architecture</cell><cell cols="2">Labels MLP Epochs Top-1(â†‘)</cell></row><row><cell>SL</cell><cell>ResNet50</cell><cell>100</cell><cell>55.9</cell></row><row><cell>SL-MLP</cell><cell>ResNet50</cell><cell>100</cell><cell>63.1</cell></row><row><cell>Byol</cell><cell>ResNet50</cell><cell>300</cell><cell>62.3</cell></row><row><cell>SL</cell><cell>ResNet50</cell><cell>300</cell><cell>54.4</cell></row><row><cell>SL-MLP</cell><cell>ResNet50</cell><cell>300</cell><cell>64.1</cell></row><row><cell>SL</cell><cell>ResNet34</cell><cell>100</cell><cell>50.1</cell></row><row><cell>SL-MLP</cell><cell>ResNet34</cell><cell>100</cell><cell>55.0</cell></row><row><cell>Byol</cell><cell>ResNet34</cell><cell>300</cell><cell>54.8</cell></row><row><cell>SL</cell><cell>ResNet34</cell><cell>300</cell><cell>50.2</cell></row><row><cell>SL-MLP</cell><cell>ResNet34</cell><cell>300</cell><cell>55.8</cell></row><row><cell>SL</cell><cell>ResNet101</cell><cell>100</cell><cell>56.0</cell></row><row><cell>SL-MLP</cell><cell>ResNet101</cell><cell>100</cell><cell>63.6</cell></row><row><cell>SL</cell><cell>ResNet101</cell><cell>300</cell><cell>53.9</cell></row><row><cell>SL-MLP</cell><cell>ResNet101</cell><cell>300</cell><cell>64.7</cell></row><row><cell>SL</cell><cell>MobileNetv2(s=1.4)</cell><cell>200</cell><cell>54.5</cell></row><row><cell>SL-MLP</cell><cell>MobileNetv2(s=1.4)</cell><cell>200</cell><cell>61.5</cell></row><row><cell>SL</cell><cell>EfficientNetb2</cell><cell>100</cell><cell>57.6</cell></row><row><cell>SL-MLP</cell><cell>EfficientNetb2</cell><cell>100</cell><cell>64.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Object detection results. All methods are pretrained on ImagNet-1K, then finetuned on COCO using Mask-RCNN (R50-FPN) based on Detectron2<ref type="bibr" target="#b50">[51]</ref>. Sup. and Unsup. are short for supervised learning and unsupervised learning, respectively. Results of methods â€  are from<ref type="bibr" target="#b52">[53]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">object detection</cell></row><row><cell>Method</cell><cell>Sup. Unsup. Epoch</cell><cell>AP</cell><cell cols="2">AP50 AP75</cell></row><row><cell>SL</cell><cell>100</cell><cell>38.9</cell><cell>59.6</cell><cell>42.7</cell></row><row><cell>SL-MLP</cell><cell>100</cell><cell>39.7</cell><cell>60.4</cell><cell>43.1</cell></row><row><cell>InsDis â€  [52]</cell><cell>200</cell><cell>37.4</cell><cell>57.6</cell><cell>40.6</cell></row><row><cell>PIRL â€  [32]</cell><cell>200</cell><cell>37.5</cell><cell>57.6</cell><cell>41.0</cell></row><row><cell>SwAV â€  [5]</cell><cell>200</cell><cell>38.5</cell><cell>60.4</cell><cell>41.4</cell></row><row><cell>Mocov2 â€  [10]</cell><cell>200</cell><cell>38.9</cell><cell>59.4</cell><cell>42.4</cell></row><row><cell>Byol [18]</cell><cell>300</cell><cell>39.4</cell><cell>60.4</cell><cell>43.2</cell></row><row><cell>SL-MLP</cell><cell>300</cell><cell>40.7</cell><cell>61.8</cell><cell>44.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Linear evaluation on fixed backbone, full network finetuning, and few-shot learning performance on 12 classification datasets in terms of top-1 accuracy. All models are pretrained for 300 epochs with the same code base except for SelfSupCon â€  (Mocov2) which pretrained for 400 epochs using the results illustrated in<ref type="bibr" target="#b23">[24]</ref>. Average results style: best, second best.</figDesc><table><row><cell>Method</cell><cell cols="12">ChestX CropDisease DeepWeeds DTD EuroSAT Flowers102 Kaokore Omniglot Resisc45 Sketch SVHN ISIC Average</cell></row><row><cell>linear evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SL</cell><cell>45.45</cell><cell>96.80</cell><cell>84.02</cell><cell>66.22</cell><cell>95.07</cell><cell>83.69</cell><cell>75.40</cell><cell>64.14</cell><cell>85.36</cell><cell>67.82</cell><cell>67.13 79.58</cell><cell>75.89</cell></row><row><cell>SL-MLP</cell><cell>49.89</cell><cell>99.02</cell><cell>87.86</cell><cell>72.61</cell><cell>96.63</cell><cell>93.46</cell><cell>81.12</cell><cell>76.73</cell><cell>91.66</cell><cell>74.51</cell><cell>75.16 81.53</cell><cell>81.68</cell></row><row><cell>SupCon w/o MLP</cell><cell>41.38</cell><cell>91.52</cell><cell>73.16</cell><cell>62.93</cell><cell>89.84</cell><cell>73.23</cell><cell>66.38</cell><cell>44.54</cell><cell>76.55</cell><cell>55.21</cell><cell>61.45 68.54</cell><cell>67.06</cell></row><row><cell>SupCon</cell><cell>47.71</cell><cell>98.79</cell><cell>85.66</cell><cell>74.20</cell><cell>95.83</cell><cell>92.24</cell><cell>79.42</cell><cell>73.42</cell><cell>91.14</cell><cell>76.80</cell><cell>74.26 79.78</cell><cell>80.77</cell></row><row><cell>SelfSupCon â€ </cell><cell>48.08</cell><cell>99.06</cell><cell>87.88</cell><cell>72.71</cell><cell>96.97</cell><cell>89.62</cell><cell>81.67</cell><cell>69.66</cell><cell>90.88</cell><cell>69.12</cell><cell>69.95 81.51</cell><cell>79.70</cell></row><row><cell>finetuned with 1000 training samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SL</cell><cell>40.86</cell><cell>94.31</cell><cell>86.95</cell><cell>62.12</cell><cell>94.05</cell><cell>88.94</cell><cell>78.22</cell><cell>46.16</cell><cell>80.32</cell><cell>14.17</cell><cell>82.16 78.28</cell><cell>70.54</cell></row><row><cell>SL-MLP</cell><cell>42.34</cell><cell>94.48</cell><cell>89.64</cell><cell>63.90</cell><cell>95.30</cell><cell>90.20</cell><cell>77.98</cell><cell>46.66</cell><cell>83.13</cell><cell>17.32</cell><cell>80.19 78.82</cell><cell>71.66</cell></row><row><cell>SupCon w/o MLP</cell><cell>41.72</cell><cell>93.52</cell><cell>84.95</cell><cell>58.09</cell><cell>95.15</cell><cell>88.23</cell><cell>78.95</cell><cell>45.68</cell><cell>80.63</cell><cell>14.39</cell><cell>82.25 77.96</cell><cell>70.12</cell></row><row><cell>SupCon</cell><cell>41.84</cell><cell>93.46</cell><cell>88.70</cell><cell>61.81</cell><cell>94.54</cell><cell>91.28</cell><cell>78.35</cell><cell>46.02</cell><cell>81.62</cell><cell>15.84</cell><cell>81.85 78.51</cell><cell>71.15</cell></row><row><cell>SelfSupCon â€ </cell><cell>43.09</cell><cell>93.95</cell><cell>88.10</cell><cell>62.95</cell><cell>95.47</cell><cell>88.92</cell><cell>79.41</cell><cell>45.33</cell><cell>81.14</cell><cell>10.57</cell><cell>82.37 78.27</cell><cell>70.88</cell></row><row><cell>5-ways 5-shots few-shot classification</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SL</cell><cell>25.64</cell><cell>89.07</cell><cell>54.32</cell><cell>78.58</cell><cell>82.96</cell><cell>93.14</cell><cell>46.14</cell><cell>92.82</cell><cell>84.17</cell><cell>87.06</cell><cell>38.03 41.22</cell><cell>67.76</cell></row><row><cell>SL-MLP</cell><cell>26.89</cell><cell>93.45</cell><cell>59.08</cell><cell>83.04</cell><cell>87.16</cell><cell>96.88</cell><cell>50.77</cell><cell>95.73</cell><cell>89.00</cell><cell>89.84</cell><cell>41.96 46.76</cell><cell>71.71</cell></row><row><cell>SupCon w/o MLP</cell><cell>23.62</cell><cell>75.64</cell><cell>49.34</cell><cell>73.04</cell><cell>73.90</cell><cell>82.16</cell><cell>38.10</cell><cell>67.87</cell><cell>75.18</cell><cell>81.01</cell><cell>34.92 35.16</cell><cell>59.16</cell></row><row><cell>SupCon</cell><cell>26.18</cell><cell>94.09</cell><cell>59.36</cell><cell>85.02</cell><cell>87.97</cell><cell>96.55</cell><cell>51.02</cell><cell>94.49</cell><cell>89.01</cell><cell>89.75</cell><cell>41.67 43.48</cell><cell>71.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Empirical analysis of architectural design of the MLP projector. We incrementally add different components to the MLP projector. We pretrain models over 100 epochs and set the output dimension to 2048. Top-1 accuracy on eval-D is reported.</figDesc><table><row><cell></cell><cell>Components</cell><cell></cell><cell></cell></row><row><cell>Exp</cell><cell>Input FC BN ReLU</cell><cell>Output FC</cell><cell cols="2">+Params Top-1</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>/</cell><cell>55.9</cell></row><row><cell>(b)</cell><cell></cell><cell></cell><cell>4.196M</cell><cell>56.6</cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell>8.395M</cell><cell>61.0</cell></row><row><cell>(d)</cell><cell></cell><cell></cell><cell>8.391M</cell><cell>60.1</cell></row><row><cell>(e)</cell><cell></cell><cell></cell><cell>0.004M</cell><cell>60.5</cell></row><row><cell>SL-MLP</cell><cell></cell><cell></cell><cell>8.395M</cell><cell>62.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Quantitative analysis of structural design of inserted MLP, including discriminative ratio on pre-D, Feature Mixtureness Î  and feature redundancy R. (b-e) denote experiments in which different components are added on the SL baseline (a). When incrementally adding components of the MLP into SL, the distriminative ratio on pre-D and feature redundancy will decrease while the Feature Mixtureness will increase. ReLU, a batch normalization layer can increase Feature Mixtureness, reduce discriminative ratio, which could improve transferability of the pretrained model. Specifically, the ReLU layer brings a little improvement on feature redundancy. Comparing (a,b) with (c,e), we can conclude that BN not only reduces the discriminative ratio on pre-D, but also increases Feature Mixtureness. BN has a significant influence on future redundancy, which reduces feature redundancy by 50% (from 0.0671 to 0.0369). Last but not least, the combination of all components achieves the best transferability with the lowest feature redundancy, the highest Feature Mixtureness and a relatively large intra-class variation.</figDesc><table><row><cell></cell><cell>Components</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Exp</cell><cell>Input FC BN ReLU Output FC</cell><cell cols="2">Top-1 D pre inter /D pre intra</cell><cell>Î (â†‘)</cell><cell>R(â†“)</cell></row><row><cell>(a)</cell><cell></cell><cell>55.9</cell><cell>2.034</cell><cell cols="2">0.515 0.0776</cell></row><row><cell>(b)</cell><cell></cell><cell>56.6</cell><cell>1.505</cell><cell cols="2">0.679 0.0671</cell></row><row><cell>(c)</cell><cell></cell><cell>61.0</cell><cell>1.269</cell><cell cols="2">0.870 0.0369</cell></row><row><cell>(d)</cell><cell></cell><cell>60.1</cell><cell>1.362</cell><cell cols="2">0.804 0.0654</cell></row><row><cell>(e)</cell><cell></cell><cell>60.5</cell><cell>1.045</cell><cell cols="2">0.846 0.0369</cell></row><row><cell>SL-MLP</cell><cell></cell><cell>62.5</cell><cell>1.124</cell><cell cols="2">0.871 0.0351</cell></row><row><cell cols="6">ratio on pre-D. Second, non-linear layer brings considerable improvements. Comparing (b) to (d), we can summarize that</cell></row><row><cell>incrementally adding a</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Top-1 linear evaluation accuracy on eval-D when pretraining the model on pre-D by cosine-softmax cross-entropy loss. where w i is the i-th class prototype, Î² is the scale factor. Accordingly, we add an MLP projector before the classifier to construct cosine-softmax-mlp cross-entropy loss, i.e.,L cos-mlp (x i , y i ) = âˆ’ log exp(Î² â€¢ cos(w yi , g(f (x i ))))C i=j exp(Î² â€¢ cos(w j , g(f</figDesc><table><row><cell cols="3">epoch cos cos-mlp</cell></row><row><cell>20</cell><cell>47.1</cell><cell>45.0</cell></row><row><cell>40</cell><cell>47.8</cell><cell>49.6</cell></row><row><cell>60</cell><cell>50.9</cell><cell>52.6</cell></row><row><cell>80</cell><cell>53.5</cell><cell>56.5</cell></row><row><cell>100</cell><cell>53.7</cell><cell>59.0</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">All experiments in Sec.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">and Sec.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">are conducted with ResNet50.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">Strictly speaking, larger intra-class variation is relative to inter-class distance, which is theoretically defined as discriminative ratio. We use "intra-class variation" to be consistent with previous work[24,  </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="57" xml:id="foot_4">].</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following the method proposed in <ref type="bibr" target="#b35">[36]</ref>, we visualize the maximum response of convolution channels in layer 4 of ResNet50 pretrained with different methods.</p><p>epochs, using a batch size of 4096. We finally sweep over 7 learning rate over {0.16, 0.48, 1.44, 4.8, 14.4, 48} and report the best accuracy on the test set of eval-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3. Transfer to Other Classification Tasks</head><p>Follow the downstream image classification tasks and the evaluation methods mentioned in <ref type="bibr" target="#b23">[24]</ref>, we use 12 datasets from different domains to evaluate the transferability of different methods, including natural <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37]</ref>, satellite <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>, symbolic <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>, illustrative <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>, medical <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b47">48]</ref>, and texture <ref type="bibr" target="#b12">[13]</ref>. The statistics of datasets are illustrated in Tab. 8. Linear Evaluation. For fixed-feature linear evaluation, we add a linear layer on the frozen pretrained backbone to train the model on the downstream datasets. A batch normalization layer is added between the backbone and linear layer. All models are trained for 50 epochs with step learning scheduler which decreases the learning rate by 0.1 at epoch 25 and 37. 70% of the training set is used for training and the rest is used for validation, the models are then trained with â€¢ learning rate: 0.001, 0.01, 0.1;</p><p>â€¢ batch size: 32, 128;</p><p>â€¢ weight decay: 0, 1 Ã— 10 âˆ’4 , 1 Ã— 10 âˆ’5 .</p><p>The optimal hyperparameters are chosen based on the performance on the validation set. The top-1 accuracy is reported as the evaluation metric. Full Network Finetuning. In full network finetuning, the whole pretrained backbone and a linear classifier are trained on the downstream dataset. All models are trained for 50 epochs with step learning scheduler which decreases the learning rate by 0.1 at epoch 25 and 37. A batch normalization layer is added between the backbone and linear layer to make the extracted features comparable among different models. The models are trained with â€¢ learning rate: 0.001, 0.01, 0.1;</p><p>â€¢ batch size: 32, 128;</p><p>â€¢ weight decay: 0, 1 Ã— 10 âˆ’4 , 1 Ã— 10 âˆ’5 .</p><p>The optimal hyperparameters are chosen based on the performance on the validation set. Few-shot Learning. For few-shot learning, following <ref type="bibr" target="#b44">[45]</ref>, we use a logistic regression layer on the top of the features during meta-testing phase. The implementation from scikit-learn is used for logistic regression. Same as <ref type="bibr" target="#b23">[24]</ref>, we also provide the mean of 600 randomly sampled tasks as the accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4. Object Detection on COCO</head><p>For object detection, we train Mask-RCNN <ref type="bibr" target="#b20">[21]</ref> (R50-FPN) on COCO 2017 train split and report results on the val split. We use a learning rate of 0.001 and keep the other parameters the same as in the 1Ã— schedule in detectron2 <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. More Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1. Few-shot Recognition Results</head><p>Using the code provided by <ref type="bibr" target="#b23">[24]</ref>, we pretrain all models over 300 epochs with a cosine decay learning scheduler on ImageNet-1K, and then testing on 12 downstream datasets (shown in Tab.8). We provide 5-ways 5-shots and 20-shots results in Tab. 9. All reported accuracy is the mean of 600 randomly sampled tasks. Comparing average results among different methods, we observe that supervised pretraining methods with the MLP projector, i.e. SL-MLP and SupCon, outperform their no MLP counterparts, i.e. SL and SupCon w/o MLP, on both 5-ways 5-shots and 20-shots few-shot classification tasks. Table <ref type="table">9</ref>. 5-ways 5-shots and 20-shots classification performance on 12 downstream datasets in terms of top-1 accuracy. Using the code in <ref type="bibr" target="#b23">[24]</ref>, we pretrain all models over 300 epochs on ImageNet-1K. The reported accuracy is the mean of 600 randomly sampled tasks. Average results style: best, second best.  Table <ref type="table">10</ref>. Linear evaluation results and top-1 accuracy during pretraining on SL and SL-MLP. We remove the MLP in SL-MLP for linear evaluation, only the fixed backbones of SL and SL-MLP are used. For top-1 accuracy during pretraining, accuracy of the whole SL-MLP is reported. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epochs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. Ablation on Hidden Units and Output Dimensions</head><p>On concept generalization task, we also explore whether hidden units and output dimensions of the added MLP projector influence the final transferability. We pretrain SL-MLP on pre-D over 100 epochs using various hidden units and output dimensions of the added MLP projector, and report the evaluation results on eval-D (illustrated in Fig. <ref type="figure">19</ref>). We observe that, different from other unsupervised pretraining methods, e.g. BYOL and SimCLR, where the output dimension of the MLP projector have considerable impacts on transferability, the hidden units and output dimensions of the added MLP projector has little influence on the performance of SL-MLP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Pretrain Results on pre-D</head><p>We also provide the top-1 accuracy of SL-MLP on pre-D in Tab. 10. We remove the MLP in SL-MLP for linear evaluation on pre-D, only the fixed backbones of SL and SL-MLP are used to train new classifiers over 100 epochs. We also report top-1 accuracy during pretraining in which accuracy of the whole SL-MLP is reported. Which features are used to evaluate these two metrics are illustrated in Fig. <ref type="figure">20</ref>. As backbones and classifiers are jointly trained during pretraining, classifiers are not well optimized at small pretraining epochs. Thus, models always achieve better performance on linear evaluation at small pretraining epochs because linear evaluation provides more epochs for networks to optimize better classifiers on fixed backbones. For SL, two evaluation methods display the same result at epoch 100, as they have all trained well-optimized classifiers.</p><p>Note that SL-MLP shows slight âˆ’2.6% performance drop (80.8% to 78.2%) on linear evaluation when SL and SL-MLP have all been pretrained over 100 epochs, which achieves closer performance gap than Exemplar-v2 <ref type="bibr" target="#b57">[57]</ref> when compared with SL. Besides, as SL-MLP only adds an MLP projector before the classifier, the whole SL-MLP shows almost the same performance of SL on top-1 accuracy during pretraining at epoch 100.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A critical analysis of self-supervision, or what we can learn from a single image</title>
		<author>
			<persName><surname>Ym Asano</surname></persName>
		</author>
		<author>
			<persName><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Linear discriminant analysis-a brief tutorial. Institute for Signal and information Processing</title>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Balakrishnama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Ganapathiraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An introduction to inequalities</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Edwin F Beckenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Ernest</forename><surname>Bellman</surname></persName>
		</author>
		<author>
			<persName><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>Mathematical Association of America</publisher>
			<biblScope unit="page">17</biblScope>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning bounds for domain adaptation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">HervÃ©</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanqing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7546" to="7554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Remote sensing image scene classification: Benchmark and state of the art</title>
		<author>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)</title>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronica</forename><surname>Rotemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Tschandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Dusza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Helba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aadi</forename><surname>Kalloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Liopyris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Marchetti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
				<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14548</idno>
		<title level="m">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">How well do self-supervised models transfer?</title>
		<author>
			<persName><forename type="first">Linus</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5414" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>AltchÃ©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020. 1, 2, 5, 6, 7, 19</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Piotr DollÃ¡r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Helber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Bischke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Dengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Ashraful</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13517</idno>
		<title level="m">A broad study on the transferability of visual representations with contrastive learning</title>
				<imprint>
			<date type="published" when="2021">2021. 1, 2, 3, 4, 6, 7, 8, 21</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="1920">1920-1929, 2019. 20</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Negative margin matters: Understanding margin in few-shot classification</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Hong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12031</idno>
		<title level="m">Towards understanding the transferability of deep representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using deep learning for image-based plant disease detection</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Sharada P Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><surname>SalathÃ©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in plant science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Feature visualization. Distill</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deepweeds: A multiclass weed species image dataset for deep learning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><forename type="middle">A</forename><surname>Konovalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bronson</forename><surname>Philippa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Ridd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><forename type="middle">C</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Johns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Banks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Girgenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Whinney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transfusion: Understanding transfer learning for medical imaging</title>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3347" to="3357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Concept generalization in visual representation learning</title>
		<author>
			<persName><forename type="first">Mert</forename><surname>Bulent Sariyildiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Kaokore: A pre-modern japanese art facial expression dataset</title>
		<author>
			<persName><forename type="first">Yingtao</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chikahiko</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarin</forename><surname>Clanuwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Bober-Irizar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asanobu</forename><surname>Kitamoto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08595</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
				<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part XIV 16</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A comparison of machine learning methods for cross-domain few-shot learning</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mayo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Australasian Joint Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="445" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Can semantic labels assist self-supervised visual representation learning?</title>
		<author>
			<persName><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08621</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Various proofs of the cauchy-schwarz inequality. Octogon mathematical magazine</title>
		<author>
			<persName><forename type="first">Hui-Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanhe</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04803</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="16684" to="16693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Scaling sgd batch size to 32k for imagenet training</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StÃ©phane</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">What makes instance discrimination good for transfer learning?</title>
		<author>
			<persName><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rynson</forename><forename type="middle">Wh</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06606</idno>
		<imprint>
			<date type="published" when="1920">2020. 1, 2, 3, 4, 7, 8, 20</date>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
