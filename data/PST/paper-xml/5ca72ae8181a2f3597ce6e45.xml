<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</title>
				<funder>
					<orgName type="full">SRC Center for Research on Intelligent Storage and Processing-in-memory</orgName>
					<orgName type="abbreviated">CRISP</orgName>
				</funder>
				<funder ref="#_Ez3ERDM">
					<orgName type="full">Stanford Platform Lab, NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
							<email>xuany@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
							<email>horowitz@stanford.edu</email>
						</author>
						<author>
							<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
							<email>kozyraki@stanford.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University Google, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Stanford University Google, Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Tangram: Optimized Coarse-Grained Dataflow for Scalable NN Accelerators</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3297858.3304014</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>parallelism</term>
					<term>dataflow</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The use of increasingly larger and more complex neural networks (NNs) makes it critical to scale the capabilities and efficiency of NN accelerators. Tiled architectures provide an intuitive scaling solution that supports both coarse-grained parallelism in NNs: intra-layer parallelism, where all tiles process a single layer, and inter-layer pipelining, where multiple layers execute across tiles in a pipelined manner.</p><p>This work proposes dataflow optimizations to address the shortcomings of existing parallel dataflow techniques for tiled NN accelerators. For intra-layer parallelism, we develop buffer sharing dataflow that turns the distributed buffers into an idealized shared buffer, eliminating excessive data duplication and the memory access overheads. For interlayer pipelining, we develop alternate layer loop ordering that forwards the intermediate data in a more fine-grained and timely manner, reducing the buffer requirements and pipeline delays. We also make inter-layer pipelining applicable to NNs with complex DAG structures. These optimizations improve the performance of tiled NN accelerators by 2? and reduce their energy consumption by 45% across a wide range of NNs. The effectiveness of our optimizations also increases with the NN size and complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>? Computer systems organization ? Neural networks; Data flow architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural networks (NNs) are currently the most effective solution for many challenging classification, recognition, and prediction problems <ref type="bibr" target="#b23">[24]</ref>. Hence, there is significant interest in finding scalable and energy efficient ways to run NNs on devices ranging from datacenter servers to mobile clients.</p><p>Recent research has shown that domain-specific NN accelerators can achieve more than two orders of magnitude improvements over CPUs and GPUs in terms of performance and energy efficiency <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>. NN accelerators typically use spatial architectures with 1D or 2D arrays of processing elements (PEs) and on-chip SRAM buffers to facilitate data reuse. The software that orchestrates the dataflow between the on-chip and off-chip memories and the PEs is also critical in achieving high performance and energy efficiency <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The need for higher accuracy on increasingly complex problems leads to larger NNs with higher compute and memory requirements. For example, recent NNs use up to a few hundreds of layers, with each layer sized at several megabytes <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. Hence, it is important to scale up NN accelerators to efficiently support larger NNs. An intuitive approach for scalable acceleration is to use tiled architectures, where each tile includes a small 2D PE array and a local SRAM buffer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>. A network-on-chip interconnects the tiles. To get scalable performance on tiled accelerators, we must optimize the coarse-grained parallelism across multiple tiles, in addition to the fine-grained parallelism within each engine. Existing dataflow schemes for coarse-grained parallelism suffer from significant inefficiencies. Parallelizing a single NN layer (intra-layer parallelism) leads to significant data duplication <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, and pipelining Session: Machine Learning II ASPLOS <ref type="bibr">'19, April 13-17, 2019</ref>, Providence, RI, USA the processing of multiple layers (inter-layer pipelining) results in substantial challenges in resource utilization and on-chip buffer requirements <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>To overcome these challenges, we present Tangram, a scalable tiled accelerator with novel dataflow optimizations for coarse-grained parallelization of NN workloads. Tangram uses the same hardware resources as prior tiled NN architectures. Its primary contribution is the optimized dataflow. For intra-layer parallelism, we develop buffer sharing dataflow (BSD) that eliminates the inefficiencies resulted from data duplication in on-chip buffers. It effectively turns the distributed SRAM buffers into an idealized shared buffer that always has the necessary data close to the processing tile. For inter-layer pipelining, Tangram introduces alternate layer loop ordering (ALLO) dataflow to forward intermediate fmap data in a more fine-grained and timely manner, which reduces the on-chip buffering requirements and the pipeline filling/draining delays. Tangram also includes pipelining optimizations for the complex DAG structures in advanced CNNs and LSTMs, which can minimize the number of complex data dependencies served through the off-chip memory. This extends the applicability of inter-layer pipelining beyond the simple linear NNs targeted in previous work.</p><p>We evaluate Tangram using large-scale, state-of-the-art, CNN, MLP, and LSTM models. We show that by using optimized parallel dataflow, Tangram improves upon an already optimized baseline with the same tiled hardware, with 2.0? higher performance and 45% less energy. These benefits allow Tangram to sustain 6107.4 GOPS performance and 439.8 GOPS/W energy efficiency. These numbers represent an efficiency improvement equivalent to two technology node generations. We also perform a detailed analysis of the intra-layer and inter-layer dataflow optimizations to understand their individual contributions. The effectiveness of these optimizations increases for larger NNs (intra-layer) and NNs with complex DAGs (inter-layer). Hence, we believe that Tangram represents an effective way to accelerate advanced NNs in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Network Algorithms</head><p>Deep (DNNs), Convolutional (CNNs), and Recurrent Neural Networks (RNNs) are the most widely used NNs today. The typical NN structure is a directed acyclic graph (DAG) composed of multiple layers. While DNNs and many CNNs use a single linear chain, RNNs, such as Long Short-Term Memories (LSTMs), and advanced CNNs exhibit the complex DAG structures shown in Figure <ref type="figure" target="#fig_0">1</ref>. During inference, data propagate in the forward direction, from the first layer that accepts the input (e.g., image, text), to the last layer that produces the result (e.g., image label, translated text). During training, data first propagate forward to generate an inference result to compare against the ground truth, and then Ifmaps</p><formula xml:id="formula_0">d in d out 1 ? 1 Conv 1 ? 1 Conv 3 ? 3 Pool 1 ? 1 Conv 3 ? 3 Conv 5 ? 5 Conv 1 ? 1 Conv | | d in d out 1 ? 1 Conv 1 ? 1 Conv 3 ? 3 Pool 1 ? 1 Conv 3 ? 3 Conv 5 ? 5 Conv 1 ? 1 Conv | (a) GoogLeNet inception module. c t-1 h t-1 c t h t x t I-Gate F-Gate O-Gate ? ? + + ? ? FC ? ? tanh | | c t-1 h t-1 c t h t x t I-Gate F-Gate O-Gate ? + ? FC ? tanh | (b) LSTM cell.</formula><formula xml:id="formula_1">N i * = * = N b N b N o N o N i N i * = N b N b N o N o N i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weights</head><p>Ofmaps Ifmaps the errors propagate backward to update the model weights in each layer. The most common NN layer types are fully-connected (FC) and convolutional (CONV). The gates in LSTM cells are essentially FC layers. An FC layer generates a 1D vector output by performing matrix-vector multiplication between its weight matrix and the input vector. The output of a CONV layer is organized as multiple 2D feature maps (fmaps), as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Each output fmap (ofmap) is the sum of 2D convolutions between all input fmaps (ifmaps) and a set of filter weights. To amortize the cost of weight accesses, NN computations are often performed on a batch of data samples. Since an FC layer can be viewed as a CONV layer with 1 ? 1 fmaps, the computation of both FC and CONV layers can be summarized as:</p><formula xml:id="formula_2">N i * = N b N b N o N o N i Weights Ofmaps for sample b ? 0 to N b do for ifmap i ? 0 to N i do for ofmap o ? 0 to N o do O[b][o] += I[b][i] * W[o][i]</formula><formula xml:id="formula_3">O[b][o] = N i -1 i=0 I[b][i] * W[o][i] + B[o], 0 ?o &lt; N o , 0 ?b &lt; N b (1)</formula><p>where I and O are the 4D ifmaps and ofmaps (2D image, number of fmaps, and batch), W is the filter weights, and B is a 1D bias. " * " denotes 2D convolution. N i , N o , N b are the number of ifmaps, ofmaps, and the size of batch, respectively.</p><p>NNs also include other types of layers. CONV and FC layers are typically followed by activation (ACT) layers, which apply non-linear functions such as ReLU or sigmoid. Maximum or average pooling (POOL) layers are optionally added between CONV layers to reduce fmap dimensions. ACT and POOL layers do not have weights, thus data can be easily processed in a streaming fashion.</p><p>For training, the forward computation is the same as inference. The most common algorithm for backward propagation is gradient descent. In CONV layers, the errors are convolved with the previously generated ofmaps; in FC layers, the errors are multiplied with the output vectors. They can be formulated as CONV and FC layers with different dimensions <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. Because the output of each layer is needed for backward propagation, they must be written to off-chip memory during forward computation and fetched back later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NN Accelerators and Dataflow Scheduling</head><p>Several accelerator designs have been developed to address the high compute and memory requirements of NNs (see <ref type="bibr">Section 7)</ref>. We use the state-of-the-art Eyeriss accelerator as our baseline NN engine <ref type="bibr" target="#b6">[7]</ref>. As shown in Figure <ref type="figure">3</ref>, the Eyeriss NN engine includes a number of processing elements (PEs) organized in a 2D array. Each PE contains a simple ALU for multiply-accumulate (MAC) operations and a small register file of 64 to 512 bytes. A larger SRAM buffer is shared by all PEs. Other NN accelerators use a similar architecture <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>The efficiency of NN engines depends on how the nested loops in NN computations shown in Figure <ref type="figure" target="#fig_1">2</ref> are scheduled <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">46]</ref>. Since the total data size for NNs is too large to fit entirely on-chip, the on-chip and off-chip dataflows are crucial for performance. Loop transformations, such as blocking and reordering, can maximize data reuse across loop iterations in the on-chip buffer and minimize accesses to the off-chip memory <ref type="bibr" target="#b45">[46]</ref>. Array mapping techniques optimize the spatial mapping of the 2D convolution for each pair of ifmap</p><formula xml:id="formula_4">I[b][i] and ofmap O[b][o]</formula><p>on the PE array, in order to maximize parallelism and capture the locality in the PE registers <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parallelizing NN Acceleration</head><p>Since large NNs with more layers and more complex DAG structures provide higher accuracy on more challenging tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>, there is strong interest in scaling NN accelerators. Simply increasing the number of PEs in the monolithic engine in Figure <ref type="figure">3</ref> is not efficient. First, small layers cannot fully utilize a large PE array, as computations sharing the same data are typically placed on one PE to exploit register locality. While one can map multiple 2D convolutions to a single PE array <ref type="bibr" target="#b5">[6]</ref>, these convolutions are independent and can only result in interference. Second, larger PE arrays incur higher latency and energy overheads for the data multicast needed when each time we start processing a new set of fmaps <ref type="bibr" target="#b7">[8]</ref> (see Figure <ref type="figure" target="#fig_9">11</ref>). Third, the increasing distance between the shared buffer and the PEs also becomes a bottleneck. While banking can help, most PEs would still be quite far from the banks they need to access. Finally, a monolithic array with rigid PE interconnects cannot support the dataflow for inter-layer pipelining.</p><p>An efficient approach to get scalable performance is to build a tiled architecture with multiple NN engines <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. As shown in Figure <ref type="figure">4</ref>, engines communicate through a network-on-chip (NoC) that also connects them to off-chip memory channels. The tiled architecture allows for coarsegrained parallelism, where NN computations are coarsely parallelized onto different engines. This is in addition to the fine-grained parallelism of the spatially mapped 2D convolutions on the PE array in each engine. To make an analogy to general-purpose processors, fine-grained parallelism corresponds to SIMD or instruction-level parallelism, while coarse-grained parallelism corresponds to multi-core.</p><p>There are two types of coarse-grained parallelism for NN computations. First, multiple engines can process in parallel the computations of a single layer <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. Table <ref type="table" target="#tab_1">1</ref> shows different schemes to leverage such intra-layer parallelism <ref type="bibr" target="#b13">[14]</ref>. Batch parallelization partitions the batch so that each engine processes different data samples (data parallelism). Fmap parallelization tiles the i/ofmaps and uses each engine to process a sub-tile region of all fmaps. Output parallelization parallelizes the ofmap loop in Figure <ref type="figure" target="#fig_1">2</ref> and uses each engine to process a subset of the ofmaps. Similarly, input parallelization parallelizes the ifmap loop.</p><p>Alternatively, we can use multiple engines to process multiple layers in a pipelined manner by spatially mapping the NN DAG structures <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. Such inter-layer pipelining is effective in increasing the hardware utilization when layers are small and hardware resources are abundant. This is the case for many recent NNs that use large numbers of layers but each individual layer is rather small <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45]</ref>. With inter-layer pipelining, the intermediate fmaps can be forwarded between layers through the on-chip NoC, reducing the energy cost for off-chip memory accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Baseline Architecture and Its Inefficiencies</head><p>We focus on optimizing intra-layer parallelism and interlayer pipelining on tiled NN architectures. Existing, state-ofthe-art techniques have significant inefficiencies.</p><p>Baseline hardware: We start with an optimized tiled NN accelerator (Figure <ref type="figure">4</ref>) similar to recent proposals <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b43">44]</ref>. We consider 16 ? 16 tiles, where each tile is an Eyeriss-like NN engine that includes an 8 ? 8 PE array and a 32 kB private SRAM buffer <ref type="bibr" target="#b6">[7]</ref> (Figure <ref type="figure">3</ref>). We leverage the state-of-theart row stationary dataflow for PE array mapping <ref type="bibr" target="#b5">[6]</ref>, and the loop transformation techniques by Yang, et al. to optimally manage SRAM buffers <ref type="bibr" target="#b45">[46]</ref>. When switching layers, we elide the off-chip accesses (no writeback, no refetch) if the intermediate fmaps can fit entirely in on-chip buffers.</p><p>Baseline intra-layer dataflow: The baseline system supports intra-layer parallelism using hybrid output and fmap parallelization as proposed in TETRIS <ref type="bibr" target="#b13">[14]</ref>. All engines fetch data in parallel from multiple off-chip memory channels. If input data is shared between engines, they are fetched from memory once and broadcast. Similarly, output data are fully accumulated across engines and only the final results are written back to memory, following the dataflow in ScaleDeep <ref type="bibr" target="#b43">[44]</ref>.</p><p>This approach uses the buffer within each engine as a private cache that holds a full copy of any shared data. Such data duplication can waste significant amounts of overall SRAM buffer capacity. For example, duplicating a moderate size of 64 kB data across the 16 ? 16 tiles could result in a waste of 16 MB buffer space. This scenario is likely to happen. Table <ref type="table" target="#tab_1">1</ref> shows that none of the intra-layer parallelization schemes can fully partition all data types among NN engines. At least one data type (ifmaps, ofmaps, or weights) is shared and thus must be duplicated (denoted in italics). Data duplication reduces the effective capacity of on-chip buffers, which leads to reduced reuse opportunities and more off-chip accesses. For example, duplication may prevent the intermediate fmaps between two layers from fitting in the on-chip SRAM. Also, duplication leads to significantly larger area requirements for the accelerator chip with larger SRAM buffers.</p><p>Baseline inter-layer pipelining: Previous proposals for interlayer pipelining assumed sufficient hardware resources, so that the entire NN (all layers) can be mapped onto a single or multiple chips <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>. This approach does not scale to large NNs with hundreds of layers.</p><p>Our baseline system supports inter-layer pipelining by dividing the layers of large NNs into segments. At each time, only a single segment of layers is scheduled on the tiled architecture. On-chip resources are allocated to the layers in the segment proportional to their total number of operations <ref type="bibr" target="#b43">[44]</ref>. Only the first layer input and the last layer output in the segment require off-chip accesses. The intermediate fmaps are directly forwarded through the on-chip buffers between layers. In addition, when all layers in the NN spatially execute on different engines and their weights fit entirely in the on-chip buffers, we support model weight pinning and avoid accessing weights from the off-chip memory <ref type="bibr" target="#b12">[13]</ref>.</p><p>Still, the baseline approach has several inefficiencies. First, direct forwarding intermediate fmaps requires large on-chip buffers. The entire fmaps must be stored on-chip and double buffered for concurrent read and write accesses from adjacent layers <ref type="bibr" target="#b43">[44]</ref>. This translates to tens or hundreds of MBytes of on-chip SRAM (see Table <ref type="table" target="#tab_3">2</ref>), and can only be made worse by the data duplication in intra-layer dataflow. Second, when switching between segments, the hardware resource utilization drops substantially due to the overheads of filling and draining the segment pipelines. Later layers in the segment cannot start processing until the previous layers produce their output data. Finally, all prior work has limited the inter-layer pipelining to linear NN structures, with no support for complex DAGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Tangram Parallel Dataflows</head><p>We propose Tangram, a tiled NN architecture with novel dataflow optimizations for intra-layer parallelism and interlayer pipelining. Tangram uses the same hardware resources as the baseline, but its novel parallel dataflow overcomes the buffer requirement and resource utilization challenges in previous designs (Sections 3.1 and 3.2). It also extends inter-layer pipelining to support complex DAG structures in advanced CNNs and LSTMs (Section 3.3). Tangram is designed for inference tasks, but its dataflow optimizations can be similarly applied to training (Section 3.4). This section focuses on the dataflow optimizations. We discuss their hardware and software implementations in Section 4.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Intra-Layer Parallelism with Buffer Sharing</head><p>We start by improving the dataflow across multiple NN engines processing a single layer. Figure <ref type="figure" target="#fig_3">5</ref>(a) reviews the dataflow in the baseline (Section 2.4). In this example, we consider output parallelization, where each engine processes a different ofmap subset using the corresponding weights. The same set of ifmaps are shared and duplicated in the buffers of all engines (e.g., first at time ? I[:][0 : 3] from memory 0, then at time ? I[:][3 : 6] from memory 1). When parallelizing over p o engines, there are p o copies of the same ifmaps in the SRAM. Similarly, with fmap or batch parallelization, the shared weights are duplicated; with input parallelization, the shared ofmaps are duplicated. This duplication wastes expensive on-chip buffers and fails to benefit from data reuse patterns both within one layer and across adjacent layers.</p><p>Buffer sharing dataflow (BSD): BSD is a parallel dataflow optimization that eliminates shared data duplication across engines. With BSD, each engine contributes its buffer capacity to store a subset of the shared data and continuously exchanges data with other engines, until all shared data have passed through and been processed locally by all engines. Figure <ref type="figure" target="#fig_3">5</ref>(b) illustrates BSD. We first skew the computation order across the engines to make each engine fetch and process a different subset of the shared data from the nearby memory channel. At time ?, engine 0 starts with I[:][0 : 3], engine 1 starts with I[:][3 : 6], etc. Since the ofmap accumulation is commutative, skewing the order does not affect the accuracy. When all engines finish processing their current subsets of data, we rotate the shared data in order to allow each engine to process a different subset. At time ?, after the first rotation, engine 0 processes I[:] <ref type="bibr">[3 : 6]</ref>  BSD is also applicable to weight sharing with fmap or batch parallelization, as well as hybrid parallelization as shown in Figure <ref type="figure">6</ref>. With hybrid ifmap and weight sharing, data are logically distributed in 2D, and rotation also happens in 2D. We first rotate the weights vertically to complete processing the fmaps currently in the local buffers (e.g., I[0 : 3][0 : 2] and O[0 : 3][0 : 1] in engine (0, 0)), and then rotate the ifmaps horizontally to obtain new ranges of ifmaps (e.g., engine (0, 0) gets I[0 : 3][2 : 4] from engine (0, 1)).</p><p>Loop transformation model for BSD: We represent BSD using the loop transformation model in <ref type="bibr" target="#b45">[46]</ref>. As shown in Figure <ref type="figure" target="#fig_4">7</ref>, originally the on-chip buffer of each engine can store N i /t i ifmaps and N o /t o ofmaps, with 1/t b of the batch N b , corresponding to the top-level loop blocking factors. Each time the engine fetches a subset of ifmaps, ofmaps, and weights for on-chip processing according to the loop indices. With BSD, the p o engines with output parallelization now buffer different ifmaps, p o times larger than before, reducing the ifmap loop factor by p o . To rotate the ifmaps between engines, an additional blocking loop level i ?? 0 is introduced, and skewed by the engine index x. Because the rotation loop is inside the ofmap loop, the N i /(t i /p o ) ifmaps are rotated r = t o rounds on-chip and fully reused by all N o ofmaps.</p><p>In general, assume that N data are shared by p engines (e.g., N i ifmaps shared by p o engines), and that each engine buffer stores 1/t of the shared data. If the total number of rotation rounds is r decided by the outer loop factors, the subset index of the shared data in the xth engine at time step T will be</p><formula xml:id="formula_5">i 0 = ? T rp ? ? p + (x + T mod p) mod p, 0 ? T &lt; ? t p ? ? r ? p (2)</formula><p>In the case of hybrid parallelization, the index of each shared data can be calculated independently.</p><formula xml:id="formula_6">for b 0 ? 0 to t b do for i 0 ? 0 to t i do // fetch ifmap subset [b 0 ][i 0 ]. for o 0 ? 0 to t o do // fetch ofmap subset [b 0 ][o 0 ], weight subset [o 0 ][i 0 ]. // on-chip processing.</formula><p>(a) Loop blocking without BSD.</p><formula xml:id="formula_7">for b 0 ? 0 to t b do for i ? 0 ? 0 to t i /p o do // fetch ifmap subset [b 0 ][i ? 0 ] into p o engines. for o 0 ? 0 to t o do // fetch ofmap subset [b 0 ][o 0 ], weight subset [o 0 ][i 0 ]. for i ?? 0 ? 0 to p o do i 0 = i ? 0 ? p o + (x + i ?? 0 ) mod p o // rotate to get ifmap subset [b 0 ][i 0 ]. // on-chip processing.</formula><p>(b) Loop blocking with BSD. Using the above model, the compiler can statically analyze the dataflow and fully manage the hardware at runtime. We present further implementation details in Section 4.</p><p>BSD benefits: BSD optimizes the use of SRAM buffers across all engines. With ifmap sharing as an example, we can rotate the ifmaps multiple rounds, each for a different set of ofmaps. This allows for the reuse of a larger set of ifmaps (those across all engines, I[:][0 : 9], rather than those in a single engine, I[:][0 : 3], in Figure <ref type="figure" target="#fig_3">5(b)</ref>) across all the ofmaps without the need for off-chip accesses. With the loop blocking shown in Figure <ref type="figure" target="#fig_4">7</ref>, the number of ofmap off-chip fetches is decided by the outer level ifmap loop factor. Previously the ofmaps are fetched t i times, each being updated using N i /t i ifmaps. With BSD, the ofmaps are updated with p o ? more ifmaps each time, and thus only fetched t i /p o times from off-chip.</p><p>BSD also improves data reuse when switching between adjacent layers. Without BSD, each engine needs a private copy of the input data from the previous layer. If these ifmaps do not fit in the buffer of a single engine, we have to spill them using off-chip memory. By eliminating data duplication with BSD, as long as the ifmaps can fit on-chip using all SRAM buffers, we can directly reuse the buffered intermediate fmaps from the previous layer, and elide off-chip accesses when switching layers.</p><p>In fact, BSD is equivalent to the ideal case where a single, large buffer with the aggregate capacity of all engine buffers stores all data with no duplication. So it achieves the maximum on-chip data reuse. Moreover, by combining computation skew and data rotation, we ensure that the data </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Off-chip</head><p>Off-chip Buffer all fmaps Buffer one fmap <ref type="bibr">Figure 9</ref>. Intermediate fmap buffering of inter-layer pipelining with alternate layer loop ordering (ALLO). Blue and green boxes denote ifmaps and ofmaps of each layer, respectively. currently being processed always reside in the local buffer and do not need to be accessed remotely. Hence the buffers operate like an optimal NUCA cache <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. Data rotation happens between neighbor engines, which minimizes the number of NoC hops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inter-Layer Pipelining with ALLO</head><p>As discussed in Section 2, while inter-layer pipelining can improve overall performance and resource utilization in the presence of small layers, it requires significant on-chip buffer capacity to hold the intermediate fmap data between layers. Moreover, data dependencies between layers result in long pipeline filling/draining delays that degrade performance.</p><p>A simple approach to alleviate these issues is to break up the input data batch of each pipeline stage into multiple subsets. Instead of waiting for its entire input to be available, each layer can start processing as soon as a subset of its input samples are ready. This approach requires that the outermost loop of each layer is blocked by the same factor t b . It reduces on-chip buffer requirements and pipeline filling/draining delays to 1/t b . However, t b is constrained by the total batch size N b , which is small for inference tasks. It also sacrifices the weight reuse, as the weights must be fetched t b times (once per each data subset). Therefore, t b can only be moderate, around 4 to 8, leading to limited savings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternate layer loop ordering (ALLO) dataflow:</head><p>We propose a novel pipelining dataflow, called Alternate layer loop ordering (ALLO), that further reduces the pipeline delays and buffer requirements on top of breaking up the data batch. ALLO modifies the intermediate fmap data access patterns, in order to allow the next layer to start processing as soon as a subset of the fmaps within a single data sample are ready. For example, in Figure <ref type="figure" target="#fig_5">8</ref>, L-1 computes each ofmap sequentially. If the next layer (L-2) also sequentially accepts these data as its ifmaps, it can start processing after waiting for a single fmap rather than all fmaps. Since each ofmap generated by L-1 is immediately used as an ifmap by L-2, we only need to store a single fmap that is currently being processed, rather than all fmaps, as Figure <ref type="figure">9</ref> shows.</p><p>ALLO dataflow makes two adjacent layers in the pipeline segment access their shared intermediate fmaps with the same sequential pattern, in order to forward and buffer the fmaps in a finer granularity. However, because CONV and FC layers cannot have sequential accesses to both the ifmaps and ofmaps, it is only possible to apply ALLO to alternate pairs of adjacent layers in a pipeline segment. For example, in Figures <ref type="figure" target="#fig_5">8</ref> and<ref type="figure">9</ref>, while L-1 and L-2 can be optimized with ALLO, the ofmaps of L-2 must be updated multiple times with the sequentially accessed ifmaps, therefore they must be fully buffered, and cannot benefit from ALLO.</p><p>Loop transformation model for ALLO: Similar to BSD, ALLO can also be realized using loop transformation techniques. We notice that, sequentially accessing the i/ofmaps corresponds to putting the i/ofmap loop at the outer level, right below the top batch loop required by breaking up batches. Therefore, ALLO requires the adjacent layers in the pipeline segment to use alternate orders for the ifmap loop and the ofmap loop. To enforce exactly the same subsets, the loop blocking factors should also match. As shown in Figure <ref type="figure" target="#fig_5">8</ref>, L-1 and L-2 use alternate loop orders, having the ofmap loop and the ifmap loop at the outer level, respectively (denoted in red). They also use the same blocking factor 4. So every time L-1 will produce one fourth of the ofmaps and immediately forward them to L-2 to be consumed.</p><p>ALLO benefits: If two adjacent layers have matched outer i/ofmap loops with blocking factor t, ALLO reduces the pipeline filling/draining delays and the on-chip buffer capacity for intermediate fmaps both by a factor of t. These benefits are on top of breaking up the pipelined data batch. Since CONV and FC layers typically have hundreds of fmaps (N i , N o &gt; N b ), the savings from ALLO (t i , t o ) can be substantially higher than pipelining the batch (t b ).</p><p>Nevertheless, ALLO can only be applied to half of the pairs of adjacent layers in a pipeline segment. When there are ? layers in the segment and ?-1 intermediate fmap data, ALLO still requires to fully delay and buffer ? ?-1 2 ? of intermediate data. Segments with two layers are a special case; ALLO can optimize the intermediate fmap dataflow and require no fully delay or buffering.</p><p>Combining ALLO and BSD: ALLO is compatible with the BSD optimization for intra-layer parallelism. ALLO orchestrates the dataflow between layers, while BSD is applied within a layer. Moreover, ALLO requires specific orders and blocking factors within the top-level loops for off-chip access (Figure <ref type="figure" target="#fig_5">8</ref>), while BSD adds an additional loop level below the top level (Figure <ref type="figure" target="#fig_4">7</ref>) for on-chip data rotation.</p><p>Combining ALLO and BSD enables higher savings in onchip buffer capacity. In fact, BSD helps most with the half of layers in the pipeline segment that cannot use ALLO. For these layers, the intermediate fmap data must be fully buffered on-chip. BSD ensures no data duplication within these layers, so the required buffer capacity is minimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-Layer Pipelining for Complex NN DAGs</head><p>Recent NNs, such as ResNet <ref type="bibr" target="#b16">[17]</ref> and various LSTMs <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b44">45]</ref>, feature complex DAG structures that go beyond the single linear chain of layers in early NNs. To support inter-layer pipelining for such complex NN DAG structures, we first improve the allocation strategy, and then provide practical and general heuristics to optimize the spatial mapping of NN layers on the tiled accelerator.</p><p>2D region allocation: Prior designs used static 1D allocation strategies to divide on-chip engines into regions that process different layers <ref type="bibr" target="#b43">[44]</ref>. Each layer gets one or more columns in the 2D array of NN engines, and fmap data flow in the horizontal direction through all layers. This allocation strategy is not sufficient for NN DAG structures with complex data forwarding patterns. Instead, we propose a 2D zig-zag allocation strategy shown in Figure <ref type="figure" target="#fig_8">10(a)</ref>. Regions are folded into the next row when they cannot fit in the remaining space in the current row (e.g., R1 and R4). This 2D allocation strategy has two major advantages. First, it is more fine-grained than 1D allocation and allows to more accurately size the regions to match the computation needs of the layers. Hence, the resources are better utilized and the latencies of pipeline stages are equalized. Second, when the fmap data are forwarded to non-adjacent regions (e.g., R0 to R3), 2D allocation results in shorter distances across the NoC.</p><p>Spatial layer mapping heuristics: In complex NN DAG structures, a layer can have multiple predecessor and/or successor layers. Correspondingly, a region may need to receive input data from both on-chip and off-chip sources. Output data may also need to be forwarded to multiple destinations, and possibly also stored back to memory. Hence, it is no longer trivial to determine which subset of layers should be mapped concurrently for pipeline execution (segment selection), and how to map layers within a segment to available   regions (region mapping). We propose the following practical heuristics to prune the design space that the compiler has to consider when optimizing layer pipelining for a wide set of NNs (see Section 4.2).</p><formula xml:id="formula_8">R1 R2 R3 R0 1 ? 1 Conv 3 ? 3 Conv 1 ? 1 Conv 1 ? 1 Conv + + R1 R2 R3 R0 1 ? 1 Conv 3 ? 3 Conv 1 ? 1 Conv 1 ? 1 Conv + R5 R3 R2 R1 R4 R6 R0 1 ? 1 Conv 1 ? 1 Conv 3 ? 3 Pool 1 ? 1 Conv 3 ? 3 Conv 5 ? 5 Conv 1 ? 1 Conv R5 R3 R2 R1 R4 R6 R0 1 ? 1 Conv 1 ? 1 Conv 3 ? 3 Pool 1 ? 1 Conv 3 ? 3 Conv 5 ? 5 Conv 1 ? 1 Conv R2 R3 R1 R0 I-Gate F-Gate O-Gate ? ? + + ? ? FC ? ? tanh R2 R3 R1 R0 I-Gate F-Gate O-Gate ? + ? FC ? tanh<label>(</label></formula><p>Segment selection: The layers in an NN are considered in the DAG topological order to form pipeline segments. A layer may be added into a segment only if it shares some data dependencies with other layers in the segment. The data dependencies can be either receiving the output data from a layer in the segment as in the simple linear pipelining case, or sharing the input data fetched from off-chip memory with another layer in the segment. The later case is specific to complex DAGs with forks. For instance, the GoogLeNet inception module shown in Figure <ref type="figure" target="#fig_8">10</ref>(b) has four layers sharing the off-chip input data (R0, R1, R3, and R5). Spatially mapping them on-chip in a single segment allows us to fetch the shared data only once.</p><p>The output data of each layer should be either used exclusively by the other layers in the same segment, or directly stored back to the off-chip memory. There is little benefit to pack half of the consuming layers in the same segment as the producing layer, because the intermediate data still need to be stored back to memory for the other half. It is better to gather all the consuming layers into a new segment, if they cannot fit in the current one. This is the case for GoogLeNet in Figure <ref type="figure" target="#fig_8">10(b)</ref>. For LSTM cells, we relax this constraint and allow at most one consuming layer to be in a different segment (see Figure <ref type="figure" target="#fig_8">10(b) LSTM cell,</ref><ref type="figure" target="#fig_1">R2</ref>). This relaxation also helps with training as Section 3.4 will discuss.</p><p>Region mapping: We group ACT, POOL, and element-wise layers into the regions of their previous CONV or FC layers. These layers do not need to access any weights and typically have small computation loads. Hence they increase only slightly the compute and buffer needs for each region.</p><p>NN layers can be mapped to a single region if they form a linear dependency chain. The engine buffers in the region sequentially store the output fmap data of each layer in the chain, and make them directly available only to the single successor layer. In Figure <ref type="figure" target="#fig_8">10(b)</ref>, the R2 region of the LSTM cell has a linear chain of four layers (see blue arrows), where the three element-wise layers are merged with the FC F-gate.</p><p>The layers that use the same region sequentially can have only one dependency from a neighbor region. This input dependency determines the timing of this region relative to the other regions. In classical linear pipelining, each layer has a single dependency on its predecessor layer in the previous region. With complex DAGs, regions can form a linear chain or a tree of dependencies. We do not allow more than one neighbor dependency per region to avoid conflicting timing requirements that lead to pipeline stalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataflow Optimizations for NN Training</head><p>While Tangram is primarily designed for NN inference, the intra-layer and inter-layer dataflow optimizations can be applied similarly to NN training. The error backward propagation of CONV and FC layers can be formulated as new CONV or FC layers with different dimensions <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. The BSD optimization in Section 3.1 can be applied to better parallelize these backward layers. For inter-layer pipelining, training extends the NN DAG structure with more layers for backward propagation. Our improved inter-layer pipelining scheme in Section 3.3 can handle such complex DAG structures. ALLO from Section 3.2 can also be used for both forward and backward pipeline segments. The new backward layers have input data dependencies on the output fmaps of their corresponding forward layers. With the relaxed rule of allowing one consuming layer in a different segment, the forward portion of the NN DAG can still be well pipelined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tangram Implementation</head><p>The intra-layer and inter-layer dataflow optimizations in Tangram require only small hardware changes in tiled NN architectures. Their implementations are mostly software, including (a) a search tool that identifies the optimized parallelization schemes for each NN, and (b) a compiler that produces the code for the selected schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hardware Support</head><p>Existing tiled NN architectures support limited data forwarding within one layer and across layers <ref type="bibr" target="#b43">[44]</ref>. Similar to the prior design, all parallel dataflows in Tangram are statically scheduled. At runtime, each NN engine in the tiled accelerator simply executes the instructions generated by the compiler to access data from SRAM buffers and perform NN computations. The lack of dynamic scheduling greatly simplifies the hardware design.</p><p>To implement the intra-layer buffer sharing dataflow, we enhance the engine buffer controllers so that they can issue accesses to both the off-chip memory and the other on-chip engine buffers. Additional control logic orchestrates data rotation and computation skew according to the loop transformation model in Figure <ref type="figure" target="#fig_4">7</ref> and Equation <ref type="bibr" target="#b1">(2)</ref>. We also need to synchronize the data rotation among the engines to avoid stalls or data overwrites. We leverage the MEMTRACK primitive from ScaleDeep <ref type="bibr" target="#b43">[44]</ref>, which relies on the hardware buffer controllers to track whether the data has received enough updates before it can be read, and enough read accesses before it can be overwritten. Deadlock is eliminated by transferring the data in units of buffer lines and reserving a few free lines in each buffer. Load imbalance is not an issue as long as the hybrid parallelization partitions the data approximately uniformly <ref type="bibr" target="#b13">[14]</ref>.</p><p>The data forwarding needed for inter-layer pipelining is already available in tiled NN architectures <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b43">44]</ref>. In Tangram, ALLO forwards the intermediate data in a more fine-grained manner as soon as a subset of fmaps are ready. We use the same MEMTRACK primitive to synchronize the data at the necessary granularity. For complex NN DAGs, onchip fmaps can be forwarded to multiple destination regions, and one engine may require data from multiple sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataflow Design Space Exploration</head><p>There are a large number of design options for how to map a multi-layer NN on a tiled accelerator. Recent work has already established the need for hybrid parallelization for intra-layer processing <ref type="bibr" target="#b13">[14]</ref>. With inter-layer pipelining, we can either choose a deeper pipeline (longer segments) with fewer engines per layer, or use a shallower pipeline (shorter segments) and give each layer more engines and buffer capacity. For complex NN DAGs, there are additional tradeoffs for segment selection and mapping (see Section 3.3). In fact, the choice of pipeline depth reveals a tradeoff in performance and energy. Deeper pipelines avoid more intermediate offchip accesses, but reduce per-layer resources and likely make per-layer dataflow suboptimal. The pipeline filling/draining delays also increase with the number of layers per segment, leading to resource underutilization for deeper pipeline.</p><p>To manage these tradeoffs, we developed a search tool that explores different intra-layer and inter-layer dataflow schemes for a tiled accelerator. The tool takes the NN topology and the hardware specification as input. It supports common NN types including CNNs, DNNs, and LSTMs. It relies on well-known optimizations for the dataflow within each NN engine <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b45">46]</ref>. The tool generates a large number of configurations with different inter-layer pipelining, intra-layer parallelism, off-chip loop blocking, and on-chip dataflow. It uses the heuristics discussed in Section 3.3 to trim the design space. Our cost model is similar to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b45">46]</ref>. For each individual layer, on-chip and off-chip dataflows are exhaustively searched; across layers, we use a combination of dynamic programming and beam search in the layer topological order. We leverage this tool to compare parallel dataflow schemes for tiled architectures in Section 6. The tool is available at https://github.com/stanford-mast/nn_dataflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Code Generation</head><p>The Tangram compiler generates the code for the parallel dataflow selected by the search tool. In addition to the NN topology, the input to the compiler includes the pipeline segment partitioning, the region allocation for each segment, the hybrid intra-layer parallelization scheme, the BSD and ALLO information given as loop transformation models, and the array mapping and loop blocking for each single engine.</p><p>Our compiler focuses on the data exchanges between onchip buffers and the data transfers to/from off-chip memories. The parallel dataflow optimizations in Tangram do not change how the NN engine itself works once data are available in each local buffer. Hence, our compiler can be easily retargeted to tiled accelerators using different engines <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref>. Based on the dataflow schemes, the compiler combines the loop transformation models at different levels (ALLO, BSD, and single-engine loop blocking) to get the complete nested loop structure <ref type="bibr" target="#b45">[46]</ref>. Then, it generates the data access instructions according to the specific order dictated by the loop structure. The compiler also inserts necessary data synchronization primitives at certain points in the nested loops. The instructions are offloaded to the engine buffer controller, which notifies the PE array to start the computation when all data have been fetched <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methodology</head><p>Workloads: We evaluate Tangram using four state-of-theart CNNs from ImageNet ILSVRC, as well as two DNNs (multi-layer perceptrons, MLPs) and two LSTMs in medium (-M) and large (-L) scales, summarized in Table <ref type="table" target="#tab_3">2</ref>. The variety of characteristics in these NNs allows us to explore various tradeoffs in parallel dataflow optimizations. All CNNs have several hundreds of MBytes memory footprints. VGGNet has significantly larger layers than the others. GoogLeNet and ResNet have large numbers of layers. MLPs and LSTMs only contain FC layers and their sizes are dominated by the model weights, with very small fmaps. AlexNet, GoogLeNet, ResNet,  <ref type="figure" target="#fig_0">1</ref>.</p><p>We use a default batch size of 64, and also explore batch sizes from 1 to 256. Datacenter NN inference accelerators can use batch sizes as high as 200 <ref type="bibr" target="#b20">[21]</ref>.</p><p>System models: We model the NN engine shown in Figure <ref type="figure">3</ref> after Eyeriss <ref type="bibr" target="#b6">[7]</ref>. Assuming 28 nm technology, the engine runs at 500 MHz. The PE area and power are scaled from <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b27">28]</ref>, assuming 0.004 mm 2 and 1 pJ for each 16-bit MAC.</p><p>We use <ref type="bibr">McPAT 1.3</ref> to model the area and power of the register files and the SRAM buffers at different capacities, and to calculate the characteristics of the PE array bus wires at different lengths <ref type="bibr" target="#b25">[26]</ref>. The NoC power is estimated to be 0.61 pJ/bit per hop <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b42">43]</ref>. The default configuration for the NN engine contains an 8 ? 8 PE array, with a 64 B register file per PE and a 32 kB shared SRAM buffer. We evaluate a tiled hardware architecture with a 100 mm 2 cost-effective chip area budget, with 16 ? 16 engines. This results in a total of 16384 PEs and 8 MB on-chip SRAM. The chip connects to four off-chip memory channels of LPDDR4-3200 chips, with a total of 24 Gb capacity and 25.6 GBps bandwidth. The power consumption is calculated using the Micron model <ref type="bibr" target="#b28">[29]</ref> and the parameters from datasheets <ref type="bibr" target="#b29">[30]</ref>.</p><p>We use performance in GOPS (giga ops per second) and energy efficiency in GOPS/W as the main comparison metrics. The performance captures both the impact of PE utilization across the NN engines and the impact of off-chip memory accesses. We also model the impact of data multicast latencies from SRAM buffers to PE registers. We aggressively assume separate buses for each data type and that each bus can multicast eight data elements per cycle <ref type="bibr" target="#b6">[7]</ref>. We have validated the performance model against cycle-accurate memory access trace simulations with zsim <ref type="bibr" target="#b34">[35]</ref> and DRAMSim2 <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We start with an overall comparison of Tangram against the baseline systems in Section 6.1, followed by a detailed analysis of the parallel dataflow optimizations in Section 6.2.</p><p>In Section 6.3 we further investigate other batch sizes and hardware configurations. Compared to the monolithic and baseline systems, Tangram improves on average the performance by 7.2? and 2.0?, and saves 41% and 45% system energy, respectively. Tangram sustains 6107.4 GOPS performance with 88 mm 2 at 28 nm, and achieves 439.8 GOPS/W energy efficiency including off-chip memories, or 936.4 GOPS/W for the chip itself. These numbers are on par with ScaleDeep, a recent NN accelerator in 14 nm <ref type="bibr" target="#b43">[44]</ref>. It demonstrates that optimizing intra-layer and inter-layer dataflow in software can provide energy efficiency improvements equivalent to two hardware technology node generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Overall Tangram Comparison</head><p>The monolithic engine spends significant energy on the array buses (up to 20%), and its performance is also limited primarily by the high latency of data multicast on these long buses. On the other hand, since all on-chip SRAM resources are aggregated into a single global buffer, it is quite effective at capturing reuse within and across layers, resulting in low energy consumption for off-chip memories. However, we do not partition the monolithic buffer to store the weights from multiple layers simultaneously on-chip, so model pinning is not supported (e.g., for MLP-M).</p><p>The two tiled architectures use multiple but smaller PE arrays, so the overheads of the array bus within each engine are reduced. However, the baseline suffers from the increased pressure for buffer capacity due to significant data duplication, and the delays of filling/draining pipelines. Both limit the effectiveness of parallelization. Therefore, it consumes substantially higher energy on the off-chip memory and the NoC, in particular for the CNNs with large intermediate fmap data. In contrast, Tangram uses the optimized BSD and ALLO dataflows for intra-layer and inter-layer parallelism, and supports pipelining of complex NN DAGs such as GoogLeNet and LSTMs. These optimizations reduce off-chip accesses, resulting in energy and performance benefits.</p><p>Notice that for MLPs and LSTMs, the energy efficiency is dominated by the weight access through the memory hierarchy in the monolithic engine. The two tiled architectures   support model pinning when the weights of all layers can fit in the on-chip SRAM, which fully eliminates off-chip weight access and greatly improves efficiency. This is the case for MLP-M and MLP-L in Figure <ref type="figure" target="#fig_9">11</ref>. Tangram uses dataflow optimizations to reduce the buffer capacity requirements, which enables additional model pinning opportunities such as for LSTM-M. The reduced pressure also allows MLP-L to use more optimized dataflow within each layer, and reduces its energy consumption in the engine buses and buffers. Finally, LSTM-L is too large to use model pinning even with Tangram optimizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Parallel Dataflow Analysis</head><p>In order to better understand the effectiveness of intra-layer and inter-layer dataflow optimizations proposed in Section 3, Figure <ref type="figure" target="#fig_12">12</ref> compares the energy consumption of Tangram against two systems without the two sets of optimizations, respectively. For intra-layer parallelism, all CNNs significantly benefit from the buffer sharing dataflow (BSD) due to their large fmap sizes. Eliminating duplication for the large fmaps greatly saves buffer spaces and improves data reuse within and across layers. BSD also helps with MLP-L and LSTM-M, because the reduced buffer pressure allows more optimized per-layer dataflow to be used with the pipeline schemes, decreasing the buffer access and array multicast energy cost (see Section 4.2). On the other hand, MLP-M and LSTM-L exhibit limited benefits from BSD. With layer pipelining optimizations, AlexNet, GoogLeNet, ResNet, and the two LSTMs enjoy substantial energy savings. Even with complex DAG structures, these NNs are able to use deeper pipeline segments with more layers in Tangram, since ALLO reduces the intermediate data buffering requirements. First, deeper pipelining eliminates more DRAM accesses of the intermediate data. Second, by allocating a smaller region for each layer, the NoC energy for intra-layer traffic is reduced. Third, by using fewer spatial PEs, more operations are co-located to increase PE-level data reuse, which also reduces the bus and buffer energy. In particular, simultaneously pipelining all layers in LSTM-M enables model pinning, greatly saving the energy by 4.6?.</p><p>Finally, VGGNet and the two MLPs do not benefit much from Tangram inter-layer optimizations, as they use only simple linear topologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Hardware and Batch Size Scaling</head><p>Figure <ref type="figure" target="#fig_14">13</ref>(a) shows the energy of Tangram relative to the baseline as we scale from 16 to 1024 engines (tiles). The benefits of Tangram increase when more hardware resources are available and are organized into more tiles. In this case, it becomes more difficult for a single layer to utilize all resources, and the inefficiencies of the baseline system mentioned in Section 2.4 become more critical. Tangram successfully addresses these issues, and achieves up to 67% energy saving when scaling to 1024 engines with 65536 PEs.</p><p>Figure <ref type="figure" target="#fig_14">13</ref>(b) shows the impact of batch sizes. The tiled baseline actually performs worse than the monolithic engine at large batches, because the inefficient resource utilization becomes more serious when the data size becomes larger. Tangram allows for more efficient data reuse and pipelining with more independent data samples, enabling higher energy improvements with larger batches.</p><p>For small batches, Tangram can still provide benefits, although the savings become smaller. When using batch size 1, which is common in latency-sensitive inference scenarios <ref type="bibr" target="#b12">[13]</ref>. Tangram slightly improves the energy efficiency for most NNs such as AlexNet by roughly 10% compared to the tiled baseline. GoogLeNet exhibits more significant energy saving because multiple layers in the pipeline segment share the input, and LSTM-M benefits from model pinning with Tangram which eliminates the dominant weight access (not shown in the figure).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>NN accelerators: The importance of NNs has motivated a number of accelerators with 1D inner-product engines <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> or 2D spatial PE arrays <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28]</ref> with low-precision arithmetics and small control overheads. Tangram uses a similar architecture as tiled accelerators <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>. Recent work has also prototyped NN accelerators on FPGAs [2, 25,    <ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, and leveraged ReRAM crossbars for in-situ analog dot-product operations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>A notable technique to further improve NN efficiency is to exploit sparsity in NN workloads in order to avoid redundant operations and reduce memory footprints. Recent work either dynamically pruned zero and small values <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">33]</ref>, or statically compressed the NN structures into sparse formats <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b48">49]</ref>. While Tangram focuses on dense NNs, its insights should be useful for scaling sparse NN accelerators as well. In fact, sparsity mostly affects the fine-grained parallelism by changing the dataflow inside the engines <ref type="bibr" target="#b30">[31]</ref>. The coarse-grained parallelism remains similar to the dense case. We will explore this issue in details in future work.</p><p>Intra-layer parallelism: DaDianNao was a tiled architecture with on-chip eDRAM <ref type="bibr" target="#b4">[5]</ref>; however the dataflow between tiles was neither thoroughly studied nor optimized. NN accelerators with 3D memory associate one NN engine to each of the 3D channels. Neurocube proposed a simple heuristic for NN partitioning across tiles <ref type="bibr" target="#b21">[22]</ref> and TETRIS extended it to hybrid partitioning schemes <ref type="bibr" target="#b13">[14]</ref>. They both suffered from the inefficiencies discussed in Section 2.4 and modeled in our baseline system. Our BSD proposal shares similar insights with non-uniform cache access (NUCA) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. It leverages application-specific knowledge of NNs to statically schedule computation skew and data rotation in order to optimally migrate data between tiles.</p><p>Inter-layer pipelining: ISAAC <ref type="bibr" target="#b35">[36]</ref> and PipeLayer <ref type="bibr" target="#b39">[40]</ref> used inter-layer pipelining in ReRAM-based accelerators, but did not consider dataflow optimizations. The fused-layer CNN accelerator sacrificed programmability to fuse the operations between layers in a fine-grained manner <ref type="bibr" target="#b1">[2]</ref>. Li et al. implemented an inter-layer pipeline with an optimization for FC layers similar to ALLO <ref type="bibr" target="#b24">[25]</ref>. But the design was limited to small CNNs and did not scale to large, complex networks such as ResNet and LSTMs. Shen et al. proposed to use heterogeneous engines to process different layers for higher utilization, but intermediate data were still written back to DRAM with no bandwidth and energy saving <ref type="bibr" target="#b37">[38]</ref>. ScaleDeep was a scale-out server architecture that mapped entire NNs for training <ref type="bibr" target="#b43">[44]</ref>. Its coarse-grained dataflow suffered from the inefficiencies as our baseline. To pipeline an NN across multiple GPUs, Jia et al. used dynamic programming to find the optimal strategies under a cost model <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The above designs mostly mapped the whole NNs to the systems, and none of them studied the tradeoff of pipeline depth as Tangram does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>This work focused on dataflow that improves coarse-grained parallelism on tiled NN accelerators. We presented optimizations for both intra-layer parallelism and inter-layer pipelining that decrease buffer requirements, reduce off-chip accesses, alleviate frequent stalls in pipelined execution, and target complex DAG patterns in recent NNs. These optimizations provide significant performance and energy advantages over existing tiled and monolithic designs. Moreover, the benefits of these parallel dataflow optimizations will increase as NNs become larger and more complex.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Complex NN DAG structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. CONV layer computation. N i ifmaps are convolved with different filter weights and accumulated to N o ofmaps. Computations are performed in batch size of N b .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Buffer sharing dataflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Intra-layer dataflow with output parallelization sharing ifmaps. I[b][i] means the ith ifmap of the bth sample. W[o][i] means the weights for the ith ifmap and oth ofmap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Loop transformation model for BSD, for output parallelization that shares ifmaps. Ifmap data rotation is orchestrated by the additional loop level i ?? 0 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Timing diagram of inter-layer pipelining with alternate layer loop ordering (ALLO). Orange, blue, and green boxes denote top level batch, ifmap, ofmap loops, respectively. Arrows indicate data dependencies and inter-layer data forwarding. The top level loop blocking of each layer is shown on the right. Matched fmap access patterns are denoted in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>b) Spatial mapping of complex DAGs. Left: GoogLeNet inception; middle: ResNet module; right: LSTM. Green, red, and blue arrows indicate data dependencies from/to off-chip, neighbor, and local regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Inter-layer pipelining support for complex NN DAGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11</head><label>11</label><figDesc>Figure11shows the energy and performance comparison over all evaluated NNs on the three systems with the same hardware resources. The monolithic engine (M) organizes all PEs in a single 128 ? 128 array, with a heavily banked global buffer of 8 MB. The baseline architecture (B) and Tangram (T) both tile the resources into 16 ? 16 smaller engines as shown in Figure4. They support both intra-layer parallelism and inter-layer pipelining. The baseline uses the techniques summarized in Section 2.4, while Tangram uses the optimizations presented in Section 3. All three systems support direct fmap reuse across adjacent layers without DRAM writeback as long as the intermediate fmaps fit on-chip. The two tiled architectures also allow model pinning.Compared to the monolithic and baseline systems, Tangram improves on average the performance by 7.2? and 2.0?, and saves 41% and 45% system energy, respectively. Tangram sustains 6107.4 GOPS performance with 88 mm 2 at 28 nm, and achieves 439.8 GOPS/W energy efficiency including off-chip memories, or 936.4 GOPS/W for the chip itself. These numbers are on par with ScaleDeep, a recent NN accelerator in 14 nm<ref type="bibr" target="#b43">[44]</ref>. It demonstrates that optimizing intra-layer and inter-layer dataflow in software can provide energy efficiency improvements equivalent to two hardware technology node generations.The monolithic engine spends significant energy on the array buses (up to 20%), and its performance is also limited primarily by the high latency of data multicast on these long buses. On the other hand, since all on-chip SRAM resources are aggregated into a single global buffer, it is quite effective at capturing reuse within and across layers, resulting in low energy consumption for off-chip memories. However, we do not partition the monolithic buffer to store the weights from multiple layers simultaneously on-chip, so model pinning is not supported (e.g., for MLP-M).The two tiled architectures use multiple but smaller PE arrays, so the overheads of the array bus within each engine are reduced. However, the baseline suffers from the increased pressure for buffer capacity due to significant data duplication, and the delays of filling/draining pipelines. Both limit the effectiveness of parallelization. Therefore, it consumes substantially higher energy on the off-chip memory and the NoC, in particular for the CNNs with large intermediate fmap data. In contrast, Tangram uses the optimized BSD and ALLO dataflows for intra-layer and inter-layer parallelism, and supports pipelining of complex NN DAGs such as GoogLeNet and LSTMs. These optimizations reduce off-chip accesses, resulting in energy and performance benefits.Notice that for MLPs and LSTMs, the energy efficiency is dominated by the weight access through the memory hierarchy in the monolithic engine. The two tiled architectures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Comparison between monolithic architecture, baseline tiled architecture, and Tangram. All three designs use 16384 PEs and 8 MB buffers. In the two tiled architectures resources are organized into 256 tiled engines (16 ? 16).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Comparison between Tangram and two systems that disable intra-layer and inter-layer optimizations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Batch sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Effectiveness of Tangram optimizations using different numbers of engines and batch sizes for AlexNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Data sharing characteristics of different NN coarsegrained parallelization schemes.</figDesc><table><row><cell>0</cell></row><row><cell>Mem</cell></row></table><note><p>* Partitioned inside one fmap. ? Partitioned across different fmaps.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Representative NNs for evaluation. Fmap and weight sizes are shown for the largest layer and the entire NN (largest/total) with 16-bit fixed-point data and batch 64.</figDesc><table><row><cell></cell><cell cols="2">CONVs FCs</cell><cell>Fmap size</cell><cell>Weight size</cell></row><row><cell>AlexNet [23]</cell><cell>10</cell><cell>3</cell><cell>17.7/ 95.4 MB</cell><cell>72/116 MB</cell></row><row><cell>VGGNet [39]</cell><cell>13</cell><cell>3</cell><cell cols="2">392.0/1841.7 MB 196/264 MB</cell></row><row><cell>GoogLeNet [42]</cell><cell>57</cell><cell>1</cell><cell>98.0/ 453.4 MB</cell><cell>2/ 13 MB</cell></row><row><cell>ResNet [17]</cell><cell>155</cell><cell>1</cell><cell>98.0/4318.5 MB</cell><cell>5/115 MB</cell></row><row><cell>MLP-M [9]</cell><cell>-</cell><cell>4</cell><cell>125/220 kB</cell><cell>1.5/2.7 MB</cell></row><row><cell>MLP-L [9]</cell><cell>-</cell><cell>4</cell><cell>188/376 kB</cell><cell>2.9/6.1 MB</cell></row><row><cell>LSTM-M [45]</cell><cell>-</cell><cell>4</cell><cell>64/ 576 kB</cell><cell>1/ 4 MB</cell></row><row><cell>LSTM-L [41]</cell><cell>-</cell><cell>16</cell><cell>125/4125 kB</cell><cell>4/61 MB</cell></row><row><cell cols="5">and both LSTMs have complex DAG structures as in Figure</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Engine 0,0 Engine 0,1 Engine 0,2 Engine 0,3 Engine 1,0 Engine 1,1 Engine 1,2 Engine 1,3 Engine 0,0 Engine 0,1 Engine 0,2 Engine 0,3 Engine 1,0 Engine 1,1 Engine 1,2 Engine 1,3</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors want to thank <rs type="person">Zhihao Jia</rs> for the insightful discussion, and the anonymous reviewers for their valuable comments. This work was supported by the <rs type="funder">Stanford Platform Lab, NSF</rs> grant <rs type="grantNumber">SHF-1408911</rs>, and <rs type="funder">SRC Center for Research on Intelligent Storage and Processing-in-memory (CRISP)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Ez3ERDM">
					<idno type="grant-number">SHF-1408911</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnvlutin: Ineffectual-Neuron-Free Deep Neural Network Computing</title>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Albericio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tayler</forename><surname>Hetherington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tor</forename><surname>Aamodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalie</forename><forename type="middle">Enright</forename><surname>Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fused-Layer CNN Accelerators</title>
		<author>
			<persName><forename type="first">Manoj</forename><surname>Alwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Managing Wire Delay in Large Chip-Multiprocessor Caches</title>
		<author>
			<persName><forename type="first">Bradford</forename><forename type="middle">M</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DianNao: A Small-Footprint High-Throughput Accelerator for Ubiquitous Machine-Learning</title>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengyong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">19th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DaDianNao: A Machine-Learning Supercomputer</title>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghui</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">47th International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Eyeriss: A Spatial Architecture for Energy-Efficient Dataflow for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="367" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Solid-State Circuits Conference (ISSCC)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="262" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Yu-Hsin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivienne</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits (JSSC)</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="127" to="138" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PRIME: A Novel Processing-in-Memory Architecture for Neural Network Computation in ReRAM-Based Main Memory</title>
		<author>
			<persName><forename type="first">Ping</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongpan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="27" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CirCNN: Accelerating and Compressing Deep Neural Networks Using Blockcirculant Weight Matrices</title>
		<author>
			<persName><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinru</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">50th International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>MICRO</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="395" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Temam</surname></persName>
		</author>
		<title level="m">42nd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
	<note>ShiDianNao: Shifting Vision Processing Closer to the Sensor</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neuflow: A Runtime Reconfigurable Dataflow Processor for Vision</title>
		<author>
			<persName><forename type="first">Cl?ment</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berin</forename><surname>Martini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Corda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Akselrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
	<note>Eugenio Culurciello, and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Fowers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalin</forename><surname>Ovtcharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Papamichael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Massengill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shlomi</forename><surname>Alkalay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Haselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Ghandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Heil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prerak</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Sapek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Woods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sitaram</forename><surname>Lanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">M</forename><surname>Caulfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<title level="m">45th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
	<note>A Configurable Cloud-Scale DNN Processor for Real-Time AI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TETRIS: Scalable and Efficient Neural Network Acceleration with 3D Memory</title>
		<author>
			<persName><forename type="first">Mingyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="751" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EIE: Efficient Inference Engine on Compressed Deep Neural Network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reactive NUCA: Near-Optimal Block Placement and Replication in Distributed Caches</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring Hidden Dimensions in Parallelizing Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Conference on Systems and Machine Learning (SysML)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Luc Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><surname>Omernick</surname></persName>
		</author>
		<title level="m">44th International Symposium on Computer Architecture (ISCA)</title>
		<editor>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><surname>Walter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Walter</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eric</forename><surname>Wilcox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Doe</forename><surname>Hyun Yoon</surname></persName>
		</editor>
		<meeting><address><addrLine>Bo Tian, Horia Toma, Erick Tuttle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>Datacenter Performance Analysis of a Tensor Processing Unit</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neurocube: A Programmable Digital Neuromorphic Architecture with High-Density 3D Memory</title>
		<author>
			<persName><forename type="first">Duckhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeha</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sek</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudhakar</forename><surname>Yalamanchili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saibal</forename><surname>Mukhopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="380" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ima-geNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep Learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A High Performance FPGA-based Accelerator for Large-Scale Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xitian</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuegong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th International Conference on Field Programmable Logic and Applications (FPL)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">McPAT: An Integrated Power, Area, and Timing Modeling Framework for Multicore and Manycore Architectures</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jung</forename><surname>Ho Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">D</forename><surname>Strong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">B</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">42nd International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="469" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cambricon: An Instruction Set Architecture for Neural Networks</title>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="393" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">FlexFlow: A Flexible Dataflow Accelerator Architecture for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhe</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="553" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">TN-41-01: Calculating Memory System Power for DDR</title>
		<ptr target="https://www.micron.com/support/tools-and-utilities/power-calc" />
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Micron Technology Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m">Mobile LPDDR4 SDRAM: 272b: x64 Mobile LPDDR4 SDRAM Features</title>
		<imprint>
			<publisher>Micron Technology Inc</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Angshuman</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minsoo</forename><surname>Rhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Puglielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangharajan</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brucek</forename><surname>Khailany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Emer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="27" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Memory-Centric Accelerator Design for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Maurice</forename><surname>Peemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Arnaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henk</forename><surname>Mesman</surname></persName>
		</author>
		<author>
			<persName><surname>Corporaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st International Conference on Computer Design (ICCD)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Enabling Low-Power, Highly-Accurate Deep Neural Network Accelerators</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Reagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Whatmough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Adolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saketh</forename><surname>Rama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunkwang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sae</forename><surname>Kyu Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<meeting><address><addrLine>Minerva</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DRAM-Sim2: A Cycle Accurate Memory System Simulator</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliott</forename><surname>Cooper-Balis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Architecture Letters</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="16" to="19" />
			<date type="published" when="2011-01">2011. Jan 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ZSim: Fast and Accurate Microarchitectural Simulation of Thousand-core Systems</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">40th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="475" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ISAAC: A Convolutional Neural Network Accelerator with In-situ Analog Arithmetic in Crossbars</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Nag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Paul Strachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Stanley</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">43rd International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From High-level Deep Neural Models to FPGAs</title>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jongse</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Amaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyung</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenkai</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asit</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><surname>Esmaeilzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Maximizing CNN Accelerator Efficiency Through Resource Partitioning</title>
		<author>
			<persName><forename type="first">Yongming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mechael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Milder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="535" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<imprint>
			<date type="published" when="2014-09">2014. Sept 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PipeLayer: A Pipelined ReRAM-Based Accelerator for Deep Learning</title>
		<author>
			<persName><forename type="first">Linghao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="541" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th International Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jenga: Sotware-Defined Cache Hierarchies</title>
		<author>
			<persName><forename type="first">Po-An</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ScaleDeep: A Scalable Compute Architecture for Learning and Evaluating Deep Networks</title>
		<author>
			<persName><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subarno</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashok</forename><surname>Jagannathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajaya</forename><surname>Durg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheemanth</forename><surname>Nagaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anand</forename><surname>Raghunathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="13" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Burton Rister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Bhagdikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahar</forename><surname>Kvatinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ardavan</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Horowitz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04209</idno>
		<title level="m">A Systematic Approach to Blocking Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2016-06">2016. Jun 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Scalpel: Customizing DNN Pruning to the Underlying Hardware Parallelism</title>
		<author>
			<persName><forename type="first">Jiecao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lukefahr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Palframan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Dasika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reetuparna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Mahlke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">44th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="548" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">23rd International Symposium on Field-Programmable Gate Arrays (FPGA)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cambricon-X: An Accelerator for Sparse Neural Networks</title>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zidong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoli</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunji</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">49th International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
