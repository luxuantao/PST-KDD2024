<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RGB-T Salient Object Detection via Fusing Multi-level CNN Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
							<email>qzhang@xidian.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">with Center for Complex Systems</orgName>
								<orgName type="department" key="dep3">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Electronic Equipment Structure Design</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">with Center for Complex Systems</orgName>
								<orgName type="department" key="dep3">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Electronic Equipment Structure Design</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nianchang</forename><surname>Huang</surname></persName>
							<email>nchuang@stu.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">with Center for Complex Systems</orgName>
								<orgName type="department" key="dep3">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Electronic Equipment Structure Design</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lin</forename><surname>Yao</surname></persName>
							<email>lyao@stu.xidian.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">with Center for Complex Systems</orgName>
								<orgName type="department" key="dep3">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Electronic Equipment Structure Design</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">are with Center for Complex Systems</orgName>
								<orgName type="institution">Xidian Uni-versity</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Caifeng</roleName><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">with Center for Complex Systems</orgName>
								<orgName type="department" key="dep3">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Electronic Equipment Structure Design</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">are with Center for Complex Systems</orgName>
								<orgName type="institution">Xidian Uni-versity</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Ministry of Education</orgName>
								<orgName type="department" key="dep2">with Center for Complex Systems</orgName>
								<orgName type="department" key="dep3">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">Key Laboratory of Electronic Equipment Structure Design</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Philips Research</orgName>
								<address>
									<settlement>Eindhoven</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
							<email>caifeng.shan@philips.com</email>
							<affiliation key="aff2">
								<orgName type="department">School of Mechano-Electronic Engineering</orgName>
								<orgName type="laboratory">are with Center for Complex Systems</orgName>
								<orgName type="institution">Xidian Uni-versity</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an Shaanxi</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RGB-T Salient Object Detection via Fusing Multi-level CNN Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">78F7A0E5D26F207DBAD6BBB890171496</idno>
					<idno type="DOI">10.1109/TIP.2019.2959253</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959253, IEEE Transactions on Image Processing JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING 1 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959253, IEEE Transactions on Image Processing JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING 2 This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959253, IEEE Transactions on Image Processing JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING 3</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGB-induced salient object detection has recently witnessed substantial progress, which is attributed to the superior feature learning capability of deep convolutional neural networks (CNNs). However, such detections suffer from challenging scenarios characterized by cluttered backgrounds, low-light conditions and variations in illumination. Instead of improving RGB based saliency detection, this paper takes advantage of the complementary benefits of RGB and thermal infrared images. Specifically, we propose a novel end-to-end network for multimodal salient object detection, which turns the challenge of RGB-T saliency detection to a CNN feature fusion problem. To this end, a backbone network (e.g., VGG-16) is first adopted to extract the coarse features from each RGB or thermal infrared image individually, and then several adjacent-depth feature combination (ADFC) modules are designed to extract multi-level refined features for each single-modal input image, considering that features captured at different depths differ in semantic information and visual details. Subsequently, a multi-branch group fusion (MGF) module is employed to capture the crossmodal features by fusing those features from ADFC modules for a RGB-T image pair at each level. Finally, a joint attention guided bi-directional message passing (JABMP) module undertakes the task of saliency prediction via integrating the multi-level fused features from MGF modules. Experimental results on several public RGB-T salient object detection datasets demonstrate the superiorities of our proposed algorithm over the state-of-the-art approaches, especially under challenging conditions, such as poor illumination, complex background and low contrast. Index Terms-RGB-T salient object detection, Adjacent-depth feature combination, Multi-branch group fusion, Joint attention guided bi-directional message passing I. INTRODUCTION S ALIENT object detection aims to identify the most visu- ally distinctive objects or regions in an image, and has attracted lots of attention in recent years. As a preprocessing step, salient object detection plays a critical role in many computer vision tasks, including visual tracking [1], [2], recognition [3], [4], content based image compression [5], [6], image fusion [7], [8] and so on. While numerous salient</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>object detection methods have been presented <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b24">[25]</ref>, most of them are designed for RGB images only, which may fail to distinguish salient objects from backgrounds when being exposed to challenging conditions, such as poor illumination, complex background, and low contrast. To address such issues, we advocate a multi-modal salient object detection method by fusing RGB and thermal infrared (RGB-T) images considering the popularity of thermal infrared sensors. More specifically, we present an end-to-end RGB-T salient detection model by using the recently developed deep convolutional neural networks (CNNs).</p><p>Unlike RGB cameras, thermal infrared cameras are a kind of passive sensors that capture the thermal infrared radiation emitted by all objects with a temperature above absolute zeros, meaning that thermal infrared images are invariant to illumination conditions <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>. As a result, when applied to salient object detection, thermal infrared images are likely to provide additional saliency cues to boost the saliency detection performance. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the validity of integrating RGB-T images for salient object detection under some challenging conditions.</p><p>In fact, RGB-T images have shown significant superiorities over RGB images in many computer vision tasks, such as face recognition <ref type="bibr" target="#b28">[29]</ref> and video surveillance <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Motivated by that, a few works have already exploited RGB-T images to boost the saliency detection performance. For example, Li et al. <ref type="bibr" target="#b26">[27]</ref> presented a robust salient object detection method based on multi-task manifold ranking with cross-modality consistency. Although the experimental results demonstrated its performance superiority over the traditional RGB-induced saliency detection methods, using low-level hand-crafted features might be a bottleneck for further performance improvement in <ref type="bibr" target="#b26">[27]</ref>. In <ref type="bibr" target="#b27">[28]</ref>, a deep CNNs based RGB-T salient object detection method was developed, in which the saliency map of each modality, i.e., RGB and thermal infrared, was independently induced at the first stage. Afterwards, the saliency map for each RGB or thermal infrared image was first independently induced by using the deep CNNs. Then the saliency maps of these two modalities were fused to derive the final saliency map. However, such a fusion at the saliency map level does not seem to well explore the complementary information/features across RGB and thermal images. In addition, the deep CNNs in <ref type="bibr" target="#b27">[28]</ref> were pre-trained for image classification on ImageNet dataset <ref type="bibr" target="#b30">[31]</ref>, rather than for salient object detection, meaning that the saliency cues might not be well explored.</p><p>In this paper, rather than integrating RGB-T information  <ref type="bibr" target="#b15">[16]</ref>; (d) Saliency maps induced from the thermal images by <ref type="bibr" target="#b15">[16]</ref>; (e) Saliency maps induced from the RGB-T images by <ref type="bibr" target="#b26">[27]</ref>; (f) Ground truth. The saliency maps in (c), (d) and (e) clearly demonstrate that thermal infrared images can provide complementary saliency cues for RGB images under challenging scenes with poor illumination (the first row) or complex background (the second row).</p><p>only at the saliency map level <ref type="bibr" target="#b27">[28]</ref>, which might be already too late, we propose a novel end-to-end deep neural network for RGB-T salient object detection by fusing multi-modal information at various stages. The proposed method turns the challenging RGB-T salient object detection into a CNN feature fusion problem, which covers the following three subproblems: 1) How to effectively extract the single-modal features from the input RGB or thermal infrared images; 2) How to fuse the extracted multi-modal features in a comprehensive way; and 3) How to infer the final saliency map using the fused features.</p><p>To address the first problem, we adopt a backbone network (i.e., VGG-16 net <ref type="bibr" target="#b31">[32]</ref> or Res-Net <ref type="bibr" target="#b32">[33]</ref>) to extract the features from each single-modal input image. Afterwards, an adjacentdepth features combination (ADFC) module is employed to capture the multi-level features of single-modal images, considering that different-depth features capture varieties of semantic information and fine visual details.</p><p>With respect to the second problem, motivated by the group convolution in <ref type="bibr" target="#b33">[34]</ref>, a multi-branch group fusion (MGF) module is put in place to fuse the features of RGB-T image pairs, which consists of two branches at each level. One branch contains several paths via group convolutions to reduce the network parameters while the other branch has just one path to capture the wholly cross-modal features. As a result, MGF module is expected to capture the cross-modal features between RGB and thermal infrared images effectively but at a considerably lower computational complexity.</p><p>For the third problem, we introduce a joint attention guided bi-directional message passing (JABMP) module for saliency prediction via integrating the multi-level fused features obtained from MGF modules. With the proposed JABMP module, high-level semantic information in deeper layers will be passed to shallower layers, and low-level spatial details contained in shallower layers will also be passed to deeper layers. Accordingly, the cross-level complementary information among the fused features will be well captured by using the proposed module. Moreover, a joint channel-spatial attention (JCSA) block, different from the gate function in <ref type="bibr" target="#b34">[35]</ref>, is adopted to control the message passing in JABMP module. By using JCSA, some important features with higher channel attention as well as spatial attention will be selected and propagated to the next level, and some superfluous features will be suppressed during the message passing, which will enhance the feature discriminability for the final RGB-T saliency prediction.</p><p>In summary, the main contributions of this work are as follows:</p><p>1) An end-to-end CNN based RGB-T salient object detection method is proposed, which achieves the state-of-the-art performance on several RGB and thermal infrared datasets, including <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. To the best of our knowledge, it is the first end-to-end CNN established for RGB-T salient object detection.</p><p>2) An ADFC module is dedicated to extract each singlemodal image features. By using multiple ADFC modules, multi-level features of input images containing rich spatial details as well as semantic information, rather than one specific level of features as in the traditional methods <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>, are extracted for the subsequent fusion and saliency prediction modules.</p><p>3) A MGF module, instead of the simple concatenation, is presented to capture the cross-modal complementary information between each RGB-T image pairs and reduce the number of network parameters.</p><p>4) In order to effectively capture the cross-level complementary information among the fused features, a JABMP module is employed for the final saliency prediction in the proposed network. Especially, a JCSA block, rather than a gate function as in <ref type="bibr" target="#b34">[35]</ref>, is adopted to control the message passing in the proposed bi-directional message passing module.</p><p>The rest of the paper is organized as follows. Section II briefly reviews some related work, and Section III illustrates the proposed multi-modal salient object detection model in detail. Experimental results and conclusions are given in Section IV and Section V, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RGB salient object detection</head><p>Over the past two decades, a considerable number of RGB salient object detection methods have been developed [11]- <ref type="bibr" target="#b24">[25]</ref>. Early salient object detection methods utilized low-level hand-crafted features with specific statistical priors, such as color contrast [11], <ref type="bibr" target="#b11">[12]</ref>, object prior <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and background prior <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, to model and approximate human saliency. A complete survey on RGB salient object detection methods is beyond the scope of this paper and we refer the readers to a recent survey paper <ref type="bibr" target="#b17">[18]</ref> for details.</p><p>Recently, to extract more sophisticated features, tremendous deep learning based saliency detectors have been proposed <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[39]</ref>- <ref type="bibr" target="#b40">[41]</ref>, and achieved substantially better performance than those previous methods. For example, Lee et al. <ref type="bibr" target="#b22">[23]</ref> proposed to first encode low-level distance map and high-level sematic features of deep CNNs to form a new feature vector, and then evaluate saliency by a multi-level fully connected neural network classifier. Hou et al. <ref type="bibr" target="#b23">[24]</ref> presented a salient object detection method by introducing a series of short connections between shallower and deeper side-output layers. Zhang et al. <ref type="bibr" target="#b24">[25]</ref> introduced a generic aggregating multi-level convolutional feature framework for salient object detection, which first integrated multi-level feature maps into multiple resolutions and then adaptively learned to combine these feature maps at each resolution to predict the saliency maps. In <ref type="bibr" target="#b38">[39]</ref>, two pooling based modules, i.e., a global guidance module (GGM) and a feature aggregation module (FAM), aided to improve the performance for salient object detection. A novel recurrent residual refinement network (R3Net) equipped with residual refinement blocks (RRBs) was presented in <ref type="bibr" target="#b39">[40]</ref> to detect salient regions from an input image more accurately.</p><p>However, most of these studies focus on the RGB salient object detection. Under some challenging conditions, such as poor illumination, complex background or low contrast, these RGB-induced salient object detection models may fail to distinguish salient objects from backgrounds, as shown in Fig. <ref type="figure" target="#fig_0">1 (c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. RGB-D salient object detection</head><p>As a departure from RGB images, depth images provide affluent spatial structures and 3D information for salient objects and backgrounds, which benefit the salient object detection <ref type="bibr" target="#b41">[42]</ref>. Therefore, RGB-D salient object detection has attracted much attention in recent years. So far, a variety of RGB-D salient object detection models have been presented to boost the performance of saliency detection <ref type="bibr" target="#b41">[42]</ref>- <ref type="bibr" target="#b47">[48]</ref>. For examples, Chen et al. <ref type="bibr" target="#b41">[42]</ref> presented a progressively complementarityaware fusion network for RGB-D salient object detection by adding the cross-level complementarity in the process of cross-modal fusion. In <ref type="bibr" target="#b44">[45]</ref>, two saliency maps were first pre-deduced from the source RGB and depth images via a two-streamed CNN , respectively. Then a switch map was generated by using a saliency fusion module to adaptively fuse the two saliency maps. In <ref type="bibr" target="#b43">[44]</ref>, the depth information was first enhanced by utilizing contrast prior into a CNNs based architecture. Then the enhanced depth cues were integrated with RGB features for salient object detection by using a fluid pyramid integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RGB-T salient object detection</head><p>Recently, considering their complementary benefits, a few works also attempted to exploit RGB-T images to boost the saliency detection performance. For example, Li et al. <ref type="bibr" target="#b26">[27]</ref> proposed a robust multi-task manifold ranking based RGB-T salient object detection method with cross-modality consistency. Ma et al. <ref type="bibr" target="#b27">[28]</ref> presented an adaptive RGB-T saliency detection method by learning multiscale deep CNN features and SVM regressors. In <ref type="bibr" target="#b48">[49]</ref>, a novel collaborative graph learning algorithm was presented for RGB-T image saliency detection. Specifically, superpixels were taken as graph nodes, and hierarchical deep features were collaboratively used to jointly learn the graph affinity and node saliency in a unified optimization framework.</p><p>Although CNN based RGB-T salient object detection algorithms are not well investigated yet, a large number of deep neural networks with RGB-T inputs have been presented for some other computer vision or image processing tasks, such as pedestrian detection <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>, image fusion <ref type="bibr" target="#b49">[50]</ref>, object tracking <ref type="bibr" target="#b50">[51]</ref>- <ref type="bibr" target="#b52">[53]</ref>. For example, Wagner et al. <ref type="bibr" target="#b36">[37]</ref> presented an RGB-T pedestrian detection method by fusing information with CNNs, where information from the RGB and thermal infrared images was integrated via an earlyfusion and a late-fusion based CNN architecture. In addition to early-fusion (also called low-level fusion in <ref type="bibr" target="#b37">[38]</ref>) and latefusion (also called high-level fusion in <ref type="bibr" target="#b37">[38]</ref>), another two CNN architectures for information fusion, i.e., middle-level fusion and confidence-level fusion, were explored for RGB-T pedestrian detection. Their experimental results revealed that the middle-level fusion provides the best performance among the four fusion models on RGB-T pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED RGB-T SALIENT OBJECT DETECTION MODEL WITH MULTI-LEVEL CNN FEATURE FUSION</head><p>In this section, we will discuss the proposed RGB-T salient object detection model in detail. Fig. <ref type="figure">2</ref> shows the diagram of the proposed network, which is composed of three components: single-modal image feature extraction, multi-modal image feature fusion, and saliency map prediction. These will be described in detail in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-level feature extraction for each single-modal image using ADFC modules</head><p>The RGB-T salient object detection network may be incorporated with any basic network, such as VGG-16 net <ref type="bibr" target="#b31">[32]</ref> and Res-Net <ref type="bibr" target="#b32">[33]</ref>. Here, we employ the VGG-16 net as the backbone network to carry out the feature extraction from RGB and thermal infrared branches, which is well known for its elegance, simplicity, and good generalization. For saliency detection, we make two modifications on the VGG-16 net</p><formula xml:id="formula_0">JCSA3 32×32×128 JCSA2 64×64×128 MGF3 32×32×128 MGF2 64×64×128 JCSA1 128×128×128 MGF1 128×128×128 ADFC3 32×32×128 ADFC2 64×64×128 ADFC1 128×128×128 RGB Conv1-2 256×256×64 Conv2-2 128×128×128 Conv3-3 64×64×256 Conv4-3 32×32×512 Conv5-3 16×16×512 ADFC3 32×32×128 ADFC2 64×64×128 ADFC1 128×128×128 Thermal Conv1-2 256×256×64 Conv2-2 128×128×128 Conv3-3 64×64×256 Conv4-3 32×32×512 P3 256×256×1 P2 256×256×1 P1 256×256×1 P0 256×256×1</formula><p>Single-modal image feature extraction Cross-modal feature fusion JABMP Saliency map prediction</p><formula xml:id="formula_1">Conv5-3 16×16×512 1 1 H 1 2 H 1 3 H 2 1 H 2 2 H 2 3 H 1 1 F 1 0 F 1 2 F 1 3 F 1 4 F 2 1 F 2 0 F 2 2 F 2 3 F 2 4 F 1 H 2 H 3 H 1 H 1 2 H 2 3 H 3</formula><p>Fig. <ref type="figure">2</ref>. The overall architecture of our proposed RGB-T salient object detection. Each colorful box is considered as a feature block. The solid arrows between blocks indicate the information streams. RGB-T input images are assumed to have been well registered in advance and have been rescaled to the fixed sizes (e.g., 256 × 256 in this paper). The RGB image and thermal infrared image are first fed into the backbone network, i.e., VGG-16 net, respectively. Based on that, multi-level features are further generated for each RGB or thermal infrared image by using the proposed ADFC modules. Then the features from ADFC modules that correspond to the same level in the two branches are fused by using MGF module. After that, JABMP module is performed on the fused multi-level features to obtain the final saliency map, where a JCSA block is adopted to control the message passing.</p><p>i.e.,removal of all the fully-connected layers and skip of the pool5 layer to maintain more spatial information for the input image. The modified VGG-16 net includes five convolutional blocks.</p><p>After the RGB image or thermal infrared image is fed into the backbone network, features at different levels/depths are extracted for each single-modal input image, which capture various semantic information and visual details. Shallowerlevel features contain more visual details but lack some semantic information, while deeper levels of features carry more semantic information but are limited when it comes to details. Therefore, features from different levels in the backbone networks are complementary to each other.</p><p>In this work, we propose an adjacent-depth feature combination (ADFC) module to integrate the multi-level features of single-modal images. We select some middle-level features from the backbone networks for each single-modal input image by using multiple proposed ADFC modules to obtain multi-level features and reduce the burden of network parameters. More specifically, we first extract five layers of RGB or thermal infrared image features from different depths of VGG- </p><formula xml:id="formula_2">H n d = φ C Cat φ C F n d-1 ; θ n,1 d , 2 , φ C F n d ; θ n,2 d , 1 , φ D F n d+1 ; γ n d , 1/2 ; θ n,3 d , 1 ,<label>(1)</label></formula><p>where . As a result, the redundancy among F n d-1 ,F n d and F n d+1 are reduced from H n d by using the proposed ADFC module. Finally, it should be noted that the idea of ADFC is similar to those of the feature pyramid network (FPN) <ref type="bibr" target="#b54">[55]</ref> and the hierarchical feature integration mechanism (HIFI) <ref type="bibr" target="#b55">[56]</ref>. All of the three modules investigate the integration of multi-level features to improve the saliency detection performance. But in HIFI, the features from all convolutional layers of different levels are integrated. While, in ADFC and FPN, only the features from the last convolutional layers of different levels are integrated, considering that the features from the deepest layer of each level are the strongest <ref type="bibr" target="#b54">[55]</ref>. As a result, HIFI has much more to-be-learned parameters than ADFC and FPN, and thus has higher computational complexity. In addition, the integration of large numbers of features by HIFI will also introduce much more redundant information and degrade the subsequent saliency detection performance. In FPN, only the features from two adjacent levels (i.e., the current level and its deeper level) are integrated. Differently, in ADFC, features from three adjacent levels (i.e., the current level, its deeper level and its shallower level) are integrated. Accordingly, more spatial details will be captured by ADFC than by FPN, which will improve the subsequent salient detection results. This will be verified in the later experimental part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fusion of multi-modal image features using MGF modules</head><p>Given the features {H n d |d = 1, 2, 3; n = 1, 2} of RGB and thermal infrared images generated from ADFC modules, most of existing models capture the cross-modal features between the two modalities by first simply concatenating H 1 d and H 2 d at the same level d and then performing the Conv+ReLU operators on the concatenated features, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>. This may well capture the wholly cross-modal correlations among the concatenated features. However, some salient features from individual single-modal image may also be drowned in the concatenated features because of the large numbers of features, which will diminish the discriminability of the subsequent fused features <ref type="bibr" target="#b56">[57]</ref>. In addition, under the premise of multilevel features, direct concatenation may increase the network parameters, which is not desirable for the training of multimodal methods.</p><p>Such problems can be solved by using group convolution, which may date back to the AlexNet <ref type="bibr" target="#b57">[58]</ref> or even earlier and is supported by Caffe <ref type="bibr" target="#b58">[59]</ref>, PyTorch <ref type="bibr" target="#b59">[60]</ref>, and so on. The basic idea behind group convolution is split-transformmerge, similar to the Inception models <ref type="bibr" target="#b60">[61]</ref>- <ref type="bibr" target="#b62">[63]</ref>. In group convolution, the input features are first divided into a few small groups along the channel. Then, a set of regular 3×3 or 5×5 convolutions are performed on these small groups. All of the outputs from these small groups are concatenated as the final output. But different from Inception models, where each path may be carefully customized, group convolution shares the same topology among all the paths. More specifically, Xie et al. <ref type="bibr" target="#b33">[34]</ref> proposed to use stacked group convolutions in the process of transformation to reduce complexity and model sizes. Although group convolution may greatly reduce parameters, it can only capture the partly cross-modal correlations among the features within the same group, which may weaken the cross-modal correlations among all of the feature maps.</p><p>In this work, we propose a multi-branch group fusion (MGF) module to fuse the features H 1 d |d = 1, 2, 3 and</p><formula xml:id="formula_3">H 2 d |d = 1, 2, 3</formula><p>. MGF is expected to effectively capture the cross-modal features between RGB and thermal infrared images but at a considerably lower computational complexity.</p><p>As shown in Fig. <ref type="figure">5</ref>, the proposed MGF consists of two branches for feature fusion at each level. One branch (named multi-group fusion branch) has M (e.g., M=8 in this paper) paths via group convolutions to reduce the network parameters while the other branch (named single-group fusion branch) has just one path to capture the wholly cross-modal features as in the traditional fusion module in Fig. <ref type="figure" target="#fig_3">4</ref>. The two branches produce the same number of feature maps (e.g., 64 in this paper), so the number of the finally fused feature maps is doubled (e.g., 128 in this paper).</p><p>The details of the proposed MGF module are described as follows. In the multi-group fusion branch, the input single-modal features H adopt a ReLU activation function. Finally, the outputs from the M groups are concatenated to obtain the fused features H 1,d via the multi-group fusion branch. Mathematically, the multigroup fusion branch is expressed as</p><formula xml:id="formula_4">H 1,d = Cat Z 1 Cat H 1 d,1 , H 2 d,1 ; ϕ d,1 , ..., Z M Cat H 1 d,M , H 2 d,M ; ϕ d,M ,<label>(2)</label></formula><p>where Z m * , ϕ d,m denotes the stacked convolutions with ReLU activation function mentioned above, and ϕ d,m denotes the network parameters in the m-th path.</p><p>The single-group fusion branch in MGF module can be seen as a special case of the multi-group fusion branch with M = 1. Therefore, the single-group fusion branch can be simply expressed by</p><formula xml:id="formula_5">H 2,d = Z * Cat H 1 d , H 2 d ; ϕ * d ,<label>(3)</label></formula><p>where H 2,d is the d-th level of fused features from the single-group fusion branch, and Z * * , ϕ * d consists of two stacked convolution layers (a 1 × 1 convolutional layer with 64 channels followed by a 3 × 3 convolutional layer with 64 channels). Similarly, the two convolutional layers also have a ReLU activation function. ϕ * d denotes the network parameters for Z * .</p><p>The final fused features H d for the d-th level are obtained by simply concatenating H 1,d and, H 2,d i.e.,</p><formula xml:id="formula_6">H d = Cat H 1,d , H 2,d .<label>(4)</label></formula><p>As discussed above, MGF module can capture the wholly cross-modal correlations among the features of RGB-T images via the single-group fusion branch. As well, it can extract more salient features from each single-modal input image via the multi-group fusion branch. As a result, the proposed MGF module can potentially better capture the cross-modal features of RGB-T images than those exiting fusion methods <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref>. By using multiple MGF modules, different levels of fused features containing semantic information as well as visual details can be extracted for RGB-T salient object detection. More importantly, due to the employed group convolution, MGF module requires much fewer network parameters 2 than the traditional fusion method shown in Fig. <ref type="figure" target="#fig_3">4</ref>, which first concatenates H 1 d and, H 2 d and then performs a 1 × 1 convolutional layer with 128 channels and a 3 × 3 convolutional layer with 128 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Saliency map prediction using JABMP module</head><p>With multiple MGF modules, three levels of fused features {H d |d = 1, 2, 3} are obtained, which will be used to predict the final saliency map. A straightforward method is to perform the side output on each level H d independently, and then derive the final saliency map by adding a new convolutional layer 2 Assume that 128-channels of fused features are generated by using two sets of 128 channels of single-modal features. The number of parameters in traditional fusion method is a 1 = (128 + 128) × 128 × 1 × 1 + 128 × 128 × 3 × 3 = 180224, and the number of parameters in MGF is</p><formula xml:id="formula_7">a 2 = 8 × (128 + 128) /8 × 64/8 × 1 × 1 + 64/8 × 3 × 3 + 64/8 × 1 × 1 = 54080.</formula><p>For each level fusion, the number of parameters is reduced by about 130000.</p><p>1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959253, IEEE Transactions on Image Processing to fuse these side outputs of different levels. Although this method can detect salient objects with features at different levels, the inner correlations among different levels of features are missing. As a result, the prediction may not be optimal, and some post-processing may be further needed as in <ref type="bibr" target="#b63">[64]</ref>. To facilitate the interaction among multiple predictions, a series of connections from deeper side output layers to shallower ones were suggested in <ref type="bibr" target="#b23">[24]</ref>. This method only considered the information transmitted from deeper layers to shallower ones, but ignored the information flow from shallower layers to deeper ones. Thus, the deep side outputs still lack the lowlevel details contained in the shallow layers. For that, Zhang et al. <ref type="bibr" target="#b24">[25]</ref> proposed a bi-directional message passing module for salient object detection by concatenating feature maps from both high levels and low levels. However, their module just used simple concatenation operations to integrate multi-level features without considering their importance. As the multilevel features are not always equally useful for every input image, this aggregation method would lead to information redundancy. Considering that, a gated bi-directional message passing module was presented in <ref type="bibr" target="#b34">[35]</ref>, where a gate function was employed to transmit the useful features and suppress the superfluous features.</p><p>Inspired by the work in <ref type="bibr" target="#b34">[35]</ref>, we propose a joint attention guided bi-directional message passing (JABMP) module for saliency map prediction by effectively integrating the multilevel features from MGF modules. module can capture the cross-level complementarity among the fused features in two directions. With the proposed module, high-level semantic information in deeper layers will be passed to shallower layers and low-level spatial details contained in shallower layers will also be passed to deeper layers. As is well known, each layer has multiple channels of feature maps. But not all these channels are effective for saliency prediction. Similarly, in each feature map, features from different spatial positions may play different roles in salient object detection. Therefore, a joint channel-spatial attention (JCSA) block, instead of a gate function as in <ref type="bibr" target="#b34">[35]</ref>, is introduced to control the message passing in the proposed JABMP module. Fig. <ref type="figure">6</ref> illustrates the architecture of the proposed JABMP module, which consists of two directional connections. One is the bottom-up information stream, where the features from the current level and the weighted features from the previous level are integrated to produce the current level of attentive features via JCSA block. The other is the top-down information stream, where we hierarchically propagate the predictions from higher-level to lower-level to obtain more accurate side outputs. Next,we discuss each step in detail.</p><p>1) Bottom-up information stream with joint channelspatial attention: Given the multi-level fused features {H d |d = 1, 2, 3} from MGF modules, the attentive features at different levels are sequentially generated by using the introduced JCSA block. Mathematically, the process of the message passing from shallower layer to deeper layer is described by</p><formula xml:id="formula_8">H d = A H d + φ C H d-1 ; θ d , 2 , s, τ , d = 2, 3 A (H d , s, τ ) , d = 1 ,<label>(5)</label></formula><p>where C * , θ d, 2 denotes a 3 × 3 convolutional layer to ensure the adjacent-level features have the same number of channels (i.e., 128) and the same spatial resolutions. φ (•) is a ReLU activation. A ( * , s, τ ) is the joint attention function to weight the features. s = [s 1 , s 2 , ..., s Q ]</p><p>T ∈ R Q is a set of channel-wise weights, and τ ∈ R W ×H denotes the importance of each local spatial position in the feature maps. Q, W, and H represent the number of channels, width and length of the input features for the JCSA block, respectively. Detailed implementation of JCSA will be described as follows.</p><p>As shown in the bottom of Fig. <ref type="figure">6</ref>, the proposed JCSA block consists of a "Squeeze-and-Excitation" (SE) block <ref type="bibr" target="#b64">[65]</ref> and a "Spatial Attention" (SA) block <ref type="bibr">[66]</ref>. The SE block reflects the global channel-wise importance of each feature map by introducing some channel-dependent weights, and the SA block indicates the local spatial importance of features by introducing some position-dependent weights.</p><p>Suppose that the input features</p><formula xml:id="formula_9">H = [h 1 , h 2 , ..., h Q ] ∈ R W ×H×Q for JCSA contain Q channels of feature maps, and h q ∈ R W ×H is the q-th feature map . H = h 1 , h 2 , ..., h Q ∈ R W ×H×Q is the output of JCSA, i.e., H = A (H, s, τ ).</formula><p>Similar to that in <ref type="bibr" target="#b64">[65]</ref> , a global average pooling is first performed on H to generate a set of channel-wise statistical features</p><formula xml:id="formula_10">v = [v 1 , v 2 , ..., v Q ]</formula><p>T ∈ R Q in the SE block. Then two fully connected (FC) layers and a simple sigmoid activation function are performed on v, and a set of channel-wise weights s are obtained. The output H = h 1 , h 2 , ..., h Q ∈ R W ×H×Q of the SE block can be obtained by the following channel-wise multiplication</p><formula xml:id="formula_11">h q = s q × h q ,<label>(6)</label></formula><p>where s q is the q-th element of s, and h q is the q-th feature map in H .</p><p>The output H of the SE block is further fed into the subsequent SA block. More specifically, in the SA block, a 1 × 1 convolutional operation and a simple sigmoid activation function are performed on H to obtain the spatial weight map τ . Then the output of the SA block, i.e., the final output H = h 1 , h 2 , ..., h Q of JCSA block, is obtained by the element-wise product between each feature map in H and the spatial weights (or importance) , i.e.,</p><formula xml:id="formula_12">h q = τ • h q ,<label>(7)</label></formula><p>where • represents element-wise product, and h q is the q-th feature map in H.</p><p>As shown in Eq. ( <ref type="formula" target="#formula_11">6</ref>) and Eq. ( <ref type="formula" target="#formula_12">7</ref>), some important features with higher channel attention (or weights) as well as spatial attention (or weights) will be selected and transmitted to the next level, and some superfluous features will be suppressed in the bottom-up information stream by using the proposed </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JCSA SE block SA block</head><p>Fig. <ref type="figure">6</ref>. Illustration of the proposed JABMP module. This module consists of two directional connections. One is the bottom-up information stream and the other is the top-down information stream. In the bottom-up stream, the features from the current level and the weighted features from the previous level are integrated to produce the current level of attentive features via a joint channel-spatial attention (JCSA) block. In the top-down stream, the predictions from higher-level to lower-level are hierarchically propagated to obtain more accurate saliency detection results.</p><p>JCSA block. This will boost the discriminability of the fused features when predicting the saliency maps.</p><p>2) Top-down information stream: Having obtained the attentive features H d |d = 1, 2, 3 , the multiple side outputs {P d |d = 1, 2, 3} for different levels can be sequentially obtained in a deep-to-shallow manner, i.e., 3) Saliency map prediction: Let {S d |d = 1, 2, 3} denote the side output maps, and they can be computed by S d = σ (P d ). σ (•) is a sigmoid activation function.</p><formula xml:id="formula_13">P d =        C φ C D H d ; γ d , 1/2 d ; θ 1 d , 1 + P d+1 ; θ 2 d , 1 , d = 1, 2 C D H d ; γ d , (1/2) d ; θ 2 d , 1 , d = 3<label>(</label></formula><p>These side outputs are further fused to obtain the fusion output P 0 by using a 1 × 1 convolutional layer C * ; θ 0 , 1 i.e., P 0 = C Cat (P 1 , P 2 , P 3 ) ; θ 0 , 1 .</p><p>Thus, the fusion output map S can be computed by S 0 = σ (P 0 ), and we take S 0 as the final saliency map of our model. Then, the proposed model is trained end to end using the crossentropy loss L between the ground truth and the predicted results {S t |t = 0, 1, 2, 3}, which is defined as <ref type="bibr" target="#b66">[67]</ref> </p><formula xml:id="formula_15">L = -β 3 i=0 i,j G (i, j) log (S t (i, j)) -(1 -β ) 3 i=0 i,j (1 -G (i, j)) log (1 -S t (i, j)) ,<label>(10)</label></formula><p>where G (i, j) ∈ {0, 1} is the label of the pixel (i, j) in the ground truth, and S t (i, j) is the probability of pixel (i, j) belonging to the foreground in the predicted saliency map S t . To increase the detection accuracy for salient objects of various sizes, a class-balancing weight β is used to balance the foreground and background, and is set to the ratio of the number of background pixels to that of all the pixels in the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first describe the experimental setup and the employed evaluation metrics. Afterwards, we compare the proposed RGB-T salient object detection model with the state-of-the-art (SOTA) methods on some publicly available datasets. Finally, we perform several sets of ablation experiments to show the validity of each component in our proposed saliency detection model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup 1) Datasets:</head><p>We train and evaluate our approach on three publicly available datasets, including RGB-thermal dataset <ref type="bibr" target="#b26">[27]</ref>, Grayscale-thermal dataset <ref type="bibr" target="#b25">[26]</ref> and MSRA-B dataset <ref type="bibr" target="#b67">[68]</ref>.</p><p>RGB-thermal dataset <ref type="bibr" target="#b26">[27]</ref>  and richness of data scenarios, such as multiple salient objects, low illumination, and similar appearances.</p><p>Grayscale-thermal dataset <ref type="bibr" target="#b25">[26]</ref> includes 25 aligned grayscale-thermal video pairs with high diversity. However, this dataset is collected for object tracking rather than for image salient object detection. For object tracking, only moving objects need to be detected, while for salient object detection, stationary objects may also be seen as salient ones. Moreover, moving objects sometimes are too small or occluded so that they may not be salient objects. Considering these, we selected 843 pairs of Grayscale-thermal images from the dataset in our experiments, which can be divided into two sets, i.e., a pedestrian set with 537 frame pairs from 10 aligned video pairs and a car set with 306 frame pairs from another 10 aligned video pairs.</p><p>MSRA-B dataset <ref type="bibr" target="#b67">[68]</ref> contains 5000 RGB images (2500 images as training set and 2500 images as testing set) and is widely used for single-modal image salient object detection.</p><p>2) Training: We start with the backbone VGG-16 nets in our proposed model, whose convolutional layers are initialized with the weights that are pre-trained on the ImageNet dataset <ref type="bibr" target="#b30">[31]</ref>. Then we adopt a 3-step training strategy to ensure that our proposed network is converged quickly. First, we train the RGB branch by using the cross entropy loss between the predicted saliency map and the ground truth. For that, we remove the MGF modules from the whole network, i.e., the outputs from the ADFC modules in the RGB branch model are directly fed into the JCSA block in the proposed JABMP module for saliency prediction. Secondly, we train the thermal infrared branch in a similar way as that in the training of RGB branch. Finally, the whole multi-modal salient object detection model is trained, where the network parameters for each single-modal feature extraction branch, including the VGG-16 net and the ADFC modules, are initialized by using their corresponding pre-trained ones in the first and second steps.</p><p>Due to the lack of large RGB-thermal image datasets, we have to use different training data during the network training, which is similar to that in <ref type="bibr" target="#b35">[36]</ref>  <ref type="bibr" target="#b36">[37]</ref>. Concretely, we randomly select 410 RGB-thermal image pairs from the RGB-thermal dataset and 200 RGB-thermal image pairs from the selected Grayscale-thermal dataset (i.e., the car and pedestrian sets) as the training set. The rest of RGB-thermal image pairs in RGBthermal dataset and Grayscale-thermal dataset are used as the testing set. Then, 830 samples are randomly selected from the training set of the MSRA-B as an auxiliary set to train the RGB/thermal branch of the proposed model.</p><p>Subsequently, in the first training step, the RGB images in the training set and those in the auxiliary set are employed to fine-tune the RGB branch model. In the second training step, the thermal images in the training set and the red color channels of RGB images in the auxiliary set are used to finetune the thermal infrared branch model. In the third training step, the RGB-thermal image pairs in training set are used to fine-tune the whole multi-modal saliency detection network.</p><p>3) Implementation: The proposed network model is implemented on the MATLAB R2014b platform with the Caffe toolbox <ref type="bibr" target="#b58">[59]</ref> and a NVIDIA 1080Ti GPU (with 11G memory).</p><p>The stochastic gradient descent (SGD) method is adopted to train the proposed network with a momentum 0.9 and a weight decay 0.0001. The base learning rate is set to 10 -8 , and then turned into a tenth of the previous set when the training loss reaches a flat. During training and testing, all the input images are rescaled to the spatial resolution of 256 × 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation metrics</head><p>We adopt six widely used metrics [11]- <ref type="bibr" target="#b24">[25]</ref>, including the precision-recall (PR) curves, F-measure curves, average Fmeasure (F ave ), mean absolute error (MAE), S-measure (S α ) <ref type="bibr" target="#b68">[69]</ref> and weighted F ω β -measure (F ω β ) <ref type="bibr" target="#b69">[70]</ref>, to objectively evaluate different saliency detection models.</p><p>Given a predicted saliency map S of size W × H, a binary mask B is first obtained by using a threshold. Then precision and recall can be, respectively, computed by Precsion = |B ∩ G|/|B| and Recall = |B∩G|/|G|, where G is the ground-truth and | • | denotes the non-zero entries in a mask. F-measure is a weighted harmonic mean of precision and recall, and is defined by</p><formula xml:id="formula_16">F β = 1 + β 2 × Precision × Recall β 2 × Precision + Recall ,<label>(11)</label></formula><p>where β 2 is set to 0.3 as suggested in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b24">[25]</ref>. With different thresholds, the PR and F-Measure curves are thus obtained. F ave is the mean of all the F β values obtained by different thresholds. Weighted F ω β -measure (F ω β ) is an intuitive generalization of the F-measure, which is computed by</p><formula xml:id="formula_17">F ω β = (1 + β 2 ) * P recision ω • Recall ω β 2 • P recision ω + Recall ω ,<label>(12)</label></formula><p>where P recision ω and Recall ω are weighted precision and recall, respectively. Here, β 2 is also set to 0.3 as default. More details are seen in <ref type="bibr" target="#b69">[70]</ref>. MAE is computed by</p><formula xml:id="formula_18">MAE = 1 W × H W i=1 H j=1 |S (i,j) -G (i,j) |.<label>(13)</label></formula><p>S-measure (S α ) <ref type="bibr" target="#b68">[69]</ref> is employed for the important structure information evaluation, which combines a region-aware (S r ) and an object-aware (S o ) structural similarity as their final structure metric:</p><formula xml:id="formula_19">S α = α * S o + (1 -α) * S r ,<label>(14)</label></formula><p>where α ∈ [0, 1] is the balance parameter and is set to 0.5 as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the state-of-the-art methods</head><p>To validate the proposed RGB-T salient detection model, we compare our model with 10 SOTA methods, which are further divided into three types, i.e., (1)RGB salient object detection methods: PoolNet <ref type="bibr" target="#b38">[39]</ref>, R3Net <ref type="bibr" target="#b39">[40]</ref>, and CPDNet <ref type="bibr" target="#b40">[41]</ref>; (2) RGB-D salient object detection methods: AFNet <ref type="bibr" target="#b44">[45]</ref>, TSAA <ref type="bibr" target="#b45">[46]</ref>, PDNet <ref type="bibr" target="#b46">[47]</ref>, and SSRC <ref type="bibr" target="#b47">[48]</ref>; and (3) RGB-T salient object detection methods: MFSR <ref type="bibr" target="#b27">[28]</ref>, GCL <ref type="bibr" target="#b48">[49]</ref>, and MRCM <ref type="bibr" target="#b26">[27]</ref>.</p><formula xml:id="formula_20">(a) (b) (c) (i) (j) (d) (e) (f) (g)<label>(h)</label></formula><p>For fair comparisons, we modify these RGB and RGB-D salient object detection methods for RGB-T saliency detection. For those RGB methods, their original networks are seen as single-modal branches for RGB or thermal infrared image feature extraction. The outputs before the final saliency predictions in these networks for RGB and thermal infrared images are first concatenated and then fed into the saliency prediction layers to obtain the final multi-modal saliency maps. Those RGB-D methods are also re-trained for RGB-T saliency detection, where the input channels of depth images are replaced by the thermal images. These multi-modal versions (PoolNet+, R3Net+, and CPDNet+, for short, respectively) modified from the RGB models and those re-trained RGB-D models are fine-tuned in the same way as described in Subsection IV-A.</p><p>1) Visual Evaluation: Fig. <ref type="figure" target="#fig_7">7</ref> illustrates some saliency detection results by those RGB methods and their multi-modal versions. As shown in Fig. <ref type="figure" target="#fig_7">7</ref>, compared with those singlemodal salient detection methods for RGB images, their multimodal counterparts generally perform better, especially when the input RGB images have poor visual quality (e.g, low contrast or much more noise). This further indicates that the use of the complementary information between the input RGB-T image pairs can improve the saliency detection performance.</p><p>Fig. <ref type="figure" target="#fig_9">8</ref> illustrates some visual comparisons of different methods in various cases, including large objects, small objects, simple background, complex background, poor illumination, and low contrast. As shown in the first four rows of Fig. <ref type="figure" target="#fig_9">8</ref> , under the condition of simple background and sufficient illumination, most of these methods work well for small salient objects, but poorly for large objects. Differently, our method can detect objects of various sizes effectively. Comparing the results in the fifth and sixth rows, we can also see that our method can effectively capture saliency information from RGB-T images when the infrared image or the RGB image has low contrast. As shown in the last four rows of Fig. <ref type="figure" target="#fig_9">8</ref>, for those images with poor illumination and complex background, some SOTA methods cannot achieve desirable results. For example, some salient objects are not uniformly detected and even mistakenly detected. Parts of the backgrounds are not well suppressed during the saliency detection. In contrast, our proposed method still works well for these images. This may be attributed to the good collaborations among the different modules in our proposed network, i.e., ADFC for multi-level feature extraction of each single-modal image, MGF for crossmodal feature fusion of RGB-T images, and JABMP for final saliency prediction.</p><p>2) Quantitative Evaluation: PR and F-measure curves of different methods are shown in Fig. <ref type="figure" target="#fig_11">9</ref> <ref type="foot" target="#foot_1">3</ref> . F ave , F ω β , S α and M AE values of different methods are listed in Table <ref type="table" target="#tab_2">I</ref>. In Table <ref type="table" target="#tab_2">I</ref>, the type of 'RGB' means that these methods are specifically designed for RGB salient object detection, which have been fine-tuned by using the RGB images in our RGB-T datasets. The evaluation values for these methods are obtained by performing these RGB salient object detection methods just on the RGB images in our RGB-T datasets. The type of 'RGB→RGB-T' means that these methods are modified from the RGB salient detection methods, as discussed previously. The type of 'RGB-D→RGB-T' means that these methods are re-trained versions of those RGB-D salient detection methods. Finally, the type of 'RGB-T' means that these methods are designed for RGB-T salient object detection. As shown in Fig. <ref type="figure" target="#fig_11">9</ref>, the proposed method scores the best on both PR and F-measure curves among these methods. Similar conclusions can also be drawn from Table <ref type="table" target="#tab_2">I</ref>. It can also be found from Table <ref type="table" target="#tab_2">I</ref> that their multi-modal versions of those RGB salient object detection methods significantly outperform their original versions that are designed for RGB images. In addition, the processing time of different methods is also provided in Table <ref type="table" target="#tab_2">I</ref>, which indicates that the computational complexity of the proposed method is acceptable. Especially, it has the highest computational efficiency among the four RGB-T salient detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation analysis 1) Validity of multi-level feature extraction using ADFC:</head><p>As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, multiple levels of features can be extracted from each single-modal input image by using several ADFC modules. In order to verify the validity of multiple levels      ), and the other is used to extract the corresponding level of features from thermal infrared images (e.g., H 2 1 ). In the fourth, fifth and sixth schemes, two pairs of ADFC modules are employed to extract two specific levels of features from RGB images (e.g., H 1  1 and H 1 2 ) and thermal infrared images (e.g., H 2 1 and H 2 2 ), respectively. In the last scheme, i.e., the scheme employed in our proposed network, three pairs of ADFC modules are employed to extract the RGB image features (e.g., H 1  1 , H 1 2 , and H 1 3 ) and thermal infrared image features (e.g., H 2 1 ,H 2 2 , and H 2 3 ), respectively. From the experimental results in Table <ref type="table" target="#tab_2">II</ref>, it can be concluded that higher saliency detection performance can be obtained by those schemes extracting multiple levels of features than those just extracting one specific level of features. Especially, the proposed scheme that extracts three levels of features achieves the best performance among the schemes mentioned here.</p><formula xml:id="formula_21">. (a) (b) (c) (i) (j) (k) (d) (e) (f) (g) (h) (l) (m) (n)</formula><p>Furthermore, we compare the proposed ADFC module with several SOTA feature extraction modules for RGB images, including resolution based feature combination (RFC) structure in Amulet <ref type="bibr" target="#b24">[25]</ref>, Hierarchical feature integration(HIFI) module in <ref type="bibr" target="#b55">[56]</ref> and FPN in <ref type="bibr" target="#b54">[55]</ref>. For that, we first design a baseline method, where the ADFC modules are removed from our proposed method and the features from the backbone networks are directly fed into the subsequent feature fusion and saliency inference modules. Then several versions of our proposed methods are compared by replacing ADFC with the other feature extraction modules mentioned above, while the rest modules in our proposed method are kept unchanged. The  2) Validity of multi-modal image feature fusion using MGF : In order to verify the validity of the proposed MGF module, we also provide six schemes for the fusion of multi-modal   The performance of different schemes is given in Table <ref type="table" target="#tab_5">IV</ref>. By comparing ADD and MGF1, it can be easily found that the simple concatenation fusion strategy significantly outperforms the element-wise addition fusion strategy for multi-modal RGB-T images salient object detection. This may be attributed to the fact that simple element-wise addition may easily weaken the discriminability of the fused features because of the polarity inverse between the RGB and thermal image intensities. The experiment also demonstrates that multi-group fusion schemes can obtain better results than the traditional single-group fusion schemes. The performance can be further improved by combining the two schemes. Especially, when the number of groups M is set to 8 in the proposed MGF module, the best saliency detection performance can be obtained. As discussed in <ref type="bibr" target="#b56">[57]</ref>, some salient features from individual singlemodal image may be easily drowned in the concatenated features because of the large number of features. It is hard for single-group fusion scheme (i.e., the regular convolution) to boost those salient features from all of the input features. When the group numbers are too large (e.g., M &gt; 8), the correlations among the feature maps will be weaken, since fewer channels of input features will be fed into each convolutional filter. This will also diminish the discriminability of the subsequent fused features <ref type="bibr" target="#b70">[71]</ref>. Therefore, in our proposed MGF module, M is set 8.</p><p>3) Validity of the proposed JCSA block for saliency prediction: In this part, five versions (No Bi, Bi NA, Bi CA, Bi SA, Bi JCSA, for short, respectively) of the proposed bidirectional message passing module are compared to test the validity of the JCSA block. In No Bi, the proposed JABMP is removed from our proposed network. Instead, as in <ref type="bibr" target="#b54">[55]</ref>, simple up-sampling and concatenation strategies are employed to integrate the multi-level fused features for the final saliency prediction in a coarser-to-finer way. In Bi NA, no attention guidance is employed in the bi-directional message passing module. In Bi CA, only the channel-wise attention mechanism (i.e., SE block in <ref type="bibr" target="#b64">[65]</ref>) is employed to control message passing in the bi-directional message passing model. Similarly, only the spatial attention mechanism (i.e., CA block in [66]) is employed in Bi SA, and the joint channel-spatial attention (JCSA) mechanism is employed in Bi JCSA (i.e., the proposed attention guided bi-directional message passing module, shown in Fig. <ref type="figure">6</ref>). The experimental results in Table <ref type="table" target="#tab_6">V</ref> indicate that the attention mechanism, especially the joint channelspatial attention mechanism, can greatly improve the saliency detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Failure cases</head><p>In most cases, the complementary information in the thermal images will improve the saliency detection performance. However, in some special cases, this does not always work. For example, as shown in Fig. <ref type="figure" target="#fig_12">10</ref>, when the temperatures of the salient objects (e.g., the umbrella) in the RGB images are not the highest ones in the infrared images, those regions with higher temperatures in the infrared images will be mistakenly determined to be salient ones by using our proposed method. More contextual information may improve the performance of our proposed method under this case. We leave this as one of our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a novel end-to-end deep neural network model for RGB-T salient object detection, where the multi-modal saliency detection is formulated as a CNN feature fusion problem. The proposed model consists of three components, i.e., multi-level feature extraction of single-modal images using multiple adjacent-depth feature combination (ADFC) modules, cross-modal feature fusion of RGB-T image pairs using multi-branch group fusion (MGF) modules, and saliency prediction using a joint attention guided bi-directional message passing (JABMP) module. With ADFC, each level of extracted single-modal features are formed by some adjacent-depth features from every individual backbone network branch, leading to more semantic information and visual details. The cross-modal feature fusion between RGB and thermal infrared images can be better done by using the proposed MGF, especially at a considerably computational complexity because of the employed group convolutions. Owing to the bi-directional message passing module, high-level semantic information in deeper layers and the low-level spatial details in shallower layers are incorporated efficiently at each level. A joint channel-spatial attention (JCSA) mechanism is further employed in the proposed message passing module to focus on those important features with high channel attention as well as spatial attention but suppress those superfluous features. Experimental results demonstrate that the proposed RGB-T salient object detection method performs better than the state-of-the-art methods, especially for those challenging scenes with poor illumination, complex background or low contrast.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the validity for salient object detection by integrating RGB-T images. (a) RGB images; (b) Thermal infrared images; (c) Saliency maps induced from the RGB images by<ref type="bibr" target="#b15">[16]</ref>; (d) Saliency maps induced from the thermal images by<ref type="bibr" target="#b15">[16]</ref>; (e) Saliency maps induced from the RGB-T images by<ref type="bibr" target="#b26">[27]</ref>; (f) Ground truth. The saliency maps in (c), (d) and (e) clearly demonstrate that thermal infrared images can provide complementary saliency cues for RGB images under challenging scenes with poor illumination (the first row) or complex background (the second row).</figDesc><graphic coords="2,392.35,121.30,84.06,63.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Generation of the first level features H 1 1 for RGB image by using ADFC module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Cat (•) denotes the cross-channel concatenation, and φ (•) is a ReLU activation function [54]. As discussed above, the d-th level of constructed features H n d contains the features F n d as well as those from its adjacent layers F n d-1 and F n d+1 , which means that H n d contains more rich and accurate semantics because it integrates differentresolution information. In addition, H n d has much less amount of data than the simple combination of F n d-1 ,F n d and F n d+1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Traditional feature fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig.5. Illustration of the proposed MGF module. The left part is the multi-group fusion branch, where the concatenated input features are first divided into M groups along the channel and then fused at each group by using several convolutional layers. The right part is the single-group fusion branch, where two stacked regular convolutional layers are directly performed on the concatenated input features to obtain the fused features. The finally fused features are obtained by concatenating the outputs from the two branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>8) where D * ; γ d , (1/2) d is a 2 d × 2 d deconvolutional layer to ensure the features to be fused have the same spatial resolutions, C * ; θ 1 d , 1 and C * ; θ 2 d , 1 denote two 1 × 1 convolutional layers, which are used to fuse features and obtain side outputs, respectively. It should be noted that all of the side outputs {P d |d = 1, 2, 3} have the same spatial resolutions as those of the input images because of the employed deconvolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Illustrations of the saliency detection results by some RGB methods and their modified multi-modal versions. (a) RGB images; (b) Thermal infrared images; (c) and (d) Saliency maps for RGB and RGB-T images obtained by R3Net and R3Net+, respectively; (e) and (f) Saliency maps for RGB and RGB-T images obtained by PoolNet and PoolNet+, respectively; (g) and (h) Saliency maps for RGB and RGB-T images obtained by CPDNet and CPDNet+, respectively; (i) Saliency maps for RGB-T images obtained by our proposed method; (j) Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>.</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Visual comparisons of different methods. From left to right: (a) RGB images; (b) Thermal infrared images; (c) R3Net+; (d) PoolNet+; (e) CPDNet+; (f) AFNet; (g) PDNet; (h) TSAA; (i) SSRC; (j) MRCMC; (k) MFSR; (l) CGL; (m) Ours; (n) Ground truth.</figDesc><graphic coords="11,219.95,402.36,170.95,135.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(a) RGB-thermal (b) Car (c) Pedestrian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Quantitative comparisons between the proposed algorithm and the other state-of-the-art methods. The first and second rows are the PR curves and F-measure curves of different methods on different data sets, respectively.</figDesc><graphic coords="11,393.50,539.81,165.93,130.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Failure cases of our method. (a) RGB image; (b) Thermal image; (c)-(e) Saliency maps for RGB, thermal infrared and RGB-T images obtained by our method, respectively; (f) Ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959253, IEEE Transactions on Image Processing</figDesc><table /><note><p>contains 821 aligned RGB-T image pairs under different conditions to ensure the diversity 1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. JOURNAL OF IEEE TRANSACTIONS ON IMAGE PROCESSING</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIP.2019.2959253, IEEE Transactions on Image Processing</figDesc><table /><note><p>1057-7149 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>OF DIFFERENT SALIENCY DETECTION METHODS ON THE THREE DATASETS. THE BEST RESULTS ARE SHOWN IN BOLD.</figDesc><table><row><cell>Methods</cell><cell>Type</cell><cell></cell><cell cols="2">RGB-thermal</cell><cell></cell><cell></cell><cell>Car</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell><cell>Runtime(s)</cell></row><row><cell></cell><cell></cell><cell>Fave</cell><cell>F ω β</cell><cell>MAE</cell><cell>Sα</cell><cell>Fave</cell><cell>F ω β</cell><cell>MAE</cell><cell>Sα</cell><cell>Fave</cell><cell>F ω β</cell><cell>MAE</cell><cell>Sα</cell><cell></cell></row><row><cell>R3Net</cell><cell>RGB</cell><cell>0.803</cell><cell>0.755</cell><cell>0.046</cell><cell>0.831</cell><cell>0.602</cell><cell>0.528</cell><cell>0.022</cell><cell>0.661</cell><cell>0.274</cell><cell>0.249</cell><cell>0.047</cell><cell>0.572</cell><cell>0.16</cell></row><row><cell>R3Net+</cell><cell>RGB→RGB-T</cell><cell>0.852</cell><cell>0.785</cell><cell>0.049</cell><cell>0.838</cell><cell>0.532</cell><cell>0.358</cell><cell>0.014</cell><cell>0.591</cell><cell>0.492</cell><cell>0.307</cell><cell>0.015</cell><cell>0.574</cell><cell>0.33</cell></row><row><cell>PoolNet</cell><cell>RGB</cell><cell>0.674</cell><cell>0.645</cell><cell>0.070</cell><cell>0.763</cell><cell>0.119</cell><cell>0.025</cell><cell>0.013</cell><cell>0.498</cell><cell>0.035</cell><cell>0.031</cell><cell>0.019</cell><cell>0.489</cell><cell>0.09</cell></row><row><cell>PoolNet+</cell><cell>RGB→RGB-T</cell><cell>0.716</cell><cell>0.654</cell><cell>0.051</cell><cell>0.863</cell><cell>0.183</cell><cell>0.307</cell><cell>0.029</cell><cell>0.699</cell><cell>0.209</cell><cell>0.217</cell><cell>0.099</cell><cell>0.601</cell><cell>0.19</cell></row><row><cell>CPDNet</cell><cell>RGB</cell><cell>0.788</cell><cell>0.768</cell><cell>0.041</cell><cell>0.861</cell><cell>0.199</cell><cell>0.301</cell><cell>0.055</cell><cell>0.668</cell><cell>0.165</cell><cell>0.154</cell><cell>0.079</cell><cell>0.578</cell><cell>0.08</cell></row><row><cell>CPDNet+</cell><cell>RGB→RGB-T</cell><cell>0.860</cell><cell>0.838</cell><cell>0.028</cell><cell>0.889</cell><cell>0.319</cell><cell>0.481</cell><cell>0.016</cell><cell>0.811</cell><cell>0.341</cell><cell>0.463</cell><cell>0.018</cell><cell>0.750</cell><cell>0.16</cell></row><row><cell>AFNet</cell><cell>RGB-D→RGB-T</cell><cell>0.700</cell><cell>0.682</cell><cell>0.062</cell><cell>0.841</cell><cell>0.143</cell><cell>0.044</cell><cell>0.162</cell><cell>0.495</cell><cell>0.070</cell><cell>0.044</cell><cell>0.338</cell><cell>0.412</cell><cell>0.18</cell></row><row><cell>PDNet</cell><cell>RGB-D→RGB-T</cell><cell>0.803</cell><cell>0.750</cell><cell>0.048</cell><cell>0.869</cell><cell>0.561</cell><cell>0.399</cell><cell>0.016</cell><cell>0.751</cell><cell>0.451</cell><cell>0.286</cell><cell>0.041</cell><cell>0.658</cell><cell>0.11</cell></row><row><cell>TSAA</cell><cell>RGB-D→RGB-T</cell><cell>0.817</cell><cell>0.778</cell><cell>0.040</cell><cell>0.882</cell><cell>0.642</cell><cell>0.532</cell><cell>0.013</cell><cell>0.793</cell><cell>0.529</cell><cell>0.453</cell><cell>0.021</cell><cell>0.730</cell><cell>0.13</cell></row><row><cell>SSRC</cell><cell>RGB-D→RGB-T</cell><cell>0.750</cell><cell>0.694</cell><cell>0.055</cell><cell>0.833</cell><cell>0.309</cell><cell>0.068</cell><cell>0.189</cell><cell>0.507</cell><cell>0.635</cell><cell>0.573</cell><cell>0.014</cell><cell>0.773</cell><cell>0.24</cell></row><row><cell>MRCMC</cell><cell>RGB-T</cell><cell>0.661</cell><cell>0.428</cell><cell>0.109</cell><cell>0.688</cell><cell>0.078</cell><cell>0.048</cell><cell>0.072</cell><cell>0.501</cell><cell>0.319</cell><cell>0.065</cell><cell>0.074</cell><cell>0.613</cell><cell>1.99</cell></row><row><cell>MFSR</cell><cell>RGB-T</cell><cell>0.701</cell><cell>0.673</cell><cell>0.073</cell><cell>0.823</cell><cell>0.177</cell><cell>0.155</cell><cell>0.046</cell><cell>0.613</cell><cell>0.201</cell><cell>0.138</cell><cell>0.106</cell><cell>0.569</cell><cell>0.18</cell></row><row><cell>CGL</cell><cell>RGB-T</cell><cell>0.771</cell><cell>0.585</cell><cell>0.086</cell><cell>0.765</cell><cell>0.103</cell><cell>0.088</cell><cell>0.033</cell><cell>0.455</cell><cell>0.109</cell><cell>0.062</cell><cell>0.117</cell><cell>0.495</cell><cell>2.33</cell></row><row><cell>Ours</cell><cell>RGB-T</cell><cell>0.873</cell><cell>0.858</cell><cell>0.025</cell><cell>0.911</cell><cell>0.708</cell><cell>0.620</cell><cell>0.007</cell><cell>0.795</cell><cell>0.745</cell><cell>0.667</cell><cell>0.010</cell><cell>0.838</cell><cell>0.12</cell></row></table><note><p><p><p>of extracted features for saliency detection, we compare our proposal with another six schemes with different numbers of ADFC modules in our proposed network. Seven schemes as well as their corresponding saliency detection results on the RGB-thermal dataset are listed in Table</p>II</p>. In the first three schemes, only a pair of ADFC modules are employed. One is used to extract a specific level of features from RGB images (e.g., H 1 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV PERFORMANCE</head><label>IV</label><figDesc>OF DIFFERENT SCHEMES IN THE PROPOSED FEATURE FUSION MODULE FOR MULTI-MODAL IMAGES ON RGB-THERMAL DATASET.</figDesc><table><row><cell>Metrics</cell><cell>ADD</cell><cell>MGF1</cell><cell>MGF2 (M=8)</cell><cell>MGF 4 (M=4)</cell><cell>MGF 8 (M=8)</cell><cell>MGF 16 (M=16)</cell><cell>MGF 32 (M=32)</cell></row><row><cell>Fave</cell><cell>0.749</cell><cell>0.843</cell><cell>0.849</cell><cell>0.853</cell><cell>0.873</cell><cell>0.854</cell><cell>0.860</cell></row><row><cell>MAE</cell><cell>0.039</cell><cell>0.033</cell><cell>0.030</cell><cell>0.030</cell><cell>0.025</cell><cell>0.031</cell><cell>0.028</cell></row><row><cell>Sα</cell><cell>0.867</cell><cell>0.898</cell><cell>0.899</cell><cell>0.899</cell><cell>0.911</cell><cell>0.900</cell><cell>0.907</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V PERFORMANCE</head><label>V</label><figDesc>OF DIFFERENT VERSIONS OF THE PROPOSED BI-DIRECTIONAL MESSAGE PASSING MODULE ON RGB-THERMAL MGF 8 with M=8, MGF 16 with M=16 MGF 32 with M=32, for short, respectively). The output channels of these models are all set to 128 for fair comparison.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">DATASET.</cell><cell></cell><cell></cell></row><row><cell>Metrics</cell><cell>No Bi</cell><cell>Bi NA</cell><cell>Bi CA</cell><cell>Bi SA</cell><cell>Bi JCSA</cell></row><row><cell>Fave</cell><cell>0.818</cell><cell>0.845</cell><cell>0.855</cell><cell>0.850</cell><cell>0.873</cell></row><row><cell>MAE</cell><cell>0.037</cell><cell>0.032</cell><cell>0.030</cell><cell>0.032</cell><cell>0.025</cell></row><row><cell>Sα</cell><cell>0.868</cell><cell>0.895</cell><cell>0.899</cell><cell>0.897</cell><cell>0.911</cell></row><row><cell cols="6">image features. Specifically, in the first two schemes (ADD</cell></row><row><cell cols="6">and MGF1, for short, respectively), only the single-group</cell></row><row><cell cols="6">fusion branch is employed. Simple element-wise addition and</cell></row><row><cell cols="6">concatenation are first performed on the features from the input</cell></row><row><cell cols="6">images, respectively, in the two schemes. Then the regular</cell></row><row><cell cols="6">convolution is applied to obtain the fused features. In the</cell></row><row><cell cols="6">third scheme (MGF2, for short), only the multi-group fusion</cell></row><row><cell cols="6">branch with M=8 is employed. In the rest of schemes, the two</cell></row><row><cell cols="6">branches are jointly employed but with different numbers of</cell></row><row><cell cols="3">groups (MGF 4 with M=4,</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The symbol C * ,θ,k denotes a convolutional layer with pixel stride k and network parameters θ. The same symbol is used in the rest of the paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>PR and F-measure curves of the RGB salient detection methods are not provided in Fig.9for better displaying.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work is supported by the National Natural Science Foundation of China under Grant No. 61773301 and 61876140, and the China Postdoctoral Support Scheme for Innovative Talents under Grant No. BX20180236.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Weighted attentional blocks for probabilistic object tracking</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="229" to="243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust infrared target tracking based on particle filter with embedded saliency detection</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="page" from="215" to="226" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1005" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Saliency guided local and global descriptors for effective action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abdulmunem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="106" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A novel H. 264 rate control algorithm with consideration of visual attention</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">General fusion method for infrared and visual images via latent low-rank representation and local nonsubsampled shearlet transform</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-scale image fusion of visible and infrared images using saliency detection</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bavirisetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dhuli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Infrared Physics &amp; Technology</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Advanced deep learning techniques for salient and category-specific object detection: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="100" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised salient object detection via inferring from imperfect saliency models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1101" to="1112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Salient region detection via highdimensional color transform and local spatial support</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="23" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection via multiple instance learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1911" to="1922" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense and sparse reconstruction error based saliency descriptor</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1592" to="1603" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Salient object detection via structured matrix decomposition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="818" to="832" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency detection with spaces of background-based distribution</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="683" to="687" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Salient object detection via a local and global method based on deep residual network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="212" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep CNN features</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Eld-net: An efficient deep learning architecture for accurate saliency detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1599" to="1610" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">815</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weighted lowrank decomposition for robust grayscale-thermal foreground detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="725" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A unified RGB-T saliency detection benchmark: dataset, baselines, analysis and a novel approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02829</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning multiscale deep features and SVM regressors for adaptive RGB-T saliency detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computational Intelligence and Design</title>
		<meeting>the International Symposium on Computational Intelligence and Design</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="389" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face recognition in multisensor images based on a novel modular feature selection technique</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gundimada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="132" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Superpixel-based causal multisensor video fusion</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Gangapure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A bi-directional message passing model for salient object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fully convolutional region proposal networks for multispectral person detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Konig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jarvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Layher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection using deep fusion convolutional neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<meeting>24th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="509" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multispectral deep neural networks for pedestrian detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">of the 27th British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName><forename type="first">J.-J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">R3Net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="684" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for RGB-D salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rethinking RGB-D salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Contrast prior and fluid pyramid integration for RGB-D salient object detection</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3927" to="3936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adaptive fusion for RGB-D salient object detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">284</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for RGB-D salient object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">PDNet: Priormodel guided depth-enhanced network for salient object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo</title>
		<meeting>the IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Salient object detection for RGB-D image by single stream recurrent convolution neural network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">RGB-T image saliency detection via collaborative graph learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06741</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Infrared and visible image fusion with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Wavelets, Multiresolution and Information Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fusing two-stream convolutional neural networks for RGB-T object tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning collaborative sparse representation for grayscale-thermal tracking</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5743" to="5756" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08982</idno>
		<title level="m">RGB-T object tracking: Benchmark and baseline</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hi-Fi: Hierarchical feature integration for skeleton detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1191" to="1197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep salient object detection with contextual information guidance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="360" to="374" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Pytorch: Tensors and dynamic neural networks in python with strong gpu acceleration</title>
		<author>
			<persName><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://pytorch.org/" />
	</analytic>
	<monogr>
		<title level="m">Website</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep contrast learning for salient object detection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="18" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4548" to="4557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How to evaluate foreground maps</title>
		<author>
			<persName><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
