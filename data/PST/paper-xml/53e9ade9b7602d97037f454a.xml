<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Coordination in Multiagent Reinforcement Learning: A Bayesian Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Georgios</forename><surname>Chalkiadakis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3H5</postCode>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Toronto Toronto</orgName>
								<address>
									<postCode>M5S 3H5</postCode>
									<region>ON</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Coordination in Multiagent Reinforcement Learning: A Bayesian Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A86A7FBEED515C869BFD53335139D2C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Much emphasis in multiagent reinforcement learning (MARL) research is placed on ensuring that MARL algorithms (eventually) converge to desirable equilibria. As in standard reinforcement learning, convergence generally requires sufficient exploration of strategy space. However, exploration often comes at a price in the form of penalties or foregone opportunities. In multiagent settings, the problem is exacerbated by the need for agents to "coordinate" their policies on equilibria. We propose a Bayesian model for optimal exploration in MARL problems that allows these exploration costs to be weighed against their expected benefits using the notion of value of information. Unlike standard RL models, this model requires reasoning about how one's actions will influence the behavior of other agents. We develop tractable approximations to optimal Bayesian exploration, and report on experiments illustrating the benefits of this approach in identical interest games.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The application of reinforcement learning (RL) to multiagent systems has received considerable attention <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b3">3]</ref>. However, in multiagent settings, the effect (or benefit) of one agent's actions are often directly influenced by those of other agents. This adds complexity to the learning problem, requiring that an agent not only learn what effects its actions have, but also how to coordinate or align its action choices with those of other agents. Fortunately, RL methods can often lead to coordinated or equilibrium behavior. Empirical and theoretical investigations have shown that standard (single-agent) RL methods can under some circumstances lead to equilibria <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b3">3]</ref>, as can methods designed to explicitly account for the behavior of other agents <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b9">9]</ref>.</p><p>Multiagent reinforcement learning (MARL) algorithms face difficulties not encountered in single-agent settings: the existence of multiple equilibria. In games with unique equilibrium strategies (and hence values), the "target" being learned by agents is well-defined. With multiple equilibria, MARL methods face the problem that agents must coordinate their choice of equilibrium (and not just their ac-tions). <ref type="foot" target="#foot_0">1</ref> Empirically, the influence of multiple equilibria on MARL algorithms is often rather subtle, and certain game properties can make convergence to undesirable equilibria very likely <ref type="bibr" target="#b4">[4]</ref>. For obvious reasons, one would like RL methods that converge to desirable (e.g., optimal) equilibria. A number of heuristic exploration strategies have been proposed that in fact increase the probability (or even guarantee) that optimal equilibria are reached in identical interest games <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b19">19]</ref>.</p><p>Unfortunately, methods that encourage or force convergence to optimal equilibria often do so at a great cost. Coordination on a "good" strategy profile often requires exploration in parts of policy space that are very unrewarding. In such a case, the benefits of eventual coordination to an optimal equilibrium ought to be weighed against the cost (in terms of reward sacrificed while learning to play that equilibrium) <ref type="bibr" target="#b1">[1]</ref>. This is simply the classic RL exploration-exploitation tradeoff in a multiagent guise. In standard RL, the choice is between: exploiting what one knows now about the effects of actions and their rewards by executing the action that-given current knowledgeappears the best; and exploring to gain further information about actions and rewards that has the potential to change the action that appears best. In the multiagent setting, this tradeoff exists with respect to action and reward information. But another aspect comes to bear: the influence one's action choice has on the future action choices of other agents. In other words, one can exploit one's current knowledge of the strategies of others, or explore to try to find out more information about those strategies.</p><p>In this paper, we develop a model that accounts for this generalized exploration-exploitation tradeoff in MARL. We adopt a Bayesian, model-based approach to MARL, much like the single-agent model described in <ref type="bibr" target="#b5">[5]</ref>. Value of information will play a key role in determining an agent's exploration policy. Specifically, the value of an action consists of two components: its estimated value given current model estimates, and the expected decision-theoretic value of information it provides (informally, the ability this information has to change future decisions). We augment both parts of this value calculation in the MARL context. The estimated value of an action given current model estimates requires predicting how the action will influence the future action choices of other agents. The value of information associated with an action includes the information it provides about other agents's strategies, not just the environment model. Both of these changes require that an agent possess some model of the strategies of other agents, for which we adopt a Bayesian view <ref type="bibr" target="#b11">[11]</ref>. Putting these together, we derive optimal exploration methods for (Bayesian) multiagent systems.</p><p>After reviewing relevant background, and describing related work in MARL, we develop a general Bayesian model, and describe certain computational approximations for optimal exploration. We describe a number of experiments illustrating this approach, with our experiments focusing on identical interest games, since these have has been the almost exclusive target of recent research on heuristic exploration methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND</head><p>We begin with basic background on RL and stochastic games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Bayesian Reinforcement Learning</head><p>We assume an agent learning to control a stochastic environment modeled as a Markov decision process (MDP) S, A, R, Pr , with finite state and action sets S, A, reward function R, and dynamics Pr. The dynamics Pr refers to a family of transition distributions Pr(s, a, •), where Pr(s, a, s ) is the probability with which state s is reached when action a is taken at s. R(s, r) denotes probability with which reward r is obtained when state s is reached. <ref type="foot" target="#foot_1">2</ref> The agent is charged with constructing an optimal Markovian policy π : S → A that maximizes the expected sum of future discounted rewards over an infinite horizon:</p><formula xml:id="formula_0">Eπ[ È ∞ t=0 γ t R t |S 0 = s].</formula><p>This policy, and its value, V * (s) at each s ∈ S, can be computed using standard algorithms such as policy or value iteration.</p><p>In the RL setting, the agent does not have direct access to the model components R and Pr. As such, it must learn a policy based on its interactions with the environment. Any of a number of RL techniques can be used to learn an optimal policy π : S → A.</p><p>We focus here on model-based RL methods, in which the learner maintains an estimated MDP S, A, R, Pr , based on the set of experiences s, a, r, t obtained so far. At each stage (or at suitable intervals) this MDP can be solved exactly, or approximately.</p><p>Bayesian methods in model-based RL allow agents to incorporate priors and explore optimally. We assume some prior density P over possible dynamics D and reward distributions R, which is updated with each data point s, a, r, t . <ref type="foot" target="#foot_2">3</ref> Letting H denote the (current) state-action history of the observer, we use the posterior P (D, R|H) to determine an appropriate action choice at each stage. The formulation of <ref type="bibr" target="#b5">[5]</ref> renders this update tractable by assuming a convenient prior. Specifically, the following assumptions are made: (a) the density P is factored over R and D; (b) P (D) is the product of independent local densities P (D s,a ) for each transition distribution Pr(s, a, •); and (c) each density P (D s,a ) is a Dirichlet. <ref type="foot" target="#foot_3">4</ref>To model P (D s,a ) we require a Dirichlet parameter vector n s,a with entries n s,a,s for each possible successor state s . The expectation of Pr(s, a, s ) w.r.t. P is given by n s,a,s / È i n s,a,s i . Update of a Dirichlet is a straightforward: given prior P (D s,a ; n s,a ) and data vector c s,a (where c s,a i is the number of observed transitions from s to si under a), the posterior is given by parameter vector n s,a + c s,a . Thus the posterior P (D|H) over transition models can be factored into posteriors over local families, each of the form:</p><formula xml:id="formula_1">P (D s,a |H s,a ) = α Pr(H s,a |D s,a )P (D s,a )<label>(1)</label></formula><p>where H s,a is the subset of history involving s, a-transitions and updates are of the Dirichlet parameters.</p><p>The Bayesian approach has several advantages over other approaches to model-based RL. First, it allows the natural incorporation of prior knowledge. Second, approximations to optimal Bayesian exploration can take advantage of this model <ref type="bibr" target="#b5">[5]</ref>. We elaborate on optimal exploration below in the MARL context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Stochastic Games and Coordination</head><p>A normal form game is a tuple G = α, {Ai}i∈α, {Ui}i∈α , where α is a collection of agents, Ai is the set of actions available to agent i, and Ui is agent i's payoff function. Letting A = ×Ai denote the set of joint actions, Ui(a) denotes the real-valued utility obtained by i if the agents execute a ∈ A. We refer to any σi ∈ ∆(Ai) as a mixed strategy. A strategy profile σ is a collection of strategies, one per agent. We often write σi to refer to agent i's component of σ, and σ-i to denote a reduced strategy profile dictating all strategies except that for i. We use σ-i • σi to denote the (full) profile obtained by augmenting σ-i with σi. Let σ-i be some reduced strategy profile. A best response to σ-i is any strategy σi</p><formula xml:id="formula_2">s.t. Ui(σ-i • σi) ≥ Ui(σ-i • σ i ) for any σ i ∈ ∆(Ai).</formula><p>We define BR(σ-i) to be the set of such best responses. A Nash equilibrium is any profile σ s.t. σi ∈ BR(σ-i) for all agents i.</p><p>Nash equilibria are generally viewed as the standard solution concept to games of this form. However, it is widely recognized that the equilibrium concept has certain (descriptive and prescriptive) deficiencies. One important problem (among several) is the fact that games may have multiple equilibria <ref type="bibr" target="#b8">[8]</ref>, leading to the problem of equilibrium selection. As an example, consider the simple two-player identical interest game called the penalty game <ref type="bibr" target="#b4">[4]</ref>, shown here in standard matrix form:</p><formula xml:id="formula_3">a0 a1 a2 b0 10 0 k b1 0 2 0 b2 k 0 10</formula><p>Here agent A has moves a0, a1, a2 and B has moves b0, b1, b2.</p><p>The payoffs to both players are identical, and k &lt; 0 is some penalty. There are three pure equilibria (corresponding to the agents matching moves). While a0, b0 and a2, b2 are the optimal equilibria, the symmetry of the game induces a coordination problem for the agents. With no means of breaking the symmetry, and the risk of incurring the penalty if they choose different optimal equilibria, the agents might in fact focus on the suboptimal equilibrium a1, b1 .</p><p>Learning models have become popular as a means tackling the equilibrium selection problem <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b7">7]</ref>. Assuming repeated play of some "stage game," these methods require an agent to make some prediction about the play of others at the current stage based on the history of interactions, and play the current stage game using these predictions. One simple model is fictitious play <ref type="bibr" target="#b16">[16]</ref>: at each stage, agent i uses the empirical distribution of observed actions by other agents over past iterations as reflective of the mixed strategy they will play at the current stage; agent i then plays some best response to these estimated mixed strategies. This method is known to converge (in various senses) to equilibria for certain classes of games.</p><p>Another interesting learning model is the Bayesian approach of Kalai and Lehrer <ref type="bibr" target="#b11">[11]</ref>. In this model, an agent maintains a distribution over all strategies that could be played by other agents. This strategy space is not confined to strategies in the stage game, but allows for beliefs about strategies another agent could adopt for the repeated game itself. <ref type="foot" target="#foot_4">5</ref> Standard Bayesian updating methods are used to maintain these beliefs over time, and best responses are played by the agent w.r.t. the expectation over strategy profiles.</p><p>Repeated games are a special case of stochastic games <ref type="bibr" target="#b17">[17,</ref><ref type="bibr">6]</ref>, which can be viewed as a multiagent extension of MDPs. Formally, a stochastic game G = α, {Ai}i∈α, S, Pr, {Ri}i∈α consists of five components. The agents α and action sets Ai are as in a typical game, and the components S and Pr are as in an MDP, except that Pr now refers to joint actions a ∈ A = ×Ai. Ri is a the reward function for agent i, defined over states s ∈ S (pairs s, a ∈ S×A). The aim of each agent is, as with an MDP, to act to maximize the expected sum of discounted rewards. However, the presence of other agents requires treating this problem in a game theoretic fashion. For particular classes of games, such as zero-sum stochastic games <ref type="bibr" target="#b17">[17,</ref><ref type="bibr">6]</ref>, algorithms like value iteration can be used to compute Markovian equilibrium strategies.</p><p>The existence of multiple "stage game" equilibria is again a problem that plagues the construction of optimal strategies for stochastic games. Consider another simple identical interest example, the stochastic game shown in Figure <ref type="figure" target="#fig_2">3(a)</ref>. In this game, there are two optimal strategy profiles that maximize the two agents's reward. In both, the first agent chooses to "opt in" at state s1 by choosing action a, which takes the agents (with high probability) to state s1; then at s1 both agents either choose a or both choose b-either joint strategy gives an optimal equilibrium. Intuitively, the existence of these two equilibria gives rise to a coordination problem at state s1. Without some means of coordination, assuming the agents choose their part of the equilibrium randomly, there is a 0.5 chance that they miscoordinate at s1, thereby obtaining an expected immediate reward of 0. On this basis alone, one might be tempted to propose methods whereby the agents decide to "opt out" at s1 (the first agent takes action b) and obtain the safe payoff of 6. However, if we allow some means of coordination-for example, simple learning rules like fictitious play or randomization-the sequential nature of this problem means that the short-term risk of miscoordination at s2 can be more than compensated for by the eventual stream of high payoffs should they coordinate. Boutilier <ref type="bibr" target="#b1">[1]</ref> argues that the solution of games like this, assuming some (generally history dependent) mechanism for resolving these stage game coordination problems, requires explicit reasoning about the odds and benefits of coordination and its expected cost, and comparing these to the alternatives.</p><p>Repeated games can be viewed as stochastic games with a single state. If our concern is not just with equilibrium in the stage game, but with performance over the sequence of interactions, techniques for solving stochastic games can be applied to solving such repeated games. Furthermore, the points made above regarding the risks associated with using specific learning rules for coordination can be applied with equal force to repeated games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multiagent RL</head><p>In this section, we describe some existing approaches to MARL, and point out recent efforts to augment standard RL schemes to encourage RL agents in multiagent systems to converge to optimal equilibria. Intuitively, MARL can be viewed as the direct or indirect application of RL techniques to the stochastic games in which the underlying model (i.e., transitions and rewards) are unknown. In some cases, it is assumed that the learner is even unaware (or chooses to ignore) the existence of other agents.</p><p>Formally, we suppose we have some underlying stochastic game G = α, {Ai}i∈α, S, Pr, {Ri}i∈α . We consider the case where each agent knows the "structure" of the games-that is, it knows the set of agents, the actions available to each agent, and the set of states-but knows neither the dynamics Pr, nor the reward functions Ri. The agents learn how to act in the world through experience. At each point in time, the agents are at a known state s, and each agent i executes one of its actions at state s; the resulting joint action a induces a transition to state t (with probability Pr(s, a, t)) and a reward ri for each agent (according to Ri(s)). We assume that each agent can observe the actions chosen by other agents, the resulting state t, and the rewards ri.</p><p>One of the first game-theoretic approaches to MARL was proposed by Littman <ref type="bibr" target="#b14">[14]</ref>, who devised an extension of Q-learning for zero-sum Markov games called minimax-Q. At each state, the agents have estimated Q-values over joint actions, which can be used to compute an (estimated) equilibrium value at that state. Using a generalization of the Q-update rule in which this equilibrium value replaces the usual max operator, minimax-Q can be shown to converge to the equilibrium value of the stochastic game <ref type="bibr" target="#b15">[15]</ref>. Hu and Wellman <ref type="bibr" target="#b9">[9]</ref> apply similar ideas-using an equilibrium computation on estimated Q-values to estimate state values-to general sum games, with somewhat weaker convergence guarantees <ref type="bibr" target="#b2">[2]</ref>. Algorithms have also been devised for agents that do not observe the behavior of their counterparts <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b10">10]</ref>.</p><p>Identical interest games have drawn much attention in AI, providing suitable models for task distribution and decomposition among teams of agents. Claus and Boutilier <ref type="bibr" target="#b4">[4]</ref> proposed several MARL methods for repeated games in this context. A simple joint-action learner (JAL) protocol learned the (myopic, or one-stage) Q-values of joint actions using standard Q-learning updates. The novelty of this approach lies in its exploration strategy: (a) a fictitious play protocol estimates the strategies of other agents; and (b) exploration is biased by the expected Q-value of actions. Specifically, the estimated value of an action is given by its expected Q-value, where the expectation is taken w.r.t. the fictitious play beliefs over the other agents's strategies. When semi-greedy exploration is used, this method will converge to an equilibrium in the underlying stage game.</p><p>One drawback of the JAL method is the fact that the equilibrium it converges to depends on the specific path of play, which is stochastic. Certain equilibria can exhibit serious resistance-for example, the odds of converging to an optimal equilibrium in the penalty game described above are quite small (and decrease dramatically with the magnitude of the penalty). Claus and Boutilier propose several heuristic methods that bias exploration toward optimal equilibria: for instance, action selection can be biased toward actions that form part of an optimal equilibrium. In the penalty game, for instance, despite the fact that agent B may be predicted to play a strategy that makes the a0 look unpromising, the repeated play of the a0 by A can be justified by optimistically assuming B will play its part of this optimal equilibrium. This is further motivated by the fact that repeated play of a0 would eventually draw B toward this equilibrium.</p><p>This issue of learning optimal equilibria in identical interest games has been addressed recently in much greater detail. Lauer and Riedmiller <ref type="bibr" target="#b13">[13]</ref> describe a Q-learning method for identical interest stochastic games that explicitly embodies this optimistic assumption in its Q-value estimates. Kapetanakis and Kudenko <ref type="bibr" target="#b12">[12]</ref> propose a method called FMQ for repeated games that uses the optimistic assumption to bias exploration, much like <ref type="bibr" target="#b4">[4]</ref>, but in the context of individual learners (that do not have explicit access to the actions performed by other agents). Wang and Sandholm <ref type="bibr" target="#b19">[19]</ref> similarly use the optimistic assumption in repeated games to guarantee convergence to an optimal equilibrium. We critique these methods below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A BAYESIAN VIEW OF MARL</head><p>The spate of activity described above on MARL in identical interest games has focused exclusively on devising methods that ensure eventual convergence to optimal equilibria. In cooperative games, this pursuit is well-defined, and in some circumstances may be justified. However, these methods do not account for the fact thatby forcing agents to undertake actions that have potentially dras-tic effects in order to reach an optimal equilibrium-they can have a dramatic impact on accumulated reward. The penalty game was devised to show that these highly penalized states can bias (supposedly rational) agents away from certain equilibria; yet optimistic exploration methods ignore this and blindly pursue these equilibria at all costs. Under certain performance metrics (e.g., average reward over an infinite horizon) one might justify these techniques. <ref type="foot" target="#foot_5">6</ref> However, using the discounted reward criterion (which all of these methods are designed for), the tradeoff between long-term benefit and short-term cost should be addressed.</p><p>This tradeoff was discussed above in the context of known-model stochastic games. In this section, we attempt to address the same tradeoff in the RL context. To do so, we formulate a Bayesian approach to model-based MARL. By maintaining probabilistic beliefs over the space of models and the space of opponent strategies, our learning agents can explicitly account for the effects their actions can have on (a) their knowledge of the underlying model; (b) their knowledge of the other agent strategies; (c) expected immediate reward; and (d) expected future behavior of other agents. Components (a) and (c) are classical parts of the single-agent Bayesian RL model <ref type="bibr" target="#b5">[5]</ref>. Components (b) and (d) are key to the multiagent extension, allowing an agent to explicitly reason about the potential costs and benefits of coordination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical Underpinnings</head><p>We assume a stochastic game G in which each agent knows the game structure, but not the reward or transition models. A learning agent is able to observe the actions taken by all agents, the resulting game state, and the rewards received by other agents. Thus an agent's experience at each point in time is simply s, a, r, t , where s is a state in which joint action a was taken, r = r1, • • • , rn is the vector of rewards received by each agent, and t is the resulting state.</p><p>A Bayesian MARL agent has some prior distribution over the space of possible models as well as the space of possible strategies being employed by other agents. These beliefs are updated as the agent acts and observes the results of its actions and the action choices of other agents. The strategies of other agents may be history-dependent, and we allow our Bayesian agent (BA) to assign positive support to such strategies. As such, in order to make accurate predictions about the actions others will take, the BA must monitor appropriate observable history. In general, the history (or summary thereof) required will be a function of the strategies to which the BA assigns positive support. We assume that the BA keeps track of sufficient history to make such predictions. <ref type="foot" target="#foot_6">7</ref>The belief state of the BA has the form b = PM , PS, s, h , where: PM is some density over the space of possible models (i.e., games being played); PS is a joint density over the possible strategies played by other agents; s is the current state of the system; and h is a summary of the relevant aspects of the current history of the game, sufficient to predict the action choice of any agent given any strategy consistent with PS. Given an experience s, a, r, t , the BA will update its belief state using standard Bayesian methods. The updated belief state is given by b = b( s, a, r, t ) = P M , P S , t, h</p><p>Here the updated densities are obtained by Bayes rule; specifically, P M (m) = z Pr(t, r|a, m)PM (m) and P S (σ-i) = z Pr(a-i|s, h, σ-i)PS(σ-i). And h is a suitable update of the observed history. This model combines aspects of Bayesian reinforcement learning <ref type="bibr" target="#b5">[5]</ref> and Bayesian strategy modeling <ref type="bibr" target="#b11">[11]</ref>.</p><p>To make belief state maintenance tractable (and admit computationally viable methods for action selection below), we assume a specific form for these beliefs, following <ref type="bibr" target="#b5">[5]</ref>. First, our prior over models will be factored into independent local models for both rewards and transitions. We assume independent priors P s R over reward distributions at each state s, and P s,a D over system dynamics for each state and joint action pair. We also assume that these local densities are Dirichlet, which are conjugate for the multinomial distributions we wish to learn. This means that each density can be represented using a small number of hyperparameters, expected transition probabilities can be computed readily, and the density can be updated easily. For example, our BA's prior beliefs about the transition probabilities for joint action a at state s will be represented by a vector n s,a with one parameter per successor state t. The expected transition probability to a specific state t and updating these beliefs are as described in Section 2.1. Note that the independence of these local densities is assured after each update.</p><p>Second, we assume that the beliefs about opponent strategies can be factored and represented in some convenient form. For example, it would be natural to assume that the strategies of other agents are independent. Simple fictitious play type models could be used to model the BA's beliefs about each opponent's strategy-these would correspond to Dirichlet priors over agent's mixed strategy spaceand allow ready update and computation of expectations. Such beliefs would not require that history be stored in the belief state. Similarly, distributions over specific classes of finite state controllers could also be used. We will not pursue further development of such models in this paper, since we use only simple opponent models in our experiments below. But the development of tractable classes of (realistic) opponent models remains an interesting problem.</p><p>We provide a different perspective on Bayesian exploration than that described in <ref type="bibr" target="#b5">[5]</ref>. The value of performing an action ai at a belief state b can be viewed as involving two main components: an expected value with respect to the current belief state; and its impact on the current belief state. The first component is typical in RL, while the second captures the expected value of information (EVOI) of an action. Since each action gives rise to some "response" by the environment that changes the agent's beliefs, and these changes in belief can influence subsequent action choice and expected reward, we wish to (possibly indirectly) quantify the value of that information by determining its impact on subsequent decisions. EVOI need not be computed directly, but can be combined with "object-level" expected value through the following Bellman equations over the belief state MDP:</p><formula xml:id="formula_5">Q(ai, b) = a -i Pr(a-i|b) t Pr(t|ai • a-i, b) r Pr(r|ai • a-i, b)[r + γV (b( s, a, r, t ))]</formula><p>(3)</p><formula xml:id="formula_6">V (b) = max a i Q(ai, b)<label>(4)</label></formula><p>These equations describe the solution to the POMDP that represents the exploration-exploitation problem, by conversion to a belief state MDP. These can (in principle) be solved using any method for solving high-dimensional continuous MDPs-of course, in practice, a number of computational shortcuts and approximations will be required (as we detail below). We complete the specification with the </p><formula xml:id="formula_7">Pr(a-i|b) = σ -i Pr(a-i|σ-i)PS(σ-i) (5) Pr(t|a, b) = m Pr(t|s, a, m)PM (m)<label>(6)</label></formula><formula xml:id="formula_8">Pr(r|b) = m Pr(r|s, m)PM (m)<label>(7)</label></formula><p>We note that the evaluation of Eqs. 6 and 7 is trivial using the decomposed Dirichlet priors mentioned above. This formulation determines the optimal policy as a function of the BA's belief state. This policy incorporates the tradeoffs between exploration and exploitation, both with respect to the underlying (dynamics and reward) model, and with respect to the behavior of other agents. As with Bayesian RL and Bayesian learning in games, no explicit exploration actions are required. Of course, it is important to realize that this model may not converge to an optimal policy for the true underlying stochastic game. Priors that fail to reflect the true model, or unfortunate samples early on, can easily mislead an agent. But it is precisely this behavior that allows an agent to learn how to behave well without drastic penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Computational Approximations</head><p>Solving the belief state MDP above will generally be computationally infeasible. In specific MARL problems, the generality of such a solution-defining as it does a value for every possible belief state-is not needed anyway. Most belief states are not reachable given that an agent has a specific initial belief state. A more directed search-based solution technique can be used to solve this MDP for the agent's current belief state b. We consider a form of myopic EVOI in which only immediate successor belief states are considered, and their optimal values are estimated without using VOI or lookahead.</p><p>Formally, myopic action selection is defined as follows. Given belief state b, we define the myopic Q-function for each ai ∈ Ai as:</p><formula xml:id="formula_9">Qm(ai, b) = a -i Pr(a-i|b) t Pr(t|ai • a-i, b) r Pr(r|ai • a-i, b)[r + γVm(b( s, a, r, t ))] (8) Vm(b) = max a i m σ -i Q(ai, s|m, σ-i)PM (m)PS(σ-i) (9)</formula><p>The action performed is that with maximum myopic Q-value. Eq. 8 differs from Eq. 3 in the use of the myopic value function Vm, which is defined as the expected value of the optimal action at the current state, assuming a fixed distribution over models and strategies. <ref type="foot" target="#foot_7">8</ref> Intuitively, this myopic approximation performs one step-lookahead in belief space, then evaluates these successor states by determining the expected value to BA w.r.t. a fixed distribution over models, and a fixed distribution over successor states. This computation involves the evaluation of a finite number of successor belief states-A • R • S such states, where A is the number of joint actions, R is the number of rewards, and S is the size of the state space (unless b restricts the number of reachable states, plausible strategies, etc.). It is important to note that more accurate results can be realized by doing multistage lookahead, with the requisite increase in computational cost. Conversely, the myopically optimal action can be approximated by sampling successor belief states (using the induced distributions defined in Eqs. 5, 6, 7) if the branching factor A • R • S is problematic.</p><p>The final bottleneck involves the evaluation of the myopic value function Vm(b) over successor belief states. Note that the terms Q(ai, s|m, σ-i) are Q-values for standard MDPs, and can be evaluated using standard methods, but direct evaluation of the integral over all models is generally impossible. However, sampling techniques can be used <ref type="bibr" target="#b5">[5]</ref>. Specifically, some number of models can be sampled, the corresponding MDPs solved, and the expected Qvalues estimated by averaging over the sampled results. Various techniques for making this process more efficient can be used as well, including importance sampling (allowing results from one MDP to be used multiple times by reweighting) and "repair" of the solution for one MDP when solving a related MDP <ref type="bibr" target="#b5">[5]</ref>.</p><p>Furthermore, for certain classes of problems, such as repeated games, this evaluation can be performed directly. For instance, suppose a repeated game is being learned, and the BA's strategy model consists of fictitious play beliefs. The immediate expected reward of any action ai taken by the BA (w.r.t. a specific successor belief state b ) is given by its expectation with respect to its estimated reward distribution and fictitious play beliefs. The maximizing action a * i with highest immediate reward will be the best action at all subsequent stages of the repeated game-and has a fixed immediate reward r(a * i ) under the myopic (value) assumption that beliefs are fixed by b . Thus the long-term value at b is given by r(a * i )/(1-γ). The approaches above are motivated by approximating the direct myopic solution to the "exploration POMDP." A different approach to this approximation is proposed in <ref type="bibr" target="#b5">[5]</ref>. That algorithm does not do one-step lookahead in belief space, but rather estimates the (myopic) value of obtaining perfect information about Q(a, s). Suppose that, given an agent's current belief state, the expected value of action a is given by Q(a, s). Let a1 be the action with highest expected Q-value and state s and a2 be the second-highest. We define the gain associated with learning that the true value of Q(a, s) (for any a) is in fact q as follows:</p><formula xml:id="formula_10">gain s,a (q) = Q(a2, s) -q, if a = a1 and q &lt; Q(a2, s) q -Q(a1, s), if a = a1 and q &gt; Q(a1, s) 0, otherwise</formula><p>Intuitively, the gain reflects the effect on decision quality of learning the true Q-value of a specific action at state s. In the first two cases, what is learned causes us to change our decision (in the first case, the estimated optimal action is learned to be worse than predicted, and in the second, some other action is learned to be better than predicted). In the third case, no change in decision at s induced, so the information has no impact on decision quality. Adapted to our setting, a computational approximation to this approachcalled naive sampling in <ref type="bibr" target="#b5">[5]</ref>-involves the following steps:</p><p>(a) a finite set of k models is sampled from the density PM ;</p><p>(b) each sampled MDP j is solved (w.r.t. the density PS over strategies), giving optimal Q-values Q j (ai, s) for each ai in that MDP, and average Q-value Q(ai, s) (over all k MDPs);</p><p>(c) for each ai, compute the gain w.r.t. each Q j (ai, s); define EVPI(ai, s) to be the average (over the k MDPs) of gain s,a i (Q j (ai, s));</p><p>(d) define the value of ai to be Q(ai, s) + EVPI(ai, s) and execute the action with highest value.</p><p>This approach can benefit from the use of importance sampling and repair (with minor modifications). A key advantage of this approach is that it can be more computationally effective than one-step lookahead (which requires sampling and solving MDPs from multiple belief states). The price paid is approximation inherent in the perfect information assumption: the execution of joint action a does not come close to providing perfect information about Q(a, s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL RESULTS</head><p>We have conducted a number of experiments with both repeated games and stochastic games to evaluate the Bayesian approach. We focus on two-player identical interest games largely to compare to existing methods for "encouraging" convergence to optimal equilibria. The Bayesian methods examined are the one-step lookahead method (BOL) and the naive sampling method for estimating VPI (BVPI) described in Section 3. In all cases, the Bayesian agents use a simple fictitious play model to represent their uncertainty over the other agent's strategy. Thus at each iteration, a BA believes its opponent to play an action with the empirical probability observed in the past (for multistate games, these beliefs are independent at each state).</p><p>BOL is used only for repeated games, since these allow the immediate computation of expected values over the infinite horizon at the successor belief states: expected reward for each joint action can readily be combined with the BA's fictitious play beliefs to compute the expected value of an action (over the infinite horizon) since the only model uncertainty is in the reward. BVPI is used for both repeated and multi-state games. In all cases, five models are sampled to estimate the VPI of the agent's actions. The strategy priors for the BAs are given by Dirichlet parameters ("prior counts") of 0.1 or 1 for each opponent action. The model priors are similarly uninformative, with each state-action pair given the same prior distribution over reward and transition distributions (except for one experiment as noted).</p><p>We first compare our method to several different algorithms on a stochastic version of the penalty game described above. The game is altered so that joint actions provide a stochastic reward, whose mean is the value shown in the game matrix. <ref type="foot" target="#foot_8">9</ref> We compare the Bayesian approach to the following algorithms. KK refers to the algorithm of Kapetanakis and Kudenko <ref type="bibr" target="#b12">[12]</ref> that biases exploration to encourage convergence to optimal equilibria in exactly these types of games. Two related heuristic algorithms that also bias exploration optimistically are the Optimistic Boltzmann (OB) and Combined OB (CB) <ref type="bibr" target="#b4">[4]</ref> are also evaluated. Unlike KK, these algorithms observe and make predictions about the other agent's play. Finally, we test the more general algorithm WoLF-PHC <ref type="bibr" target="#b3">[3]</ref>, which works with arbitrary, general-sum stochastic games (and has no special heuristics for equilibrium selection). In each case the game is played by two learning agents of the same type. The parameters of all algorithms were empirically tuned to give good performance.</p><p>The first set of experiments tested these agents on the stochastic penalty game with k set to -20 and discount factors of 0.95 and 0.75. Both BOL and BVPI agents use uninformative priors over the set of reward values. <ref type="foot" target="#foot_9">10</ref> Results appear in in Figures <ref type="figure" target="#fig_0">1(a</ref> vides a suitable way to measure both the cost being paid in the attempt to coordinate as well as the benefits of coordination (or lack thereof). The results show that both Bayesian methods perform significantly better in this regard than the methods designed to force convergence to an optimal equilibrium. Indeed, OB, CB and KK converge to an optimal equilibrium in virtually all of their 30 runs, but clearly pay a high price for this. <ref type="foot" target="#foot_10">11</ref> By comparison, when γ = 0.95, the ratio of convergence to an optimal equilibrium, the nonoptimal equilibrium, or a nonequilibrium is, for BVPI 25/1/4 and for BOL 14/16/0. Surprisingly, WoLF-PHC does better than OB, CB and KK in both tests. In fact, this method outperforms the BVPI agent in the case of γ = 0.75 (this is due largely to the fact that the BVPI converges on a nonequilibrium 5 times and the suboptimal equilibrium 4 times). WoLF-PHC does converge in each instance to an optimal equilibrium.</p><p>We repeated this comparison on a more "difficult" problem with a mean penalty of -100 (and increasing the variance of the reward for each action). To test one of the benefits of the Bayesian perspective we included a version of BOL and BVPI with informative priors (giving it strong information about the value of the rewards). Results are shown in Figure <ref type="figure" target="#fig_0">1</ref>(c) (averaged over 30 runs). Wolf-PHC and CB converge to the optimal equilibrium each time, while the increased reward stochasticity made it impossible for KK and OB to converge at all. All four methods perform poorly w.r.t. discounted reward. The Bayesian methods perform much better. Not surprisingly, the agents BOL and BVPI with informative priors do better than their "uninformed" counterparts; however, because of the high penalty (despite the high discount factor), they converge to the suboptimal equilibrium most of the time (22 and 23 times, respectively).</p><p>We also applied the Bayesian approach to two identical-interest, multi-state, stochastic games. The first is a version of Chain World <ref type="bibr" target="#b5">[5]</ref> modified for multiagent coordination, and is illustrated in Figure <ref type="figure" target="#fig_1">2(a)</ref>. The optimal joint policy is for the agents to do action a at each state, though these actions have no payoff until state s5 is reached. Coordinating on b leads to an immediate, but smaller, payoff at each state, and resets the process. <ref type="foot" target="#foot_11">12</ref> Unmatched actions a, b and b, a result in zero-reward self-transitions (omitted from the diagram for clarity). Transitions are noisy, with a 10% chance that an agent's action has the "effect" of the opposite action. The original Chain World is difficult for standard RL algorithms, and is made especially difficult here by the requirement of coordination to make progress.</p><p>We compared BVPI to WoLF-PHC on this domain using two different discount factors, plotting the total discounted reward received (averaged over 30 runs) in Figure <ref type="figure" target="#fig_1">2(b)</ref> and<ref type="figure">(c</ref>). These graphs show the initial segments only, though the results project smoothly to 50000 iterations. We see that BVPI compares favorably to Wolf-PHC in terms of online performance. The Bayesian agents converged to the optimal policy in 7 (of 30) runs with γ = 0.99 and in 3 runs with γ = 0.75, intuitively reflecting the increased risk aversion due to increased discounting. WoLF-PHC rarely even managed to reach state s5, though in 2 (of 30) runs with γ = 0.75 it stumbled across s5 early enough to converge to the optimal policy. The Bayesian approach in this case manages to encourage intelligent exploration of action space in a way that trades off risks and predicted rewards; and we see increased exploration with the higher discount factor, as expected.</p><p>The second multi-state game is "Opt in or Out" game shown in Figure <ref type="figure" target="#fig_2">3</ref>(a). The transitions are stochastic, with the action selected by an agent having the "effect" of the opposite action with some probability. Two versions of the problem were tested, one with low "noise" (probability 0.05 of an action effect being reversed), and one with "medium" noise level (probability roughly 0.11). With low noise, the optimal policy is as if the domain were deterministic (the first agent opts in at s1 and both play a coordinated choice at s2), while with medium noise, the "opt in" policy and the "opt out" policy (where the safe play of moving to s6 is adopted) have roughly equal value. BVPI is compared to WoLF-PHC under two different discount rates, with low noise results shown in Figure <ref type="figure" target="#fig_2">3</ref>(b) and (c), and high noise results in Figure <ref type="figure" target="#fig_3">4</ref>(a) and (b). Again in terms of discounted reward, the Bayesian agents compare favorably to WoLF-PHC. In the low noise problem, BVPI converged to the optimal policy in 18 (of 30) runs with γ = 0.99 and 15 runs with γ = 0.75. The WoLF agents converged in the optimal policy only once with γ = 0.99, but 17 times with γ = 0.75. With medium noise, BVPI chose the "opt in" policy in 10 (0.99) and 13 (0.75) runs, but did manage to learn to coordinate at s2 even in the "opt out" cases. Interestingly, WoLF-PHC always converged on the "opt out" policy (recall both policies are optimal with medium noise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUDING REMARKS</head><p>We have described a Bayesian approach to the modeling of MARL problems that allows agents to explicitly reason about their uncertainty regarding the underlying domain and the strategies of their counterparts. We've provided a formulation of optimal exploration The experimental results presented here demonstrate quite effectively that Bayesian exploration enables agents to make the tradeoffs described. Our results show that this can enhance online performance (measured as reward accumulated while learning) of MARL agents in coordination problems, when compared to heuristic exploration techniques that explicitly try to induce convergence to optimal equilibria. This implies that BAs run the risk of converging on a suboptimal policy; but this risk is taken "willingly" through due consideration of the learning process given the agent's current beliefs about the domain. Still we see that BAs often find optimal strategies in any case. Key to this is a BA's willingness to exploit what it knows before it is very confident in this knowledge-it simply needs to be confident enough to be willing to sacrifice certain alternatives.</p><p>While the framework is general, our experiments were confined to identical interest games and the use of fictitious play-style beliefs as (admittedly simple) opponent models. These results are encouraging but need to be extended in several ways. Empirically, the application of this framework to more general problems is of critical importance to verify its utility. More work on computational approximations to estimating VPI or solving the belief state MDP is also needed. Finally, the development of computationally tractable means of representing and reasoning with distributions over strategy models is required.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Penalty Game Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Chain World Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: "Opt In or Out" Results: Low Noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: "Opt In or Out" Results: Medium Noise</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The existence of multiple equilibria can have a negative impact on known theoretical results for MARL<ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b10">10]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We will treat this distribution as if it has support over a finite set of possible values r, but more general density functions can be used.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We write D for the family of distributions for notational clarity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We assume reward densities are modeled similarly, with a Dirichlet prior over reward probabilities for each s. Dearden et al.<ref type="bibr" target="#b5">[5]</ref> allow for Gaussian reward distributions, which incurs no serious complications.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>A strategy in the repeated game is any mapping from the observed history of play to a (stochastic) action choice. This admits the possibility of modeling other agents's learning processes.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>Even then, more refined measures such as bias optimality might cast these techniques in a less favorable light.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>For example, should the BA believe that its opponent's strategy lies in the space of finite state controller that depends on the last two joint actions played, the BA will need to keep track of these last two actions. If it uses fictitious play beliefs (which can be viewed as Dirichlet priors) over strategies, no history need be maintained.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>We note that Vm(b) only provides a crude measure of the value of belief state b under this fixed uncertainty. Other measures could include the expected value of V (s|m, σ-i).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>Each joint action gives rise to X distinct rewards.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>Specifically, any possible reward (for any joint action) is given equal (expected) probability in the agent's Dirichlet priors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>All penalty game experiments were run to 500 games, though only the interesting initial segments of the graphs are shown.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>As above, rewards are stochastic with means shown in the figure.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequential optimality and coordination in multiagent systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Sixteenth International Joint Conference on Artificial Intelligence<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convergence problems of general-sum multiagent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="89" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rational and convergent learning in stochastic games</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Seventeenth International Joint Conference on Artificial Intelligence<address><addrLine>Seattle</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1021" to="1026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The dynamics of reinforcement learning in cooperative multiagent systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Claus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth National Conference on Artificial Intelligence</title>
		<meeting>the Fifteenth National Conference on Artificial Intelligence<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="746" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-based bayesian exploration</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dearden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Andre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="150" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Competitive Markov Decision Processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Filar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vrieze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The Theory of Learning in Games</title>
		<author>
			<persName><forename type="first">Drew</forename><surname>Fudenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A General Theory of Equilibrium Selection in Games</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Harsanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Selten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiagent reinforcement learning: Theoretical framework and an algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Wellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth International Conference on Machine Learning</title>
		<meeting>the Fifteenth International Conference on Machine Learning<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="242" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to play games in extensive form by valuation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jehiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAJ Economics, Peer Reviews of Economics Publications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rational learning leads to Nash equilibrium</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lehrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1019" to="1045" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Reinforcement learning of coordination in cooperative multi-agent systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kapetanakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kudenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth National Conference on Artificial Intelligence</title>
		<meeting>the Eighteenth National Conference on Artificial Intelligence<address><addrLine>Edmonton</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="326" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm for distributed reinforcement learning in cooperative multi-agent systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Machine Learning</title>
		<meeting>the Seventeenth International Conference on Machine Learning<address><addrLine>Stanford, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="535" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Markov games as a framework for multi-agent reinforcement learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on Machine Learning<address><addrLine>New Brunswick, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A generalized reinforcement-learning model: Convergence and applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Machine Learning</title>
		<meeting>the Thirteenth International Conference on Machine Learning<address><addrLine>Bari, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="310" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An iterative method of solving a game</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="296" to="301" />
			<date type="published" when="1951">1951</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stochastic games</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="327" to="332" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-agent reinforcement learning: Independent vs. cooperative agents</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning<address><addrLine>Amherst, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Reinforcement learning to play an optimal nash equilibrium in team markov games</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sandholm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 15 (NIPS-2002)</title>
		<meeting><address><addrLine>Vancouver</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
