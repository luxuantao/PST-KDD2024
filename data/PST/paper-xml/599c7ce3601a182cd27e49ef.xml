<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepSD: Supply-Demand Prediction for Online Car-hailing Services using Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<email>wang-dong12@mails</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
								<orgName type="laboratory">Bigdata Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
								<orgName type="laboratory">Bigdata Research Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
							<email>lijian83@mail.tsinghua.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Interdisciplinary Information Sciences</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
							<email>yejieping@didichuxing.com</email>
							<affiliation key="aff1">
								<orgName type="department">Didi Chuxing</orgName>
								<orgName type="laboratory">Bigdata Research Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepSD: Supply-Demand Prediction for Online Car-hailing Services using Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5F673EEC3A03A0D63E655C4DAA55531C</idno>
					<idno type="DOI">10.1109/ICDE.2017.83</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The online car-hailing service has gained great popularity all over the world. As more passengers and more drivers use the service, it becomes increasingly more important for the the car-hailing service providers to effectively schedule the drivers to minimize the waiting time of passengers and maximize the driver utilization, thus to improve the overall user experience. In this paper, we study the problem of predicting the real-time car-hailing supply-demand, which is one of the most important component of an effective scheduling system. Our objective is to predict the gap between the car-hailing supply and demand in a certain area in the next few minutes. Based on the prediction, we can balance the supply-demands by scheduling the drivers in advance. We present an end-to-end framework called Deep Supply-Demand (DeepSD) using a novel deep neural network structure. Our approach can automatically discover complicated supply-demand patterns from the car-hailing service data while only requires a minimal amount hand-crafted features. Moreover, our framework is highly flexible and extendable. Based on our framework, it is very easy to utilize multiple data sources (e.g., car-hailing orders, weather and traffic data) to achieve a high accuracy. We conduct extensive experimental evaluations, which show that our framework provides more accurate prediction results than the existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Online car-hailing apps/platforms have emerged as a novel and popular means to provide on-demand transportation service via mobile apps. To hire a vehicle, a passenger simply types in her/his desired pick up location and destination in the app and sends the request to the service provider, who either forwards the request to some drivers close to the pick up location, or directly schedule a close-by driver to take the order. Comparing with the traditional transportation such as the subways and buses, the online car-hailing service is much more convenient and flexible for the passengers. Furthermore, by incentivizing private cars owners to provide car-hailing services, it promotes the sharing economy and enlarges the transportation capacities of the cities. Several car-hailing mobile apps have gained great popularities all over the world, such as Uber, Didi, and Lyft. Large number of passengers are served and volume of carhailing orders are generated routinely every day. For example, Didi, the largest online car-hailing service provider in China, handles around 11 million orders per day all over China. 1   As a large number of drivers and passengers use the service, several issues arise: Sometimes, some drivers experience a hard time to get any request since few people nearby call the rides; At the same time, it is very difficult for some passengers to get the 1 Homepage: http://www.xiaojukeji.com/en/index.html ride, in bad weather or rush hours, because the demand in the surrounding areas significantly exceeds the supply. Hence, it is an very important yet challenging task for the service providers to schedule the drivers in order to minimize the waiting time of passengers and maximize the driver utilization. One of the most important ingredient of an effective driver scheduler is the supply-demand prediction. If one could predict/estimate how many passengers need the ride service in a certain area in some future time slot and how many close-by drivers are available, it is possible to balance the supply-demands in advance by dispatching the cars, dynamically adjusting the price, or recommending popular pick-up locations to some drivers.</p><p>In this paper, we study the problem of predicting the car-hailing supply-demand. More concretely, our goal is to predict the gap between the car-hailing supply and demand (i.e., max(0, demandsupply)) for a certain area in the next few minutes. Our research is conducted based on the online car-hailing order data of Didi. To motivate our approach, we first present some challenges of the problem and discuss the drawback of the current standard practice for such problem.</p><p>• The car-hailing supply-demand varies dynamically due to different geographic locations and time intervals. For example, in the morning the demand tends to surge in the residential areas whereas in the evening the demand usually tends to surge in the business areas. Furthermore, the supply-demand patterns under different days of a week can be extremely different. Prior work usually distinguishes different geographic locations, time intervals or days of week and build several sub-models respectively <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>.</p><p>Treating the order data separately and creating many submodels are tedious, and may suffer from the lack of training data since each sub-model is trained over a small part of data.</p><p>• The order data contains multiple attributes such as the timestamp, passenger ID, start location, destination etc, as well as several "environment" factors, such as the traffic condition, weather condition etc. These attributes together provide a wealth of information for supply-demand prediction. However, it is nontrivial how to use all the attributes in a unified model. Currently, the most standard approach is to come up with many "hand-crafted" features (i.e., feature engineering), and fit them into an off-the-shelf learning algorithm such as logistic regression or random forest. However, feature engineering typically requires substantial human efforts (it is not unusual to see data science/ machine learning practitioners creating hundreds different features in order to achieve a competitive performance) and there is little general principle how this should be done. Some prior work only keeps a subset of attributes for training, such as the timestamp, start location and drops other attributes <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b5">[6]</ref>. While this makes the training easier, discarding the attributes leads to the information loss and reduces the prediction accuracy.</p><p>To provide some intuitions for the readers and to illustrate the challenges, we provide an example in Fig. <ref type="figure">1</ref>.</p><p>Example 1: Fig. <ref type="figure">1</ref> shows the demand curves for two areas on March 9th (Wednesday) and March 13th (Sunday). From the figure, we can see very different patterns under different timeslots for the two areas. For the first area, few people require the car-hailing services on Wednesday. However, the demand increased sharply on Sunday. Such pattern usually occurs in the entertainment area. For the second area, we observe a heavy demand on Wednesday, especially during two peak hours around 8 o'clock and 19 o'clock (which are the commute times for most people during the weekdays). On Sunday, the demand of car-hailing services on this area reduced significantly. Moreover, the supply-demand patterns change from day to day. There are many other complicated factors that can affect the pattern, and it is impossible to list them exhaustively. Hence, simply using the average value of historic data or empirical supply-demand patterns can lead to quite inaccurate prediction results, which we show in our experiments (see Section VI).</p><p>To address the above challenges, we propose an end-toend framework for supply-demand prediction, called Deep Supply-Demand (DeepSD). Our framework is based on the deep learning technique, which has successfully demonstrated its power in a number of application domains such as vision, speech and natural language processing <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. In particular, we develop a new neural network architecture, that is tailored to our supply-demand prediction task. Our model demonstrates a high prediction accuracy, requires little hand-crafted feature, and can be easily extended to incorporate new dataset and features. A preliminary version of our model achieved the 2nd place among 1648 teams in the Didi supply-demand prediction competition. <ref type="foot" target="#foot_0">2</ref> Our technical contributions are summarized below:</p><p>• We proposed an end-to-end framework based on a deep learning approach. Our approach can automatically learn the patterns across different spatio-temporal attributes (e.g. geographic locations, time intervals and days of week), which allows us to process all the data in a unified model, instead of separating it into the sub-models manually.</p><p>Comparing with other off-the-shelf methods (e.g., gradient boosting, random forest <ref type="bibr" target="#b9">[10]</ref>), our model requires a minimal amount feature-engineering (i.e., hand-crafted features), but produces more accurate prediction results.</p><p>• We devise a novel neural network architecture, which is inspired by the deep residual network (ResNet) proposed very recently by He et al. <ref type="bibr" target="#b10">[11]</ref> for image classification.</p><p>The new network structure allows one to incorporate the "environment factor" data such as the weather and traffic data very easily into our model. On the other hand, we can easily utilize the multiple attributes contained in the order data without much information loss.</p><p>• We utilize the embedding method <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, a popular technique used in natural language processing, to map the high dimensional features into a smaller subspace. In the experiment, we show that the embedding method enhances the prediction accuracy significantly. Furthermore, with embedding, our model also automatically discovers the similarities among the supply-demand patterns of different areas and timeslots.</p><p>• We further study the extendability of our model. In real applications, it is very common to incorporate new extra attributes or data sources into the already trained model. Typically we have to re-train the model from the scratch. However, the residual learning component of our model can utilize these already trained parameters by a simple fine tuning strategy. In the experiment, we show that the fine-tuning can accelerate the convergence rate of the model significantly.</p><p>• Finally, we conduct extensive experiments on a large scale real dataset of car-hailing orders from Didi. The experimental results show that our algorithm outperforms the existing method significantly. The prediction error of our algorithm is 11.9% lower than the best existing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FORMULATION AND OVERVIEW</head><p>We present a formal definition of our problem. We divide a city into N non-overlapping square areas a 1 , a 2 , . . . , a N and each day into 1440 timeslots (one minute for one timeslot). Then we define the car-hailing orders in Definition 1. If the a driver answered the request, we say it is a valid order. Otherwise, if no driver answered the request, we say it is an invalid order. Definition 2 (Supply-demand Gap): For the d-th day, the supply-demand gap of the time interval [t, t + C) in area a is defined as the total amount of invalid orders in this time interval. We fix the constant C to be 10 in this paper <ref type="foot" target="#foot_1">3</ref> and we denote the corresponding gap as gap d,t a . We further collected the weather condition data and traffic condition data of different areas which we refer to as the environment data. Definition 3 (Weather Condition): For a specific area a at timeslot t in the d-th day, the weather condition (denoted as wc) is defined as a tuple: the weather type (e.g., sunny, rainy, cloudy etc.) wc.type, the temperature wc.temp and the PM2.5 wc.pm. All areas share the same weather condition at the same timeslot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Car-hailing Order</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 4 (Traffic Condition):</head><p>The traffic condition describes the congestion level of road segments in each area: from Level 1 (most congested) to Level 4 (least congested). For a specific area a at timeslot t in the d-th day, the traffic condition is defined as a quadruple: the total amount of road segments in area a under four congestion levels. Now, we can define our problem as below.</p><p>Problem Suppose the current date is the d-th day and the current time slot is t. Given the past order data and the past environment data, our goal is to predict the supply-demand gap gap d,t a for every area a, i.e., the supply-demand gap in the next 10 minutes.</p><p>Our paper is organized as follows. We first present several preliminaries in Section III, including the embedding method and the residual network which we mentioned in the introduction part. Then, we show a basic version of our model in Section IV. The basic version adopt a simple network structure and only uses the order data in the current day. In Section V, we present an advanced version which is an extension of the basic version. The advanced version utilize more attributes in the order data and it further incorporates the historical order data to enhance the prediction accuracy. In Section VI we conduct extensive experiment evaluations. Finally, we briefly review some related work in Section VII and conclude our paper in Section VIII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Embedding Method</head><p>Embedding method is a feature learning technique which is widely used in Deep Learning, especially in natural language processing (NLP) tasks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. It is a parameterized function which maps the categorical values to the real numbers. Specifically, neural networks treat every input as a real value. A simple way to transform a categorical value to real numbers is one-hot representation. For example, suppose the value of a categorical feature is 3 and the corresponding vocabulary size (highest possible value) is 5. Then, its one-hot representation is (0, 0, 1, 0, 0). However, using such representation can be The embedding method overcomes such issues by mapping each categorical value into a low-dimensional space (relative to the vocabulary size). For example, the categorical value with one-hot representation equal to (0, 0, 1, 0, 0) can be represented as the form of (0.2, 1.4, 0.5). Formally, for each categorical feature, we build an embedding layer with parameter matrix W ∈ R I×O . Here I is the vocabulary size of input categorical value and O is the dimension of the output space (which we refer to as the embedding space). For a specific categorical value i ∈ [I], we use onehot(i) ∈ R 1×I to denote its onehot representation. Then, its embedded vector embed(i) ∈ R 1×O is equal to onehot(i) multiply the matrix W , i.e., the i-th row of matrix W . We usually have that O I. Thus, even the vocabulary size is very large, we can still handle these categorical values efficiently. Furthermore, an important property of embedding method is that the categorical values with similar semantic meaning are usually very close in the embedding space. For example in our problem, we find that if two different areas share similar supply-demand patterns, then their area IDs are close in the embedding space. See Section VI for the details. We stress that the parameter matrix W in the embedding layer is optimized with other parameters in the network. We do not train the Embedding Layers separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Residual Network</head><p>Many non-trivial tasks have greatly benefited from very deep neural networks, which reveals that network depth is of crucial importance <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. However, an obstacle to train a very deep model is the gradient vanishing/exploding problem, i.e., the gradient vanishes or explodes after passing through several layers during the backpropagation <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. To overcome such issue in deep neural networks, He et al. <ref type="bibr" target="#b10">[11]</ref> proposed a new network architecture called the residual network (ResNet), which allows one to train very deep convolutional neural networks successfully.</p><p>The residual learning adds the shortcut connections (dashed line in Fig. <ref type="figure">2</ref>) and direct connections (solid line in Fig. <ref type="figure">2</ref>) between different layers. Thus, the input vector can be directly passed to the following layers though the shortcut connections. For example, in Fig. <ref type="figure">2</ref>, we use x to denote the input vector and H(x) to denote the desired mapping after two stacked layers. In the residual network, instead of learning the mapping function H(x) directly, we learn the residual mapping F(x) = H(x)-x and broadcast F(x) + x to the following layers. It has been shown that optimizing the residual mapping is much easier than optimizing the original mapping <ref type="bibr" target="#b10">[11]</ref>, which is the key to the success of deep residual network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. BASIC VERSION</head><p>We first present the basic version of our model in this section. In Section V, we extend the basic version with a few new ideas, and present the advanced version of our model. The basic model consists of three parts. Each part consists of one or more blocks (recall that the block is the base unit of our model). In Section IV-A, we first process the "identity features" (area ID, timeslot, day of week) in the identity part. Next in Section IV-B, we describe the order part which processes the order data. The order part is the most important part of our model. In Section IV-C, we present the environment part. The environment part processes the weather data and traffic data. Finally, in Section IV-D, we illustrate how we connect different blocks. The structure of our basic model is shown in Fig. <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Identity Part</head><p>The identity part consists of one block called the identity block. We call the features which identify the data item we want to predict as the "identity features". The identity features include the ID of area AreaID, the timeslot TimeID and the day of week (Monday, Tuesday, ..., Sunday) WeekID. For example, if we want to predict the supply-demand gap of area a in the time interval [t, t + 10) in the d-th day and that day is Monday, then we have that AreaID = a, TimeID = t and WeekID = 0.</p><p>Note that the features in the identity block are categorical. As we mentioned in Section III, we can either using the onehot representation or embedding representation to transform the categorical values to real numbers. In our problem, since the vocabularies of AreaID and TimeID are very large, the one-hot representation leads to a high cost. Moreover, the one-hot representation treats the different areas or timeslots independently. However, we find that different areas at different time can share similar supply-demand patterns, especially when they are spatio-temporally close. For example, the demands of car-hailing are usually very heavy for all the areas around the business center at 19:00. Clustering these similar data items helps enhance the prediction accuracy. In our model, we use Formally, the structure of the identity part is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We use three Embedding Layers to embed AreaID, TimeID and WeekID respectively. We then concatenate the outputs of three Embedding Layers by a Concatenate Layer. The Concatenate Layer takes a list of vectors as the input and simply outputs the concatenation of the vectors. We use the output of the Concatenate Layer as the output of the identity block, denoted as X id . Furthermore, we stress that prior work <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b18">[19]</ref> also clusters the similar data items to enhance the prediction accuracy. However, they treat the clustering stage as a separate sub-task and they need to manually design the distance measure, which is a non-trivial task. Our model is end-to-end and we can optimize the embedding parameters together with other parameters in the neural network. Hence we do not need to design any distance measure separately. The parameters are optimized through backpropagation towards minimizing the final prediction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Order Part</head><p>The order part in the basic version consists only one block called the supply-demand block. The supply-demand block can be regarded as a three layer perception, which processes the order data. For a specific area a, to predict the supply-demand gap gap d,t a of the time interval [t, t + 10) in the d-th day, we consider the order set with timestamp in [t -L, t) of the d-th day, which we denote as S d,t . Here L is the window size which is specified as 20 minutes in the experiment section (Section VI). We then aggregate S d,t into a real-time supply-demand vector. Definition 5 (Real-time supply-demand vector): For a specific area a, we define the real-time supply-demand vector in the d-th day at timeslot t as V d,t sd . V d,t sd is a 2L-dimensional vector, which consists of two parts. We denote the first</p><formula xml:id="formula_0">L dimensions of V d,t sd as V A d,t sd . The -th dimension of V A d,t</formula><p>sd is defined as:</p><formula xml:id="formula_1">V A d,t sd ( ) = |{o | o is valid ∧ o ∈ S d,t ∧ o.ts = t -}|</formula><p>In another word, V A d,t sd ( ) describes the amount of valid orders at tin the current day. Similarly, we define the remaining part as V B d,t sd which corresponds to the invalid orders in the previous L minutes.</p><p>We use V d,t sd as the Input Layer of the supply-demand block. We then pass V d,t sd through two Fully-Connected (abbr. FC) layers. A Fully-Connected Layer with input x is defined as </p><formula xml:id="formula_2">FC sz (x) = f (x • W + b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Environment Part</head><p>In the environment part, we incorporate the information from the weather data through adding the weather block to the network and the traffic data through the traffic block.</p><p>For the weather condition, we first create a weather condition vector V d,t wc . We show the structure of the weather block in Fig. <ref type="figure">6</ref>. The vector V d,t wc consists of L parts. For a specific ∈ [L], we have the weather condition wc at timeslot t-in the d-th day and we embed the weather type wc.type into a low dimensional space. Then the -th part of V d,t wc is defined as the concatenation of the embedded weather type wc.type, the temperature wc.temp and the PM 2.5 wc.pm. Furthermore, note that the weather block also receives the output of the supply-demand block X sd through a direct connection. We concatenate X sd and V d,t wc by a Concatenate Layer and pass the output of the Concatenate Layer through two Fully-Connected layers FC 64 and FC 32 . We denote the output of FC 32 as R wc . Then, the output of the weather block X wc is defined as:</p><formula xml:id="formula_3">X wc = X sd ⊕ R wc</formula><p>where ⊕ is the element-wise add operation and X sd is obtained through the shortcut connection.</p><p>Note that the structure we used here is similar with ResNet as we mentioned in Section III. However, there are two main differences between our model and ResNet. First, instead of adding shortcut connections between layers, we add the shortcut connections between different blocks. Second, in ResNet, a layer only receives the input from previous layers through a direct connection whereas in our model a block receives the inputs from both the previous block and the dataset. Such structure on one hand is more suitable for handling the data from multiple sources. On the other hand, we show that in Section VI-H, such structure is highly extendable. We can easily incorporate new datasets or attributes into our model based on such structure.</p><p>For the traffic condition, recall that at each timeslot the traffic condition of a specific area can be represented as the total amount of road segments in four different congestion levels. We thus create a traffic condition vector V d,t tc with L parts. Each part consists of four real values corresponding to Fig. <ref type="figure">6</ref>. Weather Block and Traffic Block the traffic condition at that time slot. We construct the traffic block in the same way as we construct the weather block. Then, we use X tc = X wc ⊕ R tc as the output of the traffic block, as shown in Fig. <ref type="figure">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Block Connections</head><p>We then connect all the blocks. Note that the supply-demand block, the weather block and the traffic block are already connected through the residual learning. The output vector of these stacked blocks is X tc . We then concatenate the output of the identity block X id and X tc with a Concatenate Layer. We append a Fully-Connected Layer FC 32 and a single neuron after the Concatenate Layer. The single neuron finally outputs the predicted supply-demand gap with the linear activation function, as shown in Fig. <ref type="figure" target="#fig_3">3</ref>. We stress that our model is endto-end, once we obtain the predicted value, we can calculate the loss based on the loss function and update each parameter with its gradient through backpropagation.</p><p>We further illustrate the intuition of our model. To predict the supply-demand gap, the most relevant and important data is the car-hailing order data. We use the supply-demand block to learn the useful feature vector from the order data. In our model, the learnt feature corresponds to the output of the supplydemand block, X sd . The environment data can be regarded as the supplementary of the learnt features. Thus, we add the weather block to extract the residual R wc and adjust the previous learnt features by adding R wc to X sd . The same argument holds for the traffic block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ADVANCED VERSION</head><p>In this section, we present an advanced version of our model. Comparing with the basic model, the advanced model replaces the order part in Fig. <ref type="figure" target="#fig_3">3</ref> with an extended order part as shown in Fig. <ref type="figure">7</ref>, which is composed of three blocks. The first block extended supply-demand block extends the original supply-demand block with a well-designed structure. Such structure enables our model to learn the dependence of the historical supply-demand over different days automatically, which we present in Section V-A. In Section V-B, we present the remaining two blocks, the last call block and the waiting time block, which have the same structure as the extended supply-demand block. Comparing with the basic version where we only use the number of orders, the new blocks contains passenger information as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Extended supply-demand block</head><p>Recall that in the basic version, we use the real-time supplydemand vector V d,t sd to predict the supply-demand gap. In the extended order block, we further incorporate the historical order data to enhance the prediction accuracy, i.e., the car-hailing orders with date prior to the d-th day. We present the extended supply-demand block in two stages. In the first stage, we obtain an empirical supply-demand vector in time interval [t -L, t) in the d-th day. Such empirical supply-demand vector is an estimation of V d,t sd based on the historical order data. In the second stage, we use the real-time supply-demand vector and the empirical supply-demand vector to construct our extended supply-demand block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) First Stage:</head><p>We first extract the empirical supply-demand vector in [t -L, t) in the d-th day, denoted as E d,t sd . It has been shown that due to the regularity of human mobility, the patterns in the traffic system usually show a strong periodicity in time on a weekly basis <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, for different days of week, the supply-demand patterns can be very different. For example, in Huilongguan, a district in Beijing where most of IT employees live, the demand of car-hailing services in Monday morning is usually much more than that in Sunday morning. Motived by this, we first consider the historical supply-demands in different days of week. Formally, we use M to denote all the Mondays prior to the d-th day. For each day m ∈ M, we calculate the corresponding realtime supply-demand vector in that day, denoted as V m,t sd as we defined in Definition 5. We average the vectors V m,t sd for all m ∈ M. We call such average the historical supply-demand vector on Monday, denoted as H (Mon),d,t sd . Thus, we have that,</p><formula xml:id="formula_4">H (Mon),d,t sd = 1 |M| m∈M V m,t sd .</formula><p>Similarly, we define the historical supply-demand vector on the other days of week: }. We refer to the weight vector as combining weights of different weekdays, denoted as p. In our model, such weight vector p is automatically learnt by the neural network according to the current AreaID and WeekID. The network structure is shown in Fig. <ref type="figure">8</ref>. We first embed the current AreaID and WeekID into a low-dimensional space. We concatenate the embedded vectors and pass it into a Softmax Layer. A Softmax Layer takes the concatenation x as the input and outputs the weight vector p by p (i) = e x•W.i j e x•W.j , ∀i = 1 . . . 7 where W .j is the j-th column of the parameter matrix W in the Softmax Layer. Then, we have that E d,t sd = p (1) • H (Mon),d,t sd + . . . + p (7) • H (Sun),d,t sd .</p><formula xml:id="formula_5">H (Tue),</formula><p>(1) We stress that most of prior work simply distinguish the historical data in weekdays and weekends separately [1]-[3], <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, on one hand, such method may suffer from the lack of training data. We only utilizes part of the data when we calculate the historical supply-demand vector. On the other hand, different areas can show different dependences over days of week. For example, in our experiment (Section VI), we find that for some areas, the supply-demands in Tuesdays are very different from the other days of week. Thus, to predict the supply-demand in Tuesday, we mainly consider the historical data in the past Tuesdays. For some other areas, the supplydemands in all the days of week are very similar. In this case, taking all the historical data into consideration leads to a more accurate result. Obviously, simply separating the historical data in weekdays and weekends can not such patterns.</p><p>2) Second Stage: Next, we use the obtained empirical supply-demand vector and real-time supply-demand vector to construct our block. First, using the same method as we obtain E d,t sd , we calculate another empirical supply-demand vector in time interval [t -L + 10, t + 10) in the current day, denoted as E d,t+10 sd . Note that E d,t+10 sd is the empirical estimation of the real-time supply-demand vector V d,t+10 sd . If we can estimate V d,t+10 sd accurately, we can easily predict the currently supplydemand gap.</p><p>In our model, we use the empirical estimations E d,t sd , E d,t+10 sd and the real-time supply-demand vector V d,t sd to estimate V d,t+10 sd . We first use the Fully-Connection Layers to project these three vectors onto the same low-dimensional space (in our experiment we fix the dimensionality to be 16). We denote the projected vectors as Proj(V d,t sd ), Proj(E d,t sd ) and Proj(E d,t+10 sd ). Instead of estimating V d,t+10 sd directly, we estimate the projection of V d,t+10 sd . We denoted the estimated ).</p><p>Finally, we concatenate Proj(V d,t sd ), Proj(E d,t sd ), Proj(E d,t+10 sd ), P roj(V d,t+10 sd ), with a Concatenate Layer and pass it through two Fully-Connected layers FC 64 and FC 32 . We use the output of FC 32 as the output of the extended supply-demand block. See Fig. <ref type="figure" target="#fig_7">9</ref> for an illustration.</p><p>We explain the reason that we estimate V d,t+10 sd in such way. The vector Proj(V d,t sd )-Proj(E d,t sd ) indicates how the realtime supply-demand of [t -L, t) deviates from its empirical estimation. We thus estimate Proj(V d,t+10 sd ) by adding such deviation to the projection of empirical estimation Proj(E d,t+10 sd ). Moreover, the projection operation on one hand reduce the dimension of each supply-demand vector from 2L to 16. On the other hand, we find that using the projection operation in our experiment makes our model more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Last Call Block and Waiting Time Block</head><p>In this section, we present two additional blocks, called the last call block and the waiting time block. Note that the order data contains multiple attributes. However, when calculating the supply-demand vector, we did not consider the attribute o.pid. Thus, the supply-demand vector V d,t sd does not contain any "passenger information". From V d,t sd , we can not answer the questions such as "how many unique passengers did not get the rides in the last 5 minutes" or "how many passengers waited for more than 3 minutes" etc. However, we find that the passenger information is also very important to supplydemand gap prediction. For example, if many passengers failed on calling the rides or waited for a long time, it reflects that the current demand exceeds the supply significantly which can lead to a large supply-demand gap in the next few minutes. We use the last call block and the waiting time block to provide the passenger information. Both of these two blocks have the same structure as the extended supply-demand block. In another word, we just replace the real-time supply-demand vector V d,t sd in the extended supply-demand vector with the real-time last call vector and real-time waiting time vector.</p><p>For the last call block, we define the last call vector as follow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6 (Real-time last call vector):</head><p>For a specific area a at timeslot t in the d-th day, we first pick out the last call orders in [t -L, t) for all passengers, (i.e. for a specific passenger pid, we only keep the last order sent by pid), and denote the order set as S L d,t . Then, the real-time last call vector V d,t lc is defined as a 2L-dimensional vector. We denote the first L dimension as V A d,t lc . For the -th dimension of V A d,t lc , we have that</p><formula xml:id="formula_6">V A d,t lc ( ) = |{pid | ∃o ∈ S L d,t s.t. o is valid ∧ o.pid = pid ∧ o.ts = t -}| V Alc ( ) d,t</formula><p>describes the amount of passengers whose last call is at tand she/he successfully got the ride. Similarly, we define V B d,t lc which corresponds to the passengers who did not get the rides.</p><p>We explain the reason that we define the real-time last call vector. In our data, we find that if a passenger failed on calling a ride, she/he is likely to sent the car-hailing request again in the next few minutes. Especially, the last calls near timeslot t are highly relevant to the supply-demand gap in [t, t + 10).</p><p>Based on V d,t lc , we can further obtain the empirical last call vector E d,t lc with the same way as we obtain E d,t sd . We thus construct the extended real-time last call block with the same structure as the extended supply-demand block.</p><p>For the waiting time block, we define the real-time waiting time vector V d,t wt ∈ R 2L in the same way as we defining V d,t sd and V d,t lc . Definition 7 (Real-time waiting time vector): For a specific area a at timeslot t in the d-th day, we define the real-time waiting time vector as V d,t wt . The -th dimension in the first part V A d,t wt (first L dimensions) is the total amount of passengers who waited for minutes (from her/his first call in [t -L, t) to the last call) and did get the rides at last. Similarly, we define the second part V B d,t wt which corresponds to the wait time of passengers who did not get the ride.</p><p>We thus construct the extended waiting time block with the same structure of the extended supply-demand block.</p><p>Finally, we connect the supply-demand block, the last call block and the waiting time block through residual learning, as shown in Fig. <ref type="figure">7</ref>. These three blocks together form the extended order part in the advanced model. We use the extended order part to replace the original order part and we thus obtain the advanced version of DeepSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extendability</head><p>Finally, in this section we present the extendability of our model. In real applications, it is very common to incorporate new extra attributes or data sources into the previous model. For example, imagine that we have already trained a model based on the order data and the weather data. Now we obtained the traffic data and we want to incorporate such data to enhance the prediction accuracy. Typically, we have to discard the already trained parameters and re-train the model from beginning. However, our model makes a good use of the already trained parameters. In our model, such scenario corresponds to that we have trained a model with the order block and the weather block. To incorporate the traffic data, we construct the traffic block and connect the traffic block with previous blocks through residual learning, as we show in Fig. <ref type="figure" target="#fig_3">3</ref>. Instead of re-training the model from the scratch, we use the already trained parameters as the initialized parameters and keep optimizing the parameters of the new model through backpropagation. We refer to such strategy fine-tuning. In the experiment (Section VI), we show that the fine-tuning accelerates the convergence rate significantly and makes our model highly extendable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENT</head><p>In this section, we report our experimental results on an real dataset from Didi. We first describe the details of our dataset in Section VI-A and the experimental setting in Section VI-B. Then, we compare our models with several other most popular machine learning algorithms in Section VI-C. In Section VI-D to Section VI-F, we show the effects of different components in our model. The advanced DeepSD can automatically extract the weights to combine the features of different days of a week. We present some interesting properties of the weights in Section VI-G. Finally, we show some results about the extendability of our model in Section VI-H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Description</head><p>In our experiment, we use the public dataset released by Didi in the Di-tech supply-demand prediction competition 4 .</p><p>The order dataset contains the car-hailing orders from Didi over more than 7 weeks of 58 square areas in Hangzhou, China. Each area is about 3km × 3km large. The order dataset consists of 11, 467, 117 orders. The gaps in our dataset is approximately power-law distributed. The largest gap is as large as 1434. On the other hand, around 48% of test items are supply-demand balanced, i.e., gap = 0. Auxiliary information include weather conditions (weather type, temperature, PM 2.5) and traffic conditions (total amount of road segments under different congestion levels in each area).</p><p>The training data is from 23th Feb to 17th March (24 days in total). To construct the training set, for each area in each training day, we generate one training item every 5 minutes from 0:20 to 24:00. Thus, we have 58(areas) × 24(days) × 283(items) = 393, 936 training items in total. Due to the restriction of test data, we set the window size L = 20.</p><p>The test data is from 18th March to 14th April (28 days in total). During the test days, the first time slot is 7:30 and the last time slot is 23:30. We select one time slot t every 2 hours from the first time slot unit the last time slot, i.e., t = 7:30, 9:30, 11:30, ..., 23:30. For each time slot t, we generate one test item. We use T to denote the set of test items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Error Metrics:</head><p>We evaluate the predicted results using the mean absolute error (MAE) and the root mean squared error (RMSE). Formally, we use pred d,t a to denote the predicted value of gap d,t a . Then, the mean absolute error and the root mean squared error can be computed as follows: </p><formula xml:id="formula_7">MAE = 1 |T | (a,d,t)∈T gap d,t a -pred d,t a RMSE = 1 |T | (a,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Model Details</head><p>We describe the model setting in this section.</p><p>1) Embedding: Recall that we map all the categorical values to a low-dimensional vectors via embedding (in Section IV-A and Section IV-C). The detailed settings of different embedding layers are shown in Table <ref type="table" target="#tab_1">I</ref>.</p><p>2) Activation Function: For all Fully-Connected layers, we use leaky rectified linear function (LReL) <ref type="bibr" target="#b22">[23]</ref> as the corresponding activation function. An LReL function is defined as:</p><formula xml:id="formula_8">LReL(x) = max{0.001 • x, x}.</formula><p>For the final output neuron, we simply use the linear activation.</p><p>3) Optimization Method: We apply the Adaptive Moment Estimation (Adam) method <ref type="bibr" target="#b23">[24]</ref> to train our model. Adam is a robust mini-batch gradient descent algorithm. We fix the batch size to be 64. To prevent overfitting, we further apply the dropout method <ref type="bibr" target="#b24">[25]</ref> with probability 0.5 after each block (except the identity block).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Platform:</head><p>Our model is trained on a GPU server with one GeForce 1080 GPU (8GB DDR5) and 24 CPU cores (2.1GHz) in Centos 6.5 platform. We implement our model with Theano 0.8.2, a widely used Deep Learning Python library <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison</head><p>We train both the basic model and advanced model for 50 epochs. We evaluate the model after each epoch. To make our model more robust, our final model is the average of the models in the best 10 epochs.</p><p>To illustrate the effectiveness of our model, we further compare our model with several existing methods. The parameters of all the models are fine-tuned through the grid search.</p><p>• Empirical Average: For a specific t in area a, we simply use the empirical average gap</p><formula xml:id="formula_9">1 |Dtrain| d∈Dtrain gap d,t a</formula><p>as the prediction for the supply-demand gap in time interval [t, t + 10).</p><p>• LASSO <ref type="bibr" target="#b9">[10]</ref>: The Lasso is a linear model that estimates sparse coefficients. It usually produces better prediction result than simple linear regression. Since LASSO can not handle the categorical variables, we transform each categorical variable to the one-hot representation. We use the LASSO implementation from the scikit-learn library <ref type="bibr" target="#b26">[27]</ref>. • Gradient Boosting Decision Tree: Gradient Boosting Decision Tree (GBDT) is a powerful ensemble method which is widely used in data mining applications. In our experiment, we use a fine-tuned and efficient GBDT implementation XGBoost <ref type="bibr" target="#b27">[28]</ref>. • Random Forest: Random Forest (RF) is another widely used ensemble method which offers comparable performance with GBDT. We use the RF implementation from the scikit-learn library <ref type="bibr" target="#b26">[27]</ref>.</p><p>For fair comparisons, we use the same input features for the above methods (except empirical average) as those used in DeepSD, including:</p><p>-AreaID, TimeID, WeekID -Real-time supply-demand vector V  <ref type="table" target="#tab_2">II</ref>, we can see that the empirical average gap is much larger than the other methods. By carefully tuning the parameters, LASSO provides a much better prediction result than the empirical average. GBDT achieves the best prediction accuracy among all existing methods, for both MAE and RMSE. The overall error of the RF is somewhat worse than that of LASSO. Our models significantly outperform all existing methods. Basic DeepSD only uses the real-time order data, yet already outperforms the other methods even when they use more input features. The advanced DeepSD achieves the best prediction results for both MAE and RMSE, which demonstrates its prediction power. The RMSE of the advanced DeepSD is 11.9% lower than the best existing method.</p><p>In Fig. <ref type="figure" target="#fig_9">10</ref>, we further enumerate a threshold and compare the models under different threshold. For a specific threshold, we evaluate the models on a subset of test data which has the gaps smaller than the threshold. Basic DeepSD shows a comparable result with GBDT for RMSE. However, for MAE, Basic DeepSD is significantly better than GBDT. For all the thresholds, Advanced DeepSD gives out the best result for both evaluations.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects of Embedding</head><p>Our model uses the embedding representation instead of one-hot representation for the categorical values. To show the effectiveness of embedding, we list in Table <ref type="table" target="#tab_4">III</ref> the errors of different models with both embedding representation and one-hot representation respectively. The experimental results show that utilizing the embedding methods improves both the time-cost and the accuracy.</p><p>Moreover, recall that in Section IV-A, we claim that the embedding technique can cluster the data with similar supplydemand patterns to enhance the prediction accuracy. To verify this, we consider the embedded vectors of different areas. We compare the supply-demand curves of different areas. We find that if two area IDs are close in the embedding space, their supply-demand patterns are very similar. As an example, we show the pairwise Euclidean distances among four different areas in the embedding space in Table <ref type="table" target="#tab_6">IV</ref>. We can see that in the embedding space, Area 3 is very close to Area 19 and Area 4 is very close to Area 24. We plot the car-hailing demand curves in 1st March in these areas, as shown in Fig. <ref type="figure" target="#fig_2">12(a</ref>) and Fig. <ref type="figure" target="#fig_2">12(b)</ref>. From the figure we can see that for the areas which are close in the embedding space, their demand curves are very similar. Meanwhile, for the areas which are far apart from each other, the corresponding demand curves are very different. More importantly, in the experiment, we find that our model is able to discover the supply-demand similarity under different scales. In another word, our model discovers the similarity of supply-demand "trends" regardless of the scales. For example, Fig. <ref type="figure" target="#fig_2">12(c)</ref> shows the demand curves of Area 4 and Area 46. The demands in these two areas are in different scales and the demand curves do not even overlap. However, the distance of these two areas obtained by our model in the embedding space is only 13.34. Actually, if we plot two demand curves under the same scale (as shown in Fig. <ref type="figure" target="#fig_2">12(d</ref>)), we can see that the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Effects of Environment Part</head><p>In our model, we incorporate the environment data (e.g., weather, traffic) to further improve the prediction accuracy. To show the effectiveness of supplementary part, we compare the performances of the models under different cases. In Case A, we only use the order part/extended order part. In Case B, we further incorporate the weather block. In Case C, we use all the blocks as we presented in our paper. Fig. <ref type="figure" target="#fig_3">13</ref> shows the prediction accuracies under different cases. Clearly, incorporating the environment data further reduce the prediction error for both the basic and advanced versions of DeepSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Effects of Residual</head><p>We adopt the residual learning technique to connect different blocks. To show the effects of residual learning, we eliminate all the shortcut/direct connections and simply concatenate all the   <ref type="table" target="#tab_6">V</ref>. We find that the residual learning improves the prediction accuracy effectively. In contrast, simply concatenating different blocks leads to a larger error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Combining Weights of Different Weekdays</head><p>Our DeepSD model learns the relative importance for different days of a week, and use a weight vector to combine the features for different days. Specifically, from the current AreaID and WeekID, we obtain a 7-dimensional vector p, which indicates the weights of different days of week (See   Equ.( <ref type="formula">1</ref>)). We visualize the weight vectors in two different areas at different days of week, as shown in Fig. <ref type="figure" target="#fig_13">15</ref>. The blue bars correspond to the weight vector at Tuesday, and the red bars correspond to the weight vector at Sunday. As we can see, the weight vector on the Tuesday is extremely different from that on the Sunday. If the current day is Sunday, the weight is only concentrated on the weekends. This also explains the effectiveness of distinguishing the data in weekdays and weekends which is used in prior work <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>. However, even for the same day of week, the weights in different areas can be different. For example in Fig. <ref type="figure" target="#fig_13">15</ref>(a), the weight of Tuesday is significantly higher than the other days whereas in Fig. <ref type="figure" target="#fig_13">15</ref>(b) the weight of all the days are relatively uniform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Extendability</head><p>As we claimed in Section I, our model is highly extendable. When introducing new attributes, we can utilize the previous trained model instead of re-training from the beginning. For example, we first train an advanced DeepSD model without the weather block and the traffic block. Now, as the weather data and the traffic data become available, we want to incorporate them to improve the prediction accuracy. For our model, we only need to add the weather block and the traffic block on top of the previous (trained) model and keep fine-refining the parameters. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prediction with Spatio-temporal Data</head><p>There is a large body of literature on learning and prediction with spatio-temporal data and we only mention a few closely related ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Taxi Route Recommendation:</head><p>The taxi route recommendation aims to predict the best routes for drivers in order to maximize their utilization. Yuan et al. <ref type="bibr" target="#b0">[1]</ref> presented an algorithm to suggest the taxi drivers with locations towards which he/she is most likely to pick up a passenger soon. They used a Poisson model to predict the probability of picking up a passenger for each parking place. In their work, the pick-up locations are fixed in advance. Our work aims to predict the supply-demand gap in every area. Wang et al. <ref type="bibr" target="#b28">[29]</ref> investigated the problem of recommending a cluster of roads to the taxi drivers. They used a single hidden layer neural network with carefully selected hand-crafted features. Our work uses a deep neural network with little hand-crafted features. Ge et al. <ref type="bibr" target="#b29">[30]</ref> provided a cost-efficient route recommendation algorithm which can recommend a sequence of pick-up locations. They learnt the knowledge from the historical data of the most successful drivers to improve the taxi driver utilization of remaining ones. However, such problem setting is much different from ours.</p><p>2) Taxi Demand Prediction: The taxi demand prediction studies the problem of forecasting the demands in every pick up location. Moreira-Matias et al. <ref type="bibr" target="#b4">[5]</ref> combined the Poisson Model and AutoRegressive Moving Average (ARMA) model to predict the demand in each taxi stand. Again, they only considered the demands in several fixed locations. Moreover, in their work they treated the data in each taxi stand separately. As we mentioned in Section I, such implementation suffers from the lack of training data. In a recent work, Chiang et al. <ref type="bibr" target="#b2">[3]</ref> proposed a generative model, called Grid-based Gaussian Mixture Model, for modeling spatio-temporal taxi bookings. Their approach was able to predict the demand of taxis in any time interval for each area in the city. Nevertheless, on one hand, they treated the orders in weekdays and weekends separately. On the other hand, in their approach, the total amount of taxi bookings was decided by a Poisson model in advance. When the real-time taxi demand changed rapidly, their approach may lead to a large prediction error.</p><p>We stress that prior work only studied the demand prediction but ignored the supply. In the real applications such as taxi route recommendation, taxi dispatching etc, it is important to predict the equilibrium of the supply-demand. Moreover, none of these work studied incorporating the environment data such as the weather or traffic conditions to enhance the prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Deep Learning</head><p>Recently, an increasing number of researchers studied applying the deep learning technique to prediction problems <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b33">[34]</ref>. However, few work studied the prediction with spatio-temporal data using deep learning. Lv et al. <ref type="bibr" target="#b34">[35]</ref> studied predicting the traffic flow with deep neural networks. They adopted a stack autoencoder to train the network layer by layer greedily. They showed that the deep model is more accurate comparing with the baseline methods. Zhang et al. <ref type="bibr" target="#b35">[36]</ref> designed a novel architecture called DeepST to predict the crowd flow. Their model learnt the spatio-temporal patterns by a sequence of convolutional neural networks. To the best of our knowledge, applying the deep learning technique to enhance car-hailing supply-demand prediction accuracy has not been studied so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>In this paper, we study the problem of predicting the realtime car-hailing supply-demand. We propose an end-to-end framework called Deep Supply-Demand (DeepSD), based on a novel deep neural network structure. Our approach automatically discovers the complicated supply-demand patterns in historical order, weather and traffic data, with minimal amount of hand-crafted features. We conduct extensive experiments on a real-word dataset from Didi. The experimental results show that our model outperforms the existing methods significantly. Furthermore, our model is highly flexible and extendible. We can easily incorporate new data sources or attributes into our model without re-training. We are currently working on incorporating our prediction model into the scheduling system of Didi.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )Fig. 1 .</head><label>a1</label><figDesc>Fig. 1. Car-hailing demands under four different situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>): A car-hailing order o is defined as a tuple: the date when the car-hailing request was sent o.d, the corresponding timeslot o.ts ∈ [1, 1440], the passenger ID o.pid, the area ID of start location o.loc s ∈ [N ] and the area ID of destination o.loc d ∈ [N ].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 +Fig. 2 .</head><label>12</label><figDesc>Fig. 2. Residual Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Structure of basic DeepSD</figDesc><graphic coords="4,77.47,69.58,198.95,165.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Identity Block</figDesc><graphic coords="4,363.73,69.58,139.44,85.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Structure of Supply-demand Block</figDesc><graphic coords="5,77.45,72.26,198.95,105.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig. 7. Extended Order Part</figDesc><graphic coords="6,359.05,72.31,149.36,114.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Extended Supply-demand Block</figDesc><graphic coords="7,76.90,72.27,198.95,128.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11</head><label>11</label><figDesc>Fig. 11 shows the prediction curves of the advanced model and that of GBDT (which performs the best among all other methods). The figure shows that GBDT is more likely to overestimate or underestimate the supply-demand gap under rapid variations. See the curves in the circles in the figure. Our model provide a relatively more accurate prediction result even under very rapid variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Accuracy under different thresholds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .Fig. 12 .Fig. 13 .</head><label>111213</label><figDesc>Fig. 11. Comparison of GBDT and DeepSD. See the curves in the circles, where the ground truth changes drastically.</figDesc><graphic coords="10,69.42,67.34,471.87,131.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. The network structure of Basic DeepSD without Residual Learning</figDesc><graphic coords="10,324.21,368.19,218.79,93.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>(a) Area 1 (b) Area 26</head><label>126</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Weight vectors combining different days of a week.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Fig. 16. Convergence results of re-training and the fine-tuning method..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig 16 shows the training curves of re-training and fine-tuning respectively. The experimental result shows that refining the parameters when incorporating new extra attributes effectively accelerates the convergence rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>.</cell><cell cols="2">EMBEDDING SETTING</cell></row><row><cell>Embedding Layers</cell><cell>Setting</cell><cell></cell><cell>Occurred Parts</cell></row><row><cell>Embedding of AreaID</cell><cell cols="2">R 58 → R 8</cell><cell>Identity Part, Extended Order Part</cell></row><row><cell>Embedding of TimeID</cell><cell cols="2">R 1440 → R 6</cell><cell>Identity Part</cell></row><row><cell>Embedding of WeekID</cell><cell cols="2">R 7 → R 3</cell><cell>Identity Part, Extended Order Part</cell></row><row><cell>Embedding of wc.type</cell><cell cols="2">R 10 → R 3</cell><cell>Environment Part</cell></row></table><note><p>d,t)∈T gap d,t apred d,t a 2 . 4 http://research.xiaojukeji.com/competition/main.action?competitionId= DiTech2016&amp;&amp;locale=en</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II .</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">PERFORMANCE COMPARISON</cell></row><row><cell>Model</cell><cell cols="2">Error Metrics MAE RMSE</cell></row><row><cell>Average</cell><cell>14.58</cell><cell>52.94</cell></row><row><cell>LASSO</cell><cell>3.82</cell><cell>16.29</cell></row><row><cell>GBDT</cell><cell>3.72</cell><cell>15.88</cell></row><row><cell>RF</cell><cell>3.92</cell><cell>17.18</cell></row><row><cell>Basic DeepSD</cell><cell>3.56</cell><cell>15.57</cell></row><row><cell>Advanced DeepSD</cell><cell>3.30</cell><cell>13.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>TableIIshows the comparison results. From Table</figDesc><table><row><cell cols="5">d,t sd ; Historical supply-demand vector of different days of week</cell></row><row><cell>H</cell><cell>(Mon),d,t sd</cell><cell>, . . . , H</cell><cell>(Sun),d,t sd</cell><cell>.</cell></row><row><cell cols="5">-Real-time last call vector V d,t lc ; Historical last call vector of different days of week H (Mon),d,t lc , . . . , H (Sun),d,t lc .</cell></row><row><cell cols="5">-Real-time waiting time vector V d,t wt ; Historical wait time vector of different days of week H (Mon),d,t wt , . . . , H (Sun),d,t wt .</cell></row><row><cell cols="5">-Weather conditions; Traffic conditions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>.</cell><cell cols="3">EFFECTS OF EMBEDDING</cell><cell></cell></row><row><cell>Representation</cell><cell>MAE</cell><cell cols="2">Basic DeepSD RMSE (per epoch) Time</cell><cell cols="2">Advanced DeepSD MAE RMSE</cell><cell>Time (per epoch)</cell></row><row><cell>One-hot</cell><cell>3.65</cell><cell>16.12</cell><cell>26.4s</cell><cell>3.42</cell><cell>14.52</cell><cell>49.8s</cell></row><row><cell>Embedding</cell><cell>3.56</cell><cell>15.57</cell><cell>22.8s</cell><cell>3.30</cell><cell>13.99</cell><cell>34.8s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V .</head><label>V</label><figDesc>EFFECTS OF RESIDUAL LEARNING</figDesc><table><row><cell></cell><cell></cell><cell>With</cell><cell cols="2">Without</cell></row><row><cell>Model</cell><cell cols="2">Residual Learning</cell><cell cols="2">Residual Learning</cell></row><row><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>Basic DeepSD</cell><cell>3.56</cell><cell>15.57</cell><cell>3.63</cell><cell>16.40</cell></row><row><cell>Advanced DeepSD</cell><cell>3.30</cell><cell>13.99</cell><cell>3.46</cell><cell>15.06</cell></row></table><note><p>blocks by a Concatenate Layer. We show the structure of basic DeepSD without residual learning in Fig 14. The advanced DeepSD without residual learning can be constructed in the same way. The experimental results are shown in Table</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>http://research.xiaojukeji.com/competition/main.action?competitionId= DiTech2016. The preliminary model we used for the competition was almost the same as the basic version of our model described in Section IV. Our final model, described in Section V, further refines the basic model by introducing a few new ideas, and is more stable and accurate. We are currently in an effort of deploying the model and incorporate it into the scheduling system in Didi.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The constant 10 (minutes) is due to the business requirement. It can be replaced by any other constant.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. ACKNOWLEDGEMENT</head><p>The research is supported in part by the National Basic Research Program of China Grant 2015CB358700, 2011C-BA00300, 2011C-BA00301, the National Natural Science Foundation of China Grant 61202009, 61033001, 61361136003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">T-finder: A recommender system for finding passengers and vacant taxis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2390" to="2403" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Crowdsourcing-based real-time urban traffic speed estimation: From trends to speeds</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd IEEE International Conference on Data Engineering</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-16">2016. May 16-20, 2016, 2016</date>
			<biblScope unit="page" from="883" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Where are the passengers?: a grid-based gaussian mixture model for taxi bookings</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sigspatial International Conference on Advances in Geographic Information Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling human location data with mixtures of kernel densities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lichman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting taxi-passenger demand using streaming data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moreira-Matias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mendes-Moreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Damas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1393" to="1402" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Where to wait for a taxi</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD International Workshop on Urban Computing</title>
		<meeting>the ACM SIGKDD International Workshop on Urban Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="149" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Investigation of recurrentneural-network architectures and learning methods for spoken language understanding</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="3771" to="3775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer series in statistics Springer</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="1337" to="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Travel cost inference from sparse, spatio temporally correlated time series using markov models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="769" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discovering spatiotemporal causal interactions in traffic data streams</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1010" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Etcps: An effective and scalable traffic condition prediction system</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Database Systems for Advanced Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="419" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transportation mode inference from anonymized and aggregated mobile phone call detail records</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Calabrese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">Di</forename><surname>Lorenzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ratti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Transportation Systems (ITSC), 2010 13th International IEEE Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="318" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Theano: A Python framework for fast computation of mathematical expressions</title>
		<author>
			<persName><forename type="first">Theano</forename><surname>Development</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Team</forename></persName>
		</author>
		<idno>abs/1605.02688</idno>
		<ptr target="http://arxiv.org/abs/1605.02688" />
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02754</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Taxirec: recommending road clusters to taxi drivers using ranking-based extreme learning machines</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page">53</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An energy-efficient mobile recommender system</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tuzhilin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 16th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep hybrid model for weather forecasting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Personal recommendation using deep recurrent neural networks in netease</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd IEEE International Conference on Data Engineering</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-16">2016. May 16-20, 2016, 2016</date>
			<biblScope unit="page" from="1218" to="1229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning for eventdriven stock prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Joint Conference on Artificial Intelligence (ICJAI15</title>
		<meeting>the 24th International Joint Conference on Artificial Intelligence (ICJAI15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2327" to="2333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Demand forecasting at low aggregation levels using factored conditional restricted boltzmann machine</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gibescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Power Systems Computation Conference (PSCC)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Traffic flow prediction with big data: a deep learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="865" to="873" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dnn-based prediction model for spatio-temporal data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Junbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dekang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ruiyuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiuwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGSPATIAL 2016</title>
		<imprint>
			<date type="published" when="2016-10">October 2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
