<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Yao</surname></persName>
							<email>tiyao@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
							<email>tmei@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yong</forename><surname>Rui</surname></persName>
							<email>yongrui@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<postCode>100080</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">P. R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
							<email>jluo@cs.rochester.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<postCode>14627</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Action Recognition by Learning Deep Multi-Granular Spatio-Temporal Video Representation *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B8C66B8EB9E97022EC8C99CB04C65C3</idno>
					<idno type="DOI">10.1145/2911996.2912001</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action Recognition</term>
					<term>Video Analysis</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing actions in videos is a challenging task as video is an information-intensive media with complex variations. Most existing methods have treated video as a flat data sequence while ignoring the intrinsic hierarchical structure of the video content. In particular, an action may span different granularities in this hierarchy including, from small to large, a single frame, consecutive frames (motion), a short clip, and the entire video. In this paper, we present a novel framework to boost action recognition by learning a deep spatio-temporal video representation at hierarchical multigranularity. Specifically, we model each granularity as a single stream by 2D (for frame and motion streams) or 3D (for clip and video streams) convolutional neural networks (CNNs). The framework therefore consists of multi-stream 2D or 3D CNNs to learn both the spatial and temporal representations. Furthermore, we employ the Long Short-Term Memory (LSTM) networks on the frame, motion, and clip streams to exploit long-term temporal dynamics. With a softmax layer on the top of each stream, the classification scores can be predicted from all the streams, followed by a novel fusion scheme based on the multi-granular score distribution. Our networks are learned in an end-to-end fashion. On two video action benchmarks of UCF101 and HMDB51, our framework achieves promising performance compared with the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recognizing actions in videos is one of the fundamental problems of computer vision for a wide variety of applications, ranging from video surveillance, indexing and re-ICMR <ref type="bibr">'16</ref> Figure <ref type="figure">1</ref>: An action may span different granularities. For example, the action of "playing piano" can be recognized from individual frames, "jumping jack" may have high correlation with the optical flow images (motion computed from consecutive frames), "cliff diving" should be recognized from a short clip since this action usually lasts for few seconds, while "basketball dunk" can be reliably identified at the video granularity due the complex nature of this action.</p><p>Recognizing actions therefore should take the hierarchical multi-granularity and spatio-temporal properties into consideration.</p><p>trieval, to human computer interaction <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b32">32]</ref>. However, video is an information-intensive media with large variations and complexities, e.g., intra-class variations caused by camera motion, cluttered background, illumination conditions, and so on. These have made action recognition a very challenging task. Moreover, as shown in Figure <ref type="figure">1</ref>, an action may span different granularities in a video including, from small to large, a single frame, consecutive frames (motion), a short clip, and the entire video. Therefore, recognizing actions in videos therefore should take the hierarchical multi-granularity and spatio-temporal properties into consideration.</p><p>There has been extensive research on video action recognition, including hand-crafted feature-based methods <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref> or deep learning of video representations <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b32">32]</ref>. The first category of research predominantly focuses on the detection of spatio-temporal interest points followed by the description of these points with local representations, while the deep learning methods heavily rely on using the Convolutional Neural Networks (CNNs) to learn visual appearance or the Recurrent Neural Networks (RNNs) to model the temporal dynamics in the video. However, most these methods treat video as a flat data sequence while ignoring the aforementioned intrinsic hierarchical structure of the video content deeply.</p><p>In this work, we aim at investigating a multi-granular architecture to learn the deep spatio-temporal video representation for action recognition. A video is represented by a hierarchical structure with multiple granularities, including from small to large, a single frame, consecutive frames (motion), a short clip, and the entire video. We model each video granularity as a single stream by 2D CNN (for frame and motion streams) or 3D CNN (for clip and video streams). The framework therefore learns both the spatial and temporal representations via the multi-stream 2D or 3D CNNs. Furthermore, we employ the Long Short-Term Memory (L-STM) networks on the frame, motion, and clip streams to exploit long-term temporal dynamics. With a softmax layer on the top of each stream, the classification scores can be predicted from each stream. A novel fusion scheme based on the multi-granular score distribution is proposed to predict the final recognition results, where the weights of each individual stream are learnt on the score distribution. This is an effective way to reflect the importance of each stream (and its components) to the overall action recognition. It is worth noting that the entire architecture is trainable in an end-to-end fashion.</p><p>The contributions of this paper are as follows.</p><p>• We propose a novel framework to learn a deep multigranular spatio-temporal representation for action recognition. The learned representation can capture not only the spatio-temporal nature of video sequence but also the contributions from different granularities for action recognition.</p><p>• We adopt the LSTM to model long-term temporal dynamics on the top of frame, motion and clip streams, and optimize the recognition results from all the streams through a novel fusion scheme based on the multi-granular score distribution.</p><p>• We conduct extensive experiments and show that our framework outperforms several state-of-the-art methods by clear margins on two well-known benchmarks.</p><p>The remaining sections are organized as follows. Section 2 describes related work on action recognition. Section 3 presents our multi-granular architecture for action recognition, while Section 4 formulates the importance of each component over the predicted score distribution. Implementation details are given in Section 5. Section 6 provides empirical evaluations on two popular datasets, i.e., UCF101 and HMDB51, followed by the conclusions in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Video action recognition has attracted intensive research attention. We briefly group the methods for action recognition into two categories: hand-crafted feature-based and deep learning-based methods.</p><p>Hand-crafted feature-based methods usually start by detecting spatio-temporal interest points and then describe these points with local representations. Many video representations are derived from image domain and extended to measure the temporal dimension of 3D volumes. For example, Laptev and Lindeberg propose space-time interest points (STIP) by extending the 2D Harris corner detector into 3D space <ref type="bibr" target="#b14">[14]</ref>. The global color moment feature <ref type="bibr" target="#b34">[34]</ref>, Histogram of Gradient (HOG) and Histogram of Optical Flow (HOF) <ref type="bibr" target="#b15">[15]</ref>, 3D Histogram of Gradient (HOG3D) <ref type="bibr" target="#b10">[10]</ref>, SIFT-3D <ref type="bibr" target="#b21">[21]</ref>, Extended SURF <ref type="bibr" target="#b31">[31]</ref>, and Cuboids <ref type="bibr" target="#b2">[2]</ref> are good descriptors as the local spatio-temporal features. Recently, Wang et al. propose dense trajectory features, which densely sample local patches from each frame at different scales and then track them in a dense optical flow field <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29]</ref>. The further improvements are achieved by the compensation of camera motion <ref type="bibr" target="#b6">[6]</ref>, and the use of advanced feature encoding methods such as Bag-of-Words (BoW) <ref type="bibr" target="#b15">[15]</ref> and Fisher Vectors <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b22">22]</ref>.</p><p>The most recent approaches to action recognition are to devise deep architectures for learning video representations. Qiu et al. perform action recognition using the Support Vector Machine with mean pooling of the CNN-based representations over frames <ref type="bibr" target="#b20">[20]</ref>. Karparthy et al. extend the CNN-based architecture by stacking visual features in a fixed size of windows and using spatio-temporal convolutions for video classification <ref type="bibr" target="#b9">[9]</ref>. Later in <ref type="bibr" target="#b27">[27]</ref>, Tran et al. employ 3D ConvNets trained on the Sports-1M dataset to learn video descriptors. Zha et al. leverage both spatial and temporal pooling on the CNN features computed on patches of video frames <ref type="bibr" target="#b35">[35]</ref>. The late fusion is then exploited to combine spatio-temporal representations. In the work by Wang et al. <ref type="bibr" target="#b30">[30]</ref>, the local ConvNet responses over the spatio-temporal tubes centered at the trajectories are pooled as the video descriptors. Fisher vector is then used to encode these local descriptors to a global video representation. Most recently, the LSTM-RNN networks have been successfully employed for modeling temporal dynamics in videos. In <ref type="bibr" target="#b16">[16]</ref>, temporal pooling and LSTM are used to combine frame-level (optical flow images) representation and discover long-term temporal relationships. Srivastava et al. further formulate the video representation learning as an autoencoder model, which consists of the encoder and decoder LSTMs <ref type="bibr" target="#b26">[26]</ref>. Wu et al. present an approach using regularization in neural networks to exploit feature and class relationships <ref type="bibr" target="#b32">[32]</ref>.</p><p>It can be observed that most existing methods treat video as a flat data sequence while ignoring the aforementioned intrinsic hierarchical structure of video content deeply. The most closely related work is the two-stream CNN approach <ref type="bibr" target="#b23">[23]</ref>. The work applies the CNN separately on visual frames and stacked optical flows. Our method is different from <ref type="bibr" target="#b23">[23]</ref> in that we extend two-stream to hierarchical multi-granular streams, employ 3D CNN to learn the spatio-temporal representation of video, and further utilize LSTM networks to model long-term temporal cues. In addition, our work is able to derive the fusion weights of each component from all the streams in a principled way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MULTI-GRANULAR ARCHITECTURE FOR ACTION RECOGNITION</head><p>Video is essentially an information-intensive media with multiple granularities. For example, a video can be represented by a hierarchical structure including, from large to small, the entire video, short clips, consecutive frames (called motion, or optical flow), and individual frames. Different granularity depicts distinct capability to describe different actions. Recognizing actions from videos should take this intrinsic structure into account. Motivated by the above observations, we exploit such hierarchical nature of video structure and decompose video representation into multi-  granular streams. We design a deep architecture consisting of modeling of individual streams, 2D/3D CNNs with LSTM networks for learning spatio-temporal representations, and a novel multi-granularity score distribution scheme for fusion. Figure <ref type="figure" target="#fig_1">2</ref> shows the overall framework of our proposed multi-granular architecture for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modeling Frame Stream</head><p>For action recognition, individual video frames can provide useful characteristics as some actions are strongly associated with particular scenes and objects. To make full use of static frame appearance, VGG 19 <ref type="bibr" target="#b24">[24]</ref>, the recent superior CNN architecture for image classification, is adopted to extract high level visual features for each sampled video frame. VGG 19 is a very deep convolutional network with up to 19 weight layers (16 convolutional layers and 3 fully-connected layers). Thanks to the pre-train process using large dataset from ImageNet challenge, the VGG 19 model can be used to extract plentiful visual concepts like scenes and objects. Thus, we choose the outputs of the fully-connected layer in VGG 19 as the deep representation of a single frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling Motion Stream</head><p>To model the displacement of consecutive frames, optical flow is extracted to describe the motion between consecutive frames. As the input to 2D CNN, the optical flow is firstly computed between a temporal window of consecutive frames with <ref type="bibr" target="#b1">[1]</ref>, and then converted to flow "image" by centering horizontal (x) and vertical (y) flow values around 128 and multiplying by a scalar such that flow values fall be-tween 0 and 255. By this transformation, we can obtain two channels of optical flow "image," while the third channel is created by calculating the flow magnitude. Furthermore, to suppress the displacement caused by camera motion, which may introduce additional noise into CNN, a global motion component is estimated by the mean vector of each flow and then subtracted from the flow <ref type="bibr" target="#b23">[23]</ref>. Although the input image is generated from optical flow, we also choose the pretrained VGG 19 architecture as used for frame stream, and then fine-tune on the extracted optical flow "images."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Modeling Clip and Video Streams</head><p>Besides static frame and motion information between consecutive frames, 3D CNN is used to construct video clip features from both spatial and temporal dimensions by utilizing 3D convolutions. Unlike traditional 2D CNN, 3D CNN architecture takes video clip (multiple continuous frames) as the inputs and consists of alternating 3D convolutional and 3D pooling layers, which are further topped by a few fullyconnected layers as described in <ref type="bibr" target="#b7">[7]</ref>. For describing video clip by more powerful feature using 3D CNN, we choose the superior architecture in <ref type="bibr" target="#b27">[27]</ref>, named C3D, which aims to learn spatio-temporal features for videos using 3D CNN trained on Sports-1M video dataset <ref type="bibr" target="#b9">[9]</ref>. As the deep network architecture is pre-trained on a large-scale video dataset, C3D can model general appearance and motion information simultaneously, which is important for action recognition. Similar to 2D CNN used for single frame and optical flow "image," we regard fully-connected layer outputs of C3D as the ex- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Modeling Temporal Dynamics with LSTM</head><p>To model the long-term temporal information in videos, we apply the Long Short-Term Memory (LSTM) on the frame, optical flow and video clip streams. The standard LSTM is a variant of RNN, which can capture long-term temporal information in the sequential data. To address the vanishing/exploding gradients issues when training traditional RNN, LSTM introduces a new structure called a memory cell. As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, a memory cell is composed of four main elements: an input gate, a neuron with a self-recurrent connection, a forget gate and an output gate. The self-recurrent connection has a weight of 1.0 and ensures that, barring any outside interference, the state of a memory cell can remain constant from one timestep to another. The gates serve to modulate the interactions between the memory cell itself and its environment. The input gate can allow incoming signal to alter the state of the memory cell or block it. On the other hand, the output gate can allow the state of the memory cell to have an effect on other neurons or prevent it. Finally, the forget gate can modulate the memory cell's self-recurrent connection, allowing the cell to remember or forget its previous state, as needed. Many improvements have been made to the LSTM architecture since its original formulation <ref type="bibr" target="#b5">[5]</ref> and we adopt the LSTM architecture as described in <ref type="bibr" target="#b17">[17]</ref>.</p><p>For timestep t, x t and h t are the input and output vector respectively, T are input weights matrices, R are recurrent weight matrices and b are bias vectors. Logic sigmoid σ(x) = 1 1+e -x and hyperbolic tangent φ(x) = e x -e -x e x +e -x are element-wise non-linear activation functions, mapping real values to (0, 1) and (-1, 1) separately. The dot product and sum of two vectors are denoted with and + respectively. Given inputs x t , h t-1 and c t-1 , the LSTM unit updates for timestep t are:</p><formula xml:id="formula_0">g t = φ(Tgx t + Rgh t-1 + bg) cell input i t = σ(Tix t + Rih t-1 + bi) input gate f t = σ(T f x t + R f h t-1 + b f ) f orgetgate c t = g t i t + c t-1 f t cell state o t = σ(Tox t + Roh t-1 + bo) output gate h t = φ(c t ) o t cell output</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">FUSION WITH MULTI-GRANULAR SCORE DISTRIBUTION (MSD)</head><p>Given the multi-granular streams in a video, we can predict the action score for each component from each stream. Inspired by the idea of addressing the temporal ambiguity of actions by learning score distribution in <ref type="bibr" target="#b4">[4]</ref>, we develop a multi-granular score fusion architecture in our deep networks rather than only using max or mean. Consequently, an improved action recognition score will be obtained by automatically aligning the relative importance to each component from all the streams based on the score distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formulation</head><p>Given a video, we combine the scores of all the components from multi-granular streams in a distribution matrix as</p><formula xml:id="formula_1">S = (s 1 , ..., sc, ..., s C ) ∈ L×C ,<label>(1)</label></formula><p>where sc ∈ L denotes the score column vector of L components from all the streams on the c th action class. Next, we will define a sort function on the score distribution matrix as sort(S) = sort(s 1 ), ..., sort(sc), ..., sort(s</p><formula xml:id="formula_2">C ) ∈ L×C ,<label>(2)</label></formula><p>where sort(sc) ∈ L is a function to reorder all elements of the vector sc in descending order. With large L, sort(sc) can represent the score distribution of all the components on the c th action class. Given N videos, each represented by the score distribution matrix sort(S), we can learn a MSD classifier by formulating the optimization problem as</p><formula xml:id="formula_3">min wc,bc N i=1 C c=1 max{1 -y c i (wc • sort(sc) + bc), 0}<label>(3)</label></formula><formula xml:id="formula_4">s.t. L l=1 w l c = 1, c = 1, ..., C ,<label>(4)</label></formula><formula xml:id="formula_5">w 1 c ≥ w 2 c ≥ ... ≥ w L c ≥ 0, c = 1, ..., C.<label>(5)</label></formula><p>For the c th action class, the weight vector wc and the bias item bc can separate score distributions of positive and negative data. The loss function (3) is the sum of Hinge Losses. The constraint (4) requires the weights to have unit sum because we are learning weights for each component. The constraint <ref type="bibr" target="#b5">(5)</ref> requires the weights to be monotonic and non-negative, because sort(sc) are classification scores in descending order and we want to emphasize the relative importance of the components with high classification scores.</p><p>From the constraints, we can see that the feasible set of wc includes two special cases: 1)</p><formula xml:id="formula_6">w 1 c = 1, w 2 c = ... = w L c = 0, 2) w 1 c = w 2 c = ... = w L c = 1 L .</formula><p>The former corresponds to max pooling, while the later refers to mean pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Solution</head><p>As our architecture is an "end-to-end" neural network which is learnt by using standard backpropagation, we solve the optimization problem (3) by using stochastic gradient descent in a deep learning framework Caffe <ref type="bibr">[8]</ref>. However, the two constraints (4) and ( <ref type="formula" target="#formula_5">5</ref>) make the optimization difficult to be solved. To address this problem, we relax the two constraints by appending two penalty terms to the cost function J as</p><formula xml:id="formula_7">J = L + α C c=1 wc 2 + β C c=1 1 - L l=1 w l c 2 + γ C c=1 L l=1 m l c ,<label>(6)</label></formula><formula xml:id="formula_8">m l c = w l+1 c -w l c , if w l+1 c &gt; w l c 0, if w l+1 c ≤ w l c l = 1, ..., L and w L+1 c = 0,<label>(7)</label></formula><p>where the first part L is the loss function in Eq. ( <ref type="formula" target="#formula_3">3</ref>), the second is a regularization term preventing over-fitting, the rest are two penalty terms and α, β, γ are the tradeoff parameters. Finally, the above cost function J is minimized with respect to {wc} C c=1 and the gradients are calculated by </p><formula xml:id="formula_9">∂J ∂w l c = ∂L ∂w l c + 2αw l c -2β(1 -w l c ) + γ ∂m l c ∂w l c + ∂m l-1 c ∂w l c ,<label>(8)</label></formula><formula xml:id="formula_10">∂m l c ∂w l c = -1, if w l+1 c &gt; w l c 0, if w l+1 c ≤ w l c l = 1, ..., L ,<label>(9)</label></formula><formula xml:id="formula_11">)<label>10</label></formula><p>Each gradient is calculated upon a sorted score distribution. As the score order changes in each SGD iteration, backpropagation of the sorting operator is similar to that of max pooling layer. We store the index of sorted score in original vector and propagate the gradients to the corresponding element in the original vector when backpropagation. After the optimization of J in Eq. ( <ref type="formula" target="#formula_7">6</ref>), we can obtain the optimal {wc} C c=1 . With this, we compute the final improved action score for the video as</p><formula xml:id="formula_12">pc = wc • sort(sc) + bc. (<label>11</label></formula><formula xml:id="formula_13">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">IMPLEMENTATIONS</head><p>Frame stream. We uniformly select 25 frames per video and adopt the VGG 19 <ref type="bibr" target="#b24">[24]</ref> to extract frame features. The VGG 19 is first pre-trained with the ILSVRC-2012 training set of 1.2 million images and then fine-tuned by using the video frames, which is observed to be better than training from scratch. Following <ref type="bibr" target="#b23">[23]</ref>, we also use data augmentation like cropping and flipping. The learning rate starts from 10 -3 and decreases to 10 -4 after 14,000 iterations, then to 10 -5 after 20,000 iterations. For temporal modeling, we extract the outputs of 4096-way fc6 layer from VGG 19 as inputs and adopt one-layer LSTM. We conduct experiments with different number of hidden states in LSTM. The LST-M weights are learnt by using the BPTT algorithm with a mini-batch size of 10. The learning rate starts from 10 -2 and decreases to 10 -3 after 100K iterations. The training is stopped after 150,000 iterations. Motion stream. We compute the optical flow between consecutive frames using the GPU implementation of <ref type="bibr" target="#b1">[1]</ref> in OpenCV toolbox. The optical flow is converted to a flow "image" by linearly rescaling horizontal (x) and vertical (y) flow values to [0, 255] range. The transformed x and y flows are the first two channels for the flow image and the third channel is created by calculating the flow magnitude. Moreover, the settings of VGG 19 and LSTM are the same with frame stream.</p><p>Clip stream. We define a clip as consecutive 16 frames, which is the same setting as <ref type="bibr" target="#b27">[27]</ref>. The C3D is exploited to model video clip, which is pre-trained on Sports-1M <ref type="bibr" target="#b9">[9]</ref> dataset with 1.1 million sports videos and then fine-tuned on UCF101 and HMDB51, respectively. As designed in C3D architecture, the input of C3D model is 16-frame clip and we uniformly sample 20 clips in each video. The learning rate starts from 10 -4 and decreases to 10 -5 after 10,000 iterations, then the training is stopped after 20,000 iterations. Again, the LSTM setting is the same with frame stream.</p><p>Video stream. The settings of video stream are similar to the clip stream. The only difference is that we do not involve LSTM after C3D and simply fuse the features of all video clips by mean pooling to generate the video-level representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>We empirically evaluate our multi-granular framework on the UCF101 <ref type="bibr" target="#b25">[25]</ref> and HMDB51 <ref type="bibr" target="#b12">[12]</ref> datasets. The UCF101 dataset is one of the most popular action recognition benchmarks. It consists of 13, 320 videos from 101 action categories. The action categories are divided into five groups: Human-Object Interaction, Body-Motion Only, Human-Human Interaction, Playing Musical Instruments, and Sports. The HMDB51 dataset contains 6, 849 video clips divided into 51 action categories, each containing a minimum of 101 clips. The experimental setup is the same for both datasets and three training/test splits are provided by the dataset organisers. Each split in UCF101 includes about 9.5K training and 3.7K test video, while a HMDB51 split we conduct our analyses of different streams on the first split of the UCF101 and HMDB51 datasets. The average accuracy over three splits on both datasets are reported when compared with the state-of-the-art techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation of Frame and Motion Streams</head><p>We first examine the recognition performances of frame and optical flow streams from three aspects: 1) when different 2D CNN are used, 2) when LSTM is utilized to explore longer-term temporal information, and 3) how performance is affected by the size of hidden layer in LSTM learning.</p><p>The results and comparisons on UCF101 (split 1) are summarized in Table <ref type="table" target="#tab_1">1</ref>. Table <ref type="table" target="#tab_1">1a</ref> compares the accuracy of different CNN and LSTM on frame and optical flow stream, respectively, while Table <ref type="table" target="#tab_1">1b</ref> compares the performances with the hidden layer size of LSTM in the range of 128, 256, 512, 1024, and 2048. Compared to AlexNet <ref type="bibr" target="#b11">[11]</ref>, VGG 19 <ref type="bibr" target="#b24">[24]</ref> with a deeper CNN exhibits significantly better on frame stream. Interestingly, using VGG 19 on optical flow stream gives only marginal performance gain. By additionally utilizing LSTM to explore longer-term temporal information, the improvements can be expected on both frame and optical flow streams. Furthermore, when augmenting the test frame (flow image) by cropping and flipping four corners and the center of the frame and averaging the scores across the frame and its crops, the performance can achieve 80.2% and 74.6% on frame and optical flow, respectively.</p><p>In general, increasing the hidden layer size of LSTM can lead to the improvement of the accuracy. When the hidden layer size reaches 1024 in our case, no further improvement can be obtained on both frame and optical flow streams. Note that the performances are reported based on the original frame or optical flow image with only cropping center and no flipping operation in this comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Evaluation of Clip and Video Streams</head><p>Next, we turn to measure the performance of the clip and video streams in terms of features extracted from different layers of 3D CNN (C3D) on both datasets. We extract C3D features: fc6, fc7, and prob for each video clip. The recognition score is computed by late fusing the predicted score on each video clip and the accuracy comparison by using the outputs from these three different layers is shown in Table <ref type="table" target="#tab_2">2a</ref>. As indicated by our results, the recognition using the C3D feature of fc6 layer leads to a larger performance boost against the C3D features of fc7 and prob layers. Furthermore, the accuracy by using the feature of fc6 can achieve Table <ref type="table">4</ref>: The performance in terms of mean accuracy (over three splits) on UCF101 and HMDB51. Please note that the methods in <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b13">13]</ref> are based on traditional dense trajectory which is computationally expensive, while the methods in <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b30">30]</ref> combine dense trajectory and deep learning based algorithms. Our approach outperforms the deep learningbased methods without combination of dense trajectory <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b16">16]</ref> with a large margin. "-" means that the authors did not report their performance on this dataset. IDT: improved dense trajectory <ref type="bibr" target="#b29">[29]</ref>; MIFS: Multi-skip Feature Stacking <ref type="bibr" target="#b13">[13]</ref>; LRCN: Long-term Recurrent Convolutional Networks <ref type="bibr" target="#b3">[3]</ref>; C3D: Convolutional 3D <ref type="bibr" target="#b27">[27]</ref>; TDD: Trajectory-pooled Deep-convolutional Descriptor <ref type="bibr" target="#b30">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>UCF101 HMDB51 IDT <ref type="bibr" target="#b29">[29]</ref> 85.9% 57.2% IDT w/ Encodings <ref type="bibr" target="#b18">[18]</ref> 87.9% 61.1% MIFS <ref type="bibr" target="#b13">[13]</ref> 89.1% 65.1% "Slow Fusion" ConvNet <ref type="bibr" target="#b9">[9]</ref> 65.4% -LRCN <ref type="bibr" target="#b3">[3]</ref> 82.9% -C3D <ref type="bibr" target="#b27">[27]</ref> 85.2% -Two-stream model <ref type="bibr" target="#b23">[23]</ref> 88.0% 59.4% Composite LSTM <ref type="bibr" target="#b26">[26]</ref> 84.3% -CNN + IDT <ref type="bibr" target="#b35">[35]</ref> 89.6% -Temporal Pooling + LSTM <ref type="bibr" target="#b16">[16]</ref> 88.6% -TDD <ref type="bibr" target="#b30">[30]</ref> 90.3% 63.2% Ours 90.8% 63.6%</p><p>51.3% and 83.9% on HMDB51 and UCF101 after longerterm temporal modeling with LSTM networks, respectively. The features for video stream are computed by averaging the video clip features separately for each type of feature and Table <ref type="table" target="#tab_2">2b</ref> reports the comparison of different C3D features on video stream. Similar to the observations on video clip stream, the features of fc6 layer achieves the best performance among all the three layers with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation of MSD</head><p>Here we evaluate the complete multi-granular architecture, which combines the four streams with the MSD fusion method. Table <ref type="table" target="#tab_3">3</ref> details the accuracy across different fusion strategies on three splits of HMDB51 and UCF101, respectively. MSD consistently outperforms Max and Mean in every split of both two datasets. The improvement is observed in different types of actions. For instance, the actions "playing piano" and "biking" are better fused with Mean as the videos relevant to the two actions are consistent in content. On the other hand, the recognition of actions "cliff diving" and "basketball dunk" show much better results with Max fusion. In the experiment, MSD boosts the accuracy of these actions. Figure <ref type="figure" target="#fig_4">4</ref> shows the top eight weights learnt by MSD and their corresponding components of three exemplary videos from category "baseball pitch,""front crawl," and "hammering." We can easily see that all the eight components are highly related to each action. More importantly, the top eight components come from four different streams, which validates the effectiveness of MSD on fusing multigranular information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparisons with State-of-the-Art</head><p>We compare with several state-of-the-art techniques on three splits of UCF101 and HMDB51. As shown in Table <ref type="table">4</ref>, our multi-granular spatio-temporal architecture shows the  baseball pitch, middle: front crawl, bottom: hammering). We can see that MSD is able to learn the contributions from different components for particular actions. For example, two clip components play important roles for recognizing "baseball pitch," while two motion (optical flow) components contribute more to the recognition of "hammering."</p><p>best performance on UCF101 dataset. It makes the improvement over <ref type="bibr" target="#b30">[30]</ref> by 0.5%, which is generally considered as a significant progress on this dataset. On the HMDB51, the works <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b30">30]</ref> with competitive results are based on the motion trajectory, while our approach fully relies on the deep learning architecture and is trained end-to-end. Compared with the two-stream model <ref type="bibr" target="#b23">[23]</ref>, our architecture by additionally incorporating more temporal modeling and utilizing a sophisticated fusion strategy leads to a performance boost on both datasets. It is also worth noting that in the training of the HMDB51 dataset, the work in <ref type="bibr" target="#b23">[23]</ref> exploits UCF101 as additional training data through multi-task learning, while our approach is purely trained on HMDB51 only. In addition, the recent works in <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b26">26]</ref> also use the LSTM to exploit temporal information. Our approach achieves more promising results as more dimensions of cues are included. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Run Time</head><p>Table <ref type="table" target="#tab_4">5</ref> listed the detailed run time of each stream averaged over all test videos in UCF101 dataset. The experiments are conducted on a regular server (Intel Xeon 2.40GHz CPU and 256 GB RAM) with a single NVidia K80 GPU. As each stream could be executed in parallel and the fusion with MSD provides instant responses, the average prediction time of our architecture on each video in UCF101 is about 762 milliseconds, which is very efficient. This is much faster than trajectory-based approaches, e.g. IDT, which requires about seven minutes on each video in UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>We have presented a multi-granular deep architecture for action recognition in videos, which is able to incorporate information at a multitude of granularity including frame, consecutive frames (motion), clip and the entire video. Specifically, we model each granularity with two types of CNNs (2D CNN trained on frame and motion streams, and 3D C-NN on clip and video streams). We employ LSTM networks to incorporate long-term temporal modeling based on the granularity features. To fuse the recognition scores of individual components in multiple streams, the distribution of scores is exploited to find the optimal weight for each component. We show that our approach outperforms several state-of-the-art methods on two benchmark datasets.</p><p>Our future works are as follows. First, video action recognition can be enhanced by further considering audio information. The audio features can be exploited together with current four streams to more comprehensively characterize the actions. Second, the method of learning the representations of the entire video could be explored by using RNNs in an encoder-decoder framework. In addition, we will continue to conduct more in-depth investigations on how fusion weights of individual streams can be dynamically determined to boost the action recognition performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Frames</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multi-granular spatio-temporal architecture for video action recognition. A video is represented by the hierarchical structure with multiple granularities including, from small to large, frame, consecutive frames (motion), clip, and video. Each granularity is modeled as a single stream. 2D CNNs are used to model the frame and motion (optical flow images) streams, while 3D CNNs are used to model the clip and video streams. LSTMs are used to further model the temporal information in the frame, motion, and clip streams. A softmax layer is built on the top of each stream to obtain the prediction from each component. Suppose we have Nc clips, Np motions (consecutive frame pairs), and N f frames, then we have Nc + Np + N f + 1 components. The final action recognition result of the input video is obtained by linearly fusing the prediction scores from all the components with the weights learned on the score distribution. Note that this deep architecture is trainable in an end-to-end fashion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A diagram of a LSTM memory cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples showing the top eight weights learned by the MSD and their corresponding components in a video (top:baseball pitch, middle: front crawl, bottom: hammering). We can see that MSD is able to learn the contributions from different components for particular actions. For example, two clip components play important roles for recognizing "baseball pitch," while two motion (optical flow) components contribute more to the recognition of "hammering."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, June 06-09, 2016, New York, NY, USA c 2016 ACM. ISBN 978-1-4503-4359-6/16/06. . . $15.00 DOI: http://dx.doi.org/10.1145/2911996.2912001</figDesc><table><row><cell>playing piano</cell><cell></cell></row><row><cell>jumping jack</cell><cell>optical flow</cell></row><row><cell>cliff diving</cell><cell></cell></row><row><cell>basketball dunk</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The accuracy of frame and motion streams on UCF101 (split 1).(a) The accuracy of different 2D CNN and LSTM used on frame and motion streams. The results are reported for late fusion.</figDesc><table><row><cell>Training setting</cell><cell>Frame Motion</cell></row><row><cell>AlexNet</cell><cell>67.1% 68.4%</cell></row><row><cell>AlexNet + LSTM</cell><cell>69.3% 70.3%</cell></row><row><cell>VGG 19</cell><cell>77.9% 70.6%</cell></row><row><cell>VGG 19 + LSTM</cell><cell>79.3% 73.8%</cell></row><row><cell cols="2">VGG 19 + LSTM + Augmentation 80.2% 74.6%</cell></row><row><cell cols="2">(b) The effect of hidden layer size in the LSTM (VGG 19).</cell></row><row><cell cols="2">Hidden layer size Frame Motion</cell></row><row><cell>128</cell><cell>78.2% 71.2%</cell></row><row><cell>256</cell><cell>78.8% 72.6%</cell></row><row><cell>512</cell><cell>79.1% 73.5%</cell></row><row><cell>1024</cell><cell>79.3% 73.8%</cell></row><row><cell>2048</cell><cell>78.5% 73.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The accuracy of clip and video streams on UCF101 (split 1) and HMDB51 (split 1).</figDesc><table><row><cell cols="5">(a) The comparisons of using features from different layers of C3D</cell></row><row><cell>on clip stream.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>fc6</cell><cell>fc7</cell><cell>prob</cell><cell>fc6+LSTM</cell></row><row><cell cols="4">HMDB51 50.36% 48.65% 38.97%</cell><cell>51.3%</cell></row><row><cell>UCF101</cell><cell cols="3">83.11% 81.23% 69.81%</cell><cell>83.9%</cell></row><row><cell cols="5">(b) The comparisons of using the features from different layers of</cell></row><row><cell cols="2">C3D on video stream.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Dataset</cell><cell>fc6</cell><cell>fc7</cell><cell>prob</cell></row><row><cell cols="5">HMDB51 51.09% 48.52% 39.10%</cell></row><row><cell cols="2">UCF101</cell><cell cols="3">83.77% 80.76% 67.01%</cell></row><row><cell cols="5">contains 3.5K training and 1.5K test videos. Following [23],</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The comparisons of the proposed MSD with Mean and Max fusion schemes in terms of accuracy on three splits of HMDB51 and UCF101. 5% 59.6% 63.1% 61.8% 59.5% 63.5% 62.1% 60.1% 64.1% UCF101 89.6% 87.6% 90.2% 89.6% 87.4% 90.3% 91.2% 88.1% 91.9%</figDesc><table><row><cell>Dataset</cell><cell>Mean</cell><cell>split 1 Max</cell><cell>MSD</cell><cell>Mean</cell><cell>split 2 Max</cell><cell>MSD</cell><cell>Mean</cell><cell>split 3 Max</cell><cell>MSD</cell></row><row><cell cols="2">HMDB51 61.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Run time of different streams averaged over all test videos in UCF101 dataset (milliseconds).</figDesc><table><row><cell cols="4">Stream 2D/3D CNN LSTM SUM</cell></row><row><cell>frame</cell><cell>750</cell><cell>12</cell><cell>762</cell></row><row><cell>motion</cell><cell>750</cell><cell>12</cell><cell>762</cell></row><row><cell>clip</cell><cell>490</cell><cell>10</cell><cell>500</cell></row><row><cell>video</cell><cell>490</cell><cell>-</cell><cell>490</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS-PETS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4389</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving human action recognition using score distribution and ranking</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Better exploiting motion for better action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Beyond short snippets deep networks for video classification</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.01861v3</idno>
		<title level="m">Jointly modeling embedding and translation to bridge video and language</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4506</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the fisher kernel for large-scale image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Msr asia msm at thumos challenge 2015</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR THUMOS Challenge Workshop</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep fisher networks for large-scale image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.0767</idno>
		<title level="m">Learning spatiotemporal features with 3d convolutional networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">G</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring inter-feature and inter-class relationships with deep neural networks for video classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annotation for free: Video tagging by mining user search behavior</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic video genre categorization using hierarchical svm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Exploiting image-trained CNN architectures for unconstrained video classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Luisier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.04144</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Building a comprehensive ontology to refine video concept detection</title>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGMM Workshop on MIR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
