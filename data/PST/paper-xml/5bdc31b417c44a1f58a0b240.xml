<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
							<email>christopher.morris@tu-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
							<email>matthias.fey@tu-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">McGill University and MILA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
							<email>janeric.lenssen@tu-dortmund.de</email>
							<affiliation key="aff0">
								<orgName type="institution">TU Dortmund University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Rattan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically-showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the 1-dimensional Weisfeiler-Leman graph isomorphism heuristic (1-WL). We show that GNNs have the same expressiveness as the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called k-dimensional GNNs (k-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Graph-structured data is ubiquitous across application domains ranging from chemo-and bioinformatics to image and social network analysis. To develop successful machine learning models in these domains, we need techniques that can exploit the rich information inherent in graph structure, as well as the feature information contained within a graph's nodes and edges. In recent years, numerous approaches have been proposed for machine learning graphs-most notably, approaches based on graph kernels <ref type="bibr" target="#b36">(Vishwanathan et al. 2010)</ref> or, alternatively, using graph neural network algorithms <ref type="bibr" target="#b15">(Hamilton, Ying, and Leskovec 2017b)</ref>.</p><p>Kernel approaches typically fix a set of features in advancee.g., indicator features over subgraph structures or features of local node neighborhoods. For example, one of the most successful kernel approaches, the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b35">(Shervashidze et al. 2011)</ref>, which is based on the 1dimensional Weisfeiler-Leman graph isomorphism heuristic <ref type="bibr">(Grohe 2017, pp. 79 ff.)</ref>, generates node features through an iterative relabeling, or coloring, scheme: First, all nodes are assigned a common initial color; the algorithm then iteratively recolors a node by aggregating over the multiset of colors in its neighborhood, and the final feature representation of a graph is the histogram of the resulting node colors. By iteratively aggregating over local node neighborhoods in this way, the WL subtree kernel is able to effectively summarize the neighborhood substructures present in a graph. However, while powerful, the WL subtree kernel-like other kernel methodsis limited because this feature construction scheme is fixed (i.e., it does not adapt to the given data distribution). Moreover, this approach-like the majority of kernel methods-focuses only on the graph structure and cannot interpret continuous node and edge labels, such as real-valued vectors which play an important role in applications such as bio-and chemoinformatics.</p><p>Graph neural networks (GNNs) have emerged as a machine learning framework addressing the above challenges. Standard GNNs can be viewed as a neural version of the 1-WL algorithm, where colors are replaced by continuous feature vectors and neural networks are used to aggregate over node neighborhoods <ref type="bibr" target="#b14">(Hamilton, Ying, and Leskovec 2017a;</ref><ref type="bibr" target="#b20">Kipf and Welling 2017)</ref>. In effect, the GNN framework can be viewed as implementing a continuous form of graph-based "message passing", where local neighborhood information is aggregated and passed on to the neighbors <ref type="bibr">(Gilmer et al. 2017)</ref>. By deploying a trainable neural network to aggregate information in local node neighborhoods, GNNs can be trained in an end-to-end fashion together with the parameters of the classification or regression algorithm, possibly allowing for greater adaptability and better generalization compared to the kernel counterpart of the classical 1-WL algorithm.</p><p>Up to now, the evaluation and analysis of GNNs has been largely empirical, showing promising results compared to kernel approaches, see, e.g., <ref type="bibr" target="#b40">(Ying et al. 2018b</ref>). However, it remains unclear how GNNs are actually encoding graph structure information into their vector representations, and whether there are theoretical advantages of GNNs compared to kernel based approaches. Present Work. We offer a theoretical exploration of the relationship between GNNs and kernels that are based on the 1-WL algorithm. We show that GNNs cannot be more powerful than the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs, e.g., the properties of subgraphs around each node. This result holds for a broad class of GNN architectures and all possible choices of parameters for them. On the positive side, we show that given the right parameter initialization GNNs have the same expressiveness as the 1-WL algorithm, completing the equivalence. Since the power of the 1-WL has been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015;</ref><ref type="bibr" target="#b19">Kiefer, Schweitzer, and Selman 2015)</ref>, we can transfer these results to the case of GNNs, showing that both approaches have the same shortcomings.</p><p>Going further, we leverage these theoretical relationships to propose a generalization of GNNs, called k-GNNs, which are neural architectures based on the k-dimensional WL algorithm (k-WL), which are strictly more powerful than GNNs. The key insight in these higher-dimensional variants is that they perform message passing directly between subgraph structures, rather than individual nodes. This higher-order form of message passing can capture structural information that is not visible at the node-level.</p><p>Graph kernels based on the k-WL have been proposed in the past <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref>. However, a key advantage of implementing higher-order message passing in GNNs-which we demonstrate here-is that we can design hierarchical variants of k-GNNs, which combine graph representations learned at different granularities in an end-to-end trainable framework. Concretely, in the presented hierarchical approach the initial messages in a k-GNN are based on the output of lower-dimensional k -GNN (with k &lt; k), which allows the model to effectively capture graph structures of varying granularity. Many real-world graphs inherit a hierarchical structure-e.g., in a social network we must model both the ego-networks around individual nodes, as well as the coarse-grained relationships between entire communities, see, e.g., <ref type="bibr" target="#b28">(Newman 2003</ref>)-and our experimental results demonstrate that these hierarchical k-GNNs are able to consistently outperform traditional GNNs on a variety of graph classification and regression tasks. Across twelve graph regression tasks from the QM9 benchmark, we find that our hierarchical model reduces the mean absolute error by 54.45% on average. For graph classification, we find that our hierarchical models leads to slight performance gains. Key Contributions. Our key contributions are summarized as follows: 1. We show that GNNs are not more powerful than the 1-WL in terms of distinguishing non-isomorphic (sub-)graphs. Moreover, we show that, assuming a suitable parameter initialization, GNNs have the same power as the 1-WL. 2. We propose k-GNNs, which are strictly more powerful than GNNs. Moreover, we propose a hierarchical version of k-GNNs, so-called 1-k-GNNs, which are able to work with the fine-and coarse-grained structures of a given graph, and relationships between those. 3. Our theoretical findings are backed-up by an experimen-tal study, showing that higher-order graph properties are important for successful graph classification and regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Our study builds upon a wealth of work at the intersection of supervised learning on graphs, kernel methods, and graph neural networks.</p><p>Historically, kernel methods-which implicitly or explicitly map graphs to elements of a Hilbert space-have been the dominant approach for supervised learning on graphs. Important early work in this area includes random-walk based kernels <ref type="bibr" target="#b9">(Gärtner, Flach, and Wrobel 2003;</ref><ref type="bibr" target="#b17">Kashima, Tsuda, and Inokuchi 2003)</ref>) and kernels based on shortest paths (Borgwardt and Kriegel 2005). More recently, developments in graph kernels have emphasized scalability, focusing on techniques that bypass expensive Gram matrix computations by using explicit feature maps. Prominent examples of this trend include kernels based on graphlet counting <ref type="bibr" target="#b34">(Shervashidze et al. 2009)</ref>, and, most notably, the Weisfeiler-Lehman subtree kernel <ref type="bibr" target="#b35">(Shervashidze et al. 2011)</ref> as well as its higher-order variants <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref>. Graphlet and Weisfeiler-Leman kernels have been successfully employed within frameworks for smoothed and deep graph kernels (Yanardag and Vishwanathan 2015a; 2015b). Recent works focus on assignment-based approaches <ref type="bibr" target="#b22">(Kriege, Giscard, and Wilson 2016;</ref><ref type="bibr" target="#b30">Nikolentzos, Meladianos, and Vazirgiannis 2017;</ref><ref type="bibr" target="#b16">Johansson and Dubhashi 2015)</ref>, spectral approaches <ref type="bibr" target="#b21">(Kondor and Pan 2016)</ref>, and graph decomposition approaches <ref type="bibr" target="#b30">(Nikolentzos et al. 2018)</ref>. Graph kernels were dominant in graph classification for several years, leading to new state-of-the-art results on many classification tasks. However, they are limited by the fact that they cannot effectively adapt their feature representations to a given data distribution, since they generally rely on a fixed set of features. More recently, a number of approaches to graph classification based upon neural networks have been proposed. Most of the neural approaches fit into the graph neural network framework proposed by <ref type="bibr">(Gilmer et al. 2017)</ref>. Notable instances of this model include Neural Fingerprints <ref type="bibr" target="#b7">(Duvenaud et al. 2015)</ref>, Gated Graph Neural Networks <ref type="bibr" target="#b24">(Li et al. 2016)</ref>, GraphSAGE <ref type="bibr" target="#b14">(Hamilton, Ying, and Leskovec 2017a)</ref>, SplineCNN <ref type="bibr" target="#b8">(Fey et al. 2018)</ref>, and the spectral approaches proposed in <ref type="bibr" target="#b3">(Bruna et al. 2014;</ref><ref type="bibr" target="#b6">Defferrard, X., and Vandergheynst 2016;</ref><ref type="bibr" target="#b20">Kipf and Welling 2017)</ref>-all of which descend from early work in (Merkwirth and Lengauer 2005) and <ref type="bibr" target="#b33">(Scarselli et al. 2009b)</ref>. Recent extensions and improvements to the GNN framework include approaches to incorporate different local structures around subgraphs <ref type="bibr">(Xu et al. 2018</ref>) and novel techniques for pooling node representations in order perform graph classification <ref type="bibr" target="#b41">(Zhang et al. 2018;</ref><ref type="bibr" target="#b40">Ying et al. 2018b)</ref>. GNNs have achieved state-of-the-art performance on several graph classification benchmarks in recent years, see, e.g., <ref type="bibr" target="#b40">(Ying et al. 2018b</ref>)-as well as applications such as protein-protein interaction prediction <ref type="bibr">(Fout et al. 2017)</ref>, recommender systems <ref type="bibr" target="#b39">(Ying et al. 2018a)</ref>, and the analysis of quantum interactions in molecules <ref type="bibr" target="#b34">(Schütt et al. 2017)</ref>. A survey of recent advancements in GNN techniques can be found in <ref type="bibr" target="#b15">(Hamilton, Ying, and Leskovec 2017b)</ref>.</p><p>Up to this point (and despite their empirical success) there has been very little theoretical work on GNNs-with the no-table exceptions of <ref type="bibr">Li et al.'s (Li, Han, and Wu 2018)</ref> work connecting GNNs to a special form Laplacian smoothing and <ref type="bibr" target="#b23">Lei et al.'s (Lei et al. 2017)</ref> work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels. Moreover, Scarselli et al. <ref type="bibr" target="#b32">(Scarselli et al. 2009a)</ref> investigates the approximation capabilities of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminaries</head><p>We start by fixing notation, and then outline the Weisfeiler-Leman algorithm and the standard graph neural network framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation and Background</head><p>A graph G is a pair (V, E) with a finite set of nodes V and a set of edges E ⊆ {{u, v} ⊆ V | u = v}. We denote the set of nodes and the set of edges of G by V (G) and E(G), respectively. For ease of notation we denote the edge {u,</p><formula xml:id="formula_0">v} in E(G) by (u, v) or (v, u). Moreover, N (v) denotes the neighborhood of v in V (G), i.e., N (v) = {u ∈ V (G) | (v, u) ∈ E(G)}.</formula><p>We say that two graphs G and H are isomorphic if there exists an edge preserving bijection ϕ :</p><formula xml:id="formula_1">V (G) → V (H), i.e., (u, v) is in E(G) if and only if (ϕ(u), ϕ(v)) is in E(H).</formula><p>We write G H and call the equivalence classes induced by isomorphism types. </p><formula xml:id="formula_2">Let S ⊆ V (G) then G[S] = (S, E S ) is the sub- graph induced by S with E S = {(u, v) ∈ E(G) | u, v ∈ S}. A node coloring is a function V (G) → Σ with arbitrary codomain Σ. Then a node colored or labeled graph (G, l) is a graph G endowed with a node coloring l : V (G) → Σ. We say that l(v) is a label or color of v ∈ V (G). We say that a node coloring c refines a node coloring d, written c d, if c(v) = c(w) implies d(v) = d(w) for every v, w in V (G).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weisfeiler-Leman Algorithm</head><p>We now describe the 1-W L algorithm for labeled graphs. Let (G, l) be a labeled graph. In each iteration, t ≥ 0, the 1-WL computes a node coloring c (t) l : V (G) → Σ, which depends on the coloring from the previous iteration. In iteration 0, we set c</p><formula xml:id="formula_3">(0) l = l. Now in iteration t &gt; 0, we set c (t) l (v) = H A S H c (t−1) l (v), { {c (t−1) l (u) | u ∈ N (v)} } (1)</formula><p>where H A S H bijectively maps the above pair to a unique value in Σ, which has not been used in previous iterations. To test two graph G and H for isomorphism, we run the above algorithm in "parallel" on both graphs. Now if the two graphs have a different number of nodes colored σ in Σ, the 1-W L concludes that the graphs are not isomorphic. Moreover, if the number of colors between two iterations does not change, i.e., the cardinalities of the images of c (t−1) l and c (t) l are equal, the algorithm terminates. Termination is guaranteed after at most max{|V (G)|, |V (H)|} iterations. It is easy to see that the algorithm is not able to distinguish all non-isomorphic graphs, e.g., see <ref type="bibr" target="#b4">(Cai, Fürer, and Immerman 1992)</ref>. Nonetheless, it is a powerful heuristic, which can successfully test isomorphism for a broad class of graphs <ref type="bibr" target="#b1">(Babai and Kucera 1979)</ref>.</p><p>The k-dimensional Weisfeiler-Leman algorithm (k-WL), for k ≥ 2, is a generalization of the 1-WL which colors tuples from V (G) k instead of nodes. That is, the algorithm computes a coloring c</p><formula xml:id="formula_4">(t) l,k : V (G) k → Σ.</formula><p>In order to describe the algorithm, we define the j-th neighborhood</p><formula xml:id="formula_5">N j (s) = {(s 1 , . . . , s j−1 , r, s j+1 , . . . , s k ) | r ∈ V (G)} (2) of a k-tuple s = (s 1 , . . . , s k ) in V (G) k .</formula><p>That is, the j-th neighborhood N j (t) of s is obtained by replacing the j-th component of s by every node from V (G). In iteration 0, the algorithm labels each k-tuple with its atomic type, i.e., two k-tuples s and s in V (G) k get the same color if the map s i → s i induces a (labeled) isomorphism between the subgraphs induced from the nodes from s and s , respectively. For iteration t &gt; 0, we define</p><formula xml:id="formula_6">C (t) j (s) = H A S H { {c (t−1) l,k (s ) | s ∈ N j (s)} } , (3) and set c (t) k,l (s) = H A S H c (t−1) k,l (s), C (t) 1 (s), . . . , C (t) k (s) . (4)</formula><p>Hence, two tuples s and s with c</p><formula xml:id="formula_7">(t−1) k,l (s) = c (t−1) k,l</formula><p>(s ) get different colors in iteration t if there exists j in [1 : k] such that the number of j-neighbors of s and s , respectively, colored with a certain color is different. The algorithm then proceeds analogously to the 1-W L. By increasing k, the algorithm gets more powerful in terms of distinguishing non-isomorphic graphs, i.e., for each k ≥ 2, there are non-isomorphic graphs which can be distinguished by the (k + 1)-WL but not by the k-WL <ref type="bibr" target="#b4">(Cai, Fürer, and Immerman 1992)</ref>. We note here that the above variant is not equal to the folklore variant of k-WL described in <ref type="bibr" target="#b4">(Cai, Fürer, and Immerman 1992)</ref>, which differs slightly in its update rule. However, it holds that the k-WL using Equation ( <ref type="formula">4</ref>) is as powerful as the folklore (k−1)-WL <ref type="bibr" target="#b12">(Grohe and Otto 2015)</ref>. WL Kernels. After running the WL algorithm, the concatenation of the histogram of colors in each iteration can be used as a feature vector in a kernel computation. Specifically, in the histogram for every color σ in Σ there is an entry containing the number of nodes or k-tuples that are colored with σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Neural Networks</head><p>Let (G, l) be a labeled graph with an initial node coloring</p><formula xml:id="formula_8">f (0) : V (G) → R 1×d that is consistent with l. This means that each node v is annotated with a feature f (0) (v) in R 1×d such that f (0) (u) = f (0) (v) if and only if l(u) = l(v).</formula><p>Alternatively, f (0) (v) can be an arbitrary real-valued feature vector associated with v. Examples include continuous atomic properties in chemoinformatic applications where nodes correspond to atoms, or vector representations of text in social network applications. A GNN model consists of a stack of neural network layers, where each layer aggregates local neighborhood information, i.e., features of neighbors, around each node and then passes this aggregated information on to the next layer.</p><p>A basic GNN model can be implemented as follows <ref type="bibr" target="#b15">(Hamilton, Ying, and Leskovec 2017b)</ref>. In each layer t &gt; 0, we compute a new feature</p><formula xml:id="formula_9">f (t) (v) = σ f (t−1) (v)•W (t) 1 + w∈N (v) f (t−1) (w)•W (t) 2 (5) in R 1×e for v, where W (t) 1 and W (t) 2</formula><p>are parameter matrices from R d×e , and σ denotes a component-wise non-linear function, e.g., a sigmoid or a ReLU. 1  Following <ref type="bibr">(Gilmer et al. 2017)</ref>, one may also replace the sum defined over the neighborhood in the above equation by a permutation-invariant, differentiable function, and one may substitute the outer sum, e.g., by a column-wise vector concatenation or LSTM-style update step. Thus, in full generality a new feature f (t) (v) is computed as</p><formula xml:id="formula_10">f W1 merge f (t−1) (v), f W2 aggr { {f (t−1) (w) | w ∈ N (v)} } ,<label>(6)</label></formula><p>where f W1 aggr aggregates over the set of neighborhood features and f W2 merge merges the node's representations from step (t − 1) with the computed neighborhood features. Both f W1 aggr and f W2 merge may be arbitrary differentiable, permutation-invariant functions (e.g., neural networks), and, by analogy to Equation <ref type="formula">5</ref>, we denote their parameters as W 1 and W 2 , respectively. In the rest of this paper, we refer to neural architectures implementing Equation ( <ref type="formula" target="#formula_10">6</ref>) as 1-dimensional GNN architectures (1-GNNs).</p><p>A vector representation f GNN over the whole graph can be computed by summing over the vector representations computed for all nodes, i.e.,</p><formula xml:id="formula_11">f GNN (G) = v∈V (G) f (T ) (v),</formula><p>where T &gt; 0 denotes the last layer. More refined approaches use differential pooling operators based on sorting <ref type="bibr" target="#b41">(Zhang et al. 2018</ref>) and soft assignments <ref type="bibr" target="#b40">(Ying et al. 2018b)</ref>.</p><p>In order to adapt the parameters W 1 and W 2 of Equations ( <ref type="formula">5</ref>) and ( <ref type="formula" target="#formula_10">6</ref>), to a given data distribution, they are optimized in an end-to-end fashion (usually via stochastic gradient descent) together with the parameters of a neural network used for classification or regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship Between 1-WL and 1-GNNs</head><p>In the following we explore the relationship between the 1-WL and 1-GNNs. Let (G, l) be a labeled graph, and let</p><formula xml:id="formula_12">W (t) = W (t ) 1 , W (t ) 2</formula><p>t ≤t denote the GNN parameters given by Equation ( <ref type="formula">5</ref>) or Equation (6) up to iteration t. We encode the initial labels l(v) by vectors f (0) (v) ∈ R 1×d , e.g., using a 1-hot encoding.</p><p>Our first theoretical result shows that the 1-GNN architectures do not have more power in terms of distinguishing between non-isomorphic (sub-)graphs than the 1-WL algorithm. More formally, let f W1 merge and f W2 aggr be any two functions chosen in (6). For every encoding of the labels l(v) as vectors f (0) (v),</p><p>1 For clarity of presentation we omit biases. and for every choice of W (t) , we have that the coloring c (t) l of 1-WL always refines the coloring f (t) induced by a 1-GNN parameterized by W (t) .</p><p>Theorem 1. Let (G, l) be a labeled graph. Then for all t ≥ 0 and for all choices of initial colorings f (0) consistent with l, and weights W (t) , c</p><p>(t) l f (t) .</p><p>Our second result states that there exist a sequence of parameter matrices W (t) such that 1-GNNs have exactly the same power in terms of distinguishing non-isomorphic (sub-)graphs as the 1-WL algorithm. This even holds for the simple architecture ( <ref type="formula">5</ref>), provided we choose the encoding of the initial labeling l in such a way that different labels are encoded by linearly independent vectors. Theorem 2. Let (G, l) be a labeled graph. Then for all t ≥ 0 there exists a sequence of weights W (t) , and a 1-GNN architecture such that c</p><p>(t) l ≡ f (t) . Hence, in the light of the above results, 1-GNNs may viewed as an extension of the 1-WL which in principle have the same power but are more flexible in their ability to adapt to the learning task at hand and are able to handle continuous node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shortcomings of Both Approaches</head><p>The power of 1-WL has been completely characterized, see, e.g., <ref type="bibr" target="#b0">(Arvind et al. 2015)</ref>. Hence, by using Theorems 1 and 2, this characterization is also applicable to 1-GNNs. On the other hand, 1-GNNs have the same shortcomings as the 1-WL. For example, both methods will give the same color to every node in a graph consisting of a triangle and a 4-cycle, although vertices from the triangle and the vertices from the 4-cycle are clearly different. Moreover, they are not capable of capturing simple graph theoretic properties, e.g., triangle counts, which are an important measure in social network analysis <ref type="bibr" target="#b26">(Milo et al. 2002;</ref><ref type="bibr" target="#b28">Newman 2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>k-dimensional Graph Neural Networks</head><p>In the following, we propose a generalization of 1-GNNs, so-called k-GNNs, which are based on the k-WL. Due to scalability and limited GPU memory, we consider a set-based version of the k-WL. For a given k, we consider all k-element subsets</p><formula xml:id="formula_13">[V (G)] k over V (G). Let s = {s 1 , . . . , s k } be a k-set in [V (G)] k , then we define the neighborhood of s as N (s) = {t ∈ [V (G)] k | |s ∩ t| = k − 1} .</formula><p>The local neighborhood N L (s) consists of all t ∈ N (s) such that (v, w) ∈ E(G) for the unique v ∈ s \ t and the unique w ∈ t \ s. The global neighborhood N G (s) then is defined as N (s) \ N L (s).   Let (G, l) be a labeled graph. In each k-GNN layer t ≥ 0, we compute a feature vector f</p><formula xml:id="formula_14">(t) k (s) for each k-set s in [V (G)] k . For t = 0, we set f (0)</formula><p>k (s) to f iso (s), a one-hot encoding of the isomorphism type of G[s] labeled by l. In each layer t &gt; 0, we compute new features by</p><formula xml:id="formula_15">f (t) k (s) = σ f (t−1) k (s) • W (t) 1 + u∈N L (s)∪N G (s) f (t−1) k (u) • W (t) 2 .</formula><p>Moreover, one could split the sum into two sums ranging over N L (s) and N G (s) respectively, using distinct parameter matrices to enable the model to learn the importance of local and global neighborhoods. To scale k-GNNs to larger datasets and to prevent overfitting, we propose local k-GNNs, where we omit the global neighborhood of s, i.e.,</p><formula xml:id="formula_16">f (t) k,L (s) = σ f (t−1) k,L (s) • W (t) 1 + u∈N L (s) f (t−1) k,L (u) • W (t) 2 .</formula><p>The running time for evaluation of the above depends on |V |, k and the sparsity of the graph (each iteration can be bounded by the number of subsets of size k times the maximum degree). Note that we can scale our method to larger datasets by using sampling strategies introduced in, e.g., <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017;</ref><ref type="bibr" target="#b14">Hamilton, Ying, and Leskovec 2017a)</ref>. We can now lift the results of the previous section to the k-dimensional case. Proposition 3. Let (G, l) be a labeled graph and let k ≥ 2. Then for all t ≥ 0, for all choices of initial colorings f (0) k consistent with l and for all weights W (t) , c</p><p>k . Again the second result states that there exists a suitable initialization of the parameter matrices W (t) such that k-GNNs have exactly the same power in terms of distinguishing nonisomorphic (sub-)graphs as the set-based k-WL. Proposition 4. Let (G, l) be a labeled graph and let k ≥ 2. Then for all t ≥ 0 there exists a sequence of weights W (t) , and a k-GNN architecture such that c</p><formula xml:id="formula_18">(t) s,k,l ≡ f (t) k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchical Variant</head><p>One key benefit of the end-to-end trainable k-GNN framework-compared to the discrete k-WL algorithm-is that we can hierarchically combine representations learned at different granularities. Concretely, rather than simply using one-hot indicator vectors as initial feature inputs in a k-GNN, we propose a hierarchical variant of k-GNN that uses the features learned by a (k − 1)-dimensional GNN, in addition to the (labeled) isomorphism type, as the initial features, i.e., f</p><formula xml:id="formula_19">k (s) = σ f iso (s), u⊂s f (T k−1 ) k−1 (u) • W k−1 ,<label>(0)</label></formula><p>for some T k−1 &gt; 0, where W k−1 is a matrix of appropriate size, and square brackets denote matrix concatenation. Hence, the features are recursively learned from dimensions 1 to k in an end-to-end fashion. This hierarchical model also satisfies Propositions 3 and 4, so its representational capacity is theoretically equivalent to a standard k-GNN (in terms of its relationship to k-WL). Nonetheless, hierarchy is a natural inductive bias for graph modeling, since many real-world graphs incorporate hierarchical structure, so we expect this hierarchical formulation to offer empirical utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Study</head><p>In the following, we want to investigate potential benefits of GNNs over graph kernels as well as the benefits of our proposed k-GNN architectures over 1-GNN architectures. More precisely, we address the following questions: Q1 How do the (hierarchical) k-GNNs perform in comparison to state-of-the-art graph kernels? Q2 How do the (hierarchical) k-GNNs perform in comparison to the 1-GNN in graph classification and regression tasks? Q3 How much (if any) improvement is provided by optimizing the parameters of the GNN aggregation function, compared to just using random GNN parameters while optimizing the parameters of the downstream classification/regression algorithm?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>To compare our k-GNN architectures to kernel approaches we use well-established benchmark datasets from the graph kernel literature <ref type="bibr" target="#b18">(Kersting et al. 2016)</ref>. The nodes of each graph in these dataset is annotated with (discrete) labels or no labels.  To demonstrate that our architectures scale to larger datasets and offer benefits on real-world applications, we conduct experiments on the Q M 9 dataset <ref type="bibr" target="#b31">(Ramakrishnan et al. 2014;</ref><ref type="bibr" target="#b31">Ruddigkeit et al. 2012;</ref><ref type="bibr">Wu et al. 2018)</ref>, which consists of 133 385 small molecules. The aim here is to perform regression on twelve targets representing energetic, electronic, geometric, and thermodynamic properties, which were computed using density functional theory.</p><formula xml:id="formula_20">1-G N N 1-2-G N N 1-3-G N N 1-2-3-G N N Gain µ 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines</head><p>We use the following kernel and GNN methods as baselines for our experiments. Kernel Baselines. We use the Graphlet kernel <ref type="bibr" target="#b34">(Shervashidze et al. 2009)</ref>, the shortest-path kernel <ref type="bibr" target="#b2">(Borgwardt and Kriegel 2005)</ref>, the Weisfeiler-Lehman subtree kernel (W L) <ref type="bibr" target="#b35">(Shervashidze et al. 2011)</ref>, the Weisfeiler-Lehman Optimal Assignment kernel (W L -O A) <ref type="bibr" target="#b22">(Kriege, Giscard, and Wilson 2016)</ref>, and the global-local k-WL <ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref> with k in {2, 3} as kernel baselines. For each kernel, we computed the normalized Gram matrix. We used the C-SVM im-plementation of LIBSVM <ref type="bibr" target="#b5">(Chang and Lin 2011)</ref> to compute the classification accuracies using 10-fold cross validation. The parameter C was selected from {10 −3 , 10 −2 , . . . , 10 2 , 10 3 } by 10-fold cross validation on the training folds. Neural Baselines. To compare GNNs to kernels we used the basic 1-GNN layer of Equation (5), DCNN <ref type="bibr" target="#b37">(Wang et al. 2018)</ref>, PatchySan <ref type="bibr" target="#b29">(Niepert, Ahmed, and Kutzkov 2016)</ref>, DGCNN <ref type="bibr" target="#b41">(Zhang et al. 2018)</ref>. For the Q M 9 dataset we used a 1-GNN layer similar to <ref type="bibr">(Gilmer et al. 2017)</ref>, where we replaced the inner sum of Equation ( <ref type="formula">5</ref>) with a 2-layer MLP in order incorporate edge features (bond type and distance information). Moreover, we compare against the numbers provided in <ref type="bibr">(Wu et al. 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Configuration</head><p>We always used three layers for 1-GNN, and two layers for (local) 2-GNN and 3-GNN, all with a hidden-dimension size of 64. For the hierarchical variant we used architectures that use features computed by 1-GNN as initial features for the 2-GNN (1-2-GNN) and 3-GNN (1-3-GNN), respectively. Moreover, us-ing the combination of the former we componentwise concatenated the computed features of the 1-2-GNN and the 1-3-GNN (1-2-3-GNN). For the final classification and regression steps, we used a three layer MLP, with binary cross entropy and mean squared error for the optimization, respectively. For classification we used a dropout layer with p = 0.5 after the first layer of the MLP. We applied global average pooling to generate a vector representation of the graph from the computed node features for each k. The resulting vectors are concatenated column-wise before feeding them into the MLP. Moreover, we used the Adam optimizer with an initial learning rate of 10 −2 and applied an adaptive learning rate decay based on validation results to a minimum of 10 −5 . We trained the classification networks for 100 epochs and the regression networks for 200 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Protocol</head><p>For the smaller datasets, which we use for comparison against the kernel methods, we performed a 10-fold cross validation where we randomly sampled 10% of each training fold to act as a validation set. For the Q M 9 dataset, we follow the dataset splits described in <ref type="bibr">(Wu et al. 2018)</ref>. We randomly sampled 10% of the examples for validation, another 10% for testing, and used the remaining for training. We used the same initial node features as described in <ref type="bibr">(Gilmer et al. 2017)</ref>. Moreover, in order to illustrate the benefits of our hierarchical k-GNN architecture, we did not use a complete graph, where edges are annotated with pairwise distances, as input. Instead, we only used pairwise Euclidean distances for connected nodes, computed from the provided node coordinates. The code was built upon the work of <ref type="bibr" target="#b8">(Fey et al. 2018)</ref> and is provided at https: //github.com/chrsmrrs/k-gnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Discussion</head><p>In the following we answer questions Q1 to Q3. Table <ref type="table" target="#tab_0">1</ref> shows the results for comparison with the kernel methods on the graph classification benchmark datasets. Here, the hierarchical k-GNN is on par with the kernels despite the small dataset sizes (answering question Q1). We also find that the 1-2-3-GNN significantly outperforms the 1-GNN on all seven datasets (answering Q2), with the 1-GNN being the overall weakest method across all tasks. <ref type="foot" target="#foot_1">3</ref> We can further see that optimizing the parameters of the aggregation function only leads to slight performance gains on two out of three datasets, and that no optimization even achieves better results on the P R O T E I N S benchmark dataset (answering Q3). We contribute this effect to the one-hot encoded node labels, which allow the GNN to gather enough information out of the neighborhood of a node, even when this aggregation is not learned.</p><p>Table <ref type="table" target="#tab_1">2</ref> shows the results for the Q M 9 dataset. On eleven out of twelve targets all of our hierarchical variants beat the 1-GNN baseline, providing further evidence for Q2. For example, on the target H we achieve a large improvement of 98.1% in MAE compared to the baseline. Moreover, on ten out of twelve datasets, the hierarchical k-GNNs beat the baselines from <ref type="bibr">(Wu et al. 2018)</ref>. However, the additional structural information extracted by the k-GNN layers does not serve all tasks equally, leading to huge differences in gains across the targets.</p><p>It should be noted that our k-GNN models have more parameters than the 1-GNN model, since we stack two additional GNN layers for each k. However, extending the 1-GNN model by additional layers to match the number of parameters of the k-GNN did not lead to better results in any experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We presented a theoretical investigation of GNNs, showing that a wide class of GNN architectures cannot be stronger than the 1-WL. On the positive side, we showed that, in principle, GNNs possess the same power in terms of distinguishing between non-isomorphic (sub-)graphs, while having the added benefit of adapting to the given data distribution. Based on this insight, we proposed k-GNNs which are a generalization of GNNs based on the k-WL. This new model is strictly stronger then GNNs in terms of distinguishing non-isomorphic (sub-)graphs and is capable of distinguishing more graph properties. Moreover, we devised a hierarchical variant of k-GNNs, which can exploit the hierarchical organization of most real-world graphs. Our experimental study shows that k-GNNs consistently outperform 1-GNNs and beat state-of-the-art neural architectures on largescale molecule learning tasks. Future work includes designing task-specific k-GNNs, e.g., devising k-GNNs layers that exploit expert-knowledge in bio-and chemoinformatic settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Two colorings are equivalent if c d and d c, and we write c ≡ d. A color class Q ⊆ V (G) of a node coloring c is a maximal set of nodes with c(v) = c(w) for every v, w in Q. Moreover, let [1 : n] = {1, . . . , n} ⊂ N for n &gt; 1, let S be a set then the set of k-sets [S] k = {U ⊆ S | |U | = k} for k ≥ 2, which is the set of all subsets with cardinality k, and let { {. . .} } denote a multiset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Pooling from 2to 3-GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of the proposed hierarchical variant of the k-GNN layer. For each subgraph S on k nodes a feature f is learned, which is initialized with the learned features of all (k − 1)-element subgraphs of S. Hence, a hierarchical representation of the input graph is learned.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies in percent on various graph benchmark datasets.</figDesc><table><row><cell>Method</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean absolute errors on the Q M 9 dataset. The far-right column shows the improvement of the best k-GNN model in comparison to the 1-GNN baseline.</figDesc><table><row><cell>Target</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Note that the definition of the local neighborhood is different from the the one defined in<ref type="bibr" target="#b27">(Morris, Kersting, and Mutzel 2017)</ref> which is a superset of our definition. Our computations therefore involve sparser graphs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Note that in very recent work, GNNs have shown superior results over kernels when using advanced pooling techniques<ref type="bibr" target="#b40">(Ying et al. 2018b)</ref>. Note that our layers can be combined with these pooling layers. However, we opted to use standard global pooling in order to compare a typical GNN implementation with standard off-the-shelf kernels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the German research council (DFG) within the Research Training Group 2236 UnRAVeL and the Collaborative Research Center SFB 876, Providing Information by Resource-Constrained Analysis, projects A6 and B2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the power of color refinement</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arvind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Köbler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Verbitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Fundamentals of Computation Theory</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="339" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Canonical labelling of graphs in linear average time</title>
		<author>
			<persName><forename type="first">L</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kucera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Foundations of Computer Science</title>
				<imprint>
			<date type="published" when="1979">1979</date>
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An optimal lower bound on the number of variables for graph identifications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fürer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Immerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="410" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous B-spline kernels</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<editor>
			<persName><surname>Cvpr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Fout</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Byrd</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Shariat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ben-Hur</forename></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="page" from="6533" to="6542" />
		</imprint>
	</monogr>
	<note>Protein interface prediction using graph convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pebble games and linear equations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Otto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Logic</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="797" to="844" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Descriptive Complexity, Canonisation, and Definable Graph Structure Theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Logic</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="52" to="74" />
			<date type="published" when="2017">2017b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning with similarity functions on graphs using matchings of geometric embeddings</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dubhashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="467" to="476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Benchmark data sets for graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphs identified by logics with counting</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kiefer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Selman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MFCS</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The multiscale laplacian graph kernel</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On valid optimal assignment kernels and applications to graph classification</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-L</forename><surname>Giscard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1615" to="1623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deriving neural architectures from sequence and graph kernels</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2024" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detecting similar programs via the Weisfeiler-Leman graph kernel</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schäf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Reuse</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="315" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Automatic generation of complementary descriptors with molecular graph networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-M ;</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Merkwirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2005">2018. 2005</date>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
	<note>Deeper insights into graph convolutional networks for semi-supervised learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Network motifs: simple building blocks of complex networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Milo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shen-Orr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Itzkovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kashtan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chklovskii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Alon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">298</biblScope>
			<biblScope unit="issue">5594</biblScope>
			<biblScope unit="page" from="824" to="827" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glocalized Weisfeiler-Lehman kernels: Global-local feature maps of graphs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="327" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The structure and function of complex networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E J</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="167" to="256" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Matching node embeddings for graph similarity</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Meladianos, P.; and Vazirgiannis</title>
				<meeting><address><addrLine>Nikolentzos, G</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2018. 2017</date>
			<biblScope unit="page" from="2429" to="2435" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lilienfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Reymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012">2014. 2012</date>
		</imprint>
	</monogr>
	<note>Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data 1</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Computational capabilities of graph neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="102" />
			<date type="published" when="2009">2009a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SchNet: A continuous-filter convolutional neural network for modeling quantum interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Sauceda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tkatchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
				<imprint>
			<date type="published" when="2009">2017. 2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Weisfeiler-Lehman graph kernels</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<editor>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</editor>
		<editor>
			<persName><surname>-I.;</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
	<note>Moleculenet: a benchmark for molecular machine learning</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2015">2015a</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for webscale recommender systems</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>KDD</publisher>
			<date type="published" when="2015">2015b. 2018a</date>
			<biblScope unit="page" from="2134" to="2142" />
		</imprint>
	</monogr>
	<note>A structural smoothing framework for robust graph comparison</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An endto-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yixin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4428" to="4435" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
