<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Dicecco</surname></persName>
							<email>dicecco1@eecg.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Griffin</forename><surname>Lacey</surname></persName>
							<email>laceyg@uoguelph.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jasmina</forename><surname>Vasiljevic</surname></persName>
							<email>vasiljev@eecg.toronto.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><surname>Chow</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Graham</forename><surname>Taylor</surname></persName>
							<email>gwtaylor@uoguelph.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shawki</forename><surname>Areibi</surname></persName>
							<email>sareibi@uoguelph.ca</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Guelph</orgName>
								<address>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Caffeinated FPGAs: FPGA Framework For Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3686551BF47C9F33ED791A6CF0A2B757</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) have gained significant traction in the field of machine learning, particularly due to their high accuracy in visual recognition. Recent works have pushed the performance of GPU implementations of CNNs showing significant improvements in their classification and training times. With these improvements, many frameworks have become available for implementing CNNs on both CPUs and GPUs, with no support for FPGA implementations. In this work we present a modified version of the popular CNN framework Caffe, with FPGA support. This allows for classification using CNN models and specialized FPGA implementations with the flexibility of reprogramming the device when necessary, seamless memory transactions between host and device, simpleto-use test benches, and the ability to create pipelined layer implementations. To validate the framework, we use the Xilinx SDAccel environment to implement an FPGA-based Winograd convolution engine and show that it can be used alongside other layers running on a host processor to run several popular CNNs (AlexNet, GoogleNet, VGG A, Overfeat). The results show that our framework achieves 50 GFLOPS across 3×3 convolutions in the benchmarks. This is achieved within a practical framework, which will aid in future development of FPGA-based CNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Convolutional Neural Networks (CNNs) are highly accurate deep learning networks inspired by the mammalian visual cortex. A number of works have explored the implementation of CNNs on FPGAs <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> to take advantage of their low-power, customizable and programmable fabric. While FPGAs show promise in efficiently computing CNNs, they lack framework support, making FPGAs inaccessible to deep learning scientists. CNN frameworks allow the programmer to launch any model, and contain comprehensive tests for both layer-wise and system execution on CPUs and GPUs <ref type="bibr" target="#b3">[4]</ref>. In contrast, to implement a CNN on an FPGA each model has to be manually designed, tested for correctness, and optimized for performance; essentially rebuilding from scratch, rather than taking advantage of existing work.</p><p>CNNs are very computationally intensive with most of the computation in the convolution layers. This large computational complexity motivates efforts to reduce the number of required operations. To reduce the number of operations, the Winograd minimal filtering algorithm can be used to take advantage of the overlapping computations between adjacent convolution windows <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Considering the lack of FPGA framework support and the computational complexity of convolution layers, this paper makes the following contributions:</p><p>• We present an adaptation of the Caffe CNN framework with support for Xilinx SDAccel <ref type="bibr" target="#b6">[7]</ref>, which allows CNN classification to be launched on CPU-FPGA systems.</p><p>• We implement the Winograd convolution algorithm targeting 3 × 3 convolution layers with unity stride.</p><p>Results show that the architecture achieves 50 GFLOPS across the 3×3 convolution layers of a CNN benchmark suite, using 83.2% of the available SDAccel resources in a Xilinx Virtex 7 (XC7VX690T). • Finally, the software and hardware implementation details have been made open-source and can be found at https://github.com/dicecco1/fpga_caffe. This work is organized as follows: Section II provides background information on CNNs and Xilinx SDAccel. Section III discusses the Winograd convolution algorithm and FPGA implementation. Section IV details the features included in the FPGA Caffe framework and Section V shows the area utilization and performance results of the Winograd convolution engine. Section VI reviews related work and compares this work to other recent FPGA implementations. Finally, Section VII concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>The following subsections detail the necessary background information regarding CNNs and the Xilinx SDAccel OpenCL development environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convolutional Neural Networks</head><p>CNNs are a popular type of supervised machine learning algorithm. Similar to other machine learning algorithms, CNNs can be trained using back propagation to learn complex representations useful for many applications. CNNs are commonly used for performing object recognition in pixelbased input. A popular CNN model such as AlexNet <ref type="bibr" target="#b7">[8]</ref> can be used to classify up to 1000 different objects in images with high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parallelism Strategies</head><p>Given the large computational requirements and inherent parallelism of neural networks, these architectures are ideal for hardware accelerators. Popular parallelism strategies can be reduced to three main categories. Data Parallelism -splitting the data across different execution threads, but using the same model. Fine-grained data parallelism can be applied using operations applied concurrently to all pixels, while coarse-grained data parallelism can be applied by processing "mini-batches" of hundreds or thousand of images. Model parallelism -splitting the model across different execution threads, but using the same data. This strategy offers several advantages, such as being able to accommodate large neural network sizes by splitting the weights across </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. SDAccel OpenCL FPGA Programming Model</head><p>The SDAccel OpenCL environment for FPGAs involves host and kernel code. The host code is used for programming the FPGA, passing data between the host's memory and the FPGA's global memory, and launching compute kernels on the FPGA. The FPGA is segmented into two regions, the programmable region and the static region. The programmable region contains the kernel, which is the computation to be accelerated. The static region is programmed upon power-up and it contains the interfaces to global memory and PCIe. The synthesized kernel can contain one or more compute units (CUs), where a CU is the hardware unit responsible for the required computation. One approach to increasing parallelism in the host code is to instantiate multiple CUs as shown in Fig. <ref type="figure">1</ref>, with each CU handling a portion of the problem <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FPGA WINOGRAD CONVOLUTION</head><p>Winograd convolution exploits the Winograd minimal filtering algorithm to implement convolution using less floating point operations <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. This is achieved through a set of transformations on the input data and weights to reduce the convolution to a dot product. The Winograd convolution algorithm output is referred to as F (m × m, r × r ), where m × m is the output tile size, while r × r is the filter size. For 3 × 3 convolutions, F (2 × 2, 3 × 3) has been shown to provide significant reduction in floating point operations <ref type="bibr" target="#b5">[6]</ref>. Therefore, in this work we focus primarily on implementing F (2×2, 3×3) due to its potential for reduced DSP utilization and smaller tile sizes that allows for more replication of transformations and dot products.</p><p>The Winograd input transformation requires a total of eight instances of the partial transform (PT) in Equation <ref type="formula" target="#formula_0">1</ref>per 4 × 4 tile, with four applied to the columns and four applied to the rows (indices of Equation 1 are swapped for row calculations) as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. This results in 32 floating point additions and 64 DSPs per instantiation. Given that each input tile overlaps every two rows and every two columns with its neighbor, additional resource savings can be achieved by precomputing all of the column-wise PTs. This reduces the number of PTs required for the columnwise instances from four to two per tile, which reduces the number of floating point additions to 24 from 32 per input tile transformation (with potentially one additional set of column transformations for the edge of each row).</p><formula xml:id="formula_0">O 0, j = I 0, j -I 2, j , O 1, j = I 1, j + I 2, j O 2, j = I 2, j -I 1, j , O 3, j = I 1, j -I 3, j<label>(1)</label></formula><p>Where I n,m is the input of the partial transform at (n, m); O n,m is the output of the partial transform at (n, m).</p><p>We exploit this change in tile size in the input stage of our architecture by first burst reading the input frames from off-chip SDRAM memory to on-chip BRAMs and then computing the PTs of each column of the 4 × 2 tiles. The BRAMs are organized such that four tiles can be accessed per cycle, allowing for eight PTs to be applied per cycle to reduce preprocessing overhead.</p><p>Following the input stage, each 4×2 tile and its neighbor are fed into a pipelined processing element. The processing element completes the remaining set of PTs, performs the dot product between the input and the weights, computes the output transform, and accumulates the result within a buffer. This process is repeated with different weights per output feature map until all of the output feature maps have been computed. To reduce the cycles required per output feature map, the processing element is replicated four times. Once the output feature map buffer is full, the output is transferred back into the off-chip DDR memory. To further improve performance, the CU has been instantiated twice. Each CU handles a separate image to exploit coarse-grained data parallelism, resulting in double the throughput for batch sizes that are multiples of two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FPGA CAFFE FRAMEWORK</head><p>The Caffe framework <ref type="bibr" target="#b3">[4]</ref> is used to describe CNNs based on predefined layer implementations with CPU and GPU backends. In this section we describe our approach to augmenting the Caffe framework to enable CNN classification using FPGAs and Xilinx SDAccel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. OpenCL Brew</head><p>In Caffe, a Brew is referred to as a mode of operation that determines the target architecture on which CNN classification or training is executed. This work extends the Caffe framework to include the OCL (OpenCL) Brew, which provides support for Xilinx FPGA-based CNNs. The user can choose between the different Brews by building the framework using the corresponding Makefile flags and changing the Brew to OCL. Fig. <ref type="figure">3</ref> shows an overview of the augmented system with the OCL Brew, where inputs and outputs are the same as in the CPU and GPU Brews, but the underlying hardware of the system is comprised of the CPU for host code and the FPGA for layer computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. OpenCL Memory Management and Synchronization</head><p>Data in Caffe is represented as a flattened array, with allocation, resizing, and synchronization between CPU and GPU resources abstracted from its usage <ref type="bibr" target="#b3">[4]</ref>. Support for memory synchronization between the host and the FPGA in the FPGA Caffe framework builds on the memory synchronization features used for GPUs. To accomplish similar functionality, OpenCL APIs are used with an additional object corresponding to the FPGA device memory. When data is passed from the host to the FPGA, the state of the memory changes to HEAD_AT_OCL from HEAD_AT_CPU such that on subsequent accesses it will either stay in the device memory or be transferred back to host memory. When the data is synchronized between the host CPU and the FPGA, the state of the memory will change to SYNCED.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FPGA Testbenches</head><p>Testing a layer in FPGA Caffe can be accomplished in two ways depending on the stage of development. Layers can be tested using individual test cases through the test cases provided in Caffe. A test case in Caffe can be used to test an FPGA implementation by changing the Brew to OCL and modifying parameters to suit the layer. Alternatively, the FPGA implementations can be tested through the use of standalone host code by invoking only the host code required to launch the kernel. In either case, a layer can be tested using a hardware or software emulation based implementation created with Xilinx SDAccel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. FPGA Layers</head><p>To accommodate the differing compute and programming models of FPGAs compared to CPUs and GPUs, FPGA specific layers are required in the FPGA Caffe framework.</p><p>XCLProgram Layer -Programming an FPGA has significant overhead (100-400 ms) compared to a GPU (0.001-0.005 ms). This programming overhead conflates the measured execution time for a layer in the Caffe benchmarking functionality. To overcome this, the XCLProgram layer can be used as a method for giving the user greater control over how the FPGA is programmed, as well as the ability to separately benchmark the execution time of each layer and programming overhead.</p><p>Pipeline Layers -In the GPU-based approach native to Caffe, before each layer is executed the GPU is programmed and memory is synchronized. In FPGA Caffe this is a bottleneck due to the overhead of programming FPGAs and the inability to exploit pipeline parallelism between kernels. To address these issues, pipeline layers can be used by packaging multiple kernels into one binary, with kernelkernel communication occurring through local memory structures on the FPGA. Pipeline layers reduce the number of times the FPGA is programmed, and eliminate the need to synchronize memory with the host between layers. Most importantly, pipeline layers allow pipeline parallelism strategies across layers, increasing throughput by allowing multiple layers to execute concurrently. While pipeline layers violate some modularity assumptions of Caffe, we argue that this is practical given that combinations of layers are predictable in practice (e.g. convolution, ReLU, pooling).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS</head><p>This section shows the results gathered from the Winograd convolution engine running within FPGA Caffe. The platform we use includes an Alpha-Data ADM-PCIE-7V3 card with a Xilinx Virtex 7 XC7VX690T running at 200 MHz and an Intel Xeon CPU E5-2620 running at 2.0 GHz for the host code. The Xilinx Virtex 7 is contained within a server that has been virtualized to support virtual machines (VM) and is connected through PCIe. The VM in use has 8GB RAM and four cores. An Intel i7-4770k running at 3.5 GHz was used for CPU comparison and an nVidia Quadro K620 for GPU comparisons. The Xilinx SDAccel version number is 2015.1.3, CUDA version number is 7.5, cuDNN version number is 4.0 <ref type="bibr" target="#b8">[9]</ref> and the CPU host code uses OpenBLAS <ref type="bibr" target="#b9">[10]</ref> with eight threads enabled. The CPU, GPU, and FPGA implementations use 32 bit floating point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Winograd Resource Utilization</head><p>The resource utilization post place and route is shown in Table <ref type="table">I</ref>. The LUT utilization is highest at 83.2% of the SDAccel region's available LUTs. The utilization accounts for additional resources required to integrate into the SDAccel framework, which drives the significant LUT utilization in comparison to other resources and makes it impossible to place more than two CUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FPGA Caffe Benchmark Results</head><p>To evaluate the Winograd convolution engine within the FPGA Caffe framework, a set of benchmark CNNs is required to view its performance across varying workloads (number of output feature maps and output sizes). The benchmark suite that we use is adopted from the Soumith Chintala convnet-benchmarks <ref type="bibr" target="#b10">[11]</ref>, which is composed of previous ImageNet winners.</p><p>Table <ref type="table">II</ref> shows the performance of the system in comparison to both CPU and GPU implementations of the 3 × 3 convolution layers of each CNN. To calculate the GFLOPS of the Winograd convolution engine, the number of floating-point operations is taken to be the same as direct convolution, which is considered to be the effective GFLOPS <ref type="bibr" target="#b5">[6]</ref>. Comparing the geometric averages in Table <ref type="table">II</ref>, the Winograd convolution engine performs 2.1 times slower than the CPU and 9.4 times slower than the GPU. VI. RELATED WORK An independent effort is underway to add OpenCL support to Caffe <ref type="bibr" target="#b15">[15]</ref>, though this support is meant primarily for exposing more GPUs to Caffe rather than FPGAs. The work in <ref type="bibr" target="#b15">[15]</ref> provides functionality to support AMD devices and some features similar to those explored in this work through OpenCL. However, our work is differentiated in that the OpenCL implementations are abstracted away from being GPUs through a separate Brew for OpenCL to allow for implementations across many architectures. Additionally, given the non-standard development model in FPGA OpenCL tools, frameworks that support standard OpenCL will not work with FPGAs without significant framework modification. This is the case because the FPGA OpenCL tools require offline compilation of kernels and vendorspecific attributes to achieve suitable performance.</p><p>Table <ref type="table">III</ref> shows the performance of our work compared to several FPGA works. The highest performing implementation is the work of Qiu <ref type="bibr" target="#b2">[3]</ref>, which is enabled by their use of fixed point representations. The work of Zhang <ref type="bibr" target="#b0">[1]</ref> is 1.2fold higher performance while using 32 bit float, though as expected our DSP utilization is significantly lower while achieving comparable throughput. While the throughput of this work is lower, it does not require precision analysis and it does not need to be resynthesized for new work loads as is the case in all other prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work we presented a framework for implementing CNNs using FPGAs based on Caffe. The framework allows for transparent support for individual FPGA implementations of layers for testing and verification. This framework </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Winograd Forward Transform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Winograd convolution and testing it across several CNNs. The results show that with 83.2% of the available SDAccel resources we achieve 50 GFLOPS across the 3 × 3 convolution layers of four CNNs. While this does not improve upon prior implementations in terms of performance, it demonstrates the capabilities of the framework, which allows for further work that could lead to higher performance.</figDesc><table><row><cell></cell><cell></cell><cell>TABLE III</cell><cell></cell><cell></cell></row><row><cell cols="4">COMPARISON BETWEEN EXISTING FPGA WORKS</cell><cell></cell></row><row><cell>Metric</cell><cell>Zhang [1]</cell><cell>Suda [2]</cell><cell>Qiu [3]</cell><cell>This Work</cell></row><row><cell>Frequency (MHz)</cell><cell>100</cell><cell>120</cell><cell>150</cell><cell>200</cell></row><row><cell>Precision</cell><cell>32 bit float</cell><cell>8-16 bit fixed</cell><cell>16 bit fixed</cell><cell>32 bit float</cell></row><row><cell>FPGA Version</cell><cell>Virtex 7 VX485T</cell><cell>Stratix-V GSD8</cell><cell>Zynq XC7Z045</cell><cell>Virtex 7 VX690T</cell></row><row><cell>DSP Utilization</cell><cell>2,240</cell><cell>Unspecified</cell><cell>780</cell><cell>1,307</cell></row><row><cell>GFLOPS/GOPS</cell><cell>61.62</cell><cell>136.5</cell><cell>187.8</cell><cell>50</cell></row><row><cell cols="3">was validated by implementing</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Xilinx, CMC and em-SYSCAN, NSERC, and CFI for the funding and resources provided for this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;15</title>
		<meeting>the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Throughput-Optimized OpenCL-based FPGA Accelerator for Large-Scale Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Suda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;16</title>
		<meeting>the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Going Deeper with Embedded FPGA Platform for Convolutional Neural Network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;16</title>
		<meeting>the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays, FPGA &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="26" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional Architecture for Fast Feature Embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Arithmetic Complexity of Computations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CBMS-NSF Regional Conference Series in Applied Mathematics</title>
		<imprint>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fast Algorithms for Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lavin</surname></persName>
		</author>
		<idno>CoRR, abs/1509.09308</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m">SDAccel Development Environment User Guide, September</title>
		<imprint>
			<publisher>Xilinx Inc</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">cuDNN: Efficient Primitives for Deep Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<idno>CoRR, abs/1410.0759</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model-driven Level 3 BLAS Performance Optimization on Loongson 3A Processor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xianyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yunquan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel and Distributed Systems (ICPADS), 2012 IEEE 18th International Conference on</title>
		<imprint>
			<date type="published" when="2012-12">Dec 2012</date>
			<biblScope unit="page" from="684" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convnet-benchmarks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://github.com/soumith/convnet-benchmarks" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>CoRR, abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Corr</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient Convolutional Neural Networks for Pixelwise Classification on Heterogeneous Hardware Systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tschopp</surname></persName>
		</author>
		<idno>CoRR, abs/1509.03371</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
