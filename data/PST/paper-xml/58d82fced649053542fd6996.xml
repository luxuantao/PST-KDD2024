<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale JPEG image steganalysis using hybrid deep-learning framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Large-scale JPEG image steganalysis using hybrid deep-learning framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F7C10831B2B2D42653B9FD6A0F9CE9D9</idno>
					<idno type="DOI">10.1109/TIFS.2017.2779446</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2017.2779446, IEEE Transactions on Information Forensics and Security 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>hybrid deep-learning framework</term>
					<term>CNN network</term>
					<term>steganalysis</term>
					<term>steganography</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adoption of deep learning in image steganalysis is still in its initial stage. In this paper we propose a generic hybrid deep-learning framework for JPEG steganalysis incorporating the domain knowledge behind rich steganalytic models. Our proposed framework involves two main stages. The first stage is hand-crafted, corresponding to the convolution phase and the quantization &amp; truncation phase of the rich models. The second stage is a compound deep neural network containing multiple deep subnets in which the model parameters are learned in the training procedure. We provided experimental evidences and theoretical reflections to argue that the introduction of threshold quantizers, though disable the gradient-descent-based learning of the bottom convolution phase, is indeed cost-effective. We have conducted extensive experiments on a large-scale dataset extracted from ImageNet. The primary dataset used in our experiments contains 500,000 cover images, while our largest dataset contains five million cover images. Our experiments show that the integration of quantization and truncation into deeplearning steganalyzers do boost the detection performance by a clear margin. Furthermore, we demonstrate that our framework is insensitive to JPEG blocking artifact alterations, and the learned model can be easily transferred to a different attacking target and even a different dataset. These properties are of critical importance in practical applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>I MAGE steganography can be divided into two main cate- gories: spatial-domain and frequency-domain steganography. The latter focuses primarily on JPEG images due to their ubiquitous nature. Both categories in state-of-the-art algorithms adopt content-adaptive embedding schemes <ref type="bibr" target="#b0">[1]</ref>. Most of these schemes use an additive distortion function defined as the sum of embedding costs of all changed elements. From early HUGO <ref type="bibr" target="#b1">[2]</ref>, to latest HILL <ref type="bibr" target="#b2">[3]</ref> and MiPOD <ref type="bibr" target="#b3">[4]</ref>, the past few years witnessed the flourish of additive schemes in spatial domain. In JPEG domain, UED <ref type="bibr" target="#b4">[5]</ref> and UERD <ref type="bibr" target="#b5">[6]</ref> are two additive schemes with good security performance. UNIWARD proposed in <ref type="bibr" target="#b6">[7]</ref> is an additive distortion function which can be applied for embedding both in spatial and JPEG domains. Its JPEG version, J-UNIWARD, achieves This work was supported in part by the NSFC (61772349, U1636202, 61402295, 61572329, 61702340), Guangdong NSF (2014A030313557), Shenzhen R&amp;D Program (JCYJ20160328144421330). This work was also supported by Alibaba Group through Alibaba Innovative Research (AIR) Program. (Corresponding author: Shunquan Tan.) S. Tan is with College of Computer Science and Software Engineering, Shenzhen University. J. Zeng, B. Li, and J. Huang are with College of Information Engineering, Shenzhen University.</p><p>All the members are with Shenzhen Key Laboratory of Media Security, Guangdong Province, 518060 China. (e-mail: tansq@szu.edu.cn).</p><p>best performance <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Research on non-additive distortion functions has made great progress in the spatial domain <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. However, analogous schemes have not yet been proposed in the JPEG domain. Although utilizing side information of a pre-cover image (raw or uncompressed) can improve the security of JPEG steganography <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, its applicability remains limited due to scarce availability of pre-cover images.</p><p>Most of modern universal steganalytic detectors use a rich model with tens of thousands of features <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> and an ensemble classifier <ref type="bibr" target="#b13">[14]</ref>. In spatial domain, SRM <ref type="bibr" target="#b10">[11]</ref> and its selection-channel-aware variants <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> reign supreme. In JPEG domain, DCTR <ref type="bibr" target="#b14">[15]</ref> feature set combines relatively low dimensionality and competitive performance, while PHARM <ref type="bibr" target="#b15">[16]</ref> and GFR <ref type="bibr" target="#b16">[17]</ref> exhibit better performance, although at the cost of higher dimensionality w.r.t. DCTR. SCA proposed in <ref type="bibr" target="#b17">[18]</ref> is a selection-channel-aware variant of JPEG rich models targeted at content-adaptive JPEG steganography <ref type="foot" target="#foot_0">1</ref> .</p><p>In recent years, with help of parallel computing accelerated by GPU (Graphics Processing Unit) and huge amounts of training data, deep learning frameworks have achieved overwhelming superiority over conventional approaches in many pattern recognition and machine learning problems <ref type="bibr" target="#b18">[19]</ref>. Researchers in image steganalysis have also tried to investigate the potential of deep learning frameworks in this field. Tan et al. explored the application of stacked convolutional autoencoders, a specific form of deep learning frameworks in image steganalysis <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr">Qian et al.</ref> proposed a steganalyzer based on CNN (Convolutional Neural Network) which achieving performance close to SRM <ref type="bibr" target="#b20">[21]</ref>, and demonstrated its transfer ability <ref type="bibr" target="#b21">[22]</ref>. In <ref type="bibr" target="#b22">[23]</ref>, Pibre et al. revealed CNN based steganalyzers can achieve superior performance in the scenario that embedding key is reused for different stego images. Xu et al. constructed another CNN-based steganalyzer <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> equipped with BN (Batch Normalization) layers <ref type="bibr" target="#b25">[26]</ref>. Its performance slightly surpass SRM. In this paper the model proposed by Xu et al. in <ref type="bibr" target="#b23">[24]</ref> is referred as Xu's model and is used for detection performance comparison. In <ref type="bibr" target="#b26">[27]</ref>, Sedighi and Fridrich implemented a specific CNN layer to imitate rich steganalytic model but failed to reached state-of-the-art performance. However, all of the above approaches <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, focusing on spatial-domain steganalysis, are all evaluated on the BOSSBase (v1.01) dataset <ref type="bibr" target="#b27">[28]</ref>. BOSSBase is arguably not representative of real-world steganalysis performance <ref type="bibr" target="#b28">[29]</ref>.</p><p>With only 10,000 images, deep learning frameworks trained on BOSSBase are prone to overfitting. Furthermore, except our work which study the effect of fitting deep-learning steganalytic framework to a JPEG rich-model features extraction procedure <ref type="bibr" target="#b29">[30]</ref>, no prior works addressed the application of deep learning frameworks in JPEG steganalysis.</p><p>In this paper, we proposed a generic hybrid deep-learning framework for large-scale JPEG steganalysis. Our proposed framework combines the bottom hand-crafted convolutional kernels and threshold quantizers pairing with the upper compact deep-learning model. Experimental evidences and theoretical reflections are provided to show the rationale of our proposed framework. Furthermore, we have conducted extensive experiments on a large-scale dataset extracted from ImageNet <ref type="bibr" target="#b30">[31]</ref> to demonstrate the capacity of our proposed generic framework under different scenarios.</p><p>The rest of the paper is organized as follows. In Sect. II, we describe the proposed hybrid deep-learning framework in detail, and provide experimental and theoretical testimonies to support its rationale. Results of experiments conducted on large-scale datasets are presented in Sect. III. Finally, we make a conclusion in Sect. IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. Our proposed JPEG steganalytic framework</head><p>In this section, we firstly introduce the training procedure of CNN as preliminaries. Then we discuss the motivations and challenges related to the introduction of quantization and truncation in JPEG deep-learning steganalysis. Finally we describe our generic framework with experimental evidences and theoretical reflection to support our design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head><p>The principal part of CNN is a cascade of alternating convolutional layers, regulation layers (e.g. BN layers <ref type="bibr" target="#b25">[26]</ref>) and pooling layers. On top of the principal part, there are usually multiple fully-connected layers. Please note that in CNN, only convolutional layers and fully-connected layers contain neuron units with learnable weights and biases 2 . Whether belongs to a convolutional layer or a fully-connected layer, each neuron unit receives inputs from a previous layer, performs a dot product with weights and optionally follows it with a nonlinear point-wise activation function. CNNs can be trained using backpropagation. For clarity, we omit those layers without learnable weights and biases, and denote the cascade of layers with learnable weights and biases in a given CNN as</p><formula xml:id="formula_0">[L 1 , L 2 , • • • , L n ],</formula><p>where L 1 is the input layer and L n is the output layer. L 2 , • • • , L n-1 are the layers whose weights and biases are trained in backpropagation, namely convolutional layers and fully-connected layers. Let a (l)  i denote the activation (output) of unit i in layer L l . For L 1 , a (1)   i is the i-th input fed to the framework. W (l)  i j denotes the weight associated with unit i in L l and unit j in L l+1 , while b (l) j denotes the bias associated with unit j in L l+1 . The weighted sum of inputs to unit j in L l+1 is defined as:</p><formula xml:id="formula_1">z (l+1) j = i W (l) i j a (l) i + b (l) j<label>(1)</label></formula><p>2 The learnable parameters {γ, β} for BN layers are omitted for brevity.</p><p>and a (l+1)</p><formula xml:id="formula_2">j = f (z (l+1) j</formula><p>) where f (•) is the activation function. The set of all W (l) i j and b (l) j constitutes the parameterization of a neural network and is denoted as W and b, respectively. For a mini-batch of training features-label pairs {(x (1) , y (1) ), • • • , (x (m) , y (m) )}, the goal of backpropagation is to minimize the overall cost function J(W, b) with respect to W and b:</p><formula xml:id="formula_3">J(W, b) = 1 m m h=1 J(W, b; x (h) , y (h) ) + R(W)<label>(2)</label></formula><p>where R(W) is a regularization term which suppresses the magnitude of the weights, and J(W, b; x (h) , y (h) ) is an error metric with respect to a single example (x (h) , y (h) ). <ref type="foot" target="#foot_1">3</ref> For each training sample, the backpropagation algorithm firstly performs a feedforward pass and computes the activations for layers L 2 , L 3 and so on up to the output layer L n . For the j-th output unit in the output layer L n , set the corresponding partial derivative of J(W, b; x (h) , y (h) ) with respect to z (n) j :</p><formula xml:id="formula_4">ϑ (n) j = ∂ ∂a (n) j J(W, b; x (h) , y (h) ) f (z (n) j )<label>(3)</label></formula><p>Then in the backpropagation pass, partial derivatives are propagated from L n back to the second last layer L 2 . For the j-th neuron unit in layer L l , set:</p><formula xml:id="formula_5">ϑ (l) j = ( k W (l) jk ϑ (l+1) k ) f (z (l) j )<label>(4)</label></formula><p>The partial derivatives with respect to</p><formula xml:id="formula_6">W (l) i j and b (l) j , l = n - 1, n -2, • • • , 1 are calculated as:          ∂ ∂W (l) i j J(W, b; x (h) , y (h) ) = a (l) i ϑ (l+1) j , ∂ ∂b (l) j J(W, b; x (h) , y (h) ) = ϑ (l+1) j ,<label>(5)</label></formula><p>Gradient descent is used to find the optimal W and b. In the optimization procedure, it updates W and b according to steps proportional to the negative of the average of m gradients each of which is the vector whose components are the partial derivatives in (5) <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The introduction of quantization and truncation in deeplearning based steganalysis</head><p>State-of-the-art rich models for JPEG steganalysis <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> take decompressed (non-rounded and non-truncated) JPEG images as input. The feature extraction procedure of JPEG rich models can be divided into three phases:</p><p>• Convolution: The target image is convolved with a set of kernels to generate diverse noise residuals. The purpose of this phase is to suppress the image contents as well as boost SNR (Signal-to-Noise Ratio). • Quantization and truncation (Q&amp;T): Different quantized and truncated versions of each residual are calculated to further improve diversity of resulting features, as well as reduce the computational complexity.  Take DCTR <ref type="bibr" target="#b14">[15]</ref> for example. Given a M × N JPEG image, it is firstly decompressed to the corresponding spatial-domain version X ∈ R M×N . Sixty-four 8 × 8 DCT basis patterns are defined as B (k,l) = (B (k,l) mn ), 0 ≤ k, l ≤ 7, 0 ≤ m, n ≤ 7:</p><formula xml:id="formula_7">B (k,l) mn = w k w l 4 cos πk(2m + 1) 16 cos πl(2n + 1) 16 ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_8">w 0 = 1 √ 2 , w k = 1 for k &gt; 0.</formula><p>X is convolved with B (k,l) to generate 64 noise residuals U (k,l) , 0 ≤ k, l ≤ 7:</p><formula xml:id="formula_9">U (k,l) = X * B (k,l) ,<label>(7)</label></formula><p>Then the elements in each U (k,l) are quantized with quantization step q and truncated to a threshold T . The DCTR features are constructed based on certain aggregation operation that collect specific first-order statistics of the absolute values of the quantized and truncated elements in each U (k,l) .</p><p>In <ref type="bibr" target="#b19">[20]</ref>, we pointed out that in general the above structure of rich models resembles CNN. Quantization and truncation has become an indispensable part of rich steganalytic models <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>. However, as far as we know, there still has been no published works regarding to the integration of quantization and truncation into deep-learning steganalyzers.</p><p>In this paper, we would like to utilize the domain knowledge behind rich models, especially the specific kernel matrices in the convolutional phase and the Q&amp;T phase. But, The introduction of quantization and truncation, namely the Q&amp;T phase on top of the bottom convolution phase, is a doubleedged sword. It cannot be put in the pipeline of gradientdescent-based learning. The Q&amp;T phase takes noise residuals generated by convolution phase as input, and can be modeled as:</p><formula xml:id="formula_10">a (2) j = f (z (2) j ) =        min([z (2) j /q], T ) if z (2) j &gt;= 0 max([z (2) j /q], -T ) if z (2) j &lt; 0 (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>where z (2)  j is an element of a given noise residual generated by the bottom convolution phase, a (2)  j is the corresponding activation output, q is the quantization step, [•] denotes the rounding operation, and T is a predefined threshold. It is obvious that f (z (2)  i ) is zero along the entire domain of z (2)  j except the set of points {(-T + 0.5)q, (-T + 1.5)q, • • • , (T -1.5)q, (T -0.5)q} where it is infinite. Therefore (8) cannot be put in the pipeline of gradient descent, since the derivative it passes on in backpropagation will vanish. More specifically, the derivative does not exist if z (2)  j is located at one of the points in the set {(-T + 0.5)q, (-T + 1.5)q, • • • , (T -1.5)q, (T -0.5)q}, otherwise the derivative is equal to zero. The corresponding gradient saturates if the partial derivative it passes on approaches to zero, and is nullified if there is no derivative.</p><p>Incompatibility between Q&amp;T phase and gradient-descentbased learning presents a dilemma in the design of deeplearning steganalytic framework. The introduction of Q&amp;T phase implies that gradient descent cannot be back propagated to the bottom convolution phase without the usage of some unconventional bypass trick. The generic hybrid deep-learning framework for JPEG steganalysis proposed in this paper is intended to provide a solution to this dilemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Our proposed hybrid deep-learning framework</head><p>Our proposed generic framework is composed of two stages. The first stage takes decompressed (non-rounded and non-  In the two figures "ABS" denotes the activation layer which outputs absolute values of the corresponding inputs, "BN" denotes the batch normalization layer, and "ReLU" denotes the layer with rectified-linear-unit activation functions. truncated) JPEG images as input, and corresponds to the convolution phase and the Q&amp;T phase of rich models. The proposed generic framework can be implemented in different way. The conceptual architecture of one implementation with twenty-five 5 × 5 DCT basis patterns and three Q&amp;T combinations is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. In this implementation, the first stage incorporated the first two phases of DCTR <ref type="bibr" target="#b14">[15]</ref>. All model parameters in this stage are hand-crafted and gradientdescent-based learning is disabled. What makes this stage different from DCTR is that DCTR uses sixty-four 8 × 8 DCT basis patterns and only one Q&amp;T combination, while our proposed approach contains twenty-five 5 × 5 DCT basis patterns which are defined as B (k,l) = (B (k,l)  mn ), 0 ≤ k, l ≤ 5, 0 ≤ m, n ≤ 5:</p><formula xml:id="formula_12">B (k,l) mn =</formula><p>w k w l 5 cos πk(2m + 1) 10 cos πl(2n + 1) 10 ,</p><formula xml:id="formula_13">w 0 = 1, w k = √ 2 for k &gt; 0. (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>and three Q&amp;T combinations, namely (T = 4, Q = 1), (T = 4, Q = 2) and (T = 4, Q = 4). Given an input image, the convolution phase outputs twenty-five residual maps. The residual maps pass through the Q&amp;T phase. Three different groups of quantized and truncated residual maps are generated. They constitute the input of the second stage. The intention behind the design of the first stage of our proposed framework is that we would like to utilize the domain knowledge behind rich models, especially the specific kernel matrices in the convolutional phase and the Q&amp;T phase. We agree with the concepts in rich models <ref type="bibr" target="#b10">[11]</ref>: model diversity is crucial to the performance of steganalytic detectors. The model diversity of our proposed framework is represented in twenty-five DCT basis patterns in the hand-crafted convolutional layer and the three Q&amp;T combinations that followed. There are total 25 × 3 = 75 sub-models in our proposed framework.</p><p>The second stage is a compound deep CNN network in which the model parameters are learned in the training procedure. The bottom of the second stage is composed of three independent subnets with identical structure. Each subnet corresponds to one group of quantized and truncated residual maps. They take the residual maps as input and generate three feature vectors. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, within this implementation, two types of subnet configurations are adopted. Both of them contain three convolutional layers and output a 512-D (512 dimensional) feature vector. Type1 subnet (Fig. <ref type="figure" target="#fig_1">2(a)</ref>) adopts 1×1 convolutional kernels in the top-most convolutional layer and uses a single average pooling layer with large 32 × 32 pooling windows at the end, as suggested in Xu's model <ref type="bibr" target="#b23">[24]</ref>. However, deviated from the recipe suggested in Xu's model <ref type="bibr" target="#b23">[24]</ref> that using TanH (Hyperbolic Tangent) activation function in the lower part, we always use ReLU (Rectified Linear Unit) activation function in Type1 subnet. Type2 subnet (Fig. <ref type="figure" target="#fig_1">2(b)</ref>) is a traditional CNN configuration. Compared with Type1 subnet, it adopts progressive pooling layers and uses 3 × 3 convolutional kernels in the top-most convolutional layer. Due to the progressing pooling layers, Type2 subnet is a relative GPU memory-efficient model. The GPU memory requirement of Type2 subnet is only one-seventh of that of Type1 subnet. Both configurations have in common are the BN layers which follow every convolutional layer.</p><p>In this implementation, three 512-D feature vectors output by the bottom subnets are concatenated together to generate a single 1536-D feature vector. The feature vector is subsequently fed into a four-layer fully-connected neural network which makes the final prediction. The successive layers of the fully-connected network contain 800, 400, 200, and 2 neurons, respectively. ReLU activation functions are used in all three hidden layers. The final layer contains two neurons which denote "stego" prediction and "cover" prediction, respectively. Softmax function is used to output predicted probabilities.</p><p>Recent researches on deep-learning revealed that ensemble prediction with independently trained deep-learning models can improve the performance <ref type="bibr" target="#b32">[33]</ref>. In <ref type="bibr" target="#b24">[25]</ref>, Xu et al. also demonstrated the potential of ensemble prediction in deeplearning based steganalysis. Therefore, when compared to state of the art in Sect. III-C, we also introduce model ensemble in the final prediction in order to further promote the detection performance. Different from the approaches in <ref type="bibr" target="#b24">[25]</ref>, we adopt a simple ensemble strategy, like the one used in <ref type="bibr" target="#b32">[33]</ref>. Five versions of our proposed deep-learning models are independently trained with the same learning setting and training dataset. They differ only in initial weights of the learnable stage. When testing, the decision of the five models are combined with majority voting.</p><p>There is significant difference between our proposed framework and other existing deep-learning steganalyzers <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Firstly, we explicitly introduce the Q&amp;T phase used in rich models into our proposed deep-learning steganalytic framework, which have never been seen in previous works. Secondly, we adopt an array of dozens of hand-crafted convolutional kernels in the bottom layer of our proposed framework, instead of an image pre-processing layer with a single high-pass filter used in previous works. And finally, there are three parallel CNN subnets with identical structure in the central portion of our proposed framework, which also have never been seen in previous works.</p><p>Our large-scale experiments reported in the following Sect. III demonstrated that the introduction of Q&amp;T phase do bring substantial detection performance improvement. The performance improvement is not only due to the model diversity brought by different Q&amp;T combinations (as shown in Sect. III-B). The discretization brought by quantization and truncation itself also has an obvious impact on the detection performance. We report the following experimental evidences to support our argument. The experiments were conducted on basic500K with setups shown in Sect. III-A. J-UNIWARD stego images with 0.4bpnzAC (bits per non-zero cover AC DCT coefficient) were included in the experiments. In the experiments our proposed framework was equipped with Type1 subnet. A corresponding model was trained and tested independently for each configuration combination. We tested the trained model every 10, 000 iterations, and reported the best testing accuracy in 20×10<ref type="foot" target="#foot_3">4</ref> iterations. No ensemble prediction was involved in this experiment, as in Sect. III-B. The basic evidences are listed as follows:</p><p>From the above experimental evidences we can clearly see that both quantization and truncation effectively improve the detection performance.</p><p>As mentioned in the last section, the introduction of Q&amp;T phase implies that gradient descent cannot be back propagated to the bottom convolution phase. However, we still can backpropagate a fixed fake tiny derivative d to the bottom convolution phase. 4 However, our extensive experiments show that such a fake derivative just leads to serious performance degradation. For example, using a Q&amp;T phase with a fixed fake derivative d, the detection accuracy of our proposed framework as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref> is merely 60.5% when d = 0.01, and 56.8% when d = 0.001. Therefore, at present no compromise solution to the incompatibility can be found.</p><p>But, does gradient-descent optimization of the bottom convolution phase really matter? The following two experimental evidences reveal that gradient-descent optimization of the bottom convolution phase cannot improve the detection performance:</p><p>• The detection accuracy of Xu's model with a learnable bottom convolutional kernel, which is initialized as the high-pass filter used in <ref type="bibr" target="#b23">[24]</ref>, is 54.6%. Its performance is slightly worse than the one with fixed high-pass filter.</p><p>• The detection accuracy of our proposed framework without Q&amp;T phase is 61.3%, under the condition that gradient-descent-based learning is enabled for the bottom convolution phase. Its performance is also slightly worse than the one with fixed DCT basis patterns. Recently in a similar field, image forensics, Bayar et al. proposed a convolutional-layer regularizer which was claimed can be used to suppress the content of an image <ref type="bibr" target="#b33">[34]</ref>. However, we observed that regularizing the bottom convolutional kernels using the approach in <ref type="bibr" target="#b33">[34]</ref> did not lead to positive changes in the above two experimental evidences:</p><p>• The detection accuracy of Xu's model is still 54.6%.</p><p>• The detection accuracy of our proposed framework without Q&amp;T phase is 61.2%, slightly worse than the prior one. All of the above experimental evidences reveal that at least in the field of JPEG steganalysis, it is extraordinary difficult for an existing deep-learning steganalytic framework to benefit from gradient-descent optimization of the bottom convolution phase, under the premise that the kernels in the bottom convolution phase have already possessed the same parameters as those used in rich models. We attribute this difficulty to the contradiction between the design philosophy (or domain knowledge) of the kernels in rich models and the gradient descent algorithm used in deep-learning frameworks. The long and widely accepted philosophy behind rich steganalytic models is that high-pass kernels should be designed to extract the noise component (noise residual) of images rather than their content <ref type="bibr" target="#b10">[11]</ref>. However, as shown in the theoretical reflection in Appendix A, for a deep-learning framework, we argue that the optimization of the bottom convolutional kernels in favor of the extraction of stego noises is hard to achieve with gradient descent.</p><p>The experimental demonstration in this section indicates that the introduction of Q&amp;T phase do bring substantial detection performance improvement. Certainly, the introduction of Q&amp;T phase is with negative side effect: it blocks the back-propagated gradients. But the theoretical reflection in Appendix A shows that such negative side effect can be ignored, since even not cut off by Q&amp;T phase, the backpropagated gradients is still hard to properly guide the optimization of the bottom convolutional layer, as long as its optimization goal is to benefit the extraction of stego noises. In fact, the authors believe that we cannot directly draw on the design philosophy of rich models to understand the underlying mechanism of deep-learning steganalytic framework. Deeplearning frameworks are trained and optimized as a whole. It may be not suitable to isolate one part of a given deeplearning framework (e.g. the learnable bottom convolutional layer) and force it to comply with existing design philosophy. Therefore our proposed hybrid deep-learning framework for JPEG steganalysis is designed to be composed of two stages. The bottom hand-crafted stage, which contains the convolution phase and the Q&amp;T phase incorporated from rich models and complied with its design philosophy, is not involved in gradient-descent-based optimization. The second stage is a compound deep CNN network which does not need to comply with the design philosophy of rich models, and is free to be optimized using backpropagation as a whole.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experiment setups</head><p>We adopted ImageNet <ref type="bibr" target="#b30">[31]</ref>, a large-scale image dataset containing more than fourteen million JPEG images, to evaluate the steganalytic performance of our proposed hybrid deeplearning framework. All of the experiments were conducted on a GPU cluster with eight NVIDIA ® Tesla ® K80 dual-GPU cards. Independent models are trained and tested in parallel, each of which is assigned one GPU. By considering the computation capacity, we restricted the size of the target images to 256 × 256. We randomly selected 50 thousand, 500 thousand and 5,000 thousand (namely 5 million) JPEG images with size larger than 256 × 256 from ImageNet. Their lefttop 256 × 256 regions were cropped, converted to grayscale and then re-compressed as JPEG with quality factor 75. 5 The resulting images constituted the following three basic cover image datasets:</p><p>• basic50K: The small-scale dataset used in our experiments. By comparing the detection performance of our proposed framework on basic50K and basic500K (see 5 The original quality factors of ImageNet images are diverse. Out of 10 million ImageNet images with size larger than 256 × 256, there are more than 1.5 million images whose quality factors cannot be detected by ImageMagick utility "identify", and roughly 8.3 million images with diverse quality factors which are larger than 75. We uniformly converted the quality factors of the selected images to 75 due to the following two reasons: Firstly, all the reported experiments of previous works, including DCTR, PHARM, GFR, and SCA, are conducted on images with quality factor 75 and 95. And secondly, if the target quality factor is set to 95, then for a majority of the selected images, we need to elevate their quality factors which may introduce exploitable artifacts. below), we can highlight the superiority of our proposed framework in large-scale dataset.</p><p>• basic500K: The major dataset for most all of our experiments, including the verification experiments to determine hyper-parameters of our proposed framework. • basic5000K: The largest-scale dataset used in our experiments. Due to the limitation of computation capacity, we only conducted the experiments on stego images with 0.4 bpnzAC. Our implementation was based on the publicly available Caffe toolbox <ref type="bibr" target="#b34">[35]</ref> with our implemented hand-crafted convolutional layer (with 5 × 5 DCT basis patterns) and Q&amp;T layer according to <ref type="bibr" target="#b7">(8)</ref>. Our proposed models were trained using mini-batch stochastic gradient descent with "step" learning rate starting from 0.001 (stepsize: 5000; weight decay: 0.0005; gamma: 0.9) and a momentum fixed to 0.9. The batch size in the training procedure was 64 and the maximum number of iterations was set to 20 × 10 4 . In each experiment, we tested the trained model in the corresponding standalone testing set every 10, 000 iterations, and reported the best testing accuracy in 20 × 10 4 iterations. Please note that as later shown in Fig. <ref type="figure">4</ref>, when trained on a large-scale dataset such as basic500K, our proposed framework exhibited good convergence and stability after less than 5 × 10 4 iterations. Therefore validation set was omitted for the sake of resources saving. The source code and auxiliary materials are available for download from GitHub<ref type="foot" target="#foot_4">6</ref> . J-UNIWARD <ref type="bibr" target="#b6">[7]</ref>, UERD <ref type="bibr" target="#b5">[6]</ref> and UED <ref type="bibr" target="#b4">[5]</ref>, the three stateof-the-art JPEG steganographic schemes, were our attacking targets in the experiments. The default parameters of the three steganographic schemes were adopted in our experiments. 50% cover images were randomly selected from basic50K, basic500K, and basic5000K, respectively. They constituted the training set along with their corresponding stego images. The rest 50% cover-stego pairs in the dataset were for testing. We further guaranteed that the cover images included in an arbitrary training set of the three datasets would not appear in any of the three testing sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of the framework architecture on the performance</head><p>In Tab. I, we compare the effect of different Q&amp;T combinations, different hand-crafted convolutional kernels, and the presence of BN layers. The experiment was conducted on basic500K. A corresponding model is trained and tested independently for each configuration combination. No ensemble prediction is involved in this experiment. We can see that under the same conditions, DCT basis patterns (including the 8 × 8 DCTR kernels <ref type="bibr" target="#b14">[15]</ref>) always perform better than PHARM kernels <ref type="bibr" target="#b15">[16]</ref>. The experimental results support our choice of DCT basis patterns. 5 × 5 DCT basis patterns can achieve significant performance improvement compared to 3 × 3 DCT basis patterns. However, the performance of the more complex 8 × 8 DCTR kernels is not even as good as the 3 × 3 DCT basis patterns, which indicates that increasing the size of the convolutional kernels is not always beneficial at the cost of increasing model complexity. The performance of GFR kernels <ref type="bibr" target="#b16">[17]</ref> is slightly better than 5 × 5 DCT basis patterns. However, with as many as two hundred and fifty-six output residual maps, GFR kernels are too resource consuming to be included in our proposed framework. Different Q&amp;T combinations also affect the performance of our proposed framework. Combinations with three different quantization steps and the same threshold are of relatively cost-effective. BN layers in the subnets are crucial, especially the first one at the bottom of the subnets. Therefore, based on the described results, we adopt twenty-five 5 × 5 DCT basis patterns, T = 4, Q = [1, 2, 4] and subnet configurations with a BN layer following every convolutional layer in our final proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to state of the art</head><p>In Fig. <ref type="figure" target="#fig_3">3</ref>, we compare the performance of our proposed framework and other steganalytic models in the literature. Please note that for a fair comparison, Xu's model <ref type="bibr" target="#b23">[24]</ref> is also fed with decompressed (non-rounded and non-truncated) images, and is trained with the same learning protocol as that for our own model 7 . Fig. <ref type="figure" target="#fig_3">3</ref> we can see that our proposed framework can obtain significant performance improvement compared with DCTR <ref type="bibr" target="#b14">[15]</ref>, GFR <ref type="bibr" target="#b15">[16]</ref>, and even recently proposed selection-channel-aware JPEG rich model SCA-GFR <ref type="bibr" target="#b17">[18]</ref>. For all of the three steganographic algorithms, the performance of Xu's model was unsatisfactory. The degraded performance of Xu's model is acceptable, since it is designed for spatial-domain steganalysis. The superiority of our proposed framework is more obvious in basic500K. This is due to the fact that with more training samples raised by one magnitude, the large-scale basic500K dataset with 500,000 training samples (covers plus the corresponding stegos) is more favor of deep-learning frameworks like the one proposed by us. If only consider the performance of a single model, our proposed framework with Type1 subnets behaved better than its companion with Type2 subnets. Furthermore, the final prediction conducted by the ensemble of five independently trained models shows that model ensemble could improve the detection accuracy by 1% regardless of the type of the underlying subnet configurations. 8 Since the performance of our proposed framework with Type1 subnets is always better than that with Type2 subnets, we insisted on using Type1 subnets in the following experiments. However, please note that Type2 subnet can potentially be used in more complex deep-learning steganalytic frameworks in the future since it is a memory-efficient model.</p><p>In Fig. <ref type="figure">4</ref> we show how the testing accuracy changes with successive training iterations in the experiments which were conducted on basic50K, basic500K and basic5000K, our largest-scale dataset, respectively. The tests were performed 7 The original Xu's model is fed with 512 × 512 images. In order to make it adapt to 256×256 inputs used in our experiments, we explicitly set "stride=2" for its bottom convolutional layer which takes the residual map generated by the KV kernel as input. Please note that we also set "stride=2" in the bottom convolutional layer of Type1 and Type2 subnets of our proposed framework. 8 Please note that the ensemble approach of Xu's model <ref type="bibr" target="#b23">[24]</ref> can also probably obtain better results. The experimental results of ensemble prediction of Xu's model are omitted in Fig. <ref type="figure" target="#fig_3">3</ref> for clarity.  <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b3">4)</ref> 74.6% 72.5% 50.0% (2,1), (4,2), <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b3">4)</ref> 74.1% 72.4% 50.0% (6,1), (4,2), <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4)</ref> 72.3% 71.5% 50.0%</p><p>a Logograms are used in expressing Q&amp;T combinations. For example, (4,1) denotes (T = 4, Q = 1). on standalone testing dataset every 10, 000 training iterations and the models were trained for 20 × 10 4 iterations in total. Only stego images with 0.4bpnzAC were included in the experiments due to the limited computational capacity. Even so for basic5000K there were five million images (covers plus the corresponding stegos) involved in a training epoch. Our proposed deep-learning framework showed strong learning capacity that further improves along with the growth of training samples. From Fig. <ref type="figure">4</ref> we can also see that the curve of testing accuracy for the framework trained on basic5000K not only is of the best performance but also is of the best stability. Please note that 20 × 10 4 iterations is roughly equivalent to 256 epochs for basic50K, <ref type="bibr" target="#b24">25</ref>    have been fully exploited. 9  Throughout the experiments, our proposed framework ran steadily. During the training procedure, it could accomplish 1,000 iterations every 20 minutes. That is to say, 20 × 10 4 training iterations could be finished in about 67 hours. With K80 GPU cards, We can expect to finish one epoch of training 9 The implementation of ensemble classifier <ref type="bibr" target="#b13">[14]</ref> used by rich models cannot be scaled to large-scale datasets. Therefore we cannot provide the testing accuracy of DCTR and GFR in basic5000K for comparison in Fig. <ref type="figure">4</ref>. in 0.26 hour, 2.6 hours, and 26 hours for basic50K, basic500K, and basic5000K, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance with mismatched targets, altered blocking artifact, doubled-sized inputs and single-compressed images</head><p>First of all, please note that in the following experiments, our proposed framework is equipped with Type1 subnet. No ensemble prediction is involved to reduce the time of experiments. In Fig. <ref type="figure">5</ref>, we observe the attacking-target transfer ability of our proposed framework. The framework was trained with J-UNIWARD cover/stego pairs and then tested with UERD/UED cover/stego pairs. The detection accuracy is roughly 3% -4% worse compared with that trained and tested with the same type of stego images. However, the degradation of detection performance is acceptable especially for the detection of UED stego images given that UED works in a very different way compared with J-UNIWARD.</p><p>8 × 8 block processing during JPEG compression introduces blocking artifacts, which can be used as intrinsic statistical characteristic of JPEG cover images. Secret bits embedded in the DCT domain tend to impair blocking artifacts, therefore leave traces which can be utilized by steganalyzers. An interesting problem is to access the performance of our proposed framework depending on the intrinsic statistical characteristic of blocking artifacts. In Fig. <ref type="figure" target="#fig_5">6</ref>, we observe the impact of altered blocking artifacts on the performance of our proposed framework. The default testing set in basic500K contains The legend "C" in parentheses denotes those tested on central-cropped images, while "L" in parentheses denotes those tested on the original basic500K testing set. For example, "J-UNIWARD (C)" means that the corresponding framework was trained and tested with J-UNIWARD stego images. It was trained on basic500K training set and then tested on the corresponding testing set with central-cropped images.</p><p>left-top cropped images in which the original DCT grid alignment is preserved. In this experiment for all the testing images in basic500K, we re-compressed their corresponding original images in ImageNet with quality factor 75 and then converted them to grayscale images again. We cropped their central 256 × 256 regions to constitute a new testing set. The motivation is that central cropping cannot preserve the original DCT grid alignment in most cases, therefore the blocking artifacts from two different sources coexist. As a result the blocking artifacts in the images of the new testing set are different from those in the training set. However, Fig. <ref type="figure" target="#fig_5">6</ref> reveals that the impact of altered blocking artifact on the performance of our proposed framework is small. Our proposed framework has captured more complex intrinsic statistical characteristic besides blocking artifact. All of the above experiments used images of size 256 × 256 pixels. This limitation stems mainly from the following two 1556-6013 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.  Firstly, target images with larger size, e.g. 512 × 512 pixels result in deep-learning models hard to train with K80 GPU cards we have in hands. Secondly, large-sized ImageNet images are in the minority. Out of fourteen million ImageNet images, only roughly 0.7 million of them are larger than 512 × 512 pixels. In the following experiment, we tested our proposed framework with double-sized inputs on this dataset. 500 thousand JPEG images with size larger than 512 × 512 were randomly picked out from ImageNet  and were converted to 512 × 512 with the same processing procedure as mentioned in Sect. III-A. Due to GPU memory constraints, we simplified the model by using doubled stride in the convolutional layer of each subnet (i.e. 4 instead of 2). All other experiment setups were remained the same except that the batch size in the training procedure is reduced to 32. Only J-UNIWARD stego images with 0.4bpnzAC were included in the experiment. Fig. <ref type="figure">7</ref> shows the testing accuracy in successive training iterations. The training procedure also converged quickly and delivered better performance than the DCTR and GFR models. Due to the limited computational capacity, subnets with wider and deeper structures were not evaluated in this experiment. Its potential for target images with larger size may have not been fully demonstrated.</p><p>Up to now we used double-compressed images in the experiments. As reported by Pibre et al. <ref type="bibr" target="#b22">[23]</ref>, CNN based steganalyzers can take advantage of seemingly irrelevant subtle patterns to boost their performance. We must eliminate the possibility that our proposed framework makes use of the double compression artifacts to dispel the doubts of the colleagues. Hence, we conducted two more experiments with single-compressed JPEG images.</p><p>Firstly, There are about 410, 000 ImageNet images can be confirmed as being compressed with quality factor 75.</p><p>They were all selected. Their left-top 256 × 256 regions were cropped, converted to grayscale without double compression to constitute a new dataset "basicQ75". 200, 000 cover images were randomly selected from them for training while the rest were for testing. In Fig. <ref type="figure">8</ref>, we compare the performance of our proposed framework with three other steganalyzers for J-UNIWARD on bacicQ75 dataset. For the sake of brevity, only the results of half of the steganalyzers listed in Fig. <ref type="figure" target="#fig_3">3</ref> are listed in Fig. <ref type="figure">8</ref>. However, by comparing Fig. <ref type="figure">8</ref> and Fig. <ref type="figure" target="#fig_3">3</ref>(b), we still can find that as with all other three steganalyzers, our proposed framework only suffered slight performance degradation, which may be attributed to the relative lack of diversity in basicQ75 dataset.</p><p>Secondly, we divided every image in BOSSBase public dataset <ref type="bibr" target="#b27">[28]</ref> into four equal parts and then JPEG compressed them with quality factor 75. Through this method, we obtained 40, 000 single-compressed JPEG cover images. We denoted them as "boss40K" dataset, and used all of the 40, 000 cover images and the corresponding stego images to test the performance of our proposed framework and other steganalyzers trained on basic500K. We prefer to use all of the images in boss40K dataset in testing rather than in training, which is based on the following two aspects: 1. Merely 40, 000 images are not suitable for training a deep-learning steganalyzer with hundreds of thousands of learnable parameters. 2. As a dataset with totally different source, boss40K is more suitable for checking transfer ability of steganalyzers trained with ImageNet images.</p><p>In Fig. <ref type="figure" target="#fig_7">9</ref>, we show the testing results of our proposed framework (with Type1 subnet, without ensemble) and other steganalyzers on boss40K. Please note that all the steganalyzers used in this experiment were trained on basic500K. By comparing Fig. <ref type="figure" target="#fig_7">9</ref> and Fig. <ref type="figure" target="#fig_3">3</ref>(b), we are delighted to find that our proposed framework even achieved better detection performance, and its superiority over all other three steganalyzers became more obvious. Fig. <ref type="figure" target="#fig_8">10</ref> shows testing accuracy of our proposed framework on boss40K in successive training iterations. Please note that the model was also trained on basic500K. From Fig. <ref type="figure" target="#fig_8">10</ref> we can see our proposed framework trained on basic500K exhibited rapid convergence even when evaluated on a dataset with totally different source, which provides complementary evidence to support the removal of validation set in our large-scale experiments.</p><p>For the sake of completeness, we also show the testing results of our proposed framework (with Type1 subnet, without ensemble) and other steganalyzers on boss40K dataset, when all of them were also trained on boss40K in Fig. <ref type="figure" target="#fig_9">11</ref>. Since validation cannot be omitted for a small-scale dataset, boss40K was split into 60/15/25 ratio, for training, validating, and testing, respectively. We guaranteed that all the sub-images of a given BOSSBase image could only be assigned to one sub-dataset. Please note that our proposed framework aims at large-scale JPEG image steganalysis. It needs to be fed with a great deal of labeled samples in the training procedure. Therefore from Fig. <ref type="figure" target="#fig_9">11</ref>, it is no doubt that superiority of our proposed framework in such a small-scale dataset was not obvious. However, it still retained equal or even slightly better performance than GFR. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Comparison to newly emerging works</head><p>During the course of the review process, we noticed that two new research works in the field of deep-learning JPEG steganalysis have been published <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Due to the limited computational capacity, we only conducted a comparative study of our proposed framework and the framework proposed in <ref type="bibr" target="#b35">[36]</ref> (referred as Xu's new model) since it also aims at largescale JPEG image steganalysis.</p><p>In <ref type="bibr" target="#b35">[36]</ref>, Xu compared his framework with this work preprinted on arXiv, and claimed that his framework can achieve significant performance improvement compared to the implementation of our generic framework illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. However, Please note that as shown in Tab. II, Xu's new model <ref type="bibr" target="#b35">[36]</ref> is a behemoth with about triple parameters and more than five times of computational complexity compared with our proposed framework with Type1 subnet. Therefore it is natural for Xu's new model <ref type="bibr" target="#b35">[36]</ref> to achieve better detection performance with multiple expansions in capacity.</p><p>Xu deprecated the use of quantization in deep-learning based steganalyzer, which we do not agree with. We conducted a verification experiment. As shown in Fig. <ref type="figure" target="#fig_1">12</ref>, on the standalone basic500K testing set which contains 500, 000 cover-stego pairs, simply adding back quantization with Q = 1 to Xu's new model <ref type="bibr" target="#b35">[36]</ref> could not only make the detection performance more stable but also improve testing accuracy. That is to say, even with Xu's new model <ref type="bibr" target="#b35">[36]</ref>, experimental evidence also supports the introduction of Q&amp;T phase in deep-learning steganalyzers, and supports our opinion that recognizing threshold quantizers as a whole.</p><p>As  per is a generic hybrid architecture for deep-learning JPEG steganalyzers. It is composed of two stages. The first stage is equipped with hand-crafted model parameters, while the second stage is a compound deep CNN network with a sequence of independent subnets, and the actual number of the subnets is determined by the Q&amp;T combinations experimentally. Newly emerging deep-learning steganalyzers can be used as the prototypes of the subnets in the second stage of our proposed hybrid architecture. Via this way, we can incorporate them into our framework. For example, Type1 subnet used in our work is inspired by Xu's model <ref type="bibr" target="#b23">[24]</ref>. Certainly we can also incorporate Xu's new model <ref type="bibr" target="#b35">[36]</ref> into our framework. However, a complete incorporation of Xu's new model <ref type="bibr" target="#b35">[36]</ref> in our framework involves a great deal of experiments for architecture adjustments (e.g. evaluating different Q&amp;T combinations), and is beyond the scope of this work. Here we just provide a straightforward incorporation to demonstrate the generality and potentiality of our proposed framework. As shown in Fig. <ref type="figure" target="#fig_10">13</ref>(a), Xu's new model <ref type="bibr" target="#b35">[36]</ref> is incorporated in our hybrid framework as the prototype of two subnets, one is with (T = 8, Q = 1) while another is with (T = 8) and quantization disabled (the original setting in Xu's new model <ref type="bibr" target="#b35">[36]</ref>). Fig. <ref type="figure" target="#fig_10">13</ref>(b) shows the testing accuracy in successive training iterations for this new hybrid framework and Xu's new model <ref type="bibr" target="#b35">[36]</ref>. From Fig. <ref type="figure" target="#fig_10">13</ref>(b), we can see our proposed framework incorporated with Xu's new model outperformed the original one by a clear margin. We expect that greater performance improvement can be achieved with more complete incorporation of Xu's new model <ref type="bibr" target="#b35">[36]</ref> in our hybrid generic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Concluding remarks</head><p>Application of deep-learning frameworks in image steganalysis has drawn attention of many researchers. In this paper we proposed a hybrid deep-learning framework for large-scale JPEG steganalysis, which for the first time utilize quantization and truncation into deep-learning steganalyzers. We have provided experimental and theoretical testimonies to support the utilization of quantization and truncation in the proposed framework. Our proposed framework is generic, so that existing deep-learning based steganalyzers is easy to be incorporated into it as a subnet prototype. We have demonstrated the capacity of the proposed framework with different subnet configurations, including one that incorporated from a new JPEG deep-learning steganalyzer emerged during the review process. The extensive experiments conducted on a large-scale dataset extracted from ImageNet clearly show that our proposed framework provides a boost of performances with quantitative metrics.</p><p>Our future work will focus on two aspects: (1) incorporation of adversarial machine learning into our proposed framework to make it jointly optimized with its opponent; (2) further exploration of the application of our proposed framework in the field of multimedia forensics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Theoretical reflection</head><p>State-of-the-art steganalytic feature extractors, either in spatial domain or in JPEG domain, take the spatial representation (usually type-casted to real) of target image as input <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Furthermore, please note that JPEG steganalytic feature extractors are usually fed with decompressed (non-rounded and non-truncated) JPEG images. We follow this approach in our research. Therefore, a grayscale input image can be represented as X = (x pq ) M×N = C + N, where C = (c pq ) M×N , c pq ∈ R denotes the corresponding cover image, and N = (n pq ) M×N , n pq ∈ R denotes the additive stego noise 10 .</p><p>Our reflection starts from one easily-verified fact: the magnitude of most of the elements of N matrix remain tiny with respect to the corresponding elements of C even for a stego image with high embedding rate (on average close to two orders of magnitude larger). State-of-the-art content-adaptive steganography, whether in spatial domain or in JPEG domain, tends to embed secret bits in highly textured area. As a result, even filtered by state-of-the-art steganalytic kernels (e.g. KV kernel used in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>) the magnitudes of most of the filtered residual elements are still much larger than the corresponding stego noises.</p><p>Suppose that we apply a convolutional layer with kernels of size of m × n, and suppose we take as input X of size M × N. Since in the context the input and output of a convolutional layer are of two-dimensions, we adopt twodimensional indexing here. Convolution is just a dot product with local-connected-and-shared weights. That is to say, for each given z (2)  rs , it is only the weighted sum of lower-layer inputs located in a m × n local area with index (r, s) as its centre irrespective of boundary condition, and the weights used in the weighted sum are shared in the calculation of all the z (2)  rs , 1 ≤ r ≤ M, 1 ≤ s ≤ N.</p><p>By rewriting (1) using two-dimensional indexing, setting l = 1, a (1)  pq = x pq and restrict the size of the dot product to m × n (m and n assume to be odd to omit unimportant details), we get: W (1)   (r-m 2 +p)(s-n 2 +q),rs n (r-m 2 +p)(s-n 2 +q) + b (1)  rs <ref type="bibr" target="#b9">(10)</ref> In <ref type="bibr" target="#b9">(10)</ref>, • denotes the ceiling operation. <ref type="bibr" target="#b9">(10)</ref> we can see that if the convolutional layer is initialized with kernels which are already sensitive to the stego noise (e.g. KV kernel) or is regularized as high-pass as proposed in <ref type="bibr" target="#b33">[34]</ref>, then m p=1 n q=1 W (1)   (r-m 2 +p)(s-n 2 +q),rs c (r-m 2 +p)(s-n 2 +q) can be suppressed. However, as we mentioned above, the magnitudes of most of the filtered residual elements are still much larger than the corresponding stego noises, and the accumulation in <ref type="bibr" target="#b9">(10)</ref> helps reduce the influence of outliers. Therefore in either scenario, on average m p=1 n q=1 W (1)   (r-m 2 +p)(s-n 2 +q),rs c (r-m 2 +p)(s-n 2 +q) still accounts for the vast majority magnitude when compared with m p=1 n q=1 W (1)   (r-m 2 +p)(s-n 2 +q),rs n (r-m 2 +p)(s-n 2 +q) in <ref type="bibr" target="#b9">(10)</ref>. 10 For JPEG steganography, the additive stego noise is directly added to quantized DCT coefficients. However, the linearity property of the DCT/IDCT transform guarantees that the corresponding stego noise in the spatial-domain representation is still additive.</p><p>For a given index ( p, q) where p = r-m 2 + p, q = s-n 2 +q, according to (4) and ( <ref type="formula" target="#formula_6">5</ref>) we can see that when the gradient is backpropagated to the layer 1 : ∂ ∂W (1)   p q,rs J(W, b; x (h) , y (h) ) = x p q • ϑ (2) rs = (c p q + n p q) • ϑ (2)   rs <ref type="bibr" target="#b10">(11)</ref> in which: ϑ (2)  rs = ( k W (2)  rs,k ϑ (3) k ) f (z (2)  rs )</p><p>In <ref type="bibr" target="#b11">(12)</ref> k W (2)  rs,k ϑ (3)  k is fixed when the gradient is backpropagated to the layer L 2 . As a result ϑ (2)  rs ∝ f (z (2)  rs ). Please note that f (z (2)  rs ) is the derivative of the activation function of z (2)  rs . The derivatives of all of the existing practical activation functions, including Sigmoid, TanH, and ReLU, have narrow ranges. And furthermore, if only consider the curve in positive axis (or negative axis), it is easy to verify that they are linear, or quasi-linear, namely: min{ f (z 1 ), f (z 2 )} ≤ f (λz 1 + (1 -λ)z 2 ) ≤ max{ f (z 1 ), f (z 2 )}, (13) for any λ ∈ (0, 1) and z 1 z 2 . Based on the fact that ϑ (2)  rs ∝ f (z (2)  rs ), ϑ (2)  rs is proportional/inverse proportional to, or quasi-proportional/inverse quasi-proportional to z (2)  rs provided the polarity of z (2)  rs remains the same. Return to <ref type="bibr" target="#b9">(10)</ref> 2 +q) , the polarity of z (2)  rs will not change. Therefore the linearity (quasi-linearity) between ϑ (2)  rs and z (2)  rs holds. Consequently, due to the linearity (quasi-linearity) between ϑ (2)  rs and z (2)  rs , the magnitude of ϑ (2)  rs mainly depends on the weighted sum of the cover image pixels located in the corresponding × n local area, rather than the weighted sum of those stego noises.</p><note type="other">.</note><p>Furthermore, in <ref type="bibr" target="#b10">(11)</ref> we can see there is a multiply factor to ϑ (2)  rs , (c p q + n p q). Since by average |c p q| is close to two orders of magnitude larger than |n p q| even with a high embedding rate, the impact of the neighboring cover image pixels on ∂ ∂W (1)   p q,rs J(W, b; x (h) , y (h) ) is further amplified. As a result, the influence of n p q, and the neighboring stego noise in the corresponding m × n local area, to ∂ ∂W (1)   p q,rs J(W, b; x (h) , y (h) ) becomes very weak. At last, since in a convolutional layer the weights are shared, all the partial derivatives with respect to a given shared weight should be accumulated: ∂ ∂W (1)    ∂W (1)   (r-m 2 +p)(s-n 2 +q),rs ,</p><formula xml:id="formula_17">1 ≤ p ≤ m, 1 ≤ q ≤ n. (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>The accumulation in <ref type="bibr" target="#b13">(14)</ref> again helps reduce the influence of outliers. As a result, it is safe for us to make a conclusion that the influence of stego noises to ∂ ∂W (1)   pq J(W, b; x (h) , y (h) ), 1 ≤ p ≤ m, 1 ≤ q ≤ n is weak in statistical sense. Consequently, gradient descent algorithm in the bottom convolutional layer will be always guided by the cover image contents rather than the stego noises. In other words, the optimization of the bottom convolutional layer in favor of the extraction of stego noises is hard to achieve with gradient descent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Conceptual architecture of one implementation of our proposed hybrid deep-learning framework with twenty-five 5 × 5 DCT basis patterns and three Q&amp;T combinations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Two types of subnet configurations. (a) Type1 subnet. (b) Type2 subnet.In the two figures "ABS" denotes the activation layer which outputs absolute values of the corresponding inputs, "BN" denotes the batch normalization layer, and "ReLU" denotes the layer with rectified-linear-unit activation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of testing accuracy of our proposed frameworks with four steganalytic models described in the literature, two hand-crafted JPEG domain rich models (DCTR and GFR), a selection-channel-aware variant of GFR (SCA-GFR) and a deep-learning steganalytic model proposed by Xu et al. [24]. (a) and (b) are the results for J-UNIWARD; (c) and (d) are for UERD; (e) and (f) are for UED. The experiments for (a), (c) and (e) were conducted on basic50K, while those for (b), (d) and (f) were conducted on basic500K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Testing accuracies versus training iterations for our proposed framework. The experimental results on basic50K, basic500K and basic5000K are reported. For brevity, only stego images with 0.4bpnzAC were included in the experiments. (a) is for J-UNIWARD steganography while (b) is for UERD steganography. In (a) and (b), The dash-dotted and the dashed reference lines denote the best testing accuracy of GFR and DCTR in basic500K, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The impact of altered blocking artifact on the performance of our proposed framework. Only stego images with 0.4bpnzAC were included in the experiments. All of the models were trained on basic500K training set and then tested on the corresponding testing set with central-cropped images. The legend "C" in parentheses denotes those tested on central-cropped images, while "L" in parentheses denotes those tested on the original basic500K testing set. For example, "J-UNIWARD (C)" means that the corresponding framework was trained and tested with J-UNIWARD stego images. It was trained on basic500K training set and then tested on the corresponding testing set with central-cropped images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Fig.7. Testing accuracies versus training iterations for our modified framework which takes 512×512 images as input. Only J-UNIWARD stego images with 0.4bpnzAC are included in the experiment. As in Fig.4, The dash-dotted reference line denotes the best testing accuracy of GFR, while the dashed reference line denotes the best testing accuracy of DCTR in the same testing dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison of testing accuracy of our proposed framework with GFR, DCTR, and Xu's model for J-UNIWARD on boss40K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Testing accuracies versus training iterations for our proposed framework. The models are trained on basic500K while tested on boss40K. Only J-UNIWARD stego images with 0.4bpnzAC are included in the experiment. The dash-dotted line and the dashed line denote the best testing accuracy of GFR and DCTR, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparison of testing accuracy of our proposed framework with DCTR, and Xu's model for J-UNIWARD on boss40K dataset, when all them were also trained on boss40K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. (a) Conceptual architecture of our proposed hybrid deep-learning framework incorporated with Xu's new model [36]. (b) Testing accuracies versus training iterations for Xu's new model<ref type="bibr" target="#b35">[36]</ref> and our hybrid framework incorporated with Xu's new model as shown in (a). The experimental setup is the same as in Fig.12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>m 2 +p)(s-n 2 +q),rs c (r-m 2 +p)(s-n 2 +q) accounts for the vast majority magnitude, with or without m 2 +p)(s-n 2 +q),rs n (r-m 2 +p)(s-n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>pq J(W, b; x (h) , y (h) ) = , b; x (h) , y (h) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I Effect</head><label>I</label><figDesc>of different Q&amp;T combinations, different hand-crafted convolutional kernels, and the presence of BN layers. Only J-UNIWARD stego images with 0.4bpnzAC were included in the experiments. The best results in every sub-table are underlined. Those hyper-parameters adopted in our proposed framework are marked in bold. a</figDesc><table><row><cell>Threshold &amp;</cell><cell></cell><cell>BN Layers</cell><cell></cell></row><row><cell>Quantization Steps</cell><cell>With BNs</cell><cell>Without BN1</cell><cell>Without BNs</cell></row><row><cell cols="3">Nine 3 × 3 DCT basis patterns</cell><cell></cell></row><row><cell>(4,1), (4,1.5), (4,2)</cell><cell>73.1%</cell><cell>70.6%</cell><cell>50.1%</cell></row><row><cell>(4,2), (4,2), (4,2)</cell><cell>72.8%</cell><cell>70.1%</cell><cell>50.0%</cell></row><row><cell>(4,1), (4,2), (4,4)</cell><cell>73.2%</cell><cell>71.0%</cell><cell>50.1%</cell></row><row><cell>(2,1), (4,2), (6,4)</cell><cell>71.2%</cell><cell>68.5%</cell><cell>50.0%</cell></row><row><cell>(6,1), (4,2), (2,4)</cell><cell>70.6%</cell><cell>67.8%</cell><cell>50.0%</cell></row><row><cell cols="3">Twenty-five 5 × 5 DCT basis patterns</cell><cell></cell></row><row><cell>(4,1), (4,1.5), (4,2)</cell><cell>74.3%</cell><cell>72.4%</cell><cell>50.1%</cell></row><row><cell>(4,2), (4,2), (4,2)</cell><cell>74.1%</cell><cell>72.4%</cell><cell>50.1%</cell></row><row><cell>(4,1)</cell><cell>70.8%</cell><cell>69.4%</cell><cell>50.1%</cell></row><row><cell>(4,1), (4,2)</cell><cell>72.5%</cell><cell>70.2%</cell><cell>50.1%</cell></row><row><cell>(4,1), (4,2), (4,4)</cell><cell>74.5%</cell><cell>72.5%</cell><cell>50.1%</cell></row><row><cell>(2,1), (4,2), (6,4)</cell><cell>73.6%</cell><cell>72.0%</cell><cell>50.1%</cell></row><row><cell>(6,1), (4,2), (2,4)</cell><cell>72.6%</cell><cell>71.7%</cell><cell>50.0%</cell></row><row><cell cols="3">Sixty-four 8 × 8 DCTR kernels [15]</cell><cell></cell></row><row><cell>(4,1), (4,1.5), (4,2)</cell><cell>72.5%</cell><cell>71.4%</cell><cell>50.0%</cell></row><row><cell>(4,2), (4,2), (4,2)</cell><cell>72.7%</cell><cell>71.2%</cell><cell>50.1%</cell></row><row><cell>(4,1), (4,2), (4,4)</cell><cell>72.9%</cell><cell>71.2%</cell><cell>50.1%</cell></row><row><cell>(2,1), (4,2), (6,4)</cell><cell>71.9%</cell><cell>70.2%</cell><cell>50.0%</cell></row><row><cell>(6,1), (4,2), (2,4)</cell><cell>71.5%</cell><cell>70.1%</cell><cell>50.1%</cell></row><row><cell cols="3">Thirty 5 × 5 PHARM kernels [16]</cell><cell></cell></row><row><cell>(4,1), (4,1.5), (4,2)</cell><cell>72.0%</cell><cell>70.8%</cell><cell>50.1%</cell></row><row><cell>(4,2), (4,2), (4,2)</cell><cell>70.6%</cell><cell>68.8%</cell><cell>50.0%</cell></row><row><cell>(4,1), (4,2), (4,4)</cell><cell>72.1%</cell><cell>70.8%</cell><cell>50.1%</cell></row><row><cell>(2,1), (4,2), (6,4)</cell><cell>70.3%</cell><cell>68.6%</cell><cell>50.0%</cell></row><row><cell>(6,1), (4,2), (2,4)</cell><cell>70.2%</cell><cell>68.7%</cell><cell>50.0%</cell></row><row><cell cols="4">Two hundred and fifty-six 8 × 8 GFR kernels [17]</cell></row><row><cell>(4,1), (4,1.5), (4,2)</cell><cell>74.1%</cell><cell>72.5%</cell><cell>50.1%</cell></row><row><cell>(4,2), (4,2), (4,2)</cell><cell>74.0%</cell><cell>72.6%</cell><cell>50.1%</cell></row><row><cell>(4,1), (4,2),</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TIFS.2017.2779446, IEEE Transactions on Information Forensics and Security</figDesc><table /><note><p>.6 epochs for bsic500K, and only 2.56 epochs for basic5000K. Therefore the full potential of our proposed framework with large-scale training datasets may not 1556-6013 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II Comparison</head><label>II</label><figDesc>of number of parameters and computational complexity for our proposed framework and Xu's new model. The computational complexity is measured in terms of FLOPs (floating-point operations).Fig.12. Testing accuracies versus training iterations for Xu's new model (with or without quantization Q = 1 enabled). The experiment was conducted on basic500K. Only J-UNIWARD stego images with 0.4bpnzAC were included in the experiment. We adopted the training settings in Xu's work so that the training of the models were stopped after 9 × 10 4 iterations. Polyak averaging was enabled, as suggested by Xu.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">ours with Type1 subnet</cell><cell></cell><cell cols="2">Xu's new model</cell><cell></cell></row><row><cell>Parameters</cell><cell></cell><cell></cell><cell>1.66 × 10 6</cell><cell></cell><cell></cell><cell cols="2">4.86 × 10 6</cell><cell></cell></row><row><cell>FLOPs</cell><cell></cell><cell></cell><cell>2.77 × 10 8</cell><cell></cell><cell></cell><cell cols="2">1.53 × 10 9</cell><cell></cell></row><row><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>78</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Quantization disabled Quantization with Q=1</cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Throughout this paper, the acronyms used for the steganographic and steganalytic algorithms are taken from the original papers. The corresponding full names are omitted for brevity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>There are various forms of J(W, b; x (h) , y (h) ) and R(W), and their definitions are omitted here, since irrelevant to the subject of this paper.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>• The detection accuracy of Xu's model<ref type="bibr" target="#b23">[24]</ref>, which is without Q&amp;T phase, is merely 54.7%. • The detection accuracy of our proposed framework as illustrated in Fig.1is 74.5%. • The detection accuracy of our proposed framework without Q&amp;T phase is 61.5%. • The detection accuracy of our proposed framework without quantization step in the Q&amp;T phase, is 57.6%, even worse than the above one without the entire Q&amp;T phase. • The detection accuracy of our proposed framework without truncation step in the Q&amp;T phase, is 65.4%.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>In practice, fake partial derivative can be back propagated to bottom layers when the actual partial derivative vanishes. For example, this trick is used in the Caffe implementation of ReLU layer (https://github.com/BVLC/caffe/blob/ master/src/caffe/layers/relu layer.cpp).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/tansq/hybrid deep learning framework for jpeg steganalysis</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank DDE Laboratory in SUNY Binghamton and Dr. Guanshuo Xu for sharing the source code of their steganalysis models online. We also appreciate Prof. Jiangqun Ni in Sun Yat-sen University, China for permission to use their implementation of UED and UERD in our experiments. Specifically, we are grateful to Dr. Paweł Korus at that time in Shenzhen University for valuable advice.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Practical methods for minimizing embedding impact in steganography</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE, Electronic Imaging, Security, Steganography, and Watermarking of Multimedia Contents IX</title>
		<meeting>SPIE, Electronic Imaging, Security, Steganography, and Watermarking of Multimedia Contents IX</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">6505</biblScope>
			<biblScope unit="page" from="502" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using high-dimensional image models to perform highly undetectable steganography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Information Hiding Workshop (IH&apos;2010)</title>
		<meeting>12th Information Hiding Workshop (IH&apos;2010)</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="161" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new cost function for spatial image steganography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 2014 International Conference on Image Processing</title>
		<meeting>IEEE 2014 International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="4206" to="4210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Content-adaptive steganography by minimizing statistical detectability</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cogranne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="221" to="234" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uniform embedding for efficient JPEG steganography</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="814" to="825" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using statistical image model for JPEG steganography: Uniform embedding revisited</title>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2669" to="2680" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal distortion function for steganography in an arbitrary domain</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Information Security</title>
		<imprint>
			<biblScope unit="volume">2014</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving steganographic security by synchronizing the selection channel</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Information Hiding and Multimedia Security Workshop</title>
		<meeting>3rd ACM Information Hiding and Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A strategy of clustering modification directions in spatial image steganography</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1905" to="1917" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Side-informed steganography with additive distortion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th IEEE International Workshop on Information Forensic and Security (WIFS&apos;2015</title>
		<meeting>7th IEEE International Workshop on Information Forensic and Security (WIFS&apos;2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich models for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="868" to="882" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Selection-channel-aware rich model for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cogranne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 6th IEEE International Workshop on Information Forensic and Security</title>
		<meeting>6th IEEE International Workshop on Information Forensic and Security</meeting>
		<imprint>
			<publisher>WIFS</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive steganalysis based on embedding probabilities of pixels</title>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="734" to="745" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ensemble classifiers for steganalysis of digital media</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kodovský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="432" to="444" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Low-complexity features for JPEG steganalysis using undecimated DCT</title>
		<author>
			<persName><forename type="first">V</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Phase-aware projection model for steganalysis of JPEG images</title>
		<idno>pp. 94 090T-1-94 090T-11</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T/SPIE Electronic Imaging 2015 (Media Watermarking, Security, and Forensics)</title>
		<meeting>IS&amp;T/SPIE Electronic Imaging 2015 (Media Watermarking, Security, and Forensics)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Steganalysis of adaptive JPEG steganography using 2d Gabor filters</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd ACM Information Hiding and Multimedia Security Workshop</title>
		<meeting>3rd ACM Information Hiding and Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="15" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Steganalysis features for content-adaptive JPEG steganography</title>
		<author>
			<persName><forename type="first">T</forename><surname>Denemark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1736" to="1746" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for steganalysis of digital images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA&apos;2014)</title>
		<meeting>Asia-Pacific Signal and Information essing Association Annual Summit and Conference (APSIPA&apos;2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for steganalysis via convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IS&amp;T/SPIE Electronic Imaging 2015 (Media Watermarking, Security, and Forensics)</title>
		<meeting>IS&amp;T/SPIE Electronic Imaging 2015 (Media Watermarking, Security, and Forensics)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="94" to="090J" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning and transferring representations for image steganalysis using convolutional neural network</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 2016 International Conference on Image Processing</title>
		<meeting>IEEE 2016 International Conference on Image essing</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2752" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning is a good steganalysis tool when embedding key is reused for different images, even if there is a cover source-mismatch</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pibre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jérôme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaumont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging (EI&apos;2016)</title>
		<meeting>Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging (EI&apos;2016)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02">February 2016</date>
			<biblScope unit="page" from="14" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structural design of convolutional neural networks for steganalysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="708" to="712" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ensemble of CNNs for steganalysis: An empirical study</title>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Information Hiding and Multimedia Security Workshop</title>
		<meeting>4th ACM Information Hiding and Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="103" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Histogram layer, moving convolutional neural networks towards feature-based steganalysis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging (EI&apos;2017)</title>
		<meeting>Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging (EI&apos;2017)<address><addrLine>Burlingame, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-02">29 Juanuary-2 February 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Break our steganographic system-the ins and outs of organizing BOSS</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B T</forename><surname>Filler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pevný</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 13th Information Hiding Workshop (IH&apos;2011</title>
		<meeting>13th Information Hiding Workshop (IH&apos;2011</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toss that BOSSbase, Alice</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cogranne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging (EI&apos;2016)</title>
		<meeting>Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging (EI&apos;2016)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-02">February 2016</date>
			<biblScope unit="page" from="14" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pre-training via fitting deep neural network to rich-model features extraction procedure and its effect on deep learning for steganalysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging</title>
		<meeting>Media Watermarking, Security, and Forensics, Part of IS&amp;T International Symposium on Electronic Imaging<address><addrLine>Burlingame, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-02">2017. 29 Juanuary-2 February 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">ImageNet</title>
		<ptr target="http://image-net.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CS231n: Convolutional Neural Networks for Visual Recognition</title>
		<ptr target="http://cs231n.github.io/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A deep learning approach to universal image manipulation detection using a new convolutional layer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Stamm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th ACM Information Hiding and Multimedia Security Workshop</title>
		<meeting>4th ACM Information Hiding and Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="5" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<ptr target="http://arxiv.org/abs/1408.5093" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network to detect J-UNIWARD</title>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACM Information Hiding and Multimedia Security Workshop</title>
		<meeting>5th ACM Information Hiding and Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">JPEG-phaseaware convolutional neural network for steganalysis of JPEG images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sedighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boroumand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th ACM Information Hiding and Multimedia Security Workshop</title>
		<meeting>5th ACM Information Hiding and Multimedia Security Workshop</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="75" to="84" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
