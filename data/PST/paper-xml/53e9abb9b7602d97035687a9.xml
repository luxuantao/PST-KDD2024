<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dominant Sets and Pairwise Clustering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Massimiliano</forename><surname>Pavan</surname></persName>
							<email>pavan@dsi.unive.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<address>
									<addrLine>Universita`Ca&apos; Foscari di Venezia, Via Torino 155</addrLine>
									<postCode>30172</postCode>
									<settlement>Venezia Mestre</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
							<email>pelillo@dsi.unive.it</email>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<address>
									<addrLine>Universita`Ca&apos; Foscari di Venezia, Via Torino 155</addrLine>
									<postCode>30172</postCode>
									<settlement>Venezia Mestre</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dominant Sets and Pairwise Clustering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8CD3DB9DAC0D690508A4948B1BA0CE38</idno>
					<note type="submission">received 16 Nov. 2005; revised 10 Apr. 2006; accepted 6 June 2006; published online 13 Nov. 2006.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Clustering</term>
					<term>quadratic optimization</term>
					<term>evolutionary game dynamics</term>
					<term>image segmentation</term>
					<term>perceptual organization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We develop a new graph-theoretic approach for pairwise data clustering which is motivated by the analogies between the intuitive concept of a cluster and that of a dominant set of vertices, a notion introduced here which generalizes that of a maximal complete subgraph to edge-weighted graphs. We establish a correspondence between dominant sets and the extrema of a quadratic form over the standard simplex, thereby allowing the use of straightforward and easily implementable continuous optimization techniques from evolutionary game theory. Numerical examples on various point-set and image segmentation problems confirm the potential of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>PAIRWISE or proximity-based, data clustering techniques are gaining increasing popularity over traditional central grouping techniques, which are centered around the notion of "feature" (see, e.g., <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b3">[4]</ref>). In many real-world applications, in fact, a feasible feature-based description of objects might be difficult to obtain or inefficient for learning purposes while, on the other hand, it is often possible to obtain a measure of the (dis)similarity between objects. This is the case, for example, when features consist of both continuous and categorical variables or when the objects to be classified are represented in terms of graphs or structural representations.</p><p>A classical approach to pairwise clustering uses concepts and algorithms from graph theory <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Indeed, it is natural to map the data to be clustered to the nodes of a weighted graph (the so-called similarity graph), with edge weights representing similarity relations. These methods are of significant interest since they cast clustering as pure graph-theoretic problems for which a solid theory and powerful algorithms have been developed. As pointed out in <ref type="bibr" target="#b1">[2]</ref>, these methods can produce highly intricate clusters, but they rarely optimize an easily specified global cost function. Graph-theoretic algorithms basically consist of searching for certain combinatorial structures in the similarity graph, such as a minimum spanning tree <ref type="bibr" target="#b22">[23]</ref> or a minimum cut <ref type="bibr" target="#b21">[22]</ref> and, among these methods, a well-known approach (the "complete-link" algorithm <ref type="bibr" target="#b7">[8]</ref>) reduces to a search for a complete subgraph, namely, a clique. 1 Indeed, some authors <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref> argue that the maximal clique is the strictest definition of a cluster. Unfortunately, while the minimum spanning tree and the minimum cut (with variations thereof) are notions that are explicitly defined on edge-weighted graphs, the concept of a maximal clique is defined on unweighted graphs, and it is not clear how to generalize it to the edge-weighted case. As a consequence, maximal-clique-based clustering algorithms typically work on unweighted graphs derived from the similarity graph by means of some threshold operation <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Although such threshold operations can be used to generate a hierarchy of clusters displayed to a user in the form of a dendogram <ref type="bibr" target="#b7">[8]</ref>, in tasks involving a large number of data items, such as image segmentation, this approach is infeasible. It is therefore of considerable interest to extend the notion of a maximal clique to edgeweighted graphs, and this is precisely what we do in this work, which appeared in a preliminary form in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Motivated by the previous arguments, we propose a new approach for pairwise data clustering which is centered around a novel graph-theoretic concept (that of a dominant set) arising from the study of a continuous formulation of the maximum clique problem originally due to Motzkin and Straus <ref type="bibr" target="#b10">[11]</ref>. Ours is a nontrivial generalization of the notion of a maximal clique in the context of edge-weighted graphs since, in the unweighted case, dominant sets turn out to be equivalent to (strictly) maximal cliques. Formal properties, intuition, and empirical findings make dominant sets reasonable candidates for a new formal definition of a cluster in the context of edge-weighted graphs. A nice feature of our approach is that it naturally provides a principled measure of a cluster's cohesiveness as well as a measure of a vertex participation to each group.</p><p>We establish an exact correspondence between dominant sets and local extrema of a (continuous) quadratic form over the standard simplex. Interestingly, well-known spectral approaches lead to similar (though intrinsically different) quadratic optimization problems <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Computationally, this allows us to find dominant sets (clusters) using straightforward continuous optimization techniques known as replicator dynamics, a class of dynamical systems arising in evolutionary game theory <ref type="bibr" target="#b20">[21]</ref>. Such systems can be coded in a few lines of any high-level programming language, can easily be implemented in a parallel network of locally interacting computational units, and offer the advantage of biological plausibility. Numerical examples on both point data sets as well as image segmentation problems confirm the effectiveness of the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">GRAPH-THEORETIC DEFINITION OF A CLUSTER</head><p>We represent the data to be clustered as an undirected edgeweighted (similarity) graph with no self-loops G ¼ ðV ; E; wÞ, where V ¼ f1; . . . ; ng is the vertex set, E V Â V is the edge set, and w : E ! IR Ã þ is the (positive) weight function. Vertices in G correspond to data points, edges represent neighborhood relationships, and edge-weights reflect similarity between pairs of linked vertices. As is customary, we represent the graph G with the corresponding weighted adjacency (or similarity) matrix, which is the n Â n symmetric matrix A ¼ ða ij Þ, where a ij ¼ wði; jÞ if ði; jÞ 2 E, and a ij ¼ 0 otherwise. Clearly, since there are no selfloops, all the elements on the main diagonal of A are zero.</p><p>A common informal definition states that "a cluster is a set of entities which are alike, and entities from different clusters are not alike" <ref type="bibr">[8, p. 1]</ref>. Hence, a cluster should satisfy two fundamental conditions: 1) it should have high internal homogeneity and 2) there should be high inhomogeneity between the entities in the cluster and those outside. When the entities are represented as an edgeweighted graph, these two conditions amount to saying that the weights on the edges within a cluster should be large, and those on the edges connecting the cluster nodes to the external ones should be small. Clearly, it is not at all obvious what "large" and "small" precisely mean.</p><p>To give our formal definition of a cluster, we start with the intuitive idea that the assignment of the edge-weights induces, in some way to be described, an assignment of weights on the vertices. This perspective gives us a chance to analyze the assignment of the edge-weights in a fruitful way. Let S V be a nonempty subset of vertices and i 2 S. The (average) weighted degree of i with regard to S is defined as:</p><formula xml:id="formula_0">awdeg S ðiÞ ¼ 1 jSj X j2S a ij :<label>ð1Þ</label></formula><p>1. Recall that a subset of vertices of a graph is said to be a clique if all its nodes are mutually adjacent; a maximal clique is one which is not contained in any larger clique, whereas a maximum clique is one having largest cardinality.</p><p>Observe that awdeg fig ðiÞ ¼ 0 for any i 2 V . Moreover, if j = 2 S, we define:</p><formula xml:id="formula_1">S ði; jÞ ¼ a ij À awdeg S ðiÞ :<label>ð2Þ</label></formula><p>Note that fig ði; jÞ ¼ a ij , for all i; j 2 V with i 6 ¼ j. Intuitively, S ði; jÞ measures the relative similarity between nodes j and i, with respect to the average similarity between node i and its neighbors in S. Note that S ði; jÞ can be either positive or negative.</p><p>We are now in a position to formalize the notion of "induction" of node-weights, which is captured by the following recursive definition: Definition 1. Let S V be a nonempty subset of vertices and i 2 S. The weight of i with regard to S is</p><formula xml:id="formula_2">w S ðiÞ ¼ 1; i f S j j ¼ 1 P j2Snfig Snfig ðj; iÞw Snfig ðjÞ; otherwise: (<label>ð3Þ</label></formula><p>Moreover, the total weight of S is defined to be:</p><formula xml:id="formula_3">WðSÞ ¼ P i2S w S ðiÞ.</formula><p>Note that w fi;jg ðiÞ ¼ w fi;jg ðjÞ ¼ a ij , for all i; j 2 V ði 6 ¼ jÞ. Also, observe that w S ðiÞ is calculated simply as a function of the weights on the edges of the subgraph induced by S. For example, in Fig. <ref type="figure" target="#fig_0">1a</ref>, we have: w f1;2;3g ð3Þ ¼ f1;2g ð1; 3Þw f1;2g ð1Þþ f1;2g ð2; 3Þw f1;2g ð2Þ ¼ 18.</p><p>Similarly, we obtain w f1;2;3g ð1Þ ¼ 10 and w f1;2;3g ð2Þ ¼ 16, which yield Wðf1; 2; 3gÞ ¼ 44.</p><p>Intuitively, w S ðiÞ gives us a measure of the overall (relative) similarity between vertex i and the vertices of S n fig with respect to the overall similarity among the vertices in S n fig. For example, for the graph in Fig. <ref type="figure" target="#fig_0">1b</ref>, we have w f1;2;3;4g ð1Þ &lt; 0, while for that in Fig. <ref type="figure" target="#fig_0">1c</ref> we have w f1;2;3;4g ð1Þ &gt; 0. This can be explained by considering that in Fig. <ref type="figure" target="#fig_0">1b</ref>, vertex 1 is loosely coupled with the remaining vertices, which on their own form a tightly coupled group, whereas in Fig. <ref type="figure" target="#fig_0">1c</ref>, exactly the opposite is true. Further, referring again to the graph in Fig. <ref type="figure" target="#fig_0">1a</ref>, observe that the edges incident to vertex 1 are the lightest ones, the heaviest ones are incident to vertex 3, and those incident to 2 are the lightest as well as the heaviest ones. This induces a sort of natural ranking among the vertices of the graph, which is indeed captured by the notions introduced above: In fact, we have w f1;2;3g ð1Þ &lt; w f1;2;3g ð2Þ &lt; w f1;2;3g ð3Þ.</p><p>The following definition represents our formalization of the concept of a cluster in an edge-weighted graph. Definition 2. A nonempty subset of vertices S V such that WðT Þ &gt; 0 for any nonempty T S, is said to be dominant if:</p><formula xml:id="formula_4">1. w S ðiÞ &gt; 0, for all i 2 S, 2. w S[fig ðiÞ &lt; 0, for all i = 2 S.</formula><p>The two conditions of the above definition correspond to the two main properties of a cluster: the first regards internal homogeneity, whereas the second regards external inhomogeneity. The condition WðT Þ &gt; 0 for any nonempty T S is a technicality explained in some detail in <ref type="bibr" target="#b11">[12]</ref>.</p><p>To illustrate, in the graph of Fig. <ref type="figure" target="#fig_0">1d</ref>, the subset of vertices f1; 2; 3g is dominant, and this may be explained by observing that the edge weights "internal" to that set (60, 70, and 90) are larger than those between internal and external vertices (which are between 5 and 25).</p><p>As the example suggests, the main property of a dominant set is that the overall similarity among internal nodes is higher than that between external and internal nodes, and this fact is the motivation of considering a dominant set as a cluster of nodes. Note that, by their own definition, dominant sets are expected to capture highly compact structures. Indeed, it is simple to show that our definition of a dominant set is equivalent to that of a (strictly) maximal clique when applied to unweighted graphs <ref type="bibr" target="#b11">[12]</ref>. This means that we have the same concept in the limit of uniform similarity of all objects. This is a further motivation to consider dominant sets as clusters since maximal cliques are a classic formalization of the notion of a cluster <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>Before concluding this section, we provide a useful characterization of the notions introduced above in terms of determinants. To this end, we need some new notations. If S V , we denote by A S the submatrix of A formed by the rows and the columns indexed by the elements of S. Additionally, we define the matrix B S as:</p><formula xml:id="formula_5">B S ¼ 0 e T e A S ;</formula><p>where e is a vector of appropriate length consisting of unit entries, and "T" denotes transposition. Assuming S ¼ fi 1 ; . . . ; i m g with i 1 &lt; Á Á Á &lt; i m , the matrix j B S is defined to be:</p><formula xml:id="formula_6">j B S ¼ 0 e T e A 1 S Á Á Á A jÀ1 S 0 A jþ1 S Á Á Á A m S ;</formula><p>where A i S denotes the ith column of A S . Lemma 1. Let S ¼ fi 1 ; . . . ; i m g V be a nonempty subset of vertices and, without loss of generality, assume i 1 &lt; Á Á Á &lt; i m . Then, we have:</p><formula xml:id="formula_7">w S ði h Þ ¼ ðÀ1Þ m det h B S À Á ;<label>ð4Þ</label></formula><p>for any i h 2 S. Moreover,</p><formula xml:id="formula_8">WðSÞ ¼ ðÀ1Þ m detðB S Þ:<label>ð5Þ</label></formula><p>Proof. Proceeds by induction and exploits elementary properties of the determinant (see <ref type="bibr" target="#b11">[12]</ref>, for details). t u An alternative, useful way of computing the w S ðiÞs (when jSj &gt; 1) is given by the formula:</p><formula xml:id="formula_9">w S ðiÞ ¼ X j2Snfig ða ij À a hj Þw Snfig ðjÞ;<label>ð6Þ</label></formula><p>where h is an arbitrary element of S n fig (it can be shown <ref type="bibr" target="#b11">[12]</ref> that the sum in <ref type="bibr" target="#b5">(6)</ref> does not depend upon the choice of h).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FROM DOMINANT SETS TO LOCAL OPTIMA</head><p>Consider a similarity graph G ¼ ðV ; E; wÞ with n vertices, and its weighted adjacency matrix A. A common way to represent a cluster of vertices (see, e.g., <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>) is to associate a (realvalued) n-dimensional vector to it, where its components express the participation of nodes in the cluster: If a component has a small value, then the corresponding node is weakly associated with the cluster, whereas if it has a large value, the node is strongly associated with the cluster. Components corresponding to nodes not participating in the cluster are zero. As pointed out before, a good cluster is one where elements that are strongly associated with it also have large values connecting one another in the similarity matrix. Hence, a natural way of defining the cohesiveness of a cluster is given by the following quadratic form:</p><formula xml:id="formula_10">fðxÞ ¼ x T Ax<label>ð7Þ</label></formula><p>and this allows us to formulate the (pairwise) clustering problem as the problem of finding a vector x that maximizes f. However, note that the objective function is useless without some normalization of the components of x and, thus, we impose to it simplex (or probability) constraints. This yields the following standard quadratic program, which is a generalization of the so-called Motzkin-Straus program <ref type="bibr" target="#b10">[11]</ref>:</p><formula xml:id="formula_11">maximize fðxÞ subject to x 2 Á;<label>ð8Þ</label></formula><p>where</p><formula xml:id="formula_12">Á ¼ fx 2 IR n : x ! 0 and e T x ¼ 1g</formula><p>is the standard simplex of lR n . Thus, within this continuous formulation, a maximally cohesive cluster corresponds to a (local) solution of program <ref type="bibr" target="#b7">(8)</ref>. It is the purpose of this section to show that this notion of a cluster is intimately related to dominant sets, and that the two notions are indeed two sides of the same coin. Given a vector x 2 Á, the support of x is defined as the set of indices corresponding to its nonzero components, that is, ðxÞ ¼ fi 2 V : x i 6 ¼ 0g. A point x 2 Á satisfies the Karush-Kuhn-Tucker (KKT) conditions for problem <ref type="bibr" target="#b7">(8)</ref>, i.e., the first-order necessary conditions for local optimality <ref type="bibr" target="#b8">[9]</ref>, if there exist n þ 1 real constants (Lagrange multipliers) 1 ; . . . ; n and , with i ! 0 for all i ¼ 1 . . . n, such that:</p><formula xml:id="formula_13">ðAxÞ i À þ i ¼ 0<label>ð9Þ</label></formula><p>for all i ¼ 1 . . . n, and P n i¼1 x i i ¼ 0. Note that, since both x i and i are nonnegative for all i ¼ 1 . . . n, the latter condition is equivalent to saying that i 2 ðxÞ implies i ¼ 0. Hence, the KKT conditions can be rewritten as:</p><formula xml:id="formula_14">ðAxÞ i ¼ ; if i 2 ðxÞ ; otherwise &amp;<label>ð10Þ</label></formula><p>for some real constant (indeed, it is immediate to see that ¼ x T Ax). A point x 2 Á satisfying (10) will be called a KKT point throughout.</p><p>With the notations introduced at the end of the previous section, note that the KKT equality conditions in <ref type="bibr" target="#b9">(10)</ref> amount to saying that there exists a real number such that:</p><formula xml:id="formula_15">B ð; x i1 ; . . . ; x im Þ T ¼ ð1; 0; . . . ; 0Þ T ;<label>ð11Þ</label></formula><p>where ¼ ðxÞ ¼ fi 1 ; . . . ; i m g with i 1 &lt; Á Á Á &lt; i m .</p><p>Definition 3. We say that a nonempty subset of vertices S admits weighted characteristic vector x S 2 Á if it has nonnull total weight WðSÞ, in which case, we set:</p><formula xml:id="formula_16">x S i ¼ wS<label>ðiÞ</label></formula><formula xml:id="formula_17">WðSÞ ; if i 2 S 0; otherwise: &amp;<label>ð12Þ</label></formula><p>Note that, by definition, dominant sets always admit a weighted characteristic vector.</p><p>The next two results establish useful connections between KKT points of program ( <ref type="formula" target="#formula_11">8</ref>) and weighted characteristic vectors. Lemma 2. Let ¼ ðxÞ be the support of a vector x 2 Á which admits weighted characteristic vector x . Then, x satisfies the KKT equality conditions in <ref type="bibr" target="#b9">(10)</ref> if and only if x ¼ x . Moreover, in this case, we have:</p><formula xml:id="formula_18">w [fjg ðjÞ WðÞ ¼ ðAxÞ j À ðAxÞ i ¼ À j<label>ð13Þ</label></formula><p>for all i 2 and j = 2 , where the j s are the (nonnegative) Lagrange multipliers of program <ref type="bibr" target="#b7">(8)</ref>.</p><p>Proof. Note that conditions <ref type="bibr" target="#b10">(11)</ref>, which are equivalent to the KKT equality conditions in <ref type="bibr" target="#b9">(10)</ref>, can be regarded as a system of linear equations in the unknowns and x i s (i 2 ). From Lemma 1, the system has a unique solution since det B ð Þ 6 ¼ 0. Hence, supposing ¼ fi 1 ; . . . ; i m g and, without loss of generality, i 1 &lt; . . . &lt; i m , from Cramer's rule and Lemma 1, we have:</p><formula xml:id="formula_19">x ih ¼ det h B À Á detðB Þ ¼ ðÀ1Þ m w ði h Þ ðÀ1Þ m WðÞ ¼ w ði h Þ<label>WðÞ</label></formula><formula xml:id="formula_20">for any 1 h m. Therefore, x ¼ x .</formula><p>The fact that ðAxÞ j À ðAxÞ i ¼ À j , for i 2 and j = 2 , follows immediately from <ref type="bibr" target="#b8">(9)</ref>. Finally, using <ref type="bibr" target="#b5">(6)</ref>, we obtain:</p><formula xml:id="formula_21">w [fjg ðjÞ WðÞ ¼ P h2 ða jh À a ih Þw ðhÞ WðÞ ¼ X h2 a jh x h À X h2 a ih x h ¼ ðAx Þ j À ðAx Þ i ;</formula><p>which concludes the proof since x ¼ x . t u Proposition 1. Let x 2 Á be a vector whose support ¼ ðxÞ has positive total weight WðÞ and, hence, admitting weighted characteristic vector x . Then, x is a KKT point for <ref type="bibr" target="#b7">(8)</ref> if and only if the following conditions hold:</p><formula xml:id="formula_22">1. x ¼ x , 2. w [fjg<label>ðjÞ</label></formula><p>0, for all j = 2 .</p><p>Proof. Vector x satisfies the KKT conditions <ref type="bibr" target="#b9">(10)</ref> if and only if x ¼ x (cf. Lemma 2) and ðAxÞ j ðAxÞ i for any j = 2 and i 2 , but from (13) the latter condition amounts to saying that w [fjg ðjÞ 0, since WðÞ &gt; 0.</p><p>t u The following theorem, which is the main result of this section, establishes an interesting connection between dominant sets and local solutions of program <ref type="bibr" target="#b7">(8)</ref>.</p><p>Theorem 1. If S is a dominant subset of vertices, then its weighted characteristic vector x S is a strict local solution of program <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_23">Conversely, if x Ã is a strict local solution of program (8) then its support ¼ ðx Ã Þ is a dominant set, provided that w [fig ðiÞ 6 ¼ 0 for all i = 2 .</formula><p>Proof. First, we note that the well-known bordered Hessian test from nonlinear programming <ref type="bibr" target="#b8">[9]</ref> can be reformulated in the following way (see <ref type="bibr" target="#b11">[12]</ref> for details): Given a subset of m vertices Q V , A Q is negative definite in the subspace fy 2 lR m : P m i¼1 y i ¼ 0g if and only if WðT Þ &gt; 0 for any nonempty subset T Q. Now, let S be a dominant set. Then, from Proposition 1, it follows that x S is a KKT point for <ref type="bibr" target="#b7">(8)</ref>. Moreover, by Lemma 2, we have that the jth nonnegative Lagrange multiplier j (j = 2 S) is positive if and only if w S[fjg ðjÞ &lt; 0. Therefore, the secondorder sufficient conditions for local optimality <ref type="bibr" target="#b8">[9]</ref>, together with the bordered Hessian test, imply that x S is a strict local solution for program <ref type="bibr" target="#b7">(8)</ref>.</p><p>Conversely, suppose that x Ã is a strict local solution of (8), and let ¼ ðx Ã Þ be its support. After some algebra, it follows that the submatrix A is negative definite in the subspace fy 2 lR m : P m i¼1 y i ¼ 0g, where m ¼ jj. Hence, from the bordered Hessian test, we have WðT Þ &gt; 0 for any nonempty subset T .</p><p>Moreover, we have w S ðiÞ &gt; 0 for all i 2 S. This follows directly from Lemma 2 (in fact, x Ã is a KKT point) and the definition of weighted characteristic vector. Finally, Proposition 1 states that x Ã ¼ x and w [fjg ðjÞ 0, for all j = 2 . Therefore, the fact that is dominant follows trivially from the hypotheses. 2 is a technicality due to the presence of "spurious" solutions in (8), namely, solutions whose support does not admit a weighted characteristic vector. However, this corresponds to a nongeneric situation and, thus, in the following, we shall ignore it.</p><p>By virtue of Theorem 1, dominant sets are in correspondence with (strict local) solutions of the quadratic program shown in <ref type="bibr" target="#b7">(8)</ref>. This is interesting because, recently, other quadratic programming formulations have been proposed for clustering and segmentation, though motivated by the different idea of finding cuts in a similarity graph <ref type="bibr" target="#b19">[20]</ref> or computing eigenvalues and eigenvectors of the weighted adjacency matrix <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>. In particular, note that we use the same objective function as Sarkar and Boyer <ref type="bibr" target="#b18">[19]</ref> (see also <ref type="bibr" target="#b16">[17]</ref>), which provides a measure of the cohesiveness of a cluster. However, we differ from them in the feasible region, namely, we look for solutions in the standard simplex, whereas they consider the sphere. This is important as the components of the weighted characteristic vectors give us a measure of the participation of the corresponding vertices in the cluster. Hence, in contrast to Sarkar and Boyer's approach, we automatically avoid the nuisance of dealing with negative components, which are meaningless. Note also that no exact combinatorial interpretation is offered for Sarkar and Boyer's "eigenclusters."</p><p>The quadratic program we have considered in this section was first analyzed by Motzkin and Straus <ref type="bibr" target="#b10">[11]</ref> limited to the case of unweighted graphs, where the matrix A in ( <ref type="formula" target="#formula_11">8</ref>) is a standard 0/1 adjacency matrix. In this case, it turns out that there exists a correspondence between local/global solutions of the program and maximal/maximum cliques of the (unweighted) graph <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Since, in unweighted graphs, dominant sets turn out to be equivalent to (strictly) maximal cliques <ref type="bibr" target="#b11">[12]</ref>, Theorem 1 can be considered as a step toward generalizing the Motzkin-Straus theorem to edge-weighted graphs (see <ref type="bibr" target="#b4">[5]</ref> for a generalization involving vertex-weighted graphs).</p><p>A straightforward way to find (local) solutions of the program shown in ( <ref type="formula" target="#formula_11">8</ref>) is given by the so-called replicator dynamics, a class of continuous and discrete-time dynamical systems arising in evolutionary game theory <ref type="bibr" target="#b20">[21]</ref> which are also intimately related to relaxation labeling processes. In our simulations, we used the following model:</p><formula xml:id="formula_24">x i ðt þ 1Þ ¼ x i ðtÞ ðAxÞ i xðtÞ T AxðtÞ<label>ð14Þ</label></formula><p>for i ¼ 1 . . . n, which corresponds to the discrete-time version of first-order replicator equations (see, e.g., <ref type="bibr" target="#b20">[21]</ref>). It is readily seen that the simplex Á is invariant under these dynamics, which means that every trajectory starting in Á will remain in Á for all future times.</p><p>Moreover, it can be proven that, since A is symmetric, the objective function fðxÞ ¼ x T Ax is strictly increasing along any nonconstant trajectory of ( <ref type="formula" target="#formula_24">14</ref>), and its asymptotically stable points are in one-toone correspondence to strict local solutions of ( <ref type="formula" target="#formula_11">8</ref>) <ref type="bibr" target="#b20">[21]</ref>. These, in turn, correspond to dominant sets for the similarity matrix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NUMERICAL EXAMPLES</head><p>A simple, yet effective strategy to obtain a hard partition of the input data into coherent groups, is as follows: 1) Find a dominant set (i.e., a cluster), 2) remove the vertices in the cluster from the similarity graph, and 3) reiterate on the remaining vertices. Thus, the algorithm iteratively peels off clusters (dominant sets) and, at each iteration, it determines one by finding a local solution of the quadratic program shown in <ref type="bibr" target="#b7">(8)</ref>. Clearly, as we proceed, the graphs wherein dominant sets are looked for become smaller and smaller, and this makes the algorithm particularly efficient. In principle, we should keep peeling off clusters until all data have been covered, but in applications involving large and noisy data sets, such as, for example, image segmentation, this makes little sense. In these cases, a better strategy is to stop the algorithm prematurely when most of the data points have been classified and then assign the unprocessed ones to the "nearest" cluster according to some distance criterion. Typically, these unassigned items are few noisy and peripheral points that cannot be naturally grouped into the major clusters.</p><p>In order to understand the behavior of this peeling strategy and to evaluate its robustness against perturbations of the similarity values, we conducted the following experiment on the two-Gaussian data set shown in Fig. <ref type="figure" target="#fig_2">2a</ref>. Each point in the original set was randomly perturbed by adding a normally distributed noise, with increasing values of the standard deviation . For each value of , 100 different data sets were constructed, on each of which we ran our peeling clustering technique. For the sake of comparison, we also ran on the same data K-means <ref type="bibr" target="#b7">[8]</ref>, Normalized Cut (NCut) <ref type="bibr" target="#b19">[20]</ref>, and DBSCAN <ref type="bibr" target="#b9">[10]</ref>. Fig. <ref type="figure" target="#fig_2">2</ref> shows the classification accuracy of the four algorithms as a function of noise. As can be seen, K-means and our peeling strategy exhibit essentially the same robust behavior, whereas NCut and DBSCAN turn out to be more sensitive to noise.</p><p>In many computer vision problems, one would like to extract structure from cluttered background. This is the case, for example, with figure/ground separation and perceptual grouping. In such cases, standard algorithms such as K-means or graph partitioning techniques are not expected to work well, due to their insisting on partitioning all the input data and, hence, the unstructured clutter points too, into coherent groups. Our approach, on the contrary, appears to be particularly suited for such applications since it allows one to extract as many clusters as desired, while leaving the remaining points (namely, the clutter) ungrouped.</p><p>To illustrate this point, consider the data set shown in Fig. <ref type="figure" target="#fig_3">3a</ref>, containing a dense central cluster of random points (the "figure"), surrounded by equally distributed clutter points (the "background"). As expected, on these data, both K-means and NCut failed as they both split the central group in two pieces, whereas our peeling algorithm, as well as DBSCAN, produced accurate results.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Four example edge-weighted graphs.</figDesc><graphic coords="2,87.70,69.17,391.07,89.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t u The condition that w [fig ðiÞ 6 ¼ 0 for all i =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Evaluating the robustness of the peeling strategy against random perturbations. (a) Original 1,000-point data set obtained from a mixture of two Gaussian distributions. (b), (c), (d), and (e) Classification accuracy obtained with (b) K-means, (c) NCut, (d) DBSCAN, and (e) dominant sets, as a function of noise.</figDesc><graphic coords="4,49.04,69.17,468.40,119.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. An example of separating structure from background clutter. (a) Original 400-point data set (100 points for the central, dense group, and 300 for the background). (b), (c), (d), and (e) Precision curves obtained with (b) K-means, (c) NCut, (d) DBSCAN, and (e) dominant sets, as a function of clutter.</figDesc><graphic coords="5,47.11,210.16,472.25,104.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A clustering example with structurally different groups. (a) Original 1,000-point data set. (b), (c), (d), and (e) Results obtained with (b) K-mean, (c) NCut, (d) DBSCAN, and (e) dominant sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Image segmentation results. Top row: Original images (from left to right: three brightness and two color images). Middle row: Segmentations obtained with dominant sets. Bottom row: Segmentations obtained with NCut.</figDesc><graphic coords="6,79.31,69.17,407.91,255.17" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We used Shi's implementation, which can be downloaded from: http://www.hid.ri.cmu.edu/Hid/software_ncutPublic.html.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In order to study the robustness of the approach against random noise in the background, we let the level of clutter vary, starting from 100 to 1,000 points. Note that DBSCAN automatically distinguishes between noise (i.e., ground) and nonnoise (i.e., figure) points, whereas, for K-means and NCut, the decision as to label a given group as figure and the other as ground was taken using a simple majority rule (using the ground truth). As for our approach, we simply declared figure the (first) dominant set found and background the remaining points. For each noise level, we calculated the precision of the four algorithms, as the percentage of true figure points among the number of points classified as figure. Fig. <ref type="figure">3</ref> shows the behavior of the precision curves as a function of noise for the four algorithms. As it turns out, ours substantially outperforms both K-means and NCut, and performs slightly better than DBSCAN.</p><p>A third experiment was done on the 1,000-point data set shown in Fig. <ref type="figure">4a</ref>, which has become a standard benchmark for pairwise clustering techniques. The main feature of this data set is that it contains two structurally different (noisy) clusters, one being compact, and the other having an elongated structure. Here, K-means produces totally wrong results, as shown in Fig. <ref type="figure">4b</ref>. A direct application of our algorithm to the similarity matrix whose entries are taken to be inversely proportional to the Euclidean distances would yield an oversegmentation of the external ring (the central disc being separated correctly). Indeed, this is not surprising due to the intrinsic feature of dominant sets of capturing compact groups. To avoid this phenomenon, we used the pathbased (dis)similarity measure recently proposed by Fischer and Buhmann in <ref type="bibr" target="#b2">[3]</ref>, which stresses connectedness of data points via mediating elements. The results obtained are shown in Fig. <ref type="figure">4e</ref> and are similar to those produced by NCut and DBSCAN: All three algorithms were able to separate correctly the data into two classes.</p><p>As for the computational time, we remark that in the three series of experiments all algorithms (except K-means which was by far the fastest) typically took a few seconds to converge, with our algorithm being two to three time faster than both NCut and DBSCAN.</p><p>Finally, we apply our clustering framework to the image segmentation problem. The image to be segmented is represented as an edge-weighted undirected graph, where vertices correspond to individual pixels and the edge-weights reflect the "similarity" between pairs of vertices. In our experiments, the similarity between pixels i and j was measured by wði; jÞ</p><p>where is a positive real number which affects the decreasing rate of w, and FðiÞ is defined as the intensity value at node i, normalized to a real number in the interval [0, 1], for segmenting brightness images, and as FðiÞ ¼ ½v; vs sinðhÞ; vs cosðhÞðiÞ, where h, s, and v are the HSV values of pixel i, for color segmentation.</p><p>For the sake of comparison, we also ran NCut on the same images. 2 The results presented here were obtained after a careful tuning of its parameters. To get cleaner segmentations for both algorithms, connected components whose area was around 0.1 percent of that of the whole image were incorporated into larger adjacent regions using a straightforward spatial proximity criterion. We remark that only 2-3 percent of the pixels in the whole images were involved in this operation, which means that the overall quality of the segmentations cannot be credited to this postprocessing. Fig. <ref type="figure">5</ref> shows the results obtained with our segmentation algorithm and NCut on various natural brightness and color images (typical image size is 90 Â 120 pixels). On average, our algorithm (and NCut too) took only a few seconds to return a segmentation on a machine equipped with a 2 GHz Intel Pentium IV. As can be seen, the dominant-set segmentations are substantially cleaner than those obtained with NCut, which typically tends to produce oversegmented results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have introduced the notion of a dominant set of vertices in an edge-weighted graph and have shown how this concept can be relevant in pairwise data clustering. We have established a connection between the (combinatorial) problem of finding dominant sets and (continuous) quadratic programming, and this allows the use of straightforward dynamics from evolutionary game theory to determine them. Experimentally, we have demonstrated the potential of our approach on various point-set and image segmentation examples. Extensions of the approach presented here involving hierarchical data partitioning and outof-sample extensions of dominant-set clusters can be found in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, respectively. We are currently working toward providing a massive experimental evaluation of our approach on (highresolution) image and spatio-temporal video segmentation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Analysis of Some Graph Theoretical Clustering Techniques</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Auguston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Minker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="571" to="588" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Pattern Classification. J. Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Path-Based Clustering for Grouping Smooth Curves and Texture Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="518" />
			<date type="published" when="2003-04">Apr. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-Organization in Vision: Stochastic Clustering for Image Segmentation, Perceptual Grouping, and Image Database Organization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gdalyahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1053" to="1074" />
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Continuous Characterizations of the Maximum Clique Problem</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Hearn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Pardalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Ramana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Operations Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="754" to="768" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic Clustering of Index Terms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Gotlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="493" to="513" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pairwise Data Clustering by Deterministic Annealing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="1997-01">Jan. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Linear and Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Luenberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Addison-Wesley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Int&apos;l Conf. Knowledge Discovery and Data Mining</title>
		<meeting>Second Int&apos;l Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maxima for Graphs and a New Proof of a Theorem of Tura ´n</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Motzkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Straus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian J. Math</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="533" to="540" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A New Graph-Theoretic Approach to Clustering, with Applications to Computer Vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Universita `Ca&apos; Foscari di</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Venezia, Italy</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A New Graph-Theoretic Approach to Clustering and Segmentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dominant Sets and Hierarhical Clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="362" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Out-of-Sample Extension of Dominant-Set Clusters</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1057" to="1064" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feasible and Infeasible Maxima in a Quadratic Program for Maximum Clique</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jagota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artificial Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="411" to="420" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Factorization Approach to Grouping</title>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Burkhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Neumann</surname></persName>
		</editor>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="655" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Comparison of the Stability Characteristics of Some Graph Theoretic Clustering Methods</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="393" to="402" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Quantitative Measures of Change Based on Feature Organization: Eigenvalues and Eigenvectors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="110" to="136" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Weibull</surname></persName>
		</author>
		<title level="m">Evolutionary Game Theory</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Optimal Graph Theoretic Approach to Data Clustering: Theory and Its Application to Image Segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leahy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1101" to="1113" />
			<date type="published" when="1993-11">Nov. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph-Theoretic Methods for Detecting and Describing Gestalt Clusters</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Zahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="68" to="86" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">For more information on this or any other computing topic, please visit our Digital Library at www</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
