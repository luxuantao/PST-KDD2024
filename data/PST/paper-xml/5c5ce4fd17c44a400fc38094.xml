<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Reinforcement Learning for Resource Management in Network Slicing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rongpeng</forename><surname>Li</surname></persName>
							<email>lirongpeng@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Zhao</surname></persName>
							<email>zhaozf@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenyang</forename><surname>Yang</surname></persName>
							<email>cyyang@buaa.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xianfu</forename><surname>Chen</surname></persName>
							<email>xianfu.chen@vtt.fi</email>
						</author>
						<author>
							<persName><forename type="first">Minjian</forename><surname>Zhao</surname></persName>
							<email>mjzhao@zju.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
							<email>honggangzhang@zju.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">are with Zhejiang Univer-sity</orgName>
								<address>
									<postCode>310027</postCode>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">China Mobile Research Institute</orgName>
								<address>
									<postCode>100053</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">C. Yang is with Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">VTT Technical Research Centre of Finland</orgName>
								<address>
									<postCode>FI-90571</postCode>
									<settlement>Oulu</settlement>
									<country key="FI">Finland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Reinforcement Learning for Resource Management in Network Slicing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5CC2E779BD4255DE90C4630AC88A7CDE</idno>
					<idno type="DOI">10.1109/ACCESS.2018.2881964</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep Reinforcement Learning</term>
					<term>Network Slicing</term>
					<term>Neural Networks</term>
					<term>Q-Learning</term>
					<term>Resource Management</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network slicing is born as an emerging business to operators, by allowing them to sell the customized slices to various tenants at different prices. In order to provide better-performing and cost-efficient services, network slicing involves challenging technical issues and urgently looks forward to intelligent innovations to make the resource management consistent with users' activities per slice. In that regard, deep reinforcement learning (DRL), which focuses on how to interact with the environment by trying alternative actions and reinforcing the tendency actions producing more rewarding consequences, is assumed to be a promising solution. In this paper, after briefly reviewing the fundamental concepts of DRL, we investigate the application of DRL in solving some typical resource management for network slicing scenarios, which include radio resource slicing and priority-based core network slicing, and demonstrate the advantage of DRL over several competing schemes through extensive simulations. Finally, we also discuss the possible challenges to apply DRL in network slicing from a general perspective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Introduction</head><p>The fifth-generation cellular networks (5G) is assumed to be the key infrastructure provider for the next decade, by means of profound changes in both radio technologies and network architecture design <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. Besides the pure performance metrics like rate, reliability and allowed connections, the scope of 5G also incorporates the transformation of the mobile network ecosystem and accommodates heterogeneous services using one infrastructure. In order to achieve such a goal, 5G will fully glean the recent advances in the network virtualization and programmability <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, and provide a novel technique named network slicing <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Network slicing tries to get rid of the current, relatively monolithic architecture like the forth-generation cellular networks (4G) and slice the whole network into different parts, each of which is tailed to meet specific service requirement. Therefore, network slicing is born as an emerging business to operators and allows them to sell the customized network slices to various tenants at different prices. In a word, network slicing could act as a service (NSaaS) <ref type="bibr" target="#b4">[5]</ref>. NSaaS is quite similar to the mature business "infrastructure as a service (IaaS)", the benefit of which service providers like Amazon and Microsoft have happily enjoyed for a while. However, in order to provide better-performing and cost-efficient services, network slicing involves more challenging technical issues even for the real-time resource management on existing slices, since (a) for radio access networks, spectrum is a scarce resource and it is meaningful to guarantee the spectrum efficiency (SE) <ref type="bibr" target="#b7">[8]</ref>, while for core networks, virtualized functionalities are limited by computing resources; (b) the service level agreements (SLAs) with slice tenants usually impose stringent requirements on quality of experience (QoE) perceived by users <ref type="bibr" target="#b8">[9]</ref>; and (c) the actual demand of each slice heavily depends on the request patterns of mobile users. Hence, in the 5G era, it is critical to investigate how to intelligently respond to the dynamics of service request from mobile users <ref type="bibr" target="#b6">[7]</ref>, so as to obtain satisfactory QoE in each slice at the cost of acceptable spectrum or computing resources <ref type="bibr" target="#b3">[4]</ref>. There has been several works towards the resource management for the network slicing, particularly in specific scenarios like edge computing [?] and Internet of things <ref type="bibr">[?]</ref>. However, it is still very appealing to discuss a approach in generalized scenarios. In that regard, <ref type="bibr">[?]</ref> proposes to adopt genetic algorithm as an evolutionary means for inter-slice resource management. However, [?] does not reflect the explicit relationship that one slice might require more resources due to its more stringent SLA.</p><p>On the other hand, partially inspired by the psychology of human learning, the learning agent in reinforcement learning (RL) algorithm focuses on how to interact with the environment (represented by states) by trying alternative actions and reinforcing the tendency actions producing more rewarding consequences <ref type="bibr" target="#b9">[10]</ref>. Besides, reinforcement learning also embraces the theory of optimal control and adopts some ideas like value functions and dynamic programming. However, reinforcement learning faces some difficulties in dealing with large state space, since it is challenging to traverse every state and obtain a value function or model for every station-action pair in a direct and explicit manner. Hence, benefiting from the advances in graphics processing units (GPUs) and the less concern for the computing power, some researchers propose to sample only a fraction of states and further apply neural networks (NN) to train a sufficiently accurate value function or model. Following this idea, Google DeepMind has pioneered to combine NN with one typical RL algorithm (i.e., Q-Learning), and proposed one deep reinforcement learning (DRL) algorithm with enough performance stabilities <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>The well-known success of AlphaGo <ref type="bibr" target="#b10">[11]</ref> and following exciting results to apply DRL to solve resource allocation issues in some specific fields like power control <ref type="bibr" target="#b12">[13]</ref>, green communications <ref type="bibr" target="#b13">[14]</ref>, cloud radio access networks <ref type="bibr" target="#b14">[15]</ref>, mobile edge computing and caching [?], [?], <ref type="bibr" target="#b15">[16]</ref>, have aroused some research interest to apply DRL to the field of network slicing. However, given the challenging technical issues in the resource management on existing slices, it is critical to carefully investigate the performance of applying DRL in the following aspects:</p><p>• The basic concern is whether or not the application of DRL is feasible. More specifically, does DRL produce satisfactory QoE results while consuming acceptable network resources (e.g., spectrum)?</p><p>• The research community has proposed some schemes for the resource management in network slicing scenarios. For example, the resource management could be conducted by either following a meticulously designed prediction algorithm, or equally dividing the available resource into each slice. The former implies one reasonable option, while the latter saves a lot of computational cost. Hence, a comparison between DRL and these interesting schemes is also necessary. In this paper, we strive to address these issues.</p><p>The remainder of the paper is organized as follows. Section II starts with the fundamentals of RL and talks about the motivation to evolve towards DRL from RL. As the main part of the paper, Section III addresses two resource management issues in network slicing and highlights the advantages of DRL by extensive simulation analyses. Section IV concludes the paper and points out some research directions to apply DRL in a general manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. From Reinforcement Learning to Deep Reinforcement</head><p>Learning In this section, we give a brief introduction over RL or more specifically Q-Learning, and then talk about the motivation to evolve from Q-Learning to Deep Q-Learning (DQL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Reinforcement Learning</head><p>RL learns how to interact with the environment to achieve maximum cumulative return (or average return), and has been successfully applied in the fields like robot control, self driving, and chess playing for years. Mathematically, RL follows the typical concept of Markov decision process (MDP), while the MDP is a generalized framework for modeling decision-making problems in cases where the result is partially random and affected by the applied decision. An MDP can be formulated by a 5-tuple as M = ⟨S, A, P (s ′ |s, a), R, γ⟩, where S and A denote a finite state space and action set, respectively. P (s ′ |s, a) indicates the probability that the action a ∈ A under state s ∈ S at slot t leads to state s ′ ∈ S at slot t + 1. R(s, a) is an immediate reward after performing the action a under state s, while γ ∈ [0, 1] is a discount factor to reflect the diminishing importance of current reward on future ones. Usually, the goal of MDP is to find a policy a = π(s) that determines the selected action a under state s, so as to maximize the value function, which is typically defined as the expected discounted cumulative reward by the Bellman equation:</p><formula xml:id="formula_0">V π (ŝ) = E π [ ∞ ∑ k=0 γ k R(s (k) , π(s (k) ))|s (0) = ŝ) ] = E π [ R(ŝ, π(ŝ))) + γ ∑ s ′ ∈S P (s ′ |ŝ, π(ŝ))V π (s ′ ) ] .</formula><p>(1) Dynamic programming could be exploited to solve the Bellman equation when the state transition probability P (s ′ |s, a) is known apriori with no random factors. But inspired by both control theory and behaviorist psychology, RL aims to obtain the optimal policy π * under circumstances with unknown and partially random dynamics. Since RL does not have explicit knowledge over whether it has come close to its goal, it needs the balance between exploring new potential actions and exploiting the already learnt experience. So far, there has been some classical RL algorithms like Q-learning, actor-critic method, SARSA, TD(λ), etc <ref type="bibr" target="#b9">[10]</ref>. Given by the detailed methodologies and practical application scenarios, we can classify these RL algorithms according to different criteria:  at two consecutive episodes. For example, Q-learning updates its Q-value by the TD update as</p><formula xml:id="formula_1">Q(s, a) ← Q(s, a)+α(R(s, a)+γ max a ′ Q(s ′ , a ′ )-Q(s, a))</formula><p>, where α is the learning rate. Specifically, the term R(s, a) + γ max a ′ Q(s ′ , a ′ ) -Q(s, a) is also named as the TD error, since it captures the difference between the current (sampled) estimate R(s, a) + γ max a ′ Q(s ′ , a ′ ) and previous one Q(s, a).</p><p>• On-policy versus Off-policy: The value function update is also coupled with the executed update policy. Before updating the value function, the agent also needs to sample and learn the environment by performing some non-optimal policy. If the update policy is irrelevant to the sampling policy, the agent is called to perform an off-policy update.</p><p>Taking the example of Q-learning, this off-policy agent updates the Q-value by choosing the action corresponding to the best Q-value, while it could learn the environment by adopting sampling policies like ϵ-greedy or Boltzmann distribution to balance the "exploration and exploitation" problem <ref type="bibr" target="#b9">[10]</ref>. The Qlearning proves to converge regardless of the chosen sampling policy. On the contrary, the SARSA agent is on-policy, since it updates the value function by</p><formula xml:id="formula_2">Q(s, a) ← Q(s, a) + α(R(s, a) + γQ(s ′ , a ′ ) -Q(s, a))</formula><p>where a ′ and a need to be chosen according to the same policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. From Q-Learning to Deep Q-Learning</head><p>We first summarize the details of Q-Learning. Generally speaking, Q-Learning belongs to a model-free, TD update, off-policy RL algorithm, and consists of three major steps:</p><p>1) The agent chooses an action a under state s according to some policy like ϵ-greedy. Here, the ϵ-greedy policy means the agents chooses the action with the largest Q-value Q(s, a) with a probability of ϵ, and equally chooses the other actions with a probability of 1-ϵ |A| , where |A| denotes the size of the action space.</p><p>2) The agent learns the reward R(s, a) from the environment, and the state transitions to the next state s ′ .</p><p>3) The agent updates the Q-value function in a TD manner as</p><formula xml:id="formula_3">Q(s, a) ← Q(s, a) + α (R(s, a) + γ max a ′ Q(s ′ , a ′ ) -Q(s, a)</formula><p>). Classical RL algorithms usually rely on two different ways (i.e., explicit table or function approximation) to store the estimated value functions. For the table storage, RL algorithm uses an array or hash table to store the learnt results for each state-action pair. For large state space, it not only requires intensive storage, but also is unable to quickly transverse the complete the stateaction pair. Due to the curse of dimensionality, function approximation sounds more appealing.</p><p>The most straightforward way for function approximation is a linear approach. Taking the example of Qlearning, the Q-value function could be approximated by a linear combination of n orthogonal bases ψ(s,</p><formula xml:id="formula_4">a) = {ψ 1 (s, a), • • • ψ n (s, a)}, that is, Q(s, a) = θ 0 • 1 + θ 1 • ψ 1 (s, a) + • • • + θ n • ψ n (s, a) = θ T ψ(s, a)</formula><p>, where θ 0 is a biased term with 1 absorbed into the ψ for simplicity of representation and θ is a vector with the dimension of n. The function approximation in the Q-learning means that Q(s, a) = θ T ψ(s, a) should be as close as the learnt "target" value</p><formula xml:id="formula_5">Q + (s, a) = ∑ s P (s ′ |s, a) [ R(s, a) + γ max a ′ Q + (s ′ , a ′ )</formula><p>] over all the state-action pairs. Since it is infeasible to transverse all the state-action pairs, the "target" value could be approximated based on the minibatch samples and</p><formula xml:id="formula_6">Q + (s, a) ≈ R(s, a)+γ max a ′ Q + (s ′ , a ′ ).</formula><p>In order to make Q(s, a) = θ T ψ(s, a) approach the "target" value Q + (s, a), the objective function could be defined as</p><formula xml:id="formula_7">L(θ) = 1<label>2</label></formula><formula xml:id="formula_8">( Q + (s, a) -Q(s, a) ) 2 (2) = 1<label>2</label></formula><formula xml:id="formula_9">( Q + (s, a) -θ T ψ(s, a) ) 2 .</formula><p>The parameter θ minimizing L(θ) could be achieved by a gradient-based approach as</p><formula xml:id="formula_10">θ (i+1) ←θ (i) -α∇L(θ (i) )<label>(3)</label></formula><formula xml:id="formula_11">=θ (i) -α ( Q + (s, a) -θ T ψ(s, a) ) ψ(s, a).</formula><p>For a large state-action space, the function approximation reduces the number of unknown parameters to a vector with dimension n and the related gradient method further solves the parameter approximation in an computationally efficient manner.</p><p>Apparently, the linear function approximation could not accurately model the estimated value function. Hence, researchers have proposed to replace the approximation Q(s, a; θ) by some non-linear means. In that regard, NN is skilled in approximating non-linear functions <ref type="bibr" target="#b16">[17]</ref>. Therefore, in AlphaGo <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, NN has been exploited and the loss function can be re-defined as</p><formula xml:id="formula_12">L(θ) = 1 2 ( Q + (s, a) -Q(s, a; θ) ) 2 .</formula><p>Besides, deep neural network has made novel progress in the following aspects:</p><p>• Experience Replay <ref type="bibr" target="#b11">[12]</ref>: The agent stores the past experience (i.e., the tuple e t = ⟨s t , a t , s ′ t , R(s t , a t )⟩) at episode t into a dataset D t = (e 1 , • • • , e t ) and uniformly selects some (mini-batch) items from the dataset to update the Q-value neural network Q(s, a; θ).</p><p>• Network Cloning: The agent uses a separate network Q to guide how to select an action a in state s, and the network Q is replaced by Q every C episodes.</p><p>Simulation results demonstrate that this network cloning enhances the learning stability <ref type="bibr" target="#b11">[12]</ref>. Both experience replay and network cloning motivate to choose the off-policy Q-learning, since the sampling policy is only contingent on previously trained Q-value NN and the updating policy, which relies on the information from the new episodes, is irrespective of the sampling policy. On the other hand, the DQL agent could collect the information (i.e., state-action-reward pair) and train its policy in background. Also, the learned policy is stored in the neural networks and can be conveniently transferred among similar scenarios. In other words, the DQL could efficiently perform and timely make the resource allocation decision according to its already learned policy.</p><p>Finally, we illustrate the deep Q-learning in Fig. <ref type="figure" target="#fig_0">1</ref> and summarize the general steps in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. Resource Management for Network Slicing</head><p>Resource management is a permanent topic during the evolution of wireless communication. Intuitively, resource management for network slicing can be considered from several different perspectives.</p><p>• Radio Resource and Virtualized Network Functions:</p><p>As depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, resource management for network slicing involves both radio access part and core network part with slightly different optimization goals. Due to the limited spectrum resource, the resource management for the radio access puts considerable efforts in allocating resource blocks (RBs) to one slice, so as to maintain acceptable SE while trying to bring appealing rate and small delay. The widely adopted optical transmission in core networks has shifted the optimization of core network to design common or dedicated virtualized network functions (VNFs), so as to appropriately forward the packets from one specific slice with minimal scheduling delay. By balancing the relative importance of resource utilization (e.g, SE) and QoE satisfaction ratio, the resource management problem could be formulated • Equal or Prioritized Scheduling: As part of the control plane, IETF <ref type="bibr" target="#b17">[18]</ref> has defined the common control network function (CCNF) to all or several slices. The CCNF includes the access and mobility management function (AMF) as well as the network slice selection function (NSSF), which is in charge of selecting core network slice instances. Hence, besides equally treating flows from different slices, the CCNF might differentiate flows. For example, flows from ultra-reliable low-latency communications (URLLC) service can be scheduled and provisioned in higher priority, so as to experience as little latency as possible. In this case, in order to balance the resource utilization (RU) and the waiting time (WT) of flows, the objective goal could be similarly written as a weighted summation of RU and WT.</p><p>Based on the aforementioned discussions, we can safely reach a conclusion that, the objective of resource management for network slicing should take account of several variables and a weighted summation of these variables can be considered as the reward for the learning agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Radio Resource Slicing</head><p>In this part, we address how to apply DRL for radio resource slicing. Mathematically, given a list of existing slices </p><formula xml:id="formula_13">arg w max E{R(w, d)} = arg w max E { ζ • SE(w, d) + β • QoE(w, d) } s.t.: w = (w 1 , • • • , w N )<label>(4</label></formula><p>) At episode t, the DQL agent observes the state s t .</p><formula xml:id="formula_14">w 1 + • • • + w N = W d = (d 1 , • • • , d N ) d i ∼ Certain Traffic Model, ∀i ∈ [1, • • • , N ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3:</head><p>The agent chooses action a t with a probability ϵ or selects a t satisfying a t = arg max a Q(s , a; θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>After executing the action a t , the agent observes the reward R(s t , a t ) and a new state s t+1 = s ′ t for the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>The agent stores the episode experience e t = ⟨s t , a t , s ′ t , R(s t , a t )⟩ into D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>The agent samples a minibatch of experiences from D and sets</p><formula xml:id="formula_15">Q + (s t , a t ) = R(s t , a t ) + γ max a ′ Q + (s ′ t , a ′ ).</formula><p>In cases where episode terminates at t, Q + (s t , a t ) = R(s t , a t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>The agent updates the weights θ for the evaluation network by a gradient-based approach in (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>The agent clones the evaluation network Q to the target network Q every C episodes by assigning the weights θ as θ = θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>The episode index is updated by t ← t + 1. 10: until A predefined stopping condition (e.g., the gap between θ and θ, the episode length, etc) is satisfied.</p><p>The key challenge to solve (4) lies in the volatile demand variations without having known a priori due to the traffic model. Hence, DQL is exactly the matching solution to solve the problem. We evaluate the performance to adopt DQL to solve (4) by simulating a scenario containing one single BS with three types of services (i.e., VoIP, video, URLLC). There exist 100 registered subscribers randomly located within a 40 meter-radius circle surrounding the BS. These subscribers generate service models summarized in Table <ref type="table" target="#tab_1">I</ref>(b). VoIP and video services exactly take the parameter settings of VoLTE and video streaming models, while URLLC service takes the parameter settings of FTP 2 model <ref type="bibr" target="#b18">[19]</ref>. It can be observed from Table I(b), URLLC has less frequent packets compared with the others, while VoLTE requires the smallest bandwidth for its packets.</p><p>We consider DQL by using the mapping in Table <ref type="table" target="#tab_1">I</ref>(a) to optimize the weighted summation of system SE and slice QoE. Specifically, we perform round-robin scheduling method within each slice at the granularity of 0.5 ms. In other words, we sequentially allocate the bandwidth of each slice to the active users within each slice every 0.5 ms. Besides, we adjust the bandwidth allocation to each slice per second. Therefore, the DQL agent updates its Q-value neural network every second. We compare the simulation results with the following three methods, so as to explain the importance of DQL.</p><p>• Demand-prediction based method: The method tries to estimate the possible demand by using long short-term memory (LSTM) to predict the number of active users requesting VoIP, video and URLLC respectively. Afterwards, the bandwidth is allocated by two ways: (1) DP-No allocates the whole bandwidth to each slice proportional to the number of predicted packets. In particular, assuming that the total bandwidth is B and the predicted number of packets for VoIP, video and URLLC is N VoIP , N Video and N URLLC , the allocated bandwidth to these three slices (i.e., VoIP, video and URLLC) is </p><formula xml:id="formula_16">B•NVoIP NVoIP+N Video +NURLLC , B•N Video NVoIP+N Video +NURLLC , B•NURLLC NVoIP+N Video +NURLLC ,</formula><formula xml:id="formula_17">BN Video R Video NVoIPRVoIP+N Video R Video +NURLLCRURLLC , BNURLLCRURLLC NVoIPRVoIP+N Video R Video +NURLLCRURLLC , respectively.</formula><p>Round-robin is conducted within each slice.</p><p>• Hard slicing: Hard slicing means that each service slice is always allocated 1 3 of the whole bandwidth, since there exists 3 types of service in total. Again, round-robin is conducted within each slice.</p><p>• No slicing: Irrespective of the related SLA, all users are scheduled equally. Round-robin is conducted within all users. We primarily consider the downlink case and adopt system SE and QoE satisfaction ratio as the evaluation metrics. In particular, the system SE is computed as the number of bits transmitted per second per unit bandwidth, where the rate from the BS to users is derived based on Shannon capacity formula. Therefore, if part of the bandwidth has been allocated to one slice but the slice has no service activities at one slot, such part of bandwidth has been wasted, thus degrading the system SE. QoE satisfaction ratio is obtained by dividing the number of completely transmitted packets satisfying rate and latency requirement by the total number of arrived packets.</p><p>Fig. <ref type="figure" target="#fig_3">3</ref> presents the learning process of DQL 1 in radio resource management. In particular, Fig. <ref type="figure" target="#fig_3">3(a)∼3(f)</ref> give the initial performance of DQL when the QoE weight is 5000 and the SE weight is 0.1. Fig. <ref type="figure" target="#fig_3">3</ref>(g)∼3(l) provide the performance during the last 50 of 50000 learning updates. From these sub-figures, it can be observed that DQL could not well learn the user activities at the very beginning and the allocated bandwidth fluctuates heavily. But after nearly 50000 updates, DQL has gained better knowledge over user activities and yielded a state bandwidth-allocation strategy. Besides, Fig. <ref type="figure" target="#fig_3">3</ref>(m) and Fig. <ref type="figure" target="#fig_3">3</ref>(n) show the variations of SE and QoE along with each learning epoch. From both subfigures, a larger QoE weight produces policies with superior QoE performance while bringing certain loss in the system SE performance.</p><p>1 Notably, γ is set as 0.9.</p><p>Fig. <ref type="figure">4</ref> provides a detailed performance comparison among the candidate techniques, where the results for DQL are obtained after 50000 learning updates. Fig. <ref type="figure">4</ref>(a)∼4(f) gives the percentage of total bandwidth allocated to each slice using the pie charts and highlights the QoE satisfaction ratio by surrounding text. From Fig. <ref type="figure">4</ref>(a)∼4(b), a reduction in transmission antennas from 64 to 16, which implies a decrease in network capability and an increase in potential collisions across slices, leads to a re-allocation of network bandwidth inclined to the bandwidth-consuming yet activity-limited URLLC slice. Also, it can be observed from Fig. <ref type="figure">4</ref>(f), when the downlink transmission uses 64 antennas, "no slicing" performs the best, since the transmission capability is sufficient and the scheduling period is 0.5 ms while the bandwidth allocated to each slice is adjusted per second and thus slower to catch the demand variations. When the number of downlink antenna turns to 32, the DQL-driven scheme produces 81% QoE satisfaction ratio for URLLC, while "no slicing" and "hard slicing" schemes only provision 15% and 41% satisfied URLLC packets, respectively. Notably, applying DQL mainly leads to the QoE gain of URLLC. The reason lies in that as summarized in Table I(b), the distribution of packet size for URLLC follows a truncated lognormal distribution with the mean value of 2 MByte, which is far larger than those of VoLTE and Video services. Given the larger transmission volume and strictly lower latency requirement, it is far more difficult to satisfy the QoE of URLLC. In this case, it is still satisfactory that DQL outperforms the other competitive schemes to render higher QoE gain of URLLC at a slight cost of spectrum efficiency (SE). Meanwhile, Fig. <ref type="figure">4(d</ref>) and Fig. <ref type="figure">4</ref>(e) demonstrate the allocation results for the demandprediction based schemes and show significantly inferior performance, since Fig. <ref type="figure" target="#fig_3">3</ref>(a)∼3(c) and Fig. <ref type="figure" target="#fig_3">3</ref>(g)∼3(i) show the number of video packets dominates the transmission and simple packet-number based prediction could not capture the complicated relationship between demand and QoE. On the other hand, Fig. <ref type="figure">4</ref>(g) illustrates that this QoE advantage of DQL comes at the cost of a decrease in SE. Recalling the definition of the reward in DQL, if we decrease the QoE weight from 5000 to 1, DQL could learn another bandwidth allocation policy (in Fig. <ref type="figure">4(c)</ref>) yielding a larger SE yet a lower QoE. Fig. <ref type="figure">4</ref>(g) ∼ 4(j) further summarize the performance comparison in terms of SE or QoE satisfaction ratios, where the vertical errorbars show the standard derivation. These subfigures validate the DQL's flexibility and advantage in resource-limited scenarios to ensure the QoE per user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Priority-based Scheduling in Common VNFs</head><p>Section III-A has discussed how to apply DRL in radio resource slicing. Similarly, if we virtualize the computation resources as VNFs for each slice, the problem to allocate computation resources to each VNF could be solved similar to the radio resource slicing case. Therefore, in this part, we talk about another important issue, that is, priority-based core network slicing for common VNFs. Specifically, we simulate a scenario where there exists 3 service function chains (SFCs) possessing the same basic capability but working at the expenditure of different computation processing units (CPUs) and yields different provisioning results (e.g., waiting time). Also, based on the commercial value or related SLA, flows could be classified into 3 categories (e.g., Category A, B, and C) with decreasing priority from Category A to Category C, and a priority-based scheduling rule is defined as that SFC I prioritizes Category A flows over the others, while SFC II equally treats Category A and B users but serves Category C flows with lower priority. SFC III treats all flows equally. Besides, SFCs process flows with equal priority according to the arrival time. The eventually utilized CPUs of each SFC depend on the number of its processed flows. Besides, SFC I, II and III cost 2, 1.5, and 1 CPU(s), but incur 10, 15, and 20 ms regardless of the flow size, respectively. Hence, subject to the limited number of CPUs, flows for each type will be scheduled to an appropriate SFC, so as to incur acceptable waiting time. Therefore, the scheduling of flows should match and learn the arrival of flows in three categories, and DQL is considered as a promising solution.</p><p>Similarly, it is critical to design an appropriate mapping of DRL elements to this slicing issue. As Table I(a) implies, we use a mapping slightly different from that for radio resource slicing, so as to manifest the flexibility of DQL. In particular, we abstract the state of DQL as a summary of the category and arrival time of last 5 flows and the category of the newly arrived flow, while the reward is defined as the weighted summation of processing and queue time of this flow, where a larger weight in this summation is adopted to reflect the importance of flows with higher priority. Also, we first pre-train its NN by emulating some flows with lognormal distributed interarrival time from the three categories' users.</p><p>We compare the DQL scheme with an intuitive "no priority" solution, which allocate the flow to the SFC yielding minimum waiting time. Fig. <ref type="figure">5</ref> provides the related performance by randomly generating 10000 flows and provisioning accordingly, where the vertical and horizontal axes represent the number of utilized CPUs and the waiting time of flows respectively. Specifically, the bidimensional shading color reflects the number of flows corresponding to the specific waiting time and utilized CPUs. In particular, the darker color implies the larger number. Compared with the "no priority" solution, the DQLempowered slicing results provision flows with smaller average waiting time (i.e., 10.5% lower than "no priority") and significantly more sufficient CPU usage (i.e., 27.9% larger than "no priority"). In other words, DQL could support alternative solutions to exploit the computing resources and reduce the waiting time by first serving the users with higher commercial value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. Conclusion and Future Directions</head><p>From the discussions in this article, we found that matching the allocated resource to slices with the users' activity demand will be the most critical challenge for effectively realizing network slicing, while DRL could be a promising solution. Starting with the introduction of fundamental concept for DQL, one typical type of DRL, we explained the working mechanism and application motivation of DQL to solve this problem. We further demonstrated the advantage of DQL in managing this demand-aware resource allocation in two typical slicing scenarios including radio resource slicing and prioritybased core network slicing through extensive simulations. Our results showed that compared with the demand prediction-based and some other intuitive solutions, DQL could implicitly incorporate more deep relationship between demand (i.e., activities) and supply (i.e., resource allocation) in resource-constrained scenarios, and enhance the effectiveness and agility for network slicing. Finally, in order to fulfill the application of DQL in a broader sense, we pointed out some noteworthy issues. We believe DRL could play a crucial role in network slicing in the future.</p><p>However, network slicing involves many aspects and a successful application of DQL needs some careful considerations: (a) Slice admission control on incoming requests for new slices: the success of network slicing implies a dynamic and agile slice management scheme. Therefore, if requests for new slices emerge, how to apply DQL is also an interesting problem since the defined state and action space requires to adapt to the changes in the "slice" space. (b) Abstraction of states and actions: Section III has provided two ways to abstract state and action. Both methods sound practical in the related scenarios and reflect the flexibility of DQL. Hence, for new scenarios, it becomes an important issue to choose appropriate abstraction of states and actions, so as to better model the problem and save the learning cost. Up to date, it remains an open question on how to give some abstraction guidelines. (c) Latency and accuracy to retrieve rewards: The simulations in Section III has assumed the instantaneous and accurate acquirement of rewards for a state-action pair. But, such an assumption no longer holds in practical complex wireless environment, since it takes time for user equipment to report the information and the network may not successfully receive the feedback. Also, similar to the case for state and action, the abstraction of reward might be difficult and the defined reward should be as simple as possible. (d) Policy learning cost: The time-varying nature of wireless channel and user activities requires a fast policy-learning scheme. However, the current cost of policy training still lacks the necessary learning speed. For example, our pre-training for the priority-based network slicing policy takes two days in an Intel Core i7-4712MQ processor to converge the Qvalue function. Though GPU could speedup the training process, the learning cost is still heavy. Therefore, there are still a lot of interesting questions to be addressed.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of deep Q-learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. An illustration of resource management for network slicing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1, • • • , N sharing the aggregated bandwidth W and having fluctuating demands d = (d 1 , • • • , d N ), DQL tries to give a bandwidth sharing solution w = (w 1 , • • • , w N ), so as to maximize the long-term reward expectation E{R(w, d)} where the notation E(•) denotes to take the expectation of the argument, that is,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The performance of DQL for radio resource slicing w.r.t. the learning steps (QoE Weight = 5000).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. The performance comparison among different schemes for radio resource slicing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>of Aeronautics and Astronautics, BUAA), China, in 1997. She has been a full professor with the School of Electronics and Information Engineering, BUAA since 1999. She has published over 200 papers and filed over 80 patents in the fields of energy efficient transmission, URLLC, wireless local caching, CoMP, interference management, cognitive radio, and relay, etc. She was supported by the 1st Teaching and Research Award Program for Outstanding Young Teachers of Higher Education Institutions by Ministry of Education of China. She was the chair of Beijing chapter of IEEE Communications Society during 2008-2012, and the MDC chair of APB of IEEE Communications Society during 2011-2013. She has served as TPC Member, TPC cochair or Track co-chair for IEEE conferences. She has ever served as an associate editor for IEEE Trans. on Wireless Communication, guest editor for IEEE Journal of Selected Topics in Signal Processing and IEEE Journal of Selected Areas in Communications. Her recent research interests lie in mobile AI, wireless caching, and URLLC. Xianfu Chen received his Ph.D. degree in Signal and Information Processing, from the Department of Information Science and Electronic Engineering at Zhejiang University, Hangzhou, China, in March 2012. He is currently a Senior Scientist with the VTT Technical Research Centre of Finland Ltd, Oulu, Finland. His research interests cover various aspects of wireless communications and networking, with emphasis on network virtualization, softwaredefined radio access networks, green communications, centralized and decentralized resource allocation, and the application of machine learning to cognitive radio networks. He is an IEEE member. Minjian Zhao received the M.Sc. and Ph.D. degrees in communication and information systems from Zhejiang University, Hangzhou, China, in 2000 and 2003, respectively. He is currently a Professor with the College of Information Science and Electronic Engineering, Zhejiang University. His research interests include modulation theory, channel estimation and equalization, and signal processing for wireless communications.Honggang Zhang is currently a Full Professor with the College of Information Science and Electronic Engineering, Zhejiang University, Hangzhou, China. He was an Honorary Visiting Professor with the University of York, U.K and an International Chair Professor of Excellence for Université Européenne de Bretagne and Supèlec, France. He was the Co-Author and an Editor of two books "Cognitive Communications-Distributed Artificial Intelligence (DAI), Regulatory Policy and Economics, Implementation (John Wiley &amp; Sons)" and "Green Communications: Theoretical Fundamentals, Algorithms and Applications (CRC Press)", respectively. He is also active in the research on green communications and was the leading Guest Editor of the IEEE Communications Magazine special issues on Green Communications. He is taking the role of an Associate Editorin-Chief of China Communications and the Series Editors of the IEEE Communications Magazine for its Green Communications and Computing Networks Series. He served as the Chair of the Technical Committee on Cognitive Networks of the IEEE Communications Society from 2011 to 2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I A</head><label>I</label><figDesc>Brief Summary of Key Settings in DRL for Network Slicing Simulations (a) The Mapping from Resource Management for Network Slicing to DRL</figDesc><table><row><cell></cell><cell cols="2">Radio Resource Slicing</cell><cell></cell><cell>Priority-based Core Network Slicing</cell></row><row><cell>State</cell><cell cols="3">The number of arrived packets in each slice within a specific time window</cell><cell>The priority and time-stamp of last arrived five flows in each service function chain (SFC)</cell></row><row><cell>Action</cell><cell cols="2">Allocated bandwidth to each slice</cell><cell></cell><cell>Allocated SFC for the flow at current time-stamp</cell></row><row><cell>Reward</cell><cell cols="3">Weighted sum of SE and QoE in 3 sliced bands</cell><cell>Weighted sum of average time in 3 SFCs</cell></row><row><cell></cell><cell></cell><cell cols="3">(b) Parameter settings for radio resource slicing</cell></row><row><cell></cell><cell></cell><cell>VoLTE</cell><cell>Video</cell><cell>URLLC</cell></row><row><cell>Bandwidth</cell><cell></cell><cell>10 MHz</cell><cell></cell></row><row><cell>Scheduling</cell><cell></cell><cell cols="2">Round robin per slot (0.5 ms)</cell></row><row><cell cols="2">Slice Band Adjustment (Q-Value Update)</cell><cell cols="2">1 second (2000 scheduling slots)</cell></row><row><cell>Channel</cell><cell></cell><cell>Rayleigh fading</cell><cell></cell></row><row><cell cols="2">User No. (100 in all)</cell><cell>46</cell><cell>46</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Truncated Pareto [Ex-</cell></row><row><cell cols="2">Distribution of Inter-</cell><cell>Uniform [Min = 0, Max</cell><cell cols="2">ponential Para = 1.2,</cell><cell>Exponential [Mean =</cell></row><row><cell>Arrival Time</cell><cell></cell><cell>= 160ms]</cell><cell cols="2">Mean = 6 ms, Max =</cell><cell>180 ms]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">12.5 ms]</cell></row><row><cell cols="2">Distribution of Packet Size</cell><cell>Constant (40 Byte)</cell><cell cols="2">Truncated Pareto [Ex-ponential Para = 1.2, Mean = 100 Byte, Max = 250 Byte]</cell><cell>Truncated Lognormal [Mean = 2 MB, Standard Deviation = 0.722 MB, Maximum =5 MB]</cell></row><row><cell>SLA: Rate</cell><cell></cell><cell>51 kbps</cell><cell cols="2">5 Mbps</cell><cell>10 Mbps</cell></row><row><cell>SLA: Latency</cell><cell></cell><cell>10 ms</cell><cell>10 ms</cell><cell>5 ms</cell></row><row><cell cols="3">Algorithm 1 The general steps of deep reinforcement</cell><cell></cell></row><row><cell>learning.</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>input: An evaluation network Q with weights θ; a target network Q with weights θ = θ. initialize: A replay memory dataset D with size of N ; the episode index t = 0. 1: repeat 2:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>respectively. (2) DP-BW performs the allocation by multiplying the number of predicted packets by the least required rate in Table I(b) and then computing the proportion. In this regard, assuming that the required rate for the three slices is R VoIP , R Video and R URLLC , the allocated bandwidth to VoIP, video and URLLC is</figDesc><table /><note><p><p>BNVoIPRVoIP</p>NVoIPRVoIP+N Video R Video +NURLLCRURLLC ,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>several papers in the related fields. He serves as an Editor of China Communications.Zhifeng Zhao is an Associate Professor at the Department of Information Science and Electronic Engineering, Zhejiang University, China. He received the Ph.D. degree in Communication and Information System from the PLA University of Science and Technology, Nanjing, China, in 2002. Prior to that, he received the Master degree of Communication and Information System in 1999 and Bachelor degree of Computer Science in 1996, from the PLA University of Science and Technology, respectively. From September 2002 to December 2004, he acted as a postdoctoral researcher at the Zhejiang University, where his researches were focused on multimedia NGN (nextgeneration networks) and soft-switch technology for energy efficiency. From January 2005 to August 2006, he acted as a senior researcher at the PLA University of Science and Technology, Nanjing, China, where he performed research and development on advanced energyefficient wireless router, Ad Hoc network simulator and cognitive mesh networking test-bed. His research area includes cognitive radio, wireless multi-hop networks (Ad Hoc, Mesh, WSN, etc.), wireless multimedia network and Green Communications. Qi Sun received her Ph.D. degree in information and communication engineering from Beijing University of Posts and Telecommunications in 2014. After graduation, she joined the Green Communication Research Center of the China Mobile Research Institute. Her research interest focuses on 5G communications, including new waveforms, non-orthogonal multiple access, massive MIMO, full duplex. Chih-Lin I is CMCC Chief Scientist of Wireless Technologies, launched 5G R&amp;D in 2011, and leads C-RAN, Green and Soft initiatives. Chih-Lin received IEEE Trans. COM Stephen Rice Best Paper Award, and IEEE ComSoc Industrial Innovation Award. She was on IEEE ComSoc Board, GreenTouch Executive Board, WWRF Steering Board, M&amp;C Board Chair, and WCNC SC Founding Chair. She is on IEEE ComSoc SPC and EDB, ETSI/NFV NOC, and Singapore NRF SAB. Chenyang Yang received her Ph.D. degrees in Electrical Engineering from Beihang University (formerly Beijing University</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2018.2881964, IEEE Access</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to express their sincere gratitude to Chen Yu and Yuxiu Hua of Zhejiang University for the valuable discussions to implement part of simulation codes.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by National Key R&amp;D Program of China (No. 2018YFB0803702), National Natural Science Foundation of China (No. 61701439, 61731002), Zhejiang Key Research and Development Plan (No. 2018C03056).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Network slices toward 5G communications: Slicing the LTE network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Katsalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Nikaein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ksentini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Braun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="146" to="154" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NFV and SDN -Key technology enablers for 5G networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Z</forename><surname>Yousaf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bredel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schaller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Area. Comm</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2468" to="2478" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimising 5G infrastructure markets: The business of network slicing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gramaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sciancalepore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Samdanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Costa-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE INFOCOM</title>
		<meeting>IEEE INFOCOM<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05">2017. May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Intelligent 5G: When cellular networks meet artificial intelligence</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Wireless Commun</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="175" to="183" />
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network slicing as a service: Enable industries own software-defined cellular networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="146" to="153" />
			<date type="published" when="2016-07">Jul. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Network slicing for 5G: Challenges and opportunities</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Samaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bhamare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The algorithmic aspects of network slicing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vassilaras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gkatzikis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liakopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Stiakogiannakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Debbah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Paschos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="112" to="119" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network slicing for service-oriented networks resource constraints</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Farmanbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Area. Comm</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2512" to="2521" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">QoS-aware and reliable traffic steering for service function chaining in mobile networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Area. Comm</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2522" to="2531" />
			<date type="published" when="2017-11">Nov. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barto</surname></persName>
		</author>
		<ptr target="http://webdocs.cs.ualberta.ca/~sutton/book/ebook/" />
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V D</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<ptr target="https://www.nature.com/articles/nature16961" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016-01">Jan. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<ptr target="http://www.nature.com/nature/journal/v518/n7540/full/nature14236.html" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for distributed dynamic power allocation in wireless networks</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Nasir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<idno>arxiv, p. cs.IT 1808.00490</idno>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepnap: Data-driven base station sleeping operations through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deep reinforcement learning based framework for power-efficient resource allocation in cloud RANs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Gursoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICC 2017</title>
		<meeting>IEEE ICC 2017<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05">May 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Software-defined networks with mobile edge computing and caching for smart cities: A big data deep reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C M</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Commun. Mag</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="31" to="37" />
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989-01">Jan. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Network Slicing -3GPP Use Case</title>
		<author>
			<persName><forename type="first">X</forename><surname>Foy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahman</surname></persName>
		</author>
		<ptr target="https://tools.ietf.org/id/draft-defoy-netslices-{3GPP}-network-slicing-02.html" />
	</analytic>
	<monogr>
		<title level="j">Network Working Group, IETF, Tech. Rep</title>
		<imprint>
			<date type="published" when="2017-10">Oct. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">NGMN radio access performance evaluation methodology</title>
		<author>
			<persName><surname>Ngmn</surname></persName>
		</author>
		<ptr target="https://www.ngmn.org/publications/all-downloads" />
		<imprint/>
	</monogr>
	<note>html?tx_news_pi1%5Bnews%5D= 604&amp;cHash=94dc3082a0b35f5ec64dbf9e33d2298a Biographies</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">sponsored by the National Postdoctoral Program for Innovative Talents. His research interests currently focus on Reinforcement Learning, Data Mining and all broad-sense network problems</title>
		<author>
			<persName><forename type="first">Rongpeng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010-06">June 2015. June 2010. August 2015 to September 2016</date>
			<publisher>Huawei Technologies Co. Ltd</publisher>
			<pubPlace>Hangzhou China; Xi&apos;an, China; Shanghai, China; Hangzhou, China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>College of Information Science and Electronic Engineering, Zhejiang University ; D and B.E. from Zhejiang University, Hangzhou, China and Xidian University ; Computer Science and Technologies, Zhejiang University</orgName>
		</respStmt>
	</monogr>
	<note>He returned to academia in November 2016 as a postdoctoral researcher in College of. e.g., resource management, security, etc) and he has authored/coauthored</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
