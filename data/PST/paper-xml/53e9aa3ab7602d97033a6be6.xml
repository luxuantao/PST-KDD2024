<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Big Data: A Research Agenda</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alfredo</forename><surname>Cuzzocrea</surname></persName>
							<email>cuzzocrea@si.deis.unical.it</email>
							<affiliation key="aff0">
								<orgName type="laboratory">ICAR-CNR and University of Calabria</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Domenico</forename><surname>Saccà</surname></persName>
							<email>sacca@unical.it</email>
							<affiliation key="aff1">
								<orgName type="department">DIMES Dept</orgName>
								<address>
									<country>University of Calabria Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
							<email>ullman@cs.stanford.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Big Data: A Research Agenda</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3321177F01FBD58731E130F919720DDC</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.2.4 [DATABASE MANAGEMENT]: Systems Algorithms</term>
					<term>Design</term>
					<term>Management</term>
					<term>Performance</term>
					<term>Theory Big Data</term>
					<term>OLAP over Big Data</term>
					<term>Big Data Posting</term>
					<term>Privacy of Big Data</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, a great deal of interest for Big Data has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories (mostly being of unstructured nature). Inspired by this main trend, in this paper we discuss three important aspects of Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. We also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in this research field.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Recently, a great deal of interest for Big Data (e.g., <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16]</ref>) has risen, mainly driven from a widespread number of research problems strongly related to real-life applications and systems, such as representing, modeling, processing, querying and mining massive, distributed, large-scale repositories. The term "Big Data" identifies specific kinds of data sets, manly of unstructured data, which populate the data layer of scientific computing applications (e.g., <ref type="bibr" target="#b24">[24]</ref>) and the Web (e.g., <ref type="bibr" target="#b30">[30]</ref>). Data stored in the underlying layer of all these application scenarios have some specific characteristics in common, among which we recall <ref type="bibr" target="#b15">[15]</ref>: (i) large-scale data, which refers to the size and the distribution of data repositories; (ii) scalability issues, which refers to the capabilities of applications running on large-scale, enormous data repositories (i.e., big data, for short) to scale over growing-in-size inputs rapidly; (iii) supporting advanced Extraction-Transformation-Loading (ETL) processes Among the plethora of research aspects lying in the Big Data research context, a leading role is played by the so-called analytics over Big Data problem (e.g., <ref type="bibr" target="#b15">[15]</ref>), which, in turn, implies a wide family of (sub-)problems mainly arising in the context of Database, Data Warehousing and Data Mining research. Analytics can be intended as complex procedures running over large-scale, enormousin-size data repositories whose main goal is that of extracting useful knowledge kept in such repositories. Analytics over Big Data repositories has recently received a great deal of attention from the research communities. One of the most significant application scenarios where Big Data arise is, without doubt, scientific computing. Here, scientists and researchers produce huge amounts of data per-day via experiments (e.g., think of disciplines like high-energy physics, astronomy, biology, bio-medicine, and so forth) but extracting useful knowledge for decision making purposes from these massive, large-scale data repositories is almost impossible for actual DBMS-inspired analysis tools. From a methodological point of view, two main research challenges arise. The first one is represented by the issue of conveying Big Data stored in heterogeneous and different-in-nature data sources (e.g., legacy systems, Web, scientific data repositories, sensor and stream databases, social networks) into a structured, hence well-interpretable, format, which is then ready to populating OLAP data cubes modeling the target analytics. The second one is represented by the issue of managing, processing and transforming so-extracted structured data repositories in order to derive Business Intelligence (BI) components like diagrams, plots, dashboards, and so forth, for decision making purposes, hence effectively realizing the complex analytics view.</p><p>Directly related to the Big Data and analytics over Big Data research areas, three important aspects arise, which we discuss in this paper: OLAP over Big Data, Big Data Posting, and Privacy of Big Data (e.g., <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b20">20]</ref>). These aspects indeed challenging research problems which will attract the attention from the research communities in future years. Inspired by this main trend, in this paper we focus on the above-mentioned research aspects, and we also depict future research directions, hence implicitly defining a research agenda aiming at leading future challenges in the Big Data era.</p><p>The remaining part of this paper is organized as follows. Section 2 (by Alfredo Cuzzocrea) discusses research issues and open problems in the context of OLAP over Big Data. Section 3 (by Domenico Saccà) focuses on posting methodologies over Big Data. Section 4 (by Jeffrey Ullman) moves the attention over privacy preserving issues of Big Data. Finally, in Section 5, we provide conclusions of our research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OLAP OVER BIG DATA</head><p>Due to the intrinsic nature of Big Data application scenarios (e.g., <ref type="bibr" target="#b8">[8]</ref>), it is natural to adopt Data Warehousing and OLAP methodologies <ref type="bibr" target="#b21">[21]</ref> with the goal of collecting, extracting, transforming, loading, warehousing and OLAPing such kinds of datasets, by adding significant add-ons supporting analytics over Big Data (e.g., <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24]</ref>). Data Warehousing and OLAP are classical scientific fields which have been addressed since several decades by the Database and Data Warehousing research community. Symmetrically, the fundamental problem of computing OLAP data cubes has been contextualized in a wide family of types, ranging from classical relational data sets (e.g., <ref type="bibr" target="#b23">[23]</ref>) to graph data sets (e.g., <ref type="bibr" target="#b7">[7]</ref>), and from XML (e.g., <ref type="bibr" target="#b25">[25]</ref>) and stream data (e.g., <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b12">12]</ref>) to novel social network data (e.g., <ref type="bibr" target="#b38">[38]</ref>), and so forth.</p><p>With the advent of the Big Data research context, it is natural to think of the problem of computing OLAP data cubes over Big Data as one of the top-interesting challenges in the research community, with also powerful technological achievements to be reached within the scope of real-life large-scale data-intensive applications and systems. Unfortunately, despite the clear convergence, state-ofthe-art solutions are not capable to deal with computing OLAP data cubes over Big Data, mainly due to two intrinsic factors of Big Data repositories: (i) size, which becomes really explosive in such data sets; (ii) complexity (of multidimensional data models), which can be very high in such data sets (e.g., cardinality mappings, irregular hierarchies, dimensional attributes etc.). As a consequence, there emerge the forceful needs of designing novel models, techniques, algorithms and computational platforms for supporting the problem of computing OLAP data cubes over Big Data, which, indeed, literally represents an effective call to arms for next-generation Data Warehousing and OLAP research.</p><p>Several research problems arise when computing OLAP data cubes over Big Data. First, as mentioned above, size plays a first-class role. Indeed, fact tables can easily become huge when computed over Big Data sets. This adds severe computational issues as the size can become a real bottleneck from practical applications (e.g., <ref type="bibr" target="#b26">[26]</ref>). Also, complexity is very relevant at the same, as building OLAP data cubes over Big Data also implies complexity problems which do not arise in traditional OLAP settings (e.g., in relational environments). For instance, the number of dimensions can really become explosive, due to the strongly unstructured nature of Big Data sets, as well as there could be multiple (and heterogeneous) measures for such data cubes. Design methodologies for OLAP data cubes have been of relevant interest for Database and Data Warehousing research too. In the specific case of designing methodologies of OLAP over Big Data, the performance aspect must be taken into greater consideration, due to obvious spin-offs given by such design task. In this case, designers must move the attention on the following critical questions: (i) what is the overall building time of the data cube to be designed (computing aggregations over Big Data may become prohibitive)?; (ii) how the data cube should be updated and which maintenance plan should be selected?; (iii) which building strategy should be adopted (e.g., divide &amp; conquer (e.g., <ref type="bibr" target="#b36">[36]</ref>)?</p><p>Due to the enormous size, computing methodologies for OLAP data cubes over Big Data will turn (again!) into a challenging research problem, similarly to what happened for early OLAP data cube computing experiences (e.g., <ref type="bibr" target="#b23">[23]</ref>). Here, the most promising technology to follow seems to be the emerging Cloud Computing paradigm (e.g., <ref type="bibr" target="#b2">[2]</ref>), perhaps inspired by classical parallel computing methodologies (e.g., <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b3">3]</ref>). Strongly related to the computing methodology problem, in-memory representation plays a relevant role as well. In this respect, the main problem to be addressed concerns with how an OLAP data cube over Big Data should be mapped in memory. This is a critical challenge to be considered, due to the fact that the very high number of dimensions in such cubes easily convey to explosive cell cardinalities; as a consequence, solutions based on tertiary memory should be deeply investigated. Also, it is natural to figure-out that innovative hardware solutions, such as GPU-based data processing (e.g., <ref type="bibr" target="#b35">[35]</ref>), will play an important role with respect to the issue of computing OLAP data cubes over Big Data.</p><p>As regards query and user-centric aspects, we identify the following research issues: (i) query languages and optimization: classical MDX approaches do not incorporate optimization solutions prone to deal with Big Data needs -future investigations must focus the attention on optimization issues given by processing Big Data in a multidimensional fashion; (ii) end-user performance: OLAP data cubes computed over Big Data tend to be huge, hence end-user performance easily becomes poor on such cubes, especially during the aggregation and query phases; therefore, it follows that end-user performance must be included as a critical factor within the design process of OLAP data cubes over Big Data; Quality aspects will more and more become a critical factor in next generation Data Warehousing and OLAP methodologies over Big Data. In fact, due to the strongly unstructured nature of Big Data sources, aggregations computed on such data sources can easily turn out to be "poor"; hence, it is easy to understand how much important controlling the quality of final data cubes will become. Also, since OLAP data cubes over Big Data must, prominently, be processed and managed to extract and build useful analytics, usability aspects will be relevant as well. Indeed, this aspect opens the door to a wide family of research problems, such as devising methodologies to "measure" how much usable an OLAP data cube built on Big Data repositories is.</p><p>Still at the user layer, visualization and interactive exploration are authoritative problems in OLAP over Big Data. In more detail, since Big Data expose explosive size, visualization issues of OLAP data cubes (e.g., <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref>) over Big Data play a first-class role in this research field. As a consequence, a novel class of visualization metaphors, methodologies and solutions must be devised, in order to cope with emerging challenges posed by visualizing massive OLAP data cubes over Big Data; real-time visualization of extracted core data, visualization of mashuped data, and effective visualization over mobile devices should also be considered. Coupled with visualization issues, interactive exploration issues are severe milestones to traverse in the context of OLAP data cubes over Big Data research; in fact, enormous-in-size data cubes are difficult to explore (e.g., under the execution of a fixed analytical process) while extracting useful knowledge, with important implications such as conceptual navigation, concept drift, interaction metaphors (e.g., <ref type="bibr" target="#b34">[34]</ref>), and so forth.</p><p>As mentioned above, analytics over Big Data (cubes <ref type="bibr" target="#b15">[15]</ref>) represent a topic of emerging interest for the Database and Data Ware-housing research community. Here, there exist several problems to be investigated, running from how to design an analytical process over OLAP data cubes computed on top of Big Data to how to optimize the execution of so-obtained analytical processes, and from the seamless integration of OLAP (Big) data cubes with other kinds of unstructured information (within the scope of analytics). Also, integration with classical data-intensive platforms is an important issue that focuses on how to integrate models, techniques, algorithms and computational platforms devised for OLAP over Big Data with classical data-intensive platforms, in the view of a seamless vision of comprehensive large-scale data-intensive systems.</p><p>Finally, last but not least, suitable development tools for supporting the design and the development of OLAP data cubes over Big Data, according to a methodology able of incorporating all the aspects discussed above, represent a non-secondary challenge to be dealt with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">BIG DATA POSTING</head><p>It is well known that a Web Search Engine (e.g., Google or Yahoo) mainly executes string (word) selection queries across public resources on the Web. In a sense, for those who have spent decades of their research effort to elaborate query languages advancing SQL, it may be frustrating to eventually witness the victory of query languages that are much more elementary than SQL, as they only enable to list words for making a simple selection with no mention of user-indigestible join operations. Dislike of join is manifested not only by humans but also by most NoSQL platforms, mainly those that have a column-oriented structure <ref type="bibr" target="#b30">[30]</ref> (e.g., Cassandra or HBase) providing effective support to column selection queries only. These systems may represent a second reason of frustration for traditional database researchers: table normalization failed to become the ultimate guide for database designers.</p><p>We have added "may be" to the two above-mentioned frustrations as we believe that advanced query languages and db design tools may take their revenge if moved to the backstage of novel Big Data applications, where the complexity stays: the large technological infrastructure for crawling, indexing and accessing huge amount of contents on the Web. Advanced query languages and db design tools may have a crucial role in enriching the semantics of database tuples that are posted on the Web cloud. We fully agree with the arguments of <ref type="bibr" target="#b5">[5]</ref> and <ref type="bibr" target="#b4">[4]</ref>: to solve the challenges of the emerging "Big Data" platforms, database technology (including database theory) may continue to have a crucial role, if it will be suitably revised and immersed into the new technological and applicative perspectives. For instance, much effort is being presently put in providing intelligent answers to simple queries, see the numerous semantic Web proposals and, among them, the Knowledge Graph of Google. The database community has significant expertise in declarative data processing and how to do it efficiently as well as to make it scale. The community should apply its expertise to the design of more intelligent and more efficient future "Big Data" platforms.</p><p>Since recently, Google provides an interesting solution, the Google Search Appliance (GSA), to access public and private contents, such as emails and database tuples, that cannot be directly browsed by the search engine. To this end, so-called connectors extend the reach of the GSA to non-Web repositories <ref type="bibr" target="#b18">[18]</ref>. For instance database tuples can be materialized as XML documents by ad-hoc connectors and, afterwards, transformed into HTML documents for the search. A database connector can be thought of as a simple exchange data setting for posting data on the Web.</p><p>Inspired by this solution, we next shape our vision on data posting, that is: publishing of database contents that can be enriched by supplying additional concepts <ref type="bibr" target="#b32">[32]</ref>. Let us first stress that data posting as the process of enriching content while publishing them is nowadays very popular in social networks -in particular is is the main strength of the Twitter's success. Since Twitter's debut more than seven years ago, more than 200 million people now use the site and a billion of tweets are presently sent every two and half days. A tweet is a short message typically composed by a URL (a reference to existing data or news) and of a number of hashtags (key words adding values to referenced data) so that tweeting can be thought of as a social network example of data posting.</p><p>Data exchange <ref type="bibr" target="#b19">[19]</ref> is the problem of migrating a data instance from a source schema to a target schema such that the materialized data on the target schema satisfies a number of given integrity constraints (mainly inclusion and functional dependencies). The target schema typically contains some new attributes that are defined using existentially quantified variables: the main issue is to reduce arbitrariness in selecting such variable values.</p><p>Recently a different approach to data exchange has been proposed in <ref type="bibr" target="#b33">[33]</ref> that considers a new type of data dependency, called count constraint (an extension of cardinality constraint), that prescribes the result of a given count operation on a relation to be within a certain range. We illustrate this approach by means of an example. Consider a source relation scheme S with the following attributes: I (Item), B (Brand), P (Price). The target scheme is the relation scheme T with attributes: I, B, P, W (Warehouse), C (product Category), R (price Range). We assume that the domains of I, B and P for T are the projections of the source relation S on the respective attributes, e.g., DB = πB(S). Also the domains of the other attributes of T are finite and are defined by supplementary source relations: DW (the list of all available warehouses for storing items), DC (a 2-arity relation listing the category for each product) and DR (a 3-arity relation listing all price ranges together with the interval extremes). The mapping is defined as follows (as usual, lower-case and upper-case letters denote variables that are respectively universally and existentially quantified -in addition, dotted letters denote free variables used for defining sets):</p><p>(1):</p><formula xml:id="formula_0">S(i, p, b) ∧ DC(i, c) ∧ DR(r, p, p) ∧ (p ≤ p &lt; p) → 1 ≤ #({ Ẅ : T (i, p, b, Ẅ, c, r)}) ≤ 5 (2): T (_, _, b, w, _, _) → #({ Ï : T ( Ï, P, b, w, C, R)}) ≥ 5 (3): T (_, _, _, w, c, _) → #({ Ï : T ( Ï, P, B, w, c, R)}) ≥ 5</formula><p>Note that # is an interpreted function symbol for computing the cardinality of a set.</p><p>The values of the new attributes C and R are univocally determined by the domains DC and DR, whereas the values for the attribute W may be arbitrarily taken from the domain DW provided that the following count constraints are satisfied: (1) every item must be stored into at least one and at most 5 warehouses, (2)-(3) if a warehouse stores an item of a given brand (resp., category) , it must store at least other 4 items of the same brand (resp., category).</p><p>In the example, we added values derived from a sort of "ontology" for the classification of products and of prices. We have also added the attribute W (warehouse) together with its domain, but in this case the values for the attribute are not predetermined by the available domain values but are selected on the basis of the constraints. We point out that the issue of inventing new values to be included into the target relation is one of the goals of classical data exchange setting. The main difference with data posting is the focus: to preserve the relationships with the source database, classical data exchange only considers dependencies delivering universal solutions, whereas we look for more expressive constraints for enriching the contents of the exchanged data at the cost of loosing certainty. As witnessed by data mining applications, the process of knowledge discovery is inherently uncertain.</p><p>Let us now formulate the data posting problem. A data posting setting (S, D, T, Σst, Σt) consists of a source database schema S, a domain database scheme D, a target flat fact table T , a set Σst of source-to-target count constraints and a set Σt of target count constraints. The data posting problem associated with this setting is: given finite source instances IS for S and ID for D, find a finite instance IT for T such that IS, ID, IT satisfies both Σst and Σt.</p><p>The main difference w.r.t. classical data exchange is the presence of the domain database scheme that stores "new" values (dimensions) to be added while exchanging data. In a motto we can say that "data posting is enriching data while exchanging them". Another important peculiarity of data posting is the structure of the target database scheme. We assume that it consists of a unique relation scheme that corresponds to a flat fact table as defined in OLAP analysis -we recall that an OLAP system is characterized by multidimensional data cubes that enable manipulation and analysis of data stored in a source database from multiple perspectives (see for instance <ref type="bibr">[6]</ref>).</p><p>A fact table is relation scheme whose attributes are dimensions (i.e., properties, possibly structured at various levels of abstraction) and measures (i.e., numeric values). In addition to a fact table, an OLAP scheme typically includes other tables describing dimension attributes (e.g., star or snowflake scheme <ref type="bibr">[6]</ref>). We instead denormalize all tables into a unique flat fact table in order to comply with search engine strategies -string selection queries are easier to express on denormalized tables and can be massively parallelized on them.</p><p>Perhaps, after almost thirty years, the time has really come for revenge of the Universal Relation <ref type="bibr" target="#b29">[29]</ref>. In 2004 Alberto Mendelzon claimed that the universal relation is an idea re-invented once every 3 years but it is still unclear whether it is eventually the winner of its war. The serious problem of handling null values can be now solved by column-oriented databases: they can simply be left out so that the representation can be made compact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">New Research Lines</head><p>In our example we have used count constraints to perform an elementary task of grouping items into warehouses on the basis of their categories and brands. Grouping objects is the goal of two important data mining techniques: clustering and classification. A first research line is to include some features of these techniques in the data posting setting. This is coherent with our ambitious goal of posting data with knowledge value added.</p><p>In the example we have also used hierarchy domains C and R to classify items. We dared to say with a bit of shame that the two domains represent a sort of ontology. A second line of research is to add "real" ontology tools to the data posting setting. We conclude by evidencing some possible interesting relationships between data posting and data integration. It is known that a target instance need not be materialized in data integration; the main focus there is on answering queries posed over the target schema using views that express the relationship between the target and source schemata <ref type="bibr" target="#b27">[27]</ref>. Data posting can be thought of as a bottomup enriched view directly provided by information source experts, which should be integrated with the classical top-down local view designed by the data integration administrator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BIG DATA AND PRIVACY</head><p>"Data mining" is a term that used to be bad, then it was good, now it's bad again. So we have coined the term "big data" to represent the benign side of data mining. Recall that "data mining" was a term used by Statisticians to mean roughly drawing conclusions from data that weren't supportable by the statistics. It was something to avoid. But Computer Scientists and others started using the term to mean extracting useful (and presumably valid) conclusions from data. Then, "data mining" started to be associated with things that should not be extracted from the data. People started to worry about data mining being used for purposes they regarded as improper. The principal concerns:</p><p>1. Mining by companies such as Google in order to decide what ads to show you.</p><p>2. Mining by governments, especially the US government to help discover terrorist activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">I Don't Care if My Toaster Sees Me Naked</head><p>That phrase has been attributed to me, and although I don't remember saying it in public, I do agree with it, so here it is. What it is trying to say is that there is a difference between people knowing something about me and machines "knowing" something about me. People tend to confuse the two. So we hear people raising concerns when Google or other companies pitch you an ad for something that they believe you want because you wrote an email about it, or looked for it in a Web search. There are many voices raised against the fact that the US National Security Agency is storing all sorts of communication, including phone calls and emails. In each case, the argument is that somehow, one's privacy is being violated by machines owned by Google or NSA.</p><p>One could argue that the right to privacy is a social construct, and that it changes from time to time, and that it needs to change given the new technological capabilities. I do not believe that there is a meaningful breach of privacy, in the currently accepted sense, when machines, and only machines, have access to your data. There is a risk, so far unrealized, that the data we are collecting will be misused, and I'll address that after examining the benefits of collecting and using massive amounts of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Applications in Advertising</head><p>First, let us remember, there is an algorithm being executed by Google servers that decides what ad to show you. No human sees the data on which the decision is based. I have heard it argued that Google needs to have humans look at at least a sample of the decisions made by its algorithm in order to verify that the algorithm is performing correctly. It is possible they do so, although it is not needed. Simply measuring the changes in click-through rate when the algorithm changes tells them which of several proposed algorithms is better. This ability is but one example of the "big data" phenomenon: if you have enough data, you can get statistically valid conclusions that could not be obtained from small samples.</p><p>Further, in exchange for the risk, that I think is negligible, of a privacy breach, there is significant value in companies learning what they can about their users. It would be great if Google and other companies would give away their services for free and not show me ads at all. But an economy doesn't work that way, and I doubt it ever could. So the next best thing is for me to see ads that have a high probability of being relevant to me. In the old days, advertising on television, in newspapers, or in magazines was relatively untargeted. Largely they still are, although at some point specialty magazines were used to do some targeting. For example, an ad for golf clubs in Golf Digest was worth about 10 times what the same ad in Time Magazine was worth.</p><p>But modern ad-targeting technology can increase the value of an ad by a much larger factor than 10. The increase has many benefits. It enables the showing of fewer ads, makes the ads more useful, and it supports a variety of free services that would not be available otherwise. For example, I'm writing this article using Google docs.</p><p>There are no ads, just free service.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combating Terrorism</head><p>The world has become a much more dangerous place. The existence of private organizations willing to kill randomly to further their view of the world, and then to hide among innocents to avoid being attacked in turn, presents a challenge that was not present a few decades ago. The terrorist of the early 1900's needed a bomb the size and weight of a bowling ball to do much harm. Today's terrorist can bring down an airplane with a bomb the size of a cigarette pack if we let him. We need equivalent advances in technology to prevent such mayhem from proliferating. While that doesn't necessarily justify the use of data mining to detect bad actors, I have not heard any serious proposals other than that for containing the danger. I would love to hear ideas other than "let's surrender to the will of the most evilly inclined among us."</p><p>It has been argued, even by those who are not sympathetic to terrorism that the same data-gathering technology can be used to catch those who violate even the most minor of laws. While that is probably true, any country with a modern legal system has the means to exclude as evidence information gathered by data mining without prior approval of the court. It could be debated whether datamining technology should be used to combat criminals of other sorts. For example, would you be pleased or angry if a serial killer was caught by searching for people who used a credit card near the locations of each murder at about the same as the murders occurred? What about catching drug users? Jaywalkers? But wherever you stand on the question of what crimes are worthy of tracking using modern data-mining technology, we should note that for all the public concern, no instance of the use of this technology has been claimed, outside the claim of a number of terrorist plots inhibited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Mitigating the Risks</head><p>There are three arguments I have heard against using all the capabilities offered by data-mining technology. I will argue briefly that none of the three are compelling reasons not to derive the benefits discussed above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">People are fond of quoting the comment of Benjamin</head><p>Franklin that "people willing to give up essential liberty for some temporary security deserve neither liberty nor security." I don't dispute this point, but what I question is whether we are giving up essential liberty, or even any liberty at all. Do we have the right to prevent machines from reading our electronic communication? Is that right, if it exists, "essential"? Of course pervading the whole discussion is the question of what happens when machines reading email turns into people reading email, and that is the thrust of the other two objections.</p><p>2. There is a concern that people suspected incorrectly of terrorism will receive a "visit in the night." This possibility is real, but is something that happens frequently, somewhere, and courts are already equipped to deal with false or erroneous accusations. You may have heard of the case of a person who sent ricin-laced letters to Pres. Obama and several other government figures. At first, someone we'll call A was accused of the crime and taken into custody. After a few weeks of investigation, it was found that it wasn't A at all, but his former friend B who had attempted to frame A. So A was released and B taken into custody. But that too was wrong.</p><p>It now appears that it was B's estranged wife who was trying to frame B by making it look like he had tried to frame A. The point is that law enforcement sometimes gets it wrong. I wish it didn't happen, but it does, and somehow society continues to function. I suspect it would continue to function even if a small fraction of such instances were generated by erroneous, machine-assisted intelligence gathering.</p><p>3. Perhaps the most compelling concern is that someone working for either the government or a corporation will use what they find for blackmail. It was put to me that I would not want to receive a call from a CIA analyst saying "We noticed that you bought a pressure cooker and a box of nails this month, so we thought you were a terrorist. But you'll be happy to note that on further investigation, we saw that was not the case. We did however, notice that you are cheating on your Wife. Oh, and on an unrelated note, my Nephew is applying for admission to Stanford, and we really hope he gets in." In fact, you could imagine a similar call from someone at at Internet company. It's a real risk, but it is not a risk that we have never faced before. We give people -soldiers, police, etc., -guns and tell them to use the guns to protect us but not to blackmail us. And there is an established legal system and social norms that prevent such misuse. It is possible that there will be attempts at misuse of the data-mining capabilities. But we'll eventually get to a state where analysts know they have the same responsibility not to misuse information in the same way police or soldiers know not to misuse their weapons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we have proposed a overview of main research problems and future research directions in the context of three challenging aspects of actual and future Big Data research, namely OLAP over Big Data, Big Data Posting, and Privacy of Big Data. Our study revealed that while many correlations with previous research efforts in massive databases representation and processing exist, there is still lot of work to do towards making Big Data a reliable and effective component of next-generation information systems. We firmly hope our work will turn to be a milestone under this challenging road.</p></div>		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hadoopdb: An architectural hybrid of mapreduce and dbms technologies for analytical workloads</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abouzeid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bajda-Pawlikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rasin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Silberschatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="922" to="933" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Big data and cloud computing: current state and future opportunities</title>
		<author>
			<persName><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Abbadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="530" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effectively and efficiently designing and querying parallel relational data warehouses on heterogeneous database clusters: The f&amp;a approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bellatreche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Benkrid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Database Manag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="17" to="51" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Inside &quot;big data management&quot;: ogres, onions, or parfaits?</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">R</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scalable sql and nosql data stores</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cattell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="12" to="27" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An overview of data warehousing and olap technology</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Dayal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph olap: a multi-dimensional framework for graph data analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Interactive analytical processing in big data systems: A cross-industry study of mapreduce workloads</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Alspaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1802" to="1813" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mad skills: New analysis practices for big data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dunlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Welton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1481" to="1492" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Retrieving accurate estimates to olap queries over uncertain and imprecise multidimensional data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="575" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Event-based lossy compression for effective and efficient olap over data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakravarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Knowl. Eng</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="678" to="708" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A distributed system for answering range queries on sensor network data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Furfaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Masciari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Mazzeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saccà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PerCom Workshops</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="369" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchy-driven compression technique for advanced olap visualization of multidimensional data cubes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saccà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Serafino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DaWaK</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="106" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantics-aware advanced olap visualization of multidimensional data cubes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saccà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Serafino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJDWM</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Analytics over large-scale multidimensional data: the big data revolution! In DOLAP</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cuzzocrea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="101" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The cgmcube project: Optimizing parallel data cube generation for rolap. Distributed and Parallel Databases</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K H A</forename><surname>Dehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eavis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rau-Chaplin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="29" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Getting the most from your google search appliance</title>
		<author>
			<persName><forename type="first">G</forename><surname>Documentation</surname></persName>
		</author>
		<ptr target="https://developers.google.com" />
	</analytic>
	<monogr>
		<title level="m">Google Developers Site</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data exchange: getting to the core</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Kolaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Popa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="174" to="210" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Privacy and integrity are possible in the untrusted cloud</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blankstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Felten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="73" to="82" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub totals</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Layman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Venkatrao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pirahesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="29" to="53" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Policy enforcement framework for cloud data management</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Hamlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kagal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kantarcioglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Implementing data cubes efficiently</title>
		<author>
			<persName><forename type="first">V</forename><surname>Harinarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Starfish: A self-tuning system for big data analytics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">B</forename><surname>Cetin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIDR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Specifying olap cubes on xml data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Møller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="255" to="280" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The performance of mapreduce: An in-depth study</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="472" to="483" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Data integration: A theoretical perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lenzerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="233" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Big privacy: protecting confidentiality in big data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Crossroads</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="23" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the foundations of the universal relation model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Vardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="308" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Big data: New opportunities and new challenges</title>
		<author>
			<persName><forename type="first">K</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="22" to="24" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>guest editors&apos; introduction</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Privacy-preserving fine-grained access control in public clouds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nabeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bertino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Data posting: a new frontier for data exchange in the big data era</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saccà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Serra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>AMW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Count constraints and the inverse olap problem: Definition, complexity and a step toward aggregate data exchange</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saccà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guzzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FoIKS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="352" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Discovery-driven exploration of olap data cubes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="168" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ameliorating memory contention of olap operators on gpu processors</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sitaridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="39" to="47" />
			<pubPlace>DaMoN</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient computation of the skyline cube</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="241" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An efficient quasi-identifier index based approach for privacy preservation over incremental data sets on cloud</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nepal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="542" to="555" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph cube: on warehousing and olap multidimensional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="853" to="864" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
