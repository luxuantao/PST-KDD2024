<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stream Chaining: Exploiting Multiple Levels of Correlation in Data Prefetching *</title>
				<funder ref="#_G873JEd">
					<orgName type="full">EC</orgName>
				</funder>
				<funder ref="#_5mnydhU">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Pedro</forename><surname>D?az</surname></persName>
							<email>pedro.diaz@ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcelo</forename><surname>Cintra</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stream Chaining: Exploiting Multiple Levels of Correlation in Data Prefetching *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>B.3 [Memory Structures]: Design Styles Data Prefetching</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data prefetching has long been an important technique to amortize the effects of the memory wall, and is likely to remain so in the current era of multi-core systems. Most prefetchers operate by identifying patterns and correlations in the miss address stream. Separating streams according to the memory access instruction that generates the misses is an effective way of filtering out spurious addresses from predictable streams. On the other hand, by localizing streams based on the memory access instructions, such prefetchers both lose the complete time sequence information of misses and can only issue prefetches for a single memory access instruction at a time.</p><p>This paper proposes a novel class of prefetchers based on the idea of linking various localized streams into predictable chains of missing memory access instructions such that the prefetcher can issue prefetches along multiple streams. In this way the prefetcher is not limited to prefetching deeply for a single missing memory access instruction but can instead adaptively prefetch for other memory access instructions closer in time.</p><p>Experimental results show that the proposed prefetcher consistently achieves better performance than a state-of-the-art prefetcher -10% on average, being only outperformed in very few cases and then by only 2%, and outperforming that prefetcher by as much as 55% -while consuming the same amount of memory bandwidth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>A well known performance bottleneck is the so-called memory gap (or wall), which refers to the speed difference between micro-processor execution and memory access latency, typically in the order of a few hundred processor cycles nowadays. In the past, semiconductor and microarchitectural advances allowed microprocessors to increase operating frequency at a steady rate, while the most significant advances in memory technology had been in density, not speed. While recent semiconductor scaling trends have greatly reduced the previous acceleration pattern of the memory gap, the gap as seen by microprocessors is still likely to continue increasing. In fact, fast increasing microprocessor operating frequencies have now been exchanged for fast increasing number of microprocessor cores per chip. This has had a similar adverse effect on the performance of the memory subsystem. On the one hand, wire latencies and power constraints limit significant increases in the size of on-chip caches, such that these are expected to increase sub-linearly with the number of cores. On the other hand, packaging limitations and signal delays to off-chip also lead to an expected sub-linear increase in memory bandwidth with respect to the number of cores. Both of these effects combined put increased pressure on the memory subsystem and increase the average off-chip access times as observed by the cores.</p><p>Hardware prefetching has long been a successful technique to help overcome the memory gap. Current hardware prefetching algorithms attempt to predict future processor memory accesses by keeping a history of recent cache misses. Prefetchers have to deal with several challenges, which include: 1) entropy of information in the miss stream leading to limited predictability; 2) irregularity in the miss stream due to order changes in the misses and spurious, infrequent, misses leading to further reduction in predictability; 3) variation in the time distance between misses leading to timeliness issues: prefetches may arrive too late, thus not completely hiding the miss latency, or too early, thus replacing other useful data from the cache or prefetch buffer; and 4) physical limitations of hardware implementations exacerbating the problems above. A common way to cope with these challenges is to divide the global miss address stream into multiple streams -a process we call localization <ref type="foot" target="#foot_0">1</ref> . With localization, misses are grouped according to some property and the expectation is that the resulting streams are more predictable than the original global stream. Once the localized streams have been generated, various mechanisms for detecting patterns in them can be used, such as strides, deltas, Markov chains, and even straightforward repetition of a sequence of addresses; the choices of the localization approach and the pattern correlation mechanism are mostly orthogonal. For instance, common stride prefetchers <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b5">4]</ref> and the predictor in <ref type="bibr" target="#b13">[12]</ref> separate the stream of misses according to the PC of the missing memory access instruction, which we call PC localization and is an instance of execution context localization. The predictors in <ref type="bibr" target="#b14">[13]</ref> and <ref type="bibr" target="#b20">[19]</ref> separate the stream of misses according to memory address ranges of the misses, which we call spatial localization. The predictor in <ref type="bibr" target="#b23">[22]</ref> groups misses according to their appearing together within a time period, which we call temporal localization.</p><p>Ideally, the choice of property used for localization exploits some inherent property of programs and data access patterns that lend the resulting streams more predictable. For instance, PC localization exploits the fact that different static memory access instructions perform different functions from their neighbor instructions in both execution time and instruction address space: e.g., the load of a "p-&gt;next" type of operation in a list traversal and the loads/stores used in the neighboring computation. Similarly, spatial localization exploits the fact that regular datastructures are commonly laid out in a continuous portion of the address space and are usually accessed in some regular pattern, possibly different from the pattern used to access data that is used at around the same time but belonging to other datastructures. Unfortunately, localization creates two problems: i) most forms of localization break the chronological order of misses, possibly leading to timeliness problems and ii) the resulting streams may be too short, possibly leading to uncovered misses across stream boundaries and also to timeliness problems. For instance, the misses from a PC localized stream may be interleaved with many misses from other PC localized streams so that prefetches are issued too early in time displacing useful data from the cache. Similarly, the misses from a space localized stream may be interleaved with some misses to addresses outside its address range. Short streams make it difficult to exploit deep and wide correlation patterns such as Markov chains.</p><p>In this paper we propose the idea of chaining localized streams in order to both partially reconstruct the chronological information in the miss stream and to build longer streams, while maintaining a higher miss prediction accuracy derived from localization. In particular, we apply this idea -which we call Stream Chaining -to a PC localized prefetcher and we show one scheme for chaining streams, which we call Miss Graph prefetching. We show that this scheme captures well common application behavior and we show that it can be easily implemented on a simple extension to the Global History Buffer (GHB) <ref type="bibr" target="#b13">[12]</ref>, which is a convenient structure to implement various prefetching algorithms. The paper focuses on the lowest level on-chip data cache, as this is the last level before hitting the memory wall. We evaluate the proposed prefetcher with benchmarks from the SPEC 2006 and BioBench suites and compare it against both a state-of-the-art PC based localization prefetcher -PC/DC <ref type="bibr" target="#b13">[12]</ref> -and its non-localized counterpart -G/DC <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b13">12]</ref>. Experimental results show that the proposed prefetcher consistently achieves better performance than PC/DCit is only outperformed in very few cases and then by only 2%, and it outperforms PC/DC by as much as 55% while consuming the same amount of memory bandwidth. This is due to much improved accuracy and timeliness of the prefetches issued.</p><p>The rest of the paper is organized as follows. Section 2 discusses prefetching algorithms, and in particular localization, and introduces the GHB data structure. Section 3 introduces the concept of stream chaining and presents the miss graph prefecher which is based on it. Section 4 describes the evaluation methodology and Section 5 presents experimental results. Related work is discussed in Section 6 and conclusions appear in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">OVERVIEW OF PREFETCHING MECHANISMS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Miss Localization and Correlation</head><p>A common strategy for building a data prefetching algorithm is to keep a history of past memory accesses. Prefetchers then use a correlation algorithm to predict the next memory reference based on this history. However, having a single global history of past memory accesses is not usually effective: program complexity, different execution phases, and random non-predictable memory accesses degrade the quality of the prefetching predictions. To overcome these problems, current state-of-the art prefetchers use a technique known as miss stream localization.</p><p>With miss stream localization past memory accesses (cache misses) are classified into groups according to a pre-set criterion. These groups, known as address streams, are then used to make predictions about future memory accesses. Popular options for miss stream localization include grouping misses according to the PC of the instruction that generated them <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b13">12]</ref>, or to the region of memory they reference <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b20">19]</ref>, or to some period of time in which the misses occur <ref type="bibr" target="#b23">[22]</ref>, or even to previously executed branch instructions <ref type="bibr" target="#b22">[21]</ref>. Figure <ref type="figure" target="#fig_0">1</ref> depicts a simple example of a global miss address stream and how it can be localized following PC, spatial, and temporal localization. With memory access instruction PC localization, upon a miss caused by a certain memory access instruction only previous miss addresses generated by the same instruction are used to predict the address(es) to prefetch. This localization scheme thus attempts to uncover and exploit some PC correlation present in the application memory access pattern. Thus, as depicted in the figure, the stream of P C A will lock onto its access pattern, regardless of where in the address space its data is located and regardless of what other misses appear concomitantly with its misses. However, the PC localized prefetch predictor in this figure will not realize that a miss to A2 tends to happen after a miss to A1. With spatial localization, upon a miss to a data address within some address range region only previous miss addresses generated to the same range are used to predict the address(es) to prefetch. This localization scheme thus attempts to uncover and exploit some spatial correlation present in the application memory access pattern. Thus, as depicted in Figure <ref type="figure" target="#fig_0">1</ref>, the stream of misses to the address region at the bottom of the address space -A11 to A14 -will lock onto its sequential access pattern, regardless of what memory access instructions are used to perform the accesses and regardless of what other misses appear concomitantly with its misses. However, the spatial localized prefetch predictor in this figure will not realize that a miss to A1 tends to happen after A12 and may not even realize that an access to A13 does not really occur in practice. Finally, with temporal localization, upon a miss only previous miss addresses generated at around the same time as the previous occurrence of this miss are used to predict the address(es) to prefetch. This localization scheme thus attempts to uncover and exploit some temporal correlation present in the application memory access pattern. Thus, as depicted in the figure, the stream of misses to A4 and A6 may be detected regardless of what memory access instructions are used to access them and regardless of where they are physically located in the data address space. However, the temporal localized prefetch predictor in this figure may not realize that misses to both A7 and A11 tend to follow shortly after misses to A1, even though there might be some alternating pattern to it. While the example in this figure is simple and somewhat artificial, it demonstrates both that there is less entropy in the localized patterns than in the global miss stream and that different localization approaches excel and struggle at different access and data layout patterns.</p><p>Once the address stream has been localized somehow, prefetchers use correlation algorithms to try to find patterns in past memory accesses. A simple correlation algorithm, known as address correlation, searches the history looking for other occurrences of the latest miss or group of misses. If such repetitions (correlations) are found, then the algorithm prefetches the next address recorded after the correlation. A more refined correlation algorithm known as delta correlation calculates differences (deltas) between the latest</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Correlation Temporal Correlation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miss stream (PC:Addr)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line accessed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Line not accessed</head><p>Space localized streams:</p><formula xml:id="formula_0">A1 -&gt; A2 A1 -&gt; A2 -&gt; A4 A7 -&gt; A8 A11 -&gt; A12</formula><p>PC localized streams:</p><formula xml:id="formula_1">A1 -&gt; A7 -&gt; A1 -&gt; A11 -&gt; A1 -&gt; A7 A2 -&gt; A8 -&gt; A2 -&gt; A12 -&gt; A2 -&gt; A8</formula><p>Time localized streams: miss addresses and searches for similar differences in the past miss history <ref type="bibr" target="#b10">[9]</ref>. Finally, once a correlation has been made, one or more prefetch requests are made for the particular miss event. The number of prefetch requests issued per miss event is known as the prefetch degree. The greater the prefetch degree the more future misses that will be possible to avoid. On the other hand, if the prefetch degree is too large issues such as bandwidth saturation, cache pollution, and increased inaccuracies commonly degrade performance.</p><formula xml:id="formula_2">A1 -&gt; A2 -&gt; A7 -&gt; A5 -&gt; A8 A1 -&gt; A2 -&gt; A7 -&gt; A8 A1 -&gt; A2 -&gt; A4 -&gt; A6 -&gt; A11 -&gt; A12</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Address Space</head><p>Based on the mostly orthogonal stages of localization and correlation, a taxonomy to classify prefetching algorithms was proposed in <ref type="bibr" target="#b13">[12]</ref>. In this taxonomy, each algorithm is denoted as a pair X/Y, where X denotes the method used to localize the miss address stream and Y is the algorithm used to correlate memory accesses within a stream. For instance, a simple stride prefetcher with PC based localization and constant stride correlation would be referred to as PC/CS, and the best prefetcher in <ref type="bibr" target="#b13">[12]</ref> uses PC based localization and address delta correlation and is then referred to as PC/DC. A prefetcher that does not localize the miss address stream in any way is referred to as global, so that for instance the Markov predictor of <ref type="bibr">[8]</ref> is referred to as G/AC (AC stands for address correlation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The Problem of Accuracy and Timeliness in Localized Prefetchers</head><p>From the discussion in the previous section it is clear that each localization method has advantages and disadvantages. Three criteria are normally used to characterize the performance of a prefetching algorithm: coverage, accuracy, and timeliness. Coverage is defined as the ratio of issued prefetches to the total number of misses produced in a non-prefetching environment. Accuracy measures the ratio of useful (i.e., eventually used by the program) prefetches to the total number of prefetches issued. The third criterion, timeliness, is more difficult to measure. A prefetch request is normally considered to be timely if it brings in the data in time before it is needed. However, a more comprehensive classification of timeliness also takes into account whether data is prefetched too early, as this may create cache pollution by evicting other data that is still needed.</p><p>Schemes based on localizing streams according to the PC of memory access instructions that generate the misses can potentially lead to higher performance gains than those that treat all misses as a single global stream <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>, although this varies according to individual applications. Their key advantage is that they can take into account the execution context and focus their correlation algorithms on streams of predictable misses from individual memory access instructions. They can then filter out less predictable misses and spurious misses that appear in the global stream due to variable interleavings of misses.</p><p>It is important to note that entries in the same PC localized miss stream are ordered chronologically, but two chronologically consecutive misses will not necessarily belong to the same miss stream. Thus, if a prefetching algorithm issues a number of prefetching requests based on a PC localized stream, each prefetched data item for a dynamic instance PC n i is expected to be used in following instances PC n+1 i to PC n+d i , where d is the degree of prefetching, and before memory accesses from some other PCj<ref type="foot" target="#foot_1">2</ref> . However, an indeterminate number of memory accesses can appear between the prefetch trigger at PC n i and the use of the prefetched data by PC n+1 i to PC n+d i , as long as these requests are not classified in the same localized stream of PCi.</p><p>Naturally, the problem mentioned above is worse with larger prefetch degrees. Unfortunately, to increase coverage and amortize long memory access times, these prefetchers often resort to large degrees of prefetching <ref type="bibr" target="#b13">[12]</ref>. The final result is that this leads to two unfortunate behaviors. First, prefetching a large number of lines for the same static memory access instruction may lead to some of these being incorrectly prefetched, either because the pattern changes or because not as many lines are actually required. This, in turn, leads to reduced accuracy for the deeper prefetch requests. Second, by expecting to hide long memory access latencies the prefetcher may err by being too aggressive and prefetching too far in advance. Such prefetched data brought to the cache in an untimely (i.e., premature) manner can evict data that is needed before it. Additionally, prefetched data brought too early to the cache may itself be evicted before it is used. Both problems result in a waste of memory bandwidth which may result in application slowdown compared to non-prefetching setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">The Global History Buffer</head><p>Many (hardware) data structures can be used to implement different prefetching algorithms, but <ref type="bibr" target="#b13">[12]</ref> has proposed a common data structure that is flexible enough to allow efficient implementation of a number of prefetching algorithms. This structure, called the Global History Buffer (GHB), is a FIFO-like structure that stores past cache miss addresses in chronological order. Each entry in the GHB also contains a pointer that allows GHB entries to be connected in time-ordered linked lists forming the different localized miss streams. A global Head Pointer points to the head of the GHB. Additionally, an Index Table (IT) is used to access the most recent address for each stream. The IT is indexed using a key appropriate for the localization scheme used (e.g., the PC of the memory access instruction that generated the miss), which allows for the implementation of different localization schemes. Figure <ref type="figure">2a</ref> shows an example GHB for the PC/DC prefetcher <ref type="bibr" target="#b13">[12]</ref>. A detailed description of a hardware implementation of delta correlation using the GHB can be found in <ref type="bibr" target="#b14">[13]</ref>. In Section 3.2 we show how the same structure can be used to implement our proposed prefetcher. The GHB offers three important advantages over other methods to store miss history, such as tables: reduction of stale data, a more complete history, and lower storage requirements <ref type="bibr" target="#b13">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">STREAM CHAINING</head><p>In this section we present the stream chaining approach to data prefetching, which adds a third level of design choice to prefetching schemes. We start by presenting the general idea in Section 3.1. In Section 3.2 we present one specific prefetcher that uses the stream chaining approach, namely the Miss Graph prefetcher. To accommodate the new level of operation, we extend the taxonomy of <ref type="bibr" target="#b13">[12]</ref> with a third term, so that prefetching schemes are denoted by a triple X/Y/Z, where X and Y are as before, and the new term Z denotes the method used to link streams into groups. We use MG to refer to our proposed prefetcher of Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>As already mentioned, the main problem with PC localized prefetching schemes is that there is no chronological information relating the streams (the same is true of other localization schemes, such as the address region localization of <ref type="bibr" target="#b14">[13]</ref>). This can lead to prefetches sometimes being triggered along streams whose memory accesses appear too far apart in time, leading to untimely (premature) prefetches. While the GHB does contain the total timing relation among individual memory accesses of all streams, this information is not in a format that can be readily used. The key to reconstructing appropriate timing information across streams is to provide a suitable stream chaining algorithm.</p><p>Our proposal is to link streams in a way that partially reconstructs the chronological information in the miss stream, such that the result corresponds directly to the common path of misses followed by the application. Note that in this way, what is reconstructed are sequences of streams, which are different from the complete sequence of individual misses that is found in the complete global miss stream. Another way of thinking about stream chaining is that it attempts to predict what miss stream will be activated next in program order. In this way, a three-level prefetcher with stream chaining can predict not only the expected next misses from the current memory access instruction but also the expected next misses from the expected next memory access instructions to miss in program order. Thus, such a prefetcher has an extra level of flexibility and can adapt to situations both in which missing memory accesses from the same static instruction are too far apart and in which missing accesses from static instructions consecutive in program order are too near. This additional level of adaptiveness can potentially improve both timeliness and accuracy with respect to traditional deep two-level prefetching.</p><p>Based on our empirical results, the flow of missing memory access instructions commonly follows stable and repeatable patterns. These patterns can be represented by a directed graph where nodes correspond to static memory access instructions (i.e., streams) and edges correspond to links between two streams, indicating that a miss in one stream is likely to be followed by a miss in the other stream. Figure <ref type="figure">2b</ref> shows the localized streams and one possible chaining of them for the complete miss stream and GHB state in Figure <ref type="figure">2a</ref>. While simple in nature, generating graphs that represent the core flow of missing memory access instructions and excludes spurious misses from memory access instructions, either in infrequent control paths or that generate only occasional misses, is not trivial and is the key to a good stream chaining prefetcher. In this example linking the streams of PC A and PCB could be deemed inappropriate by the algorithm, as it corresponds to an infrequent flow of misses (Figure <ref type="figure">2</ref> does not show explicitly a stream for PCB to keep the figure small, but it is assumed that this instruction does ex-ecute and cause misses occasionally). Note that the resulting graph does not then contain all the possible links from the total miss sequence information in the global miss stream, but only a carefully selected set of those. Constructing the complete graph is both impractical from the point of view of storage and processing time and yields too much information that can distract the algorithm from finding the most likely miss patterns.</p><p>The fact that the memory access chaining algorithm cannot and should not keep all links results in the graphs being disconnected or some graphs being acyclic. Note that in this way, using the information about linked streams to predict the next missing memory access instruction along the link may or may not be the same as predicting the next miss. In this example, the given chaining of streams allows a prefetcher on a miss from PCA to prefetch not only the next values to be consumed by PCA itself but also the next value(s) to be consumed by PCD (the next instruction likely to miss) or even PCE. Alternatively, since PCA is in a cyclic graph, if the distance between its consecutive instances is too large then it could simply rely on a peer (such as PCD and PCE) to prefetch the data it will need next.</p><p>With the addition of stream chaining to a baseline PC/DC, a prefetcher can exploit three types of correlation in the miss stream: PC correlation (from the localization), spatial correlation (from the delta correlation), and temporal (from the stream chaining).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Miss Graph Prefetching</head><p>In this section we present one prefetcher that uses the stream chaining approach. Every stream chaining prefetcher has to deal with two implementation issues: how to link streams (Section 3.2.1) and how to use the graph to issue prefetches (Section 3.2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Graph Construction</head><p>As mentioned in the previous section, the key to effective stream chaining is the choice of which links to use and which to ignore. In fact, with a large enough GHB the complete chronological reconstruction of the miss stream would result in an unmanageable graph. In this section we present a simple scheme to stream chaining that results in relatively small graphs that capture the majority of miss sequences in steady-state execution (see Section 5.3). The scheme can also be easily implemented using the GHB structure. Naturally, this is only a first attempt at a novel class of prefetching schemes and other stream chaining mechanisms are possible, including Markov chains.</p><p>In GHB-based prefetchers the Index Table (IT) holds pointers to every localized stream in the GHB (Section 2.3). Thus, chaining streams corresponds to linking entries in the IT. To do this, we extend the IT by adding a new pointer to each IT entry -NextIT -which points to the IT entry corresponding to the stream that is expected to be activated next. The NextIT field also includes an additional bit to signal if this pointer is valid. To identify strong (i.e., stable) links, we also add a saturating counter to each IT entry -Ctr. The operation of this counter is explained next. Finally, we also add a new global register to the IT -PreviousIT -which is also a pointer to an IT entry. Figure <ref type="figure">2a</ref> shows the GHB extensions in grey.</p><p>Our miss stream chaining algorithm builds a graph of past (temporal) correlations between localized streams as follows. Initially, NextIT is invalid and Ctr is set to zero on all entries of the IT. As misses occur, the IT and the GHB are populated as described in Section 2.3 and <ref type="bibr" target="#b13">[12]</ref>. The new PreviousIT pointer is left pointing to the last stream to suffer a miss (i.e., last IT entry used). Then for a subsequent miss that activates the IT entry IT[cur], we check the previous IT entry using IT[PreviousIT] and perform the following operations:  By following these operations, the NextIT pointers in the IT form a directed graph, which can be cyclic or acyclic, can be disconnected, and in which there is only one outgoing edge from each node but possibly more than one incoming edge to a node. Figure <ref type="figure">2a</ref> shows the state of the NextIT pointers, the Ctr counters, and the PreviousIT pointer just before the miss to A4 is processed. The corresponding graph is shown in Figure <ref type="figure">2b</ref>.</p><p>The graph constructed with the mechanism described shows a history of correlations between localized miss streams, showing which IT entry followed which in the past. The role of the saturating counters Ctr is to provide hysteresis and protection from noise from sporadic misses: by setting a minimum threshold to Ctr we obtain a graph with only the most stable transitions between localized streams. Next we explain in some more detail how a prefetcher can use the extended GHB entries to prefetch along nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Prefetch Operation</head><p>With the extended entries in the IT representing the miss graph, our proposed prefetcher operates as follows. First, the prefetcher identifies the current miss stream, which simply involves searching the IT for an entry that matches the PC of the current missing memory access instruction. Here, unlike PC/DC, which would follow the IT pointer into the corresponding GHB stream, our prefetcher identifies the next stream to prefetch for by following the NextIT pointer in the current IT entry. So, for instance, a miss from a memory access instruction at PCA in Figure <ref type="figure">2</ref> will first follow the corresponding NextIT pointer to the stream of PCD. For every stream that the prefetcher attempts to prefetch for, it follows the IT pointer into the corresponding GHB stream and then follows the links in the GHB entries to attempt to establish a correlation among the miss addresses. In our PC/DC/MG prefetcher we use delta correlation on the addresses. If a correlation is found along the stream the prefetcher issues one prefetch along this stream. Thus, for each stream, PC/DC/MG behaves as a PC/DC with a prefetch degree of one. After issuing a prefetch for a given stream, the prefetcher again follows the NextIT pointer to the next stream to prefetch for and repeats the steps for prefetching for this stream. This process is repeated for a number of times equal to the prefetching degree parameter. In order to avoid following "weak" links into new streams, we impose a minimum threshold on the Ctr value below which the prefetcher will not follow the NextIT and will stop prefetching from further streams.</p><p>The graph construction we propose in Section 3.2.1 leads to graphs where the outgoing edge degree is no greater than one and graphs can be cyclic. Thus, starting from some node, the chains with degree nodes created by following the operations just described are either a linear sequence of distinct nodes or a cycle. Further, the linear sequences can either have a number of distinct nodes greater than or equal to the degree of prefetching plus one, or a number of distinct nodes smaller than the degree plus one (the "plus one" comes from the fact that we skip the initial node). Figure <ref type="figure" target="#fig_2">3</ref> shows the three possible cases of graphs.</p><p>For graphs as in Figure <ref type="figure" target="#fig_2">3a</ref> the operation of the prefetcher as described above is complete. For the other two cases of graphs the operation must be slightly extended. For graphs as in Figure <ref type="figure" target="#fig_2">3b</ref> if we want to issue as many prefetches as the degree allows us, we would have to follow an edge back to some streams for which we have already issued a prefetch. The problem in this case is that the prefetcher would have to remember, for every revisited stream, the correlation used the last time around and the resulting address prefetched, in order to find the next delta and to compute the next address to prefetch. Instead, a simpler solution is to add a pre- pass stage where we quickly follow the "strong" NextIT pointers to identify whether the graph is cyclic or a long enough sequential chain. Then, if the graph is cyclic we perform the steps above except that we compute the correlations and issue more than one prefetch per stream, where each stream gets an equal share of the prefetch degree (or nearly equal when the number of nodes in the chain does not divide the prefetch degree). For graphs as in Figure <ref type="figure" target="#fig_2">3c</ref> we can simply convert them to the case of Figure <ref type="figure" target="#fig_2">3b</ref> by pretending that there is a back edge from the end of the chain to its beginning. Finally, if the graph consists of only the entry node then instead of starting from the next stream (which is unavailable) we simply prefetch for the current stream, basically reverting to PC/DC behavior. Since the prefetches for the different streams are generated in a single prefetcher activation, one optimization that is possible in the cases of Figures <ref type="figure" target="#fig_2">3b</ref> and<ref type="figure" target="#fig_2">3c</ref> is to issue the prefetches to the memory sub-system such that prefetches to consecutive streams are interleaved. Thus, for instance, in the case of Figure <ref type="figure" target="#fig_2">3b</ref> we can order the prefetch requests such that the first prefetch request for PCC appears right after the first request for PCB and the second prefetch request for PCC appears after the second prefetch request for PCB. This ordering is likely to be a better match to the order in which the prefetched data will be needed.</p><p>The proposed prefetching mechanism described above is our first attempt at stream chaining based prefetching. One possible advantage of three-level prefetchers with stream chaining is that given a fixed prefetching degree budget one can divide this budget in different ways between width -i.e., the number of streams prefetched for -and depth -i.e., the number of prefetches along each stream. For instance, if deeper prefetching gives diminishing returns due to too early prefetches or decreasing accuracy, then the prefetcher can issue fewer prefetches from more streams. Alternatively, if the links between streams are too weak then the prefetcher can issue more prefetches from fewer streams. Also, we only classify links as "strong" or "weak", but one could consider finer classifications and adapt the depth for each stream depending on the "strength" of the links followed.</p><p>As with other prefetchers <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b14">13]</ref>, in order to avoid prefetched data modifying the natural stream of misses from the program a 1bit prefetch tag is added to each cache line. This bit is set to one only in lines that come into the cache from a prefetch request and it remains set as long as the line has not yet been used. When such a line is used, a "fake" miss signal is sent to the prefetcher and the bit is reset. The prefetcher then updates its internal data structures as if it were a real miss, but no prefetch request is issued.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Hardware and Operation Complexity</head><p>As described in Section 3.2.1, our prefetcher uses an extension to the GHB structure. The additional storage our prefetcher re-quires are the NextIT and Ctr for each IT entry and a single PreviousIT register. As observed in <ref type="bibr" target="#b13">[12]</ref> and in our own experience, both an IT and a GHB with 512 entries each are sufficient to capture the prefetching working set of most applications. In this case, each NextIT and the PreviousIT consist of 9 bits, as do the other pointers in the original IT and GHB, including the Head Pointer. Our experiments show that small saturation limits for Ctr are sufficient to capture stable links between streams and we use 3 bits. Assuming a 32 bit PC the total hardware storage requirements of the original and extended IT/GHB structures are: 512 * (32 + 9) + 512 * (32 + 9) + 9 = 41, 993bits (or approx. 6KBytes) and 512 * (32 + 9 + 9 + 3) + 512 * (32 + 9) + 9 + 9 = 48, 146bits (or approx. 7KBytes). The additional storage requirement of our prefetcher is then less than 1KByte.</p><p>One drawback of GHB-based prefetchers is the time it takes to follow links to establish a correlation. With stream chaining, a prefetcher requires following links in multiple streams, which may further increase prefetcher operation time. The increase in operation time in comparison with the single-stream counterpart will depend on the common number of nodes in the miss graph, which in turn depends on the application. Our results suggest that the number of nodes is relatively small (Section 5.3). In case this overhead does become a bottleneck, we note that it is possible to search for correlations in some number of streams in parallel, at the cost of replicated hardware logic. In our experiments, however, we searched for correlations from each stream sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATION METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prefetching Algorithms</head><p>We use PC/DC <ref type="bibr" target="#b13">[12]</ref> as a representative of a modern highperformance prefetcher. This prefetcher is based on PC stream localization and was shown to consistently outperform other localized and non-localized prefetchers <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b16">15]</ref>. We also use a G/DC prefetcher in order to assess the impact of PC based localization. We evaluate our PC/DC/MG prefetcher as described in Section 3.2. With all prefetchers we vary the degree of prefetching from 1 to 16. For PC/DC/MG we use a Ctr with 3 bits and set the threshold for "strong" links at 3. All prefetchers first perform a lookup in L2 to check if the lines about to be prefetched are not already there, in which case the prefetch is dropped. In all cases, access to the GHB is exclusive and preemptive, such that a new request forces a previous request currently using the GHB to be dropped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Simulation Environment</head><p>We conduct our experiments with the SESC simulator <ref type="bibr" target="#b17">[16]</ref>. The system we simulate is a 4-issue out-of-order superscalar processor with separate L1 instruction and data caches and a unified L2 cache on chip. All caches are non-blocking. The main microarchitectural parameters are listed in Table <ref type="table" target="#tab_1">1</ref>. The 256KByte L2 cache configuration evaluated is representative of the cache share expected for a processor in a loaded multi-core environment, while the 2MByte L2 cache configuration reflects the case when a single processor is active. The access times for the different L2 cache sizes were computed using CACTI 4.2 <ref type="bibr" target="#b18">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Benchmarks</head><p>We use programs from the SPEC CPU 2006 and BioBench <ref type="bibr" target="#b1">[1]</ref> benchmark suites running the Reference data set, except clustalw for which we used hmmer's input, which is larger. Some benchmarks were left out because of the non-full-system nature of our simulation environment, which suffered from compatibility problems with libraries, run-time systems, and OS calls required by these applications. The programs were compiled with the GCC 3.4 compiler at the O3 optimization level. We fast-forward simulation for the first 1 billion instructions and we then simulate in detail and collect statistics for the next 1 billion instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overall Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Cache Size Sensitivity</head><p>Figure <ref type="figure" target="#fig_3">4</ref> shows how the relative performance (execution time) changes as the L2 cache size increases in a system without prefetching. Performance is relative to that of a system with an ideal L2 cache (i.e., 100% hit rate). From this figure we see that 4 out of the 20 benchmarks achieve performance within 10% of the ideal L2 performance for a 512KB L2 cache. These applications are less likely to benefit as much from prefetching and any such benefits could likely be alternatively achieved simply with a larger L2 cache. On the other hand, many of the other benchmarks fail to come within a reasonable margin of the ideal L2 performance even with a fairly large 2MB L2 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Prefetching Performance</head><p>As execution time improvements due to prefetching often come at the expense of increased bandwidth usage, we evaluate performance along both metrics. Figure <ref type="figure" target="#fig_4">5</ref> shows the relative performance with G/DC, PC/DC, and PC/DC/MG for prefetch degree 16 and for L2 sizes of 256KB and 2MB. The figure also shows on the right vertical axis the memory bus traffic with respect to the traffic with no prefetch. For reference we show the relative performance with no prefetching (i.e., degree 0). Performance is relative to that of a system with an ideal L2 cache.</p><p>From this figure we see that for the majority of the applications there is a clear benefit from using a prefetcher. As expected, the gains from prefetching are higher with the smaller cache size of 256KB, but even with the fairly large cache size of 2MB the applications with low relative IPC also show significant benefits from prefetching. However, with the 256KB cache size for three applications prefetching with one or more of the schemes leads to a performance degradation: gromacs and sjeng with G/DC and omnetpp with all three prefetching schemes.</p><p>Comparing the three prefetching schemes, it is clear that no scheme performs best for all applications. The following analysis is for a 256KB L2 cache. Comparing G/DC and PC/DC reveals that PC/DC outperforms G/DC in 5 of the applications -by as much as 28% in milc -G/DC outperforms PC/DC in 9 of the applicationsby as much as 59% in lbm -and they perform similarly (within 1% of each other) in the remaining 6 applications. On the other hand, comparing G/DC and our proposed prefetcher PC/DC/MG reveals that PC/DC/MG outperforms G/DC in 11 of the applications -by as much as 45% in milc -G/DC outperforms PC/DC/MG in 5 of the applications -by as much as 32% in lbm -and they perform similarly (within 1% of each other) in the remaining 4 applications. The reason why G/DC performs best in some cases is the existence of spatial (delta) correlation across misses generated by different static memory access instructions, which leads to better coverage, accuracy, or both, compared to the PC localized prefetchers. As expected, PC/DC and PC/DC/MG tend to outperform or underperform G/DC in the same applications, although in some cases PC/DC/MG outperforms G/DC even when PC/DC does not. Nevertheless, when G/DC outperforms both PC/DC and PC/DC/MG, the gap with the latter is often smaller than with the former. Finally, comparing PC/DC and PC/DC/MG reveals that PC/DC/MG outperforms PC/DC in 14 applications -by as much as 55% in libquantum -PC/DC outperforms PC/DC/MG in 4 applications -by as much as only 2% in hmmer -and they perform similarly (within 1% of each other) in the remaining 2 applications. Overall, this figure seems to indicate that PC/DC/MG shows more consistent performance across the entire range of applications than either PC/DC or G/DC. It also shows that between PC/DC and PC/DC/MG the latter seems the best performer overall. Results with the larger 2MB L2 cache show similar trends although both the relative benefits of prefetching and the relative benefits of each scheme are smaller than with the 256KB L2 cache.</p><p>Considering memory traffic, we see that the increase is often relatively small (less than 5%). However, in a few applications the traffic increase is significant. The largest increase is observed with omnetpp with increases of 108% and 123% with PC/DC and PC/DC/MG; and with a small increase of only 4% with G/DC. Despite this case, however, the traffic increase with G/DC is often larger, with the most notable cases being gromacs -with an increase of 73% with G/DC versus only 3% and 4% with PC/DC and PC/DC/MG -and h264ref -with an increase of 61% with G/DC versus 21% and 29% with PC/DC and PC/DC/MG. Finally, we note that the relative traffic increases with 2MB L2 cache are similar to those with the smaller 256KB L2 cache, although in absolute numbers the traffic with the larger cache are, obviously, smaller.</p><p>In the remaining sections we analyze the prefetching behavior in more detail in order to identify the reasons of PC/DC/MG's better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Cache Miss and Prefetch Characterization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Prefetch Coverage</head><p>To gain some insight into the behavioral differences between G/DC, PC/DC, and PC/DC/MG we compute each prefetcher's coverage, defined as the ratio of the number of used prefetches over the total number of misses without prefetching. Figure <ref type="figure">6</ref> shows the coverage versus the prefetching degree. From this figure we can see that the highest coverage varies across applications, but PC/DC/MG often offers the highest or close to highest coverage. Comparing PC/DC and PC/DC/MG, we see that the coverage of PC/DC/MG is often higher. This increased coverage can be attributed to two factors: one is the benefits of cross-stream correlations with the miss graphs and another is the improved timeliness, which means that prefetched lines are more likely to still be in the cache when needed (Section 5.2.2). For a few applications G/DC has higher coverage than either PC/DC or PC/DC/MG. This is both because by localizing the global miss stream PC/DC and PC/DC/MG may lose some opportunities to establish cross-stream correlations and because by having to observe enough misses for each stream separately PC/DC and PC/DC/MG may incur increased learning time to establish correlations. However, as will be seen in Section 5.2.2, this increased coverage almost invariably comes at the expense of much decreased prefetch accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Prefetch Accuracy</head><p>To gain further insight we compute each prefetcher's accuracy, defined as the ratio of the number of used prefetches over the total number of prefetches issued. Figure <ref type="figure" target="#fig_6">7</ref> shows the accuracy versus the prefetching degree.</p><p>From this figure we can see that in the vast majority of the cases the accuracy of PC/DC/MG prefetches is significantly higher than that of PC/DC. This difference is specially pronounced for higher prefetch degrees, which indicates that indeed PC/DC's deep prefetches tend to have lower accuracy and lead to wasted bandwidth usage. In fact, the accuracy of PC/DC/MG tends to remain stable or decrease only slightly with increasing prefetch degree while the accuracy of PC/DC tends to decrease rapidly.</p><p>Since PC/DC/MG issues prefetches in a more timely manner, the chances of these prefetches being actually used are higher than in PC/DC. In PC/DC accurate prefetches might be brought in to the cache too early and risk being evicted by some other data needed before them. PC/DC/MG tries to shorten the timespan between loading prefetched data in the cache and using it. In fact, after fur-ther investigation we found that often with larger prefetch degrees with PC/DC the fraction of prefetched lines displaced (before being used) by demand misses increases, which indicates that PC/DC's deep prefetches are not only being displaced by other prefetches but by demand misses that appear inbetween the prefetch request and its potential use. This, in turn, suggests that indeed such prefetch requests are being issued too early.</p><p>Comparatively, G/DC's accuracy is consistently lower than that of both PC/DC and PC/DC/MG, which can be linked to the often higher unpredictability of the global miss stream. Together, the coverage and accuracy numbers mostly explain the performance differences observed in Section 5.1.2. For instance, G/DC performs best for lbm because its coverage is much higher (with prefetch degree 16) despite its lower accuracy. On the other hand, in the case of milc the coverages are similar so the better performance of PC/DC/MG is mainly due to its better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Next-Stream Prediction Accuracy</head><p>In addition to final prefetch accuracy (Section 5.2.2) it is useful to know the accuracy of PC/DC/MG in predicting the next stream (i.e., memory access instruction) to generate a miss. Figure <ref type="figure">8</ref> shows how often PC/DC/MG's prediction of next stream to miss is accurate when considering the next w misses. From this figure we can see that next-stream prediction accuracies are usually very high, even for the strict case of w = 1. With larger values of w accuracy is even higher, indicating that even when the predicted next stream does not correspond to the exact next miss it still appears within the next few misses. This shows that the miss graph approach described in Section 3.2, while simple, does capture the most frequent flow of instructions that miss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Miss Distances</head><p>Another characteristic that helps explain the behavior of the three prefetch schemes is the distances between consecutive misses. Using an execution without prefetching, we measure the number of cycles between consecutive misses both coming from the same instruction and from any instruction. Figure <ref type="figure" target="#fig_7">9</ref> shows the miss distances grouped by ranges of number of processor cycles.</p><p>From this figure we can see that often a large fraction of L2 misses occur thousands or even hundreds of thousands of cycles apart. Moreover, the fraction of such distant misses often increases with cache sizes. More importantly, the fraction of such distant misses is significantly larger when we consider distances between misses from the same memory access instruction. Again, these results suggest that PC/DC with large degrees of prefetching, while still being able to eliminate some more misses, may issue such deep prefetches too early. This and the fact that many of such early prefetches are likely to be displaced before being used explain why PC/DC's accuracy is low for deep prefetching (Section 5.2.2) leading to unused prefetches and wasted bandwidth usage (Section 5.1.2). Like PC/DC/MG, G/DC does not suffer as much from such premature prefetches, but its much lower accuracy (Section 5.2.2) compensates for this and leads to a lower prefetch hit ratio.</p><p>The following section attempts to show some more insight into the behavior of stream chains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Miss Graph Characterization</head><p>To gain more insight into the behavior of stream chaining, we attempt to characterize the miss graphs obtained following the graph construction described in Section 3.2.1. For this, we take snapshots of the IT at intervals of 1000 prefetch events (we also evaluated snapshots at intervals of L2 accesses and the results were similar). At each snapshot we obtained the complete miss graph constructed with the NextIT pointers. However, only links with Ctr above the minimum threshold of 3 were considered. In our experiments we found out that the miss graph constructed this way is in fact a collection of several connected components (CC). Nodes in the miss graph in one snapshot can be directly matched to nodes in other snapshots based on their PC value.</p><p>One important parameter for the success of the miss graph approach to prefetching is how stable the observed graphs are, i.e., how often the CC repeat themselves in successive snapshots. Unstable graphs will lead to poor next stream prediction accuracy. We attempt to quantify this stability by comparing the CC subgraphs seen at different snapshots. Because comparing the CC subgraphs across a very large number of snapshots is prohibitively expensive, we only compare the CC subgraphs of a snapshot against the CC subgraphs in the following window of 30 snapshots. We classify every CC subgraph at all snapshots as either similar to at least one other CC in one other snapshot in the window, or unique, meaning that there is no other similar CC in the window. We say that two subgraphs X and Y are similar if X is a subgraph of Y and X's nodes correspond to no less than 75% of Y's nodes, or if Y is a subgraph of X and Y's nodes correspond to no less than 75% of X's nodes. Note that with this definition, if two subgraphs are exactly the same they are also classed as "similar".</p><p>Table <ref type="table">2a</ref> shows -in columns 2 and 3 -the fraction of unique CC subgraphs for the two different cache sizes we consider. From this table we can see that indeed the number of unique subgraphs is very small for most benchmarks, although some benchmarks do have a significant fraction of unique subgraphs. To attempt to assess how the subgraph similarity holds across execution time, we varied both the window size and the number of prefetch events per interval (results not shown). The results in these cases did not vary noticeably. This indicates that subgraphs stay similar across large spans of time. To see why, consider both a case with smaller interval and a case with a larger window: if the fraction of unique subgraphs were to increase in either case with respect to the baseline interval and window sizes, it would be an indication that subgraphs do change frequently. However, this does not seem to be the case.</p><p>Table <ref type="table">2a</ref> also shows -in columns 4 to 7 -the average and range of number of nodes both per snapshot and per CC. We can see that the number of nodes per snapshot and, specially, per CC is not so large that managing the graphs -and, thus, operating the stream chaining mechanism -becomes too expensive, but in some cases it can contain enough nodes to make cycles long enough that visits to the same node (i.e., memory access instruction) are far apart in time, which partially explain the results in Section 5.2.4. Table <ref type="table">2b</ref> shows the average number of GHB lookups ("hops") required by each prefetcher to establish a delta correlation on a miss event, for a prefetch degree of 16. For PC/DC/MG the table shows the average number of hops in total for all streams considered in a miss event. From this table we can see that although the average total number of hops is larger in PC/DC/MG than in PC/DC, it is in all but two cases (zeusmp and namd) considerably smaller than in G/DC <ref type="foot" target="#foot_2">3</ref> . The reason for this is that the expected number of streams is small (Table <ref type="table">2a</ref>) and the number of hops per stream (results not shown) are often similar to those for PC/DC. On the other hand, the high average number of hops for G/DC can be explained by the higher unpredictability of a global miss stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Hardware prefetching is an established technique and is used in most commercial processors nowadays <ref type="bibr" target="#b4">[3,</ref><ref type="bibr" target="#b12">11]</ref>. Such prefetchers, however, are usually limited to relatively simple stride prefetchers <ref type="bibr" target="#b3">[2,</ref><ref type="bibr" target="#b5">4,</ref><ref type="bibr" target="#b8">7]</ref> and address correlation prefetchers.</p><p>Address correlation using Markov chains was first proposed in <ref type="bibr">[8]</ref> and delta correlation was first proposed in <ref type="bibr" target="#b10">[9]</ref> for TLB prefetching. The adaptation of delta correlation to data prefetching was done in <ref type="bibr" target="#b13">[12]</ref>, where the PC/DC prefetcher was proposed. That work also proposed the GHB structure used in our work and introduced the taxonomy and notation for 2-level prefetchers that we extend here. The work in <ref type="bibr" target="#b7">[6]</ref> proposed correlating cache line tags instead of complete addresses. The work in <ref type="bibr" target="#b15">[14]</ref> proposed address localization using memory regions (called CZones) and with simple address stride correlation. This was extended in <ref type="bibr" target="#b14">[13]</ref>, which proposed a prefetcher with address localization with memory regions but with delta correlations and with dynamic tuning of region sizes and prefetch degree. The work in <ref type="bibr" target="#b20">[19]</ref> proposed a similar address localization using memory regions, but in the context of a shared-memory multiprocessor, and with an exact bit-map encoding of addresses instead of delta correlations. An advantage of such address based localization approaches is that they do not require the PC of the missing memory access instruction, which may be difficult to obtain at the site of the prefetcher. However, they do not directly manage the temporal locality of misses as PC localization approaches do, but only indirectly in as much as there is or not temporal locality in the use of data. The work in <ref type="bibr" target="#b11">[10]</ref> proposed localizing streams based on a history of the last few memory access instructions. However, to have an accurate history it tracks all memory access instructions, including those that hit in the cache, which puts greater demand on the prefetch engine and requires fairly large tables. The work in <ref type="bibr" target="#b22">[21]</ref> proposed using the PC of branches to localize streams. It requires tracking branches as well as memory access instructions, which may not always be feasible.</p><p>Closer to our work, the idea of grouping misses that occur in chronological sequence into temporal streams was first used in <ref type="bibr" target="#b23">[22]</ref>. Different from our work, that work was done in the context of shared-memory programs and focused on coherence misses. Thus, it exploited the fact that the same groups of shared data tend to be communicated in unison.</p><p>Concurrently with our work, <ref type="bibr" target="#b21">[20]</ref> also exploits temporal and spatial correlation (and also PC correlation as part of the spatial component) and attempts to improve timeliness by reconstructing the total miss order from partial representations. The mechanisms used, however, differ from ours and consist of two parts: an extension of temporal streaming <ref type="bibr" target="#b23">[22]</ref> to recurring spatially correlated blocks of misses <ref type="bibr" target="#b20">[19]</ref>, instead of individual misses, and a mechanism to reconstruct the miss order by recording miss distances inside a spatial stream. Mostly orthogonal to the issues of how to localize streams and correlate addresses, <ref type="bibr" target="#b11">[10]</ref> and <ref type="bibr" target="#b6">[5]</ref> proposed using a dead-block predictor to identify replaceable cache lines and trigger prefetches. Prefetchers based on dead-block prediction can mitigate the problem caused by early prefetched lines displacing useful data in the cache. However, they do not address the problem that such early prefetched lines may be themselves evicted before being used and having to be re-fetched. Also orthogonal to our work, <ref type="bibr">[18]</ref> proposed using a user-level thread running in a processor-in-memory configuration to execute the prefetching algorithms.</p><p>Also in the area of software prefetching, recent works have focused on exploiting temporal and spatial correlation in the miss address stream. Most notably, <ref type="bibr" target="#b24">[23]</ref> proposes a dynamic optimization framework that combines temporal prefetching with object-based spatial prefetching and is able to adaptively tune prefetch distances to improve timeliness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>In the context of data prefetching, separating the history of misses into multiple streams -for instance according to the PC of the memory access instructions causing the misses -has been shown to achieve improved performance because it allows the prefetcher to focus on groups that show predictability and discard random and less predictable accesses. Localization offers a second level of design choice on top of the choice of mechanism for correlating the addresses within a stream. The problem with localizing streams is that important sequencing information of the misses is lost. In this paper we proposed adding a third level to the operation of prefetchers that allows them to link various localized streams into predictable chains of memory access instructions -we call this stream chaining. The key idea of stream chaining is to partially reconstruct the sequencing information in the miss stream, such that the result corresponds directly to the common path of misses followed by the application. With streams localized by memory access instructions' PC's, stream chaining allows a memory access instruction to trigger prefetches not only for itself, but also for following different static memory access instructions, whose addresses have been grouped in a different stream. In this way the prefetcher is aware of its context, as in memory access instruction PC based localization schemes, and is not limited to prefetching deeply for a single memory access instruction but can instead adaptively prefetch for other memory access instructions closer in time. We presented an initial scheme for stream chaining, which we call miss graph prefetching, that is simple, captures well common application behavior, and can be easily implemented on a simple extension to the Global History Buffer (GHB). We evaluated the proposed prefetcher with benchmarks from the SPEC 2006 and BioBench suites and compared it against a state-of-the-art memory access instruction PC based localization prefetcher -PC/DC <ref type="bibr" target="#b13">[12]</ref>. Experimental results showed that the proposed prefetcher consistently achieves better performance than PC/DC -it is only outperformed in very few cases and then by only 2%, and it outperforms PC/DC by as much as 55%, with an average improvement of 10%. Our detailed experiments showed that this performance gain comes from a higher accuracy, specially for high prefetch degrees, which leads to a larger fraction of prefetched lines being actually used before being evicted.</p><p>This paper represents initial work on a novel class of prefetchers and, thus, there are many possible avenues for future work. One extension of this work is to investigate alternative ways of chaining streams (Section 3.2.1) and/or alternative prefetch policies based on the resulting graphs (Section 3.2.2). For example, an adaptive mechanism to control how much to prefetch on each graph node could be easily added to PC/DC/MG. A more comprehensive extension is to investigate the stream chaining idea with stream localization approaches other than memory access instruction PC based, such as with address region localization <ref type="bibr" target="#b14">[13]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unique</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of PC, spatial, and temporal localization approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Miss graph cases: (a) non-cyclic chain longer than the prefetch degree; (b) cyclic chain shorter than the prefetch degree; and (c) non-cyclic chain shorter than the prefetch degree. The prefetch degree is assumed to be 8 and the current miss is generated by PC A .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Relative performance versus L2 size without prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Relative performance (bars) and memory traffic (line) for prefetching degree of 16 with G/DC (left bar), PC/DC (middle bar), and PC/DC/MG (right bar), for 256KB L2 cache (top chart) and 2MB L2 cache (bottom chart).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :Figure 6 :</head><label>86</label><figDesc>Figure 8: PC/DC/MG's accuracy in predicting the next PC to appear in the next w misses, for 256KB L2 cache.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Prefetch accuracy for varying prefetch degree with G/DC, PC/DC, and PC/DC/MG.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Miss distance for 256KB (top chart) and 2MB (bottom chart) L2 cache. For each benchmark the left bar is for any consecutive misses and the right bar is for consecutive misses caused by the same memory access instruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table Global History Buffer Head Pointer PreviousIT Chained streams (Miss Graph) Figure 2: PC/DC and PC/DC/MG example: (a) GHB state for both prefetchers (entries shown in grey are extensions to the design of [12] and are described in Section 3.2); (b) streams localized according to the missing memory access instruction</head><label></label><figDesc></figDesc><table><row><cell>else</cell></row><row><cell>IT[PreviousIT]?Ctr--</cell></row><row><cell>end if</cell></row><row><cell>if IT[PreviousIT]?Ctr == 0 then</cell></row><row><cell>IT[PreviousIT]?NextIT = IT[cur]</cell></row><row><cell>IT[PreviousIT]?Ctr = 1</cell></row><row><cell>end if</cell></row><row><cell>end if</cell></row><row><cell>PreviousIT = cur</cell></row></table><note><p>'s PC (streams for P C B and P C C are not shown for simplicity) and possible chaining of the streams. if IT[PreviousIT]?NextIT is invalid then IT[PreviousIT]?NextIT = IT[cur] IT[PreviousIT]?Ctr = 1 else if IT[PreviousIT]?NextIT == IT[cur] then IT[PreviousIT]?Ctr++ (saturating increment)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Architectural parameters.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Core Frequency</cell><cell>5GHz</cell></row><row><cell>Fetch/Issue/Retire Width</cell><cell>6/ 4/ 4</cell></row><row><cell>I-Window/ROB</cell><cell>80/ 152</cell></row><row><cell>Branch Predictor</cell><cell>64Kbit 2BcgSkew</cell></row><row><cell>BTB/RAS</cell><cell>2K entries, 2-way/ 32 entries</cell></row><row><cell>Minimum misprediction</cell><cell>20 cycles</cell></row><row><cell>Ld/St queue</cell><cell>108</cell></row><row><cell>L1 ICache</cell><cell>64KB, 2-way, 64B lines, 2 cycles</cell></row><row><cell>L1 DCache</cell><cell>64KB, 4-way, 64B lines, 2 cycles</cell></row><row><cell>L1 MSHR's</cell><cell>4</cell></row><row><cell>L1-L2 bus</cell><cell>64bits</cell></row><row><cell>L2 Cache</cell><cell>256KB/2048KB, 8-way, 64B lines, 13/18 cycles</cell></row><row><cell>L2-Memory Bus</cell><cell>64bits, 1.25Ghz</cell></row><row><cell>Main Memory</cell><cell>400 cycles</cell></row><row><cell>Prefetch degree</cell><cell>1/4/8/16</cell></row><row><cell>IT</cell><cell>512 entries, 1 cycle</cell></row><row><cell>GHB</cell><cell>512 entries, 5+1*hop cycles</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We use the term localization to refer to the act of assigning misses to a particular stream in order to distinguish it from the correlation properties that may exist in the stream(s).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In our representation we use subscripts to refer to different static instructions and superscripts to refer to dynamic instances of static instructions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The larger average number of hops for PC/DC/MG compared to G/DC for namd reflects the former's difficulties to track correlations in this particular case and its consequent significant drop in accuracy (Figure7).</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>* This work was supported in part by the <rs type="funder">EC</rs> under grants <rs type="grantNumber">IP 27648 (FP6</rs>) and HiPEAC <rs type="grantNumber">IST-004408</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_G873JEd">
					<idno type="grant-number">IP 27648 (FP6</idno>
				</org>
				<org type="funding" xml:id="_5mnydhU">
					<idno type="grant-number">IST-004408</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Albayraktaroglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-W</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>BioBench: A benchmark suite</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Table 2: (a) Statistics related to miss graphs: Unique refers to percentage of unique connected components, Snapshot refers to the range and average number of nodes per snapshot, and CC refers to the range and average number of nodes per connected component. (b) Average number of &quot;hops&quot; required in the GHB structure to establish correlation(s) on a miss event, for prefetch degree 16. of bioinformatics applications</title>
		<idno>19] 15 [8, 17] 15 [1, 16] 5.4 [1, 9] 3.9 327 316 24 12 28 14 phylip 13 37</idno>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on Performance Analysis of Systems and Software</title>
		<imprint>
			<date type="published" when="2005-03">March 2005</date>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
	<note>8, 36] 26 [8, 14] 11 [1, 24] 5.1 [1, 10] 4.6 383</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty</title>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputing Conf</title>
		<imprint>
			<date type="published" when="1991-11">November 1991</date>
			<biblScope unit="page" from="176" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Inside Intel Core Microarchitecture and Smart Memory Access</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doweck</surname></persName>
		</author>
		<ptr target="http://download.intel.com/technology/architecture/sma.pdf" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Intel Corporation</orgName>
		</respStmt>
	</monogr>
	<note>White paper</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stride Directed Prefetching in Scalar Processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Janssens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on Microarchitecture</title>
		<imprint>
			<date type="published" when="1992-12">December 1992</date>
			<biblScope unit="page" from="102" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Timekeeping in the Memory System: Predicting and Optimizing Memory Behavior</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="209" to="220" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">TCP: Tag Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2003-02">February 2003</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="364" to="373" />
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prefetching Using Markov Predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="252" to="263" />
			<date type="published" when="1997-06">June 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Going the Distance for TLB Prefetching: An Application-Driven Study</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Kandiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sivasubramaniam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="195" to="206" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dead-Block Prediction &amp; Dead-Block Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">A.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="2001-06">June 2001</date>
			<biblScope unit="page" from="144" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">IBM POWER6 Microarchitecture</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Starke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Q</forename><surname>O'connell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><surname>Vaden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Research and Development</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="639" to="662" />
			<date type="published" when="2007-11">November 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Data Cache Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2004-02">February 2004</date>
			<biblScope unit="page" from="96" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AC/DC: An Adaptive Data Cache Prefetcher</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Parallel Architectures and Compilation Techniques</title>
		<imprint>
			<date type="published" when="2004-09">September 2004</date>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating Stream Buffers as a Secondary Cache Replacement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="24" to="33" />
			<date type="published" when="1994-05">May 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MicroLib: A Case for the Quantitative Comparison of Micro-Architecture Mechanisms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Microarchitecture</title>
		<imprint>
			<biblScope unit="page" from="43" to="54" />
			<date type="published" when="2004-12">December 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Renau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fraguela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prvulovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<ptr target="http://sesc.sf.net" />
		<title level="m">SESC simulator</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CACTI 3.0: An Integrated Cache Timing, Power, and Area Model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WRL Research Report</title>
		<imprint>
			<date type="published" when="2001">2001/2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using a User-Level Memory Thread for Correlation Prefetching</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="2002-05">May 2002</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
		<title level="m">Spatial Memory Streaming.&quot; Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="2006-06">June 2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Memory Streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Branch History Guided Instruction Prefetching</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on High Performance Computer Architecture</title>
		<imprint>
			<date type="published" when="2001-01">January 2001</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal Streaming of Shared Memory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hardavellas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. Symp. on Computer Architecture</title>
		<imprint>
			<biblScope unit="page" from="222" to="233" />
			<date type="published" when="2005-06">June 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Self-Repairing Prefetcher in an Event-Driven Optimization Framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Symp. on Code Generation and Optimization</title>
		<imprint>
			<date type="published" when="2006-03">March 2006</date>
			<biblScope unit="page" from="50" to="64" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
