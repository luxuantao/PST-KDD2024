<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Noisy Labels for Entity-Centric Information Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-17">17 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
							<email>muhaoche@usc.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Noisy Labels for Entity-Centric Information Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-17">17 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.08656v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent efforts for information extraction have relied on many deep neural models. However, any such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with different parameter initialization. These models are jointly optimized with task-specific loss, and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. In the end, we can take any of the trained models for inference. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural models have achieved significant success on various information extraction (IE) tasks. However, when training labels contain noise, deep neural models can easily overfit the noisy labels, leading to severe degradation of performance <ref type="bibr" target="#b1">(Arpit et al., 2017;</ref><ref type="bibr" target="#b40">Zhang et al., 2017a)</ref>. Unfortunately, labeling on large corpora, regardless with human annotation <ref type="bibr" target="#b20">(Raykar et al., 2010)</ref> or automated heuristics <ref type="bibr" target="#b23">(Song et al., 2015)</ref>, inevitably suffers from labeling errors. This problem has even drastically affected widely used benchmarks, such as CoNLL03 <ref type="bibr" target="#b27">(Tjong Kim Sang, 2002)</ref> and TA-CRED <ref type="bibr" target="#b42">(Zhang et al., 2017b)</ref>, where a notable portion of incorrect labels have been caused in annotation and largely hindered the performance of SOTA systems <ref type="bibr" target="#b21">(Reiss et al., 2020;</ref><ref type="bibr" target="#b0">Alt et al., 2020)</ref>. Hence, developing a robust learning method that better tol- The models are jointly optimized with the taskspecific loss from label y and an agreement loss, which regularizes the models to generate similar predictions to the aggregated soft target q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>erates noisy supervision represents an urged challenge for emerging IE models.</p><p>So far, few research efforts have been made to developing noise-robust IE models, and existing work mainly focuses on the weakly supervised or distantly supervised setting <ref type="bibr" target="#b26">(Surdeanu et al., 2012;</ref><ref type="bibr" target="#b19">Ratner et al., 2016;</ref><ref type="bibr" target="#b7">Huang and Du, 2019;</ref><ref type="bibr" target="#b14">Mayhew et al., 2019)</ref>. Most of such methods typically depend on bags of instances that only exist in specialized application scenarios <ref type="bibr" target="#b26">(Surdeanu et al., 2012;</ref><ref type="bibr" target="#b39">Zeng et al., 2015;</ref><ref type="bibr" target="#b19">Ratner et al., 2016)</ref>, or require an additional clean and sufficiently large reference dataset to develop a noise filtering model <ref type="bibr" target="#b17">(Qin et al., 2018)</ref>. Accordingly, those methods may not be generally adapted to many supervised training settings, where the aforementioned auxiliary learning resources are not always available. Particularly, CrossWeigh <ref type="bibr" target="#b33">(Wang et al., 2019c</ref>) is a representative work that denoises a natural language corpus without using extra learning resources. This method trains multiple independent models on different partitions of training data, and down-weighs instances on which the models disagree. Though being effective, a method of this kind requires training tens of redundant neural models, leading to excessive computational overhead for large models. As far as we know, the problem of noisy labels in supervised learning for IE tasks has not been well investigated.</p><p>In this paper, we aim to develop a general denois-ing framework that can easily easily incorporate existing supervised learning models for entity-centric IE tasks. Our method is motivated by recent studies <ref type="bibr" target="#b1">(Arpit et al., 2017;</ref><ref type="bibr" target="#b28">Toneva et al., 2019)</ref> showing that noisy labels often have delayed learning curves, as incorrectly labeled instances are more likely to contradict the inductive bias captured by the model. Hence, noisy label instances take a longer time to be picked up by the neural models, and are frequently forgotten in later epochs. Therefore, predictions by more than one model tend to disagree on noisy examples. Accordingly, we propose a simple yet effective co-regularization framework to handle noisy training labels, as illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Our framework consists of two or more neural classifiers with identical structures but different initialization. In training, all classifiers are optimized on the training dataset with task-specific losses, and are jointly regularized with regard to an agreement loss that is defined as the Kullback-Leibler (KL) divergence among the predicted probability distributions. Then for data instances where a classifier's predictions disagree with labels, the agreement loss encourages the classifier to give similar predictions as other classifiers instead of the actual (possibly noisy) labels. In this way, the learning framework prevents the incorporated model from overfitting noisy labels and makes the loss landscape smoother. We apply the framework to two important entitycentric IE tasks, i.e., named entity recognition (NER) and relation extraction (RE). We conduct extensive experiments on two prevalent but noisy benchmarks, i.e., CoNLL03 for NER and TACRED for RE, and apply the proposed learning frameworks to train various SOTA models from prior studies for these two tasks. The results demonstrate the effectiveness of our method in enhancing noise-tolerant training, and consistently leading to promising improvement on various SOTA models.</p><p>This work presents contributions as follows:</p><p>• We propose a general co-regularization framework that can effectively learn supervised IE models from noisy datasets without the need of any extra learning resources.</p><p>• We discuss in details the different design strategies of the framework and the trade-off between efficiency and effectiveness.</p><p>• Extensive experiments on NER and RE demonstrate that our framework yields promising im-provements on various SOTA models, and also outperforms existing denoising frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We discuss two lines of related work. Each has a large body of work which we can only provide as a highly selected summary.</p><p>Distant Supervision. Distant supervision <ref type="bibr" target="#b15">(Mintz et al., 2009)</ref> generates noisy training data with heuristics to align unlabeled data with labels, whereas much effort has been devoted to reducing labeling noise. Multi-instance learning <ref type="bibr" target="#b39">(Zeng et al., 2015;</ref><ref type="bibr" target="#b11">Lin et al., 2016;</ref><ref type="bibr" target="#b8">Ji et al., 2017)</ref> groups noisy training instances and assumes at least one instance in the groups is correct, then it uses heuristics or auxiliary classifiers to select correct labels. However, such instance groups depend on specialized learning resources that may not exist in a general supervised setting. Reinforcement learning <ref type="bibr" target="#b17">(Qin et al., 2018;</ref><ref type="bibr" target="#b32">Wang et al., 2020)</ref> and curricular learning <ref type="bibr" target="#b9">(Jiang et al., 2018;</ref><ref type="bibr" target="#b7">Huang and Du, 2019)</ref> methods use a clean validation set to obtain an auxiliary model for noise filtering, while constructing a perfectly labeled validation set is expensive. Relabeling methods <ref type="bibr" target="#b12">(Liu et al., 2017;</ref><ref type="bibr" target="#b37">Yang et al., 2019;</ref><ref type="bibr" target="#b22">Shang et al., 2020)</ref> use the trained model to incrementally detect possibly noisy labels and rectify them with model predictions. However, using the same model for both noise detection and prediction may introduce relabeling error propagation caused by sample-selection bias <ref type="bibr" target="#b6">(Han et al., 2018)</ref>. Our framework can learn supervised IE models without the presence of extra learning resources. Moreover, our framework learns from multiple models, which reduces error propagation during training.</p><p>Supervised Learning from Noisy Labels. A deep neural network can memorize noisy labels, and its generalizability will severely degrade when trained with noise <ref type="bibr" target="#b40">(Zhang et al., 2017a)</ref>. In computer vision, much investigation has been conducted for supervised image classification with noise, producing techniques such as robust loss functions <ref type="bibr" target="#b43">(Zhang and Sabuncu, 2018;</ref><ref type="bibr" target="#b31">Wang et al., 2019b)</ref>, noise filtering layers <ref type="bibr" target="#b25">(Sukhbaatar et al., 2015;</ref><ref type="bibr" target="#b5">Goldberger and Ben-Reuven, 2017)</ref>, and sample selection <ref type="bibr" target="#b13">(Malach and Shalev-Shwartz, 2017;</ref><ref type="bibr" target="#b9">Jiang et al., 2018;</ref><ref type="bibr" target="#b6">Han et al., 2018;</ref><ref type="bibr" target="#b38">Yu et al., 2019;</ref><ref type="bibr" target="#b34">Wei et al., 2020)</ref>  <ref type="bibr" target="#b29">(Wang et al., 2019a)</ref> assumes that noisy labels are created from randomly flipping clean labels, and uses a CNN to model the noise transition matrix <ref type="bibr" target="#b30">(Wang et al., 2018)</ref>. However, this assumption does not necessarily hold for real datasets, where the chance of wrongly labeled is different for data instances <ref type="bibr" target="#b3">(Cheng et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this paper, we focus on developing a noiserobust learning framework to improve supervised models for entity-centric IE tasks. In such tasks, (noisy) labels can be assigned to either individual tokens (NER) or pairs of entities (RE) in natural language text. Specifically, D = {(x i , y i )} N i=1 is a noisily labeled dataset, where each data instance consists of a lexical sequence or a context x and a label y. y is annotated either on tokens of x for NER, or on a pair of entity mentions in x for RE. For some instances in D, the labels are incorrect. Our objective is to learn a noise-robust model f with the presence of such noisily labeled instances from D without using external resources such as a clean development dataset <ref type="bibr" target="#b17">(Qin et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learning Process</head><p>Our framework is motivated by the delayed learning curve of a neural model on noisy data, compared with learning on clean data. On noisy data, neural models tend to fit easy and clean instances that are more consistent with the well-represented patterns of data in early steps, while needing much more steps to memorize noise <ref type="bibr" target="#b1">(Arpit et al., 2017)</ref>.</p><p>Moreover, learned noisy examples tend to be frequently forgotten in later epochs <ref type="bibr" target="#b28">(Toneva et al., 2019)</ref>, since they conflict with the general inductive bias represented by the majority of clean data. Therefore, the model's prediction is likely to be consistent with the clean labels, while is often inconsistent or oscillates on noisy labels over different training epochs. As a result, labels that are different from model's predictions in the later epochs of training are likely to be noisy and should be down-weighted or rectified, so as to reduce their impact on optimization.</p><p>In this paper, we propose a co-regularization framework for training noise-robust IE models. It incorporates several copies of a task-specific IE model which have the same architecture but different (random) parameter initialization. These task-specific IE models are jointly optimized on the noisy dataset based on their task-specific losses as well as a central agreement loss. During training, the predicted probability distributions from models are gathered as a soft target, which represents the model estimations of the true label. The agreement loss is responsible for encouraging these models to generate similar predictions to the soft target. In this learning process, the models start their training from different initialization may learn different clean labels and generate different decision boundaries, so by gathering their predictions, the soft target can better separate noisy labels from clean labels that have not yet been learned.</p><p>The learning process of our framework is described as Alg. 1. It consists of M (M ≥ 2) copies of the task-specific model, denoted {f k } M k=1 with different initialization. Regarding initialization, for models that are trained from scratch, all parameters are randomly initialized. Otherwise, for those that are built upon pre-trained language models, only the parameters that are external to the language models (e.g., those of a downstream softmax classifier) are randomly initialized, while the pre-trained parameters remain the same. Once initialized, our framework trains those models in two phases. The first α% training steps undergo a warm-up phase, where α is a hyperparameter. This phase seeks to help the model reach initial convergence on the task. When a new batch comes in, we first calculate the task-specific training losses on M models {L (k) sup } M k=1 and average them as L T , then update model parameters w.r.t. L T . After the warm-up phase, an agreement loss L agg is further introduced</p><formula xml:id="formula_0">Algorithm 1: Learning Process Input: Dataset D, hyperparameters T, α, M, γ. Output: A trained model f . Initialize M neural models {f k } M k=1 . for t = 1...T do Calculate task-specific loss {L (k) sup } M k=1 . L T = 1 M M k=1 L (k) sup if t &lt; α% × T then // Warmup Update model parameters w.r.t. L T . else q = 1 M M k=1 p (k) L agg = 1 M M k=1 d(q, p (k) ). L = L T + γ • L agg . Update model parameters w.r.t. L.</formula><p>Return f 1 or the best-performing model.</p><p>to measure the distance from the predictions of M models to the soft target q. Parameters are accordingly updated based on the joint loss L, encouraging the model to generate predictions that are consistent with both the training labels and the soft target. The formalization of the loss function is described next ( §3.2). In the end, we can either use the first model f 1 or select the best-performing model for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Co-regularization Objective</head><p>In our framework, the influence of noisy labels in training is decreased by optimizing the agreement loss. Specifically, given a batch of data instances B = {(x i , y i )} N i=1 , and predictions</p><formula xml:id="formula_1">{(x i , p (k) i )} N i=1 M k=1</formula><p>by the M models on B, p ∈ R C is the predicted probability distribution of the total C classes, we merge the M predictions by simply averaging the probabilities:</p><formula xml:id="formula_2">q i = 1 M M k=1 p (k) i ,</formula><p>which represents the models' guess of the true label. Then we define the agreement loss L agg as the average Kullback-Leibler (KL) divergence from q to p (k) , k = 1, ..., M :</p><formula xml:id="formula_3">d(q i ||p (k) i ) = C j=1 q ij log q ij + p ij + , L agg = 1 M N N i=1 M k=1 d(q i ||p (k) i ),</formula><p>where is a small positive number to avoid division by zero. The agreement loss is minimized towards the goal where predictions p (k) i are close to the same. As the KL divergence is non-negative, the minimum value of the agreement loss is achieved only when q i = p (k) i , k = 1, ..., M , which implies all p (k) i are equal as q is calculated by average. Therefore, the agreement loss L agg encourages the models to reach a consensus based on the same input.</p><p>Besides the average probability, we may also use other aggregates for q as long as they satisfy that</p><formula xml:id="formula_4">q i = p (k) i , k = 1, ..., M when all p (k) i are equal.</formula><p>We consider the following alternatives for q:</p><p>• Average logits. Given the predicted logits {l</p><formula xml:id="formula_5">(k)</formula><p>i } M k=1 of the M models, we average and normalize the logits using softmax:</p><formula xml:id="formula_6">l i = 1 M M k=1 l (k) i , q ij = exp (l ij ) C j =1 exp l ij .</formula><p>• Min probability. The models learn the labels at different paces during training. Some models may overfit to the noisy labels and thus cannot serve as the soft target. Thus, we consider choosing for each data instance the probability with the largest supervision loss among all models:</p><formula xml:id="formula_7">k * i = arg max k L sup p (k) i , y , q i = p (k * ) i</formula><p>In experiments, we find the performance of these aggregates is similar. We present the results of different q in §5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Joint Training</head><p>The main learning objective of our framework is then to optimize the joint loss L = L T + γL agg , where γ is a positive hyperparameter and L T is the average of task-specific supervision losses</p><formula xml:id="formula_8">L (k) sup M k=1</formula><p>. For classification problems such as NER and RE, the task-specific loss is defined as the following cross-entropy loss, where I denotes an indicator function:</p><formula xml:id="formula_9">L sup = − 1 N N i=1 C j=1 I [y i = j] log p ij .</formula><p>N thereof is the number of tokens for NER, and the number sentences for RE. The supervision loss L (k)</p><p>sup is minimized when the predicted probabilities of training labels are 1, while the agreement loss L agg is minimized when all models predict the same probabilities. For datasets with clean training labels, where the models' predictions usually agree on training labels, the supervision losses and the agreement loss have similar minimizers. On the other hand, for noisily labeled datasets, where the model predictions are usually different from the training labels, the agreement loss incurs dramatically different directions of gradients to the supervision losses in training, therefore preventing overfitting on noisy labels.</p><p>Aside from the co-regularization technique, denoising may also be attempted by small-loss selection <ref type="bibr" target="#b9">(Jiang et al., 2018;</ref><ref type="bibr" target="#b6">Han et al., 2018)</ref> or relabeling <ref type="bibr" target="#b12">(Liu et al., 2017)</ref>, where instances with large training losses are excluded from training or relabeled by models' predictions. However, some clean examples can also have large training losses and will be incorrectly pruned/relabeled. Such errors can accumulate during training and may hinder model performance. In our framework, errors from one model will not easily propagate to the other, as the models are trained from different initialization and generate different decision boundaries. The use of an agreement loss instead of hard pruning/relabeling decisions further reduces error propagation (see §5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Tasks</head><p>We evaluate our framework on two fundamental entity-centric IE tasks, namely RE and NER. Our framework can incorporate any kind of neural models that are dedicated to either tasks. Particularly, in this paper, we adopt off-the-shelf SOTA models that are mainly based on transformer architectures. This section introduces the two attempted tasks and the design of task-specific models.</p><p>Relation Extraction. RE aims at identifying the relations between a pair of entities in a piece of text from the given vocabulary of relations. Specifically, given a sentence x and two entities e s and e o , identified as the subject and object entities respectively, the goal is to predict the relation between e s and e o . Following <ref type="bibr" target="#b23">Shi and Lin (2019)</ref>, we formulate this task as a sentence-level classification problem. Accordingly, we first apply the entity masking technique <ref type="bibr" target="#b42">(Zhang et al., 2017b)</ref>  put sentence and replace the subject and object entities with their named entity types. For example, a short sentence "Bill Gates founded Microsoft" will become "[SUBJECT-PERSON] founded [OBJECT-ORGANIZATION]" after entity masking. We then feed the sentence to the pre-trained language model and use a softmax classifier on the [CLS] token to predict the relation.</p><p>Named Entity Recognition. NER aims at locating and classifying named entities in text into predefined categories. Following <ref type="bibr" target="#b4">Devlin et al. (2019)</ref>, we formulate the task as a token-level classification problem. In detail, a transformer language model will first tokenize an input sentence into a sub-token sequence. To classify each token, the representation of its first sub-token given by the language model will be sent into a token-level softmax classifier. We use the BIO tagging scheme <ref type="bibr" target="#b18">(Ramshaw and Marcus, 1995)</ref> and output the tag with the maximum likelihood as the predicted label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment</head><p>In this section, we present the experimentation of the proposed learning framework based on two (noisy) benchmark datasets for the two entitycentric IE tasks ( §5.1- §5.4). In addition, a noise filtering analysis is presented to show how our framework prevents an incorporated neural model from overfitting noisy training data ( §5.5), along with a detailed ablation study about configurations with varied model copies, alternative noise filtering strategies, and alternative target functions ( §5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>The experiments are conducted based on TA-CRED <ref type="bibr" target="#b42">(Zhang et al., 2017b)</ref> and CoNLL03 <ref type="bibr" target="#b27">(Tjong Kim Sang, 2002)</ref>. TACRED is a crowdsourced dataset for relation extraction. A recent study by <ref type="bibr" target="#b0">Alt et al. (2020)</ref> found a large portion of examples to be mislabeled, and relabeled the development and test sets. CoNLL03 is a humanannotated dataset for NER. Another study by <ref type="bibr" target="#b33">Wang et al. (2019c)</ref> found that in 5.38% of sentences in CoNLL03, at least one token is labeled incorrectly. Accordingly, Wang et al. also relabeled the test set. Note that neither of these two datasets comes with a relabeled (clean) training set. We summarize the statistics of both datasets in Tab. 1. For all compared methods, we report the results on both the original and relabeled evaluation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Base Models</head><p>We evaluate our framework by incorporating the following SOTA models:</p><p>• C-GCN <ref type="bibr">(Zhang et al., 2018)</ref>  We report the performance of the base models trained with and without our co-regularization framework. We also compare our framework to CrossWeigh <ref type="bibr" target="#b33">(Wang et al., 2019c)</ref>, which is another noisy-label learning framework. Specifically, CrossWeigh partitions the training set into equalsized chunks, then reserves each chunk and trains several models on the rest ones. After training, the models predict on the reserved chunk and instances on which the models disagree are down-weighted.</p><p>In the end, the chunks are combined and used to train a new model for inference. Note that learning by CrossWeigh is dependant on a high computation cost. For example, 30 models are trained for denoising CoNLL03.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Model Configurations</head><p>For base models C-GCN <ref type="bibr">(Zhang et al., 2018)</ref> and LUKE <ref type="bibr" target="#b36">(Yamada et al., 2020)</ref>, we rerun the officially released implementations using the recommended hyperparameters in the original papers. And we use BERT BASE and BERT LARGE are provided in Huggingface <ref type="bibr" target="#b35">(Wolf et al., 2020)</ref>. For CrossWeigh <ref type="bibr" target="#b33">(Wang et al., 2019c)</ref>, we reimplement this framework using those compared base models and accordingly retune the hyperparameters.</p><p>All models are optimized with Adam (Kingma and Ba, 2015) using a learning rate of 3e−5 for TACRED and that of 1e−5 for CoNLL03, with a linear decay to 0. The batch size is fixed as 64 for all models. We also apply dropout <ref type="bibr" target="#b24">(Srivastava et al., 2014)</ref> between layers with rate 0.1. We finetune the TACRED model for 10 epochs, and finetune the CoNLL03 model for 50 epochs. The best model checkpoint is chosen based on the F 1 score on the development set. We tune γ from {1.0, 2.0, 5.0, 10.0, 20.0}, and tune α from {10, 30, 50, 70, 90}. We report the median of F 1 of 5 runs using different random seeds.</p><p>For efficiency, we use the simplest setup of our framework with two model copies (M = 2) in the main experiments ( §5.4). q is set as the average probability in the main experiment. Performance with more model copies and alternative probability aggregates is later studied in §5.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head><p>The experiment results on TACRED and CoNLL03 are reported in Tab. 2 and Tab. 3 respectively, where methods incorporated in our learning framework are marked with "CR". As stated, the results are reported under the setup where M = 2. For fair comparison, the results are reported based on the predictions from the model f 1 in the framework. On TACRED, our framework leads to an absolute improvement of 2.5−4.1% in F 1 on the relabeled test set for transformer-based models, and a relatively smaller gain (0.8% in F 1 ) on C-GCN. In particular, our framework enhances the SOTA method LUKE by 2.5% in F 1 , and leading to a very promising F 1 score of 83.1%. On CoNLL03 where the noise rate is smaller than TACRED, our framework leads to a performance gain of 0.28−0.82% in F 1 on the relabeled test set. On both IE tasks, our framework also leads to a consistent improvement on the original test set. Compared to CrossWeigh, except for C-GCN where the results are similar, our framework consistently outperforms it by 0.9 − 2.2% on TA-CRED, and by 0.13 − 0.45% on CoNLL03. Moreover, as our framework requires training M models concurrently while CrossWeigh requires training redundant models (30 in experiments), the computation cost of our co-regularization framework is much lower than CrossWeigh. In general, the results here show the effectiveness and practicality of the proposed noise-robust learning techniques. <ref type="bibr">Zhang et al., 2018)</ref> 67.2 66.7 74.9 74.6 BERT BASE <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 69.1 68.9 76.4 76.9 BERT LARGE <ref type="bibr" target="#b4">(Devlin et al., 2019)</ref> 70.9 70.2 78.3 77.9 LUKE ♣ <ref type="bibr" target="#b36">(Yamada et al., 2020)</ref> 71 Table <ref type="table">3</ref>: F 1 score (%) on the test set of CoNLL03. ♣ marks results obtained using the original released code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><formula xml:id="formula_10">Original Relabeled Dev F 1 Test F 1 Dev F 1 Test F 1 C-GCN ♣ (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Noise Filtering Analysis</head><p>We present an analysis to demonstrate how our framework prevents overfitting on noisy labels. To do so, we extract the 2,526 noisy examples from the TACRED dev and test sets where the relabeling by <ref type="bibr" target="#b0">Alt et al. (2020)</ref> disagree with their original labels. Accordingly, we obtain a noisy set containing those examples with original labels, and a clean set with rectified labels. Then, we train a relation classifier on the union of the training set and the noisy set, then evaluate the model on the clean set. In this case, worse performance on the clean set indicates more severe overfitting on noise. Fig. <ref type="figure" target="#fig_1">2</ref> shows the results by C-GCN-CR and BERT BASE -CR on the clean set, where we observe that: (1) Compared to the original base models (γ = 0.0), those trained with framework achieves higher F 1 scores, indicating improved robustness against the label noise; (2) Comparing different base models, the large classifier BERT BASE is typically less noise-robust than a smaller model like C-GCN, which explains why the performance gain from our framework is more notable on BERT BASE ;</p><p>(3) For both base models, the F 1 score first increases then decreases, being consistent with the delayed learning curves the neural models have on noisy instances <ref type="bibr" target="#b1">(Arpit et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Ablation Study</head><p>Using more models. The main results show that using two copies of a model in the co-regularization framework has already improved the performance by a remarkable margin. Intuitively, more models may possibly generate higher-quality soft targets and thus further improve the performance. We further show the performance on TACRED by incorporating more model copies. We report the relabeled test F 1 on TACRED in Tab. 4. On BERT LARGE , increasing the number of model copies from 2 to 4 gradually improves the performance from 82.0% to 82.7%. While on BERT BASE , increasing the number of model copies does not improve the performance. We notice that BERT BASE fails to fit the high-quality soft target because of the relatively small model capacity. Increasing the number of models also leads to a significant increase in the agreement loss. Therefore, using models does not further improve the performance on BERT BASE .</p><p>Note that as the models can be trained in parallel, increasing the number of models does not necessarily increase the training time, though being at the cost of more computational resources.</p><p>Alternative strategies for noise filtering. Instead of co-regularization, we also experiment with two other strategies for noise filtering in our framework. Small-loss selection <ref type="bibr" target="#b9">(Jiang et al., 2018)</ref> keeps small-loss training examples in optimization and prunes others, as noisy instances take a longer time to be memorized and usually cause large training loss. Relabeling <ref type="bibr" target="#b12">(Liu et al., 2017)</ref>  Alternative aggregates for q. Besides the average probability, we evaluate two other aggregates for q, i.e. the average logits and the min probability ( §3.2). This experiment is conducted with M = 2. F 1 results on the relabeled TACRED test set (Tab. 6) suggest that different aggregates generally achieve comparable performance,with a marginal difference of up to 0.6% in F 1 . Therefore, the default setup is suggested to be the average probability, which is easier to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presents a co-regularization framework for learning supervised IE models from noisy data. This framework consists of two or more identically structured models with different initialization, which are encouraged to give similar predictions on the same inputs by optimizing an agreement loss. On noisy examples where model predictions usually differ from the labels, the agreement loss makes the landscape of the task-specific loss smoother, thus preventing the model from overfitting noisy labels. Experiments on NER and RE benchmarks show that our framework yields promising improvements over various SOTA IE models. For future work, we plan to extend the use of the proposed framework to other tasks such as event-centric IE <ref type="bibr" target="#b2">(Chen et al., 2021)</ref> and coreference resolution <ref type="bibr" target="#b23">(Peng et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Consideration</head><p>This work does not present any direct societal consequence. The proposed work seeks to develop a general learning framework that learning more robust neural models for entity-centric information extraction under noisy label settings. We believe this leads to intellectual merits that benefits the information extraction community where learning resources may often suffer from noisy labeling issues. And it potentially has broad impacts since the tackled issues also widely exists in tasks of other areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Illustration of our co-regularization framework. The models are jointly optimized with the taskspecific loss from label y and an agreement loss, which regularizes the models to generate similar predictions to the aggregated soft target q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: F 1 score (%) on the clean set of TACRED. Classifiers trained with our framework are more noiserobust compared to baselines (γ = 0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>to the in-Data statistics of TACRED and CoNLL03.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># train # dev # test # classes % noise</cell></row><row><cell cols="3">TACRED 68124 22631 15509</cell><cell>42</cell><cell>6.62</cell></row><row><cell cols="2">CoNLL03 14041 3250</cell><cell>3453</cell><cell>9</cell><cell>5.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>F 1 score (%) on the dev and test set of TACRED. ♣ marks results obtained from the original released implementation. We report the median of F 1 on 5 runs of training using different random seeds. For fair comparison, the results are reported based on the predictions from the model f 1 in our framework.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>.1</cell><cell>70.9</cell><cell>80.1</cell><cell>80.6</cell></row><row><cell cols="2">C-GCN-CrossWeigh</cell><cell></cell><cell>67.8</cell><cell>67.4</cell><cell>75.6</cell><cell>75.7</cell></row><row><cell cols="3">BERT BASE -CrossWeigh</cell><cell>71.3</cell><cell>70.8</cell><cell>79.2</cell><cell>79.1</cell></row><row><cell cols="3">BERT LARGE -CrossWeigh</cell><cell>72.1</cell><cell>71.9</cell><cell>79.5</cell><cell>79.8</cell></row><row><cell cols="2">LUKE-CrossWeigh</cell><cell></cell><cell>71.0</cell><cell>71.6</cell><cell>80.4</cell><cell>81.6</cell></row><row><cell cols="2">C-GCN-CR</cell><cell></cell><cell>67.7</cell><cell>67.2</cell><cell>75.6</cell><cell>75.4</cell></row><row><cell cols="2">BERT BASE -CR</cell><cell></cell><cell>71.5</cell><cell>71.1</cell><cell>79.9</cell><cell>80.0</cell></row><row><cell cols="2">BERT LARGE -CR</cell><cell></cell><cell>73.1</cell><cell>73.0</cell><cell>81.3</cell><cell>82.0</cell></row><row><cell cols="2">LUKE-CR</cell><cell></cell><cell>71.8</cell><cell>72.4</cell><cell>81.9</cell><cell>83.1</cell></row><row><cell>Model</cell><cell cols="2">Original Relabeled</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Test F 1</cell><cell>Test F 1</cell><cell></cell><cell></cell></row><row><cell>BERT BASE (Devlin et al., 2019)</cell><cell>91.96</cell><cell>92.91</cell><cell></cell><cell></cell></row><row><cell>BERT LARGE (Devlin et al., 2019)</cell><cell>92.24</cell><cell>93.22</cell><cell></cell><cell></cell></row><row><cell>LUKE ♣ (Yamada et al., 2020)</cell><cell>93.91</cell><cell>95.60</cell><cell></cell><cell></cell></row><row><cell>BERT BASE -CrossWeigh</cell><cell>92.15</cell><cell>93.03</cell><cell></cell><cell></cell></row><row><cell>BERT LARGE -CrossWeigh</cell><cell>92.49</cell><cell>93.61</cell><cell></cell><cell></cell></row><row><cell>LUKE-CrossWeigh</cell><cell>93.98</cell><cell>95.75</cell><cell></cell><cell></cell></row><row><cell>BERT BASE -CR</cell><cell>92.53</cell><cell>93.48</cell><cell></cell><cell></cell></row><row><cell>BERT LARGE -CR</cell><cell>92.82</cell><cell>94.04</cell><cell></cell><cell></cell></row><row><cell>LUKE-CR</cell><cell>94.22</cell><cell>95.88</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>F 1 score (%) of using different number of models on the relabeled test set of TACRED.</figDesc><table><row><cell># Models</cell><cell></cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>BERT BASE -CR</cell><cell></cell><cell cols="3">80.0 79.5 79.8</cell></row><row><cell cols="2">BERT LARGE -CR</cell><cell cols="3">82.0 82.4 82.7</cell></row><row><cell>Model</cell><cell cols="4">Original Relabeled</cell></row><row><cell></cell><cell></cell><cell>Test F 1</cell><cell>Test F 1</cell></row><row><cell>BERT BASE</cell><cell></cell><cell>68.9</cell><cell>76.9</cell></row><row><cell cols="4">Small-loss selection</cell></row><row><cell>δ = 2%</cell><cell></cell><cell>68.7</cell><cell>76.6</cell></row><row><cell>δ = 5%</cell><cell></cell><cell>68.0</cell><cell>76.3</cell></row><row><cell>δ = 8%</cell><cell></cell><cell>67.0</cell><cell>75.4</cell></row><row><cell cols="3">Relabeling</cell><cell></cell></row><row><cell>δ = 2%</cell><cell></cell><cell>69.0</cell><cell>77.7</cell></row><row><cell>δ = 5%</cell><cell></cell><cell>68.3</cell><cell>77.4</cell></row><row><cell>δ = 8%</cell><cell></cell><cell>67.4</cell><cell>75.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>F 1 score (%) of alternative noise filtering strategies on the test set of TACRED with BERT BASE .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>relabels the large-loss training examples with the most likely labels from model predictions. Both methods need tuning a hyperparameter δ, which decides the percentage of examples to be pruned/relabeled. We evaluate these noise filtering strategies on TACRED with BERT BASE as the base model. For both alternative strategies, we prune/relabel δ t = δ • t T percent of examples in each training batch with the largest training losses following Han et al. (2018), where t is the current number of training steps, T is the total number of training steps, and δ is the maximum pruning/relabeling rate. The training loss is defined as the average supervision losses of the M models, where we set M = 2 in consistent with the main experiments ( §5.4). Results are shown in Table 5, where the small-F 1 score (%) of different functions for q on the relabeled test set of TACRED. loss selection strategy generally underperforms the base model without noise filtering. Relabeling outperforms the base model when δ = 2% or 5% but the improvements are lesser in comparison to the proposed co-regularization method. We observe that these two strategies mostly prune/relabel training examples from long-tail classes, which often cause large training losses due to a relatively small number of training examples. Once pruned/relabeled, such long-tail cases are likely to be excluded from training, causing further error propagation that can lead to biased prediction. Our framework, on the contrary, adopts an agreement loss instead of pruning/relabeling, which reduces such error propagation.</figDesc><table><row><cell>Functions</cell><cell cols="3">Avg prob Avg logit Min prob</cell></row><row><cell>BERT BASE -CR</cell><cell>80.0</cell><cell>79.9</cell><cell>79.4</cell></row><row><cell>BERT LARGE -CR</cell><cell>82.0</cell><cell>81.6</cell><cell>82.2</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED revisited: A thorough evaluation of the TACRED relation extraction task</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1558" to="1569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxinder</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11">2017. 2017. 6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event-centric natural language understanding</title>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 59th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning with bounded instance and label-dependent label noise</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kotagiri</forename><surname>Ramamohanarao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1789" to="1799" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
				<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="8536" to="8546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-attention enhanced CNNs and collaborative curriculum learning for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Yuyun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="389" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
				<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-06">Jun Zhao. 2017. February 4-9, 2017</date>
			<biblScope unit="page" from="3060" to="3066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mentornet: Learning datadriven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-10">2018. July 10-15. 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2309" to="2318" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
				<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07">2015. May 7-9, 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A soft-label method for noisetolerant distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1790" to="1795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Named entity recognition with partially annotated training data</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
				<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="645" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Solving hard coreference problems</title>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="809" to="819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust distant supervision relation extraction via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Pengda</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2137" to="2147" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data programming: Creating large training sets, quickly. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3567" to="3575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName><forename type="first">Shipeng</forename><surname>Vikas C Raykar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><forename type="middle">Hermosillo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Identifying incorrect labels in the CoNLL-2003 corpus</title>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Cutler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Muthuraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Eichenberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computational Natural Language Learning</title>
				<meeting>the 24th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="215" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Are noisy sentences useless for distant supervised relation extraction</title>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="1911">2020. 1911.09788</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simple bert models for relation extraction and semantic role labeling</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno>ArXiv, abs/1904.05255</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015-01-25">2019. 2015. January 25-30, 2015</date>
			<biblScope unit="page" from="2972" to="2978" />
		</imprint>
	</monogr>
	<note>Spectral label refinement for noisy and missing text labels</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training convolutional networks with noisy labels</title>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multi-instance multi-label learning for relation extraction</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julie</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
				<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
				<imprint>
			<date type="published" when="2002">2002. 2002. CoNLL-2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An empirical study of example forgetting during deep neural network learning</title>
		<author>
			<persName><forename type="first">Mariya</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Tachet Des Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning with noisy labels for sentence-level sentiment classification</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="6286" to="6292" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiclass learning with partially corrupted labels</title>
		<author>
			<persName><forename type="first">Ruxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2568" to="2580" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
				<meeting><address><addrLine>Seoul, Korea (South), Oc</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-11-27">2019b. tober 27 -November 2, 2019</date>
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Finding influential instances for distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Shao-Lun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yefeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
		<idno>ArXiv, abs/2009.09841</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CrossWeigh: Training named entity tagger from imperfect annotations</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019c</date>
			<biblScope unit="page" from="5154" to="5163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName><forename type="first">Hongxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020</date>
			<biblScope unit="page" from="13723" to="13732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LUKE: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Exploiting noisy data in distant supervision relation classification</title>
		<author>
			<persName><forename type="first">Kaijia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-Yu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3216" to="3225" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="page" from="7164" to="7173" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. OpenReview</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017a. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Benjamin Recht, and Oriol Vinyals. net</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2205" to="2215" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generalized cross entropy loss for training deep neural networks with noisy labels</title>
		<author>
			<persName><forename type="first">Zhilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mert</forename><forename type="middle">R</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. 2018. 2018. December 3-8, 2018</date>
			<biblScope unit="page" from="8792" to="8802" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
