<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-06-21">21 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haizhou</forename><surname>Shi</surname></persName>
							<email>shihaizhou@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
							<email>siliang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhigang</forename><surname>Chen</surname></persName>
							<email>zgchen@iflytek.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<email>wufei@cs.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
							<email>yzhuang@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CIL: Contrastive Instance Learning Framework for Distantly Supervised Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-21">21 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.10855v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task. For the past decade, researchers apply the multiinstance learning (MIL) framework to find the most reliable feature from a bag of sentences. Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets. In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs. Therefore, the performance of distantly supervised RE models is bounded. In this paper, we go beyond typical MIL framework and propose a novel Contrastive Instance Learning (CIL) framework. Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance. Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims at predicting the relation between entities based on their context. Several studies have been carried out to handle this crucial and complicated task over decades as the extracted information can serve as a significant role for many downstream tasks. Since the amount of training data generally limits traditional supervised RE systems, current RE systems usually resort to distant supervision (DS) to fetch abundant training data by aligning knowledge bases (KBs) and texts. However, such a heuristic way inevitably introduces some noise to the generated data. Training a robust and unbiased RE system under DS data  noise becomes the biggest challenge for distantly supervised relation extraction <ref type="bibr">(DSRE)</ref>.</p><p>With awareness of the existing DS noise, <ref type="bibr" target="#b27">Zeng et al. (2015)</ref> introduces the multi-instance learning (MIL) framework to DSRE by dividing training instances into several bags and using bags as new data units. Regarding the strategy for selecting instances inside the bag, the soft attention mechanism proposed by <ref type="bibr" target="#b12">Lin et al. (2016)</ref> is widely used for its better performance than the hard selection method. The ability to form accurate representations from noisy data makes the MIL framework soon become a paradigm of following-up works.</p><p>However, we argue that the MIL framework is effective to alleviate data noise for DSRE, but is not data-efficient indeed: As Figure <ref type="figure" target="#fig_1">1</ref> shows: The attention mechanism in the MIL can help select relatively informative instances (e.g.h 1 , h 2 ) inside the bag, but may ignore the potential information of other abundant instances (e.g.h m ). In other words, no matter how many instances a bag contains, only the formed bag-level representation can be used for further training in the MIL, which is quite inefficient. Thus, our focus is on how to make the initial MIL framework efficient enough to leverage all instances while maintaining the ability to obtain an accurate model under DS data noise?</p><p>Here, we propose a contrastive-based method to help the MIL framework learn efficiently. In detail, we regard the initial MIL framework as the bag encoder, which provides relatively accurate representations for different relational triples. Then we develop contrastive instance learning (CIL) to utilize each instance in an unsupervised manner: In short, the goal of our CIL is that the instances sharing the same relational triples (i.e.positive pairs) ought to be close in the semantic space, while the representations of instances with different relational triples (i.e.negative pairs) should be far away.</p><p>Experiments on three public DSRE benchmarks -NYT10 <ref type="bibr" target="#b20">(Riedel et al., 2010;</ref><ref type="bibr" target="#b10">Hoffmann et al., 2011)</ref>, GDS <ref type="bibr" target="#b11">(Jat et al., 2018)</ref> and KBP <ref type="bibr" target="#b13">(Ling and Weld, 2012)</ref> demonstrate the effectiveness of our proposed framework CIL, with consistent improvements over several baseline models and far exceed the state-of-the-art (SOTA) systems. Furthermore, the ablation study shows the rationality of our proposed positive/negative pair construction strategy.</p><p>Accordingly, the major contributions of this paper are summarized as follows:</p><p>• We discuss the long-standing MIL framework and point out that it can not effectively utilize abundant instances inside MIL bags.</p><p>• We propose a novel contrastive instance learning method to boost the DSRE model performances under the MIL framework.</p><p>• Evaluation on held-out and human-annotated sets shows that CIL leads to significant improvements over the previous SOTA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>In this paper, we argue that the MIL framework is effective to denoise but is not efficient enough, as the initial MIL framework only leverages the formed bag-level representations to train models and sacrifices the potential information of numerous instances inside bags. Here, we go beyond the typical MIL framework and develop a novel contrastive instance learning framework to solve the above issue, which can prompt DSRE models to utilize each instance. A formal description of our proposed CIL framework is illustrated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Input Embeddings</head><p>Token Embedding For input sentence/instance x, we utilize BERT Tokenizer to split it into several tokens: (t 1 , t 2 , . . . e 1 . . . e 2 . . . t L ), where e 1 , e 2 are the tokens corresponding to the two entities, and L is the max length of all input sequences. Following standard practices <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref>, we add two special tokens to mark the beginning ([CLS]) and the end ([SEP]) of sentences.</p><p>In BERT, token [CLS] typically acts as a pooling token representing the whole sequence for downstream tasks. However, this pooling representation considers entity tokens e 1 and e 2 as equivalent to other common word tokens t i , which has been proven <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref> to be unsuitable for RE tasks. To encode the sentence in an entity-aware manner, we add four extra special to-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>kens ([H-CLS], [H-SEP]) and ([T-CLS], [T-SEP])</head><p>to mark the beginning and the end of two entities.</p><p>Position Embedding In the Transformer attention mechanism <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>, positional encodings are injected to make use of the order of the sequence. Precisely, the learned position embedding has the same dimension as the token embedding so that the two can be summed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentence Encoder</head><p>BERT Encoder (Transformer Blocks, see Figure <ref type="figure" target="#fig_2">2</ref>) transforms the above embedding inputs (token embedding &amp; position embedding) into hidden feature vectors: (h 1 , h 2 , . . . h e 1 . . . h e 2 . . . h L ), where h e 1 and h e 2 are the feature vectors corresponding to the entities e 1 and e 2 . By concatenating the two entity hidden vectors, we can obtain the entity-aware sentence representation h = [h e 1 ; h e 2 ] for the input sequence x. We denote the sentence encoder H as:</p><formula xml:id="formula_0">H(x) = [h e 1 ; h e 2 ] = h</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Bag Encoder</head><p>Under the MIL framework, a couple of instances x with the same relational triple [e 1 , e 2 , r] form a bag B. We aim to design a bag encoder F to obtain representation B for bag B, and the obtained bag representation is also a representative of the current relational triple [e 1 , e 2 , r], which is defined as:</p><formula xml:id="formula_1">F(B) = F([e 1 , e 2 , r]) = B</formula><p>With the help of the sentence encoder described in section 2.2, each instance x i in bag B can be first encoded to its entity-aware sentence representation h i = H(x i ). Then the bag representation B can be regarded as an aggregation of all instances' representations, which is further defined as:</p><formula xml:id="formula_2">F([e 1 , e 2 , r]) = B = K i=1 α i h i</formula><p>where K is the bag size. As for the choice of weight α i , we follow the soft attention mechanism used in <ref type="bibr" target="#b12">(Lin et al., 2016)</ref>, where α i is the normalized attention score calculated by a query-based function f i that measures how well the sentence representation h i and the predict relation r matches:</p><formula xml:id="formula_3">α i = e f i j e f j</formula><p>where f i = h i Aq r , A is a weighted diagonal matrix and q r is the query vector which indicates the representation of relation r (randomly initialized). Then, to train such a bag encoder parameterized by θ, a simple fully-connected layer with activation function softmax is added to map the hidden feature vector B to a conditional probability distribution p(r| B, θ), and this can be defined as:</p><formula xml:id="formula_4">p(r| B, θ) = e or nr i=1 e o i</formula><p>where o = M B + b is the score associated to all relation types, n r is the total number of relations, M is a projection matrix, and b is the bias term.</p><p>And we define the objective of bag encoder using cross-entropy function as follows:</p><formula xml:id="formula_5">L B (θ) = − i=1 log p(r i | B i , θ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Contrastive Instance Learning</head><p>As illustrated in section 1, the goal of our framework CIL is that the instances containing the same relational triples (i.e.positive pairs) should be as close (i.e.∼) as possible in the hidden semantic space, and the instances containing different relational triples (i.e.negative pairs) should be as far (i.e. ) away as possible in the space. A formal description is as follows.</p><p>Assume there is a batch bag input (with a batch size G): (B 1 , B 2 , . . . , B G ), the relational triples of all bags are different from each other. Each bag B in the batch is constructed by a certain relational triple [e 1 , e 2 , r], and all instances x inside the bag satisfy this triple. The representation of the triple can be obtained by bag encoder as B.</p><p>We pick any two bags B s and B t:t =s in the batch to further illustrate the process of contrastive instance learning. B s is defined as the source bag constructed with relational triple [e s1 , e s2 , r s ] while B t is the target bag constructed with triple [e t1 , e t2 , r t ]. And we discuss the positive pair instance and negative pair instances for any instance x s in bag B s .</p><p>It is worth noting that all bags are constructed automatically by the distantly supervised method, which extracts relational triples from instances in a heuristic manner and may introduce true/false positive label noise to the generated data. In other words, though the instance x is included in the bag with relational triple [e 1 , e 2 , r], it may be noisy and fail to express the relation r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Positive Pair Construction</head><p>Instance x s ∼ Random Instance x s One intuitive choice of selecting positive pair instance for instance x s is just picking another instance x s = x s from the bag B randomly. However, both of the instances x s and x s may suffer from data noise, and they are hard to express the same relational triple simultaneously. Thus, taking instance x s and randomly selected instance x s as a positive pair is not an optimal option.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Negative Pair Construction</head><p>Instance x s Random Instance x t Similarly, for instance x s in bag B s , we can randomly select an instance x t from another different bag B t as its negative pair instance. Under this strategy, x s is far away from the average representation K i=1 α i h i of the bag B t , where all α i = 1 K approximately. And the randomly selected instance x t may be too noisy to represent the relational triple of bag B t , so that the model performance may be influenced. Instance x s Relational Triple B t Compared to the random selection strategy, using relational triple representation B t as the negative pair instance for x s is a better choice to reduce the impact of data noise. As the instance x i can be seen as be far away from a weighted representation K i=1 α i h i of the bag B t , where all α i are learnable. Though the instance x s may still be noisy, x s and B t can not belong to the same relational triple.  e sim(hs,h * s ) + t:t =s e sim (hs, Bt)   where sim(a, b) is the function to measure the similarity between two representation vectors a, b, and</p><formula xml:id="formula_6">h s = H(x s ), h * s = H(x * s )</formula><p>are the sentence representations of instances x s , x * s . Besides, to inherit the ability of language understanding from BERT and avoid catastrophic forgetting <ref type="bibr" target="#b14">(McCloskey and Cohen, 1989)</ref>, we also add the masked language modeling (MLM) objective to our framework. Pre-text task MLM randomly masks some tokens in the inputs and allows the model to predict the masked tokens, which prompts the model to capture rich semantic information in the contexts. And we denote this objective as L M (θ).</p><p>Accordingly, the total training objective of our contrastive instance learning framework is:</p><formula xml:id="formula_7">L(θ) = λ(t) N B x∈B L C (x; θ)+L B (θ)+λ M L M (θ)</formula><p>where N = KG is the total number of instances in the batch, λ M is the weight of language model objective L M , and λ(t) ⊂ [0, 1] is an increasing function related to the relative training steps t:</p><formula xml:id="formula_8">λ(t) = 2 1 + e −t − 1</formula><p>At the beginning of our training, the value of λ(t) is relatively small, and our framework CIL focuses on obtaining an accurate bag encoder (L B ). The value of λ(t) gradually increases to 1 as the relative training steps t increases, and more attention is paid to the contrastive instance learning (L C ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Our experiments are designed to verify the effectiveness of our proposed framework CIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Benchmarks</head><p>We evaluate our method on three popular DSRE benchmarks -NYT10, GDS and KBP, and the dataset statistics are listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>NYT10 <ref type="bibr" target="#b20">(Riedel et al., 2010)</ref> aligns Freebase entity relations with New York Times corpus, and it has two test set versions: (1) NYT10-D employs held-out KB facts as the test set and is still under distantly supervised. (2) NYT10-H is constructed manually by <ref type="bibr" target="#b10">(Hoffmann et al., 2011)</ref> GDS <ref type="bibr" target="#b11">(Jat et al., 2018)</ref> is created by extending the Google RE corpus with additional instances for each entity pair, and this dataset assures that the at-least-one assumption of MIL always holds.</p><p>KBP <ref type="bibr" target="#b13">(Ling and Weld, 2012)</ref> uses Wikipedia articles annotated with Freebase entries as the training set, and employs manually-annotated sentences from 2013 KBP slot filling assessment results <ref type="bibr" target="#b7">(Ellis et al., 2012)</ref> as the extra test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Evaluation Metrics</head><p>Following previous literature <ref type="bibr" target="#b12">(Lin et al., 2016;</ref><ref type="bibr" target="#b21">Vashishth et al., 2018;</ref><ref type="bibr" target="#b0">Alt et al., 2019)</ref>, we first conduct a held-out evaluation to measure model performances approximately on NYT10-D and GDS. Besides, we also conduct an evaluation on two human-annotated datasets (NYT10-H &amp; KBP) to further support our claims. Specifically, Precision-Recall curves (PR-curve) are drawn to show the trade-off between model precision and recall, the Area Under Curve (AUC) metric is used to evaluate overall model performances, and the Precision at N (P@N) metric is also reported to consider the accuracy value for different cut-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Models</head><p>We choose six recent methods as baseline models.</p><p>Mintz <ref type="bibr" target="#b15">(Mintz et al., 2009</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method NYT10-D GDS</head><p>AUC P@100 P@200 P@300 P@M AUC P@500 P@1000 P@2000 P@M REDSandT (Christou and Tsoumakas, 2021) A transformer-based DSRE method that manages to capture highly informative instance and label embeddings by exploiting BERT pre-trained model.</p><p>DISTRE <ref type="bibr" target="#b0">(Alt et al., 2019)</ref> A transformer-based model, GPT fine-tuned for DSRE under the MIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation on Distantly Supervised Set</head><p>We summarize the model performances of our method and above-mentioned baseline models in  The overall PR-curve on NYT10-D is visualized in Figure <ref type="figure" target="#fig_1">10</ref>. From the curve, we can observe that:</p><p>(1) Compared to PR-curves of other baseline models, our method shifts up the curve a lot. (2) Previous SOTA model DISTRE performs worse than model RESIDE at the beginning of the curve and yields a better performance after a recall-level of approximately 0.25, and our method CIL surpasses previous two SOTA models in all ranges along the curve, and it is more balanced between precision and recall. (3) Furthermore, as a SOTA scheme of relation learning, MTB fails to achieve competitive results for DSRE. This is because MTB relies on label information for pre-training, and noisy labels in DSRE may influence its model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Evaluation on Manually Annotated Set</head><p>The automated held-out evaluation may not reflect the actual performance of DSRE models, as it gives false positive/negative labels and incomplete KB information. Thus, to further support our claims, we also evaluate our method on two human-annotated datasets, and the results<ref type="foot" target="#foot_1">2</ref> are listed in Table <ref type="table">3</ref> From the above result table, we can see that: (1) Our proposed framework CIL can still perform well under accurate human evaluation, with averagely 21.7% AUC improvement on NYT10-H and 36.2% on KBP, which means our method can generalize to real scenarios well. (2) On NYT10-H, DISTRE fails to surpass PCNN-ATT in metric P@Mean. This indicates that DISTRE gives a high recall but a low precision, but our method CIL can boost the model precision (54.1→63.0) while continuously improving the model recall (37.8→46.0). And the human evaluation results further confirm the observations in the held-out evaluation described above. We also present the PR-curve on KBP in Figure <ref type="figure" target="#fig_12">11</ref>. Under accurate sentence-level evaluation on KBP, the advantage of our model is more obvious with averagely 36.2% improvement on AUC, 17.3% on F1 and 3.9% on P@Mean, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Ablation Study</head><p>To further understand our proposed framework CIL, we also conduct ablation studies.</p><p>We firstly conduct an ablation experiment to verify that CIL has utilized abundant instances inside bags: (1) By removing our proposed contrastive instance learning, the framework degenerates into vanilla MIL framework, and we train the MIL on regular bags (MIL bag ). ( <ref type="formula">2</ref>) To prove the MIL can not make full use of sentences, we also train the MIL on sentence bags (MIL sent ), which repeats each sentence in the training corpus to form a bag 3 .  From Table <ref type="table" target="#tab_5">4</ref> we can see that: (1) MIL bag only resorts to the accurate bag-level representations to train the model and fails to play the role of each instance inside bags; thus, it performs worse than our method CIL (50.8→40.3). ( <ref type="formula">2</ref>) Though MIL sent can access all training sentences, it loses the advantages of noise reduction in MIL bag (40.3→30.6). The noisy label supervision may wrongly guide model training, and its model performance heavily suffers from DS data noise (86.0→63.3). (3) Our framework CIL succeeds in leveraging abundant instances while retaining the ability to denoise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To validate the rationality of our proposed positive/negative pair construction strategy, we also conduct an ablation study on three variants of our framework CIL. We denote these variants as: CIL randpos : Randomly select an instance x s also from bag B s as the positive pair instance for x s . CIL bagpos : Just take the relational triple representation B s as the positive pair instance for x s . CIL randneg : Randomly select an instance x t from another bag B t as the negative pair instance for x s .</p><p>And we summarize the model performances of our CIL and other three variants in Table <ref type="table" target="#tab_6">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>AUC F1 P@M CIL 50.8 52.2 86.0 CIL randpos 49.2(-1.6) 50.9(-1.3) 83.8(-2.2) CIL bagpos 47.8(-3.0) 50.5(-1.7) 79.2(-6.8) CIL randneg 48.4(-2.4) 50.6(-1.6) 78.2(-7.8) As the previous analysis in section 2.4, the three variants of our CIL framework may suffer from DS noise: (1) Both variants CIL randpos and CIL bagpos may construct noisy positive pairs; therefore, their model performances have a little drop (50.8→49.2, 50.8→47.8). Besides, the variant CIL bagpos also relies on the bag encoder, for which it performs worse than the variant CIL randpos (49.2→47.8). ( <ref type="formula">2</ref>) Though the constructed negative pairs need not be as accurate as positive pairs, the variant CIL randneg treats all instances equally, which gives up the advantage of formed accurate representations. Thus, its model performance also declines (50.8→48.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Case Study</head><p>We select a typical bag (see Table <ref type="table" target="#tab_7">6</ref>) from the training set to better illustrate the difference between MIL sent , MIL bag and our framework CIL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work is related to DSRE, pre-trained language models, and recent contrastive learning methods. DSRE Traditional supervised RE systems heavily rely on the large-scale human-annotated dataset, which is quite expensive and time-consuming. Distant supervision is then introduced to the RE field, and it aligns training corpus with KB facts to generate data automatically. However, such a heuristic process results in data noise and causes classical supervised RE models hard to train. To solve this issue, <ref type="bibr" target="#b12">Lin et al. (2016)</ref>   <ref type="bibr" target="#b5">(Defferrard et al., 2016)</ref> to encode syntactic information from the text and improves DSRE models with additional side information from KBs. (4) <ref type="bibr" target="#b0">Alt et al. (2019)</ref> extended the GPT to the DSRE, and finetuned it to achieve SOTA model performance.</p><p>Pre-trained LM Recently pre-trained language models achieved great success in the NLP field. <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref> proposed a self-attention based architecture -Transformer, and it soon becomes the backbone of many following LMs. By pre-training on a large-scale corpus, BERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> obtains the ability to capture a notable amount of "common-sense" knowledge and gains significant improvements on many tasks following the fine-tune scheme. At the same time, GPT <ref type="bibr" target="#b18">(Radford et al., 2018)</ref>, XL-Net <ref type="bibr" target="#b24">(Yang et al., 2019)</ref> and GPT2 <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> are also well-known pre-trained representatives with excellent transfer learning ability. Moreover, some works <ref type="bibr" target="#b19">(Radford et al., 2019)</ref> found that considerably increasing the size of LM results in even better generalization to downstream tasks.</p><p>Contrastive Learning As a popular unsupervised method, contrastive learning aims to learn representations by contrasting positive pairs against negative pairs <ref type="bibr" target="#b8">(Hadsell et al., 2006;</ref><ref type="bibr" target="#b16">Oord et al., 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr" target="#b9">He et al., 2020)</ref>. <ref type="bibr" target="#b23">Wu et al. (2018)</ref> proposed to use the non-parametric instance-level discrimination to leverage more information in the data samples. Our approach, however, achieves the goal of data-efficiency in a more complicated MIL setting: instead of contrasting the instance-level information during training, we find that instance-bag negative pair is the most effective method, which constitutes one of our main contributions. In the NLP field, <ref type="bibr">Dai and Lin (2017)</ref> proposed to use contrastive learning for image caption, and <ref type="bibr" target="#b4">Clark et al. (2020)</ref> trained a discriminative model for language representation learning. Recent literature <ref type="bibr" target="#b17">(Peng et al., 2020)</ref> has also attempted to relate the contrastive pre-training scheme to classical supervised RE task. Different from our work, <ref type="bibr" target="#b17">Peng et al. (2020)</ref> aims to utilize abundant DS data and help classical supervised RE models learn a better relation representation, while our CIL focuses on learning an effective and efficient DSRE model under DS data noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we discuss the long-standing DSRE framework (i.e.MIL) and argue the MIL is not efficient enough, as it aims to form accurate bag-level representations but sacrifices the potential informa-tion of abundant instances inside MIL bags. Thus, we propose a contrastive instance learning method CIL to boost the MIL model performances. Experiments have shown the effectiveness of our CIL with stable and significant improvements over several baseline models, including current SOTA systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Classical MIL framework for DSRE. (Left) A set of instances (x 1 , x 2 , . . . , x m ) with the same KB fact [e 1 , e 2 , r] form a bag B; (Right) The MIL framework trains the DSRE model at bag level( B : αihi).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BERT Encoder: N × Transformer Blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Instance x s ∼ Random Instance x s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Instance x s ∼ Relational Triple B s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Instance x s ∼ Augmented Instance x * s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>[</head><label></label><figDesc>stephen king] 's [maine] chronicles have underscored of point. [stephen king] 's [maine] chronicles have underscored goehr point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example of word substitution: The lowscoring word of is replaced with word goehr, and entity words stephen king and maine are protected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Instance x s Random Instance x t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Instance x s Relational Triple B t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Contrastive Instance Learning</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 10: PR-curve on NYT10-D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: PR-Curve on KBP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>applied the multi-instance learning framework with selective attention mechanism over all instances, and it helps RE models learn under DS data noise. Following the MIL framework, recent works improve DSRE models from many different aspects: (1) Yuan et al. (2019) adopted relation-aware attention and constructed super bags to alleviate the problem of bag label error. (2) Ye et al. (2019) analyzed the label distribution of dataset and found the shifted label problem that significantly influences the performance of DSRE models. (3) Vashishth et al. (2018) employed Graph Convolution Networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, which contains 395 sentences with human annotations. Statistics of various used datasets. Rel.: relation, Ins.: instance and MA: manually annotated.</figDesc><table><row><cell>Dataset</cell><cell># Rel.</cell><cell># Ins.</cell><cell cols="2"># Test Ins. # Test Set</cell></row><row><cell>NYT10-D</cell><cell>53</cell><cell>694,491</cell><cell>172,448</cell><cell>DS</cell></row><row><cell>NYT10-H</cell><cell>25</cell><cell>362,691</cell><cell>3,777</cell><cell>MA</cell></row><row><cell>GDS</cell><cell>5</cell><cell>18,824</cell><cell>5,663</cell><cell>Partly MA</cell></row><row><cell>KBP</cell><cell>7</cell><cell>148,666</cell><cell>1,940</cell><cell>MA</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model performances on NYT10-D and GDS. ( †)/( ‡) marks the results on (NYT10-D column)/(both columns) are reported in the previous papers. Bold and underline indicate the best and the second best scores, and * indicates that our model shows significant gains (p &gt; 0.05) over the second-best model based on Student's t-test.</figDesc><table><row><cell>Mintz  †</cell><cell>10.7 52.3</cell><cell>50.2</cell><cell>45.0</cell><cell>49.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">PCNN-ATT  ‡ 34.1 73.0</cell><cell>68.0</cell><cell>67.3</cell><cell cols="3">69.4 79.9 90.6</cell><cell>87.6</cell><cell>75.2</cell><cell>84.5</cell></row><row><cell>MTB-MIL</cell><cell>40.8 76.2</cell><cell>71.1</cell><cell>69.4</cell><cell cols="3">72.2 88.5 94.8</cell><cell>92.2</cell><cell>87.0</cell><cell>91.3</cell></row><row><cell>RESIDE  ‡</cell><cell>41.5 81.8</cell><cell>75.4</cell><cell>74.3</cell><cell cols="3">77.2 89.1 94.8</cell><cell>91.1</cell><cell>82.7</cell><cell>89.5</cell></row><row><cell cols="2">REDSandT  ‡ 42.4 78.0</cell><cell>75.0</cell><cell>73.0</cell><cell cols="3">75.3 86.1 95.6</cell><cell>92.6</cell><cell>84.6</cell><cell>91.0</cell></row><row><cell>DISTRE  †</cell><cell>42.2 68.0</cell><cell>67.0</cell><cell>65.3</cell><cell cols="3">66.8 89.9 97.0</cell><cell>93.8</cell><cell>87.6</cell><cell>92.8</cell></row><row><cell>CIL  *</cell><cell>50.8 90.1</cell><cell>86.1</cell><cell>81.8</cell><cell cols="3">86.0 91.6 98.4</cell><cell>95.3</cell><cell>88.7</cell><cell>94.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>From the results, we can observe that: (1)</figDesc><table><row><cell>On both two datasets, our proposed framework CIL</cell></row><row><cell>achieves the best performance in all metrics. (2)</cell></row><row><cell>On NYT10-D, compared with the previous SOTA</cell></row><row><cell>model DISTRE, CIL improves the metric AUC</cell></row><row><cell>(42.2→50.8) by 20.4% and the metric P@Mean</cell></row><row><cell>(66.8→86.0) by 28.7%. (3) On GDS, though the</cell></row><row><cell>metric of previous models is already high (≈ 90.0),</cell></row><row><cell>our model still improves it by nearly 2 percentage</cell></row><row><cell>points. (89.9→91.6 &amp; 92.8→94.1).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Method</cell><cell>AUC</cell><cell>NYT10-H F1 P@M AUC</cell><cell>KBP F1</cell><cell>P@M</cell></row><row><cell cols="5">PCNN-A 38.9 47.0 58.6 15.4 31.5 32.8</cell></row><row><cell cols="5">DISTRE 37.8 50.9 54.1 22.1 37.5 46.4</cell></row><row><cell>CIL</cell><cell cols="4">46.0 55.5 63.0 30.1 44.0 48.2</cell></row><row><cell cols="5">Table 3: Model performances on NTY10-H and KBP.</cell></row><row><cell cols="5">PCNN-A denotes PCNN-ATT. F1 refers to Micro-F1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Model performances of three training patterns. All the results inTable 4 are obtained under the same test setting that uses MIL bags (i.e.BERT+ATT) as test units.</figDesc><table /><note>3</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Model performances of our proposed framework CIL and its three variants.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>A typical bag selected from the training set: The bag is constructed with relational triple (john mcgahern, /place borned, dubin), and the first sentence (S1) is clean to express relation /place borned while the second instance (S2) are noisy with true relation /place deaded. S: MIL sent , B: MIL bag and C: CIL.Under MIL sent pattern, both S1, S2 are used for model training, and the noisy sentence S2 may confuse the model. As for MIL bag pattern, S1 is assigned with a high attention score while S2 has a low attention score. However, MIL bag only relies on the bag-level representations, and sentences like S2 can not be used efficiently. Our framework CIL makes full use of all instances (S1, S2) and avoids the negative effect of DS data noise from S2.</figDesc><table><row><cell>Sentence</cell><cell>Predicted Relation</cell></row><row><cell>john mcgahern, the eldest</cell><cell>S: /place borned</cell></row><row><cell>of seven children, was born</cell><cell>B: /place borned</cell></row><row><cell>on nov.12, 1934, in dublin.</cell><cell>C: /place borned</cell></row><row><cell>john mcgahern, whose stark ..., died yesterday in dublin.</cell><cell>S: /place borned B: /place borned C: /place deaded</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">For MTB-MIL, we firstly conduct MTB pre-training to learn relation representations on the entire training corpus and continually fine-tune the model by the MIL framework.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Manual evaluation is performed for each test sentence.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work has been supported in part by National Key Research and Development Program of China (2018AAA0101900), Zhejiang NSF (LR21F020004), Key Technologies and Systems of Humanoid Intelligence based on Big Data (Phase ii) (2018YFB1005100), Zhejiang University iFLY-TEK Joint Research Center, Funds from City Cloud Technology (China) Co. Ltd., Zhejiang University-Tongdun Technology Joint Laboratory of Artificial Intelligence, Chinese Knowledge Center of Engineering Science and Technology (CKCEST).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fine-tuning pre-trained transformer language models to distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hübner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1134</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1388" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
				<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07-18">2020. 13-18 July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Improving distantly-supervised relation extraction through bert-based label &amp; instance embeddings</title>
		<author>
			<persName><forename type="first">Despina</forename><surname>Christou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<idno>CoRR, abs/2102.01156</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>Addis Ababa, Ethiopia; Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2020. April 26-30, 2020. 2017. 2017. December 4-9, 2017</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="898" to="907" />
		</imprint>
	</monogr>
	<note>8th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. 2016. December 5-10, 2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Linguistic resources for 2013 knowledge base population evaluations</title>
		<author>
			<persName><forename type="first">Joe</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuansong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><forename type="middle">M</forename><surname>Strassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<pubPlace>In TAC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00975</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
				<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13">2020. June 13-19, 2020</date>
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledgebased weak supervision for information extraction of overlapping relations</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congle</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
				<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="541" to="550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving distantly supervised relation extraction using word and entity based attention</title>
		<author>
			<persName><forename type="first">Sharmistha</forename><surname>Jat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhesh</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06987</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural relation extraction with selective attention over instances</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1200</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2124" to="2133" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
				<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012-07-22">2012. July 22-26, 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Catastrophic interference in connectionist networks: The sequential learning problem</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Psychology of learning and motivation</title>
				<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
				<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning from Context or Names? An Empirical Study on Neural Relation Extraction</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.298</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3661" to="3672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling relations and their mentions without labeled text</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="148" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RESIDE: Improving distantly-supervised neural relation extraction using side information</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Suman Prayaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiranjib</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1157</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1257" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00393</idno>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
				<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-06-18">2018. June 18-22. 2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2019-12-08">2019. 2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Looking beyond label noise: Shifted label distribution matters in distantly supervised relation extraction</title>
		<author>
			<persName><forename type="first">Qinyuan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1397</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3841" to="3850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-relation cross-bag attention for distantly-supervised relation extraction</title>
		<author>
			<persName><forename type="first">Yujin</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="419" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction via piecewise convolutional neural networks</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1203</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
