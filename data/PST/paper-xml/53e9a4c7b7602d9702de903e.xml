<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Geodesic Framework for Fast Interactive Image and Video Segmentation and Matting *</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xue</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<postCode>55455</postCode>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Minnesota</orgName>
								<address>
									<postCode>55455</postCode>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Geodesic Framework for Fast Interactive Image and Video Segmentation and Matting *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">12811D4E925B772F98FCD6DC8DBC8B85</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T09:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>An interactive framework for soft segmentation and matting of natural images and videos is presented in this paper. The proposed technique is based on the optimal, linear time, computation of weighted geodesic distances to the user-provided scribbles, from which the whole data is automatically segmented. The weights are based on spatial and/or temporal gradients, without explicit optical flow or any advanced and often computationally expensive feature detectors. These could be naturally added to the proposed framework as well if desired, in the form of weights in the geodesic distances. A localized refinement step follows this fast segmentation in order to accurately compute the corresponding matte function. Additional constraints into the distance definition permit to efficiently handle occlusions such as people or objects crossing each other in a video sequence. The presentation of the framework is complemented with numerous and diverse examples, including extraction of moving foreground from dynamic background, and comparisons with the recent literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The segmentation of natural images and videos is one of the most fundamental and challenging problems in image processing. One of its applications is to extract the foreground object (or object of interest) out of the cluttered background, and, for example composite it onto a new background without visual artifacts (see also <ref type="bibr" target="#b4">[4]</ref> for additional applications in video). For complex images, as well as subjective applications, there can be more than one interpretation of the foreground or objects of interest (in absence of higher level knowledge), thus making the task ill-posed and ambiguous. It is often imperative then to incorporate some user intervention, which encodes prior information, into the process. Specifically, the user can draw rough scribbles labeling the regions of interest and then the image/video is automatically segmented. The user is allowed to add more scribbles to achieve the ideal result, although of course, the goal is to minimize as much as possible the user effort.</p><p>Closely connected to the segmentation of objects of interest, image and video matting refers to the process of reconstructing the foreground/background components and the alpha value (transparency) of each pixel. This is important for applications such as extracting hair strands or blurry edges, as well as for compositing. Being inherently under-constrained (solving for three components, F (foreground), B (background), and α transparency, with only the observed color), the matting problem also requires priors, such as user interactions, which could be in the form of scribbles as in the segmentation task, or a complete trimap.</p><p>In this paper, we propose a fast weighted-distance-based technique for image and video segmentation and matting from very few and roughly placed user scribbles (often just one scribble for the foreground and one for the background). The distance (geodesic) computation is linear in time, and thereby optimal (with minimal memory requirements as well). The weights are based on simple properties such as spatial and temporal gradients, while more sophisticated features can be naturally included as well. The proposed framework can handle diverse data, including dynamic background, moving cameras, and objects crossing each other in the video.</p><p>Following a brief literature review, Section 2, we describe the framework for segmenting and matting still images, Section 3. Examples and comparison with the literature are presented in this section as well. Then, we extend it to video applications, where a long video can be processed with little user interaction, Section 4. We explain how to add constraints to the distance computation to handle moving objects occluding each other, e.g., people/objects crossing each other. We illustrate our method with additional video examples in Section 5, and conclude and discuss future research in Section 6. Before proceeding, let us explicitly present the key attributes of the proposed framework: 1. It is based on weighted distance functions (geodesics), thereby solving a first order geometric Hamilton-Jacobi equation in computationally optimal linear time. This makes the proposed framework natural for user-interactive 978-1-4244-1631-8/07/$25.00 ©2007 IEEE processing of images and videos. 2. It produces very good, state-of-the-art results, with very few user provide scribbles and very simple attributes defining the weights in the distance computation. We often use just a couple of rough scribbles for still images (one for the foreground and one for the background) and scribble one frame every 70 or so for videos. 3. It applies to a large class of natural data, and since it avoids off-line learning, it is not limited to pre-observed and classified classes and to the availability of ground-truth and hand segmented data. 4. It can handle dynamic background in video as well as crossing objects of interest. 5. The framework is general so that additional attributes can be naturally included in the weights for the geodesic distances if so required for a particular type of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>One important class of related works is based on energy formulations which are minimized via discrete optimization techniques. The pioneering graph cuts technique, <ref type="bibr" target="#b5">[5]</ref>, addresses the foreground/background interactive segmentation in still images via max-flow/min-cut energy minimization. The energy balances between the probability of pixels belonging to the foreground (likelihood) and the edge contrast, imposing regularization. The user-provided scribbles collect statistical information on pixels and also serve as hard constraints. The Grabcut algorithm, <ref type="bibr" target="#b14">[14]</ref>, further simplifies the user interaction. Scribbles can be interactively added to improve the initial segmentation. Full color statistics are used, modeled as mixtures of Gaussians (here, in contrast, we use fast kernel density estimation), and these are updated as the segmentation progresses. This can help but also hurt by propagating segmentation errors. Very good and fast results were demonstrated with this technique. A number of methods have been proposed extending this framework, aiming at devising more sophisticated energy formulations and at extending it to higher dimensions (video). The Bilayer approach, <ref type="bibr" target="#b8">[8]</ref>, segments videos with basically static background. It incorporates an additional second order temporal transition prior term and a motion likelihood term. Each frame is segmented via graph cuts, conditionally dependent on the previous two frames. Although excellent results are reported for a particular type of videos, this method makes assumptions about the different behaviors of foreground and background pixels and deals with videos with mostly static backgrounds (they do permit a moving object in the background as long as it is different enough from the foreground). Moreover, it needs to learn the motion statistics, which is very useful as they have cleverly incorporated in their system, but requires the availability of pre-segmented ground-truth training data and of video classes (to train and apply with videos having the same type of motion).</p><p>Interactive video cutout, <ref type="bibr" target="#b18">[18]</ref>, presents a system where the user draws scribbles in 3D space. A hierarchical meanshift preprocess is employed to cluster pixels into supernodes, which greatly reduces the computation of the mincut problem. In <ref type="bibr" target="#b9">[9]</ref>, the author uses random walks for soft image segmentation. Each pixel is assigned the label with maximal probability that a random walker reaches it when starting from the corresponding scribbles. The authors of <ref type="bibr" target="#b20">[20]</ref> propose an MRF framework to solve segmentation and matting simultaneously. The basic idea is to minimize the fitting error of the matte while maintaining its smoothness. The uncertainties (0 for the scribbles and 1 for all unknown pixels) are propagated to the rest of the image using belief propagation. Once the alpha values are found, the F and B components are estimated. In <ref type="bibr" target="#b12">[12]</ref>, a local linear relation between the alpha values and image intensities is assumed, that is, the pixel's alpha value can be immediately determined in a local region if its intensity is known. The matting problem is solved by minimizing a cost function combining the prediction error, the regularization of alpha values, and the user-supplied scribbles which indicate constraints to the optimization problem.</p><p>Poisson matting, <ref type="bibr" target="#b15">[15]</ref>, and Bayesian matting, <ref type="bibr" target="#b7">[7]</ref>, are two important matting techniques that use trimaps as inputs. Poisson matting computes the alpha matte by solving the second order Poisson equation with Dirichlet boundary conditions. <ref type="foot" target="#foot_0">1</ref> An assumption is made by neglecting the gradients of F and B, considering the matte gradient proportional to the image gradient. Additional operations are performed to adjust to local regions. Bayesian matting simultaneously estimates F , B, and α by maximizing a posterior probability. For each pixel in the trimaps region, it models the known F and B colors around as mixture of oriented Gaussians in color space (again, we use fast kernel densities instead). An (F, B, α) triplet is computed as the one that most probably generates the observed color of that pixel. This technique is applied to videos in <ref type="bibr" target="#b6">[6]</ref>, where the trimap is temporally propagated using optical flow and the matte is pulled out individually in each frame by the Bayesian matting algorithm. Explicit optical flow is not used in our method, although it could be incorporated as part of the weights in the geodesic computation.</p><p>After this paper was submitted for publication, a few additional matting techniques have been published. The spectral matting technique, <ref type="bibr" target="#b10">[10]</ref>, automatically computes a set of soft matting components via a linear transformation of the smallest eigenvectors of the matting Laplacian matrix <ref type="bibr" target="#b12">[12]</ref>. These components are then selected and grouped into semantically reasonable mattes either in an unsupervised or supervised fashion. The main drawback of this algorithm is its high computational cost -it takes several minutes to compute the matting components for small sized images. In addition, it is not intuitive where to place the constraints. The authors of <ref type="bibr" target="#b19">[19]</ref> proposed an improved color sampling method for natural image matting, and demonstrated very good performance. The authors in <ref type="bibr" target="#b17">[17]</ref> implemented an interface for interactive realtime matting. The user roughly tracks the boundary with a self-adjustable brush. Like in <ref type="bibr" target="#b19">[19]</ref>, the matte is pulled out in local regions, solving a soft graph-labeling problem. Flash cut, <ref type="bibr" target="#b16">[16]</ref>, extracts the foreground layers of flash/no-flash image pairs, using the prior information that only the foreground is significantly brightened. This information is incorporated in an graph cut energy framework. The segmentation algorithm is shown to tolerate some amount of foreground motion and camera shake.</p><p>Our work is inspired by <ref type="bibr" target="#b23">[23]</ref>, where the authors, following <ref type="bibr" target="#b11">[11]</ref>, show how to use distance functions for image colorization. As here, these distances are optimally computed in linear time <ref type="bibr" target="#b22">[22]</ref>. This was then extended in <ref type="bibr" target="#b13">[13]</ref> for segmentation. In contrast with this work, we use significantly less scribbles per image (thanks in part to a more efficient modeling of the corresponding probability distribution functions), see Figure <ref type="figure" target="#fig_0">1</ref>, extend the work to video, and also produce explicit mattes (F , B, and α). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">General framework: Still images</head><p>As discussed in the introduction, our algorithm starts from two types of user-provided scribbles, F for foreground and B for background, roughly placed across the main regions of interest. Now the problem is how to learn from them and propagate this prior information/labeling to the entire image.</p><p>We use the geodesic distance from these scribbles to </p><p>where C s1,s2 (p) is a path connecting the pixels s 1 , s 2 (for p = 0 and p = 1 respectively). The weights W are set to the gradient of the likelihood that a pixel belongs to the foreground (resp. background), i.e., W = ∇P F (x). This likelihood is obtained from the samples on the provided scribbles in Luv color space, i.e., P F (x) = P r(x|F ) P r(x|F )+P r(x|B) , where P r(x|F) is the color PDF of Ω F , obtained via the fast kernel density estimation ( <ref type="bibr" target="#b21">[21]</ref>) (same process for the background PDF). A pixel is close in this metric to a scribble in the sense that there exists a path along which the likelihood function does not change much, Figure <ref type="figure">2(d)</ref>. Following <ref type="bibr" target="#b22">[22]</ref>, we can efficiently compute the distances, in optimal linear time, and assign each pixel to the label with the shorter distance. The user can progressively add new scribbles to achieve the desired result, although often a single scribble for the foreground and one for the background (regardless of how cluttered it is), is sufficient. If a refinement step is needed, a narrow band is spanned across the current boundaries (see Figure <ref type="figure">2(b)</ref>), and its borders serve as new F and B scribbles, thereby reducing the computational cost just to a few pixels in the band, while at the same time refining the likelihood functions and locally adapting them to the region of interest.</p><p>Once this distance has been obtained, the alpha channel inside the band is explicitly computed as</p><formula xml:id="formula_1">ω l (x) = D l (x) -r • P l (x), l ∈ {F, B},<label>(3)</label></formula><formula xml:id="formula_2">α(x) = ω F (x) ω F (x) + ω B (x) ,<label>(4)</label></formula><p>where P l (x) is locally recomputed using the feature vector (L, u, v, τ ), τ ∈ [0, 1] parameterizes the band along the boundary (leading to local PDF estimations), and is periodic with period 1 if the curve is closed (see Figure <ref type="figure">2(b)</ref>). r controls the smoothness of the edges. When r = 0, α(x) = P F (x); when r → ∞, α(x) becomes hard segmentation (typically 0 ≤ r ≤ 2 in our examples). This alpha matte combines the weighted distance (measuring how "close" the pixel is to the scribble) and the probability based on the fast kernel density estimation (measuring how probable is its color). Note that regularization, e,g, anisotropic diffusion of α, can be applied inside the band as well if needed. Since this is done locally, virtually no computational cost is added.</p><p>After the matte α is computed, we follow the method in <ref type="bibr" target="#b20">[20]</ref> to estimate the F x and B x components (in Luv space) for each pixel x inside the band. We randomly sample the foreground and background colors in the neighborhood of x and use the pair that gives the minimal fitting error:</p><formula xml:id="formula_3">(F x , B x ) = arg min Fi,Bj F i α x + B j (1 -α x ) -I x ,<label>(5)</label></formula><p>where i ∈ N (x) ∩ Ω F , j ∈ N (x) ∩ Ω B , F i , B j are foreground and background colors sampled on the (band boundary) scribbles within the window N (x) centered at x, and I x is the observed color.</p><p>With these components, we can now paste the object onto a new background if desired, with no noticeable visual artifacts by the simple matting equation</p><formula xml:id="formula_4">C * x = F x α x + B * x (1 -α x )</formula><p>, where the composite color C * x is a linear combination of foreground color F x and the new background color B *</p><p>x for every pixel x in the image. Figure <ref type="figure" target="#fig_2">3</ref> shows our results for still images. Note how simple scribbles can handle cluttered and diverse images. Figure <ref type="figure" target="#fig_3">4</ref> presents comparisons with the work in <ref type="bibr" target="#b20">[20]</ref> , Photoshop Extract Filter <ref type="bibr" target="#b0">[1]</ref>, Photoshop CS3 Quick Selection &amp; Refine Edge tools <ref type="bibr" target="#b3">[3]</ref> , Corel Knockout2 <ref type="bibr" target="#b2">[2]</ref>, and Spectral Matting <ref type="bibr" target="#b10">[10]</ref> (note how our proposed approach needs significantly less scribbles). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Interactive video segmentation and matting</head><p>The above described framework is now extended to videos, modeled as 3D images, in which every pixel has six neighbors, four spatial and two temporal (except the ones on the frame borders). The scribbles, drawn on one or several frames, propagate throughout the whole video by weighted distances in spatio-temporal space. In particular, spatial and temporal gradients of the likelihood function are used to define the weight W in the geodesic computation in Equation <ref type="bibr" target="#b2">(2)</ref>. Note that there is no explicit use of optical flow in the framework (or motion models as in the works described in Section 2), thereby not only simplifying the computations but also permitting to deal with dynamic background and not limiting the work to pre-specified motion classes. As we will see in the experimental section, this simple model is already very useful for numerous scenarios. We now introduce some additional extensions to make it more general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Constrained spatio-temporal distance</head><p>In still images, a single F scribble and a single B scribble always return two connected components. This can be easily proved by the triangle inequality property of the distance function (this also helps to prove the robustness of the method with respect to the exact placement of the scribbles, see <ref type="bibr" target="#b23">[23]</ref>). If the user marks a circle of B scribble around the object, all the exterior region will be classified as background. However, this is no longer guaranteed in the 3D spatio-temporal case. Consider the simple scenario in Figure <ref type="figure" target="#fig_4">5</ref>. Two objects with similar color/feature distributions move towards each other, cross, and split apart. The inside of the tube has low distances to the F scribble (shown in red). The F scribble in object A propagates to the frames with occlusions, and then backwards to object B (B refers now to the second object in Figure <ref type="figure" target="#fig_4">5</ref> and not to the background value). Although the user might intend to separate object A as foreground in the initial frame, object B is mistakenly cut out because of the connectivity in 3D space (such connectivity doesn't occur in still images). This phenomenon happens when undesired objects in the background touch the foreground in a certain frame, and the error spreads temporally throughout all frames.</p><p>We address this problem with very limited extra computation. To eliminate the branch formed by the undesired object before occlusion, we simply constrain the propagation to be temporally non-decreasing, and Equation ( <ref type="formula" target="#formula_0">2</ref>) is replaced by:</p><formula xml:id="formula_5">d(s 1 , s 2 ) := min Cs 1 ,s 2 1 0 W dp, s.t. t 1 ≤ t 2 if p 1 ≤ p 2 ,<label>(6)</label></formula><p>where p 1 , p 2 ∈ [0, 1] indicate any two positions on C s1,s2 (p) and t 1 , t 2 are their corresponding time coordinates. In other words, d(s 1 , s 2 ) is minimized among the paths that temporally go forwards. Of course we can also constrain the distance function in the opposite direction. However, it becomes the same definition if we let the video play reversely.</p><p>In the discrete scenario, the temporal links (the links that connect temporal neighbors) are replaced by directed links, i.e., the weight of going backwards in time is set to be infinity. This simple modification leads to the correct segmentation before the occlusion, but confusion might still exists after the occlusion (Figure <ref type="figure" target="#fig_4">5(c)</ref>). We can further remove the wrong branch using the same approach, but now in the opposite direction. This can be done by specifying a point in the desired tube at a latter time, letting it propagate backwards within the tubes, constrained to move only backwards. Figure <ref type="figure" target="#fig_4">5</ref> illustrates the process. As a result, the ambiguity is removed in frames where the objects are disconnected within the frame. Figure <ref type="figure" target="#fig_5">6</ref> shows the example of two people walking. The user desired to segment the person initially on the right. The two people are merged as a single object when they cross each other (since they share the features that are used to compute the weighted distance). The second column shows the results using the distance function without the constraint. The wrong segment appears in every frame (again, see Figure <ref type="figure" target="#fig_4">5</ref>(a)). The third column shows the result by the constrained distance function. We can see that the error is removed before and after the intersection. Adding scribbles in the intersection frames will manage to separate them also there, see below, but this is left without in this figure to illustrate the power of the "tubing" effect just described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Interactive refinement</head><p>For individual frames where occlusion actually happens and can not be fixed by the "tubing" approach described above, the user simply provides extra scribbles to segment the object. Since the color distribution might be inadequate to differentiate the objects (this is what led to their merge in the first case), we switch to another contrast sensitive weight to be used for the geodesic distance computation in Equation <ref type="bibr" target="#b2">(2)</ref>. This shows the power of the framework, features can be adapted to the problem at hand. For discrete images, the new feature is defined as W pq := I p -I q , where p and q are two adjacent pixels and I is the color vector in Luv space. Figure <ref type="figure" target="#fig_6">7</ref> shows how the user separates the two persons using the new weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional video experimental results</head><p>We test our algorithm on three videos of 71, 79 and 78 frames respectively. We mark scribbles on two frames for The columns correspond to the original frames, alpha matte, composites on a white background, and composites on a new movie. Finally, we compare our approach with the rotoscoping algorithm in <ref type="bibr" target="#b4">[4]</ref> for the video in Figure <ref type="figure">8</ref> (we only refer to the segmentation/tracking part, which is the contribution of our paper, and not the very nice special effects they show after the segmentation is obtained). Our approach has a number of advantages over this work: (a) We need significantly less user interaction. In <ref type="bibr" target="#b4">[4]</ref> the user basically needs to draw the boundaries for all keyframes by hand (about every 10 frames for this video), while our method only requires very few rough scribbles, see Figure <ref type="figure">8</ref>. (b) We explicitly compute the alpha matte, while <ref type="bibr" target="#b4">[4]</ref> gives spline approximations of the detected boundaries (explicit computation of the matte was not in the original goals of <ref type="bibr" target="#b4">[4]</ref> for their applications). (c) Our method can adapt to a wide variety of motions while the algorithm in <ref type="bibr" target="#b4">[4]</ref> easily loses track of the object, especially when part of the object moves out of the  frame, requiring further user intervention. To better illustrate the comparison, we generate the boundaries by thresholding and dilating the alpha matte obtained by our method. A few frames are shown in Figure <ref type="figure" target="#fig_0">11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>We presented a geodesics-based algorithm for (interactive) natural image and video segmentation and matting. We introduced the framework for still images and extended it to video segmentation and matting. We added constraints to the distance function in order to handle objects that cross each other in the video temporal domain. We showed examples illustrating the application of this framework to very different images and videos, including videos with dynamic background and moving cameras. Another application of our approach is to speed up available image matting algorithms (e.g. <ref type="bibr" target="#b20">[20]</ref>). A narrow band trimap is quickly generated from a few scribbles, and then a different matting algorithm is applied. Figure <ref type="figure" target="#fig_0">12</ref> shows our method working in conjunction with <ref type="bibr" target="#b20">[20]</ref>.  <ref type="bibr" target="#b4">[4]</ref>, obtained by their provided interface. The small squares are the control points of the splines. Bottom row: Results from our approach, obtaining similar segmentation with significantly less user intervention (see Figure <ref type="figure">8</ref>).</p><p>Although the proposed framework is general, we mainly exploited weights in the geodesic computation that depend on the pixel value distributions. As such, in this form the algorithm works best when these distributions do not significantly overlap. In principle, this can be solved with enough user interactions, but could be tedious, and would be better to solve this by enhancing the features used in deriving the weights. Our current efforts are concentrated on enhancing the features we currently use for weighting the geodesic. Also, we are investigating how to naturally add a regularization term into the model, without having to perform this as a post-processing step as currently done. Results in these directions will be reported elsewhere.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Figures (a)-(d) show the user inputs and results from [13]. Figures (e)-(h) correspond to the new inputs and results for the same images, leading to better results with less scribbles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 . 1 0</head><label>21</label><figDesc>Figure 2. (a) A hard segmentation (white curve) is quickly found by a few scribbles. (b) Automatically generated trimap, a narrow band around the white curve, and new automatically generated local scribbles (borders of the band). (c) Obtained segmentation and alpha matting. (d) PF (x). Dark indicates low probabilities and white high probabilities. (Note that this is not the final alpha matte.) (e) DF (x). (f) DB(x). Blue indicates low distances and red high distances.</figDesc><graphic coords="3,315.21,144.39,67.46,52.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Left column: original images with user-provided scribbles. Blue for foreground and green for background. Middle column: Computed alpha matte. Right column: Foregrounds pasted on blue backgrounds (blue (constant) backgrounds are selected since they often permit much more careful inspection of the results than pasting on cluttered backgrounds).</figDesc><graphic coords="4,320.01,233.67,67.58,89.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of our results (first two rows, first column) with [20] (first two rows, second column), Photoshop Extract Filter [1] (first two rows, third column), Photoshop CS3 Quick Selection &amp; Refine Edge tools [3] (last two rows, first column), Corel Knockout2 [2] (last two rows, second column), and Spectral Matting [10] (last two rows, last column). The first and third rows are the user inputs. The second and last rows are the corresponding results on blue background. ([1] and [2] require complete trimaps.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Tubes in 3D space, where t1 &lt; t2 &lt; t3 (a) Although the scribbles in the first frame intend to separate A, the F scribble (red) reaches the object B by a path in 3D space where both objects A and B overlap. (b) The scribble propagation is constrained to move forward and the branch between t1 and t2 is eliminated. (c) The user specifies a pixel in A at t3 and lets it propagate backwards. The branch of B between t2 and t3 is removed. (d) Result with the proper separation of the object A.</figDesc><graphic coords="6,72.09,187.59,96.38,96.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. A video example of two people crossing. Left column: original video. Scribbles drawn on the first frame. Middle column: Segmented results using unconstrained distance function. Right column: Segmented results using constrained distance function. See text for details.</figDesc><graphic coords="6,312.81,338.79,72.38,54.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. (a) Original segmentation obtained by gradients of the PDF. (b) The user adds new scribbles. (c) Segmentation results obtained with the new geodesic distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .Figure 9 .</head><label>89</label><figDesc>Figure 8. Video example 1. (a total of 71 frames)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .Figure 11 .</head><label>1011</label><figDesc>Figure 10. Video example 3. (a total of 78 frames)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that in contrast with this, we solve a first order Hamilton-Jacobi equation, which is computationally more efficient.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>* Work supported by ONR, NGA, NSF, DARPA, and ARO.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ADOBE SYSTEMS (a) (b) (c) (d) Figure 12. Our approach for speeding up [20]. (a) User inputs. (b) Boundary quickly found by our algorithm with the very few scribbles</title>
		<imprint>
			<publisher>Adobe Photoshop User Guide</publisher>
		</imprint>
	</monogr>
	<note>Trimap as input of [20]. (d) Foreground extracted by [20</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Incorp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Knockout User Guide. COREL CORPORATION</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="http://www.adobe.com/products/photoshop/photoshop" />
		<title level="m">Adobe Photoshop CS3 New Features</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keyframe-based tracking for rotoscoping and animation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH&apos;04</title>
		<meeting>SIGGRAPH&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary and region segmentation of objects in n-d images</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<biblScope unit="volume">01</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">105</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video matting of complex scenes</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH &apos;02</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="243" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A bayesian approach to digital matting</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2001-12">2001. December 2001. 2</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilayer segmentation of live video</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 2006</title>
		<meeting>IEEE CVPR 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="53" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random walks for image segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Grady</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1768" to="1783" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spectral matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Acha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 2007</title>
		<meeting>IEEE CVPR 2007</meeting>
		<imprint>
			<date type="published" when="2005">June 2007. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<title level="m">Colorization using optimization. SIGGRAPH&apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="689" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A closed form solution to natural image matting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 2006</title>
		<meeting>IEEE CVPR 2006</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="61" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interactive image segmentation via adaptive weighted distances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Protiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1046" to="1057" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grabcut: Interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
		<title level="m">Poisson matting. In SIGGRAPH&apos;04</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="315" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flash cut: Foreground extraction with flash and no-flash image pairs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 2007</title>
		<meeting>IEEE CVPR 2007</meeting>
		<imprint>
			<date type="published" when="2003">June 2007. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soft scissors: An interactive tool for realtime high quality matting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH&apos;07</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Colburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<title level="m">Interactive video cutout. SIG-GRAPH&apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimized color sampling for robust matting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE CVPR 2007</title>
		<meeting>IEEE CVPR 2007</meeting>
		<imprint>
			<date type="published" when="2003">June 2007. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An iterative optimization approach for unified image segmentation and matting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE ICCV 2005</title>
		<meeting>IEEE ICCV 2005</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2, 4, 5, 7, 8</date>
			<biblScope unit="page" from="936" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved fast gauss transform and efficient kernel density estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gumerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE ICCV 2003</title>
		<meeting>IEEE ICCV 2003<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">O(n) implementation of the fast marching algorithm</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartesaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="399" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast image and video colorization using chrominance blending</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
