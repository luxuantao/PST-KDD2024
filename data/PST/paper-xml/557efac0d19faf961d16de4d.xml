<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Risk-Sensitive Control and Dynamic Games for Partially Observed Discrete-Time Nonlinear Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, ZEEE</roleName><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>James</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, ZEEE</roleName><forename type="first">John</forename><forename type="middle">S</forename><surname>Baras</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Elliott</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Systems Engineering and Cooper-ative Research Centre for Robust and Adaptive Systems</orgName>
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>ACT 0200</postCode>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Institute for Systems Research</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics and Applied Probability</orgName>
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<postCode>T6G 2G1</postCode>
									<settlement>Edmonton, Alberta</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Risk-Sensitive Control and Dynamic Games for Partially Observed Discrete-Time Nonlinear Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">955D8681B836DBED7B87B82CA83DC1EA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we solve a finite-horizon partially observed risk-sensitive stochastic optimal control problem for discrete-time nonlinear systems and obtain small noise and small risk limits. The small noise limit is interpreted as a deterministic partially observed dynamic game, and new insights into the optimal solution of such game problems are obtained. Both the risk-sensitive stochastic control problem and the deterministic dynamic game problem are solved using information states, dynamic programming, and associated separated policies. A certainty equivalence principle is also discussed. Our results have implications for the nonlinear robust stabilization problem. The small risk limit is a standard partially observed risk-neutral stochastic optimal control problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>ECENT interest in risk-sensitive stochastic control prob-R lems is due in part to connections with H , or robust control problems and dynamic games. The solution of a risksensitive problem leads to a conservative optimal policy, corresponding to the controller's aversion to risk.</p><p>For lineadquadratic risk-sensitive problems with full state information, Jacobson [ 171 established the connection with dynamic games. The analogous nonlinear problem was studied recently, and a dynamic game is obtained as a small noise limit <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[12]</ref>, <ref type="bibr">[18]</ref>, <ref type="bibr" target="#b23">[27]</ref>. A risk-neutral stochastic control problem obtains as a small risk limit <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr">[18]</ref>.</p><p>Whittle <ref type="bibr" target="#b22">[26]</ref> solved the discrete-time lineadquadratic risksensitive stochastic control problem with incomplete state information and characterized the solution in terms of a certainty equivalence principle. The analogous continuoustime problem was solved by Bensoussan and van Schuppen <ref type="bibr" target="#b3">[4]</ref>, where the problem was converted to an equivalent one with full state information. A transformation technique has also been used to solve partially observed linear/quadratic H , and dynamic game problems <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b19">[23]</ref>, <ref type="bibr" target="#b20">[24]</ref>, and others). The nonlinear continuous-time partially observed risk-sensitive stochastic control problem was considered by Whittle <ref type="bibr" target="#b24">[28]</ref>, and an approximate solution was stated using a certainty equivalence principle when the noise is small; these results are not rigorous, but are very insightful.</p><p>In this paper we consider the finite-horizon partially observed risk-sensitive stochastic control problem for discretetime nonlinear systems. The solution to this problem together with large deviation limits lead to new insights into the optimal solution of partially observed dynamic game problems and related robust control problems.</p><p>The risk-sensitive stochastic control problem is solved by defining an information state and an associated value function and applying dynamic programming (Section II). The dynamic programming equation is a nonlinear infinite-dimensional recursion. Our approach is motivated by the method used by Bensoussan and van Schuppen <ref type="bibr" target="#b3">[4]</ref> and the well-known separation method for risk-neutral problems. The resulting optimal controller is expressed in terms of a separated policy through the information state. The derivation of this solution does not involve asymptotic methods.</p><p>In Section I11 we obtain the small noise limit of both the information state and the value function. Logarithmic transformations are employed in each case. The information state limit is similar to large deviations limit results for nonlinear filters [l], [15], [16], [19], <ref type="bibr" target="#b16">[20]</ref>, where the limit filter can be used as an observer for the limit deterministic system. The limit of the value function also satisfies a nonlinear infinitedimensional recursion, which in Section IV is interpreted as the dynamic programming equation for a deterministic partially observed dynamic game. As a by-product, we obtain an information state and value function for this game and a verification theorem. The optimal output feedback controller for the game is given by a separated policy through the information state. The information state, which depends on the output path, is a function of the state variable and evolves forward in time. The value function is a function of the information state, evolves backwards in time, and determines the optimal control policy. The structure of the controller we obtain is similar to that arising in the solution of the lineadquadratic problem, which involves a pair of Riccati equations, one corresponding to estimation and one to control <ref type="bibr" target="#b12">[14]</ref>, <ref type="bibr" target="#b22">[26]</ref>. We identify a certain saddle point condition under which the certainty equivalence policy proposed by Basar and Bemhard [3] and Whittle <ref type="bibr" target="#b22">[26]</ref>, <ref type="bibr" target="#b24">[28]</ref> is optimal, using our verification theorem. This policy involves the forward dynamic programming recursion for the information state and a backward recursion for the value function for the corresponding dynamic game problem with full state information. This latter value function is (like the information state) a function of the state variable and, consequently, easier to compute.</p><p>0018-9286/94$04.00 0 1994 IEEE The treatment of the robust H,nonlinear output feedback control problem using stochastic control formulations leads naturally to the "correct" feedback structure of an "observer" and "controller." Our approach leads directly to this structure through limiting processes which involve large deviation principles. The method clearly establishes the separation of the feedback policy and provides a framework for evaluating practical recipes. This correspondence and application to the discrete-time, nonlinear, robust, output feedback stabilization problem will be described in detail in a different publication El * Finally, the small risk limit is evaluated in Section V and shown to be a standard risk-neutral partially observed stochastic control problem. The notion of information state and the use of dynamic programming is well known for riskneutral problems (e.g., <ref type="bibr" target="#b18">[22]</ref>, <ref type="bibr" target="#b21">[25]</ref>). The continuous-time case is discussed in James et aE. <ref type="bibr" target="#b17">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE RISK-SENSITIVE CONTROL PROBLEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamics</head><p>On a probability space (52, F, P") we consider a risksensitive stochastic control problem for the discrete-time system on the finite time interval k = 0, 1, 2, . . , M . The process x E represents the state of the system and is not directly measured. The process yE is measured and is called the observation process. This observation process can be used to select the control actions Uk. We will write for the sequence xi, ,zE, etc. &amp; and yk denote the complete filtrations generated by (x:, k , 96, ]E) and y; , k respectively. We assume: i) x; has density p(x) = ( 2 ~) -" / ~e x p ( -1 / 2 ( ~: ( ~) .</p><p>ii) {w;} is an R" valued i.i.d noise sequence with density iii) y ; = 0. iv) {wi} is a real-valued i.i.d noise sequence with density</p><formula xml:id="formula_0">@(w) = ( 2 ~t ) -~/ ~ exp ( -1 / 2 ~~~~~) , independent of x;</formula><p>and {w;}.</p><p>v) b E C1(R" x R", R") is bounded and uniformly continuous.</p><p>vi) The controls Uk take values in U c R", assumed compact, and are yk measurable. We write uk,l for the set of such control processes defined on the interval + E ( w ) = ( 2 r ~) -" / ~e x p ( -I / ~E I w ~~) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IC,. , E . vii) h E C(R") is bounded and uniformly continuous.</head><p>The probability measure P" can be defined in terms of an equivalent reference measure Pt using the discrete analog of Girsanov's Theorem [11]. Under P i , {y;} is i.i.d with density @, independent of {xi}, and xE satisfies the first equation in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UeuO, M -1</head><p>Here, we assume: The parameters p &gt; 0 and E &gt; 0 are measures of risk sensitivity and noise variance. In view of our assumptions, the cost function is finite for all p &gt; 0, E &gt; 0. These parameters will remain Jired throughout Section 11.</p><formula xml:id="formula_1">viii) L E C(R" x R</formula><p>In terms of the reference measure, the cost can be expressed as uniformly continuous. continuous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Information State</head><p>We consider the space L"(R'") and its dual L"*(R"), which includes L1(Rn). We will denote the natural bilinear pairing between L"(R") and Lm*(R") by (7, v) for 7 E Lm*(Rn),v E L"(R"). In particular, for g E L1(R") and v E L"(R") we have We now define an information state process E L"* (R") by for all test functions q in L"(R"), for k = 1, . . , M and go"' E = p E L1 (R"). We introduce the bounded linear operator W E :</p><p>L"(R") -+ L"(Rn) defined by</p><p>The bounded linear operator W : Lm*(Rn) + L"* (R") adjoint to W E is defined by (E'"+*T, 7)) = (7, P ' 7 ) )</p><p>for all T E L"*(R"), 7) E Lw(R").</p><p>The following theorem establishes that U :</p><formula xml:id="formula_2">" is in L1(R"),</formula><p>and its evolution is governed by the operator W E * ; x"'(Uk-1, y;)7))</p><formula xml:id="formula_3">= ( x ' " ' E * ( U k -l , &amp;)a[lt, 7)).</formula><p>This holds for all 9 in L"(R"); hence (2.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The fact that W E * maps L ~( R " )</head><p>into L1(Rn) follows easily from (2.6) and the properties of $', ! P' , and L. The operator E"?' actually maps cb(Rn) into cb(Rn).</p><p>Then we can define a process vi' ' E cb(R") by</p><p>It is straightforward to establish the adjoint relationships for all U E L1(R"), v E Cb(R"), and all k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Alternate Representation of the Cost</head><p>Following [4], we define for U E UO,M-I a cost function associated with the new "state" process a ! ? '.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.4:</head><p>We have for all U E UO, ~-1</p><formula xml:id="formula_4">J'"*'(U) = K'"?'(U). (2.1 1)</formula><p>Proofi By (2.4), = J'"7'(u) using (2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0</head><p>We now define an alternate but equivalent stochastic control problem with complete state information. Under the measure P", consider the state process a: ? ' governed by (2.7) and the cost KF&gt;'(u) given by (2.10). The new problem is to find</p><formula xml:id="formula_5">U* E Uo,~-l minimizing K"9'.</formula><p>Let U;, I denote the set of control processes defined on the interval k,-..,l which are adapted to a(a;,', k I j I 1).</p><p>Such policies are called separated <ref type="bibr" target="#b18">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Dynamic Programming</head><p>The alternate stochastic control problem can be solved using dynamic programming. Consider now the state on the interval k, . . , M with initial condition u:~ e = U E L1(R"):</p><p>The corresponding value function for this control problem is defined for U E L1(R") by Note that this function is expressed in terms of the adjoint process vi*', given by (2.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 2.5 (Dynamic Programming Equation):</head><p>The</p><formula xml:id="formula_6">S P ? '(a, k) = infuEU Et[S@?'(W'* (U, Y ~+ ~) C T , k -t l)] Sp&gt;'(a, M ) = (a, exp$'P).</formula><p>value function S P ? E satisfies the recursion </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We claim that</head><formula xml:id="formula_7">SP?'(a, k) = sP"(a, k; U * ) (2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15)</head><p>For k = M, (2.15) is clearly satisfied. Assume now that for each k = 0, 1,. . . , M.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In. SMALL NOISE L m</head><p>To obtain a limit variational problem as E + 0, we must first obtain limit results for the information state (and its dual). The "sup pairing"</p><formula xml:id="formula_8">(3.1)</formula><p>is defined for p E V, q E Ca(Rn) and arises naturally in view of the Varadhan-Laplace Lemma (see the Appendix):</p><formula xml:id="formula_9">(3.2)</formula><p>(uniformly on compact subsets of VY x Ca(Rn), for each Define operators AP*: 2) + 27, and A': Cb(!Rn) -+ 7 E GI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ca(R*) by</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1%</head><p>With respect to the "sup pairing" (., .), these operators satisfy</p><formula xml:id="formula_10">(3.4) Also, Ap*(u, y): 2 7 -, V is continuous for each y E G; in fact, the map (U, y, p) H A ~* ( u , y)p, U x R x Vr ---t ID is continuous.</formula><p>The next theorem is a logarithmic limit result for the information state and its dual, stated in terms of operators (i.e., semigroups). uniformly in a = (2, U , y, p) E A, by Lemma A.l; where</p><formula xml:id="formula_11">A = B(0, R) x U x B(0, R) x K , and K c DY is compact.</formula><p>This proves the first part of (3.5). The second part is proven similarly. 0</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Risk-Sensitive Value Function</head><p>The next theorem evaluates the small noise limit of the risksensitive value function Due to the induction hypothesis and the properties of APT it is continuous in p , y, U, and bounded in y; all properties uniformly in (U, p) E U x K. Therefore we can choose R large enough so that enough so that both (3.10) is satisfied and</p><formula xml:id="formula_12">WP(AP*(u, y)p, IC -t 1) - Y ER We keep R fixed from now on.</formula><p>Combining the above limits for S P &gt; E and W E we have that We can now proceed with the further computation of (3.9). To complete the proof, we use the continuity of the map The sequence converges uniformly on K, and as a result W@(p, k) is continuous on 27'. This completes the proof. U Remark3.3: In Section IV (3.7) will be interpreted as the optimal cost function (upper value) for a dynamic game problem.</p><p>Remark3.4: Note that there are two large deviation type limits involved in the result of Theorem 3.2. The first is expressed in the use of (3.5) in (3.11) and corresponds to "state estimation" or "observers." The second is embodied in (3.6) and corresponds to the relationship of the stochastic risksensitive optimal control problem to the deterministic game that (3.7), (3.13) imply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A DYNAMIC GAME PROBLEM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamics</head><p>We consider a two-player deterministic partially observed dynamic game for the discrete-time system on the finite time interval k = 0, 1, 2, . . . M , where i) z(0) = z o is the unknown initial condition, and y(0) = 0.</p><p>ii) Player 1 selects the U-valued control U k , which is required to be a nonanticipating functional of the observation path y. We write uk, 1 for the set of such controls defined on the interval k, . . . -+ U such that U j = aj((Yk+l,j).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cost</head><p>for all IC, and P N exp -q p (4.7) ai" N exp-pg, up,' P in probability as E + 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4.2:</head><p>The asymptotic formulas (4.7) are similar to the large deviation limit for nonlinear filtering ([ 151, [16], [19], <ref type="bibr" target="#b16">[20]</ref>). In addition, if L s 0 the recursions (4.4) reduce to estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E E k</head><p>The payoff function for the game is defined for admissible <ref type="figure">~-i</ref> and<ref type="figure">(w, W</ref> where a E D.</p><formula xml:id="formula_13">U E U o ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4.1:</head><p>This formulation treats 20 as part of the uncertainty (to be chosen by nature). A priori knowledge of z c g is incorporated in the cost function a E D. Our theory also applies to the case a = 0, which corresponds to no a priori information, and note lim,,o E log p = 0, where p is the initial density for the risk-sensitive stochastic control problem. One can alternatively select a E 2, and define the initial density by p ( z ) = e, exp (-a(%)/€), where e, is a normalizing constant.</p><p>The (upper) game is defined as follows. Let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J ~( u )</head><formula xml:id="formula_14">= SUP J y U , w, U), ( w , w ) E l z ( [ O , M--l],Rn+l)</formula><p>and the (upper) partially observed dynamic game problem is to find</p><formula xml:id="formula_15">U* E U O , M -~ such that J ~( u * ) ' = inf J p ( u ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UEUO, M -1</head><p>The cost function P ( u ) can be rewritten in the form</p><formula xml:id="formula_16">M-1,</formula><p>This cost function is finite for all p &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Information State</head><p>Motivated by the use of an information state and associated separation policies in solving the risk-sensitive stochastic control problem, given a control policy U E UO, M-1 and an observation path y E Z2([0, MI, R ) , we define an "information state" p i E D and its "dual" qg E Ct,(R") for the game problem by the recursions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Alternate Representation of the Cost</head><p>Define for U E Uo, ~-1 a cost function associated with the new "state" process pg.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 4.3:</head><p>We have for all U E Uo, M-1 J'"(U) = KC"(u).</p><p>(4.9)</p><p>Proof: Iterating (4.4), we see that</p><formula xml:id="formula_17">f M-1 ~ M-1-</formula><p>Substitution of this equality into (4.8) yields (4.9). 0 We now define an alternate deterministic dynamic game problem with complete state information. Consider the state sequence p i with dynamics (4.4) and the cost K ~( u ) : find U* E U;, M-l minimizing KP. Here, U,., denotes the set of separated control policies, through the information state pf, defined on the interval Ic,.-.,l, i.e., those which are nonanticipating functionals of (pr, IC 5 j 5 1). Note that ui,l c uk,1.  Here, the interchange of the minimization over Wk+l, m-1 and maximimization over Yk+l U Remark4.5: We conclude from Theorem 3.2 and 4.4 that the small noise limit of the partially observed stochastic risksensitive problem is a partially observed deterministic dynamic game problem. and is an optimal policy for the partially observed dynamic game problem (Sections IV-A, IV-B) is justified because vk+1, ~-1 is a function of y k + l .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Dynamic Programming</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof: Define</head><p>Wc"(P, IC; U )</p><p>We claim that</p><formula xml:id="formula_18">W P ( p , IC) = wyp, IC; U * )<label>(4.12)</label></formula><p>for each IC = 0, 1, . . , M. For IC = M, (4.12) is true. Assume now that (4.12) holds for IC + l , . . -, M . Then</p><formula xml:id="formula_19">W%, IC; U * ) = sup SUP Y L + ~ E R y&amp;([k+2, MI, R) W*(u;, Y k + l ) P E , 4;+&amp;*k + 1, M -1)) 1 --2P IYk+1I2: P i = P</formula><p>which proves (4.12).</p><p>Next, from (4.12) and setting IC = 0 and p = (Y we obtain W'"(a, 0; U * ) = wqa, 0) 5 wya, 0; U ) for all U E U O , M -~, which implies K@(U*) 5 KP")</p><formula xml:id="formula_20">for all U E Uo, ~-1 .</formula><p>This together with Theorem 4.3 completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>U</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Certainty Equivalence Principle</head><p>Remark 4.7: Theorem 4.6 is the "appropriate" separation theorem for the partially observed dynamic game described in Sections IV-A and IV-B, in that it establishes that the optimal feedback policy is a separated one <ref type="bibr" target="#b18">[22]</ref> through the information state p i which carries all the information from the observations y o , k relevant for control. It is important to note that the solution of this partially observed dynamic game problem involves two infinite dimensional recursions. One is (4.4), which describes the dynamics of the information state, evolves forward in time, and is a dynamic programming equation in view of (3.3). This equation plays the role of an "observer" in the resulting controller and is determined by the control problem at hand, and not prescribed a priori. The information state $(z) is a function of the state variable z E R". The other recursion is (3.7), which describes the computation of the feedback control as a function of the information state, evolves backward in time, and is a dynamic programming equation. The value function Wfi(p, k) is a function of the information state variable p , which takes values in the infinitedimensional space D. An important aspect of our work is to point out this essential difficulty of the nonlinear robust Ha control problem. This is not surprising given that this difficulty is well known in stochastic control. For practical applications, one can try to find suboptimal finite-dimensional schemes that provide performance close to the one predicted by the optimal results obtained here. We are also pursuing the development of numerical schemes that can compute the required recursions (4.4) and (3.7), as well as the incorporation of such schemes into computer-aided design control systems design software based on optimization.</p><p>We now relate the above analysis to certainty equivalence principles suggested by Whittle [26], [28] for the risk-sensitive problem, and by Basar and Bemhard  <ref type="figure">-* -U k ( x k</ref> ) is an optimal feedback policy for this completely where 2; is a set-valued variable. For linear systems with the quadratic cost, the certainty equivazence principle asserts that if J"(u, w, v) is negative definite in (w, U ) (and positive definite in U ) , then U ; = -i i . , ( z k ) is an optimal control for the partially observed risk-sensitive problem <ref type="bibr" target="#b22">[26]</ref>. For nonlinear systems, Whittle's assertion is that this recipe gives a policy which is approximately optimal for the risk-sensitive stochastic control problem <ref type="bibr" target="#b16">[20]</ref>.</p><p>Remark4.8: The variable 2; is set-valued and is closely related to the finite-time observer results of James [19, (3.8)] and our earlier observer design methodology [l], <ref type="bibr" target="#b16">[20]</ref>. Indeed our construction brings out another essential difficulty of the nonlinear problem, which has to do with multivalued (or setvalued) variables for state estimation and control. We will have more to say about this issue in a forthcoming paper.</p><p>We now state a certainty equivalence principle for the partially observed deterministic game problem described in [3, <ref type="bibr">Chapters 5,</ref><ref type="bibr"></ref>   If for all IC = 0,. . . , M and p E 2) we have WYP, IC) = (P, 7 3 is an optimal policy for the partially observed game problem (Sections IV-A and IV-B). Proof: Let</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">( P &gt; E argmax + 7;(C)). (ER*</head><p>Then the minimum stress estimate and the candidate policy defined by (4.19) can be written as Therefore UT is a separated policy. To check the optimality of uF, we apply the verification Theorem 4. can be replaced by a simpler recursion (4.16) involving the upper value T;(z&gt;, x E R". This has obvious computational implications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 4.11:</head><p>A crucial contribution here is that we have identified precisely the condition that one needs to establish the certainty equivalence principle suggested in [3], <ref type="bibr" target="#b22">[26]</ref>, <ref type="bibr" target="#b24">[28]</ref>. The condition is (4.18), i.e., the saddle point condition in (4.21), <ref type="bibr">(4.22)</ref>. One may ask to what extent this condition can be obviated. Or alternatively, can we show under certain assumptions that (4.18) is satisfied? A counterexample to the certainty equivalence principle is given in Bernhardsson <ref type="bibr" target="#b4">[5]</ref>, and hence this principle is not valid in general. Indeed, (4.18) may not hold in general, and the proof of Theorem 4. <ref type="bibr" target="#b8">9</ref> shows that <ref type="bibr">(4.23)</ref> This is not surprising from the point of view of stochastic control, since the certainty equivalence principle is not valid generally in that context. Remark 4.12: Since the partially observed game is the limit of the partially observed risk-sensitive problem, then, if the certainty equivalence principle is valid, the policy (4.19) is an approximate optimal policy for the partially observed risksensitive problem for small E &gt; 0 <ref type="bibr" target="#b24">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark4.13:</head><p>The Bertsekas and Rhodes paper <ref type="bibr" target="#b5">[6]</ref> considers game problems similar to the type discussed here. In the Ianguage of that paper, the information state is a sufficiently informative function, i.e., the deterministic dynamic game analogue of sufficient statistic <ref type="bibr" target="#b21">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. SMALLRISKLIMIT</head><p>In this section we show that a risk-neutral stochastic control problem is obtained if in the risk-sensitive stochastic control problem the risk-sensitivity parameter p tends to zero. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Znformution State</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Risk-Neutral Control Problem</head><p>We again consider the discrete-time stochastic system (2.1) and formulate a partially observed risk-neutral stochastic control problem with cost J E ( U ) = E" L(zf, U [ ) + @(zb) ] (5.4) 1::</p><formula xml:id="formula_21">defined for U E Uo, ~-1 ,</formula><p>where UO, ~-1 , etc., are as defined in Section 11. This cost function is finite for all E &gt; 0.</p><p>We quote the following result from [ll], <ref type="bibr" target="#b18">[22]</ref>, which establishes that the optimal policy is separated through the information state a; satisfying (5.3).</p><p>Theorem 5.3: The unnormalized conditional density a; is an information state for the risk-neutral problem, and the value function defined for U E L1(Rn) by satisfies the dynamic programming equation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Risk-Sensitive Value Function</head><p>The next theorem evaluates the small risk limit of the risksensitive stochastic control problem. Note that normalization of the information state is required.   for all E &gt; 0 sufficiently small. This proves the lemma. 0</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 2 .</head><label>2</label><figDesc>1). For U E U O , M -~ where B. cost The cost function is defined for admissible U E 240, ~-1 by and the partially observed risk-sensitive stochastic control problem is to find U* E U O , M -~ such that JPtE(u*) = inf P ' ( u ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>forUETheorem 2 . 2 :</head><label>22</label><figDesc>L1(R"), 7) E Lm(Rn), we have The information state a ; l ' satisfies the recursion Further, U : " E L1(R") since p E L1(R") and E ~~" maps L1(R") into L1(Rn). Proofi We follow Elliott and Moore [ll]. From (2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>0Remark2. 3 :</head><label>3</label><figDesc>When L G 0, the recursion (2.7) reduces to the Duncan-Mortensen-Zakai equation for the unnormalized conditional density[22].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 2 . 14 ) 9 ,</head><label>2149</label><figDesc>Proofi The proof is similar to Elliott and Moore [ l l , = a] The interchange of minimization and conditional expectation is justified because of the lattice property of the set of controls Theorem 2.6 (Veri$carion): Suppose thatu* E Ut, M-l is a policy such that, for each k = 0,. . . , M -1, U: = Ez(afi'), where Ti;(. ) achieves the minimum in (2.14). Then U* E U O , M -I and is an optimal policy for the partially observed risk-sensitive stochastic control problem (Section 11-B). Proof: We follow the proof of Elliott and Moore [ l l , Theorem 4.71. Define [Chapter 161. 0 -P S ' E (a, k; U ) = Et[(ac", ~, "") IG, "" = a].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 2 .</head><label>2</label><figDesc>15) holds for k + l , . -. , M . Then -P E s ' (G, k; U * ) = Et[Et[(C'"J ( U ; , y;+l)a;' ', ' v&amp;i(Ut+~, M-I)) I yk+l] 1 Gf' ' = from (2.14). This proves (2.15).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>From ( 2 . 7 :</head><label>27</label><figDesc>15), setting k = 0 and a = p E L1(Rn) we obtain --cc ' s ' (p, 0; U') = S""(p, 0) 5 P ( p , 0; U ) for any U E UO, ~-1 . Comparing (2.10) and the definitions of S P ? ', 3" ' above this implies K P ? ' ( U * ) 5 Kr"r'(u) proof. 0 for all U E UO, ~-1 . Using Theorem 2.4, we complete the Remurk2.The significance of Theorem 2.6 is that it establishes the optimal policy of the risk-sensitive stochastic control problem as a separated policy through the process a ; ' ', defined in Section II-C, which serves as an "information state" [22]. The information state af ' is a suficienr srarisric for the risk-sensitive problem [25].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>AD</head><label></label><figDesc>= { p E C(R"): p(x) 5 -y1(zI2 + 7 2 for some 7 E G) and write c ~( R " ) 2 { q E c(P): lq(x)ls C, for some c 2 01. We equip these spaces with the topology of uniform convergence on compact subsets. In the sequel, B ( z , a) c Rp denotes the open ball centered at x E Rp of radius a &gt; 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>AP*P, d = (P, A P d . Theorem 3.1: We have lim,,o 4 log w e* ( U , y)eP/'p = A P * ( ~ 7 Y P, P lime-,o 5 log ' ( U , y)eP/'* = AP(U, y)q (3.5) P in 2) uniformly on compact subsets of U x R x V7 for each y E G, respectively in Cb(Rn) uniformly on compact subsets of U x R x Cb(Rn). Proof: From (2.6), we have 4 log CPL' E* (U, y)e'llEP(z) P Therefore, = AP*(U, Y)P(Z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>'.. 2 :.(3. 8 ). 9 )</head><label>289</label><figDesc>It involves two large deviations type limits, one corresponding to estimation and one to control. Theorem 3The function W"(p, k) defined for p E 2) by exists (i.e., the sequence converges uniformly on compact subsets of 2 7 (y E G)), is continuous on Vr (y E G), and satisfies the recursion pro08 The result is clearly true for k = M because of the second of (2.14), (3.2), and the continuity of p H ( p , 'P) on each 277. Assume the conclusions hold for k+l, . . . , M. Select y E G and K c 277 compact. In what follows C &gt; 0, etc, will denote a universal constant. From Theorem 2.5 and (3.6), we need to compute lim,,o 4 log S P ~ E(eP/Ep, IC) = lime,o 4 log inf ~t [ s ~I '(PI ( U , yi+l) eP/'P, IC + I)] = lim,,o inf 4. I ~~E + [ s ' L ' E ( c ~~' * ( ~, y;+l) UEU . ep/'P, k + 111. The last equality is due to the monotonicity of the logarithm function. Direct calculation verifies the estimate 4. log SP? '(c"&gt; E* ( U , y)ep/Ep, k + 1) 5 C(I+ ~y l ) 1-1 for all U E U, y E R, p E K , E &lt; E' for some E' &gt; 0, and the inclusion 5 log ZP9 e' y)eP/'P E vr(lrl) P for all U E U, y E R, p E K , E &lt; 8, for some y(ly1) E G. The fact that y ( Iy I) depends on Iyl complicates matters a little. If we select R &gt; 0 and consider those y for which (y( 5 R, then there exists YR E G such that 5 log C P ~E * (U, y)ep/EP E D ~R for all U E U, IyI 5 R, p E K, for all E &gt; 0 sufficiently small. Considering the right-hand side of (3.8) we have P VP' ' ( p , IC; U) 6 4 logEt[Sp*'(Cp''* ( U , y;+l)ep/'P, IC + 111 = log k@(y)Sp. E(Cp,e* (U, y)ep/'P, k + 1) dy P + / I v l l R A E = -log{A + B}. P qY(y)Sp'"(Cp"' (U, y)ePIEP, k + 1) dy (3Now using the bounds above we can write and using a standard estimate for Gaussian integrals we obtain 5 -C' (3.10) as E -+ 0, uniformly in U E U , p E K, where C' &gt; 0 if R &gt; 0 is chosen sufficiently large. By the induction hypothesis W P ( p , k + 1) exists and W p ( p , k + 1) = lim 4. logsP&gt; '(ePI'P, k + 1) 6-0 p uniformly on 'DYR. We also have from Theorem 3.1 that uniformly on U x B(0, R) x K, and A"*(u, y)p E 'DYR for all (U, y, p) E U x B(0, R) x K . Consider the function Wp(AP*(u, y)p, k + 1) -1/2pIyl2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>in U E U,p E K,y E B(0, R).Indeed, we follow the proof of LemmaA.l with s ~&gt; ~E ( w ~* ( U , y)eP/'*, IC + 1) = W p ( A P * ( ~, y)p, k + 1) --Iyl2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>replacing exp (F:/E), and 1 WP(AP*(U, YIP, k + 1) -Gl"2 replacing Fa, and a = (U, p )E A = U x K. Then 1 2P WP(A@*(u, y)p, k + 1) --1112) uniformly in U x K. It is crucial that R ischosen as above so that the uniform bound required by condition iii) of Lemma A. 1 is satisfied. Consequently lim,,oVP~ '((p, k; U) = lime+o 5 log (A + B ) = limc+o 4 logA(1 + B/A) P P = sup { WP(A@*(u, y)p, IC + 1) --IyI2} 1 Y ER 2P uniformly on U x K, since lim,,o c/plog (1 + B/A) = 0 from (3.10) and (3.12).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(p, U) H V"&gt;'(p, k; U ) , V7 x U -, R to obtain = lim,_,o inf VP7 '(p, k; U) = inf lime-,O VP9 ' ( p , k; U ) UEU UEU uniformly on K. The last equality holds by the definition (3.6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) E 1 2 ( [ 0 , A4 -11, I?."+1) by Mortensen's method of minimum energy 1=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>and A.* are as defined in (3.3). Note that, in the "sup pairing" notation of (3.1), (4.6) (PE, 4 7 3 = (Pg-1, 4 L )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Theorem 4 . 4 :</head><label>44</label><figDesc>initial condition pg = p E D Consider now the state pp on the interval IC,. . . , A4 with The (upper) value function is defined for p E 2) by W p ( p , IC) = inf SUP M-1y€t2([k+1, MI, R) The value function W p ( p , k) defined by (4.11) is the unique solution of the dynamic programming equation (3.7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>same as (3.7).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Theorem 4 . 6 (</head><label>46</label><figDesc>Verzjication): Suppose that U* E U;, M41 is a policy such that, for each k = 0,. -. , M -1, U ; = -* Uk ( p i ) , where Ei;(p) achieves the minimum in (3.7). Then U* E UO, ~-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>[3] for the game problem. Consider a completely observed dynamic game problem with dynamics and payoff function where the initial state xg is known, player 1 selects U EU;, M-l to minimize J P , and player 2 selects w E Z2( [0, Ml), R") to maximize J". Here, U:, is the set of U-valued controls which are nonanticipating functionals of the state x defined on the interval I C , . . . , Z .Define the upper value (see e.g.,[lo]) for this dynamic game byf;(z) = inf SUP { y L(x1, U l ) -M-1 w E l z ( [ k , M -I ] , R") l=k I M -1 .This function satisfies the dynamic programming equation f:(z) = i n f u ~U -{7;+1(b(z, U ) +. I ) + L ( x , U ) -$lwIz} (4.16) f a 4 = @(. I and if 'ilz(x) E U achieves the minimum in (4.16), then observed game. Whittle 1261, [28] solves the partially observed risk-sensitive stoGhastic control problem by using the solution to the completely observed game (4.16) and the modified filter or "observer" (4.4). He refers to 7; as the future stress, to p i as the past stress, and defines the minimum stress estimate z k of x k by k -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>61.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Theorem 4 . 9 (</head><label>49</label><figDesc>Certainty EquivaZence): Let 7; (z) be the upper value for the full state information game (4.13), (4.14).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>( 4 .</head><label>4</label><figDesc>18) then the policy uce E U;, M-l defined by U r = '&amp;L(Hk) (4.19)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>- 5 k</head><label>5</label><figDesc>= Xi(&amp;)and U T = '6i(X;(pi)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>2 :</head><label>2</label><figDesc>Define the bounded linear operator CE* : L1 (R") + L1 (R") by Theorem 5.1: We have uniformly on bounded subsets of U x R x L1(Rn).Proof: This result follows simply from the definitions (2.6), (5.1). 0 Next, we define a process a; E L1(Rn) by the recursion The process a; is an unnormalized conditional density of z; given &amp;, and (5.3) is known as the Duncan-lvlortensen-Zakai equation [ll],<ref type="bibr" target="#b18">[22]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>5 . 4 :</head><label>54</label><figDesc>(W'(a, 6) = infuEU If U* E U;, M-l is a policy such that, for each k = 0,. -, M -1, ug = ag(~;), where Ei;(a) achieves the minimum in (5.6), then U* E UO, ~-1 and is an optimal policy for the partially observed risk-neutral problem. Remark The function W E ( o , k) depends continuously on a E L1(Rn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Theorem 5 . 5 :</head><label>55</label><figDesc>We have uniformly on bounded subsets of L1(R").Proof: 1) We claim thatSPIE(a, k) = (a, 1) + EW'(u, k) + o(p) (5.8)as p -, 0 uniformly on bounded subsets of L1(R").</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>For k = M, S P ? €(a, M ) = (a, e+*) as p --+ 0, uniformly on bounded subsets of L1(Rn).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>Now 5</head><label>5</label><figDesc>CRexp(-C'/€) where CR, Cl, C2 &gt; 0, and C' &gt; 0 if R is chosen sufficiently large. Also E log A 5 E log exp (F:/E) dx xl&lt;R where R is chosen large enough to ensure that argmaxx,R-F,"(x) C B(0, R) for all a E A and all sufficiently small E. Thus (A.3) Eloga; 5 Fa + 36 for all E &gt; 0 sufficiently small and all a E A. Combining (A.2) and (A.3) we obtain sup IEloga: -Fa] &lt; 36 aEA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>into (2.1). Other choices for the initial density p are possible; see Remark 4.1.</head><label></label><figDesc></figDesc><table><row><cell>") is nonnegative, bounded, and</cell></row><row><cell>ix) @ E C(Rn) is nonnegative, bounded, and uniformly</cell></row><row><cell>Remark 2.1: The assumptions i) through ix) are stronger</cell></row><row><cell>than necessary. For example, boundedness assumption for b</cell></row><row><cell>can be replaced by a linear growth condition. In addition, a</cell></row><row><cell>"diffusion" coefficient can be inserted</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>, E, and note that U E &amp;, 1</figDesc><table><row><cell>if and only if for each j E [k, I] there exists a function</cell></row><row><cell>iii) Player 2 selects the R" x R-valued disturbance</cell></row><row><cell>( W k , W k ) , which is a square summable open-loop</cell></row><row><cell>sequence. We let Zz([k, I], Rp) denote the set of square</cell></row><row><cell>summable RP-valued sequences defined on the interval</cell></row></table><note><p><p>k,...,l(p = n, 1, n + 1).</p>Zj: &amp;-'+l)"</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recommended by Associate Editor V. Solo. This work was supported in part by Grant NSFD CDR 8803012 through the Engineering Research Centers Program.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Assume now that (5.8) is true for k + 1, . . . , M. Then VPv'(a, IC; U )</p><p>2 Et[S""(W'* ( U , y;+l)a, k + l)] r $ ' ( y ) S P + ( C f q u , y)a, k + 1) dy</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>=L</head><p>as p + 0 uniformly on bounded subsets of U x L1(Rn). Thus, using the continuity of (a, p ) t) W ' ( a , k; U )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S p y ' ( a , k)</head><p>uniformly on bounded subsets of L1(Rn), proving (5.8).</p><p>2) To complete the proof, note that (5.8) implies APPENDIX A</p><p>The following theorem is a version of the Varadhan-Laplace denotes the open ball centered at z of radius a.</p><p>Lemma A.1: Let A be a compact space, F:, Fa E C(Rm), and assume i)</p><p>ii) The function Fa is uniformly continuous on each set Bi" = { E E R": F:(s) &gt; F: -6).</p><p>Then the uniform coercivity hypothesis iii) ensures there exists R &gt; 0 such that B;lE c B(0, R).</p><p>By hypothesis ii) on B(0, R) and using the uniform convergence on B(0, R), given 6 &gt; 0 there exists r &gt; 0 such that</p><p>for all z, z' E B(0, R), a E A, and E &gt; 0 sufficiently small. &lt; r implies JF,"(z) -F a / &lt; 6 for all a E A, and E &gt; 0 sufficiently small. Hence for all a E A, and E &gt; 0 sufficiently small. Then for all E &gt; 0 sufficiently small and all a E A.</p><p>John S. <ref type="bibr">Baras</ref>  Dr. Elliott's work is in signal and image processing and also in applications of random processes in finance.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic observers as asymptotic limits of recursive filters: Special cases</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Baras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bensoussan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Applied Math</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1158" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Robust output feedback control for nonlinear discrete-time systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Baras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hw-Optimal Control and RelatedMinimax Design Problems: A Dynamic Game Approach</title>
		<author>
			<persName><forename type="first">T</forename><surname>Basar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bernhard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Birkhauser</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal control of partially observable stochastic systems with an exponential-of-integral performance index</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bensoussan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Van Schuppen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Contr. Optim</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="599" to="613" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Topics in digital and robust control of linear systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bernhardsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
		<respStmt>
			<orgName>Lund Institute of Technology, Lund, Sweden</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sufficiently informative functions and the minimax feedback control of uncertain dynamical systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">B</forename><surname>Rhodes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="117" to="123" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Discrete-time nonlinear risk-sensitive control</title>
		<author>
			<persName><forename type="first">M</forename><surname>Campi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Statespace solutions to standard Hz and H m control problems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Khargonekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Francis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="847" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">London: Longman Scientific and Technical</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IFAC World Congress</title>
		<title level="s">Pitman Research Notes in Mathematics Series</title>
		<meeting><address><addrLine>New York Springer-Verlag; Sydney</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1982">1982. 1987. 1993</date>
		</imprint>
	</monogr>
	<note>Discrete time partially observed control</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m">Solutions and Optimal Control</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Risk sensitive optimal control and differential games</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Mceneaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Con5 Adaptive stochastic Contr</title>
		<meeting>Con5 Adaptive stochastic Contr</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Kansas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Random Perturbations of Dynamical Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Freidlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wentzell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State space formulae for all stabilizing controllers that satisfy an H, norm bound and relations to risk sensitivity</title>
		<author>
			<persName><forename type="first">K</forename><surname>Glover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Doyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Syst. Contr. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="167" to="172" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Asmptotic Bayesian estimation of a first order equation with a small diffusion</title>
		<author>
			<persName><surname>Hijab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Prob</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="890" to="902" />
			<date type="published" when="1980">1980. 1984</date>
		</imprint>
		<respStmt>
			<orgName>Univ. of Calif., Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
	<note>Minimum energy estimation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal stochastic linear systems with exponential performance criteria and their relation to deterministic differential games</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="124" to="131" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asymptotic analysis of nonlinear stochastic risk-sensitive control and differential games</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Contr. Sign. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">954967</biblScope>
			<date type="published" when="1991">1992. 1991</date>
		</imprint>
	</monogr>
	<note>SIAM J. Contr. Optim.</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear filtering and large deviations: A PDE-control theoretic approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Baras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="412" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Output feedback risksensitive control and differential games for continuous-time nonlinear systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Baras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Elliott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>32nd IEEE CDC</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Varaiya</surname></persName>
		</author>
		<title level="m">Stochastic Systems: Estimation, Identijcation, and Adaptive Control</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A game theoretic approach to H -control for time varying systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J N</forename><surname>Limebeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Khargonekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Green</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Contr. Optim</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="262" to="283" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A game theoretic approach to a finite-time disturbance attenuation problem</title>
		<author>
			<persName><forename type="first">I</forename><surname>Rhee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Speyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Automat. Contr</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1021" to="1032" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Striebel</surname></persName>
		</author>
		<title level="m">Optimal Control of Discrete Time Stochastic Systems (Lecture Notes in Economics and Mathematical Systems)</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Risk-sensitive linear/quadratic/gaussian control</title>
		<author>
			<persName><forename type="first">P</forename><surname>Whittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A h . AppL Prob</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="764" to="777" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A risk-sensitive maximum principle</title>
	</analytic>
	<monogr>
		<title level="j">Syst. Contr. Lett</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">He received the B.Sc. degree in mathematics and the B.E. (Hon. I) in electrical engineering from the University of New South Wales</title>
	</analytic>
	<monogr>
		<title level="m">1988 and 1989 he was Visiting Assistant Professor with the Division of Applied Mathematics, Brown University, Providence, RI, and from 1989 to 1991 he was Assistant Professor with the Department of Mathematics</title>
		<meeting><address><addrLine>Sydney, Australia; College Park; Sydney, Australia; College Park; Lexington; Canberra, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1960">1991. 15. 1990. 1960. 1981. 1983. 1988. 1985. 1988</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="183" to="192" />
		</imprint>
		<respStmt>
			<orgName>Systems Research Center (now Institute for Systems Research), University of Maryland ; University of Kentucky ; Department of Systems Engineering, Australian National University</orgName>
		</respStmt>
	</monogr>
	<note>respectively. He received the Ph.D degree in applied mathematics from the University of Maryland. Since 1991 he has been Research Fellow with the. Dr. James&apos; research interests include nonlinear and stochastic systems, robotics, and advanced engineering computation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
