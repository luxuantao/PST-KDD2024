<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hyperbolic Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<email>qiliu@fb.com</email>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
							<email>dkiela@fb.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hyperbolic Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning from graph-structured data is an important task in machine learning and artificial intelligence, for which Graph Neural Networks (GNNs) have shown great promise. Motivated by recent advances in geometric representation learning, we propose a novel GNN architecture for learning representations on Riemannian manifolds with differentiable exponential and logarithmic maps. We develop a scalable algorithm for modeling the structural properties of graphs, comparing Euclidean and hyperbolic geometry. In our experiments, we show that hyperbolic GNNs can lead to substantial improvements on various benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We study the problem of supervised learning on entire graphs. Neural methods have been applied with great success to (semi) supervised node and edge classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51]</ref>. They have also shown promise for the classification of graphs based on their structural properties [e.g. <ref type="bibr">, 18]</ref>. By being invariant to node and edge permutations <ref type="bibr" target="#b2">[3]</ref>, GNNs can exploit symmetries in graph-structured data, which makes them well-suited for a wide range of problems, ranging from quantum chemistry <ref type="bibr" target="#b17">[18]</ref> to modelling social and interaction graphs <ref type="bibr" target="#b49">[50]</ref>.</p><p>In this work, we are concerned with the representational geometry of GNNs. Results in network science have shown that hyperbolic geometry in particular is well-suited for modeling complex networks. Typical properties such as heterogeneous degree distributions and strong clustering can often be explained by assuming an underlying hierarchy which is well captured in hyperbolic space <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35]</ref>. These insights led, for instance, to hyperbolic geometric graph models, which allow for the generation of random graphs with real-world properties by sampling nodes uniformly in hyperbolic space <ref type="bibr" target="#b0">[1]</ref>. Moreover, it has recently been shown that hyperbolic geometry lends itself particularly well for learning hierarchical representations of symbolic data and can lead to substantial gains in representational efficiency and generalization performance <ref type="bibr" target="#b31">[32]</ref>.</p><p>Motivated by these results, we examine if graph neural networks may be equipped with geometrically appropriate inductive biases for capturing structural properties, e.g., information about which nodes are highly connected (and hence more central) or the overall degree distribution in a graph. For this purpose, we extend graph neural networks to operate on Riemannian manifolds with differentiable exponential and logarithmic maps. This allows us to investigate non-Euclidean geometries within a general framework for supervised learning on graphs -independently of the underlying space and its curvature. Here, we compare standard graph convolutional networks <ref type="bibr" target="#b25">[26]</ref> that work in Euclidean space with different hyperbolic graph neural networks (HGNNs): one that operates on the Poincaré ball as in <ref type="bibr" target="#b31">[32]</ref> and one that operates on the Lorentz model of hyperbolic geometry as in <ref type="bibr" target="#b32">[33]</ref>. We focus specifically on the ability of hyperbolic graph neural networks to capture structural properties.</p><p>Our contributions are as follows: we generalize graph neural networks to be manifold-agnostic, and show that hyperbolic graph neural networks can provide substantial improvements for full-graph classification. Furthermore, we show that HGNNs are more efficient at capturing structural properties of synthetic data than their Euclidean counterpart; that they can more accurately predict the chemical properties of molecules; and that they can predict extraneous properties of large-scale networks, in this case price fluctuations of a blockchain transaction graph, by making use of the hierarchical structure present in the data. Code and data are available at https://github.com/facebookresearch/ hgnn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Graph neural networks (GNNs) have received increased attention in machine learning and artificial intelligence due to their attractive properties for learning from graph-structured data <ref type="bibr" target="#b6">[7]</ref>. Originally proposed by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b40">41]</ref> as a method for learning node representations on graphs using neural networks, this idea was extended to convolutional neural networks using spectral methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> and the iterative aggregation of neighbor representations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45]</ref>. <ref type="bibr" target="#b21">[22]</ref> showed that graph neural networks can be scaled to large-scale graphs. Due to their ability to learn inductive models of graphs, GNNs have found promising applications in molecular fingerprinting <ref type="bibr" target="#b13">[14]</ref> and quantum chemistry <ref type="bibr" target="#b17">[18]</ref>.</p><p>There has been an increased interest in hyperbolic embeddings due to their ability to model data with latent hierarchies. <ref type="bibr" target="#b31">[32]</ref> proposed Poincaré for learning hierarchical representations of symbolic data. Furthermore, <ref type="bibr" target="#b32">[33]</ref> showed that the Lorentz model of hyperbolic geometry has attractive properties for stochastic optimization and leads to substantially improved embeddings, especially in low dimensions. <ref type="bibr" target="#b15">[16]</ref> extended Poincaré embeddings to directed graphs using hyperbolic entailment cones. The representation trade-offs for hyperbolic embeddings were analyzed in <ref type="bibr" target="#b11">[12]</ref>, which also proposed a combinatorial algorithm to compute embeddings. Ganea et al. <ref type="bibr" target="#b16">[17]</ref> and Gulcehre et al. <ref type="bibr" target="#b20">[21]</ref> proposed hyperbolic neural networks and hyperbolic attention networks, respectively, with the aim of extending deep learning methods to hyperbolic space. Our formalism is related to the former in that layer transformations are performed in the tangent space. We propose a model that is applicable to any Riemannian manifold with differentiable log/exp maps, which also allows us to easily extend GNNs to the Lorentz model<ref type="foot" target="#foot_0">2</ref> . Our formalism is related to the latter in that we perform message passing in hyperbolic space, but instead of using the Einstein midpoint, we generalize to any Riemannian manifold via mapping to and from the tangent space.</p><p>Hyperbolic geometry has also shown great promise in network science: <ref type="bibr" target="#b27">[28]</ref> showed that typical properties of complex networks such as heterogeneous degree distributions and strong clustering can be explained by assuming an underlying hyperbolic geometry and used these insights to develop a geometric graph model for real-world networks <ref type="bibr" target="#b0">[1]</ref>. Furthermore, <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref> exploited the property of hyperbolic embeddings to embed tree-like graphs with low distortion, for greedy-path routing in large-scale communication networks.</p><p>Concurrently with this work, Chami et al. <ref type="bibr" target="#b9">[10]</ref> also proposed an extension of graph neural networks to hyperbolic geometry. The main difference lies in their attention-based architecture for neighborhood aggregation, which also elegantly supports having trainable curvature parameters at each layer. They show strong performance on link prediction and node classification tasks, and provide an insightful analysis in terms of a graph's δ-hyperbolicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hyperbolic Graph Neural Networks</head><p>Graph neural networks can be interpreted as performing message passing between nodes <ref type="bibr" target="#b17">[18]</ref>. We base our framework on graph convolutional networks as proposed in <ref type="bibr" target="#b25">[26]</ref>, where node representations are computed by aggregating messages from direct neighbors over multiple steps. That is, the message from node v to its receiving neighbor u is computed as</p><formula xml:id="formula_0">m k+1 v = W k Ãuv h k v .</formula><p>Here h k v is the representation of node v at step k, W k ∈ R h×h constitutes the trainable parameters for step k (i.e., the k-th layer), and Ã = D − 1 2 (A + I)D − 1 2 captures the connectivity of the graph. To get Ã, the identity matrix I is added to the adjacency matrix A to obtain self-loops for each node, and the resultant matrix is normalized using the diagonal degree matrix (D ii = j (A ij + I ij )). We then obtain a new representation of u at step k + 1 by summing up all the messages from its neighbors before applying the activation function σ:</p><formula xml:id="formula_1">h k+1 u = σ( v∈I(u) m t+1 v )</formula><p>, where I(u) is the set of in-neighbors of u, i.e. v ∈ I(u) if and if only v has an edge pointing to u. Thus, in a more compact notation, the information propagates on the graph as:</p><formula xml:id="formula_2">h k+1 u = σ   v∈I(u) Ãuv W k h k v   .</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Neural Networks on Riemannian Manifolds</head><p>A graph neural network comprises a series of basic operations, i.e. message passing via linear maps and pointwise non-linearities, on a set of nodes that live in a given space. While such operations are well-understood in Euclidean space, their counterparts in non-Euclidean space (which we are interested in here) are non-trivial. We generalize the notion of a graph convolutional network such that the network operates on Riemannian manifolds and becomes agnostic to the underlying space.</p><p>Since the tangent space of a point on Riemannian manifolds always is Euclidean (or a subset of Euclidean space), functions with trainable parameters are executed there. The propagation rule for each node u ∈ V is calculated as:</p><formula xml:id="formula_3">h k+1 u = σ   exp x ( v∈I(u) Ãuv W k log x (h k v ))   .<label>(2)</label></formula><p>At layer k, we map each node representation h k v ∈ M, where v ∈ I(u) is a neighbor of u, to the tangent space of a chosen point x ∈ M using the logarithmic map log x . Here Ã and W k are the normalized adjacency matrix and the trainable parameters, respectively, as in Equation <ref type="formula">1</ref>. An exponential map exp x is applied afterwards to map the linearly transformed tangent vector back to the manifold.</p><p>The activation σ is applied after the exponential map to prevent model collapse: if the activation was applied before the exponential map, i.e.</p><formula xml:id="formula_4">h k+1 u = exp x σ( v∈I(u) Ãuv W k log x (h k v ))</formula><p>, the exponential map exp x at step k would have been cancelled by the logarithmic map log x at step k + 1 as log x (exp x (h)) = h. Hence, any such model would collapse to a vanilla Euclidean GCN with a logarithmic map taking the input features of the GCN and an exponential map taking its outputs. An alternative to prevent such collapse would be to introduce bias terms as in <ref type="bibr" target="#b16">[17]</ref>. Importantly, when applying the non-linearity directly on a manifold M, we need to ensure that its application is manifold preserving, i.e., that σ : M → M. We will propose possible choices for non-linearities in the discussion of the respective manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Riemannian Manifolds</head><p>A Riemannian manifold (M, g) is a real and smooth manifold equipped with an inner product</p><formula xml:id="formula_5">g x : T x M × T x M → R at each point x ∈ M,</formula><p>which is called a Riemannian metric and allows us to define the geometric properties of a space such as angles and the length of a curve.</p><p>We experiment with Euclidean space and compare it to two different hyperbolic manifolds (note that there exist multiple equivalent hyperbolic models, such as the Poincaré ball and the Lorentz model, for which transformations exist that preserve all geometric properties including isometry).</p><p>Euclidean Space The Euclidean manifold is a manifold with zero curvature. The metric tensor is defined as g E = diag([1, 1, . . . , 1]). The closed-form distance, i.e. the length of the geodesic, which is a straight line in Euclidean space, between two points is given as:</p><formula xml:id="formula_6">d(x, y) = i (x i − y i ) 2<label>(3)</label></formula><p>The exponential map of the Euclidean manifold is defined as:</p><formula xml:id="formula_7">exp x (v) = x + v<label>(4)</label></formula><p>The logarithmic map is given as:</p><formula xml:id="formula_8">log x (y) = y − x (5)</formula><p>In order to make sure that the Euclidean manifold formulation is equivalent to the vanilla GCN model described in Equation 1, as well as for reasons of computational efficiency, we choose x = x 0 (i.e., the origin) as the fixed point on the manifold in whose tangent space we operate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Poincaré Ball Model</head><p>The Poincaré ball model with constant negative curvature corresponds to the Riemannian manifold (B, g B</p><p>x ), where</p><formula xml:id="formula_9">B = {x ∈ R n : x &lt; 1} is an open unit ball. Its metric tensor is g B x = λ 2 x g E , where λ x = 2 1− x 2</formula><p>is the conformal factor and g E is the Euclidean metric tensor (see above). The distance between two points x, y ∈ B is given as:</p><formula xml:id="formula_10">d B (x, y) = arcosh 1 + 2 x − y 2 (1 − x 2 )(1 − y 2 ) .<label>(6)</label></formula><p>For any point x ∈ B, the exponential map exp x : T x B → B and the logarithmic map log x : B → T x B are defined for the tangent vector v = 0 and the point y = 0, respectively, as:</p><formula xml:id="formula_11">exp x (v) = x ⊕ tanh λ x v 2 v v log x (y) = 2 λ x arctanh( − x ⊕ y ) −x ⊕ y − x ⊕ y , (<label>7</label></formula><formula xml:id="formula_12">)</formula><p>where ⊕ is the Möbius addition for any x, y ∈ B:</p><formula xml:id="formula_13">x ⊕ y = (1 + 2 x, y + y 2 )x + (1 − x 2 )y 1 + 2 x, y + x 2 y 2<label>(8)</label></formula><p>Similar to the Euclidean case, and following <ref type="bibr" target="#b16">[17]</ref>, we use x = x 0 . On the Poincaré ball, we employ pointwise non-linearities which are norm decreasing, i.e., where |σ(x)| ≤ |x| (which is true for e.g. ReLU and leaky ReLU). This ensures that σ :</p><formula xml:id="formula_14">B → B since σ(x) ≤ x .</formula><p>Lorentz Model The Lorentz model avoids numerical instabilities that may arise with the Poincaré distance (mostly due to the division) <ref type="bibr" target="#b32">[33]</ref>. Its stability is particularly useful for our architecture, since we have to apply multiple sequential exponential and logarithmic maps in deep GNNs, which would normally compound numerical issues, but which the Lorentz model avoids. Let x, y ∈ R n+1 , then the Lorentzian scalar product is defined as:</p><p>x, y L = −x 0 y 0 + n i=1</p><p>x n y n</p><p>The Lorentz model of n-dimensional hyperbolic space is then defined as the Riemannian manifold (L, g L x ), where L = {x ∈ R n+1 : x, x L = −1, x 0 &gt; 0} and where g L = diag([−1, 1, . . . , 1]). The induced distance function is given as:</p><formula xml:id="formula_16">d L (x, y) = arcosh(− x, y L )<label>(10)</label></formula><p>The exponential map exp x : T x L → L and the logarithmic map log x : L → T x L are defined as:</p><formula xml:id="formula_17">exp x (v) = cosh( v L )x + sinh( v L ) v v L log x (y) = arcosh(− x, y L ) x, y 2 L − 1 (y + x, y L x),<label>(11)</label></formula><p>where</p><formula xml:id="formula_18">v L = v, v L .</formula><p>The origin, i.e., the zero vector in Euclidean space and the Poincaré ball, is equivalent to (1, 0, ..., 0) in the Lorentz model, which we use as x . Since activation functions such as ReLU and leaky ReLU are not manifold-preserving in the Lorentz model, we first use Equation <ref type="formula" target="#formula_19">12</ref>to map the point from Lorentz to Poincaré and apply the activation σ, before mapping it back using Equation <ref type="formula" target="#formula_20">13</ref>:</p><formula xml:id="formula_19">p L→B (x 0 , x 1 , ..., x n ) = (x 1 , ..., x n ) x 0 + 1<label>(12)</label></formula><p>p B→L (x 0 , x 1 , ..., </p><formula xml:id="formula_20">x n ) = (1 + x 2 , 2x 1 , ..., 2x n ) 1 − x 2<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Centroid-Based Regression and Classification</head><p>The output of a hyperbolic graph neural network with K steps consists of a set of node representations {h K 1 , ..., h K |V | }, where each h K i ∈ M. Standard parametric classification and regression methods in Euclidean space are not generally applicable in the hyperbolic case. Hence, we propose an extension of the underlying idea of radial basis function networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b35">36]</ref> to Riemannian manifolds. The key idea is to use a differentiable function ψ : M → R d that can be used to summarize the structure of the node embeddings. More specifically, we first introduce a list of centroids</p><formula xml:id="formula_21">C = [c 1 , c 2 , ..., c |C| ],</formula><p>where each c i ∈ M. The centroids are learned jointly with the GNN using backpropagation. The pairwise distance between c i and h K j is calculated as:</p><formula xml:id="formula_22">ψ ij = d(c i , h K j )</formula><p>. Next, we concatenate all distances (ψ 1j , ..., ψ |C|j ) ∈ R |C| to summarize the position of h K j relative to the centroids. For node-level regression,</p><formula xml:id="formula_23">ŷ = w T o (ψ 1j , ..., ψ |C|j ),<label>(14)</label></formula><p>where w o ∈ R |C| , and for node-level classification,</p><formula xml:id="formula_24">p(y j ) = softmax W o (ψ 1j , ..., ψ |C|j ) ,<label>(15)</label></formula><p>where W o ∈ R c×|C| and c denotes the number of classes.</p><p>For graph-level predictions, we first use average pooling to combine the distances of different nodes, obtaining (ψ 1 , ..., ψ |C| ), where ψ i =</p><p>|V | j=1 ψ ij /|V |, before feeding (ψ 1 , ..., ψ |C| ) into fully connected networks. Standard cross entropy and mean square error are used as loss functions for classification and regression, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Other details</head><p>The input features of neural networks are typically embeddings or features that live in Euclidean space. For Euclidean features x E , we first apply exp x (x E ) to map it into the Riemannian manifolds. To initialize embeddings E within the Riemannian manifold, we first uniformly sample from a range (e.g. [−0.01, 0.01]) to obtain Euclidean embeddings, before normalizing the embeddings to ensure that each embedding e i ∈ M. The Euclidean manifold is normalized into a unit ball to make sure we compare fairly with the Poincaré ball and the Lorentz model. This normalization causes minor differences with respect to the vanilla GCN model of Kipf &amp; Welling <ref type="bibr" target="#b25">[26]</ref> but as we show in the appendix, in practice this does not cause any significant dissimilarities. We use leaky ReLU as the activation function σ with the negative slope 0.5. We use RAMSGrad <ref type="bibr" target="#b3">[4]</ref> and AMSGrad for hyperbolic parameters and Euclidean parameters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following experiments, we will compare the performance of models using different spaces within the Riemannian manifold, comparing the canonical Euclidean version to Hyperbolic Graph Neural Networks using either Poincaré or Lorentz manifolds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Structures</head><p>First, we attempt to corroborate the hypothesis that hyperbolic graph neural networks are better at capturing structural information of graphs than their Euclidean counterpart. To that end, we design a synthetic experiment, such that we have full control over the amount of structural information that is required for the classification decision. Specifically, our task is to classify synthetically generated graphs according to the underlying generation algorithm. We choose 3 distinct graph generation algorithms: Erdős-Rényi <ref type="bibr" target="#b14">[15]</ref>, Barabási-Albert <ref type="bibr" target="#b1">[2]</ref> and Watts-Strogatz <ref type="bibr" target="#b45">[46]</ref> (see Figure <ref type="figure">1</ref>).</p><p>The graphs are constructed as follows. For each graph generation algorithm we uniformly sample a number of nodes between 100 and 500 and subsequently employ the graph generation algorithm on the nodes. For Barabási-Albert graphs, we set the number of edges to attach from a new node to existing nodes to a random number between 1 and 100. For Erdős-Rényi, the probability for edge creation is set to 0.1 − 1. For Watts-Strogatz, each node is connected to 1 − 100 nearest neighbors in the ring topology, and the probability of rewiring each edge is set to 0.1 − 1.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows the results of classifying the graph generation algorithm (as measured by F1 score over the three classes). For our comparison, we follow <ref type="bibr" target="#b31">[32]</ref> and show results for different numbers of dimensions. We observe that our hyperbolic methods outperform the Euclidean alternative by a large margin. Owing to the representational efficiency of hyperbolic methods, the difference is particularly big for low-dimensional cases. The Lorentz model does better than the Poincaré one in all but one case. The differences become smaller with higher dimensionality, as we should expect, but hyperbolic methods still do better in the relatively high dimensionality of 256. We speculate that this is due to their having better inductive biases for capturing the structural properties of the graphs, which is extremely important for solving this particular task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Molecular Structures</head><p>Graphs are ubiquitous as data structures, but one domain where neural networks for graph data have been particularly impactful is in modeling chemical problems. Applications include molecular design <ref type="bibr" target="#b30">[31]</ref>, fingerprinting <ref type="bibr" target="#b13">[14]</ref> and poly pharmaceutical side-effect modeling <ref type="bibr" target="#b51">[52]</ref>.</p><p>Molecular property prediction has received attention as a reasonable benchmark for supervised learning on molecules <ref type="bibr" target="#b17">[18]</ref>. A popular choice for this purpose is the QM9 dataset <ref type="bibr" target="#b36">[37]</ref>. Unfortunately, it is hard to compare to previous work on this dataset, as the original splits from <ref type="bibr" target="#b17">[18]</ref> are no longer available (per personal correspondence). One characteristic with QM9 is that the molecules are relatively small (around 10 nodes per graph) and that there is high variance in the results. Hence, we instead use the much larger ZINC dataset <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23]</ref>, which has been used widely in graph generation for molecules using machine learning methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b30">31]</ref>. However, see the appendix for results on QM8 <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39]</ref> and QM9 <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>ZINC is a large dataset of commercially available drug-like chemical compounds. For ZINC, the input consists of embedding representations of atoms together with an adjacency matrix, without any additional handcrafted features. A master node <ref type="bibr" target="#b17">[18]</ref> is added to the adjacency matrix to speed up message passing. The dataset consists of 250k examples in total, out of which we randomly sample 25k for the validation and test sets, respectively. On average, these molecules are bigger (23 heavy atoms on average) and structurally more complex than the molecules in QM9. ZINC is multi-relational, i.e. there are four types of relations for molecules, i.e. single bond, double bond, triple bond and aromatic bond.  First, in order to enable our graph neural networks to handle multi-relational data, we follow <ref type="bibr" target="#b41">[42]</ref> and extend Equation 2 to incorporate a multirelational weight matrix W k r where r ∈ R is the set of relations, which we sum over. As before, we compare the various methods for different dimensionalities. The results can be found in Table <ref type="table" target="#tab_1">2</ref>.</p><p>We also compare to three strong baselines on exactly the same data splits of the ZINC dataset: graphgated neural networks [GGNN; 29], deep tensor neural networks [DTNN; 43] and message-passing neural networks <ref type="bibr">[MPNN;</ref><ref type="bibr" target="#b17">18]</ref>. GGNN adds a GRU-like update <ref type="bibr" target="#b10">[11]</ref> that incorporates information from neighbors and previous timesteps in order to update node representations. DTNN takes as the input a fully connected weighted graph and aggregates node representations using a deep tensor neural network. For DTNN and MPNN we use the implementations in DeepChem<ref type="foot" target="#foot_1">3</ref> , a well-known open-source toolchain for deep-learning in drug discovery, materials science, quantum chemistry, and biology. For GGNN, we use the publicly available open-source implementation <ref type="foot" target="#foot_2">4</ref> . A comparison of our proposed approach to these methods can be found in Table <ref type="table" target="#tab_2">3</ref>.</p><p>We find that the Lorentz model outperforms the Poincaré ball on all properties, which illustrates the benefits of its improved numerical stability. The Euclidean manifold performs worse than the hyperbolic versions, confirming the effectiveness of hyperbolic models for modeling complex structures in graph data. Furthermore, as can be seen in the appendix, the computational overhead of using non-Euclidean manifolds is relatively minor. GGNN obtains comparable results to the Euclidean GNN. DTNN performs worse than the other models, as it relies on distance matrices ignoring multi-relational information during message passing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Blockchain Transaction Graphs</head><p>In terms of publicly accessible graph-structured data, blockchain networks like Ethereum constitute some of the largest sources of graph data in the world. Interestingly, financial transaction networks such as the blockchain have a strongly hierarchical nature: the blockchain ecosystem has even invented its own terminology for this, e.g., the market has long been speculated to be manipulated by "whales". A whale is a user (address) with enough financial resources to move the market in their favored direction. The structure of the blockchain graph and its dynamics over time have been used as a way of quantifying the "true value" of a network <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b46">47]</ref>. Blockchain networks have uncharacteristic dynamics <ref type="bibr" target="#b29">[30]</ref>, but the distribution of wealth on the blockchain follows a power-law distribution that is arguably (even) more skewed than in traditional markets <ref type="bibr" target="#b4">[5]</ref>. This means that the behavior of important "whale" nodes in the graph might be more predictive of fluctations (up or down) in the market price of the underlying asset, which should be easier to capture using hyperbolic graph neural networks.</p><p>Here, we study the problem of predicting price fluctuations for the underlying asset of the Ethereum blockchain <ref type="bibr" target="#b47">[48]</ref>, based on the large-scale behavior of nodes in transaction graphs (see the appendix for more details). Each node (i.e., address in the transaction graph) is associated with the same embedding over all timepoints. Models are provided with node embeddings and the transaction graph for a given time frame, together with the Ether/USDT market rate for the given time period. The transaction graph is a directed multi-graph where edge weights correspond to the transaction amount.</p><p>To encourage message passing on the graphs, we enhance the transaction graphs with inverse edges u → v for each edge v → u. As a result, Equation 1 is extended to the bidirectional case:</p><formula xml:id="formula_25">h k+1 u = σ   exp u ( v∈I(u) Ãuv W k log x (h k v ) + v∈O(u) Ãuv Wk log x (h k v ))   ,<label>(16)</label></formula><p>where O(u) is the set of out-neighbors of u, i.e. v ∈ O(u) if and if only u has an edge pointing to v. We use the mean candlestick price over a period of 8 hours in total as additional inputs to the network.</p><p>Table <ref type="table">4</ref> shows the results. We compare against a baseline of inputting averaged 128-dimensional node2vec <ref type="bibr" target="#b19">[20]</ref> features for the same time frame to an MLP classifier. We found that it helped if we only used the node2vec features for the top k nodes ordered by degree, for which we report results here (and which seems to confirm our suspicion that the transaction graph is strongly hierarchical). In addition, we compare against using the autoregressive integrated moving average (ARIMA), which is a common baseline for time series predictions. As before, we find that Lorentz performs significantly better than Poincaré, which in turn outperforms the Euclidean manifold.</p><p>One of the benefits of using hyperbolic representations is that we can inspect the hierarchy that the network has learned. We use this property to sanity check our proposed architecture: if it is indeed the case that hyperbolic networks model the latent hierarchy, nodes for what would objectively be considered influential "whale" nodes would have to be closer to the origin. Table <ref type="table" target="#tab_3">5</ref> shows the average norm of whale nodes compared to the average. For our list of whale nodes, we obtain the top 10000 addresses according to Etherscan<ref type="foot" target="#foot_3">5</ref> , compared to the total average of over 2 million addresses. Top whale nodes include exchanges, initial coin offerings (ICO) and original developers of Ethereum. We observe a lower norm for whale addresses, reflecting their importance in the hierarchy and influence on price fluctuations, which the hyperbolic graph neural networks is able to pick up on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We described a method for generalizing graph neural networks to Riemannian manifolds, making them agnostic to the underlying space. Within this framework, we harnessed the power of hyperbolic geometry for full graph classification. Hyperbolic representations are well-suited for capturing highlevel structural information, even in low dimensions. We first showed that hyperbolic methods are much better at classifying graphs according to their structure by using synthetic data, where the task was to distinguish randomly generated graphs based on the underlying graph generation algorithm.</p><p>We then applied our method to molecular property prediction on the ZINC dataset, and showed that hyperbolic methods again outperformed their Euclidean counterpart, as well as state-of-the-art models developed by the wider community. Lastly, we showed that a large-scale hierarchical graph, such as the transaction graph of a blockchain network, can successfully be modeled for extraneous prediction of price fluctuations. We showed that the proposed architecture successfully made use of its geometrical properties in order to capture the hierarchical nature of the data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1</figDesc><graphic url="image-2.png" coords="6,241.91,72.00,130.69,115.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>logP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>± 0.12 90.0 ± 0.21 90.6 ± 0.17 94.8 ± 0.25 95.3 ± 0.17 Poincare 93.0 ± 0.05 95.6 ± 0.14 95.9 ± 0.14 96.2 ± 0.06 93.7 ± 0.05 Lorentz 94.1 ± 0.03 95.1 ± 0.25 96.4 ± 0.23 96.6 ± 0.22 95.3 ± 0.28 F1 (macro) score and standard deviation of classifying synthetically generated graphs according to the underlying graph generation algorithm (high is good).</figDesc><table><row><cell></cell><cell></cell><cell>Dimensionality</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>256</cell></row><row><cell>Euclidean 77.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean absolute error of predicting molecular properties: the water-octanal partition coefficient (logP); qualitative estimate of drug-likeness (QED); and synthetic accessibility score (SAS). Scaled by 100 for table formatting (low is good).</figDesc><table><row><cell></cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>256</cell></row><row><cell cols="2">Euclidean 6.7 ± 0.07</cell><cell>4.7 ± 0.03</cell><cell>4.7 ± 0.02</cell><cell cols="2">3.6 ± 0.00 3.3 ± 0.00</cell></row><row><cell>Poincare</cell><cell>5.7 ± 0.00</cell><cell>4.6 ± 0.03</cell><cell>3.6 ± 0.02</cell><cell cols="2">3.2 ± 0.01 3.1 ± 0.01</cell></row><row><cell>Lorentz</cell><cell>5.5 ± 0.02</cell><cell>4.5 ± 0.03</cell><cell>3.3 ± 0.03</cell><cell cols="2">2.9 ± 0.01 2.4 ± 0.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell>QED</cell><cell></cell></row><row><cell></cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>256</cell></row><row><cell cols="6">Euclidean 22.4 ± 0.21 15.9 ± 0.14 14.5 ± 0.09 10.2 ± 0.08 6.4 ± 0.06</cell></row><row><cell>Poincare</cell><cell cols="5">22.1 ± 0.01 14.9 ± 0.13 10.2 ± 0.02 6.9 ± 0.02 6.0 ± 0.04</cell></row><row><cell>Lorentz</cell><cell cols="3">21.9 ± 0.12 14.3 ± 0.12 8.7 ± 0.04</cell><cell cols="2">6.7 ± 0.06 4.7 ± 0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SAS</cell><cell></cell></row><row><cell></cell><cell>3</cell><cell>5</cell><cell>10</cell><cell>20</cell><cell>256</cell></row><row><cell cols="6">Euclidean 20.5 ± 0.04 16.8 ± 0.07 14.5 ± 0.11 9.6 ± 0.05 9.2 ± 0.08</cell></row><row><cell>Poincare</cell><cell cols="5">18.8 ± 0.03 16.1 ± 0.08 12.9 ± 0.04 9.3 ± 0.07 8.6 ± 0.02</cell></row><row><cell>Lorentz</cell><cell cols="5">18.0 ± 0.15 16.0 ± 0.15 12.5 ± 0.07 9.1 ± 0.08 7.7 ± 0.06</cell></row><row><cell></cell><cell></cell><cell>logP</cell><cell>QED</cell><cell>SAS</cell></row><row><cell></cell><cell cols="4">DTNN [43] 4.0 ± 0.03 8.1 ± 0.04 9.9 ± 0.06</cell></row><row><cell></cell><cell cols="4">MPNN [18] 4.1 ± 0.02 8.4 ± 0.05 9.2 ± 0.07</cell></row><row><cell></cell><cell cols="4">GGNN [29] 3.2 ± 0.20 6.4 ± 0.20 9.1 ± 0.10</cell></row><row><cell></cell><cell>Euclidean</cell><cell cols="3">3.3 ± 0.00 6.4 ± 0.06 9.2 ± 0.08</cell></row><row><cell></cell><cell>Poincare</cell><cell cols="3">3.1 ± 0.01 6.0 ± 0.04 8.6 ± 0.02</cell></row><row><cell></cell><cell>Lorentz</cell><cell cols="3">2.4 ± 0.02 4.7 ± 0.00 7.7 ± 0.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean absolute error of predicting molecular properties logP, QED and SAS, as compared to current state-of-the-art deep learning methods. Scaled by 100 for table formatting (low is good).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Average norm of influential "whale" nodes. Whales are significantly closer to the origin than average, indicating their importance.</figDesc><table><row><cell></cell><cell>Dev</cell><cell>Test</cell><cell></cell></row><row><cell cols="3">Node2vec 54.10 ± 1.63 52.44 ± 1.10 ARIMA 54.50 ± 0.16 53.07 ± 0.06</cell><cell></cell><cell>"Whale" nodes All nodes</cell></row><row><cell cols="3">Euclidean 56.15 ± 0.30 53.95 ± 0.20</cell><cell>Norm</cell><cell>0.20129</cell><cell>0.33178</cell></row><row><cell>Poincare</cell><cell cols="2">57.03 ± 0.28 54.41 ± 0.24</cell><cell></cell></row><row><cell>Lorentz</cell><cell cols="2">57.52 ± 0.35 55.51 ± 0.37</cell><cell></cell></row><row><cell cols="3">Table 4: Accuracy of predicting price fluctations</cell><cell></cell></row><row><cell cols="3">(up-down) for the Ether/USDT market rate based</cell><cell></cell></row><row><cell cols="2">on graph dynamics.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Other Riemannian manifolds such as spherical space are beyond the scope of this work but might be interesting to study in future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://deepchem.io/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">https://github.com/microsoft/gated-graph-neural-network-samples</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3">https://etherscan.io</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hyperbolic graph generator</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Aldecoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Orsini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Physics Communications</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="492" to="496" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Emergence of scaling in random networks</title>
		<author>
			<persName><forename type="first">Albert-László</forename><surname>Barabási</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réka</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">286</biblScope>
			<biblScope unit="issue">5439</biblScope>
			<biblScope unit="page" from="509" to="512" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Peter W Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Riemannian adaptive optimization methods</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling properties of extreme price fluctuations in bitcoin markets</title>
		<author>
			<persName><forename type="first">Stjepan</forename><surname>Begušić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zvonko</forename><surname>Kostanjčar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Podobnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica A: Statistical Mechanics and its Applications</title>
		<imprint>
			<biblScope unit="volume">510</biblScope>
			<biblScope unit="page" from="400" to="406" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sustaining the internet with hyperbolic mapping</title>
		<author>
			<persName><surname>Boguñá</surname></persName>
		</author>
		<author>
			<persName><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Krioukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometric deep learning: Going beyond euclidean data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-variable functional interpolation and adaptive networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Broomhead</surname></persName>
		</author>
		<author>
			<persName><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="321" to="355" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>ICLR 2014</idno>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">April 14-16, 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-third Conference on Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoderdecoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-10-25">2014. October 25-29, 2014. 2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03329</idno>
		<title level="m">Representation tradeoffs for hyperbolic embeddings</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alán</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015. 2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alfréd</forename><surname>Rényi</surname></persName>
		</author>
		<title level="m">On random graphs, i. Publicationes Mathematicae (Debrecen)</title>
				<imprint>
			<date type="published" when="1959">1959</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hyperbolic entailment cones for learning hierarchical embeddings</title>
		<author>
			<persName><forename type="first">O.-E</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Becigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning (ICML)</title>
				<meeting>the 35th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="1646" to="1655" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hyperbolic neural networks</title>
		<author>
			<persName><forename type="first">Octavian-Eugen</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bécigneul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08">2018. 2018, 3-8 December 2018. 2018</date>
			<biblScope unit="page" from="5350" to="5360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 2005. IJCNN&apos;05. Proceedings. 2005 IEEE International Joint Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hyperbolic attention networks</title>
		<author>
			<persName><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>ICLR 2019</idno>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
				<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 6-9, 2019, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Zinc-a free database of commercially available compounds for virtual screening</title>
		<author>
			<persName><forename type="first">J</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">K</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><surname>Shoichet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="177" to="182" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Zinc: a free tool to discover chemistry for biology</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>John J Irwin</surname></persName>
		</author>
		<author>
			<persName><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erin</forename><forename type="middle">S</forename><surname>Michael M Mysinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">G</forename><surname>Bolstad</surname></persName>
		</author>
		<author>
			<persName><surname>Coleman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1757" to="1768" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Junction tree variational autoencoder for molecular graph generation</title>
		<author>
			<persName><forename type="first">Wengong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="2328" to="2337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR 2017</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Geographic routing using hyperbolic space</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INFOCOM 2007. 26th IEEE International Conference on Computer Communications. IEEE</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1902" to="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hyperbolic geometry of complex networks</title>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36106</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR 2016</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">May 2-4, 2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Evolutionary dynamics of cryptocurrency transaction networks: An empirical study</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">e0202202</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Constrained graph variational autoencoders for molecule design</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gaunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7795" to="7804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Poincaré embeddings for learning hierarchical representations</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6338" to="6347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning continuous hierarchies in the lorentz model of hyperbolic geometry</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ICML 2018</title>
				<meeting>the 35th International Conference on Machine Learning, ICML 2018<address><addrLine>Stockholmsmässan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">July 10-15, 2018. 2018</date>
			<biblScope unit="page" from="3776" to="3785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
				<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">June 19-24, 2016. 2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Popularity versus similarity in growing networks</title>
		<author>
			<persName><forename type="first">Fragkiskos</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Kitsak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ángeles</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marián</forename><surname>Boguná</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">489</biblScope>
			<biblScope unit="issue">7417</biblScope>
			<biblScope unit="page">537</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Networks for approximation and learning</title>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Girosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="1481" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Pavlo O Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Quantum chemistry structures and properties of 134 kilo molecules</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Pavlo O Dral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Rupp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">140022</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Electronic spectra from tddft and machine learning in chemical space</title>
		<author>
			<persName><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Anatole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84111</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruud</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Louis</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung</forename><surname>Ah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference</title>
				<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-03">2018. June 3-7, 2018. 2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Kristof T Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zinc 15-ligand discovery for everyone</title>
		<author>
			<persName><forename type="first">Teague</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Irwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2324" to="2337" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Collective dynamics of &apos;small-world&apos;networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">H</forename><surname>Watts</surname></persName>
		</author>
		<author>
			<persName><surname>Strogatz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">393</biblScope>
			<biblScope unit="issue">6684</biblScope>
			<biblScope unit="page">440</biblScope>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Are bitcoin bubbles predictable. Combining a Generalised Metcalfe&apos;s Law and the LPPLS Model</title>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Wheatley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Sornette</surname></persName>
		</author>
		<author>
			<persName><surname>Reppen</surname></persName>
		</author>
		<author>
			<persName><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><surname>Gantner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swiss Finance Institute Research Paper</title>
		<imprint>
			<biblScope unit="issue">18-22</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Ethereum: A secure decentralised generalised transaction ledger</title>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="1" to="32" />
		</imprint>
	</monogr>
	<note>Ethereum project yellow paper</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Classification of crypto-coins and tokens from the dynamics of their power law capitalisation distributions</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Wheatley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Sornette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society open science</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
				<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
