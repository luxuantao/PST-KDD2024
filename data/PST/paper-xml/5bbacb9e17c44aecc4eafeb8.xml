<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-dimensionality small-instance set feature selection: A hybrid bio-inspired heuristic approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-11-02">November 2, 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hossam</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
							<email>hossam.zawbaa@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Babes-Bolyai University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Beni-Suef University</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">E</forename><surname>Emary</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Computers and Information</orgName>
								<orgName type="institution">Cairo University</orgName>
								<address>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Computer Studies</orgName>
								<orgName type="institution">Arab Open University</orgName>
								<address>
									<settlement>Cairo</settlement>
									<country key="EG">Egypt</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Crina</forename><surname>Grosan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Mathematics and Computer Science</orgName>
								<orgName type="institution">Babes-Bolyai University</orgName>
								<address>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">College of Engineering, Design and Physical Sciences</orgName>
								<orgName type="institution">Brunel University</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vaclav</forename><surname>Snasel</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Faculty of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">VSB-Technical University of Ostrava</orgName>
								<address>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-dimensionality small-instance set feature selection: A hybrid bio-inspired heuristic approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-11-02">November 2, 2017</date>
						</imprint>
					</monogr>
					<idno type="MD5">CD3BD1144D3E493777B03B194D97967E</idno>
					<idno type="DOI">10.1016/j.swevo.2018.02.021</idno>
					<note type="submission">Received Date: 26 September 2016 Revised Date: 2 November 2017 Accepted Date: 24 February 2018 Preprint submitted to Elsevier</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Swarm and Evolutionary Computation BASE DATA Bio-inspired optimization</term>
					<term>Antlion optimization</term>
					<term>Grey wolf optimization</term>
					<term>Hybrid ALO-GWO</term>
					<term>Swarm optimization</term>
					<term>Feature selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Selection of a representative set of features is still a crucial and challenging problem in machine learning. The complexity of the problem increases when any of the following situations occur: a very large number of attributes (large dimensionality); a very small number of instances or time points (small-instance set). The first situation poses problems for machine learning algorithm as the search space for selecting a combination of relevant features becomes impossible to explore in a reasonable time and with reasonable computational resources. The second aspect poses the problem of having insufficient data to learn from (insufficient examples). In this work, we approach both these issues at the same time. The methods we proposed are heuristics inspired from nature (in particular, from biology). We propose a hybrid of two methods which has the advantage of providing a good learning from fewer examples and a fair selection of features from a really large set, all these while ensuring a high standard classification accuracy of the data. The methods used are antlion optimization (ALO), grey wolf optimization (GWO), and a combination of the two (ALO-GWO). We test their performance on datasets having almost 50,000 features and less than 200 instances. The results look promising while compared with other methods such as genetic algorithms (GA) and particle swarm optimization (PSO).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the current challenges in feature selection is to deal with problems that involve a large number of features, sometimes in the order of millions. It is evident that a significant amount of these features are redundant and can be eliminated from the final classification process. It is also well known that fewer attributes increase the classification accuracy. From an experimental point of view, these attributes or features represent measurements of a specified entity. It is, of course, desirable to know which of these measurements have an effect on the final analysis. Reproducing the whole set of measurements can often be costly and time-consuming <ref type="bibr" target="#b0">[1]</ref>. This aspect is usually referred to as high-dimensionality of the data. On the other hand, some experiments are very difficult to reproduce entirely, both due to cost and time (it may sometimes take years to get a data instance or a data time point, for example, biological experiments where a data instance could be an experiment with an animal over the duration of a number of years) <ref type="bibr" target="#b1">[2]</ref>. This determines some datasets to have fewer instances. We refer to this aspect as small-instance set.</p><p>In the field of machine learning, feature selection is one of the most studied problems. There exist a large amount of work dealing either with highdimensional feature selection problems or with small-instance set problems. It is far more complex and challenging to deal with both these aspects at the same time and this for two reasons: the search space becomes too large to allow exploration of each state and the small number of instances do not provide sufficient examples to learn from <ref type="bibr" target="#b2">[3]</ref>.</p><p>Evolutionary computing (EC) and population-based algorithms adaptively search the feature space by using a set of search agents that interact in a social manner to reach the optimal solution <ref type="bibr" target="#b3">[4]</ref>  <ref type="bibr" target="#b4">[5]</ref>. EC methods are inspired by the animal social and biological behavior in nature like wolves, antlions, dragonflies, spiders, and so on, in a group <ref type="bibr" target="#b5">[6]</ref>.</p><p>Cuckoo search (CS) is a heuristic search technique applied for numerous optimization problems <ref type="bibr" target="#b6">[7]</ref>. Bat algorithm (BA) is a meta-heuristic technique that uses the echolocation behavior <ref type="bibr" target="#b7">[8]</ref>. The water cycle algorithm (WCA) is a novel metaheuristic optimization technique for constrained optimization in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>engineering design problems <ref type="bibr" target="#b8">[9]</ref>. The ordinary differential equations (ODEs) is a novel algorithm applied for solving several engineering problems. The aim of this technique is to minimize the weighted residual (error) function of the ODEs <ref type="bibr" target="#b9">[10]</ref>.</p><p>Several variants use a binary version of bio-inspired algorithms <ref type="bibr" target="#b10">[11]</ref> [34] <ref type="bibr" target="#b12">[13]</ref> and adapt and use them in different applications <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b15">[16]</ref> [17] <ref type="bibr" target="#b17">[18]</ref> [19] <ref type="bibr" target="#b19">[20]</ref>.</p><p>In general, approximation methods are well suited for feature selection problems and have been applied in the past with certain success. Among those, bio-inspired heuristics show potential in proving a good classification accuracy. These methods transform the feature selection problem into a multiobjective optimization problem with two criteria: maximize the classification accuracy and minimize the number of selected features <ref type="bibr" target="#b20">[21]</ref>.</p><p>In this work, we investigate the applicability of two methods -antlion optimization (ALO) and grey wolf optimization (GWO)-for feature selection and, based on the advantages offered by each method, we propose a combination of them in a new algorithm called ALO-GWO. In the hybrid version, we exploit GWO's global search ability to find the optimal solutions (good exploration of the search space) and ALO's local search performance to get the real optimum (good exploitation of the already found solutions). It is important to ensure a good exploration / exploitation balance in the optimization process. The hybrid ALO-GWO avoids premature convergence and provides an efficient search of the feature space.</p><p>These methods are tested and evaluated on 27 different datasets. The first datasets have a balance between the number of features and the number of data instances and have the scope to test the applicability of our bioinspired methods and the tuning of the parameters. We then perform a more challenging test on datasets having up to almost 50,000 features and fewer than 200 instances. These complex datasets are microarray experiments and image processing data. The results are compared with the ones obtained by genetic algorithms and particle swarm optimization.</p><p>Details regarding the techniques used are described in Section 3 and Section 4. Moreover, the comparison of the results is presented in Section 5. The work concluded the results and propose further directions to investigate in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Various heuristic techniques mimic the behavior of biological and physical systems in nature and prove to be robust methods for global optimizations. Some of them have been applied to feature selection (modeled as an optimization problem) and this section summarizes the contributions. A feature selection method based on genetic algorithm (GA) using a fuzzy set as fitness function has been proposed in <ref type="bibr" target="#b22">[22]</ref>. With the same fitness function, particle swarm optimization (PSO) achieves better performance than GA algorithm as evident from <ref type="bibr" target="#b23">[23]</ref>. A multi-objective method using the non-dominated sorting genetic algorithm (NSGAII) has been applied to feature selection, but the performance of the method has not been compared with any other feature selection algorithms. Hybrid NSGAII with rough sets is used as feature selection technique for microarray gene expression data <ref type="bibr" target="#b24">[24]</ref>.</p><p>A multi-objective method for feature selection based on genetic programming (GP) filter model has been applied to binary classification problems <ref type="bibr" target="#b25">[25]</ref>. Gaussian process multi-task regression using feature selection (GPMTFS) uses the multi-tree GP technique and designs a classifier utilizing the selected features <ref type="bibr" target="#b26">[26]</ref>.</p><p>Artificial bee colony (ABC) is a numerical optimization algorithm based on the foraging behavior of honeybees <ref type="bibr" target="#b27">[27]</ref>. A virtual bee algorithm (VBA) is applied to optimize numerical functions using a swarm of virtual bees that move randomly in the feature space and interact to find the food sources <ref type="bibr" target="#b28">[28]</ref>. An approach based on the natural behavior of honeybees in which randomly generated worker bees are moved in the elite bee direction, the elite representing the optimal (near to optimal) solution is proposed in <ref type="bibr" target="#b29">[29]</ref>.</p><p>Ant colony optimization (ACO) based wrapper feature selection algorithm is applied to network intrusion detection <ref type="bibr" target="#b30">[30]</ref>. ACO uses Fisher discrimination rate to adopt the heuristic information and rough set theory used for feature selection <ref type="bibr" target="#b31">[31]</ref>. Grey wolf optimization (GWO) modified to ensure a good balance between exploration and exploitation is proposed in <ref type="bibr" target="#b32">[32]</ref>. A multi-objective variant of GWO is developed to optimize multi-objective problems <ref type="bibr" target="#b33">[33]</ref>. New binary approaches of GWO (BGWO) are proposed and applied to feature selection problem <ref type="bibr" target="#b34">[34]</ref>. A variant of antlion optimization (ALO) using binary representation (BALO) is used for finding a feature subset that maximizes the classification performance while minimizing the number of selected features <ref type="bibr" target="#b35">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>This section provides an introduction to the standard GWO and ALO algorithms formulated for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Grey wolf optimization (GWO)</head><p>Grey wolf optimization is inspired by the hunting behavior of grey wolves in nature <ref type="bibr" target="#b36">[36]</ref>. The grey wolves are species have a strict social dominant hierarchy of leadership, being organized into four main levels:</p><p>1. Alpha (α) wolves (one male and one female) are the leaders. They are responsible for making decisions (hunting, sleeping place, time to wake, etc.). 2. Beta (β) wolves are second-level subordinate wolves that help α in decision making or the other pack activities. β wolf is the best candidate to be the next α. 3. Delta (δ) wolves are the third-level subordinate, and their responsibilities are scouts, sentinels, elders, hunters, and caretakers. 4. Omega (ω) wolves are the fourth-level subordinate and play the role of scapegoat.</p><p>In an optimization context, α wolves are the best solution, β and δ wolves are the second and third best solutions respectively, while ω wolves are the remaining candidate solutions. The hunting is guided by α, β, and δ, while ω follow these three candidates. In order for the pack to hunt prey, they first encircle it. The mathematical model of the encircling behavior is given in Eq. (1) <ref type="bibr" target="#b37">[37]</ref>:</p><formula xml:id="formula_0">- → X (t + 1) = - → X p (t) - - → A • - → D ,<label>(1)</label></formula><p>where -→ X is the grey wolf position, -→ X p is the prey position, t is the iteration number, and -→ D is given by:</p><formula xml:id="formula_1">- → D = | - → C • - → X p (t) - - → X (t)|. (2) - → A , - →</formula><p>C are coefficient vectors given by:</p><formula xml:id="formula_2">- → A = 2 - → a • - → r 1 -- → a ,<label>(3)</label></formula><formula xml:id="formula_3">M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT - → C = 2 - → r 2 ,<label>(4)</label></formula><p>where -→ a are linearly decreased from 2 to 0 over the course of iterations and r 1 , r 2 are random vectors in [0, 1]. The hunt is guided by α, but β and δ might also participate in hunting process. In the mathematical simulation of the hunting behavior, α, β, and δ are assumed to have better knowledge about the potential location of prey. ω update their positions according to the position of the best search agents as follows:</p><formula xml:id="formula_4">-→ D α = | -→ C 1 • -→ X α - - → X |, -→ D β = | -→ C 2 • -→ X β - - → X |, -→ D δ = | -→ C 3 • -→ X δ - - → X | (5) -→ X 1 = | -→ X α - -→ A 1 • -→ D α |, -→ X 2 = | -→ X β - -→ A 2 • -→ D β |, -→ X 3 = | -→ X δ - -→ A 3 • -→ D δ |<label>(6)</label></formula><formula xml:id="formula_5">- → X (t + 1) = -→ X 1 + -→ X 2 + -→ X 3 3<label>(7)</label></formula><p>Parameter -→ a controls the trade-off between exploitation and exploration and is linearly updated in each iteration with a range from 2 to 0 according to the Eq. ( <ref type="formula" target="#formula_6">8</ref>).</p><formula xml:id="formula_6">- → a = 2 -t • 2 M ax Iter ,<label>(8)</label></formula><p>where M ax Iter is the total iteration number allowed for the optimization. Candidate solutions tend to diverge from the prey when</p><formula xml:id="formula_7">|2 - → a • - → r 1 -- → a | &gt; 1</formula><p>and converge towards the prey when</p><formula xml:id="formula_8">|2 - → a • - → r 1 -- → a | &lt; 1.</formula><p>GWO is described as in algorithm (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Antlion Optimization (ALO)</head><p>Antlion optimization (ALO) has been proposed by Mirjalili <ref type="bibr" target="#b38">[38]</ref> and mimics the hunting mechanism of antlions in nature. They often hunt in larvae and the adulthood period is for reproduction. An antlion larvae dig a hole in the sand and, after digging the trap, the larvae hide underneath the bottom of the hole and wait for insects (preferably ants) to be trapped in the hole. Once the antlion recognizes that prey is in the trap, the antlion tries to catch Result: The optimal wolf position and its fitness. -Update α, β, and δ positions as the first best three solutions in the current population; as in Eqs. ( <ref type="formula">5</ref>), <ref type="bibr" target="#b5">(6)</ref>, and <ref type="bibr" target="#b6">(7)</ref>. end 3. Select the optimal grey wolf position. its prey. However, insects regularly are not caught immediately and try to escape from the trap. In this case, antlions intelligently throw sands towards to edge of the hole to slide the prey into the bottom of the hole. When prey is caught into the jaw, it is pulled from the soil and consumed. After consuming the prey, antlions throw the leftovers outside the hole and amend the hole for the next hunt.</p><p>The artificial antlion works in the following way: the preys are ants that move around the search space using different random walks. These random walks are affected by the traps of antlions. Antlions build holes proportional to their fitness (the higher the fitness, the larger the hole). Antlions with larger holes have a higher probability of catching ants. Each ant can be detected by an antlion in each iteration. The range of random walks decreases adaptively to mimic sliding ants towards antlions. If an ant becomes fitter than an antlion, this indicates that it is caught and pulled under the sand by the antlion. An antlion updates its position to the latest caught prey and builds a hole to enhance its chance of catching another prey after each hunt. ALO is formulated in the algorithm <ref type="bibr" target="#b1">(2)</ref>.</p><p>In order to explain the steps of ALO, we use the following notations: Result: The elitist antlion and the its fitness.</p><p>1. Initialize a population of n ant positions and n antlion positions randomly. 2. Calculate the fitness of all ants and antlions. 3. Find the fittest antlion (the elite). 4. while t ≤ M ax Iter do foreach ant i do i) Select an antlion using roulette wheel (building trap); as in Eqs. ( <ref type="formula" target="#formula_15">13</ref>) and <ref type="bibr" target="#b13">(14)</ref>.</p><p>ii) Slide ants towards the antlion;as in Eqs. ( <ref type="formula" target="#formula_17">15</ref>) and <ref type="bibr" target="#b15">(16)</ref>.</p><p>iii) Create a random walk for ant i and normalize it; as in Eq. ( <ref type="formula" target="#formula_22">18</ref>). end -Calculate the fitness of all ants.</p><p>-Replace an antlion with its corresponding ant it if becomes fitter.</p><p>-Update elite if an antlion becomes fitter than the elite. end 5. Select the optimal antlion position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>• t: the current iteration • T : the total number of iterations • Antlion t j : the position of antlion j at iteration t • Ant t i : the position ant i at iteration t • c t : is the minimum of all variables at t -th iteration.</p><p>• d t : indicates the vector including the maximum of all variables at t-th iteration.</p><p>• w: a constant defined based on the current iteration (w = 2 when t &gt; 0.1T , w = 3 when t &gt; 0.5T , w = 4 when t &gt; 0.75T , w = 5 when t &gt; 0.9T , and w = 6 when t &gt; 0.95T ). The constant w can adjust the accuracy level of exploitation.</p><p>• I: I is a ratio defined based on w using the equation</p><formula xml:id="formula_9">I = 10 w t T<label>(9)</label></formula><p>• a i : minimum random walk of i -th variable</p><formula xml:id="formula_10">• b i : maximum random walk of i -th variable.</formula><p>Random walks of ants are all based on Eq. ( <ref type="formula" target="#formula_11">10</ref>):</p><formula xml:id="formula_11">X(t) = [0, cumsum(2r(t 1 ) -1); cumsum(2r(t 2 ) -1); ...; cumsum(2r(t T ) -1)], (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>where cumsum calculates the cumulative sum, T is the maximum number of iteration, t shows the step of random walk, and r(t) is a stochastic function defined in Eq. <ref type="bibr" target="#b10">(11)</ref>.</p><formula xml:id="formula_13">r(t) = 1 if rand &gt; 0.5 0 if rand ≤ 0.5,<label>(11)</label></formula><p>with rand being a random number generated with uniform distribution in the interval of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>In order to keep the random walks inside the search space, the minmax normalization is applied:</p><formula xml:id="formula_14">X t+1 i = (X t i -a i ) × (d i -c t i ) (b t i -a i ) + c i ,<label>(12)</label></formula><p>Trapping of ants in antlion's hole is modelled by sliding of prey towards the selected antlion. The walk of the ant becomes bounded by the position of the antlion and is expressed in Eqs. <ref type="bibr" target="#b12">(13)</ref>, and ( <ref type="formula" target="#formula_16">14</ref>):</p><formula xml:id="formula_15">c t i = c t + Antlion t j ,<label>(13)</label></formula><formula xml:id="formula_16">d t i = d t + Antlion t j .<label>(14)</label></formula><p>Antlions shoot sands outwards the center of the hole once they realize that an ant is in the trap. This behavior slides down the trapped ant that is trying to escape. For mathematically modeling this behavior, the radius of ants' random walks hypersphere is decreased adaptively as shown in Eqs. ( <ref type="formula" target="#formula_17">15</ref>) and ( <ref type="formula" target="#formula_18">16</ref>):</p><formula xml:id="formula_17">c t = c t I ,<label>(15)</label></formula><formula xml:id="formula_18">d t = d t I . (<label>16</label></formula><formula xml:id="formula_19">)</formula><p>The final stage is catching the prey and re-building the hole when the antlion consumes the ant. It is assumed that catching prey occur when ants become fitter (goes inside sand) than its corresponding antlion. An antlion is then expected to update its position on the latest position of the hunted ant to improve its chance of catching new prey, modeled by Eq. ( <ref type="formula" target="#formula_20">17</ref>):</p><formula xml:id="formula_20">Antlion t j = Ant t i If f (Ant t i ) is better than f (antlion t j ). (<label>17</label></formula><formula xml:id="formula_21">)</formula><p>To maintain the best solution(s) across iterations, elitism should be applied. The random walk of an ant is guided by the selected antlion as well as the elite antlion and therefore the repositioning of a given ant takes the form of average of both random walks as in Eq. <ref type="bibr" target="#b17">(18)</ref>. where R t A is the random walk around the roulette wheel selected antlion, and R t E is the random walk around the elite antlion.</p><formula xml:id="formula_22">Ant t i = R t A + R t E 2 ,<label>(18)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The proposed Hybrid ALO-GWO for feature selection</head><p>GWO is an optimization algorithm with no internal control parameter which makes it easy to use. Moreover, GWO has a rational balance between exploration (diversification) and exploitation (intensification) through the setting of the parameter a which controls the exploration/exploitation rates at each iteration. The randomly set obstacle modeler C has an impact on the walk/step scale which provides enough variation and avoids algorithm stagnation, especially in the last optimization stages. Actually, a grey wolf can be viewed as an agent that is attracted by the best three agents in the pack but with some obstacles on its way. Figure <ref type="figure" target="#fig_0">1</ref> shows the walk step/scale obtained using Eq. ( <ref type="formula" target="#formula_23">19</ref>) that models the attraction of an agent by one of the leader wolves:</p><formula xml:id="formula_23">- → X (t + 1) = - → X p (t) -(2 - → a • - → r 1 -- → a ) • | - → C • - → X p (t) - - → X (t)|,<label>(19)</label></formula><p>with the factor (2 -→ a • -→ r 1 --→ a ) representing the walk scale/size, -→ C representing the obstacles in the wolf's way and -→ X p (t) is the attractor wolf position at time t and should be replaced by the best, second best and third best solution at every optimization iteration.</p><p>As can be seen in the Figure <ref type="figure" target="#fig_0">1</ref>, the wolf's step size has some variation while keeping a global convergence towards an attractor wolf -who is assumed</p><formula xml:id="formula_24">M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</formula><p>to know more about the prey. We can also notice that the new position of the wolf (in a given dimension) may be between the attractor position and the wolf's original position which means exploration (at positive values of the step size) or may be beyond the attractor position at negative values of the step size which means exploitation. Furthermore, for step size &gt; 1 the wolf diverges from the prey (attractor) while in the case of step size &lt; 1 the wolf converges towards the prey. Therefore, GWO has the merit of enhanced convergence capability while keeping enough variation of the population and the optimizer can be described as goal-directed optimizer which follows the best three solutions rather than following only the best solution as in particle swarm optimization (PSO) <ref type="bibr" target="#b1">[2]</ref>.</p><p>In ALO, two types of agents exist: ant and antlions. Antlions can be thought of as the set of the best solutions found ever where an antlion position is replaced only when finding better prey (ant) to replace it. Ants are in continuous motion and each ant changes its position at each iteration according to the antlions' positions. The updating mechanism of an ant relies on selecting an antlion using roulette wheel selection in combination with the best antlion and the given ant follows these two agents. Relying on an agent that is selected using roulette wheel has the merit of adding enough diversification and allows the algorithm to avoid stagnation. Stagnation avoidance capability gives such algorithm superiority on other algorithms such as PSO. Furthermore, the algorithm has minimal internal parameters compared to GA or PSO.</p><p>Choice of leader for a given swarm has a very strong impact on the explorative/exploitative capability of the algorithm. In GWO, the algorithm keeps track of the best three solutions found. So, weak agents have no chance to lead other agents and hence the exploration capability of the algorithm is lower while the exploitative capability is higher. In case of ALO, the algorithm keeps track of the best N (N is the population size) agents found and applies roulette wheel to select a leader besides the current global best to guide the swarm. So, weak agents have the chance to lead the swarm together with the current global best which enhances the exploration capability of the algorithm but at the expense of minimizing exploitation capability. An algorithm that uses both methods for leadership assignment seems to be more capable of keeping the balance between diversification and intensification. The proposed hybrid algorithm keeps both the swarm of ants in motion and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>also keeps the motion of antlion swarm. The hybrid algorithm updates both the weak agents (ants) using ALO principles to take the merits of ALO, as well as the strong agents (antlions) using GWO principles that have the merit of faster convergence and is more a goal follower. The proposed methodology is formally given in algorithm (3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 3: The proposed hybrid ALO-GWO algorithm</head><p>Input: Search space, fitness function, number of ants and antlions, number of iterations (M ax Iter ).</p><p>Result: The optimal antlion position and its fitness.</p><p>1. Initialize a population of n ant positions and n antlion positions randomly. 2. Calculate the fitness of all ants and antlions. 3. Find the fittest antlion (The elite). 4. while t ≤ M ax Iter do foreach ant i do i) Select an antlion using Roulette wheel (building trap).</p><p>ii) Slide ants towards the antlion. iii) Create a random walk for the ant i and normalize it. end -Calculate the fitness of all ants.</p><p>-Replace an antlion with its corresponding ant it if becomes fitter (Catching Prey) -Select the alpha, beta and delta from the antlion population based on their fitness.</p><p>-Update the exploration rate parameter a. -Update antlion positions.</p><p>-Update the elite if an antlion becomes fitter than the elite. end 5. Select the optimal antlion position.</p><p>The hybrid algorithm has enough diversification capabilities as a result of:</p><p>• Roulette wheel selection of agents -drawn from ALO which impacts the ants' swarm;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>• Adaptive size of the random walk as in GWO and impacts antlions swarm;</p><p>• Adaptive size of the random walks as in ALO and impacts the ants' swarm;</p><p>• The repositioning of the whole swarm rather than of ant swarm as in ALO.</p><p>Moreover, the hybrid algorithm has the following intensification capabilities:</p><p>• Goal-directed, which results from GWO walk where the three best solutions are followed and helps enhance the walk of antlions;</p><p>• Reduction of random walk scale/step size which is adopted in both GWO and ALO;</p><p>• Replacement of fitter ants with the corresponding antlion using ALO principles.</p><p>The proposed hybrid algorithm for feature selection works in a wrapperbased manner. The principal characteristic of wrapper methodologies is the use of the classification performance as a guide to feature selection procedure. K-nearest neighbor (KNN) is a supervised learning algorithm that classifies an unknown sample instance based on the majority of the K-nearest neighbor category. We use KNN as a classification method in our system to ensure the quality of the selected features <ref type="bibr" target="#b39">[39]</ref>. The feature classification criteria or the objective function in the wrapper feature selection reflects the classification performance as well as the number of selected features as given in Eq. ( <ref type="formula" target="#formula_25">20</ref>):</p><formula xml:id="formula_25">f θ = α • E + (1 -α) i θ i N ,<label>(20)</label></formula><p>where f θ is the fitness function of a binary vector θ of size N , with 0/1 elements representing unselected/selected features, N represents the total number of features in the dataset, E is the classifier error, and α is a constant controlling the importance of classification performance using the number of features selected.</p><p>In wrapper-based methods, the single evaluation of a given solution is very costly as it always applies training and testing of the given classier. Therefore, an effective selection of the search method is mandatory. In this study, we use a combination of ALO and GWO algorithms to adaptively search the features space, ensuring a maximization of the classification performance while keeping a minimum number of selected features. Iteratively, the antlion algorithm selects an antlion for hunting in a roulette wheel manner and performs the random walk of ants around the elite/best antlion. Based on the past two random walks, an ant adapts its location. Iteratively, if an ant becomes better than an antlion (in terms of fitness function value), the antlion eats it and changes its position to the ant's position. Antlion's update their positions according to the grey wolf principles, where the alpha, beta, and delta are selected among the antlions. This algorithm is iteratively applied for a number of steps. It worth mentioning that the random walk of an ant is limited by the exploration rate at the current iteration. Commonly, the exploration rate decreases as the optimization progress to allow for fine search/intensive search. The plot in Figure <ref type="figure" target="#fig_1">2</ref> outlines the exploration rate of the antlion at different optimization iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>A similar behavior is applied to the exploration rate controlling the repositioning of antlions, where antlions' exploration rate is linearly decreased from 2 to 0 at each iteration in order to allow for fine search.</p><p>An individual solution is represented as a continuous valued vector with same dimension as number of attributes in the given dataset. The solution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>vector is limited to the range [0, 1]. For a solution evaluation, the continuous values are mapped to a binary representation using Eq. ( <ref type="formula" target="#formula_26">21</ref>):</p><formula xml:id="formula_26">y ij = 0 If(x ij &lt; 0.5) 1 Otherwise,<label>(21)</label></formula><p>where x ij is the continuous value of solution i in dimension j, and y ij is the discrete representation of solution vector x.</p><p>ALO-GWO works on the wrapper-based feature selection manner employing KNN in classification data and ELM in regression data. The running time may increase when changing to another classifier, for example, support vector machine (SVM), random forest (RF), or artificial neural network (ANN). Therefore, switching to different classification methods should be carefully handled, particularly if the algorithm is adopted in real-time applications. The main limitation of the methodology presented in this paper is the nonexact repeatability of the hybrid ALO-GWO results. We monitored this at various applications of the algorithm. The subset of features selected might not be the same. Despite the fact that the resulting solutions are all good solutions, it might be confusing for the user to figure out which subset to consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results, analysis and discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets used for experiments</head><p>The global and optimizer-specific parameter settings are outlined in the Table <ref type="table" target="#tab_1">1</ref>. All the parameters are set either according to domain-specific knowledge as in the case of α, β parameters, based on trial and error on small simulations, or common in the literature such as the rest of the parameters.</p><p>We first tested the performance of ALO, GWO, and the hybrid ALO-GWO on eighteen datasets taken from the UCI machine learning repository <ref type="bibr" target="#b40">[40]</ref>. These datasets together with their number of features and the number of instances are shown in Table <ref type="table" target="#tab_2">2</ref>. We can observe that these datasets have a low ratio between the number of features and the number of instances. We then moved to more complex experiments for which the ratio between the number of attributes and the number of features is really high. These datasets (given in Table <ref type="table" target="#tab_3">3</ref>) are taken from <ref type="bibr" target="#b41">[41]</ref>. Seven of these datasets are microarray gene expression data and the other five are image (face) detection data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>10-fold cross-validation is used for the data in Table <ref type="table" target="#tab_2">2</ref> where one fold is kept for testing and the remaining 9-folds are equally divided between training and validation portions. 3-fold cross-validation is used for the data in Table <ref type="table" target="#tab_3">3</ref> where individual dataset is divided randomly into three equal portions namely training, validation, and testing <ref type="bibr" target="#b42">[42]</ref>. The validation set is used as the test set at the feature selection optimization time. The test set is hidden until the final evaluation of the selected features, the training data is used for training the classifier at both feature selection optimization and at the final testing stage. Such procedure is repeated M times to accurately assess statistical evaluation indicators. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation criteria</head><p>The well-known KNN is used as a classifier to evaluate the performance of individual algorithms with k=5 <ref type="bibr" target="#b39">[39]</ref>. Each optimization algorithm is repeated M times with different random seeds to test convergence capability.</p><p>We use a set of quantitative measures in order to analyze the results obtained by the methods we apply. The first three metrics are used to measure the mean, best, and worst expected performance of the algorithms. The fourth measure shows the ability of the optimizer to converge to the same optimal solution. The fifth metric shows the performance of the classifier on   is computed over all the sets of final solutions obtained by an optimizer in a number of individual runs <ref type="bibr" target="#b44">[44]</ref>. 5. Classification mean square error (CMSE): is a measure of classifier's average performance on the test data. It is averaged over all final sets in all the independent runs <ref type="bibr" target="#b45">[45]</ref>. 6. Average selected features: represents the average ratio of the selected features subset to the original number of features. The average is computed for each final set of solutions in multiple individual runs. 7. Average Fisher score: evaluates a feature subset such that in the data space spanned by the selected features, the distances between data points in different classes are as large as possible, while the distances between data points in the same class are as small as possible <ref type="bibr" target="#b46">[46]</ref>. Fisher score in this work is calculated for individual features given the class labels; as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><formula xml:id="formula_27">F j = c k=1 n k (µ j k -µ j ) 2 (σ j ) 2 , (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>where F j is the Fisher index for feature j, µ j and (σ j ) 2 are the mean</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>and std of the dataset, n k is the size of class k, and µ j k is the average of class k. The Fisher for a set of features is defined as:</p><formula xml:id="formula_29">F tot = 1 S S i=1 F i , (<label>23</label></formula><formula xml:id="formula_30">)</formula><p>where S is the number of selected features. The average Fisher score over a set of N runs is defined as:</p><formula xml:id="formula_31">F ishr -score = 1 N N i=1 F i tot ,<label>(24)</label></formula><p>where F i tot is the Fisher score computed for selected feature set on run i. 8. Wilcoxon rank sum test: is a nonparametric test for significance assessment. The test assigns rank to all the scores considered as one group and then sums the ranks of each group <ref type="bibr" target="#b47">[47]</ref>. The test statistic relays on calculating W as in Eq. ( <ref type="formula" target="#formula_32">25</ref>):</p><formula xml:id="formula_32">W = N i=1 (sgn(x 2,i -x 1,i ).R i ), (<label>25</label></formula><formula xml:id="formula_33">)</formula><p>where x 2,i , x 1,i are the best fitness values obtained by first and second optimizer on run i, R i is the rank of difference between x 2,i and x 1,i , and sgn(x) is the standard sign function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results and Discussion on UCI datasets</head><p>Two major scenarios are used to benchmark the proposed hybrid model versus the other methods depending on the initialization of search agents', namely uniform and small initialization. In uniform initialization, the search agents are positioned randomly on the search space following the uniform distribution random number generator (RNG). In small initialization, the search agents are initialized with a minor number of features selected. The purpose of using this initialization is to test whether certain algorithms can reach the optimum even if the initial population is very different from the optimum.</p><p>Mean, best, worst, and standard deviation fitness function values obtained by all the optimizers using small initialization are presented in Figure <ref type="figure" target="#fig_5">3</ref>, while the ones using the uniform initialization are presented in Figure <ref type="figure" target="#fig_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>We can observe that the hybrid ALO-GWO outperforms GA, PSO, GWO, and ALO, which proves the capability of ALO-GWO to adaptively search the feature space for optimal feature combination and avoid premature convergence that may be caused by stagnation in local minima. The enhanced performance of the hybrid algorithm can be justified by the fact that the algorithm takes advantages of GWO such as being more exploitative by following the three best solutions, and also the advantage of ALO for adopting roulette wheel selection which provides more diverse search capability and hence helps to avoid premature convergence. Moreover, we can see that the hybrid algorithm uses random step sizes for the random walk but within a linearly decremented envelope. The random setting of the random walk scale helps the optimizer to avoid stagnation and provides diverse solutions. Figure <ref type="figure" target="#fig_0">1</ref> depicts the adopted random walk scale used in the hybrid algorithm. We can remark also that the enhanced performance of the proposed hybrid algorithm is comparable for both small and uniform initialization which proves the capability to generate enough diversity in the population, which is a result of repositioning both the ants and the antlions rather than the repositioning the half of the swarm. Moreover, the diversity of population results from the adoption of the roulette wheel in the selection process of antlions. Figure <ref type="figure" target="#fig_7">5</ref> depicts a simple diversity measure for all search agents for GWO, ALO, and the proposed hybrid ALO-GWO. The diversity measure calculates the standard deviation of the agents' positions averaged over all dimensions. We can see from the figure the high diversification of ALO-GWO which results from using roulette wheel selection, the random setting of random walk scale and the change of positions of all ants and antlions in every iteration.</p><p>Figure <ref type="figure" target="#fig_9">7</ref> depicts the swarm mean fitness at each iteration after applying the ALO step and the GWO step as well as the global best fitness on a sample test data. We can remark from the figure that ALO occasionally squashes some search agent apart from the current swarm local region (explorative agents) and hence we can see that it occasionally worsens the swarm mean fitness. On the contrary, we see that GWO with its exploitative behavior always attracts the swarm towards the best-performing agents (alpha, beta, and delta) and hence enhances the swarm mean fitness. By combing the two we can see that the global best fitness can avoid stagnation and keeps a good performance.</p><p>For assessing the stability of the stochastic algorithms and the capability of converging to the same/similar optimum, we measure the standard deviation of the obtained fitness values over the different runs. We can again     observe that the minimum for this measure is obtained by ALO-GWO in the uniform initialization, while for the small initialization ALO-GWO and ALO have comparable standard deviation. Having a minimum value for the standard deviation measure proves the capability of the optimizer to abandon local optima and to converge to the global optimum. Again, such capability can be interpreted by the enhanced diversification of the optimizer which enables it to avoid premature convergence. Figure <ref type="figure" target="#fig_8">6</ref> highlights the convergence curve (on average) for one of the datasets using the different optimizers and one can notice that the hybrid algorithm still has the capability to find better solution at the final optimization stages while both ALO and GWO stagnate at local minima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>Regarding the size of selected features with respect to the original size, Figures <ref type="figure" target="#fig_10">8</ref> and<ref type="figure" target="#fig_11">9</ref> present this ratio. We can observe that, although ALO-GWO outperforms all other methods in classification performance, has a comparable ratio of features selected which confirms that ALO-GWO can select the optimal feature combination with comparable size. Fisher score is calculated for the output of the different feature selection systems over the different datasets as shown in Tables <ref type="table" target="#tab_4">4</ref> and<ref type="table" target="#tab_5">5</ref>. We can again observe that the best Fisher score value is achieved by ALO-GWO using small initialization. This confirms that the performance of ALO-GWO is better compared to the other methods used.</p><p>Furthermore, we measured the Wilcoxon test for all the algorithms and the results are presented in Tables <ref type="table" target="#tab_6">6</ref> and<ref type="table" target="#tab_7">7</ref>. We can see that the score obtained by ALO-GWO is significantly better compared to ALO (0.002), GWO (0), PSO (0), and GA (0.002) using small initialization. While using uniform initialization, ALO-GWO obtains significantly better results compared to ALO (0.005), PSO (0), GA (0.002), and GWO (0.0.06).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results and Discussion on microarray and image datasets</head><p>Table <ref type="table" target="#tab_8">8</ref> shows the best fitness function values obtained by all optimizers for the five microarray datasets and the four image datasets. We observe that ALO-GWO outperforms GA, PSO, GWO, and ALO for the datasets that have lower attributes, such as ORL10P, AR10P, and PIE10P. GWO outperforms GA, PSO, ALO, and ALO-GWO for the datasets that have a  <ref type="table" target="#tab_9">9</ref> outlines the worst fitness function values obtained by all the algorithms for all the nine datasets. Table <ref type="table" target="#tab_10">10</ref> presents the mean fitness function values. We remark that the results of ALO-GWO on average, outperform the results obtained by GA, PSO, ALO, and GWO. Such a huge search space with a large number of features (dimensions) adds more difficulty on the optimizer. Again, the advantages of using the hybrid algorithm become more apparent and the added diversification, as well as intensification capabilities, are clearer.</p><p>We measure the standard deviation of the solutions obtained by all the algorithms, and the results are reported in Table <ref type="table" target="#tab_11">11</ref>. The minimum value of the std measure is obtained by ALO-GWO. Regarding the size of selected features with respect to the original feature set size, we can see from Table <ref type="table" target="#tab_12">12</ref> that, although ALO-GWO outperforms all the other methods in reduction rate, has a comparable ratio of features selected, which confirms that the ALO-GWO can select the optimal feature combination with comparable size. From Table <ref type="table" target="#tab_13">13</ref>, we can observe that ALO-GWO achieves the best We measured the Wilcoxon test for all the optimizers and the results are reported in the Table <ref type="table" target="#tab_14">14</ref>. The results obtained by ALO-GWO are significantly better while compared with GWO (0.001), PSO (0), and GA (0), and insignificant while compared to GWO (0.630).</p><formula xml:id="formula_34">M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>The aim of this work is to investigate and analyze the potential of antlion optimization, grey wolf optimization, and hybrid antlion-grey wolf algorithm for selecting significant features from datasets in which the number of attributes is very large while the number of instances is relatively small. This type of feature selection datasets are challenging for machine learning algorithms as the complexity of the search space is very large due to a large    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimizers</head><p>Wilcoxon Test value Comment ALO-GWO -ALO 0.630 insignificant ALO-GWO -GWO 0.001 significant ALO-GWO -PSO 0 significant ALO-GWO -GA 0 significant</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head><p>number of features, on one hand, while on the other hand, the small number of instances do not provide sufficient information for learning. We concentrate our efforts on proving that the nature-inspired heuristics can be adapted to perform very well in these situations. The two algorithms investigated here are hybridized in a third model and the performance of these models seems to outperform that of the other models used for comparisons such as genetic algorithms and particle swarm optimization. This indicates that these methods are efficient optimizers for large-dimensional small-instance datasets and are able to obtain accurate results in terms of classification on feature subset selection. The hybrid ALO-GWO algorithm has a very good balance between the exploration of the large search space and the exploitation of optimal solutions. The algorithm has the capability to generate diverse solutions and to avoid premature convergence.</p><p>On the basis of future performance, we have a few ideas that can be investigated in addition to the work presented here:</p><p>1. Use enhanced initialization method that starts the optimization with solution pool closer to optimal; 2. Extend the hybrid algorithm to work on parallel distributed mode to enhance convergence time; 3. Test the methodology on other big datasets besides those from bioinformatics. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Grey wolf optimization (GWO)Input: Number of grey wolves (n), maximum iterations (M ax Iter ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 2 :</head><label>2</label><figDesc>Antlion optimization (ALO) algorithmInput: Search space, fitness function, number of ants and antlions, number of iterations (M ax Iter ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: GWO step size/scale for individual wolf across optimization time</figDesc><graphic coords="12,149.22,125.80,311.82,141.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Exploration rate of different ALO iterations</figDesc><graphic coords="16,177.56,125.80,255.12,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 .</head><label>4</label><figDesc>Standard deviation (Std): is used to ensure that the optimizer convergence to the same optimal and ensures repeatability of the results. It M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean, best, worst, and standard deviation fitness function values obtained by all optimizes using small initialization.</figDesc><graphic coords="23,177.57,150.50,255.12,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mean, best, worst, and standard deviation fitness function values obtained by all optimizes using uniform initialization.</figDesc><graphic coords="23,177.56,428.71,255.13,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Population diversity for GWO, ALO, and ALO-GWO</figDesc><graphic coords="24,149.22,156.34,311.82,170.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Global best fitness (average) for GWO, ALO, and ALO-GWO</figDesc><graphic coords="24,149.22,434.26,311.81,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Swarm mean fitness after applying ALO and GWO with the global best at each iteration</figDesc><graphic coords="25,163.39,125.80,283.47,198.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average selected feature values using small initialization</figDesc><graphic coords="26,135.04,125.80,340.17,255.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average selected feature values using uniform initialization</figDesc><graphic coords="27,135.04,125.80,340.17,255.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>List of used parameters</figDesc><table><row><cell>Parameter</cell><cell>Value(s)</cell></row><row><cell>K for cross validation</cell><cell>10 or 3</cell></row><row><cell>M The number of runs</cell><cell>30</cell></row><row><cell>No. of search agents</cell><cell>8</cell></row><row><cell>No. of iterations</cell><cell>70</cell></row><row><cell>Problem dimension</cell><cell>Number of features in the data</cell></row><row><cell>Search domain</cell><cell>[0, 1]</cell></row><row><cell>r1&amp;r2 in GWO &amp; ALO</cell><cell>r1 and r2 drawn from the uniform distribution</cell></row><row><cell>Crossover Fraction in GA</cell><cell>0.8</cell></row><row><cell>Inertia factor of PSO</cell><cell>0.1</cell></row><row><cell>Individual-best acceleration factor of PSO</cell><cell>0.1</cell></row><row><cell>α parameter in the fitness function</cell><cell>0.99</cell></row><row><cell>β parameter in the fitness function</cell><cell>0.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>UCI datasets. Mean fitness: is an average value of all the solutions in the final sets obtained by an optimizer in a number of individual runs<ref type="bibr" target="#b43">[43]</ref>. 2. Best fitness: is the best solution found by an optimizer in all the final sets resulted from a number of individual runs<ref type="bibr" target="#b43">[43]</ref>. 3. Worst fitness: is the worst solution found by an optimizer in all the final sets resulted from a number of individual runs<ref type="bibr" target="#b43">[43]</ref>.</figDesc><table><row><cell>Dataset</cell><cell cols="3">No. Attributes No. Instances No. Classes</cell></row><row><cell>Breastcancer</cell><cell>10</cell><cell>699</cell><cell>2</cell></row><row><cell>Exactly</cell><cell>13</cell><cell>1000</cell><cell>2</cell></row><row><cell>Exactly2</cell><cell>13</cell><cell>1000</cell><cell>2</cell></row><row><cell>Lymphography</cell><cell>18</cell><cell>148</cell><cell>8</cell></row><row><cell>M-of-n</cell><cell>13</cell><cell>1000</cell><cell>2</cell></row><row><cell>Tic-tac-toe</cell><cell>9</cell><cell>958</cell><cell>2</cell></row><row><cell>Vote</cell><cell>16</cell><cell>300</cell><cell>2</cell></row><row><cell>Zoo</cell><cell>17</cell><cell>101</cell><cell>7</cell></row><row><cell>Wine</cell><cell>13</cell><cell>178</cell><cell>3</cell></row><row><cell>Spect</cell><cell>22</cell><cell>267</cell><cell>2</cell></row><row><cell>Sonar</cell><cell>60</cell><cell>208</cell><cell>2</cell></row><row><cell>Penglung</cell><cell>325</cell><cell>73</cell><cell>7</cell></row><row><cell>Ionosphere</cell><cell>34</cell><cell>351</cell><cell>2</cell></row><row><cell>Heart</cell><cell>13</cell><cell>270</cell><cell>4</cell></row><row><cell>Congress</cell><cell>16</cell><cell>435</cell><cell>2</cell></row><row><cell>Breast</cell><cell>30</cell><cell>569</cell><cell>2</cell></row><row><cell>Chess(Krvskp)</cell><cell>36</cell><cell>3196</cell><cell>2</cell></row><row><cell>Waveform</cell><cell>40</cell><cell>5000</cell><cell>3</cell></row><row><cell cols="4">test data. The sixth and seventh metrics are a measure of the size of se-</cell></row><row><cell cols="4">lected features set (we expect a low number of features and low classification</cell></row><row><cell cols="4">error) and the resulted data compactness/separability after applying feature</cell></row><row><cell cols="4">selection. A final indicator is used to assess the statistical significance of the</cell></row><row><cell cols="2">difference between two optimizers.</cell><cell></cell><cell></cell></row><row><cell>1.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Microarray gene expression and image datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">No. Attributes No. Instances No. Classes</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>11340</cell><cell>111</cell><cell>3</cell></row><row><cell>GLA-BRA-180</cell><cell>49151</cell><cell>180</cell><cell>4</cell></row><row><cell>SMK-CAN-187</cell><cell>19993</cell><cell>187</cell><cell>2</cell></row><row><cell>TOX-171</cell><cell>5748</cell><cell>171</cell><cell>4</cell></row><row><cell>GLI-85</cell><cell>22283</cell><cell>85</cell><cell>2</cell></row><row><cell>Nci9</cell><cell>9712</cell><cell>60</cell><cell>9</cell></row><row><cell>Carcinom</cell><cell>9182</cell><cell>174</cell><cell>11</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>10304</cell><cell>100</cell><cell>10</cell></row><row><cell>PIX10P</cell><cell>10000</cell><cell>100</cell><cell>10</cell></row><row><cell>AR10P</cell><cell>2400</cell><cell>130</cell><cell>10</cell></row><row><cell>PIE10P</cell><cell>2420</cell><cell>210</cell><cell>10</cell></row><row><cell>ORL</cell><cell>1024</cell><cell>400</cell><cell>40</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Average Fisher score values using small initialization</figDesc><table><row><cell>Dataset</cell><cell cols="5">GA PSO ALO GWO ALO-GWO</cell></row><row><cell>Breastcancer</cell><cell cols="4">0.654 0.638 0.730 0.408</cell><cell>0.648</cell></row><row><cell>Breast</cell><cell cols="3">0.235 0.128 0.285</cell><cell>0.053</cell><cell>0.306</cell></row><row><cell>Congress</cell><cell cols="3">0.208 0.129 0.169</cell><cell>0.073</cell><cell>0.171</cell></row><row><cell>Exactly</cell><cell>0.001</cell><cell>0</cell><cell>0.001</cell><cell>0</cell><cell>0.001</cell></row><row><cell>Exactly2</cell><cell>0.001</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.001</cell></row><row><cell>Heart</cell><cell cols="4">0.079 0.053 0.093 0.025</cell><cell>0.079</cell></row><row><cell>Ionosphere</cell><cell cols="3">0.033 0.015 0.013</cell><cell>0.005</cell><cell>0.021</cell></row><row><cell cols="5">Chess(Krvskp) 0.019 0.011 0.021 0.003</cell><cell>0.021</cell></row><row><cell cols="4">Lymphography 0.157 0.062 0.137</cell><cell>0.020</cell><cell>0.130</cell></row><row><cell>M-of-n</cell><cell cols="4">0.027 0.020 0.031 0.009</cell><cell>0.031</cell></row><row><cell>Penglung</cell><cell cols="3">0.253 0.061 0.088</cell><cell>0.007</cell><cell>0.069</cell></row><row><cell>Sonar</cell><cell cols="3">0.019 0.008 0.016</cell><cell>0.002</cell><cell>0.015</cell></row><row><cell>Spect</cell><cell cols="3">0.027 0.005 0.023</cell><cell>0.002</cell><cell>0.023</cell></row><row><cell>Tic-tac-toe</cell><cell cols="4">0.005 0.004 0.006 0.001</cell><cell>0.005</cell></row><row><cell>Vote</cell><cell cols="3">0.190 0.112 0.161</cell><cell>0.072</cell><cell>0.163</cell></row><row><cell>Waveform</cell><cell cols="4">0.124 0.086 0.152 0.019</cell><cell>0.142</cell></row><row><cell>Wine</cell><cell cols="4">0.538 0.485 0.701 0.250</cell><cell>0.558</cell></row><row><cell>Zoo</cell><cell cols="4">11.136 6.925 11.195 3.744</cell><cell>12.899</cell></row><row><cell cols="2">classification performance.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Average Fisher score values using uniform initialization</figDesc><table><row><cell>Dataset</cell><cell>GA</cell><cell>PSO</cell><cell cols="3">ALO GWO ALO-GWO</cell></row><row><cell>Breastcancer</cell><cell cols="2">0.717 0.750</cell><cell>0.717</cell><cell>0.559</cell><cell>0.765</cell></row><row><cell>Breast</cell><cell cols="2">0.220 0.230</cell><cell>0.313</cell><cell>0.189</cell><cell>0.268</cell></row><row><cell>Congress</cell><cell cols="2">0.195 0.203</cell><cell>0.183</cell><cell>0.157</cell><cell>0.163</cell></row><row><cell>Exactly</cell><cell cols="4">0.001 0.001 0.001 0.001</cell><cell>0.001</cell></row><row><cell>Exactly2</cell><cell cols="4">0.001 0.001 0.001 0.001</cell><cell>0.001</cell></row><row><cell>Heart</cell><cell cols="2">0.094 0.084</cell><cell>0.078</cell><cell>0.072</cell><cell>0.077</cell></row><row><cell>Ionosphere</cell><cell cols="2">0.028 0.038</cell><cell>0.019</cell><cell>0.015</cell><cell>0.028</cell></row><row><cell cols="3">Chess(Krvskp) 0.021 0.020</cell><cell>0.022</cell><cell>0.019</cell><cell>0.020</cell></row><row><cell cols="3">Lymphography 0.118 0.178</cell><cell>0.146</cell><cell>0.060</cell><cell>0.119</cell></row><row><cell>M-of-n</cell><cell cols="2">0.030 0.029</cell><cell>0.031</cell><cell>0.029</cell><cell>0.031</cell></row><row><cell>Penglung</cell><cell cols="2">0.304 0.303</cell><cell>0.062</cell><cell>0.099</cell><cell>0.097</cell></row><row><cell>Sonar</cell><cell cols="2">0.019 0.020</cell><cell>0.018</cell><cell>0.014</cell><cell>0.013</cell></row><row><cell>Spect</cell><cell cols="2">0.024 0.022</cell><cell>0.025</cell><cell>0.022</cell><cell>0.021</cell></row><row><cell>Tic-tac-toe</cell><cell cols="2">0.006 0.005</cell><cell>0.006</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>Vote</cell><cell cols="2">0.192 0.199</cell><cell>0.144</cell><cell>0.150</cell><cell>0.156</cell></row><row><cell>Waveform</cell><cell cols="2">0.129 0.128</cell><cell>0.154</cell><cell>0.114</cell><cell>0.139</cell></row><row><cell>Wine</cell><cell cols="2">0.482 0.532</cell><cell>0.548</cell><cell>0.500</cell><cell>0.503</cell></row><row><cell>Zoo</cell><cell cols="4">12.573 11.229 13.617 11.650</cell><cell>10.635</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Wilcoxon test on the average fitness using small initialization.</figDesc><table><row><cell>Optimizers</cell><cell cols="2">Wilcoxon Test value Comment</cell></row><row><cell>ALO-GWO -ALO</cell><cell>0.002</cell><cell>significant</cell></row><row><cell>ALO-GWO -GWO</cell><cell>0.04</cell><cell>significant</cell></row><row><cell>ALO-GWO -PSO</cell><cell>0</cell><cell>significant</cell></row><row><cell>ALO-GWO -GA</cell><cell>0.003</cell><cell>significant</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Wilcoxon test on the average fitness using uniform initialization</figDesc><table><row><cell>Optimizers</cell><cell cols="2">Wilcoxon Test value Comment</cell></row><row><cell>ALO-GWO -ALO</cell><cell>0.006</cell><cell>significant</cell></row><row><cell>ALO-GWO -GWO</cell><cell>0.06</cell><cell>insignificant</cell></row><row><cell>ALO-GWO -PSO</cell><cell>0</cell><cell>significant</cell></row><row><cell>ALO-GWO -GA</cell><cell>0.002</cell><cell>significant</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Best fitness obtained using uniform initialization for the microarray and image detection datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="6">ALO-GWO ALO GWO GA PSO Full</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>0.163</cell><cell cols="5">0.189 0.165 0.188 0.163 0.242</cell></row><row><cell>GLA-BRA-180</cell><cell>0.151</cell><cell cols="5">0.150 0.185 0.283 0.266 0.383</cell></row><row><cell>SMK-CAN-187</cell><cell>0.118</cell><cell cols="5">0.145 0.128 0.175 0.206 0.402</cell></row><row><cell>TOX-171</cell><cell>0.192</cell><cell cols="5">0.247 0.158 0.245 0.311 0.546</cell></row><row><cell>GLI-85</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.103</cell></row><row><cell>Nci9</cell><cell>0</cell><cell>0.1</cell><cell>0.3</cell><cell cols="3">0.389 0.389 0.444</cell></row><row><cell>Carcinom</cell><cell>0.017</cell><cell cols="5">0.017 0.034 0.052 0.034 0.086</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell cols="3">0.059 0.058 0.152</cell></row><row><cell>PIX10P</cell><cell>0.115</cell><cell cols="5">0.097 0.167 0.232 0.239 0.229</cell></row><row><cell>AR10P</cell><cell>0.155</cell><cell cols="5">0.171 0.156 0.230 0.228 0.512</cell></row><row><cell>PIE10P</cell><cell>0.161</cell><cell cols="5">0.173 0.157 0.231 0.227 0.229</cell></row><row><cell>ORL</cell><cell>0.178</cell><cell cols="5">0.204 0.178 0.216 0.231 0.313</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Worst fitness obtained using uniform initialization for the microarray and image detection datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">ALO-GWO ALO GWO GA PSO Full</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>0.352</cell><cell cols="3">0.429 0.356 0.461 0.430 0.651</cell></row><row><cell>GLA-BRA-180</cell><cell>0.372</cell><cell cols="3">0.278 0.387 0.538 0.559 0.604</cell></row><row><cell>SMK-CAN-187</cell><cell>0.300</cell><cell cols="3">0.311 0.415 0.414 0.523 0.618</cell></row><row><cell>TOX-171</cell><cell>0.383</cell><cell cols="3">0.417 0.420 0.475 0.592 0.789</cell></row><row><cell>GLI-85</cell><cell>0.31</cell><cell>0.31</cell><cell cols="2">0.414 0.483 0.483 0.483</cell></row><row><cell>Nci9</cell><cell>0.7</cell><cell>0.8</cell><cell>0.824</cell><cell>0.85 0.824 0.85</cell></row><row><cell>Carcinom</cell><cell>0.31</cell><cell>0.31</cell><cell cols="2">0.345 0.31 0.345 0.379</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>0.304</cell><cell cols="3">0.381 0.379 0.471 0.467 0.486</cell></row><row><cell>PIX10P</cell><cell>0.369</cell><cell cols="3">0.358 0.414 0.526 0.526 0.397</cell></row><row><cell>AR10P</cell><cell>0.602</cell><cell cols="3">0.736 0.614 0.778 0.775 0.805</cell></row><row><cell>PIE10P</cell><cell>0.453</cell><cell cols="3">0.488 0.485 0.507 0.551 0.538</cell></row><row><cell>ORL</cell><cell>0.481</cell><cell cols="3">0.444 0.459 0.451 0.466 0.519</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Mean fitness obtained using uniform initialization for the microarray and image detection datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">ALO-GWO ALO GWO GA PSO Full</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>0.252</cell><cell>0.285 0.277 0.323 0.341 0.424</cell></row><row><cell>GLA-BRA-180</cell><cell>0.230</cell><cell>0.230 0.297 0.353 0.378 0.499</cell></row><row><cell>SMK-CAN-187</cell><cell>0.214</cell><cell>0.217 0.221 0.270 0.304 0.507</cell></row><row><cell>TOX-171</cell><cell>0.287</cell><cell>0.324 0.286 0.365 0.440 0.654</cell></row><row><cell>GLI-85</cell><cell>0.077</cell><cell>0.086 0.138 0.144 0.143 0.233</cell></row><row><cell>Nci9</cell><cell>0.491</cell><cell>0.481 0.597 0.616 0.613 0.679</cell></row><row><cell>Carcinom</cell><cell>0.144</cell><cell>0.149 0.168 0.170 0.168 0.248</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>0.180</cell><cell>0.200 0.248 0.305 0.305 0.330</cell></row><row><cell>PIX10P</cell><cell>0.239</cell><cell>0.349 0.386 0.477 0.494 0.259</cell></row><row><cell>AR10P</cell><cell>0.514</cell><cell>0.494 0.503 0.604 0.626 0.702</cell></row><row><cell>PIE10P</cell><cell>0.295</cell><cell>0.329 0.318 0.392 0.399 0.401</cell></row><row><cell>ORL</cell><cell>0.327</cell><cell>0.333 0.319 0.340 0.339 0.419</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Standard deviation fitness obtained using uniform initialization or the microarray and image detection datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">ALO-GWO ALO GWO GA PSO Full</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>0.053</cell><cell>0.058 0.059 0.073 0.069 0.094</cell></row><row><cell>GLA-BRA-180</cell><cell>0.055</cell><cell>0.043 0.049 0.056 0.067 0.057</cell></row><row><cell>SMK-CAN-187</cell><cell>0.037</cell><cell>0.042 0.062 0.055 0.074 0.054</cell></row><row><cell>TOX-171</cell><cell>0.046</cell><cell>0.053 0.072 0.061 0.068 0.071</cell></row><row><cell>GLI-85</cell><cell>0.057</cell><cell>0.061 0.083 0.086 0.086 0.089</cell></row><row><cell>Nci9</cell><cell>0.118</cell><cell>0.120 0.106 0.105 0.105 0.103</cell></row><row><cell>Carcinom</cell><cell>0.067</cell><cell>0.065 0.067 0.065 0.067 0.071</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>0.064</cell><cell>0.096 0.093 0.104 0.100 0.083</cell></row><row><cell>PIX10P</cell><cell>0.075</cell><cell>0.086 0.106 0.117 0.116 0.097</cell></row><row><cell>AR10P</cell><cell>0.071</cell><cell>0.085 0.077 0.086 0.080 0.081</cell></row><row><cell>PIE10P</cell><cell>0.078</cell><cell>0.090 0.083 0.085 0.091 0.089</cell></row><row><cell>ORL</cell><cell>0.045</cell><cell>0.048 0.053 0.049 0.046 0.050</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Average size of the selected feature set using uniform initialization for the microarray and image detection datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">ALO-GWO ALO GWO GA PSO</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>0.110</cell><cell>0.132 0.110 0.488 0.482</cell></row><row><cell>GLA-BRA-180</cell><cell>0.082</cell><cell>0.018 0.142 0.495 0.491</cell></row><row><cell>SMK-CAN-187</cell><cell>0.091</cell><cell>0.092 0.163 0.491 0.488</cell></row><row><cell>TOX-171</cell><cell>0.174</cell><cell>0.204 0.163 0.488 0.481</cell></row><row><cell>GLI-85</cell><cell>0.211</cell><cell>0.206 0.403 0.500 0.500</cell></row><row><cell>Nci9</cell><cell>0.137</cell><cell>0.119 0.370 0.500 0.500</cell></row><row><cell>Carcinom</cell><cell>0.295</cell><cell>0.278 0.412 0.499 0.499</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>0.030</cell><cell>0.005 0.082 0.480 0.480</cell></row><row><cell>PIX10P</cell><cell>0.002</cell><cell>0.003 0.072 0.479 0.479</cell></row><row><cell>AR10P</cell><cell>0.025</cell><cell>0.057 0.115 0.472 0.462</cell></row><row><cell>PIE10P</cell><cell>0.065</cell><cell>0.068 0.115 0.474 0.462</cell></row><row><cell>ORL</cell><cell>0.340</cell><cell>0.331 0.346 0.499 0.502</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Classification mean square error obtained using uniform initialization.</figDesc><table><row><cell>Dataset</cell><cell cols="2">ALO-GWO ALO GWO GA PSO Full</cell></row><row><cell>MICROARRAY GENE EXPRESSION DATA</cell><cell></cell><cell></cell></row><row><cell>CLL-SUB-111</cell><cell>0.257</cell><cell>0.281 0.271 0.321 0.336 0.416</cell></row><row><cell>GLA-BRA-180</cell><cell>0.224</cell><cell>0.224 0.294 0.355 0.374 0.495</cell></row><row><cell>SMK-CAN-187</cell><cell>0.232</cell><cell>0.216 0.229 0.269 0.308 0.505</cell></row><row><cell>TOX-171</cell><cell>0.287</cell><cell>0.325 0.281 0.359 0.443 0.655</cell></row><row><cell>GLI-85</cell><cell>0.221</cell><cell>0.223 0.226 0.226 0.230 0.221</cell></row><row><cell>Nci9</cell><cell>0.683</cell><cell>0.707 0.686 0.675 0.675 0.663</cell></row><row><cell>Carcinom</cell><cell>0.231</cell><cell>0.238 0.234 0.236 0.232 0.236</cell></row><row><cell>IMAGE (FACE) DETECTION DATA</cell><cell></cell><cell></cell></row><row><cell>ORL10P</cell><cell>0.183</cell><cell>0.202 0.246 0.309 0.305 0.321</cell></row><row><cell>PIX10P</cell><cell>0.117</cell><cell>0.097 0.161 0.231 0.236 0.256</cell></row><row><cell>AR10P</cell><cell>0.474</cell><cell>0.489 0.509 0.599 0.615 0.695</cell></row><row><cell>PIE10P</cell><cell>0.293</cell><cell>0.328 0.317 0.402 0.404 0.402</cell></row><row><cell>ORL</cell><cell>0.406</cell><cell>0.404 0.402 0.408 0.409 0.429</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Wilcoxon test on the average fitness for different optimizers using uniform initialization.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the IPROCOM Marie Curie initial training network, funded through the People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme FP7/2007-2013/ under REA grant agreement No. 316555.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M A N U S C R I P T A C C E P T E D ACCEPTED MANUSCRIPT</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hybrid genetic algorithms for feature selection</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Particle swarm optimization for feature selection in classification: a multi-objective approach</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1656" to="1671" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An introduction to variable and attribute selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nature-Inspired Metaheuristic Algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Luniver Press</publisher>
			<pubPlace>UK, 2nd Edition</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Survey on Evolutionary Computation Approaches to Feature Selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="606" to="626" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bio-Inspired Optimization Methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Valdez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer Handbook of Computational Intelligence</publisher>
			<biblScope unit="page" from="1533" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cuckoo Search via Levy Flights</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>World Congress on Nature and Biologically Inspired Computing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A New Metaheuristic Bat-Inspired Algorithm</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Inspired Cooperative Strategies for Optimization, Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">284</biblScope>
			<biblScope unit="page" from="65" to="74" />
			<date type="published" when="2010">2010</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Water cycle algorithm A novel metaheuristic optimization method for solving constrained engineering optimization problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eskandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahreininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Structures</title>
		<imprint>
			<biblScope unit="volume">110111</biblScope>
			<biblScope unit="page" from="151" to="166" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Metaheuristic algorithms for approximate solution to ordinary differential equations of longitudinal fins having various profiles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="360" to="379" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved binary artificial fish swarm algorithm for the 0-1 multidimensional knapsack problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A K</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M A C</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M G P</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm and Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Binary Grey Wolf Optimization Approaches for Feature Selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Binary ant lion approaches for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page" from="54" to="65" />
			<date type="published" when="2016-11">November 2016</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A new bioinspired optimisation algorithm: Bird Swarm Algorithm</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">B</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Experimental and Theoretical Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Review of the Applications of Bio-Inspired Flower Pollination Algorithm</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chiroma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L M</forename><surname>Shuib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Muaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Abubakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Ila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Maitama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Soft Computing and Software Engineering</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="435" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Experienced Grey Wolf Optimizer through Reinforcement Learning and Neural Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grosan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017-01">January 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Impact of chaos functions on modern swarm optimizers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2016">0158738. July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computational Intelligence Modeling of the Macromolecules Release from PLGA Microspheres-Focus on Feature Selection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Szlȩk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grosan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jachowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016">e0157610. June 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-Objective Retinal Vessel Localization Using Flower Pollination Search Algorithm With Pattern Search</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parv</forename></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Data Analysis and Classification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-05">May 2016</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature selection via chaotic antlion optimization</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grosan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016-03">March 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">particle swarm optimisation for attribute selection in classification: Novel initialisation and updating mechanisms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Browne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="261" to="276" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>A N U S C R I P T A C C E P T E D Accepted</surname></persName>
		</author>
		<author>
			<persName><surname>Manuscript</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Genetic algorithm with fuzzy fitness function for feature selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Industrial Electronics</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="315" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Genetic algorithms with multiparent recombination</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Eiben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ruttkay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPSN III: Proceedings of the International Conference on Evolutionary Computation, The Third Conference on Parallel Problem Solving from Nature</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-objective feature selection with NSGA II</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Hamdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adaptive and Natural Computing Algorithms</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4431</biblScope>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Genetic programming for feature subset ranking in binary classification problems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Neshatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genetic Programming</title>
		<imprint>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Genetic programming for simultaneous feature selection and classifier design</title>
		<author>
			<persName><forename type="first">D</forename><surname>Muni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics. Part B: Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="106" to="117" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Basturk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="459" to="471" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Engineering optimizations via nature-inspired virtual bee algorithms</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">lecture notes in computer science</title>
		<imprint>
			<publisher>GmbH</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="317" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Development of novel optimization procedure based on honey bee foraging behavior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sundareswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">T</forename><surname>Sreedevi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International conference on systems, man and cybernetics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1220" to="1225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ant colony optimization based network intrusion feature selection and detection</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3871" to="3875" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A rough set based hybrid method to feature selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Knowledge Acquisition and Modeling</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="585" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modified Grey Wolf Optimizer for Global Engineering Optimization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Computational Intelligence and Soft Computing</title>
		<imprint>
			<biblScope unit="volume">2016</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-objective grey wolf optimizer: A novel algorithm for multi-criterion optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D S</forename><surname>Coelho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Binary Grey Wolf Optimization Approaches for Feature Selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Binary ant lion approaches for feature selection</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2016-07">July 2016</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature subset selection approach by Gray-wolf optimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Zawbaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grosan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Afro-European Conference For Industrial Advancement (AECIA)</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-11-19">17-19 November, 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Grey Wolf Optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Engineering Software</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="46" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The Ant Lion Optimizer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mirjalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Engineering Software</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improved binary PSO for feature selection using gene expression data</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Biology and Chemistry</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">UCI Machine Learning Repository</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Asuncion</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<ptr target="http://featureselection.asu.edu/index.php" />
		<title level="m">Feature selection</title>
		<imprint>
			<date type="published" when="2015-11-25">25 November 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wrappers for feature subset selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="324" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Prey-Predator Algorithm: A New Metaheuristic Algorithm for Optimization Problems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Tilahun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Ong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Technology &amp; Decision Making</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1331" to="1352" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">An Improved Self-adaptive Control Parameter of Differential Evolution for Global Optimization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Theory of Point Estimation</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<pubPlace>New York, Springer</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<publisher>Wiley-Interscience Publication</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Individual Comparisons by Ranking Methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wilcoxon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics Bulletin</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="80" to="83" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
