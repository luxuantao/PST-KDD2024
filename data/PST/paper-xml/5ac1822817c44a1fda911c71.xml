<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Communicating Robot Motion Intent with Augmented Reality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Walker</surname></persName>
							<email>michael.walker-1@colorado.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hooman</forename><surname>Hedayati</surname></persName>
							<email>hooman.hedayati@colorado.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><surname>Lee</surname></persName>
							<email>jennifer.c.lee@colorado.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
							<email>daniel.szafir@colorado.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Colorado</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Communicating Robot Motion Intent with Augmented Reality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9FC61BE65B3369CCBE6D0883A93515C0</idno>
					<idno type="DOI">10.1145/3171221.3171253</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Aerial robots</term>
					<term>robots</term>
					<term>drones</term>
					<term>robot intent</term>
					<term>augmented reality</term>
					<term>mixed reality</term>
					<term>ARHMD</term>
					<term>interface design</term>
					<term>virtuality continuum</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: In this work, we explore how augmented reality might mediate collocated human-robot interactions by visually conveying robot motion intent. We developed four reference prototypes for cuing aerial robot flight motion: (A) NavPoints, (B) Arrows, (C) Gaze, (D) Utilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Effective collaboration requires that teammates quickly and accurately communicate their intentions to build common ground, coordinate joint actions, and plan future activities. For example, prior work in social, cognitive, and behavioral sciences has found that collaborative activities fundamentally depend on interpredictability-the ability of each team member to rapidly understand and predict their teammate's attitudes and actions <ref type="bibr" target="#b22">[23]</ref>. In collocated human-robot teams, poor communication of robot intentions and planned movements can lead to critical breakdowns that degrade safety, task performance, and perceptions of robot usability.</p><p>As a result, providing support for this motion inference problem, where users may have difficulties understanding when, where, and how a robot teammate will move, represents a primary challenge towards achieving safe and usable robotic systems. In human-human teams, people use a variety of implicit and explicit cues, such as gaze, gestures, or other social behaviors, that communicate planned actions and movements to enhance team effectiveness and help maintain trust. Research has demonstrated that robots may also use social cues to convey both movement intent and affective state <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref>. However, it is not always clear how to apply these findings to robots that lack anthropomorphic and zoomorphic features, such as industrial robot manipulators or aerial robotic free-flyers.</p><p>Instead, recent work in HRI has begun to explore alternative methods for supporting motion inference, including generating legible motion trajectories <ref type="bibr" target="#b9">[10]</ref>, developing expressive motion primitives <ref type="bibr" target="#b41">[42]</ref>, verbalizing robot intentions using natural language <ref type="bibr" target="#b45">[46]</ref>, using projector-based or electronic display systems to provide additional information <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b47">48]</ref>, and using light signals as explicit directionality cues <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref>. While such advances have shown promise in enhancing interaction safety and fluidity, a variety of constraints arising from environmental, task, power, computational, and platform considerations may limit their feasibility or effectiveness in certain contexts. For example, altering robot motions for legibility or expressiveness may not always be possible in dynamic or cluttered environments, natural language may not be a practical form of feedback in noisy environments (e.g., manufacturing warehouses or construction sites) or for robotic platforms that generate a great deal of noise (e.g., aerial robots), and projections may be difficult to render on non-flat surfaces, may not be salient in bright environments, and can be occluded by user or robot.</p><p>In this paper, we explore an alternative design space: using augmented reality to resolve motion inference. Our work is inspired by prior research envisioning that augmented reality head-mounted displays (ARHMDs) might one-day support intuitive human-robot communication for collocated users <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>. While past efforts integrating robotics and augmented reality were hampered by limitations in underlying AR technologies (often relying on custombuilt HMDs with limited fidelity), recent industry developments are increasing the availability of standardized, consumer-grade ARHMD technology capable of providing rich, intuitive, visual feedback. These advances are providing an opportunity for an exciting new design space: HRI mediated by hands-free, see-through augmented reality. We provide the first in-depth examination of this design space within the context of communicating robot motion intent to collocated users, specifically for aerial robots.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this work, we build on prior research focused on resolving robot motion inference by exploring a new design space where augmented reality cues communicate robot intent. Below, we review related work on communicating robot intent and provide a brief primer on augmented reality technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Communicating Robot Intent</head><p>The robot motion inference problem may be considered as an analog to the "gulf of evaluation" <ref type="bibr" target="#b28">[29]</ref> issue that commonly arises between representations provided by a system and user abilities to interpret the system <ref type="bibr" target="#b42">[43]</ref>. This can be especially challenging for robots with high degrees of freedom, such as aerial robots. Other issues may compound this problem, including a lack of robot capabilities for communicating intention and goals using traditional methods and technological novelty/lack of mature mental models for understanding robot behaviors.</p><p>Previous research has suggested that effective robot communication enhances user perceptions of the robot's reliability, predictability, and transparency and may increase user willingness to accept and use new robotic technologies in work environments <ref type="bibr" target="#b6">[7]</ref>. Research has also shown that cuing robot intent can help users anticipate and predict robot directional motion faster, enabling them to respond more quickly in interactive tasks <ref type="bibr" target="#b13">[14]</ref> while increasing user preferences for working with robots <ref type="bibr" target="#b41">[42]</ref>. Legible motion that expresses a robot's intentions can further improve interaction fluidity and efficiency in collaboration between humans and robots <ref type="bibr" target="#b9">[10]</ref>.</p><p>Past research in robot design has examined how to effectively leverage users' prior experiences and mental models in humanhuman collaboration to bootstrap human-robot collaboration, imbuing robots with social behaviors such as gaze and gestures that people commonly use. Such behaviors have been explored for a variety of robots using anthropomorphic and zoomorphic features (e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref>). For aerial robots that lack such features, prior work has examined expressive flight patterns, demonstrating that certain behaviors based on biological motion and principles from film and animation may help offset the lack of developed mental models for free-flying movement <ref type="bibr" target="#b41">[42]</ref>. Research has also explored more explicit cues, such as using lights as indicators <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b42">43]</ref> and mixed-reality projection systems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48]</ref>, finding that the use of projected imagery can be advantageous for communicating spatial intentions and instructions such as informing human collaborators of robots' intended path <ref type="bibr" target="#b47">[48]</ref>.</p><p>Although such work has shown promising benefits for improving human-robot interaction, including with aerial robots, prior methods are not without limitations. For example, expressive flight motions may not be feasible in constrained environments or effective if the user simply glances at the robot and witnesses only part of the motion. Projection systems rely on instrumenting the environment and face difficulties at distance, in chaotic environments, and run the risk of occlusion. In this work, we explore AR as a means of conveying robot motion intent in a manner not bound by these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Augmented Reality</head><p>Augmented reality (AR) is a technology that overlays computer graphics onto real world environments in real-time <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. AR interfaces have three main features: (1) users are able to view both real and virtual objects in a combined scene, (2) users receive the impression that virtual objects are actually expressed and embedded directly in the real world, and (3) the virtual objects can be interacted with in real-time <ref type="bibr" target="#b3">[4]</ref>. This contrasts with purely virtual environments or other parts of the virtuality continuum (VC), such as augmented virtuality in which real-world objects are mixed into a virtual environment <ref type="bibr" target="#b24">[25]</ref>.</p><p>AR technologies show promise in supporting human activities across a variety of domains. While early systems, starting with Sutherland's "Sword of Damocles" prototype <ref type="bibr" target="#b38">[39]</ref>, were quite limited in display fidelity, rendering speed, support for interaction, and generalizability, recent advancements in augmented reality headmounted display (ARHMD) technology is creating an ecosystem of standardized, consumer-grade see-through ARHMDs. For example, the HoloLens and Meta 2 ARHMDs both afford high resolution stereographic virtual imagery displayed at a 60Hz refresh rate, builtin gesture tracking, depth sensing, and integration with standard development tools such as Unity and Unreal Engine. This advance in hardware accessibility is creating new opportunities for exploring AR as an interaction medium for enhancing HRI.</p><p>Although we are not the first to recognize that AR holds potential for improving human-robot interactions (see <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35]</ref>), we believe that this area is critically understudied and represents a fairly nascent research space, especially within the context of the capabilities provided by modern ARHMD hardware. We think that AR may be more flexible than past solutions that rely on physicallyembodied cues, flight patterns, or projections and might be easily adapted to a variety of tasks and environments. For example, AR might overcome limitations arising from complex environments by superimposing visuals in a head-mounted display without limiting robot mobility or risking occlusion <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DESIGN PROCESS</head><p>We undertook an iterative design process to explore the space of how ARHMD technology might mediate collocated human-robot interaction by providing feedback on robot motion intent. Our design process began with an analysis of the potential of augmented and mixed reality technology. Synthesizing information from past work across the VC, including mixed-reality projection systems, AR entertainment applications, and augmented virtuality educational software, we developed a high-level framework for considering how ARHMDs might enhance human-robot interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A Design Framework for AR-HRI</head><p>Our framework classifies potential designs for augmenting humanrobot interactions with virtual imagery into three main categories, regarding whether additional information is communicated to the user by (1) augmenting the environment, (2) augmenting the robot, or (3) augmenting the user interface. This framework of broad design concepts allows us to survey the landscape of possible AR interfaces and provides a structure for reasoning about requirements, benefits, and trade-offs in designing AR for HRI. Augmenting the environment: In this paradigm, virtual imagery is represented as new cues directly embedded into the context of the shared work area using an environment-as-canvas metaphor. This notion extends past work using mixed-reality projection systems in three major ways. First, ARHMDs afford a full three-dimensional "canvas" to utilize for virtual imagery, rather than a two-dimensional canvas from projector systems. Second, ARHMD environmental augmentations do not risk being occluded, as when a user or robot interferes with projected light. Third, ARHMDs support stereographic environmental cues that can better leverage human depth perception, as opposed to monocular cues in traditional projector systems. Augmenting the robot: In this archetype, virtual imagery is directly connected to the robot platform to alter robot morphology in a robot-as-canvas metaphor. This technique may alter robot form and/or function by creating new "virtually/physically embodied" cues, where cues that are traditionally generated using physical aspects of the robot are instead generated using indistinguishable virtual imagery. For example, rather than directly modifying a robot platform to include signaling lights, as in <ref type="bibr" target="#b42">[43]</ref>, an ARHMD interface might overlay virtual signaling lights on the robot in an identical manner. Alternatively, virtual imagery might be used to give anthropomorphic or zoomorphic features to robots that don't have this physical capacity (e.g., adding a virtual body to a single manipulator or a virtual head to an aerial robot). Finally, virtual imagery might be used to obscure or make more salient various aspects of robot morphology based on user role (e.g., an override switch might be hidden for normal users but visible for a technician). Overall, we believe this novel paradigm, in which robot morphology becomes a design variable that is fast, easy, and cheap to prototype and manipulate, opens up a great deal of exciting new potential for HRI researchers and designers, especially since modifications to robot morphology are traditionally prohibitive due to cost, time, and/or task/environment constraints. Augmenting the user interface: In this paradigm, virtual imagery is provided directly in front of the user, giving them an interface to the physical world, inspired by "window-on-the-world" AR applications <ref type="bibr" target="#b10">[11]</ref> and heads-up display technologies used for pilots <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24]</ref>. This interface-as-canvas metaphor may uniquely supply egocentric cues, either directly in front of the user's view or in their periphery, Table <ref type="table">1</ref>: Visual summary of features supported by each design. compared to the exocentric feedback provided by augmenting the environment or robot <ref type="bibr" target="#b46">[47]</ref>. For example, user interface augmentations might include spatial mini-maps that provide information on the position or planned route of robots relative to the user, robot status indicators (battery level, task progress, task queue, etc.), or live video streams from a robot's camera(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Creating Reference Prototypes</head><p>Inspired by prior work in communicating robot intent, we used the design framework described above to develop several reference designs that sample from the potentially infinite solution space of AR visualizations for conveying robot intent. Our initial designs ranged from showing dynamic virtual fences that outlined "safe" and "unsafe" areas in the environment, to adding virtual engines to the robot (inspired by <ref type="bibr" target="#b42">[43]</ref>), to various ways of encoding robot motion trajectories, velocities, and accelerations. After conducting series of design iterations based on feedback from short pilot tests, in which users viewed a collocated aerial robot while using a preliminary ARHMD design prototype, we eventually narrowed our candidate prototypes to four main designs.</p><p>We refer to these four candidate designs as NavPoints, Arrows, Gaze, and Utilities (Figure <ref type="figure">1</ref>). Together, they sample from each paradigm in our design framework and offer potential trade-offs in terms of information conveyed, information precision, generalizability, and possibility for distraction/interface overdraw. Table <ref type="table">1</ref> provides a visual summary of features supported by each design.</p><p>While our work examines these designs from the context of conveying robot motion intent for aerial robots, we believe that our design framework and methodology, along with the main design metaphors and interface techniques we develop, may generalize to additional robotic platforms where conveying intent may facilitate human-robot interaction (e.g., industrial robotic manipulators that also move with high degrees-of-freedom). Note the following designs have heuristic parameters (e.g., X , Y , Z) able to be tuned to the specifications of the designer. We provide the values we chose for our specific implementations of these designs in §4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NavPoints:</head><p>The NavPoints design is an example of augmenting the environment. This design has a spatial focus as it provides virtual imagery that displays the robot's planned flight path as a series of X lines and navigation waypoints, similar to what might be seen in traditional waypoint-delegation or supervisory interfaces (e.g., <ref type="bibr" target="#b43">[44]</ref>). The lines connect the robot's current position to its future destinations in sequential order. Destination waypoints are visualized as spheres and indicate the robot's precise destinations in 3D space. Each destination sphere also renders a drop shadow on the ground directly below it, which has been shown to aid in depth estimations made by the user <ref type="bibr" target="#b8">[9]</ref>. Above each navigation point are two radial timers. An inner white timer indicates when the drone will arrive at that location and the outer dark blue timer indicates when the robot will leave that location. Smaller spheres travel along the lines, moving in the same direction and velocity the robot will travel between destinations, thus providing an anticipatory cue for future robot velocity and direction. Information displayed by this design regarding velocity and arrival/departure timings is thus explicitly displayed for the user. Arrow: The Arrow design provides an alternate example of how virtual imagery might augment a shared environment. While the NavPoints design provides users with a great deal of information, it may be distracting or confusing to the user due to potential overdraw. The Arrow design takes a more minimal approach, focusing specifically on communicating temporal information inspired by <ref type="bibr" target="#b7">[8]</ref> and common user experiences with modern GPS systems. The virtual imagery consists of a blue arrow head that travels through 3D space in the exact path the robot will eventually take X seconds in the future. As the arrow moves, a line is left behind that traces the arrow's path back to the robot. Using this line, users can explicitly see the path the arrow has taken, which the robot will eventually follow. The line the arrow creates renders a drop shadow on the ground directly beneath it. Information displayed by this design regarding velocity and arrival/departure timings must be inferred by the user by watching the discrete movements of the arrow through space. Gaze: The Gaze design represents an example of augmenting the robot itself with virtual imagery. This design is inspired by prior research that has demonstrated the remarkable potential of gaze behaviors for conveying intent, even for aerial robots <ref type="bibr" target="#b42">[43]</ref>, as well as prior designs for robotic blimps <ref type="bibr" target="#b18">[19]</ref> and research in robotic telepresence that has explored a metaphor of treating an aerial robot as a "floating head" <ref type="bibr" target="#b19">[20]</ref>. This design provides virtual imagery that completely alters the robot form by overlaying a X -meter diameter white sphere with a pupil directly over the aerial robot, effectively transforming the robot from a multirotor into a "flying eye." While moving between destinations, the eye model stares at its current destination until it enters a predetermined distance threshold of Y meters between itself and the current destination, at which point the eye turns and focuses on the robot's next destination. Gaze shifts such as this have been shown useful in predicting action intent in humans <ref type="bibr" target="#b20">[21]</ref>. Through these shifts in focus, the robot's immediate destinations are preemptively revealed to the user. If the robot is to remain stationary at a destination for longer than Z seconds, the lens over the normally transparent pupil becomes opaque. When the now stationary robot is within Z seconds of departing, the lens fades in back to transparent. This fade in is done as a linear interpolation over the course of Z seconds. This lens fade in/out effect notifies the user how long the robot will remain stationary and was inspired by accommodation-shifts as ciliary muscles contract and relax in human gaze as focus switches between near and far targets-and lens focusing in traditional cameras. The size of the display was chosen to help users more easily determine gaze direction at near and far distances from the robot. The back of the sphere directly behind the pupil is rendered as flat to help users infer the rotation of the eye when not facing the robot head on. Finally, the eye casts a drop shadow on the ground directly below it. Utilities: Our final design illustrates a potential method for augmenting a user interface to provide contextual information in an egocentric manner. This design is inspired by peripheral utilities, such as minimaps, radar, and off-screen indicators that often augment pilot interfaces <ref type="bibr" target="#b23">[24]</ref>, robot control interfaces <ref type="bibr" target="#b27">[28]</ref>, videogame interfaces <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>, and military applications <ref type="bibr" target="#b14">[15]</ref>. This design displays a 2D circular "radar" fixed at the bottom left corner of the ARHMD display. The user is displayed as a blue dot that is always centered within the radar, while the robot is rendered as a red dot on the radar relative to the user's location. The size of the robot's radar dot is directly proportional to its current height. The detection radius, X , of the radar can be customized by the interface designer or adjusted by the user. When the robot is in the user's field-of-view (FOV), it is overlaid with a targeting box, when not in the FOV, an off-screen indicator appears in the form of a directional arrow. This arrow is rendered along the side of the ARHMD display pointing to the location of the off-screen robot. Both radar and targeting box/offscreen indicator provide the user with the means to rapidly locate the robot relative to themselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT</head><p>We conducted a 5 × 1 between-participants experiment to evaluate how our designs affect user interactions with a flying robot in a shared workspace. The independent variable in this study was the type of AR feedback the user received (five levels: a baseline and our four designs described above). In the baseline condition, participants still wore an ARHMD, but did not see any virtual imagery. Instead, participants in this condition were informed that the robot has a distinct "front," which always indicates its direction of flight; this baseline behavior meant the robot would always orient itself to the direction of travel, leveraging the only physically-embodied cue that the robot's default morphology provides (Figure <ref type="figure" target="#fig_1">3</ref>). All conditions shared this baseline orientation behavior. Dependent variables included objective measures of task performance and efficiency as well as subjective ratings of communication clarity and robot usability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task and Environment</head><p>Our overall experimental setup was inspired by contexts where freeflying robots might assist with logistics management in manufacturing settings in the near future <ref type="bibr" target="#b39">[40]</ref>. In the study, participants worked with an aerial robot in a shared environment designed to mimic a small warehouse. The environment measured 20ft × 35ft × 20ft and contained motion tracking cameras that we utilized for precise robot navigation to ensure participant safety.</p><p>Six workstations were placed within the physical space in two rows of three (Figure <ref type="figure" target="#fig_0">2</ref>). Each workstation had at least five feet of surrounding free space and supported a container of colored beads.</p><p>Session We-3: Best Paper Nominees II HRI'18, March 5-8, 2018, Chicago, IL, USA Each bead container held only one color of beads, corresponding to either green, black, yellow, white, blue, or red. Participants were tasked with collecting beads from these containers and fastening them together to make beaded strings while sharing the environment with an aerial robot. Participants were instructed that their goal was to make as many beaded strings as possible in exactly eight minutes. Each completed string consisted of twenty-five beads. There were individual instructions for each string describing the target color and amount of beads to be used. For example, one string might ask for 10 blue beads, 5 red beads, and 10 green beads. Along with these directions, participants were instructed on three additional rules:</p><p>1. Participants could only pick up one bead at a time and could only place beads on strings while at the workstation. 2. Participants could collect the colors in any order, but once they chose a color they had to remain at that color station until they had strung all the beads of that color, as indicated by the string instructions (i.e., colors could not be intermixed).</p><p>3. The robot would occasionally visit each workstation (ostensibly to monitor bead supply). If the robot flew to the workstation where participants were working, participants were required to move at least 2 meters away from the workstation (i.e., moving back to social distance as informed by proxemics <ref type="bibr" target="#b0">[1]</ref>) and wait until the robot left before continuing (i.e., giving workstation priority to the robot).</p><p>This task was designed to emulate an assembly task that might be found in a warehouse, with shared resources (i.e., the workstations) between the user and robot. As the robot had priority at workstations, the task required that participants understand and predict robot intent to best plan their activities and maximize task efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Apparatus &amp; Implementation</head><p>Robotic Platform: We used an AscTec Hummingbird robot as our free-flying platform (Figure <ref type="figure" target="#fig_1">3</ref>). During the experiment, the robot flew autonomously to pre-programmed waypoints throughout the experimental environment using a PID controller that received input on robot location using a motion capture system. During the study, a researcher stood by with an emergency kill switch that could disarm the robot for safety, but this was never required. ARHMD Platform: We used a Microsoft HoloLens as our ARHMD. The HoloLens is a wireless, optical see-through stereographic augmented reality HMD with a 30°× 17.5°FOV, an inertial measurement unit, depth sensor, ambient light sensor, and several cameras and microphones that support voice input, gesture recognition, and head tracking. The HoloLens was chosen due to its emerging popularity, ease of access, ability to support hands-free AR, and high potential as a model for future consumer ARHMD systems. Experimental Framework: We developed a custom experimental framework for implementing our designs, deploying them to the HoloLens, and ensuring that visualizations are properly synchronized with robot behaviors. We first prototyped our four designs described in §3.2 using Unity, a popular game and animation engine for designing and developing virtual and AR applications. We also developed a waypoint system that enables the specification of a sequential list of desired robot destinations (i.e., target robot positions and orientations in 6 degree-of-freedom space), desired velocity to travel to each destination, and wait time at each destination (possibly zero). We then added an invisible virtual drone object to the Unity scene that navigates the scene per the specifications of the waypoint list, and whose motions control the physical robot's flight. For every iteration of the game engine's update loop, we send the virtual drone's 3D pose from the HoloLens to the backend system controlling the physical robot via UDP. Virtual drone pose values are transformed from Unity coordinates to real-world coordinates using a transformation matrix calculated a priori in an initial calibration procedure. After this transformation, pose values in the Unity scene correspond to the same positions in the user's environment, enabling the physical robot to fly through the study space in a manner that matches the virtual drone, turning our Unity application into a waypoint delegation interface as in <ref type="bibr" target="#b43">[44]</ref>. This framework allows us to calculate the exact position of the physical robot relative to the user at any time and adjust our displayed visualizations accordingly.</p><p>For the experiment, the waypoint list specified in Unity consists of destinations corresponding to the six workstations. The waypoints were placed directly above the workstations between 1-1.5 meters high. Upon arriving at a workstation, the robot would pause for 5-15 seconds to simulate monitoring bead inventory. The robot's velocity when traveling between stations ranged from 0.2-1m/s, as suggested by prior research into appropriate robot speeds for collocated interaction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b30">31]</ref>. The waypoint list was populated by randomly selecting workstation destinations, wait times, and velocities from the ranges listed above to ensure there was no flight path pattern for participants to learn. This list was populated once prior to the start of the study as a whole, then held constant for all participants across all conditions to remove potential variance arising from different robot trajectories. The total time to travel through the waypoint list took 8 minutes to complete, which was equivalent to the total task duration (chosen due to battery/flight time capabilities).</p><p>We also tuned the heuristic parameters in implementing the designs in §3.2. NavPoints has the potential to display a full flight path of all waypoints, but early pilot tests revealed this led to a distracting case of overdraw. We instead opted to show only a portion of the robot's path, which dynamically updated such that the visualization always displayed three waypoint destinations ahead of the robot's current position. The Arrow design was implemented such that the blue arrow always stayed fifteen seconds ahead of the aerial robot (i.e., showing the robot's position 15s in the future). The heuristic parameters selected for eye movement in the Gaze design were derived from research into human gaze behaviors <ref type="bibr" target="#b20">[21]</ref> and set to a distance threshold of 2m and fade threshold of 3s. The radius of the eye model was set to 0.6m so that the hologram overlaid the robot's body in its entirety. The "radar" in the Utilities design used a detection radius of 15m to cover the full area of the experiment environment. Final implementations of these designs can be seen in Figure <ref type="figure">1</ref> and in our supplemental video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Participants &amp; Procedure</head><p>We recruited a total of 60 participants (40 males, 20 females, evenly balanced across conditions) from the University of Colorado Boulder campus to take part in our study, which was approved by our university IRB. Average participant age was 20.7 (SD = 4.8), with a range of 18-45. On a seven-point scale, participants reported a moderate prior familiarity with aerial robots (M = 3.75, SD = 1.71), but a low familiarity with ARHMDs (M = 2.65, SD = 1.83).</p><p>The study took approximately 30 minutes and consisted of five phases: (1) introduction, (2) training, (3) calibration, (4) task, and (5) conclusion. (1) First, participants signed a consent form and were led into the task space. (2) Next, participants each read identical instruction sheets detailing the task and task rules. Participants assigned to any of the four AR design conditions described in §3.2 then watched a corresponding 60s tutorial video that provided a brief instruction on the AR feedback they would receive based on the relative novelty of ARHMD technology. Only participants assigned to the baseline condition were told verbally that the robot always moves towards where its marked "front" is facing, as described in §4.</p><p>(3) The ARHMD application was then started, calibrated, and fitted on each participant, including participants assigned to the baseline condition (even though they didn't receive AR feedback). ( <ref type="formula">4</ref>) Participants then performed the main task for eight minutes, making as many bead strings as possible while sharing the environment with a collocated aerial robot, as described in §4.1. ( <ref type="formula">5</ref>) Once eight minutes of task time was completed, participants were told to stop and were given a post-survey on their experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Measures &amp; Analysis</head><p>We used a combination of four objective and subjective measurements to characterize the effects of our designs. Objective task efficiency was measured by the total time participants spent waiting while interrupted by the robot, which could be avoided by planning their work with an understanding of robot intent (lower times indicate better performance/efficiency). In calculating task efficiency, we removed time variance in users backing away from the stations. The interruption timer only began when the robot was directly overhead a station. This allowed for consistent measuring between participants.</p><p>We also constructed a number of scales from 7-point Likert-style questionnaire items to measure subjective participant perceptions and preferences. Scales rated interface design clarity (4 items, Cronbach's α = .85), perceptions of the robot as a teammate, both as a personal work partner (4 items, Cronbach's α = .91), and as a potential work partner for others (2 items, Cronbach's α = .83), and overall design usability (2 items, Cronbach's α = .73).</p><p>Qualitative feedback was obtained through open-ended questions posed to each participant as part of the concluding questionnaire to describe their experiences "working with the AR user interface", "working alongside the aerial robot", and "completing [their] task".</p><p>We analyzed data using a one-way Analysis of Variance (ANOVA) with experimental condition (i.e., interface design) as a fixed effect. Post-hoc tests used Dunnett's method to control for Type I errors when evaluating the ARHMD designs against the baseline condition, while Tukey's Honestly Significant Difference (HSD) test compared effectiveness across each design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4 summarizes our main objective and subjective results.</head><p>Objective Results -We analyzed our task performance metric to confirm that our designs were useful for participants to quickly and accurately deduce robot intent and plan their own activities more effectively. We found a significant main effect of ARHMD interface design on total time spent interrupted, F(4, 55) = 12.56, p &lt; .001. Comparing the performance of each design to the baseline with Dunnett's multiple comparison test, we found that total time lost to interruptions significantly decreased using NavPoints (p &lt; .001), Arrow (p &lt; .001), and Gaze (p = .003), but not Utilities (p = .104).</p><p>Subjective Results -Participants rated the several facets regarding the communication of robot movement intent. We found a significant effect of design on perceived communication clarity, F(4, 55) = 11.04, p &lt; .001. Post-hoc comparisons using Dunnett's test revealed that the NavPoints design was rated significantly higher than the baseline (p &lt; .001), but we did not find significant effects from the other designs.</p><p>We analyzed participant responses to the robot in terms of how they might view it as a collaborative partner in a work environment. We found a marginal (0.1 &gt; p &gt; 0.05) main effect of design on participant perceptions of the robot as a good work partner for themselves, F(4, 55) = 2.48, p = .054. We also found a significant main effect of design on participant perceptions of the robot as a good work partner for others, F(4, 55) = 2.54, p = .049. Post-hoc comparisons revealed NavPoints was the only design to significantly improve perceptions of the robot as a personal work partner (p = .03) and as a work partner for others (p = .029) over the baseline.</p><p>Finally, we compared the designs to one another along a usability metric of how the displayed virtual imagery affected participant understanding of robot movement intent. We found a significant main effect of design on perceived usability for understanding intent, F(3,44) = 25.32, p &lt; .001. Post-hoc comparisons using Tukey's HSD found that NavPoints (M = 6.96), p &lt; .001, Arrow (M = 6.67), p &lt; .001, and Gaze (M = 5.83), p &lt; .001, were ranked as significantly more helpful than Utilities (M = 4.21). We also found that NavPoints was rated as significantly more helpful than Gaze, p = .012, with Arrow ranked marginally more helpful than Gaze, p = .092.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>The NavPoints, Arrow, and Gaze designs improved task performance by reducing inefficiency; participants were able to better predict robot intent and plan their own actions accordingly to reduce the length of times they spent interrupted and unproductive. However, the Utilities model did not provide similar improvements over the baseline condition. This may be due to the Utilities design emphasizing current robot positioning relative to the user rather than displaying cues that help users predict the robot's future destinations, unlike the other designs. Participant responses support this conclusion, revealing similarities between baseline and Utilities participants: P16 [Utilities]: "I wasn't able to predict when the drone would move or if there was a pattern. But when it did start moving, I could figure out where it was going and could plan my actions accordingly." P02 [Baseline]: "It was difficult to tell where the drone was going go and for how long it would stay there. . . often times you had no way of knowing what it was going to do."</p><p>Another possible reason for the poorer performance of the Utilities design could be the small scale of the task. In the experimental scenario, there was only a single robot. This allowed participants to simply always face or listen for the robot while working at the stations and navigating the environment. P25 [Utilities]: "It was just easier to look up for the drone when it sounded like it was close/moving." P38 [Utilities]: "I probably used my ears more than the interface. It was helpful and intuitive but it was simpler to listen for the drone." P44 [Utilities]: "I didn't feel the need to use the radar or arrow much because I always faced where the drone was."</p><p>If the space was shared by more than a single robot, it is unlikely that participants would be able to track all robots simultaneously with only sight and sound. We speculate that Utilities design might scale well in this case, providing unobtrusive support for tracking all proximal robots potentially even better than some of our other designs. However, for the single robot in this experiment, Navpoints, Arrow, and Gaze all performed significantly better than the baseline in terms of decreasing inefficiencies. Participants often noted that explicit visualizations of robot movements made the task easier: P13 [NavPoints]: "I found the augmented reality helpful in being able to predict where the robot would move and it made me feel more comfortable with completing the task alongside the robot." P33 [NavPoints]: "It was very helpful while completing the task. I could see clearly where the robot was going to go." P17 [Arrow]: "Once I saw the next two stations the robot was going to, it was easy to work alongside the machine and focus on my work." P18 [Gaze]: "The eyeball design was intuitive and straightforward. It made predicting the drone's movements possible, which made it easier to complete the task."</p><p>Although each of these designs improved objective task performance, only the NavPoints design was consistently rated highly across all subjective scales. Many participants noted that explicitly encoding the robot's arrivals and departures was helpful: P47 [NavPoints]: "The circles that indicated how long the robot was going to stay at each station were very useful." P50 [NavPoints]: "The lines and station timers were very helpful."</p><p>However, one aspect of the NavPoints model that was not wellreceived by participants was the movement speed indicators. Users deemed them to be largely ineffective: P13 [NavPoints]: "It was easy to tell where the robot was heading and approximately how long it would be there. It was more difficult to determine how fast the robot would move from station to station." P50 [NavPoints]: ". . . the dots showing speed were not as helpful."</p><p>Although the Arrow design provides cues that allow users to infer when the robot would leave a location, participants would have preferred this information be communicated more explicitly. Using the Arrow design, if users wished to know when the robot would leave a station, they had to watch the tip of the arrow indicator depart the station and keep a mental count of how many seconds had passed to know when the robot would also leave that location. This proved to be a mental burden for users: P29 [Arrow]: "The arrows gave clear indication about where the robot would fly and how quickly it would leave. At times, I wished for clearer indication of when it would be moving once more." P32 [Arrow]: "Intuitive but needed improvement/more detail about robot movement, such as an indicator of speed and movement pauses."</p><p>The fading opaque/transparent lens in the Gaze design explicitly communicated this information showing users when the robot would leave a location, but it only provided a cue regarding the robot's immediate next destination. Users found this information too limited: P07 [Gaze]: "It was tough to plan two steps ahead of the robot but at least I always knew where its next step was going to be."  Additionally, one participant had a concern with the appearance of the virtual eye, highlighting the need for careful consideration when altering robot appearance with AR:</p><p>P54 [Gaze]: "I had a pretty good idea of what direction the robot was going to go, but the design made me very nervous (giant floating eyeballs bring killer robots to mind). This probably caused me to be far more hesitant to move towards stations."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Limitations and Future Work</head><p>While our approach highlights the potential utility of using ARHMD interfaces to convey robot intent, it is not without practical and theoretical limitations. Our current experimental framework relies on a motion tracking system for precise robot localization and navigation, meaning we must translate designs into the motion tracking coordinate space. Future work might merge our approach with developments in robotic perception and simultaneous localization and mapping (SLAM) to remove this constraint, with an ARHMD receiving information directly from the robot regarding its localized position. Other practical limitations may arise from our use of the HoloLens apparatus. While the HoloLens offers significant benefits over prior ARHMDs and may serve as a model for future consumer-grade devices, participants found the headset to be generally uncomfortable with a narrow field of view (FOV).</p><p>In addition, the generalizability of our results may be limited due to choices in our experimental design, task, and measures. For instance, our choice of a between-subjects experimental design carries with it inherent trade-offs compared with other approaches, such as a within-subjects design. While a within design would have increased our statistical power, we were concerned it may have introduced detrimental transfer effects, with repeated trials increasing participants' familiarity and comfort with the task and robot while leading to them learn robot behaviors (e.g., orientation signaling movement). Also, several practical concerns made a between-subjects design more feasible (e.g., reducing participant fatigue, robot battery life issues, etc.). Future work might increase participant sample size and/or explore a within-participants design, or employ a different type of task. Our task design had an emphasis on participant efficiency, rather than pure productivity. We observed in our pilot tests that the task had highly variable completion rates, owing to varying degrees of participant manual dexterity; therefore, we did not analyze the total the number of bead strings completed. Future work might examine pure productivity or measure objective trade-offs between productivity, efficiency, and accuracy. For our subjective measures, we chose to construct custom scales rather than utilize established usability metrics (e.g., the SUS). This choice was motivated by our concern that prior metrics, which have been constructed based on user interaction with traditional computer systems (e.g., GUIs), may not transfer well to novel interactions with flying robotics and AR. Although our scales may be more applicable to the technologies used in this study, more work is needed to compare them with established metrics.</p><p>Finally, our designs represent only a subset of all possible design choices using ARHMDs. Our goal is not to claim our designs as optimal, but rather to show the value and potential of the AR-HRI design space and encourage further explorations. Although our designs demonstrated objective and subjective improvements, future work might further explore the concepts we have proposed, especially their generalizability in more chaotic, cluttered, or separated spaces. Alternatively, follow-up studies might explore the scalability of our designs in supporting larger team interactions, with multiple robots and/or multiple people. Future work could also explore gender effects for previous studies have found females to be more responsive to non-verbal cues, including body language in robots <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this work, we explore the design of AR interfaces for conveying robot motion intentions. We propose a framework for structuring the design of AR interfaces that support HRI and use this framework to develop four ARHMD interfaces, which provide various forms of visual feedback for communicating motion intent. We conducted a 60-participant user study comparing these designs against a baseline lacking AR feedback. We found that several of our designs significantly improved user understandings of robot intent and increased objective task efficiency. We also found several trade-offs in our designs, with overall user preferences for more explicit information, especially regarding flight timings. Although wearing an ARHMD may not always be desired or feasible in all scenarios, our research may help inform future studies by demonstrating the promise of AR technologies for mediating human-robot interactions and showcasing the design of novel interface techniques that provide intuitive, visual cues to help users better understand robot intent and as a result improve task efficiency and robot usability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ACKNOWLEDGMENT</head><p>This work was supported by an Early Career Faculty grant from NASA's Space Technology Research Grants Program under award NNX16AR58G. We thank Andrew Gorovoy for help with this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The experimental environment required that participants share a workspace with a collocated arial robot.</figDesc><graphic coords="5,53.80,83.68,240.25,237.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Participant making a bead string at one of the six assembly stations mid-task. The AscTec Hummingbird robot flies nearby (colored corner marks robot "front" for baseline participants).</figDesc><graphic coords="6,53.80,83.69,240.23,159.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Session We- 3 :</head><label>3</label><figDesc>Best Paper Nominees II HRI'18, March 5-8, 2018, Chicago, IL, USA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Objective results show that the NavPoints, Arrows, and Gaze designs improved task performance by decreasing inefficiencies and wasted time. Subjective results reveal that NavPoints outperformed other designs in terms of user preferences and perceptions of the robot.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName><surname>John R Aiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Spatial Behavior. Handbook of Environmental Psychology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="389" to="504" />
			<date type="published" when="1987">1987. 1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Projecting Robot Intentions into Human Environments</title>
		<author>
			<persName><forename type="first">Ole</forename><surname>Rasmus S Andersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heni</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amor</forename><surname>Ben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>RO-MAN&apos;16</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-like Motion of a Humanoid Robot Arm Based on a Closed-Form Solution of the Inverse Kinematics Problem</title>
		<author>
			<persName><forename type="first">Tamim</forename><surname>Asfour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rüdiger</forename><surname>Dillmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;03)</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1407" to="1412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent Advances in Augmented Reality</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Azuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yohan</forename><surname>Baillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhold</forename><surname>Behringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blair</forename><surname>Macintyre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer Graphics and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="34" to="47" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing Human Understanding of a Mobile Robot&apos;s State and Actions using Expressive Lights</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Baraka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Veloso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="652" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Psychological Effects of Behavior Patterns of a Mobile Personal Robot</title>
		<author>
			<persName><forename type="first">John</forename><surname>Travis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Butler</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Arvin</forename><surname>Agah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="185" to="202" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">That&apos;s on my Mind! Robot to Human Intention Communication through On-board Projection on Shared Floor Space</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Teja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chadalavada</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Andreasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Krug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achim</forename><forename type="middle">J</forename><surname>Lilienthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Mobile Robots (ECMR&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">World Embedded Interfaces for Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngkwan</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Payton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Hawaii International Conference on System Sciences (HICSS&apos;03)</title>
		<meeting>the Annual Hawaii International Conference on System Sciences (HICSS&apos;03)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Designing for Depth Perceptions in Augmented Reality</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">Albers</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR&apos;17</title>
		<meeting>the 2017 IEEE International Symposium on Mixed and Augmented Reality (ISMAR&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Legibility and Predictability of Robot Motion</title>
		<author>
			<persName><forename type="first">Kenton Ct</forename><surname>Anca D Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human-Robot Interaction (HRI), 2013 8th ACM/IEEE International Conference</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Windows on the World: 2D Windows for 3D Augmented Reality</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blair</forename><surname>Macintyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Haupt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliot</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on User Interface Software and Technology (UIST&apos;93)</title>
		<meeting>the ACM Symposium on User Interface Software and Technology (UIST&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Survey of Socially Interactive Robots</title>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illah</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kerstin</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="3" to="4" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Situation Awareness in an Augmented Reality Cockpit: Design, Viewpoints and Cognitive Glue</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">C</forename><surname>Foyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Hooey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Becky</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Human Computer Interaction</title>
		<meeting>the 11th International Conference on Human Computer Interaction</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="3" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating Anticipation in Robot Motion</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Gielniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Thomaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<publisher>RO-MAN</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="449" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond Line-of-Sight Information Dissemination for Force Protection</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Gillen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Loyall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Hanlon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Scally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Sterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Newkirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Kohler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Military Communications Conference (MILCOM&apos;12)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Human-Robot Collaboration: A Literature Review and Reality Approach in Design</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Scott A Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Chase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Evaluating the Augmented Reality Human-Robot Collaboration System</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scott A Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqi</forename><surname>Chase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Billinghurst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems Technologies and Applications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring the affect of abstract motion in social human-robot interaction</title>
		<author>
			<persName><forename type="first">John</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Sharlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN&apos;11)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Floating Eyeball Drone is Delightfully Dystopian</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Hart</surname></persName>
		</author>
		<ptr target="http://nerdist.com/floating-eyeball-drone-is-delightfully-dystopian/" />
		<imprint>
			<date type="published" when="2016">2017. 2017. March-2016</date>
			<biblScope unit="volume">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Flying Head: A Headsynchronization Mechanism for Flying Telepresence</title>
		<author>
			<persName><forename type="first">Keita</forename><surname>Higuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuya</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Rekimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Reality and Telexistence</title>
		<imprint>
			<publisher>ICAT</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="28" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using Gaze Patterns to Predict Task Intent in Collaboration</title>
		<author>
			<persName><forename type="first">Chien-Ming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Andrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allison</forename><surname>Sauppé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing Laser Gesture Interface for Robot Control. IFIP TC</title>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengdong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiko</forename><surname>Inami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Igarashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michita</forename><surname>Imai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Human-Computer Interaction (INTERACT&apos;09</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Common Ground and Coordination in Joint Activity</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Feltovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Woods</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Organizational Simulation</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="139" to="184" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Review and Analysis of Avionic Helmet-Mounted Displays</title>
		<author>
			<persName><forename type="first">Hua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemeng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanxiong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Engineering</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="110901" to="110901" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Taxonomy of Mixed Reality Visual Displays</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumio</forename><surname>Kishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE Transactions on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="1321" to="1329" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Applications of Augmented Reality for Human-Robot Communication</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Drascic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Grodski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;93)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;93)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1467" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A Methodological Approach Relating the Classification of Gesture to Identification of Human Intent in the Context of Human-Robot Interaction</title>
		<author>
			<persName><forename type="first">Kerstin</forename><surname>Chrystopher L Nehaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Dautenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kubacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Haegele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachid</forename><surname>Parlitz</surname></persName>
		</author>
		<author>
			<persName><surname>Alami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="371" to="377" />
		</imprint>
	</monogr>
	<note>RO-MAN&apos;05</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ecological Interfaces for Improving Mobile Robot Teleoperation</title>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">A</forename><surname>Curtis W Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName><surname>Ricks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="927" to="941" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName><surname>Donald A Norman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive engineering. User Centered System Design</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mar-cps: Measurable Augmented Reality for Prototyping Cyber-physical Systems</title>
		<author>
			<persName><forename type="first">Shayegan</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali-Akbar</forename><surname>Agha-Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kemal Ure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Vian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Surati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AlAA Infotech Aerospace Conference</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Human-Robot Embodied Interaction in Hallway Settings: A Pilot User Study</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pacchierotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><forename type="middle">I</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patric</forename><surname>Jensfelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN&apos;05</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Video Game-based Framework for Analyzing Human-Robot Interaction: Characterizing Interface Design in Real-time Interactive Multimedia Applications</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Richer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jill</forename><forename type="middle">L</forename><surname>Drury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;06)</title>
		<meeting>the ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;06)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Knowledge of Nonverbal Cues, Gender, and Nonverbal Decoding Accuracy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rosip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Nonverbal Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="267" to="286" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
	<note>Issue 4</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Videogame Interface: Artefacts and Tropes</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">W</forename><surname>Ruch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Videogame Cultures and the Future of Interactive Entertainment Global Conference</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Third Point of View Augmented Reality for Robot Intentions Visualization</title>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Ruffaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Brizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Tecchia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandro</forename><surname>Bacinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Augmented Reality</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic Analysis of Affective Postures and Body Motion to Detect Engagement with a Game Companion</title>
		<author>
			<persName><forename type="first">Jyotirmay</forename><surname>Sanghvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ginevra</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iolanda</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Mcowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Paiva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="305" to="311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Human-Robot Interface using an Interactive Hand Pointer that Projects a Mark in the Real Work Space</title>
		<author>
			<persName><forename type="first">Shin</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeyuki</forename><surname>Sakane</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA&apos;00)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A Multi-view Camera-projector System for Object Detection and Robot-human Feedback</title>
		<author>
			<persName><forename type="first">Jinglin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Gans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3382" to="3388" />
		</imprint>
	</monogr>
	<note>ICRA&apos;13</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Ivan</forename><forename type="middle">E</forename><surname>Sutherland</surname></persName>
		</author>
		<title level="m">The Ultimate Display. Multimedia: From Wagner to Virtual Reality</title>
		<imprint>
			<date type="published" when="1965">1965. 1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<title level="m">Human Interaction with Assistive Free-Flying Robots</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
		<respStmt>
			<orgName>The University of Wisconsin-Madison</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pay attention!: Designing Adaptive Agents that Monitor and Improve User Engagement</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI&apos;12)</title>
		<meeting>the SIGCHI Conference on Human Factors in Computing Systems (CHI&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Communication of Intent in Assistive Free Flyers</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;14</title>
		<meeting>the ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="358" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Communicating Directionality in Flying Robots</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;15)</title>
		<meeting>the ACM/IEEE International Conference on Human-Robot Interaction (HRI&apos;15)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Designing Planning and Control Interfaces to Support User Collaboration with Flying Robots</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Szafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilge</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="514" to="542" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Expressing Thought: Improving Robot Readability with Animation Principles</title>
		<author>
			<persName><forename type="first">Leila</forename><surname>Takayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wendy</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Human-Robot Interaction</title>
		<meeting>the 6th International Conference on Human-Robot Interaction</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="69" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Asking for Help Using Inverse Semantics</title>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS&apos;14)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enabling Effective Human-Robot Interaction using Perspective-taking in Robots</title>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">L</forename><surname>Gregory Trafton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><forename type="middle">D</forename><surname>Cassimatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">P</forename><surname>Bugajska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farilee</forename><forename type="middle">E</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="460" to="470" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Communicating Robotic Navigational Intentions</title>
		<author>
			<persName><forename type="first">Atsushi</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetsushi</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoichi</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhiko</forename><surname>Shinozawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takahiro</forename><surname>Miyashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norihiro</forename><surname>Hagita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS&apos;15</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5763" to="5769" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
