<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sentiment Analysis for Software Engineering: How Far Can We Go?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Università della Svizzera italiana</orgName>
								<address>
									<country>USI) Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fiorella</forename><surname>Zampetti</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Sannio</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Bavota</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Software Institute</orgName>
								<orgName type="institution">Università della Svizzera italiana</orgName>
								<address>
									<country>USI) Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Massimiliano</forename><forename type="middle">Di</forename><surname>Penta</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Engineering University of Sannio Italy Michele Lanza Software Institute</orgName>
								<orgName type="institution">Università della Svizzera italiana</orgName>
								<address>
									<country>USI) Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rocco</forename><surname>Oliveto</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">STAKE Lab University of Molise</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sentiment Analysis for Software Engineering: How Far Can We Go?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">777D1E2391FC51A842CF871F71D0BA6A</idno>
					<idno type="DOI">10.1145/3180155.3180195</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>sentiment analysis</term>
					<term>software engineering</term>
					<term>NLP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context.</p><p>We describe our experience in building a software library recommender exploiting developers' opinions mined from Stack Overflow. To reach our goal, we retrained-on a set of 40k manually labeled sentences/words extracted from Stack Overflow-a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort-and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of commonly used tools to identify the sentiment of SE related texts. Meanwhile, we also studied the impact of different datasets on tool performance. Our results should warn the research community about the strong limitations of current sentiment analysis tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Sentiment analysis;</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recent years have seen the rise of techniques and tools to automatically mine opinions from online sources <ref type="bibr" target="#b25">[26]</ref>. The main application of these techniques is the identification of the mood and feelings expressed in textual reviews by customers (e.g., to summarize the viewers' judgment of a movie <ref type="bibr" target="#b31">[32]</ref>). Sentiment analysis <ref type="bibr" target="#b25">[26]</ref> is a frequently used opinion mining technique. Its goal is to identify affective states and subjective opinions reported in sentences. In its basic usage scenario, sentiment analysis is used to classify customers' written opinions as negative, neutral, or positive.</p><p>The software engineering (SE) community has adopted sentiment analysis tools for various purposes. It has been used to assess the polarity of apps' reviews (e.g., Goul et al. <ref type="bibr" target="#b5">[6]</ref> and Panichella et al. <ref type="bibr" target="#b26">[27]</ref>), and to identify sentences expressing negative opinions about APIs <ref type="bibr" target="#b37">[38]</ref>. Tourani et al. <ref type="bibr" target="#b34">[35]</ref> used sentiment analysis to identify distress or happiness in a development team, while Garcia et al. <ref type="bibr" target="#b4">[5]</ref> found that developers expressing strong positive or negative emotions in issue trackers are more likely to become inactive in the open source projects they contribute. Ortu et al. <ref type="bibr" target="#b23">[24]</ref> studied the impact of sentiment expressed in issues' comments and the issue resolution time, while Sinha et al. <ref type="bibr" target="#b30">[31]</ref> investigated the sentiment of developers' commits.</p><p>Most prior works leverage sentiment analysis tools not designed to work on software-related textual documents. This "out-of-thebox" usage has been criticized due to the poor accuracy these tools achieved when applied in a context different from the one for which they have been designed and/or trained <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>. For example, the Stanford CoreNLP <ref type="bibr" target="#b31">[32]</ref> opinion miner has been trained on movie reviews. In essence, the silver bullet to make sentiment analysis successful when applied on software engineering datasets might be their customization to the specific context.</p><p>Thus, the recent trend is to customize existing sentiment analysis tools to properly work on software engineering datasets <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36]</ref>. The most widely used tool in the SE community is SentiStrength <ref type="bibr" target="#b33">[34]</ref>. SentiStrength assesses the sentiment of a sentence by looking at the single words the sentence is composed of, that is, it assigns positive/negative scores to the words and then sums up these scores to obtain an overall sentiment for the sentence. SentiStrength can be customized to provide the sentiment for domain-specific terms. For instance, Islam and Zibran <ref type="bibr" target="#b14">[15]</ref> developed SentiStrength -SE, which improved identification performance for SE-related texts.</p><p>Inspired by these works, we started a research project to design and implement an approach to recommend software libraries to developers. The idea was to assess the quality of software libraries exploiting crowdsourced knowledge by mining developers' opinions on Stack Overflow. One key component needed to succeed was a reliable sentiment analysis tool (e.g., to capture positive/negative developers' opinions about the usability of a library). Given the warning raised by previous work in our field <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> there was the need for training and customizing the sentiment analysis tool to the Stack Overflow context. Also, looking at the opinion mining literature, we decided to adopt a state-of-the-art approach based on a Recursive Neural Network (RNN), able to compute the sentiment of a sentence not by just summing up the sentiment of positive/negative terms, but by grammatically analyzing the way words compose the meaning of a sentence <ref type="bibr" target="#b31">[32]</ref>.</p><p>We built a training set by manually assigning a sentiment score to a total of ∼40k sentences/words extracted from Stack Overflow. Despite the considerable manual effort, the empirical evaluation we performed led to negative results, with unacceptable accuracy levels in classifying positive/negative opinions. Given this, we started a thorough empirical investigation aimed at assessing the actual performance of sentiment analysis tools when applied on software engineering datasets with the goal of identifying a technique able to provide acceptable results. We experimented with all major techniques used in our community, by using them out-of-the-box as well as with customization designed to work in the software engineering context (e.g., SentiStrength -SE <ref type="bibr" target="#b14">[15]</ref>). Also, we considered three different software engineering datasets: (i) our manually built dataset of Stack Overflow sentences, (ii) comments left on issue trackers <ref type="bibr" target="#b24">[25]</ref>, and (iii) reviews of mobile apps <ref type="bibr" target="#b36">[37]</ref>.</p><p>Our results show that none of the state-of-the-art tools provides a precise and reliable assessment of the sentiments expressed in the manually labeled Stack Overflow dataset we built (e.g., all the approaches achieve recall and precision lower than 40% on negative sentences). Results are marginally better in the app reviews and in the issue tracker datasets, which however represent simpler usage scenarios for sentiment analysis tools.</p><p>The goal of our paper is to share with the SE research community our negative findings, showing the current difficulties in applying sentiment analysis tools to software-related datasets, despite major efforts in tailoring them to the context of interest. Our results should also warn researchers to not simply use a (customized) sentiment analysis tool assuming that it provides a reliable assessment of the sentiments expressed in sentences, but to carefully evaluate its performance. Finally, we share our large training dataset as well as all the tools used in our experiments and the achieved results <ref type="bibr" target="#b17">[18]</ref>, to foster replications and advances in this novel field.</p><p>Structure of the paper. Section 2 presents the available sentiment analysis tools, and discusses sentiment analysis applications and studies in SE. Section 3 presents our original research plan. Section 4 reports and discusses the negative results we obtained when evaluating the sentiment analysis component of our approach. Section 5 reports the design and results of the study we performed to assess the performance of sentiment analysis tools on software engineering datasets, while Section 6 discusses the threats that could affect the validity of our results. Finally, after a discussion of lessons learned (Section 7), Section 8 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We start by providing an overview of existing sentiment analysis tools and discuss the applications of these tools in the software engineering domain. Then, we present recent studies questioning the effectiveness of sentiment analysis when applied on SE-related datasets. Table <ref type="table" target="#tab_0">1</ref> reports a summary of the main sentiment analysis tools used in software engineering application to date. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentiment Analysis Tools</head><p>There are several sentiment analysis tools available. Some of them are commercial tools, such as MeaningCloud<ref type="foot" target="#foot_0">1</ref> , GetSentiment<ref type="foot" target="#foot_1">2</ref> , or WatsonNaturalLanguageUnderstanding<ref type="foot" target="#foot_2">3</ref> . There are also sentiment analysis libraries available in popular machine learning tools, such as RapidMiner<ref type="foot" target="#foot_3">4</ref> or Weka <ref type="bibr" target="#b11">[12]</ref>, as well as SentiWordNet <ref type="bibr" target="#b0">[1]</ref> an extension of a popular lexicon database (WordNet <ref type="bibr" target="#b19">[20]</ref>) for sentiment analysis. The sentiment analysis tools applied to software engineering applications are:</p><p>SentiStrength <ref type="bibr" target="#b33">[34]</ref> is the most adopted one, originally trained on MySpace<ref type="foot" target="#foot_4">5</ref> comments. The core of SentiStrength is based on the sentiment word strength list, a collection of 298 positive and 465 negative terms with an associated positive/negative strength value. It also leverages a spelling correction algorithm as well as other word lists such as a booster word list and a negating word list, for a better sentiment assessment. SentiStrength assigns a sentiment score to each word composing a sentence under analysis, and derives the sentence sentiment by summing up the individual scores. The simple approach behind SentiStrength makes it easy to customize for a specific context by defining a list of domain-specific terms with associated sentiment scores. Despite this, only Islam and Zibran <ref type="bibr" target="#b14">[15]</ref> adopted a customized version in software engineering. NLTK <ref type="bibr" target="#b13">[14]</ref> is a lexicon and rule-based sentiment analysis tool having VADER (Valence Aware Dictionary and sEntiment Reasoner) at its core. VADER is specifically tuned to social media texts by incorporating a "gold-standard" sentiment lexicon extracted from microblog-like contexts and manually validated by multiple independent human judges. Stanford CoreNLP <ref type="bibr" target="#b31">[32]</ref> is built on top of a Recursive Neural Network, which differs from SentiStrength and NLTK thanks to its ability to compute the sentiment of a sentence based on how words compose the meaning of the sentence, and not by summing up the sentiment of individual words. It has been trained on movie reviews.</p><p>EmoTxt <ref type="bibr" target="#b1">[2]</ref>. This is a toolkit for emotion recognition from text that combines a n-gram approach proposed by Ortu et al. <ref type="bibr" target="#b23">[24]</ref> with lexical features conveying emotions in the input text: emotion lexicon, politeness, positive and negative sentiment scores (computed by using SentiStrength) and uncertainty. The novelty of EmoTxt relies on the recognition of specific emotions, such as joy, love, and anger. The tool has been preliminary evaluated on two datasets mined from Stack Overflow and JIRA <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sentiment Analysis &amp; Software Engineering</head><p>Sentiment analysis has been applied on different software engineering artifacts, such as technical artifacts (e.g., issues and commit messages) and crowd-generated content (e.g., forum messages and users' reviews), and to support different tasks.</p><p>Sentiment is commonly expressed in the developer-written commit messages and issues <ref type="bibr" target="#b16">[17]</ref>. Guzman et al. <ref type="bibr" target="#b8">[9]</ref> analyzed the sentiment of commit comments in GitHub and provided evidence that projects having more distributed teams tend to have a higher positive polarity in their emotional content. Additionally, the authors found that those comments written on Mondays tend to express more negative emotions. A similar study was conducted by Sinha et al. <ref type="bibr" target="#b30">[31]</ref> on 28,466 projects within a seven year time frame. The results indicated that a majority of the sentiment was neutral and that Tuesdays seem to have the most negative sentiment overall. Also, the authors found a strong positive correlation between the number of files changed and the sentiment expressed by the commits the files were part of. Ortu et al. <ref type="bibr" target="#b23">[24]</ref> analyzed the correlation between the sentiment in 560k JIRA comments and the time to fix a JIRA issue finding that positive sentiment expressed in the issue description might help issue fixing time. Finally, Souza and Silva <ref type="bibr" target="#b32">[33]</ref> analyzed the relation between developers' sentiment and builds performed by continuous integration servers. They found that negative sentiment both affects and is affected by the result of the build process.</p><p>Analyzing the polarity of apps' reviews is particularly useful to support the evolution of mobile applications <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27]</ref>. Goul et al. <ref type="bibr" target="#b5">[6]</ref> applied a sentiment analysis tool suite to over 5,000 reviews observing that sentiment analysis can address current bottlenecks to requirements engineering, but that certain types of reviews tend to elude algorithmic analysis. Carreño et al. <ref type="bibr" target="#b2">[3]</ref> presented a technique based on Aspect and Sentiment Unification Model (ASUM) to extract common topics from apps' reviews and present users' opinions about those topics. Guzman et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11]</ref> proposed the use of SentiStrength to support a similar task. Panichella et al. <ref type="bibr" target="#b26">[27]</ref> used a Naive Bayes classifier to assign each sentence in users' reviews to a "sentiment class" among negative, neutral, and positive. This is one of the features they use to classify reviews on the basis of the information they bring (e.g., feature request, problem discovery, etc.). Sentiment analysis has also been applied to classify tweets related to software projects <ref type="bibr" target="#b6">[7]</ref>. The results of their empirical study indicated that searching for relevant information is challenging even if this relevant information can provide valuable input for software companies and support the continuous evolution of the applications discussed in these tweets.</p><p>As emotions can impact the developer productivity, task completion quality, and job satisfaction, sentiment analysis has also been used to detect the psychological state of developers <ref type="bibr" target="#b29">[30]</ref>. Guzman and Bruegge <ref type="bibr" target="#b9">[10]</ref> used sentiment analysis to investigate the role of emotional awareness in development teams, while Gachechiladze et al. <ref type="bibr" target="#b3">[4]</ref> used sentiment analysis to build a fine-grained model for anger detection. In addition, the study by Pletea et al. <ref type="bibr" target="#b27">[28]</ref> provided evidence that developers tend to be more negative when discussing security-related topics. Finally, Garcia et al. <ref type="bibr" target="#b4">[5]</ref> analyzed the relation between the emotions and the activity of contributors in the Open Source Software project GENTOO. They found that contributors are more likely to become inactive when they express strong positive or negative emotions in the issue tracker, or when they deviate from the expected value of emotions in the mailing list.</p><p>Sentiment expressed on Q&amp;A sites such as Stack Overflow is also leveraged by researchers to recommend comments on quality, deficiencies or scopes for further improvement for source code <ref type="bibr" target="#b28">[29]</ref> or to identify problematic API design features <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Assessment of Sentiment Analysis Tools in Software Engineering Contexts</head><p>While the authors of the above works presented an extensive evaluation of the relationship between sentiment and other factors, no analysis is reported for what concerns the accuracy of the sentiment classification. Indeed, unsatisfactory results have been reported by researchers when using these sentiment analysis tools to analyze texts under software engineering contexts. Tourani et al. <ref type="bibr" target="#b34">[35]</ref> used SentiStrength to extract sentiment information from user and developer mailing lists of two major successful and mature projects from Apache software foundation: Tomcat and Ant. However, they found SentiStrength achieved a very low precision, i.e., 29.56% for positive sentences and 13.1% for negative sentences. The low precision is caused by the ambiguous technical terms and the difficulty of distinguishing extreme positive/negative texts from neutral ones. Novielli et al. <ref type="bibr" target="#b22">[23]</ref> highlighted and discussed the challenges of employing sentiment analysis techniques to assess the affective load of text containing technical lexicon, as typical in the social programmer ecosystem.</p><p>Jongeling et al. <ref type="bibr" target="#b15">[16]</ref> conducted a comparison of four widely used sentiment analysis tools: SentiStrength, NLTK, Stanford CoreNLP, and AlchemyAPI. They evaluated their performance on a human labeled golden set from a developer emotions study by Murgia et al. <ref type="bibr" target="#b20">[21]</ref> and found none of them can provide accurate predictions of expressed sentiment in the software engineering domain. They also observed that disagreement exists not only between sentiment analysis tools and the developers, but also between different sentiment analysis tools themselves. Their further experiment also confirmed that disagreement between these tools can result in contradictory results when using them to conduct software engineering studies.</p><p>The results achieved in these studies call for a sentiment analysis technique curated with software engineering related data to address the problem of low accuracy when dealing with technical terms.</p><p>Following this suggestion, sentiment analysis tools specific for software datasets have been proposed. Islam and Zibran <ref type="bibr" target="#b14">[15]</ref> developed SentiStrength -SE based on SentiStrength to address the major difficulties by creating domain dictionary and introducing other heuristic rules. The presented evaluation showed that their tool significantly outperformed SentiStrength. <ref type="bibr" target="#b35">[36]</ref> detected the polarity (positive, negative, neutral) of sentences related to API usage by using a customized version of the Sentiment Orientation algorithm <ref type="bibr" target="#b12">[13]</ref>. The algorithm was originally developed to mine and summarize customer opinions about computer products. However, Uddin and Khomh customized the tool with words specific to API reviews. To the best of our knowledge, these are the only cases where the authors tried to customize state-of-the-art sentiment analysis tools to fit the software engineering domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uddin and Khomh</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>We briefly describe our initial plan to build a tool to recommend software libraries to developers given (i) a short description of a task at hand (i.e., functional requirements) and (ii) a list of nonfunctional requirements considered more/less important by the developer for the specific implementation task (e.g., security is of paramount importance, while high performance is nice to have but not really needed).</p><p>The basic idea was to leverage crowdsourced knowledge by mining opinions posted by developers while discussing on Q&amp;A websites such as Stack Overflow. Our plan failed due to very poor results obtained when mining opinions from SE datasets. For this reason, while we present a detailed description of the opinion mining process we adopted, we only provide a brief overview of the overall idea and its different components. The overall idea is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. The dashed arrows represent dependencies (e.g., 1 and 3 ), while the full arrows indicate flows of information pushed from one component to another. The libraries miner mines from the maven central repository<ref type="foot" target="#foot_5">6</ref> all available Java libraries ( 1 in Fig. <ref type="figure" target="#fig_0">1</ref>). We extract for each library its: (i) name, (ii) description, (iii) link to the jar of the latest version, (iv) license, and (v) number and list of clients using it. All the information is stored in our database 2 . The fine-grained linker mines Stack Overflow discussions to establish fine-grained links between the libraries stored in the database 4 and relevant sentences in Stack Overflow discussions 3 .</p><p>Knowing the sentences related to a specific library, the opinion miner component can retrieve them 6 , identify the expressed sentiments (i.e., positive, neutral, or negative), classify opinions on the basis of the non-functional requirements it refers to (e.g., usability, performance, security, community support, etc.), and store them in the database 7 .</p><p>Finally, the developer interested in receiving recommendations about software libraries submits a textual query describing the task in a Web-based front-end and important non-functional requirements 8 .This information is provided to a Web service 9 to identify the most relevant and suitable libraries considering both functional and non-functional requirements.</p><p>In the following we detail our work to create the opinion miner component, where sentiment analysis plays a vital role. We report the negative results we achieved in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mining Opinions in Software Engineering Datasets</head><p>Previous work that attempted to mine opinions in SE datasets <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref> offers a clear warning: Using sentiment analysis/opinion mining techniques out-of-the-box on SE datasets is a recipe for negative results. Indeed, these tools have been designed to work on user's reviews of products/movies and do not take into consideration domain-specific terms. For example, the word robust has a clear positive polarity when referred to a software product, while it does not express a specific sentiment in a movie review. This pushed researchers to create customized versions of these tools, enriching them with information about the sentiment of domain-specific terms (e.g., SentiStrength -SE by Islam and Zibran <ref type="bibr" target="#b14">[15]</ref>). Despite the effort done by some authors in developing customized tools, there is a second major limitation of the sentiment analysis tools mostly used in SE (e.g.,SentiStrength <ref type="bibr" target="#b33">[34]</ref>). Such tools assess the sentiment of a sentence by looking at the single words in isolation, assigning positive/negative scores to the words and then summing these scores to obtain an overall sentiment for the sentence. Thus, the sentence composition is ignored. For example, a sentence such as "I would not recommend this library, even though it is robust and fast" would be assessed by these techniques as positive in polarity, given the presence of words having a positive score (i.e., robust, fast). Such a limitation has been overcome by the Stanford CoreNLP <ref type="bibr" target="#b31">[32]</ref> approach used for the analysis of sentiment in movies' reviews. The approach is based on a Recursive Neural Network (RNN) computing the sentiment of a sentence based on how words compose the meaning of the sentence <ref type="bibr" target="#b31">[32]</ref>. Clearly, a more advanced approach comes at a cost: The effort required to build its training set. Indeed, it is not sufficient to simply provide the polarity for a vocabulary of words but, to learn how positive/negative sentences are grammatically built on top of positive/negative words, it needs to know the polarity of all intermediate nodes composing a sentence used in the training set.</p><p>We discuss the example reported in Fig. <ref type="figure" target="#fig_1">2</ref>. Gray nodes represent (sequences of) words having a neutral polarity, red ones indicate negative sentiment, green ones positive sentiment. Overall, the sentence has a negative sentiment (see the root of the tree in Fig. <ref type="figure" target="#fig_1">2</ref>), despite the presence of several positive terms (the tree's leafs) and intermediate nodes. To use this sentence composed of 14 words in the training set of the RNN, we must provide the sentiment of all 27 nodes depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. This allows the RNN to learn that while "it is robust and fast" has a positive polarity if taken in isolation, the overall sentence is expressing a negative feeling about the library due to the "I would not recommend this library" sub-sentence.</p><p>Given the high context-specificity of our work to SE datasets (i.e., Stack Overflow posts), we decided to adopt the Stanford CoreNLP tool <ref type="bibr" target="#b31">[32]</ref>, and to invest a substantial effort in creating a customized training set for it. Indeed, as highlighted in previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>, it makes no sense to apply an approach trained on movie reviews on SE datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Building a Training</head><p>Set for the Opinion Miner. We extracted from the latest available Stack Overflow dump (dated July 2017) the list of all discussions (i) tagged with Java, and (ii) containing one of the following words: library/libraries, API (s). Given our original goal (i.e., recommending Java libraries on the basis of crowdsourced opinions), we wanted to build a training set as domain-specific as possible for the RNN. By applying these filters, we collected 276,629 discussions from which we extracted 5,073,452 sentences by using the Stanford CoreNLP toolkit <ref type="bibr" target="#b18">[19]</ref>. We randomly selected 1,500 sentences and manually labeled them by assigning a sentiment score to the whole sentence and to every node composing it.</p><p>The labeling process was performed by five of the authors (from now on, evaluators) and supported by a Web application we built. The Web app showed to each evaluator a node (extracted from a sentence) to label with a sentiment going from -2 to +2, with -2 indicating strong negative, -1 weak negative, 0 neutral, +1 weak positive, and +2 strong positive score. The choice of the five-levels sentiment classification was not random, but driven by the observation of the movie reviews training set made publicly available by the authors of the Stanford CoreNLP <ref type="bibr" target="#b31">[32]</ref> sentiment analysis tool 7 . Note that a node to evaluate could be a whole sentence, an intermediate node (thus, a sub-sentence), or a leaf node (i.e., a single word). To avoid any bias, the Web app did not show to the evaluator the complete sentence from which the node was extracted. Indeed, knowing the context in which a word/sentence is used could introduce a bias in the assessment of its sentiment polarity. Finally, the 7 https://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip Web application made sure to have two evaluators for each node, thus reducing the subjectivity bias. This process, which took ∼90 working hours of manual labeling, resulted in the total labeling of the sentiment polarity for 39,924 nodes (i.e., 19,962 nodes extracted from the 1,500 sentences × 2 evaluators per node).</p><p>Once the labeling was completed, two of the authors worked on conflicts resolution (i.e., cases in which two evaluator assigned a different sentiment to the same node). All the 279 conflicts involving complete sentences (18.6% of the labeled sentences) were fixed. Indeed, it is of paramount importance to assign a consistent and double-checked sentiment to the complete sentences, considering the fact that they will be used as a ground truth to evaluate our approach. Concerning the intermediate/leaf nodes, we had a total of 2,199 conflicts (11.9% of the labeled intermediate/leaf nodes). We decided to only manually solve 123 strong conflicts, meaning those for which there was a score difference ≥ 2 (e.g., one of the evaluators gave 1, the other one -1), while we automatically process the 2,076 having a conflict of only one point. Indeed, slight variations of the assigned sentiment (e.g., one evaluator gave 1 and the other 2) are expected due to the subjectivity of the task. The final sentiment score was s, in case there was agreement between the evaluators, while it was round[(s 1 + s 2 )/2] in case of unsolved conflict, where round is the rounding function to the closest integer value and s i is the sentiment assigned by the i th evaluator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">NEGATIVE RESULTS</head><p>Before incorporating the opinion miner component, we decided to assess it individually, and not in the context of the whole library recommendation task. We performed this assessment on the dataset of manually labeled 1,500 sentences. Among those sentences, 178 are positive, 1,191 are neutral, and 131 are negative. We performed a ten-fold cross validation: We divided the 1,500 sentences into ten different sets, each one composed of 150 sentences. Then, we used a set as a test set (we only use the 150 complete sentences in the test set, and not all their intermediate/leaf nodes), while the remaining 1,350 sentences, with all their labeled intermediate/leaf nodes, were used for training <ref type="foot" target="#foot_6">8</ref> . Since we are mostly interested in discriminating between negative, neutral, and positive opinions, we discretized the sentiment in the test set into these three levels. Sentences labeled with "-2" and "-1" are considered negative (-1), those labeled with "0" neutral (0), and those labeled with "+1" and "+2" as positive (+1). We discretized the output of the RNN into the same three levels. We assessed the accuracy of the opinion miner by computing recall and precision for each category. Computing the overall accuracy would not be effective, given the vast majority of neutral opinions in our dataset (i.e., a constant neutral classifier would obtain a high accuracy, ignoring negative and positive opinions).</p><p>Table <ref type="table" target="#tab_1">2</ref> reports the results achieved by applying Stanford Core-NLP SO<ref type="foot" target="#foot_7">9</ref> on sentences extracted from Stack Overflow discussions. The table shows the number of correct predictions, the number of positive/neutral/negative sentences in the batch of testing sets and the corresponding precision/recall values, while the last row reports the overall performance on the whole dataset. Table <ref type="table" target="#tab_2">3</ref> shows some concrete examples of sentiment analysis with Stanford CoreNLP SO. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negative Neutral</head><p>The results shown in Table <ref type="table" target="#tab_1">2</ref> highlight that, despite the specific training, Stanford CoreNLP SO does not achieve good performance in analyzing sentiment of Stack Overflow discussions. Indeed, its precision and recall in detecting positive and negative sentiments is below 40%, thus discouraging its usage as a fundamental part of a recommendation system. Although Stanford CoreNLP SO can correctly identify more negative than positive sentences, only a small fraction of sentences with positive/negative sentiment is identified. Also, there are more mistakenly than correctly identified sentences in both sets.</p><p>Based on the results we achieved, it is impracticable to build on the top of Stanford CoreNLP SO an effective recommender system for libraries: The high percentage of wrong sentiment classification will likely result in the recommendation of the wrong library. Thus, besides the huge effort we spent to train Stanford CoreNLP SO with a specific and large software dataset, we failed in achieving an effective sentiment analysis estimator. For this reason, we decided to change our original plan and perform a deeper analysis of the accuracy of sentiment analysis tools when used on software-related datasets. Specifically, we aim to understand whether (i) domain specific training data really helps in increasing the accuracy of sentiment analysis tool; and whether (ii) other state-of-the-art sentiment analysis tools are able to obtain good results on software engineering datasets, including our manually labeled Stack Overflow dataset. Understanding how these tools perform can also help us in gaining deeper insights into the current state of sentiment analysis for software engineering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATING SENTIMENT ANALYSIS FOR SOFTWARE ENGINEERING</head><p>The goal of the study is to analyze the accuracy of sentiment analysis tools when applied to software engineering datasets, with the purpose of investigating how different contexts can impact their effectiveness. The context of the study consists of text extracted from three software-related datasets, namely Stack Overflow discussions, mobile app reviews, and JIRA issue comments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Research Questions and Context</head><p>The study aims to answer the following research questions: We chose these types of textual documents as they have been studied by SE researchers, also in the context of sentiment analysis <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36]</ref>. As our goal is to evaluate the accuracy of different sentiment analysis tools on these three datasets, we need to define the ground truth sentiment for each of the sentences/texts they contain.</p><formula xml:id="formula_0">RQ 1 :</formula><p>The following process was adopted to collect the three datasets and define their ground truth:</p><p>• Stack Overflow discussions. We reuse the ground truth for the 1,500 sentences used to evaluate Stanford CoreNLP SO. • Mobile app reviews. We randomly selected 341 reviews from the dataset of 3k reviews provided by Villarroel et al. <ref type="bibr" target="#b36">[37]</ref>, which contains manually-labeled reviews classified on the basis of the main information they contain. Four categories are considered: bug reporting, suggestion for new feature, request for improving non-functional requirements (e.g., performance of the app), and other (meaning, reviews not belonging to any of the previous categories). When performing the random selection, we made sure to respect the proportion of reviews belonging to the four categories in the original population in our sample (e.g., if 50% of the 3k reviews belonged to the "other" category, we randomly selected 50% of our sample from that category). The 341 selected reviews represent a statistically significant sample with 95% confidence level ±5% confidence interval.</p><p>Once selected, we manually labeled the sentiment of each review. The labeling process was performed by two of the authors (from now on, evaluators). The evaluators had to decide where the text is positive, neutral, or negative. A third evaluator was involved to solve 51 conflict cases. • JIRA issue comments. We use the dataset collected by Ortu et al. <ref type="bibr" target="#b24">[25]</ref>, containing 4k sentences labeled by three raters with respect to four emotions: love, joy, anger, and sadness. This dataset has been used in several studies as the "golden set" for evaluating sentiment analysis tools <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. During the original labeling process, each sentence was labeled with one of six emotions: love, joy, surprise, anger, sadness, fear. Among these six emotions, love, joy, anger, and sadness are mostly expressed. As also done by Jongeling et al. <ref type="bibr" target="#b15">[16]</ref>, we map the sentences with the label love or joy into positive sentences, and those with label anger or sadness into negative sentences. Table <ref type="table" target="#tab_4">4</ref> reports for each dataset (i) the number of sentences extracted, and (ii) the number of positive, neutral, negative sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Collection and Analysis</head><p>On the three datasets described above we experimented with the following tools, which are popular in the SE research community:</p><p>• SentiStrength. SentiStrength does not give the sentiment of the text directly, instead, it reports two sentiment strength scores of the text analyzed: one score for the negative sentiment expressed in the text from -1 (not negative) to -5 (extremely negative), the other for the positive sentiment expressed from 1 (not positive) to 5 (extremely positive). We sum these two scores, and map the sum of over 0, 0, and below 0 into positive, neutral, and negative, respectively. • NLTK. Based on VADER Sentiment Analysis, NLTK reports four sentiment strength scores for the text analyzed: "negative", "neutral", "positive", and "compound". The scores for "negative", "neutral", and "positive" range from 0 to 1, while the "compound" score is normalized to be between -1 (most extreme negative) and +1 (most extreme positive). As suggested by the author of the VADER component <ref type="foot" target="#foot_8">10</ref> , we use the following thresholds to identify the sentiment of the text analyzed: score ≥ 0.5: positive; -0.5 &lt; score &lt; 0.5: neutral; score ≤ -0.5: negative. and uses the same format of reported results, we interpret its sentiment score by adopting the same approach we used for SentiStrength.</p><p>• Stanford CoreNLP SO. Similarly, we use the same approach adopted for Stanford CoreNLP to convert five-scale values into three-scale values. To examine the performance on app reviews and JIRA issue comments, we used the Stack Overflow labeled sentences (including internal nodes) as training set <ref type="foot" target="#foot_9">11</ref> .</p><p>We assess the accuracy of the tools by computing recall and precision for each of the three considered sentiment categories (i.e., positive, neutral, negative) in each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results</head><p>Table <ref type="table" target="#tab_6">5</ref> reports the results we achieved by applying the five sentiment analysis approaches on the three different SE datasets. The table reports the number of correct predictions made by the tool, and precision/recall for predicting sentiment of positive/neutral/negative sentences. For each dataset/metric, the best achieved results are highlighted in bold. In the following we discuss the achieved results aiming at answering our research questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">RQ 1 : How does our Stanford CoreNLP SO perform as compared to other sentiment analysis tools?</head><p>To answer RQ 1 , we analyze the results achieved by the five tools on the Stack Overflow dataset we built.</p><p>As for the comparison of Stanford CoreNLP SO with the original model of Stanford CoreNLP, the results show that on neutral sentences Stanford CoreNLP SO achieves a better recall while keeping almost the same level of precision. Also, on positive and negative sentences Stanford CoreNLP SO is still able to provide a good increment of the precision. When looking at other tools, the analysis of the results reveal that all the experimented tools achieve comparable results andmore important-none of the experimented tools is able to reliably assess the sentiment expressed in a Stack Overflow sentence. Indeed, while all the tools are able to obtain good results when predicting neutral sentences, their accuracy falls when working on positive and negative sentences. For example, even considering the tool having the highest recall for identifying positive sentences (i.e., SentiStrength) (i) there is only <ref type="bibr" target="#b34">35</ref>.9% chance that it can correctly spot a positive sentence and (ii) one out of five sentences that it will label as positive will be actually false positives (precision=20%). The recall is almost the same as randomly guessing which has 33.3% chance of success. These results reveal that there is still a long way to go before researchers and practitioners can use state-of-the-art sentiment analysis tools to identify the sentiment expressed in Stack Overflow discussions.</p><p>RQ 1 main findings: (i) the training of Stanford CoreNLP on SO discussions does not provide a significant improvement as compared to the original model trained on movie reviews; (ii) the prediction accuracy of all tools are biased towards the majority class (neutral) for which a very good precision and recall is almost always achieved; and (iii) all tools achieve similar performance and it is impossible to identify among them a clear winner or, in any case, a tool ensuring sufficient sentiment assessment of sentences from Stack Overflow discussions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">RQ 2 : Do different software-related datasets impact the performance of sentiment analysis tools?</head><p>To answer RQ 2 , we compare the accuracy of all tools on the three datasets considered in our study. When we look at results for app reviews, we can see that, differently from what observed in the Stack Overflow dataset, most tools can predict positive texts with reasonable precision/recall values. Even for negative reviews, the results are in general much better. It is worth noting that Stanford CoreNLP is competitive for identifying positive and negative sentiment as compared to other tools. Indeed, compared to other texts in software engineering datasets, such as Stack Overflow discussions and JIRA issues, app reviews can be less technical and relatively more similar to movie reviews, with which the original model of Stanford CoreNLP is trained. However, when identifying neutral app reviews, all tools exhibit poor accuracy. This is likely due to the fact that, while positive and negative app reviews could be easily identified by the presence/absence of some "marker terms" (e.g., the presence of the bug term is likely related to negative reviews), this is not the case for the neutral set of reviews, in which a wider and more variegate vocabulary might be used.</p><p>When inspecting results for JIRA issue comments, we find that SentiStrength and SentiStrength -SE have better accuracy than other tools, with SentiStrength -SE providing a better precisionrecall balance across the two categories of sentiment (i.e., positive and negative). Despite the mostly good results achieved by the experimented tools on the JIRA dataset, there are some important issues in the evaluations performed on this dataset.</p><p>First, the absence of neutral sentences does not provide a clear and complete assessment of the accuracy of the tools. Indeed, as shown in the app reviews, neutral texts might be, in some datasets, the most difficult to identify, likely due to the fact that they represent that "grey zone" close to both positive and negative sentiment.</p><p>Second, the JIRA dataset is built by mapping emotions expressed in the comments (e.g., joy or love) into sentiments (e.g., positive). However, such a mapping does not always hold. For instance, positive comments in issue tracker does not always express joy or love (e.g., thanks for the updated patch), thus allowing to obtain a very partial view of the accuracy of sentiment analysis tools.</p><p>To highlight the importance of neutral items in the evaluation of a sentiment analysis tool, Table <ref type="table" target="#tab_7">6</ref> shows the confusion matrices obtained by the five different sentiment analysis tools on the Stack Overflow dataset (see Table <ref type="table" target="#tab_4">4</ref>).</p><p>All tools are effective in discriminating between positive and negative items. For example, our Stanford CoreNLP SO only misclassified two negative sentences as positive, and 16 positive sentences as negative. NLTK only misclassifies five negative sentences as positive, and three positive sentences as negative. The errors are mostly due to negative/positive sentences classified as neutral and vice versa. This confirms the issues found by Tourani et al. <ref type="bibr" target="#b34">[35]</ref> when using SentiStrength on SE data, and this is why evaluating sentiment analysis tools on datasets not containing neutral sentences introduces a considerable bias. Similar observations hold for the app reviews dataset, in which the performance in classifying neutral reviews is, as shown in Table <ref type="table" target="#tab_6">5</ref>, extremely poor. RQ 2 main findings: The accuracy of sentiment analysis tools is, in general, poor on software engineering datasets. We claim this because we found no tool able to reliably discriminating between positive/negative and neutral items. Indeed, while the accuracy on the app reviews and JIRA datasets are acceptable (i) in the app reviews dataset the accuracy in identifying neutral items is very low, and (ii) the data obtained with the JIRA dataset can not be considered as reliable due to the discussed issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">THREATS TO VALIDITY</head><p>Threats to construct validity concern the relation between theory and observation. The first concern is related to our manual sentiment labeling. Sentiment expressed in the text might be misinterpreted by people. Also, the labeling might be impacted by subjective opinions of evaluators. Although we adopted an additional conflict resolving process, it is not guaranteed that the manually assigned sentiment is always correct.</p><p>Another threat is the sentiment score mapping, i.e., mapping five-scale sentiment to three-scale sentiment. Indeed, sentiment expressed in the text have different degrees. Predicting slightly negative sentence as neutral should be considered a smaller mistake then predicting a very negative sentence as neutral, since the threshold to draw a line between the neutral and the negative sentiment can be more subjective.</p><p>Threats to internal validity concern internal factors we did not consider that could affect the variables and the relations being investigated. In our study, they are mainly due to the configuration of sentiment analysis tools/approaches we used. In most cases, we use the default or suggested parameters, for example, the threshold for NLTK. However, some parameters might be further tuned to increase the sentiment prediction performance.</p><p>Threats to conclusion validity concern the relation between the treatment and the outcome. We randomly selected sentences from Stack Overflow discussions and app reviews from an existing dataset <ref type="bibr" target="#b36">[37]</ref>. While we considered statistically significant samples, we cannot guarantee that our samples are representative of the whole population.</p><p>Threats to external validity concern the generalizability of our findings. While the evaluation has considered the most commonly used sentiment analysis tools in software engineering, some less popular tools might have been ignored. Constantly there are lots of new ideas and approaches popping up in the natural language processing domain, but few of them have been examined and verified in the software engineering context. Since our goal is to seek a good sentiment analysis tool for software-related texts, in this paper we only select the tools already used in previous software engineering studies. Our datasets are limited to three frequently mined software engineering repositories, while texts in other contexts, such mailing list and IRC chats, are not considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LESSONS LEARNED</head><p>No tool is ready for real usage of identifying sentiment expressed in SE related discussions yet. No tool, including the ones specifically customized for certain software engineering tasks, is able to provide precision and recall levels sufficient to entail the tool adoption for a task such as recommending software libraries.</p><p>By relying on such tools, we would certainly generate wrong recommendations and miss good ones. Our results are a warning to the research community: Sentiment analysis tools should always be carefully evaluated in the specific context of usage before building something on top of them. For example, while Uddin and Khomh <ref type="bibr" target="#b35">[36]</ref> presented a very interesting approach to mine APIs opinions from Stack Overflow, they do not report the accuracy of the sentiment analysis component they exploit to identify positive/negative opinions about APIs.</p><p>Specific re-training is required, but does not represent a silver bullet for improving the accuracy. Previous literature has pointed our that sentiment analysis tools cannot be used outof-the-box for software engineering tasks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b34">35]</ref>. In some cases, tools have introduced a data preprocessing or a re-training to cope with the specific software engineering lexicon, in which there are positive or negative words/sub-sentences that are not positive or negative in other contexts, or vice versa (e.g., the word bug generally carries a negative sentiment when referred to a library, while it can be considered neutral in movie reviews). However, as results have shown, this might still be insufficient to guarantee good accuracy in terms of both precision and recall on all polarity levels. Also, customization is very dataset specific, and therefore applying the tool on different datasets would require a new training. In other words, customizing a sentiment analysis tool for JIRA does not make it ready for Stack Overflow and vice versa. Finally, some algorithms, such as recursive neural networks, require costly re-training. In our case, the training performed with 1,500 sentences (which turned into labeling almost 40k nodes) revealed to be insufficient for a clear improvement of the Stanford CoreNLP accuracy.</p><p>Some software engineering applications make sentiment analysis easier than others. Sentiment analysis tools perform better on app reviews. App reviews contain sentences that, in most cases, clearly express the opinion of a user, who wants to reward an app or penalize it, by pointing out a nice feature or a serious problem. Hence, the context is very similar to what those sentiment tools are familiar with. Still, as observed, the tools' performance on the neutral category is very poor. Looking at the issue tracker data, besides the lack of neutral sentences in the JIRA dataset (which per se makes the life of the sentiment analysis tools much easier), again the predominance of problem-reporting sentences may (slightly) play in favour of such tools. Stack Overflow is a different beast. Posts mostly contain discussions on how to use a piece of technology, and between the lines somebody points out whether an API or a code pattern is good or less optimal. In many cases, without even expressing strong opinions. This definitely makes the applicability of sentiment analysis much more difficult.</p><p>Should we expect 100% accuracy from sentiment analysis tools? No, we should not. In our manual evaluation, out of the 1,500 Stack Overflow sentences we manually labeled, there were 279 cases of disagreement (18.6%). This means that even humans are not able to agree about the sentiment expressed in a given sentence. This is also in line with findings of Murgia et al. <ref type="bibr" target="#b20">[21]</ref> on emotion mining: Except when a sentence expresses clear emotions of love, joy and sadness, even for humans it is hard to agree. Hence, it is hard to expect that an automated tool can do any better. Having said that, advances are still needed to make sentiment analysis tools usable in the software engineering domain.</p><p>Text reporting positive and negative sentiment is not sufficient to evaluate sentiment analysis tools. As discussed, the most difficult task for sentiment analysis tools is to discriminate between positive/negative vs neutral sentiment, while they are quite effective in discriminating between positive and negative sentiment. This is why datasets such as the JIRA one that we, and others, used in previous work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, is not sufficient to evaluate sentiment analysis tools. We hope that releasing our dataset <ref type="bibr" target="#b17">[18]</ref> will help in more robust evaluations of sentiment analysis tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>Some say that the road to hell is paved with good intentions. Our work started out with what we consider a promising idea: We wanted to develop an approach to automatically recommend APIs and libraries given a set of functional and non-functional requirements. To do so, we wanted to leverage the large body of knowledge that is stored in Q&amp;A websites like Stack Overflow. The approach was going to exploit opinion mining using deep learning through recurrent neural network. However, as we finalized our work we noticed that it simply did not work, because the opinion mining component had unacceptable performance.</p><p>The reason for the failure is manifold. Firstly, it highlights how machine learning, even in its most advanced forms, is and remains a black box, and it is not completely clear what happens in that black box. To this one can add the design principle "garbage in, garbage out": No matter how advanced a technique, if the input is not appropriate, it is improbable that an acceptable output can be produced. In the specific case one might argue that Stack Overflow is not really the place where emotions run high: It is a place where developers discuss technicalities. Therefore it is rather obvious that opinion mining will have a hard time. While this might be true, our study revealed that also in datasets where emotions are more evident, like app reviews and issue trackers, there is an intrinsic problem with the accuracy of current state-of-the-art sentiment analysis tools.</p><p>In the end we decided to write a "negative results" paper. As Walter Tichy writes, "Negative results, if trustworthy, are extremely important for narrowing down the search space. They eliminate useless hypotheses and thus reorient and speed up the search for better approaches". We hope that the software engineering community can appreciate and leverage the insights that we obtained during our work. We are also releasing the complete dataset as a replication package. As a final word, we would like to stress that we are not dismissing opinion mining in software engineering as impractical, but rather as not mature enough yet. We believe there is promise in the field, but that a community effort is required to bring opinion mining to a level where it actually becomes useful and usable in practice.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Our vision of the library recommender system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of the labeling needed to build the Stanford CoreNLP training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 : Sentiment analysis tools used for SE applications.</head><label>1</label><figDesc></figDesc><table><row><cell>Tool</cell><cell>Technique</cell><cell>Trained on</cell><cell>Used by</cell></row><row><cell>SentiStrength</cell><cell>Rule-based</cell><cell>MySpace</cell><cell>[7-11] [15, 22, 24] [31, 33] [36]</cell></row><row><cell>NLTK/VADER</cell><cell>Rule-based</cell><cell>Micro-Blog</cell><cell>[30] [28]</cell></row><row><cell>Stanford CoreNLP</cell><cell>Recurs. Neural Net</cell><cell>Movie Reviews</cell><cell>[29], our work</cell></row><row><cell>EmoText</cell><cell>Lexical Features</cell><cell cols="2">Stack Overflow, JIRA [2]</cell></row><row><cell cols="2">SentiStrength -SE SentiStrength</cell><cell>JIRA</cell><cell>[15]</cell></row><row><cell>Uddin and Khomh</cell><cell cols="2">Sentim. Orientation [13] API Reviews</cell><cell>[36]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 : Testing results of Stanford CoreNLP sentiment analyzer with new model trained with Stack Overflow discussions.</head><label>2</label><figDesc></figDesc><table><row><cell>batch</cell><cell># correct prediction</cell><cell># positive sentences</cell><cell>positive precision</cell><cell>positive recall</cell><cell># neutral sentences</cell><cell>neutral precision</cell><cell>neutral recall</cell><cell># negative sentences</cell><cell>negative precision</cell><cell>negative recall</cell></row><row><cell>1</cell><cell>113</cell><cell>10</cell><cell>0.250</cell><cell>0.200</cell><cell>118</cell><cell>0.835</cell><cell>0.898</cell><cell>22</cell><cell>0.333</cell><cell>0.227</cell></row><row><cell>2</cell><cell>112</cell><cell>15</cell><cell>0.294</cell><cell>0.333</cell><cell>118</cell><cell>0.853</cell><cell>0.839</cell><cell>17</cell><cell>0.471</cell><cell>0.471</cell></row><row><cell>3</cell><cell>116</cell><cell>15</cell><cell>0.000</cell><cell>0.000</cell><cell>121</cell><cell>0.819</cell><cell>0.934</cell><cell>14</cell><cell>0.273</cell><cell>0.214</cell></row><row><cell>4</cell><cell>123</cell><cell>9</cell><cell>0.600</cell><cell>0.333</cell><cell>122</cell><cell>0.875</cell><cell>0.918</cell><cell>19</cell><cell>0.471</cell><cell>0.421</cell></row><row><cell>5</cell><cell>110</cell><cell>10</cell><cell>0.167</cell><cell>0.100</cell><cell>119</cell><cell>0.833</cell><cell>0.840</cell><cell>21</cell><cell>0.375</cell><cell>0.429</cell></row><row><cell>6</cell><cell>129</cell><cell>11</cell><cell>0.600</cell><cell>0.273</cell><cell>118</cell><cell>0.891</cell><cell>0.975</cell><cell>21</cell><cell>0.688</cell><cell>0.524</cell></row><row><cell>7</cell><cell>93</cell><cell>6</cell><cell>0.111</cell><cell>0.167</cell><cell>130</cell><cell>0.911</cell><cell>0.631</cell><cell>14</cell><cell>0.196</cell><cell>0.714</cell></row><row><cell>8</cell><cell>117</cell><cell>17</cell><cell>0.400</cell><cell>0.118</cell><cell>116</cell><cell>0.809</cell><cell>0.948</cell><cell>17</cell><cell>0.556</cell><cell>0.294</cell></row><row><cell>9</cell><cell>111</cell><cell>18</cell><cell>0.333</cell><cell>0.056</cell><cell>113</cell><cell>0.770</cell><cell>0.947</cell><cell>19</cell><cell>0.375</cell><cell>0.158</cell></row><row><cell>10</cell><cell>115</cell><cell>20</cell><cell>1.000</cell><cell>0.050</cell><cell>116</cell><cell>0.799</cell><cell>0.957</cell><cell>14</cell><cell>0.300</cell><cell>0.214</cell></row><row><cell>Overall</cell><cell>1139</cell><cell>131</cell><cell>0.317</cell><cell>0.145</cell><cell>1191</cell><cell>0.836</cell><cell>0.886</cell><cell>178</cell><cell>0.365</cell><cell>0.365</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 : Examples of sentiment analysis results of Stanford CoreNLP SO.</head><label>3</label><figDesc></figDesc><table><row><cell>Sentence</cell><cell>Oracle</cell><cell>Prediction</cell></row><row><cell>It even works on Android.</cell><cell>Positive</cell><cell>Positive</cell></row><row><cell>Hope that helps some of you with the same problem.</cell><cell>Positive</cell><cell>Negative</cell></row><row><cell>There is a central interface to access this API.</cell><cell>Neutral</cell><cell>Neutral</cell></row><row><cell>How is blocking performed?</cell><cell>Neutral</cell><cell>Negative</cell></row><row><cell>I am not able to deploy my App Engine project locally.</cell><cell>Negative</cell><cell>Negative</cell></row><row><cell>Anyway, their current behavior does not allow what you</cell><cell></cell><cell></cell></row><row><cell>want.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>How does our Stanford CoreNLP SO perform compared to other sentiment analysis tools? We want to verify whether other state-of-the-art tools are able to achieve better accuracy on the Stack Overflow dataset we manually built, thus highlighting limitations of Stanford CoreNLP SO. Indeed, it could be that our choice of the Stanford CoreNLP and therefore of developing Stanford CoreNLP SO was not the most suitable one, and other existing tools already provide better performance.</figDesc><table><row><cell>RQ 2 : Do different software-related datasets impact the performance</cell></row><row><cell>of sentiment analysis tools? We want to investigate the extent to</cell></row><row><cell>which, analyzing other kinds of software engineering datasets,</cell></row><row><cell>e.g., issue comments and app reviews, sentiment analysis tools</cell></row><row><cell>would achieve different performance than for Stack Overflow</cell></row><row><cell>posts. For example, such sources might contain less neutral</cell></row><row><cell>sentences and, the app reviews in particular, be more similar to</cell></row><row><cell>the typical training sets of sentiment analysis tools.</cell></row></table><note><p>The context of the study consists of textual documents from three different SE repositories, i.e., (i) Question &amp; Answer forums, i.e., Stack Overflow discussions, (ii) app stores, i.e., users' reviews on mobile apps, and (iii) issue trackers, i.e., JIRA issue comments.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 : Dataset used for evaluating sentiment analysis tools in software engineering</head><label>4</label><figDesc></figDesc><table><row><cell>Dataset</cell><cell cols="4"># sentences # positive # neutral # negative</cell></row><row><cell>Stack Overflow</cell><cell>1,500</cell><cell>178</cell><cell>1,191</cell><cell>131</cell></row><row><cell>App reviews</cell><cell>341</cell><cell>186</cell><cell>25</cell><cell>130</cell></row><row><cell>JIRA issue</cell><cell>926</cell><cell>290</cell><cell>0</cell><cell>636</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>•</head><label></label><figDesc>Stanford CoreNLP. By default, Stanford CoreNLP reports the sentiment of the text on a five-value scale: very negative, negative, neutral, positive, and very positive. Since we are only interested in discriminating between negative, neutral, and positive opinions, we merged very negative into negative, and very positive into positive.</figDesc><table /><note><p>• SentiStrength-SE. As it is a tool based on SentiStrength,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 : Evaluation results for sentiment analysis tools applied in software engineering domain. In bold the best results.</head><label>5</label><figDesc>Stanford CoreNLP SO provides levels of recall lower than Stanford CoreNLP. The comparison between Stanford CoreNLP and Stanford CoreNLP SO should be read taking into account that the original Stanford CoreNLP model is trained on over 10k labeled sentences (i.e., &gt;215k nodes). Stanford CoreNLP SO is trained on a smaller training set. Thus, it is possible that a larger training set could improve the performance of Stanford CoreNLP SO. However, as of now, this is a mere conjecture.</figDesc><table><row><cell>dataset</cell><cell>tool</cell><cell># correct prediction</cell><cell>positive precision</cell><cell>positive recall</cell><cell>neutral precision</cell><cell>neutral recall</cell><cell>negative precision</cell><cell>negative recall</cell></row><row><cell></cell><cell>SentiStrength</cell><cell>1,043</cell><cell>0.200</cell><cell>0.359</cell><cell>0.858</cell><cell>0.772</cell><cell>0.397</cell><cell>0.433</cell></row><row><cell></cell><cell>NLTK</cell><cell>1,168</cell><cell>0.317</cell><cell>0.244</cell><cell>0.815</cell><cell>0.941</cell><cell>0.625</cell><cell>0.084</cell></row><row><cell>Stack Overflow</cell><cell>Stanford CoreNLP</cell><cell>604</cell><cell>0.231</cell><cell>0.344</cell><cell>0.884</cell><cell>0.344</cell><cell>0.177</cell><cell>0.837</cell></row><row><cell></cell><cell>SentiStrength-SE</cell><cell>1,170</cell><cell>0.312</cell><cell>0.221</cell><cell>0.826</cell><cell>0.930</cell><cell>0.500</cell><cell>0.185</cell></row><row><cell></cell><cell>Stanford CoreNLP SO</cell><cell>1,139</cell><cell>0.317</cell><cell>0.145</cell><cell>0.836</cell><cell>0.886</cell><cell>0.365</cell><cell>0.365</cell></row><row><cell></cell><cell>SentiStrength</cell><cell>213</cell><cell>0.745</cell><cell>0.866</cell><cell>0.113</cell><cell>0.320</cell><cell>0.815</cell><cell>0.338</cell></row><row><cell></cell><cell>NLTK</cell><cell>184</cell><cell>0.751</cell><cell>0.812</cell><cell>0.093</cell><cell>0.440</cell><cell>1.000</cell><cell>0.169</cell></row><row><cell>App reviews</cell><cell>Stanford CoreNLP</cell><cell>237</cell><cell>0.831</cell><cell>0.715</cell><cell>0.176</cell><cell>0.240</cell><cell>0.667</cell><cell>0.754</cell></row><row><cell></cell><cell>SentiStrength-SE</cell><cell>201</cell><cell>0.741</cell><cell>0.817</cell><cell>0.106</cell><cell>0.400</cell><cell>0.929</cell><cell>0.300</cell></row><row><cell></cell><cell>Stanford CoreNLP SO</cell><cell>142</cell><cell>0.770</cell><cell>0.253</cell><cell>0.084</cell><cell>0.320</cell><cell>0.470</cell><cell>0.669</cell></row><row><cell></cell><cell>SentiStrength</cell><cell>714</cell><cell>0.850</cell><cell>0.921</cell><cell>-</cell><cell>-</cell><cell>0.993</cell><cell>0.703</cell></row><row><cell></cell><cell>NLTK</cell><cell>276</cell><cell>0.840</cell><cell>0.362</cell><cell>-</cell><cell>-</cell><cell>1.000</cell><cell>0.269</cell></row><row><cell>JIRA issues</cell><cell>Stanford CoreNLP</cell><cell>626</cell><cell>0.726</cell><cell>0.621</cell><cell>-</cell><cell>-</cell><cell>0.945</cell><cell>0.701</cell></row><row><cell></cell><cell>SentiStrength-SE</cell><cell>704</cell><cell>0.948</cell><cell>0.883</cell><cell>-</cell><cell>-</cell><cell>0.996</cell><cell>0.704</cell></row><row><cell></cell><cell>Stanford CoreNLP SO</cell><cell>333</cell><cell>0.635</cell><cell>0.252</cell><cell></cell><cell>-</cell><cell>0.724</cell><cell>0.409</cell></row><row><cell cols="4">However, in this case the increment of precision has a price</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>to pay:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 : Confusion matrices on the Stack Overflow dataset.</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>SentiStrength</cell><cell></cell></row><row><cell></cell><cell cols="3">positive neutral negative</cell></row><row><cell>positive</cell><cell>47</cell><cell>66</cell><cell>18</cell></row><row><cell>neutral</cell><cell>173</cell><cell>919</cell><cell>99</cell></row><row><cell>negative</cell><cell>15</cell><cell>86</cell><cell>77</cell></row><row><cell></cell><cell></cell><cell>NLTK</cell><cell></cell></row><row><cell></cell><cell cols="3">positive neutral negative</cell></row><row><cell>positive</cell><cell>32</cell><cell>96</cell><cell>3</cell></row><row><cell>neutral</cell><cell>64</cell><cell>1121</cell><cell>6</cell></row><row><cell>negative</cell><cell>5</cell><cell>158</cell><cell>15</cell></row><row><cell></cell><cell cols="3">Stanford CoreNLP</cell></row><row><cell></cell><cell cols="3">positive neutral negative</cell></row><row><cell>positive</cell><cell>45</cell><cell>30</cell><cell>56</cell></row><row><cell>neutral</cell><cell>145</cell><cell>410</cell><cell>636</cell></row><row><cell>negative</cell><cell>5</cell><cell>24</cell><cell>149</cell></row><row><cell></cell><cell cols="3">SentiStrength-SE</cell></row><row><cell></cell><cell cols="3">positive neutral negative</cell></row><row><cell>positive</cell><cell>29</cell><cell>93</cell><cell>9</cell></row><row><cell>neutral</cell><cell>59</cell><cell>1108</cell><cell>24</cell></row><row><cell>negative</cell><cell>5</cell><cell>140</cell><cell>33</cell></row><row><cell></cell><cell cols="3">Stanford CoreNLP SO</cell></row><row><cell></cell><cell cols="3">positive neutral negative</cell></row><row><cell>positive</cell><cell>19</cell><cell>96</cell><cell>16</cell></row><row><cell>neutral</cell><cell>39</cell><cell>1055</cell><cell>97</cell></row><row><cell>negative</cell><cell>2</cell><cell>111</cell><cell>65</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.meaningcloud.com/developer/sentiment-analysis</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://getsentiment.3scale.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.ibm.com/watson/services/natural-language-understanding/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://rapidminer.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://myspace.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://central.maven.org/maven2/maven/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>The StanfordCoreNLP tool requires-during the training of the neural network-a so called development set to tune some internal parameters of the network. Among the 1,350 sentences with intermediate/leaf nodes in training set we randomly selected 300 sentences for composing the development set at each run.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>Stanford CoreNLP SO is the name of the tool with our new model trained with Stack Overflow discussions, while StanfordCoreNLP is the sentiment analysis component of StanfordCoreNLP with the default model trained using movie reviews.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>https://github.com/cjhutto/vaderSentiment</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>In this case, 20% of the training set was used as development set.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We gratefully acknowledge the financial support of the Swiss National Science Foundation for the projects PROBE (SNF Project No. 172799) and JITRA (SNF Project No. 172479), and CHOOSE for sponsoring our trip to the conference.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC 2010 (International Conference on Language Resources and Evaluation</title>
		<meeting>LREC 2010 (International Conference on Language Resources and Evaluation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">EmoTxt: A Toolkit for Emotion Recognition from Text</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Calefato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Lanubile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Novielli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACII 2017 (7th International Conference on Affective Computing and Intelligent Interaction)</title>
		<meeting>ACII 2017 (7th International Conference on Affective Computing and Intelligent Interaction)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Analysis of user comments: an approach for software requirements evolution</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V G</forename><surname>Carreño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Winbladh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSE 2013 (35th International Conference on Software Engineering)</title>
		<meeting>ICSE 2013 (35th International Conference on Software Engineering)</meeting>
		<imprint>
			<publisher>IEEE press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="582" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anger and Its Direction in Collaborative Software Development</title>
		<author>
			<persName><forename type="first">Daviti</forename><surname>Gachechiladze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Lanubile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Novielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Serebrenik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSE 2017 (39th IEEE/ACM International Conference on Software Engineering)</title>
		<meeting>ICSE 2017 (39th IEEE/ACM International Conference on Software Engineering)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Role of Emotions in Contributors Activity: A Case Study on the GENTOO Community</title>
		<author>
			<persName><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><forename type="middle">Serrano</forename><surname>Zanetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Schweitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CGC 2013 (3rd International Conference on Cloud and Green Computing) (CGC &apos;13</title>
		<meeting>CGC 2013 (3rd International Conference on Cloud and Green Computing) (CGC &apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="410" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Managing the Enterprise Business Intelligence App Store: Sentiment Analysis Supported Requirements Engineering</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Goul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivera</forename><surname>Marjanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Baxley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Vizecky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HICSS 2012 (45th Hawaii International Conference on System Sciences</title>
		<meeting>HICSS 2012 (45th Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="4168" to="4177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An exploratory study of Twitter messages about software applications</title>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Alkadhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norbert</forename><surname>Seyff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Requirements Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="387" to="412" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Retrieving Diverse Opinions from App Reviews</title>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bruegge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ESEM 2015 (9th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement</title>
		<meeting>ESEM 2015 (9th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sentiment analysis of commit comments in GitHub: an empirical study</title>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Azócar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2014 (11th Working Conference on Mining Software Repositories)</title>
		<meeting>MSR 2014 (11th Working Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="352" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards emotional awareness in software development teams</title>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Bruegge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ESEC/FSE 2013 (9th Joint Meeting on Foundations of Software Engineering)</title>
		<meeting>ESEC/FSE 2013 (9th Joint Meeting on Foundations of Software Engineering)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="671" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How do users like this feature? a fine grained sentiment analysis of app reviews</title>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Maalej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of RE 2014 (22nd International Requirements Engineering Conference</title>
		<meeting>RE 2014 (22nd International Requirements Engineering Conference</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The WEKA Data Mining Software: An Update</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mining and summarizing customer reviews</title>
		<author>
			<persName><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD 2004 (10th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>KDD 2004 (10th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="168" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Clayton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName><surname>Gilbert</surname></persName>
		</author>
		<title level="m">Proceedings of ICWSM 2014 (8th International AAAI Conference on Weblogs and Social Media</title>
		<meeting>ICWSM 2014 (8th International AAAI Conference on Weblogs and Social Media</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leveraging automated sentiment analysis in software engineering</title>
		<author>
			<persName><forename type="first">Rakibul</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhaz</forename><forename type="middle">F</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><surname>Zibran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2017 (14th International Conference on Mining Software Repositories)</title>
		<meeting>MSR 2017 (14th International Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="203" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On negative results when using sentiment analysis tools for software engineering research</title>
		<author>
			<persName><forename type="first">Robbert</forename><surname>Jongeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Proshanta</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhajit</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Serebrenik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Empirical Software Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sentiment Analysis in monitoring software development processes: An exploratory case study on GitHub&apos;s project issues</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Jurado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pilar</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="82" to="89" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Bin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fiorella</forename><surname>Zampetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Di Penta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Lanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Oliveto</surname></persName>
		</author>
		<ptr target="https://sentiment-se.github.io/replication.zip" />
		<title level="m">Replication Package</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">Association for Computational Linguistics (ACL) System Demonstrations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>The Stanford CoreNLP Natural Language Processing Toolkit</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Do developers feel emotions? an exploratory analysis of emotions in software artifacts</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Murgia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parastou</forename><surname>Tourani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bram</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ortu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2014 (11th Working Conference on Mining Software Repositories)</title>
		<meeting>MSR 2014 (11th Working Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="262" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards discovering the role of emotions in stack overflow</title>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Novielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Calefato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Lanubile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSE 2014 (6th International Workshop on Social Software Engineering)</title>
		<meeting>SSE 2014 (6th International Workshop on Social Software Engineering)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="33" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Challenges of Sentiment Detection in the Social Programmer Ecosystem</title>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Novielli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Calefato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Lanubile</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SSE 2015 (7th International Workshop on Social Software Engineering)</title>
		<meeting>SSE 2015 (7th International Workshop on Social Software Engineering)</meeting>
		<imprint>
			<publisher>SSE</publisher>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Are bullies more productive?: empirical study of affectiveness vs. issue fixing time</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ortu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bram</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Destefanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parastou</forename><surname>Tourani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Marchesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2015 (12th Working Conference on Mining Software Repositories)</title>
		<meeting>MSR 2015 (12th Working Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="303" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The emotional side of software developers in JIRA</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ortu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Murgia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Destefanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parastou</forename><surname>Tourani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Marchesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bram</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2016 (13th International Conference on Mining Software Repositories)</title>
		<meeting>MSR 2016 (13th International Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="480" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Opinion Mining and Sentiment Analysis</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lillian</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="135" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How Can I Improve My App? Classifying User Reviews for Software Maintenance and Evolution</title>
		<author>
			<persName><forename type="first">Sebastiano</forename><surname>Panichella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><forename type="middle">Di</forename><surname>Sorbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emitza</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corrado</forename><forename type="middle">A</forename><surname>Visaggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><surname>Canfora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><forename type="middle">C</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSME 2015 (31st International Conference on Software Maintenance and Evolution</title>
		<meeting>ICSME 2015 (31st International Conference on Software Maintenance and Evolution</meeting>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Security and emotion: sentiment analysis of security discussions on GitHub</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Pletea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Vasilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Serebrenik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2014 (11th Working Conference on Mining Software Repositories)</title>
		<meeting>MSR 2014 (11th Working Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="348" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recommending insightful comments for source code using crowdsourced knowledge</title>
		<author>
			<persName><forename type="first">Chanchal K</forename><surname>Mohammad Masudur Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iman</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><surname>Keivanloo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SCAM 2015 (15th International Working Conference on Source Code Analysis and Manipulation</title>
		<meeting>SCAM 2015 (15th International Working Conference on Source Code Analysis and Manipulation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="81" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sentiment analysis of Free/Open Source developers: preliminary findings from a case study</title>
		<author>
			<persName><forename type="first">Athanasios-Ilias</forename><surname>Rousinopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregorio</forename><surname>Robles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesús</forename><forename type="middle">M</forename><surname>González-Barahona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Revista Eletronica de Sistemas de Informacao</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Analyzing developer sentiment in commit logs</title>
		<author>
			<persName><forename type="first">Vinayak</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonita</forename><surname>Sharif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2016 (13th International Conference on Mining Software Repositories)</title>
		<meeting>MSR 2016 (13th International Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="520" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note>Proceedings of EMNLP 2013</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sentiment analysis of Travis CI builds</title>
		<author>
			<persName><forename type="first">Rodrigo</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MSR 2017 (14th International Conference on Mining Software Repositories)</title>
		<meeting>MSR 2017 (14th International Conference on Mining Software Repositories)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="459" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentiment strength detection in short informal text</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevan</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvid</forename><surname>Kappas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Association for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="2544" to="2558" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monitoring sentiment in open source mailing lists: exploratory study on the apache ecosystem</title>
		<author>
			<persName><forename type="first">Parastou</forename><surname>Tourani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bram</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CASCON 2014 (24th Annual International Conference on Computer Science and Software Engineering)</title>
		<meeting>CASCON 2014 (24th Annual International Conference on Computer Science and Software Engineering)</meeting>
		<imprint>
			<publisher>IBM Corp</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Mining API Aspects in API Reviews</title>
		<author>
			<persName><forename type="first">Gias</forename><surname>Uddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Foutse</forename><surname>Khomh</surname></persName>
		</author>
		<ptr target="http://swat.polymtl.ca/data/opinionvalue-technical-report.pdf" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Release planning of mobile apps based on user reviews</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Villarroel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Bavota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rocco</forename><surname>Oliveto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penta</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICSE 2016 (38th International Conference on Software Engineering)</title>
		<meeting>ICSE 2016 (38th International Conference on Software Engineering)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Extracting Problematic API Features from Forum Discussions</title>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daqing</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">st International Conference on Program Comprehension</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="141" to="151" />
		</imprint>
	</monogr>
	<note>Proceedings of ICPC 2013</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
